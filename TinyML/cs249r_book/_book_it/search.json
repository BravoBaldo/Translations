[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning Systems",
    "section": "",
    "text": "Prefazione\nBenvenuti in Machine Learning Systems. Questo libro è la porta d’accesso al mondo frenetico dei sistemi di intelligenza artificiale. È un’estensione del corso CS249r alla Harvard University.\nAbbiamo creato questo libro open source come sforzo collaborativo per riunire spunti di studenti, professionisti e la più ampia comunità di professionisti dell’IA. Il nostro obiettivo è sviluppare una guida completa che esplori le complessità dei sistemi di intelligenza artificiale e le loro numerose applicazioni.\nQuesto non è un libro statico; è un documento vivo e pulsante. Lo stiamo rendendo open source e lo aggiorniamo costantemente per soddisfare le esigenze in continua evoluzione di questo campo dinamico. Contiene un ricco mix di conoscenze specialistiche che guideranno attraverso la complessa interazione tra algoritmi all’avanguardia e i principi fondamentali che li fanno funzionare. Stiamo preparando il terreno per il prossimo grande balzo nell’innovazione dell’IA.",
    "crumbs": [
      "Prefazione"
    ]
  },
  {
    "objectID": "index.html#perché-abbiamo-scritto-questo-libro",
    "href": "index.html#perché-abbiamo-scritto-questo-libro",
    "title": "Machine Learning Systems",
    "section": "Perché Abbiamo Scritto Questo Libro",
    "text": "Perché Abbiamo Scritto Questo Libro\nViviamo in un’epoca in cui la tecnologia è in continua evoluzione. La collaborazione aperta e la condivisione delle conoscenze sono gli elementi costitutivi della vera innovazione. Questo è lo spirito alla base di questo lavoro. Andiamo oltre il tradizionale modello di libro di testo per creare un hub di conoscenza vivo, in modo che possiamo tutti condividere e imparare gli uni dagli altri.\nIl libro si concentra sui principi e sui casi di studio dei sistemi di IA, con l’obiettivo di fornire una comprensione approfondita che aiuterà a navigare nel panorama in continua evoluzione dei sistemi di IA. Mantenendolo “open source”, non stiamo solo rendendo accessibile l’apprendimento, ma stiamo anche invitando nuove idee e miglioramenti continui. In breve, stiamo costruendo una comunità in cui la conoscenza è libera di crescere e illuminare la strada verso la tecnologia AI globale.",
    "crumbs": [
      "Prefazione"
    ]
  },
  {
    "objectID": "index.html#cosa-cè-da-sapere",
    "href": "index.html#cosa-cè-da-sapere",
    "title": "Machine Learning Systems",
    "section": "Cosa c’è da Sapere",
    "text": "Cosa c’è da Sapere\nPer immergersi in questo libro, non si dev’essere un esperto di AI. Tutto ciò di cui c’è bisogno è una conoscenza di base dei concetti di informatica e la curiosità di esplorare su come funzionano i sistemi AI. È qui che avviene l’innovazione e una conoscenza di base della programmazione e delle strutture dati sarà la bussola.",
    "crumbs": [
      "Prefazione"
    ]
  },
  {
    "objectID": "index.html#dichiarazione-di-trasparenza-dei-contenuti",
    "href": "index.html#dichiarazione-di-trasparenza-dei-contenuti",
    "title": "Machine Learning Systems",
    "section": "Dichiarazione di Trasparenza dei Contenuti",
    "text": "Dichiarazione di Trasparenza dei Contenuti\nQuesto libro è un progetto guidato dalla comunità, con contenuti generati da numerosi collaboratori nel tempo. Il processo di creazione dei contenuti potrebbe aver coinvolto vari strumenti di editing, tra cui la tecnologia AI generativa. In qualità di autore principale, editore e curatore, il Prof. Vijay Janapa Reddi mantiene la supervisione umana e la supervisione editoriale per garantire che il contenuto sia accurato e pertinente. Tuttavia, nessuno è perfetto, quindi potrebbero comunque esserci delle inesattezze. Apprezziamo molto i feedback e invitiamo a fornire correzioni e suggerimenti. Questo approccio collaborativo è fondamentale per migliorare e mantenere la qualità del contenuto e rendere le informazioni accessibili a livello globale.",
    "crumbs": [
      "Prefazione"
    ]
  },
  {
    "objectID": "index.html#per-dare-una-mano",
    "href": "index.html#per-dare-una-mano",
    "title": "Machine Learning Systems",
    "section": "Per dare una mano",
    "text": "Per dare una mano\nSe si è interessati a contribuire, le linee guida si trovano qui.",
    "crumbs": [
      "Prefazione"
    ]
  },
  {
    "objectID": "index.html#contatti",
    "href": "index.html#contatti",
    "title": "Machine Learning Systems",
    "section": "Contatti",
    "text": "Contatti\nCi sono domande o feedback? Si è liberi di inviare una e-mail al Prof. Vijay Janapa Reddi direttamente, oppure si può avviare un thread di discussione su GitHub.",
    "crumbs": [
      "Prefazione"
    ]
  },
  {
    "objectID": "index.html#collaboratori",
    "href": "index.html#collaboratori",
    "title": "Machine Learning Systems",
    "section": "Collaboratori",
    "text": "Collaboratori\nUn grande ringraziamento a tutti coloro che hanno contribuito a rendere questo libro quello che è! L’elenco completo dei singoli collaboratori è qui e ulteriori dettagli sullo stile GitHub qui. Benvenuti come collaboratori!",
    "crumbs": [
      "Prefazione"
    ]
  },
  {
    "objectID": "index.html#copyright",
    "href": "index.html#copyright",
    "title": "Machine Learning Systems",
    "section": "Copyright",
    "text": "Copyright\nQuesto libro è open source e sviluppato in modo collaborativo tramite GitHub. Salvo diversa indicazione, questo lavoro è concesso in licenza con Creative Commons Attribuzione-Non commerciale-Condividi allo stesso modo 4.0 Internazionale (CC BY-NC-SA 4.0 CC BY-SA 4.0). Il testo completo della licenza si trova qui.\nI collaboratori di questo progetto hanno dedicato i loro contributi al pubblico dominio o con la stessa licenza aperta del progetto originale. Sebbene i contributi siano collaborativi, ogni collaboratore mantiene il copyright sui rispettivi contributi.\nPer i dettagli sulla paternità, i contributi e come contribuire, consultare il repository del progetto su GitHub.\nTutti i marchi e i marchi registrati menzionati in questo libro sono di proprietà dei rispettivi proprietari.\nLe informazioni fornite in questo libro sono ritenute accurate e affidabili. Tuttavia, gli autori, i curatori e gli editori non possono essere ritenuti responsabili per eventuali danni causati o presumibilmente causati, direttamente o indirettamente, dalle informazioni contenute nel presente libro.",
    "crumbs": [
      "Prefazione"
    ]
  },
  {
    "objectID": "contents/core/acknowledgements/acknowledgements.it.html",
    "href": "contents/core/acknowledgements/acknowledgements.it.html",
    "title": "Ringraziamenti",
    "section": "",
    "text": "Agenzie e Aziende Finanziatrici\nSiamo grati per il supporto di varie agenzie di finanziamento e aziende che hanno sostenuto gli assistenti didattici coinvolti in questo lavoro. Le seguenti organizzazioni hanno svolto un ruolo cruciale nel dare vita a questo progetto:",
    "crumbs": [
      "Ringraziamenti"
    ]
  },
  {
    "objectID": "contents/core/acknowledgements/acknowledgements.it.html#collaboratori",
    "href": "contents/core/acknowledgements/acknowledgements.it.html#collaboratori",
    "title": "Ringraziamenti",
    "section": "Collaboratori",
    "text": "Collaboratori\nEsprimiamo la nostra sincera gratitudine alla comunità open source di studenti, educatori e collaboratori. Ogni contributo, che si tratti di una sezione di capitolo o di una correzione di una sola parola, ha migliorato significativamente la qualità di questa risorsa. Riconosciamo anche coloro che hanno condiviso intuizioni, identificato problemi e fornito feedback preziosi dietro le quinte.\nUn elenco completo di tutti i collaboratori di GitHub, aggiornato automaticamente con ogni nuovo contributo, è disponibile di seguito. Per quelli interessati a contribuire ulteriormente, consultare la nostra pagina GitHub per maggiori informazioni.\n\n\n\n\n\n\n\n\nVijay Janapa Reddi\n\n\njasonjabbour\n\n\nIkechukwu Uchendu\n\n\nNaeem Khoshnevis\n\n\nMarcelo Rovai\n\n\n\n\nSara Khosravi\n\n\nKai Kleinbard\n\n\nDouwe den Blanken\n\n\nMatthew Stewart\n\n\nshanzehbatool\n\n\n\n\nElias Nuwara\n\n\nJared Ping\n\n\nItai Shapira\n\n\nMaximilian Lam\n\n\nJayson Lin\n\n\n\n\nSophia Cho\n\n\nAndrea\n\n\nJeffrey Ma\n\n\nAlex Rodriguez\n\n\nKorneel Van den Berghe\n\n\n\n\nColby Banbury\n\n\nZishen Wan\n\n\nAbdulrahman Mahmoud\n\n\nSrivatsan Krishnan\n\n\nDivya Amirtharaj\n\n\n\n\nEmeka Ezike\n\n\nAghyad Deeb\n\n\nHaoran Qiu\n\n\nmarin-llobet\n\n\nEmil Njor\n\n\n\n\nAditi Raju\n\n\nJared Ni\n\n\nMichael Schnebly\n\n\noishib\n\n\nELSuitorHarvard\n\n\n\n\nHenry Bae\n\n\nJae-Won Chung\n\n\nYu-Shun Hsiao\n\n\nMark Mazumder\n\n\nMarco Zennaro\n\n\n\n\nEura Nofshin\n\n\nAndrew Bass\n\n\nPong Trairatvorakul\n\n\nJennifer Zhou\n\n\nShvetank Prakash\n\n\n\n\nAlex Oesterling\n\n\nArya Tschand\n\n\nBruno Scaglione\n\n\nGauri Jain\n\n\nAllen-Kuang\n\n\n\n\nFin Amin\n\n\nFatima Shah\n\n\nThe Random DIY\n\n\ngnodipac886\n\n\nSercan Aygün\n\n\n\n\nBaldassarre Cesarano\n\n\nAbenezer\n\n\nBilge Acun\n\n\nyanjingl\n\n\nYang Zhou\n\n\n\n\nabigailswallow\n\n\nJason Yik\n\n\nhappyappledog\n\n\nCurren Iyer\n\n\nEmmanuel Rassou\n\n\n\n\nSonia Murthy\n\n\nShreya Johri\n\n\nJessica Quaye\n\n\nVijay Edupuganti\n\n\nCostin-Andrei Oncescu\n\n\n\n\nAnnie Laurie Cook\n\n\nJothi Ramaswamy\n\n\nBatur Arslan\n\n\nFatima Shah\n\n\na-saraf\n\n\n\n\nsonghan\n\n\nZishen",
    "crumbs": [
      "Ringraziamenti"
    ]
  },
  {
    "objectID": "contents/core/about/about.it.html",
    "href": "contents/core/about/about.it.html",
    "title": "Informazioni sul Libro",
    "section": "",
    "text": "Panoramica\nBenvenuti a questo libro collaborativo, sviluppato come parte del corso CS249r Machine Learning Systems presso l’Università di Harvard. L’obiettivo è quello di fornire una risorsa completa per educatori e studenti che desiderano comprendere i sistemi di apprendimento automatico. Questo libro viene costantemente aggiornato per incorporare le ultime intuizioni e strategie didattiche efficaci.",
    "crumbs": [
      "Informazioni sul Libro"
    ]
  },
  {
    "objectID": "contents/core/about/about.it.html#cosa-cè-nel-libro",
    "href": "contents/core/about/about.it.html#cosa-cè-nel-libro",
    "title": "Informazioni sul Libro",
    "section": "Cosa c’è nel Libro",
    "text": "Cosa c’è nel Libro\nEsploriamo le basi tecniche dei sistemi di apprendimento automatico, le sfide della creazione e distribuzione di questi sistemi nel continuum informatico e la vasta gamma di applicazioni che consentono. Un aspetto unico di questo libro è la sua funzione di canale verso opere accademiche fondamentali e documenti di ricerca accademica, mirati ad arricchire la comprensione del lettore e incoraggiare un’esplorazione più approfondita dell’argomento. Questo approccio cerca di colmare il divario tra materiali pedagogici e tendenze di ricerca all’avanguardia, offrendo una guida completa che è al passo con l’evoluzione del campo dell’apprendimento automatico applicato.\nPer migliorare l’esperienza di apprendimento, abbiamo incluso una varietà di materiali supplementari. In tutto il libro, si troveranno slide che riassumono i concetti chiave, video che forniscono spiegazioni e dimostrazioni approfondite, esercizi che rafforzano la comprensione ed esercizi pratici che offrono esperienza pratica con gli strumenti e le tecniche discussi. Queste risorse aggiuntive sono progettate per soddisfare diversi stili di apprendimento e contribuire ad acquisire una comprensione più profonda e pratica dell’argomento.",
    "crumbs": [
      "Informazioni sul Libro"
    ]
  },
  {
    "objectID": "contents/core/about/about.it.html#argomenti-esplorati",
    "href": "contents/core/about/about.it.html#argomenti-esplorati",
    "title": "Informazioni sul Libro",
    "section": "Argomenti Esplorati",
    "text": "Argomenti Esplorati\nQuesto libro di testo offre un’esplorazione completa di vari aspetti dei sistemi di apprendimento automatico, coprendo l’intero flusso di lavoro end-to-end. Partendo dai concetti fondamentali, procede attraverso aree essenziali come l’ingegneria dei dati, i framework di IA e l’addestramento dei modelli.\nPer migliorare l’esperienza di apprendimento, abbiamo incluso una vasta gamma di materiali supplementari. Queste risorse sono costituite da slide che riassumono i concetti chiave, video che forniscono spiegazioni e dimostrazioni dettagliate, esercizi progettati per rafforzare la comprensione e laboratori che offrono esperienza pratica con gli strumenti e le tecniche discussi.\nI lettori acquisiranno informazioni sull’ottimizzazione dei modelli per l’efficienza, l’implementazione dell’IA su diverse piattaforme hardware e il benchmarking delle prestazioni. Il libro approfondisce anche argomenti avanzati, tra cui sicurezza, privacy, IA responsabile e sostenibile, IA robusta e IA generativa. Inoltre, esamina l’impatto sociale dell’IA, concludendo con un’enfasi sui contributi positivi che l’IA può apportare alla società.",
    "crumbs": [
      "Informazioni sul Libro"
    ]
  },
  {
    "objectID": "contents/core/about/about.it.html#principali-risultati-dellapprendimento",
    "href": "contents/core/about/about.it.html#principali-risultati-dellapprendimento",
    "title": "Informazioni sul Libro",
    "section": "Principali Risultati dell’Apprendimento",
    "text": "Principali Risultati dell’Apprendimento\nI lettori acquisiranno competenze nel training e nell’implementazione di modelli di reti neurali profonde su diverse piattaforme, oltre a comprendere le sfide più ampie coinvolte nella loro progettazione, sviluppo e implementazione. Nello specifico, dopo aver completato questo libro, gli studenti saranno in grado di:\n\n\n\n\n\n\nConsiglio\n\n\n\n\nSpiegare i concetti fondamentali e la loro rilevanza per i sistemi di IA.\nDescrivere i componenti fondamentali e l’architettura dei sistemi di IA.\nConfrontare e mettere a contrasto varie piattaforme hardware per l’implementazione dell’IA, selezionando le opzioni appropriate per casi d’uso specifici.\nProgettare e implementare processi di training per modelli di IA su sistemi diversi.\nApplicare tecniche di ottimizzazione per migliorare le prestazioni e l’efficienza dei modelli di IA.\nAnalizzare le applicazioni di IA nel mondo reale e le loro strategie di implementazione.\nValutare le attuali sfide nei sistemi di IA e prevedere le tendenze future nel settore.\nSviluppare un progetto completo basato sull’apprendimento automatico, dalla concezione all’implementazione.\nRisolvere i problemi comuni nell’addestramento e nell’implementazione dei modelli di IA.\nValutare criticamente le implicazioni etiche e gli impatti sociali dei sistemi di IA.",
    "crumbs": [
      "Informazioni sul Libro"
    ]
  },
  {
    "objectID": "contents/core/about/about.it.html#prerequisiti-per-i-lettori",
    "href": "contents/core/about/about.it.html#prerequisiti-per-i-lettori",
    "title": "Informazioni sul Libro",
    "section": "Prerequisiti per i Lettori",
    "text": "Prerequisiti per i Lettori\n\nCompetenze di programmazione di base: Consigliamo di avere una certa esperienza di programmazione, idealmente in Python. Una conoscenza delle variabili, tipi di dati e strutture di controllo faciliterà l’interazione col libro.\nAlcune Conoscenze di Machine Learning: Sebbene non sia obbligatorio, una conoscenza di base dei concetti di apprendimento automatico aiuterà ad assorbire il materiale più facilmente. Se si è nuovi nel campo, il libro fornisce sufficienti informazioni di base per mettersi al passo.\nConoscenza di Base dei Sistemi: Si consiglia un livello di conoscenza di base dei sistemi a livello universitario junior o senior. Sarà utile comprendere l’architettura di sistema, i sistemi operativi e le reti di base.\nProgrammazione Python (Facoltativo): Se si ha familiarità con Python, si troverà più facile interagire con le sezioni di codifica del libro. Conoscere librerie come NumPy, scikit-learn e TensorFlow sarà particolarmente utile.\nVoglia di Imparare: Il libro è progettato per essere accessibile a un vasto pubblico, con diversi livelli di competenza tecnica. La volontà di sfidare se stessi e di impegnarsi in esercizi pratici aiuterà a trarne il massimo vantaggio.\nDisponibilità delle Risorse: Per gli aspetti pratici, ci sarà bisogno di un computer con Python e le librerie pertinenti installate. L’accesso facoltativo a schede di sviluppo o hardware specifico sarà utile anche per sperimentare la distribuzione del modello di apprendimento automatico.\n\nSoddisfacendo questi prerequisiti, si sarà ben posizionati per approfondire la comprensione dei sistemi di apprendimento automatico, impegnarsi in esercizi di codifica e persino implementare applicazioni pratiche su vari dispositivi.",
    "crumbs": [
      "Informazioni sul Libro"
    ]
  },
  {
    "objectID": "contents/core/about/about.it.html#chi-dovrebbe-leggerlo",
    "href": "contents/core/about/about.it.html#chi-dovrebbe-leggerlo",
    "title": "Informazioni sul Libro",
    "section": "Chi Dovrebbe Leggerlo",
    "text": "Chi Dovrebbe Leggerlo\nQuesto libro è progettato per individui in diverse fasi del loro percorso con i sistemi di machine learning, dai principianti a quelli più avanzati nel settore. Introduce concetti fondamentali e avanza verso argomenti complessi rilevanti per la comunità di apprendimento automatico e vaste aree di ricerca. I principali destinatari di questo libro includono:\n\nStudenti di Informatica e Ingegneria Elettrica: Studenti senior e laureati troveranno questo libro particolarmente prezioso. Introduce le tecniche essenziali per la progettazione e la creazione di sistemi di apprendimento automatico, concentrandosi sulle conoscenze fondamentali piuttosto che sui dettagli esaustivi, spesso al centro dell’insegnamento in classe. Questo libro fornirà il background e il contesto necessari, consentendo agli insegnanti di esplorare argomenti avanzati in modo più approfondito. Una caratteristica essenziale è la sua prospettiva end-to-end, che viene spesso trascurata nei programmi tradizionali.\nIngegneri di Sistema: Questo libro funge da guida per gli ingegneri che cercano di comprendere le complessità dei sistemi e delle applicazioni intelligenti, in particolare che coinvolgono l’apprendimento automatico. Comprende i framework concettuali e i componenti pratici che compongono un sistema ML, estendendosi oltre le aree specifiche che si potrebbero incontrare nel proprio ruolo professionale.\nRicercatori e Accademici: Per i ricercatori, questo libro affronta le sfide distinte dell’esecuzione di algoritmi di apprendimento automatico su diverse piattaforme. Man mano che l’efficienza acquisisce importanza, una solida comprensione dei sistemi, al di là dei soli algoritmi, è fondamentale per sviluppare modelli più efficienti. Il libro fa riferimento a documenti seminali, indirizzando i ricercatori verso lavori che hanno influenzato il campo e stabilendo connessioni tra varie aree con implicazioni significative per la loro ricerca.",
    "crumbs": [
      "Informazioni sul Libro"
    ]
  },
  {
    "objectID": "contents/core/about/about.it.html#come-orientarsi-in-questo-libro",
    "href": "contents/core/about/about.it.html#come-orientarsi-in-questo-libro",
    "title": "Informazioni sul Libro",
    "section": "Come Orientarsi in Questo Libro",
    "text": "Come Orientarsi in Questo Libro\nPer ottenere il massimo da questo libro, consigliamo un approccio di apprendimento strutturato che sfrutti le varie risorse fornite. Ogni capitolo include slide, video, esercizi e laboratori per soddisfare diversi stili di apprendimento e rafforzare la comprensione.\n\nI Fondamenti (Capitoli 1-3): Si inizia costruendo una solida base con i primi capitoli, che forniscono un’introduzione all’IA embedded e trattano argomenti fondamentali come sistemi di IA e deep learning.\nFlusso di Lavoro (Capitoli 4-6): Con questa base, si passa ai capitoli incentrati sugli aspetti pratici del processo di creazione del modello AI come flussi di lavoro, ingegneria dei dati e framework.\nTraining (Capitoli 7-10): Questi capitoli offrono approfondimenti su come addestrare efficacemente i modelli AI, comprese tecniche per efficienza, ottimizzazioni e accelerazione.\nDeployment (Capitoli 11-13): Si esamina come distribuire l’IA sui dispositivi e monitorarne l’operatività tramite metodi come benchmarking, on-device learning e MLOps.\nArgomenti Avanzati (Capitoli 14-18): Si esaminano criticamente argomenti come sicurezza, privacy, etica, sostenibilità, robustezza e IA generativa.\nImpatto Sociale (Capitolo 19): Esplora le applicazioni positive e il potenziale dell’IA per il bene della società.\nConclusione (Capitolo 20): Riflessioni sui principali risultati e sulle direzioni future dei sistemi di IA.\n\nSebbene il libro sia progettato per un apprendimento progressivo, incoraggiamo un approccio di apprendimento interconnesso che consente di navigare tra i capitoli in base ai propri interessi e alle proprie esigenze. In tutto il libro si trovano casi di studio ed esercizi pratici che aiuteranno a mettere in relazione la teoria con le applicazioni del mondo reale. Consigliamo inoltre di partecipare a forum e gruppi per partecipare a discussioni, discutere concetti e condividere approfondimenti con altri studenti. Rivedere regolarmente i capitoli può aiutare a rafforzare l’apprendimento e offrire nuove prospettive sui concetti trattati. Adottando questo approccio strutturato ma flessibile e interagendo attivamente con i contenuti e la community, si farà un’esperienza di apprendimento appagante e arricchente che massimizza la comprensione.",
    "crumbs": [
      "Informazioni sul Libro"
    ]
  },
  {
    "objectID": "contents/core/about/about.it.html#approfondimenti-capitolo-per-capitolo",
    "href": "contents/core/about/about.it.html#approfondimenti-capitolo-per-capitolo",
    "title": "Informazioni sul Libro",
    "section": "Approfondimenti Capitolo per Capitolo",
    "text": "Approfondimenti Capitolo per Capitolo\nEcco uno sguardo più da vicino a cosa tratta ogni capitolo. Abbiamo strutturato il libro in sei sezioni principali: Nozioni Fondamentali, Flusso di lavoro, Training, Deployment, Argomenti avanzati e Impatto. Queste sezioni riflettono da vicino i componenti principali di una tipica pipeline di machine learning, dalla comprensione dei concetti di base al la deploy e alla manutenzione dei sistemi di intelligenza artificiale in applicazioni del mondo reale. Organizzando il contenuto in questo modo, puntiamo a fornire una progressione logica che rispecchi il processo effettivo di sviluppo e implementazione dei sistemi di intelligenza artificiale.\n\nNozioni Fondamentali\nNella sezione Nozioni Fondamentali, poniamo le basi per comprendere l’intelligenza artificiale. Questo è ben lungi dall’essere un’immersione profonda negli algoritmi, ma puntiamo a introdurre concetti chiave, fornire una panoramica dei sistemi di apprendimento automatico e approfondire i principi e gli algoritmi di deep learning che alimentano le applicazioni di IA nei loro sistemi associati. Questa sezione fornisce le conoscenze essenziali necessarie per comprendere i capitoli successivi.\n\nIntroduzione: Questo capitolo prepara il terreno, fornendo una panoramica dell’intelligenza artificiale e gettando le basi per i capitoli successivi.\nSistemi di ML: Introduciamo le basi dei sistemi di machine learning [apprendimento automatico], le piattaforme in cui gli algoritmi di intelligenza artificiale sono ampiamente applicati.\nAvvio al Deep Learning: Questo capitolo offre una breve introduzione agli algoritmi e ai principi alla base delle applicazioni di IA nei sistemi di apprendimento automatico.\n\n\n\nWorkflow\nLa sezione Workflow [Flusso di lavoro] guida attraverso gli aspetti pratici della creazione di modelli AI. Analizziamo il flusso di lavoro AI, discutiamo le “best practice” di data engineering e passiamo in rassegna i framework AI più diffusi. Alla fine di questa sezione, si avrà una chiara comprensione dei passaggi coinvolti nello sviluppo di applicazioni AI competenti e degli strumenti disponibili per semplificare il processo.\n\nWorkflow IA: Questo capitolo analizza il flusso di lavoro di apprendimento automatico, offrendo approfondimenti sui passaggi che portano ad applicazioni AI competenti.\nIngegneria dei Dati: Ci concentriamo sull’importanza dei dati nei sistemi di IA, discutendo su come gestire e organizzare efficacemente i dati.\nFramework di IA: Questo capitolo esamina diversi framework per lo sviluppo di modelli di apprendimento automatico, guidando nella scelta di quello più adatto ai propri progetti.\n\n\n\nTraining\nNella sezione Training, esploriamo tecniche per il training efficiente e affidabile di modelli di IA. Trattiamo strategie per raggiungere efficienza, ottimizzazioni dei modelli e il ruolo dell’hardware specializzato nell’accelerazione IA. Questa sezione fornisce le conoscenze necessarie per sviluppare modelli ad alte prestazioni che possono essere integrati perfettamente nei sistemi di IA.\n\nTraining IA: Questo capitolo approfondisce il training [addestramento] dei modelli, esplorando tecniche per sviluppare modelli efficienti e affidabili.\nIA Efficiente: Qui, discutiamo strategie per raggiungere l’efficienza nelle applicazioni di IA, dall’ottimizzazione delle risorse computazionali al miglioramento delle prestazioni.\nOttimizzazioni dei Modelli: Esploriamo vari percorsi per ottimizzare i modelli di IA per un’integrazione senza soluzione di continuità nei sistemi di IA.\nAccelerazione dell’IA: Discutiamo il ruolo dell’hardware specializzato nel migliorare le prestazioni dei sistemi di IA.\n\n\n\nDeployment\nLa sezione Deployment [distribuzione] si concentra sulle sfide e sulle soluzioni per l’implementazione di modelli di IA. Discutiamo metodi di benchmarking per valutare le prestazioni del sistema AI, tecniche per l’apprendimento “on-device” per migliorare l’efficienza e la privacy e i processi coinvolti nelle operazioni di ML. Questa sezione fornisce le competenze per implementare e gestire in modo efficace le funzionalità di IA nei sistemi di intelligenza artificiale.\n\nBenchmark dell’IA: Questo capitolo si concentra su come valutare i sistemi di IA tramite metodi di benchmarking sistematici.\nApprendimento On-Device: Esploriamo tecniche per l’apprendimento localizzato, che migliora sia l’efficienza che la privacy.\nOperazioni di ML: Questo capitolo esamina i processi coinvolti nell’integrazione, nel monitoraggio e nella manutenzione senza soluzione di continuità delle funzionalità di IA.\n\n\n\nArgomenti Avanzati\nNella sezione Argomenti avanzati, studieremo le problematiche critiche che circondano l’IA. Affrontiamo le preoccupazioni relative a privacy e sicurezza, esploriamo i principi etici dell’intelligenza artificiale responsabile, discutiamo strategie per uno sviluppo sostenibile dell’intelligenza artificiale, esaminiamo tecniche per la creazione di modelli di intelligenza artificiale solidi e introduciamo l’entusiasmante campo dell’intelligenza artificiale generativa. Questa sezione amplia la comprensione del complesso panorama dell’IA e prepara ad affrontarne le sfide.\n\nSicurezza e Privacy: Man mano che l’intelligenza artificiale diventa sempre più onnipresente, questo capitolo affronta gli aspetti cruciali della privacy e della sicurezza nei sistemi di IA.\nIA Responsabile: Discutiamo i principi etici che guidano l’uso responsabile dell’intelligenza artificiale, concentrandoci sulla correttezza, responsabilità e trasparenza.\nIA Sostenibile: Questo capitolo esplora pratiche e strategie per un’intelligenza artificiale sostenibile, garantendo fattibilità a lungo termine e un impatto ambientale ridotto.\nIA Robusta: Parliamo di tecniche per sviluppare modelli di IA affidabili e robusti che possano funzionare in modo coerente in varie condizioni.\nIA Generativa: Questo capitolo esplora gli algoritmi e le tecniche alla base dell’IA generativa, aprendo strade all’innovazione e alla creatività.\n\n\n\nImpatto Sociale\nLa sezione Impatto Sociale evidenzia il potenziale trasformativo dell’IA in vari domini. Presentiamo applicazioni reali di TinyML in sanità, agricoltura, conservazione e altre aree in cui l’IA sta facendo una positiva differenza. Questa sezione invoglia a sfruttare la potenza dell’AI per il bene della società e a contribuire allo sviluppo di soluzioni di impatto.\n\nAI for Good: Evidenziamo applicazioni positive di TinyML in aree come sanità, agricoltura e la conservazione.\n\n\n\nChiusura\nNella sezione Chiusura, riflettiamo sugli insegnamenti chiave del libro e guardiamo al futuro dell’IA. Sintetizziamo i concetti trattati, discutiamo le tendenze emergenti e forniamo indicazioni su come proseguire nel percorso di apprendimento in questo campo in rapida evoluzione. Questa sezione lascia con una comprensione completa dell’IA e l’entusiasmo di applicare le conoscenze in modi innovativi.\n\nConclusione: Il libro si conclude con una riflessione sugli apprendimenti chiave e sulle direzioni future nel campo dell’IA.",
    "crumbs": [
      "Informazioni sul Libro"
    ]
  },
  {
    "objectID": "contents/core/about/about.it.html#apprendimento-personalizzato",
    "href": "contents/core/about/about.it.html#apprendimento-personalizzato",
    "title": "Informazioni sul Libro",
    "section": "Apprendimento Personalizzato",
    "text": "Apprendimento Personalizzato\nSappiamo che i lettori hanno interessi diversi; alcuni potrebbero voler comprendere i fondamenti, mentre altri sono desiderosi di approfondire argomenti avanzati come l’accelerazione hardware o l’etica dell’intelligenza artificiale. Per aiutare a navigare nel libro in modo più efficace, abbiamo creato una guida alla lettura basata sulla persona, su misura per i propri interessi e obiettivi specifici. Questa guida aiuta a identificare il lettore che meglio corrisponde ai propri interessi. Ogni persona rappresenta un profilo di lettore distinto con obiettivi specifici. Selezionando la persona che in sintonia, ci si può concentrare sui capitoli e sulle sezioni più pertinenti alle proprie esigenze.\n\n\n\n\n\n\n\n\n\nPersona\nDescrizione\nCapitoli\nFocus\n\n\n\n\nIl Principiante del TinyML\nÈ nuovo nel campo del TinyML e impaziente di imparare le basi.\n1-3, 8, 9, 10, 12\nComprendere i fondamenti, ottenere informazioni su ML efficiente, e ottimizzato e scoprire l’addestramento sul dispositivo.\n\n\nL’Appassionato di EdgeML\nHa una certa conoscenza di TinyML ed è interessato a esplorare il mondo più ampio di EdgeML.\n1-3, 8, 9, 10, 12, 13\nCostruire solide basi, approfondire le complessità del ML efficiente ed esplorare gli aspetti operativi dei sistemi embedded.\n\n\nIl Visionario Informatico\nÈ affascinato dalla visione artificiale e dalle sue applicazioni in TinyML ed EdgeML.\n1-3, 5, 8-10, 12, 13, 17, 20\nIniziare dalle basi, esplorare l’ingegneria dei dati e studiare i metodi per ottimizzare i modelli ML. Scoprire la robustezza e il futuro dei sistemi di ML.\n\n\nIl Maestro dei Dati\nÈ appassionato di dati e del loro ruolo cruciale nei sistemi ML.\n1-5, 8-13\nAcquisire una comprensione completa del ruolo dei dati nei sistemi ML, esplorare il flusso di lavoro ML e approfondire le considerazioni sull’ottimizzazione del modello e sulla distribuzione.\n\n\nL’Eroe dell’Hardware\nÈ entusiasta degli aspetti hardware dei sistemi di ML e di come influenzano le prestazioni del modello.\n1-3, 6, 8-10, 12, 14, 17, 20\nCostruire solide basi nei sistemi e nei framework ML, esplorare le sfide dell’ottimizzazione dei modelli per l’efficienza, la progettazione congiunta hardware-software e gli aspetti della sicurezza.\n\n\nIl campione della Sostenibilità\nÈ per la sostenibilità e vuol imparare a sviluppare sistemi di IA ecocompatibili.\n1-3, 8-10, 12, 15, 16, 20\nIniziare con i fondamenti dei sistemi ML e TinyML, esplorare le tecniche di ottimizzazione e scoprire le pratiche di IA responsabili e sostenibili.\n\n\nIl Ricercatore di Etica nell’IA\nÈ preoccupato per le implicazioni etiche dell’IA e vuol garantire uno sviluppo e un’implementazione responsabili.\n1-3, 5, 7, 12, 14-16, 19, 20\nOttenere informazioni sulle considerazioni etiche che circondano l’IA, tra cui correttezza, privacy, sostenibilità e pratiche di sviluppo responsabili.\n\n\nL’ingegnere ML Full-Stack\nÈ un esperto di ML esperto e vuole approfondire la comprensione dell’intero stack del sistema ML.\nL’intero libro\nComprendere il processo end-to-end di creazione e distribuzione di sistemi di ML, dall’ingegneria dei dati e dall’ottimizzazione dei modelli all’accelerazione hardware e alle considerazioni etiche.",
    "crumbs": [
      "Informazioni sul Libro"
    ]
  },
  {
    "objectID": "contents/core/about/about.it.html#unirsi-alla-community",
    "href": "contents/core/about/about.it.html#unirsi-alla-community",
    "title": "Informazioni sul Libro",
    "section": "Unirsi alla Community",
    "text": "Unirsi alla Community\nL’apprendimento nel mondo frenetico dell’intelligenza artificiale è un viaggio collaborativo. Ci siamo prefissati di coltivare una vivace comunità di studenti, innovatori e collaboratori. Esplorando i concetti e impegnandosi con gli esercizi, incoraggiamo a condividere le intuizioni ed esperienze personali. Che si tratti di un approccio innovativo, di un’applicazione interessante o di una domanda stimolante, i contributi dei singoli possono arricchire l’ecosistema di apprendimento. Partecipare alle discussioni, offrire e cercare indicazioni e collaborare a progetti per promuovere una cultura di crescita e apprendimento reciproci. Condividendo la conoscenza, si svolge un ruolo importante nel promuovere una comunità connessa, informata e potenziata a livello globale.",
    "crumbs": [
      "Informazioni sul Libro"
    ]
  },
  {
    "objectID": "contents/ai/socratiq.it.html",
    "href": "contents/ai/socratiq.it.html",
    "title": "SocratiQ AI",
    "section": "",
    "text": "Assistente di apprendimento IA\nBenvenuti a SocratiQ (si pronuncia “Socratic”), un assistente di apprendimento IA perfettamente integrato in questa risorsa. Ispirato al metodo di insegnamento socratico, che enfatizza domande e risposte ponderate per stimolare il pensiero critico—SocratiQ fa parte del nostro esperimento con ciò che chiamiamo Apprendimento Generativo. Combinando quiz interattivi, assistenza personalizzata e feedback in tempo reale, SocratiQ è pensato per rafforzare la comprensione e aiutare a creare nuove connessioni. Nota: SocratiQ è ancora un “work in progress” e accogliamo con favore i feedback.\nSi ascolti questo podcast generato dall’IA su SocratiQ, creato utilizzando le note di questa pagina con NotebookLM di Google.\nSi può abilitare SocratiQ cliccando sul pulsante qui sotto:\nL’obiettivo di SocratiQ è adattarsi alle proprie esigenze, generando domande mirate e impegnandosi in un dialogo significativo sul materiale didattico. A differenza dello studio tradizionale sui libri di testo, SocratiQ offre un’esperienza di apprendimento interattiva e personalizzata che può aiutare a comprendere meglio e a ricordare concetti complessi.",
    "crumbs": [
      "SocratiQ AI"
    ]
  },
  {
    "objectID": "contents/ai/socratiq.it.html#assistente-di-apprendimento-ia",
    "href": "contents/ai/socratiq.it.html#assistente-di-apprendimento-ia",
    "title": "SocratiQ AI",
    "section": "",
    "text": "SocratiQ: OFF\n\n\n\n\n\n\n\n\n\nURL per l’Accesso Diretto\n\n\n\n\n\nSi può controllare direttamente SocratiQ aggiungendo i parametri ?socratiq= all’URL:\n\nPer attivare: mlsysbook.ai/?socratiq=true\nPer disattivare: mlsysbook.ai/?socratiq=false\n\nQuesto dà un rapido accesso per attivare/disattivare la funzionalità di SocratiQ direttamente dalla barra degli indirizzi del browser se ci si trova su una pagina e non si vuol tornare qui per attivare/disattivare la funzionalità.",
    "crumbs": [
      "SocratiQ AI"
    ]
  },
  {
    "objectID": "contents/ai/socratiq.it.html#guida-rapida",
    "href": "contents/ai/socratiq.it.html#guida-rapida",
    "title": "SocratiQ AI",
    "section": "Guida Rapida",
    "text": "Guida Rapida\n\nAbilitare SocratiQ utilizzando il pulsante in basso o i parametri URL\nUtilizzare la scorciatoia da tastiera (Cmd/Ctrl + /) per aprire SocratiQ in qualsiasi momento\nImpostare il proprio livello accademico in “Settings”\nIniziare a imparare! Cercare i pulsanti dei quiz alla fine delle sezioni\n\nNotare che questa è una funzionalità sperimentale. Stiamo sperimentando l’idea di creare un’esperienza di apprendimento dinamica e personalizzata sfruttando la potenza dell’IA generativa. Ci auguriamo che questo approccio trasformi il modo in cui si interagisce e assorbono i concetti complessi.\n\n\n\n\n\n\nAvviso\n\n\n\nLe Risposte dell’IA: Sebbene SocratiQ utilizzi un’IA avanzata per generare quiz e fornire assistenza, come tutti i sistemi di IA, può occasionalmente fornire risposte imperfette o incomplete. Tuttavia, l’abbiamo progettato e testato per garantire che sia efficace nel supportare il percorso di apprendimento. Se non si è sicuri di una risposta, si faccia riferimento al contenuto del libro di testo o si chieda consiglio all’istruttore.\n\n\nUna volta abilitato SocratiQ, sarà sempre disponibile quando visitando questo sito.\nPuoi accedere a SocratiQ in qualsiasi momento utilizzando una scorciatoia da tastiera mostrata in Figura 1, che richiama l’interfaccia mostrata in Figura 2.\n\n\n\n\n\n\nFigura 1: Scorciatoia da tastiera per SocratiQ.\n\n\n\n\n\n\n\n\n\nFigura 2: L’interfaccia principale di SocratiQ, che mostra i componenti chiave dell’assistente di apprendimento IA.",
    "crumbs": [
      "SocratiQ AI"
    ]
  },
  {
    "objectID": "contents/ai/socratiq.it.html#panoramica-dei-pulsanti",
    "href": "contents/ai/socratiq.it.html#panoramica-dei-pulsanti",
    "title": "SocratiQ AI",
    "section": "Panoramica dei Pulsanti",
    "text": "Panoramica dei Pulsanti\nLa barra di navigazione superiore fornisce un rapido accesso alle seguenti funzionalità:\n\nPer regolare le impostazioni in qualsiasi momento.\nPer tenere traccia dei progressi visualizzando la dashboard.\nPer avviare o salvare le conversazioni con SocratiQ.\n\n\n\n\n\n\n\nFigura 3: Visualizzazione del menù di navigazione superiore.",
    "crumbs": [
      "SocratiQ AI"
    ]
  },
  {
    "objectID": "contents/ai/socratiq.it.html#sec-socratiq-settings",
    "href": "contents/ai/socratiq.it.html#sec-socratiq-settings",
    "title": "SocratiQ AI",
    "section": "Personalizzazione dell’Apprendimento",
    "text": "Personalizzazione dell’Apprendimento\nPrima di immergersi nello studio, ci si prenda un momento per configurare SocratiQ in base al proprio livello accademico. Questa configurazione iniziale assicura che tutte le interazioni, dalle domande del quiz alle spiegazioni, siano personalizzate in base alle proprie conoscenze di base. La Figura 4 mostra dove regolare queste preferenze.\n\n\n\n\n\n\nFigura 4: Il pannello delle impostazioni in cui personalizzare SocratiQ in base al proprio livello accademico.\n\n\n\nSi può aumentare qualsiasi risposta AI SocratiQ utilizzando il menù a discesa nella parte superiore di ogni messaggio.\n\n\n\n\n\n\nFigura 5: Ripetere un messaggio AI scegliendo un nuovo livello di esperienza.",
    "crumbs": [
      "SocratiQ AI"
    ]
  },
  {
    "objectID": "contents/ai/socratiq.it.html#sec-socratiq-learning",
    "href": "contents/ai/socratiq.it.html#sec-socratiq-learning",
    "title": "SocratiQ AI",
    "section": "Apprendimento con SocratiQ",
    "text": "Apprendimento con SocratiQ\n\nI Quiz\nMan mano che si procede in ogni sezione del libro di testo, si ha la possibilità di chiedere a SocratiQ di generare automaticamente quiz su misura per rafforzare i concetti chiave. Questi quiz sono opportunamente inseriti alla fine di ogni sottosezione principale (ad esempio, 1.1, 1.2, 1.3 e così via), come illustrato in Figura 6.\n\n\n\n\n\n\nFigura 6: I quiz vengono generati alla fine di ogni sezione.\n\n\n\nOgni quiz è in genere composto da 3-5 domande a risposta multipla e richiede solo 1-2 minuti per essere completato. Queste domande sono progettate per valutare la comprensione del materiale trattato nella sezione precedente, come mostrato in Figura 7 (a).\nDopo aver inviato le risposte, SocratiQ fornisce un feedback immediato insieme a spiegazioni dettagliate per ogni domanda, come mostrato in Figura 7 (b).\n\n\n\n\n\n\n\n\n\n\n\n(a) Esempio di domande del quiz generate dall’IA.\n\n\n\n\n\n\n\n\n\n\n\n(b) Esempio di feedback e spiegazioni generate dall’IA per i quiz.\n\n\n\n\n\n\n\nFigura 7: SocratiQ utilizza un Large Language Model (LLM) per generare e valutare automaticamente i quiz.\n\n\n\n\n\nEsempio di Flusso di Apprendimento\n\nLeggere una sezione\nSelezionare un testo impegnativo → Chiedere spiegazioni a SocratiQ\nFare il quiz sulla sezione\nEsaminare i suggerimenti sui contenuti correlati\nMonitorare i progressi nella dashboard\n\n\n\nOttenere aiuto con i concetti\nQuando si incontrano concetti impegnativi, SocratiQ offre due potenti modi per ottenere aiuto. Innanzitutto, si può selezionare qualsiasi testo dal libro di testo e chiedere una spiegazione dettagliata, come dimostrato in Figura 8.\n\n\n\n\n\n\nFigura 8: Selezione di un testo specifico per chiedere chiarimenti.\n\n\n\nDopo aver selezionato il testo, si possono porre domande al riguardo e SocratiQ fornirà spiegazioni dettagliate in base a quel contesto, come illustrato in Figura 9.\n\n\n\n\n\n\nFigura 9: Esempio di come SocratiQ fornisce spiegazioni basate sul testo selezionato.\n\n\n\nLa Figura 11 mostra la risposta alla richiesta in Figura 9.\nInoltre, si può anche fare riferimento a Sezioni, come mostrato in Figura 10, Sottosezioni e parole chiave direttamente mentre si conversa con SocratiQ. Usare il simbolo @ per fare riferimento a una sezione, sottosezione o parola chiave. Si può anche cliccare sul pulsante + Context proprio sopra l’input.\n\n\n\n\n\n\nFigura 10: Riferimento a diverse sezioni del libro di testo.\n\n\n\n\n\n\n\n\n\nFigura 11: Una sessione di chat interattiva con SocratiQ, che mostra come ottenere chiarimenti sui concetti.\n\n\n\nPer migliorare l’esperienza di apprendimento, SocratiQ non si limita a rispondere alle domande, ma suggerisce anche contenuti correlati dal libro di testo che potrebbero essere utili per una comprensione più approfondita, come mostrato in Figura 12.\n\n\n\n\n\n\nFigura 12: SocratiQ suggerisce contenuti correlati in base alle domande per migliorare la comprensione.",
    "crumbs": [
      "SocratiQ AI"
    ]
  },
  {
    "objectID": "contents/ai/socratiq.it.html#sec-socratiq-dashboard",
    "href": "contents/ai/socratiq.it.html#sec-socratiq-dashboard",
    "title": "SocratiQ AI",
    "section": "Monitoraggio dei Progressi",
    "text": "Monitoraggio dei Progressi\n\nDashboard delle Prestazioni\nSocratiQ mantiene un registro completo del percorso di apprendimento. La dashboard dei progressi (Figura 13) mostra le statistiche delle prestazioni nei quiz, le serie di apprendimento e i badge dei risultati. Questa dashboard si aggiorna in tempo reale.\n\n\n\n\n\n\nFigura 13: La dashboard dei progressi mostra le statistiche di apprendimento e i risultati.\n\n\n\nContinuando a interagire col materiale e a completare i quiz, si otterranno vari badge che riconoscono i progressi, come mostrato in Figura 14.\n\n\n🏅 Badge dei Risultati\nContinuando a progredire nei quiz, si otterranno badge speciali per contrassegnare i risultati! Ecco cosa si può guadagnare:\n\n\n\nBadge\nNome\nCome Guadagnare\n\n\n\n\n🎯\nFirst Steps\nCompletare il primo quiz\n\n\n🔢\nOn a Streak\nMantenere una sequenza di punteggi perfetti\n\n\n🏆\nQuiz Medalist\nCompletare 10 quiz\n\n\n🏆🏆\nQuiz Champion\nCompletare 20 quiz\n\n\n🏆🏆🏆\nQuiz Legend\nCompletare 30 quiz\n\n\n🏆🏆🏆🏆 x n\nQuiz AGI Super Human\nCompletare 40 quiz o più\n\n\n\n\n\n\n\n\n\nConsiglio\n\n\n\nContinuare a risolvere quiz per collezionare tutti i badge e migliorare il percorso di apprendimento! I badge attuali appariranno nella dashboard delle statistiche del quiz.\n\n\n\n\n\n\n\n\nFigura 14: Esempi di badge di risultati che si possono guadagnare tramite un impegno costante.\n\n\n\nSe si desidera un registro dei propri progressi, si può generare un report PDF. Mostrerà i progressi, le prestazioni medie e tutte le domande a cui si è risposto. Il PDF viene generato con un hash univoco e può essere convalidato in modo univoco.\n\n\n\n\n\n\nFigura 15: Si può cliccare sul pulsante “Download Report” per visualizzare il report. Si può verificare che il PDF sia stato creato da SocratiQ cliccando sul pulsante di verifica e caricando il PDF generato.",
    "crumbs": [
      "SocratiQ AI"
    ]
  },
  {
    "objectID": "contents/ai/socratiq.it.html#archiviazione-dati",
    "href": "contents/ai/socratiq.it.html#archiviazione-dati",
    "title": "SocratiQ AI",
    "section": "Archiviazione Dati",
    "text": "Archiviazione Dati\n\n\n\n\n\n\nImportante\n\n\n\nNota importante: Tutti i dati sui progressi sono archiviati localmente nel browser. La cancellazione della cronologia o della cache del browser cancellerà l’intera cronologia di apprendimento, inclusi punteggi dei quiz, serie e badge guadagnati.\n\n\nSi possono anche eliminare tutte le conversazioni salvate cliccando sul pulsante “New Chat” nella barra di navigazione.\n\n\n\n\n\n\nFigura 16: Caricare o elimina le chat precedenti o avviare una nuova chat.",
    "crumbs": [
      "SocratiQ AI"
    ]
  },
  {
    "objectID": "contents/ai/socratiq.it.html#requisiti-tecnici",
    "href": "contents/ai/socratiq.it.html#requisiti-tecnici",
    "title": "SocratiQ AI",
    "section": "Requisiti Tecnici",
    "text": "Requisiti Tecnici\nPer usare SocratiQ in modo efficace, c’è bisogno di:\n\nBrowser Chrome o Safari\nJavaScript abilitato\nConnessione Internet stabile",
    "crumbs": [
      "SocratiQ AI"
    ]
  },
  {
    "objectID": "contents/ai/socratiq.it.html#problemi-comuni-e-risoluzione-dei-problemi",
    "href": "contents/ai/socratiq.it.html#problemi-comuni-e-risoluzione-dei-problemi",
    "title": "SocratiQ AI",
    "section": "Problemi Comuni e Risoluzione dei Problemi",
    "text": "Problemi Comuni e Risoluzione dei Problemi\n\nSe SocratiQ non risponde: Aggiornare la pagina\nSe i quiz non si caricano: Controlla la connessione Internet\nSe i progressi non vengono salvati: Assicurarsi che i cookie siano abilitati\n\nPer problemi persistenti, contattare l’indirizzo vj[@]eecs.harvard.edu.",
    "crumbs": [
      "SocratiQ AI"
    ]
  },
  {
    "objectID": "contents/ai/socratiq.it.html#fornire-feedback",
    "href": "contents/ai/socratiq.it.html#fornire-feedback",
    "title": "SocratiQ AI",
    "section": "Fornire Feedback",
    "text": "Fornire Feedback\nI feedback ci aiutano a migliorare SocratiQ. Si possono segnalare problemi tecnici, suggerire miglioramenti alle domande dei quiz o condividere idee sulle risposte dell’IA utilizzando i pulsanti di feedback presenti nell’interfaccia.\nSi può inviare un GitHub issue, oppure se si preferisce lasciare un feedback tramite Google Form, lo si può fare tramite questo link:\nInvia un Feedback\nNota: SocratiQ è progettato per aiutare a imparare in modo efficace. Partecipando costantemente ai quiz, ponendo domande quando necessario e monitorando i propri progressi, si otterrà il massimo da questo assistente di apprendimento AI.",
    "crumbs": [
      "SocratiQ AI"
    ]
  },
  {
    "objectID": "contents/core/introduction/introduction.it.html",
    "href": "contents/core/introduction/introduction.it.html",
    "title": "1  Introduzione",
    "section": "",
    "text": "1.1 Perché i Sistemi di Machine Learning Sono Importanti\nL’intelligenza artificiale è ovunque. Si pensi alla routine mattutina: Ci si sveglia con una sveglia intelligente basata sull’IA che ha appreso i propri schemi di sonno. Il telefono suggerisce il percorso per andare al lavoro, avendo appreso dai pattern del traffico. Durante il tragitto, l’app musicale crea automaticamente una playlist che si pensa piacerà. Al lavoro, il client di posta elettronica filtra lo spam e dà priorità ai messaggi importanti. Durante il giorno, lo smartwatch monitora l’attività, suggerendo quando muoversi o fare esercizio. La sera, il servizio di streaming consiglia programmi che potrebbero piacere, mentre i dispositivi smart home regolano l’illuminazione e la temperatura in base alle preferenze apprese.\nMa queste comodità quotidiane sono solo l’inizio. L’IA sta trasformando il mondo in modi straordinari. Oggi, i sistemi di IA rilevano tumori in fase iniziale con una precisione senza precedenti, prevedono e tracciano eventi meteorologici estremi per salvare vite e accelerano la scoperta di farmaci simulando milioni di interazioni molecolari. I veicoli autonomi percorrono strade cittadine complesse elaborando dati di sensori in tempo reale da decine di fonti. I modelli linguistici si impegnano in conversazioni sofisticate, traducono tra centinaia di lingue e aiutano gli scienziati ad analizzare vasti database di ricerca. Nei laboratori scientifici, i sistemi di IA stanno facendo scoperte rivoluzionarie, dalla previsione di strutture proteiche che sbloccano nuovi trattamenti medici all’identificazione di materiali promettenti per celle solari e batterie di nuova generazione. Anche nei campi creativi, l’IA collabora con artisti e musicisti per esplorare nuove forme di espressione, spingendo i confini della creatività umana.\nQuesta non è fantascienza, è la realtà di come l’intelligenza artificiale, in particolare i sistemi di apprendimento automatico, si sia intrecciata nel tessuto della nostra vita quotidiana. All’inizio degli anni ’90, Mark Weiser, un pioniere dell’informatica, ha introdotto il mondo a un concetto rivoluzionario che avrebbe cambiato per sempre il modo in cui interagiamo con la tecnologia. Questa visione è stata riassunta in modo sintetico nel suo articolo fondamentale, “The Computer for the 21st Century” (Figura 1.1). Weiser immaginava un futuro in cui l’informatica sarebbe stata perfettamente integrata nei nostri ambienti, diventando una parte invisibile e integrante della vita quotidiana.\nHa definito questo concetto “ubiquitous computing”, promettendo un mondo in cui la tecnologia ci avrebbe servito senza richiedere la nostra costante attenzione o interazione. Oggi ci ritroviamo a vivere nel futuro immaginato da Weiser, in gran parte reso possibile dai sistemi di apprendimento automatico. La vera essenza della sua visione, ovvero creare un ambiente intelligente in grado di anticipare le nostre esigenze e agire per nostro conto, è diventata realtà attraverso lo sviluppo e l’implementazione di sistemi di ML che abbracciano interi ecosistemi, dai potenti data center cloud ai dispositivi edge fino ai più piccoli sensori IoT.\nEppure la maggior parte di noi raramente pensa ai sistemi complessi che rendono possibile tutto questo. Dietro ciascuna di queste interazioni apparentemente semplici si nasconde una sofisticata infrastruttura di dati, algoritmi e risorse informatiche che lavorano insieme. Comprendere come funzionano questi sistemi, le loro capacità, limitazioni e requisiti, è diventato sempre più critico man mano che si integrano sempre di più nel nostro mondo.\nPer apprezzare l’entità di questa trasformazione e la complessità dei moderni sistemi di machine learning, dobbiamo capire come siamo arrivati fin qui. Il viaggio dall’intelligenza artificiale primitiva agli odierni sistemi ML onnipresenti è una storia non solo di evoluzione tecnologica, ma anche di prospettive mutevoli su ciò che è possibile e ciò che è necessario per rendere l’IA pratica e affidabile.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduzione</span>"
    ]
  },
  {
    "objectID": "contents/core/introduction/introduction.it.html#perché-i-sistemi-di-machine-learning-sono-importanti",
    "href": "contents/core/introduction/introduction.it.html#perché-i-sistemi-di-machine-learning-sono-importanti",
    "title": "1  Introduzione",
    "section": "",
    "text": "Figura 1.1: Ubiquitous computing as envisioned di Mark Weiser.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduzione</span>"
    ]
  },
  {
    "objectID": "contents/core/introduction/introduction.it.html#levoluzione-dellia",
    "href": "contents/core/introduction/introduction.it.html#levoluzione-dellia",
    "title": "1  Introduzione",
    "section": "1.2 L’evoluzione dell’IA",
    "text": "1.2 L’evoluzione dell’IA\nL’evoluzione dell’IA, rappresentata nella cronologia mostrata in Figura 1.2, evidenzia traguardi chiave come lo sviluppo del perceptron1 nel 1957 da parte di Frank Rosenblatt, un elemento fondamentale per le moderne reti neurali. Si immagini di entrare in un laboratorio informatico nel 1965. Si troveranno mainframe delle dimensioni di una stanza che eseguono programmi in grado di dimostrare teoremi matematici di base o di giocare a semplici giochi come il tris. Questi primi sistemi di intelligenza artificiale, pur essendo rivoluzionari per l’epoca, erano ben lontani dagli odierni sistemi di apprendimento automatico in grado di rilevare il cancro nelle immagini mediche o di comprendere il linguaggio umano. La cronologia mostra la progressione dalle prime innovazioni come il chatbot ELIZA nel 1966, a importanti innovazioni come Deep Blue di IBM che ha sconfitto il campione di scacchi Garry Kasparov nel 1997. I progressi più recenti includono l’introduzione di GPT-3 di OpenAI nel 2020 e GPT-4 nel 2023, dimostrando la drammatica evoluzione e la crescente complessità dei sistemi di IA nel corso dei decenni.\n1 La prima rete neurale artificiale, un modello semplice che potrebbe imparare a classificare schemi visivi, simile a un singolo neurone che prende una decisione sì/no in base ai suoi input.\n\n\n\n\n\nFigura 1.2: Pietre miliari nell’IA dal 1950 al 2020. Fonte: IEEE Spectrum\n\n\n\nEsploriamo come siamo arrivati fin qui.\n\n1.2.1 IA Simbolica (1956-1974)\nLa storia del machine learning inizia alla storica conferenza di Dartmouth del 1956, dove pionieri come John McCarthy, Marvin Minsky e Claude Shannon coniarono per primi il termine “intelligenza artificiale”. Il loro approccio si basava su un’idea convincente: l’intelligenza poteva essere ridotta alla manipolazione dei simboli. Si consideri il sistema STUDENT di Daniel Bobrow del 1964, uno dei primi programmi di IA in grado di risolvere problemi di algebra:\n\n\n\n\n\n\nEsempio: STUDENT (1964)\n\n\n\nProblema: \"Se il numero di clienti che Tom ottiene è il doppio del \nquadrato del 20% del numero di pubblicità che gestisce e \nil numero di pubblicità è 45, qual è il numero di\nclienti che Tom ottiene?\"\n \nSTUDENT dovrebbe:\n\n1. Analizzare il testo\n2. Convertirlo in equazioni algebriche\n3. Risolvere l'equazione: n = 2(0.2 × 45)²\n4. Fornire la risposta: 162 clienti\n\n\nLe prime IA come STUDENT soffrivano di una limitazione fondamentale: potevano gestire solo input che corrispondevano esattamente ai loro schemi e regole pre-programmati. Si immagini un traduttore linguistico che funziona solo quando le frasi seguono una struttura grammaticale perfetta: anche piccole variazioni come cambiare l’ordine delle parole, usare sinonimi o schemi di linguaggio naturali causerebbero il fallimento di STUDENT. Questa “fragilità” significava che, mentre queste soluzioni potevano apparire intelligenti quando gestivano casi molto specifici per cui erano state progettate, si sarebbero completamente guastate quando si trovavano di fronte anche a piccole variazioni o complessità del mondo reale. Questa limitazione non era solo un inconveniente tecnico, ma rivelava un problema più profondo con gli approcci basati su regole all’IA: non potevano realmente comprendere o generalizzare dalla loro programmazione, potevano solo abbinare e manipolare i modelli esattamente come specificato.\n\n\n1.2.2 Sistemi Esperti (anni ’70-’80)\nVerso la metà degli anni ’70, i ricercatori si resero conto che l’IA generale era troppo ambiziosa. Si concentraronoinvece, sull’acquisizione di conoscenze esperte umane in domini specifici. MYCIN, sviluppato a Stanford, è stato uno dei primi sistemi esperti su larga scala progettati per diagnosticare le infezioni del sangue:\n\n\n\n\n\n\nEsempio: MYCIN (1976)\n\n\n\nEsempio di regola da MYCIN:\nIF\n    L'infezione è una batteriemia primaria\n    Il sito della coltura è uno dei siti sterili\n    Il presunto portale di ingresso è il tratto gastrointestinale\nTHEN\n    Ci sono prove suggestive (0,7) che l'infezione è batterioide\n\n\nMentre MYCIN ha rappresentato un importante progresso nell’IA medica con le sue 600 regole esperte per la diagnosi delle infezioni del sangue, ha rivelato sfide fondamentali che ancora oggi affliggono l’apprendimento automatico. Ottenere la conoscenza del dominio da esperti umani e convertirla in regole precise si è rivelato incredibilmente dispendioso in termini di tempo e difficile: i medici spesso non riuscivano a spiegare esattamente come prendevano le decisioni. MYCIN ha lottato con informazioni incerte o incomplete, a differenza dei medici umani che potevano fare ipotesi istruite. Forse la cosa più importante è che la manutenzione e l’aggiornamento della base di regole sono diventati esponenzialmente più complessi con la crescita di MYCIN: l’aggiunta di nuove regole spesso entrava in conflitto con quelle esistenti e la conoscenza medica stessa continuava a evolversi. Queste stesse sfide di acquisizione della conoscenza, gestione dell’incertezza e manutenzione rimangono preoccupazioni centrali nell’apprendimento automatico moderno, anche se ora utilizziamo approcci tecnici diversi per affrontarle.\n\n\n1.2.3 Apprendimento Statistico: Un Cambio di Paradigma (anni ’90)\nGli anni ’90 hanno segnato una trasformazione radicale nell’intelligenza artificiale, poiché il campo si è spostato dalle regole codificate a mano verso approcci di apprendimento statistico. Questa non è stata una scelta semplice: è stata guidata da tre fattori convergenti che hanno reso i metodi statistici possibili e potenti. La rivoluzione digitale ha significato che enormi quantità di dati erano improvvisamente disponibili per addestrare gli algoritmi. La legge di Moore2 ha fornito la potenza di calcolo necessaria per elaborare questi dati in modo efficace. E i ricercatori hanno sviluppato nuovi algoritmi come le “Support Vector Machines” [macchine a vettori di supporto] e reti neurali migliorate che potevano effettivamente apprendere modelli da questi dati anziché seguire regole pre-programmate. Questa combinazione ha cambiato radicalmente il modo in cui abbiamo costruito l’IA: invece di cercare di codificare direttamente la conoscenza umana, ora potevamo lasciare che le macchine scoprissero automaticamente i pattern da esempi, portando a un’IA più solida e adattabile.\n2 L’osservazione fatta dal co-fondatore di Intel Gordon Moore nel 1965 secondo cui il numero di transistor su un microchip raddoppia circa ogni due anni, mentre il costo si dimezza. Questa crescita esponenziale della potenza di calcolo è stata un fattore chiave dei progressi nell’apprendimento automatico, sebbene il ritmo abbia iniziato a rallentare negli ultimi anni.Si consideri come si è evoluto il filtraggio dello spam via e-mail:\n\n\n\n\n\n\nEsempio: Sistemi di Rilevamento Precoce dello Spam\n\n\n\nBasati su regole (anni '80):\nIF contains(\"viagra\") OR contains(\"winner\") THEN spam\n\nStatistici (anni '90):\nP(spam|word) = (frequency in spam emails) / (total frequency)\n\nCombinati usando Naive Bayes:\nP(spam|email) ∝ P(spam) × ∏ P(word|spam)\n\n\nIl passaggio agli approcci statistici ha cambiato radicalmente il nostro modo di pensare alla creazione di IA introducendo tre concetti fondamentali che rimangono importanti ancora oggi. In primo luogo, la qualità e la quantità dei dati di addestramento sono diventati importanti quanto gli algoritmi stessi: l’IA poteva apprendere solo i modelli presenti nei suoi esempi di addestramento. In secondo luogo, avevamo bisogno di metodi rigorosi per valutare quanto bene l’IA funzionasse effettivamente, portando a metriche che potessero misurare il successo e confrontare diversi approcci. In terzo luogo, abbiamo scoperto una tensione intrinseca tra precisione (avere ragione quando facciamo una previsione) e richiamo (catturare tutti i casi che dovremmo trovare), costringendo i progettisti a fare compromessi espliciti in base alle esigenze della loro applicazione. Ad esempio, un filtro antispam potrebbe tollerare un po’ di spam per evitare di bloccare e-mail importanti, mentre la diagnosi medica potrebbe dover catturare ogni potenziale caso anche se ciò significa più falsi allarmi.\nTabella 1.1 racchiude il percorso evolutivo degli approcci di IA di cui abbiamo discusso finora, evidenziando i punti di forza e le capacità chiave emersi con ogni nuovo paradigma. Spostandoci da sinistra a destra nella tabella, possiamo osservare diverse tendenze importanti. Parleremo di apprendimento “superficiale” e “profondo” in seguito, ma è utile comprendere i compromessi tra gli approcci trattati finora.\n\n\n\nTabella 1.1: Evoluzione dell’IA - Aspetti Chiave Positivi\n\n\n\n\n\n\n\n\n\n\n\n\nAspetto\nIA simbolica\nSistemi esperti\nApprendimento statistico\nApprendimento superficiale/profondo\n\n\n\n\nPunto di forza\nRagionamento logico\nCompetenza di dominio\nVersatilità\nRiconoscimento di pattern\n\n\nMiglior caso d’uso\nProblemi ben definiti e basati su regole\nProblemi di dominio specifici\nVari problemi di dati strutturati\nProblemi di dati complessi e non strutturati\n\n\nGestione dei dati\nDati minimi necessari\nBasato sulla conoscenza del dominio\nDati moderati richiesti\nElaborazione dati su larga scala\n\n\nAdattabilità\nRegole fisse\nAdattabilità specifica del dominio\nAdattabile a vari domini\nAltamente adattabile a diverse attività\n\n\nComplessità del problema\nSemplice, basato sulla logica\nComplicato, specifico del dominio\nComplesso, strutturato\nAltamente complesso, non strutturato\n\n\n\n\n\n\nLa tabella funge da ponte tra i primi approcci di cui abbiamo parlato e gli sviluppi più recenti nell’apprendimento superficiale e profondo che esploreremo in seguito. Pone le basi per comprendere perché determinati approcci hanno acquisito importanza in epoche diverse e come ogni nuovo paradigma si è basato e ha affrontato i limiti dei suoi predecessori. Inoltre, illustra come i punti di forza degli approcci precedenti continuino a influenzare e migliorare le moderne tecniche di IA, in particolare nell’era dei modelli di fondazione.\n\n\n1.2.4 Apprendimento Superficiale (anni 2000)\nGli anni 2000 hanno segnato un periodo affascinante nella storia del machine learning che ora chiamiamo l’era dello ““shallow learning” [apprendimento superficiale]. Per capire perché è “superficiale”, si immagini di costruire una casa: il “deep learning” [apprendimento profondo] (che è arrivato dopo) è come avere più squadre di costruzione che lavorano a diversi livelli contemporaneamente, ciascuna squadra impara dal lavoro delle squadre sottostanti. Al contrario, l’apprendimento superficiale in genere aveva solo uno o due livelli di elaborazione, come avere solo una squadra di fondazione e una squadra di intelaiatura.\nDurante questo periodo, diversi potenti algoritmi hanno dominato il panorama dell’apprendimento automatico. Ognuno ha portato punti di forza unici a problemi diversi: I “decision trees” [alberi decisionali] hanno fornito risultati interpretabili prendendo decisioni molto simili a un diagramma di flusso. I “K-nearest neighbors” hanno fatto previsioni trovando esempi simili nei dati passati, come chiedere consiglio ai vicini più esperti. La regressione lineare e logistica hanno offerto modelli semplici e interpretabili che hanno funzionato bene per molti problemi del mondo reale. Le “Support Vector Machine (SVM)” eccellevano nel trovare confini complessi tra categorie usando il “trucco del kernel”: Si immagini di poter districare una ciotola di spaghetti in linee rette sollevandola in una dimensione superiore. Questi algoritmi hanno costituito la base della macchina pratica.\nSi consideri una tipica soluzione di visione artificiale del 2005:\n\n\n\n\n\n\nEsempio: Pipeline di Visione Artificiale Tradizionale\n\n\n\n1. Estrazione Manuale delle Feature\n   - SIFT (Scale-Invariant Feature Transform)\n   - HOG (Histogram of Oriented Gradients)\n   - Filtri di Gabor\n2. Selezione/Ingegnerizzazione delle Feature\n3. Modello di Apprendimento \"Superficiale\" (ad esempio, SVM)\n4. Post-elaborazione\n\n\nCiò che ha reso questa era unica è stato il suo approccio ibrido: caratteristiche ingegnerizzate dall’uomo combinate con l’apprendimento statistico. Avevano solide basi matematiche (i ricercatori potevano dimostrare perché funzionavano). Hanno funzionato bene anche con dati limitati. Erano efficienti dal punto di vista computazionale. Hanno prodotto risultati affidabili e riproducibili.\nPrendiamo l’esempio del rilevamento dei volti, in cui l’algoritmo Viola-Jones (2001) ha ottenuto prestazioni in tempo reale utilizzando semplici “feature” [caratteristiche] rettangolari e una cascata di classificatori. Questo algoritmo ha alimentato il rilevamento dei volti delle fotocamere digitali per quasi un decennio.\n\n\n1.2.5 Deep Learning (2012-Oggi)\nMentre le “Support Vector Machine” eccellevano nel trovare confini complessi tra categorie usando trasformazioni matematiche, il deep learning ha adottato un approccio radicalmente diverso ispirato all’architettura del cervello umano. Il deep learning è costruito da “layer” [strati] di neuroni artificiali, dove ogni layer impara a trasformare i suoi dati di input in rappresentazioni sempre più astratte. Si immagini di elaborare un’immagine di un gatto: il primo layer potrebbe imparare a rilevare semplici bordi e contrasti, il layer successivo li combina in forme e texture di base, un altro layer potrebbe riconoscere baffi e orecchie a punta e quelli finali assemblano queste caratteristiche nel concetto di “gatto”. A differenza dei metodi di apprendimento superficiali che richiedevano agli esseri umani di progettare attentamente le feature, le reti di deep learning possono scoprire automaticamente feature utili direttamente dai dati grezzi. Questa capacità di apprendere rappresentazioni gerarchiche, da semplici a complesse, da concrete ad astratte, è ciò che rende il deep learning “deep” [profondo] e si è rivelato un approccio straordinariamente potente per la gestione di dati complessi del mondo reale come immagini, discorsi e testo.\nNel 2012, una rete neurale profonda chiamata AlexNet, mostrata in Figura 1.3, ha raggiunto una svolta nella competizione ImageNet che avrebbe trasformato il campo del machine learning. La sfida era formidabile: classificare correttamente 1,2 milioni di immagini ad alta risoluzione in 1.000 categorie diverse. Mentre gli approcci precedenti hanno lottato con tassi di errore superiori al 25%, AlexNet ha raggiunto un tasso di errore del 15,3%, superando notevolmente tutti i metodi esistenti.\n\n\n\n\n\n\nFigura 1.3: Architettura della rete neurale profonda per Alexnet. Fonte: Krizhevsky, Sutskever, e Hinton (2017)\n\n\nKrizhevsky, Alex, Ilya Sutskever, e Geoffrey E. Hinton. 2017. «ImageNet classification with deep convolutional neural networks». Communications of the ACM 60 (6): 84–90. https://doi.org/10.1145/3065386.\n\n\nIl successo di AlexNet non è stato solo un risultato tecnico, è stato un momento spartiacque che ha dimostrato la fattibilità pratica del deep learning. Ha dimostrato che con dati sufficienti, potenza di calcolo e innovazioni architettoniche, le reti neurali potevano superare le funzionalità progettate a mano e i metodi di apprendimento superficiale che avevano dominato il campo per decenni. Questo singolo risultato ha innescato un’esplosione di ricerca e applicazioni nel deep learning che continua ancora oggi.\nDa questa base, il deep learning è entrato in un’era di portata senza precedenti. Verso la fine del 2010, aziende come Google, Facebook e OpenAI stavano addestrando reti neurali migliaia di volte più grandi di AlexNet3. Questi modelli massicci, spesso chiamati “foundation models” [modelli di base], hanno portato il deep learning a nuovi livelli. GPT-3, rilasciato nel 2020, conteneva 175 miliardi di parametri4—si immagini uno studente che potesse leggere l’intera Wikipedia più volte e apprendere modelli da ogni articolo. Questi modelli hanno mostrato capacità straordinarie: scrivere testi simili a quelli umani, impegnarsi in conversazioni, generare immagini da descrizioni e persino scrivere codice per computer. L’intuizione chiave era semplice ma potente: man mano che ingrandivamo le reti neurali e fornivamo loro più dati, queste diventavano in grado di risolvere attività sempre più complesse. Tuttavia, questa scala ha portato sfide di sistema senza precedenti: come si addestrano in modo efficiente modelli che richiedono migliaia di GPU che lavorano in parallelo? Come si archiviano e si forniscono modelli di centinaia di gigabyte di dimensioni? Come si gestiscono gli enormi set di dati necessari per l’addestramento?\n3 Una rivoluzionaria rete neurale profonda del 2012 che ha vinto la ImageNet competition con un ampio margine e ha contribuito a innescare la rivoluzione del deep learning.4 Simile a come le connessioni neurali del cervello diventano più forti man mano che si apprende una nuova abilità, avere più parametri significa generalmente che il modello può apprendere schemi più complessi.5 Un tipo di rete neurale appositamente progettata per l’elaborazione di immagini, ispirata al funzionamento del sistema visivo umano. La parte “convoluzionale” si riferisce al modo in cui analizza le immagini in piccoli blocchi, in modo simile a come i nostri occhi si concentrano su diverse parti di una scena.La rivoluzione del deep learning del 2012 non è emersa dal nulla, ma è stata costruita sulla ricerca sulle reti neurali risalente agli anni ’50. La storia inizia con il Perceptron di Frank Rosenblatt nel 1957, che ha catturato l’immaginazione dei ricercatori mostrando come un semplice neurone artificiale potesse imparare a classificare gli schemi. Sebbene potesse gestire solo problemi linearmente separabili, una limitazione drammaticamente evidenziata dal libro del 1969 di Minsky e Papert “Perceptrons”, ha introdotto il concetto fondamentale di reti neurali addestrabili. Gli anni ’80 portarono altre importanti scoperte: Rumelhart, Hinton e Williams introdussero la backpropagation nel 1986, fornendo un modo sistematico per addestrare reti multi-layer, mentre Yann LeCun ne dimostrò l’applicazione pratica nel riconoscimento di cifre scritte a mano utilizzando “convolutional neural networks (CNN)” [reti neurali convoluzionali]5.\n\n\n\n\n\n\nVideo 1.1: Demo di Rete Convoluzionale del 1989\n\n\n\n\n\n\nTuttavia, queste reti sono rimaste in gran parte inattive negli anni ’90 e 2000, non perché le idee fossero sbagliate, ma perché erano avanti coi tempi: il campo mancava di tre ingredienti importanti: dati sufficienti per addestrare reti complesse, potenza di calcolo sufficiente per elaborare questi dati e le innovazioni tecniche necessarie per addestrare efficacemente reti molto profonde.\nIl campo ha dovuto attendere la convergenza dei big data, hardware di elaborazione migliore e innovazioni algoritmiche prima che il potenziale del deep learning potesse essere sbloccato. Questo lungo periodo di gestazione aiuta a spiegare perché il momento ImageNet del 2012 è stato meno una rivoluzione improvvisa e più il culmine di decenni di ricerca accumulata che ha finalmente trovato il suo momento. Come esploreremo nelle sezioni seguenti, questa evoluzione ha portato a due sviluppi significativi nel campo. In primo luogo, ha dato origine alla definizione del campo dell’ingegneria dei sistemi di apprendimento automatico, una disciplina che insegna come colmare il divario tra progressi teorici e implementazione pratica. In secondo luogo, ha reso necessaria una definizione più completa dei sistemi di apprendimento automatico, che comprenda non solo gli algoritmi, ma anche i dati e l’infrastruttura informatica. Le attuali sfide di scala riecheggiano molte delle stesse domande fondamentali su metodi di calcolo, dati e apprendimento con cui i ricercatori si sono confrontati sin dall’inizio del campo, ma ora all’interno di un quadro più complesso e interconnesso.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduzione</span>"
    ]
  },
  {
    "objectID": "contents/core/introduction/introduction.it.html#lascesa-dellingegneria-dei-sistemi-di-ml",
    "href": "contents/core/introduction/introduction.it.html#lascesa-dellingegneria-dei-sistemi-di-ml",
    "title": "1  Introduzione",
    "section": "1.3 L’Ascesa dell’Ingegneria dei Sistemi di ML",
    "text": "1.3 L’Ascesa dell’Ingegneria dei Sistemi di ML\nLa storia che abbiamo tracciato, dai primi giorni del Perceptron alla rivoluzione del deep learning, è stata in gran parte una storia di innovazioni algoritmiche. Ogni epoca ha portato nuove intuizioni matematiche e approcci di modellazione che hanno ampliato i confini di ciò che l’IA poteva raggiungere. Ma qualcosa di importante è cambiato nell’ultimo decennio: il successo dei sistemi di IA è diventato sempre più dipendente non solo dalle innovazioni algoritmiche, ma anche da un’ingegneria sofisticata.\nQuesto cambiamento rispecchia l’evoluzione dell’informatica e dell’ingegneria alla fine degli anni ’60 e all’inizio degli anni ’70. Durante quel periodo, man mano che i sistemi informatici diventavano più complessi, emerse una nuova disciplina: la “Computer Engineering” [ingegneria informatica]. Questo campo colmò il divario tra l’esperienza hardware dell’Ingegneria Elettrica e l’attenzione dell’Informatica su algoritmi e software. L’Ingegneria Informatica nacque perché le sfide della progettazione e della costruzione di sistemi informatici complessi richiedevano un approccio integrato che nessuna delle due discipline poteva affrontare completamente da sola.\nOggi, stiamo assistendo a una transizione simile nel campo dell’IA. Mentre l’Informatica continua a spingere i confini degli algoritmi di ML e l’Ingegneria Elettrica fa progredire l’hardware di IA specializzato, nessuna delle due discipline affronta completamente i principi di ingegneria necessari per distribuire, ottimizzare e sostenere i sistemi di ML su larga scala. Questa lacuna evidenzia la necessità di una nuova disciplina: la “Machine Learning Systems Engineering” [ingegneria dei sistemi di apprendimento automatico].\nNon esiste una definizione esplicita di cosa sia questo campo oggi, ma può essere ampiamente definito come tale:\n\n\n\n\n\n\nDefinizione di Machine Learning Systems Engineering\n\n\n\nLa “Machine Learning Systems Engineering (MLSysEng)” è la disciplina di progettazione, implementazione e gestione di sistemi di intelligenza artificiale su scale di elaborazione, dai dispositivi embedded con risorse limitate ai computer su scala industriale. Questo campo integra i principi delle discipline ingegneristiche che spaziano dall’hardware al software per creare sistemi affidabili, efficienti e ottimizzati per il loro contesto di distribuzione. Comprende il ciclo di vita completo delle applicazioni AI: dall’ingegneria dei requisiti e dalla raccolta dati allo sviluppo di modelli, all’integrazione di sistemi, alla distribuzione, al monitoraggio e alla manutenzione. Il campo enfatizza i principi ingegneristici di progettazione sistematica, vincoli di risorse, requisiti di prestazioni e affidabilità operativa.\n\n\nConsideriamo l’esplorazione spaziale. Mentre gli astronauti si avventurano in nuove frontiere ed esplorano le vaste incognite dell’universo, le loro scoperte sono possibili solo grazie ai complessi sistemi di ingegneria che li supportano: i razzi che li sollevano nello spazio, i sistemi di supporto vitale che li mantengono in vita e le reti di comunicazione che li mantengono connessi alla Terra. Allo stesso modo, mentre i ricercatori di IA spingono i confini di ciò che è possibile con gli algoritmi di apprendimento, le loro scoperte diventano realtà pratica solo attraverso un’attenta ingegneria dei sistemi. I moderni sistemi di IA necessitano di un’infrastruttura solida per raccogliere e gestire i dati, di potenti sistemi di elaborazione per addestrare i modelli e di piattaforme di distribuzione affidabili per servire milioni di utenti.\nQuesta emergenza dell’ingegneria dei sistemi di apprendimento automatico come disciplina importante riflette una realtà più ampia: trasformare gli algoritmi di IA in sistemi del mondo reale richiede di colmare il divario tra possibilità teoriche e implementazione pratica. Non basta avere un algoritmo brillante se non si riesce a raccogliere ed elaborare in modo efficiente i dati necessari, distribuirne il calcolo su centinaia di macchine, servirlo in modo affidabile a milioni di utenti o monitorarne le prestazioni in produzione.\nComprendere questa interazione tra algoritmi e ingegneria è diventato fondamentale per i moderni professionisti dell’IA. Mentre i ricercatori continuano a spingere i confini di ciò che è algoritmicamente possibile, gli ingegneri stanno affrontando la complessa sfida di far funzionare questi algoritmi in modo affidabile ed efficiente nel mondo reale. Questo ci porta a una domanda fondamentale: cos’è esattamente un sistema di “machine learning” [apprendimento automatico] e cosa lo rende diverso dai tradizionali sistemi software?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduzione</span>"
    ]
  },
  {
    "objectID": "contents/core/introduction/introduction.it.html#definizione-di-un-sistema-ml",
    "href": "contents/core/introduction/introduction.it.html#definizione-di-un-sistema-ml",
    "title": "1  Introduzione",
    "section": "1.4 Definizione di un sistema ML",
    "text": "1.4 Definizione di un sistema ML\nNon esiste una definizione univoca e universalmente accettata di un sistema di apprendimento automatico. Questa ambiguità deriva dal fatto che diversi professionisti, ricercatori e settori spesso fanno riferimento ai sistemi di apprendimento automatico in contesti diversi e con ambiti diversi. Alcuni potrebbero concentrarsi esclusivamente sugli aspetti algoritmici, mentre altri potrebbero includere l’intera pipeline dalla raccolta dati all’implementazione del modello. Questo uso approssimativo del termine riflette la natura multidisciplinare e in rapida evoluzione del campo.\nData questa diversità di prospettive, è importante stabilire una definizione chiara e completa che comprenda tutti questi aspetti. In questo libro, adottiamo un approccio olistico ai sistemi di apprendimento automatico, considerando non solo gli algoritmi ma anche l’intero ecosistema in cui operano. Pertanto, definiamo un sistema di apprendimento automatico come segue:\n\n\n\n\n\n\nDefinizione di un Sistema di Machine Learning\n\n\n\nUn sistema di “machine learning” [apprendimento automatico] è un sistema di elaborazione integrato che comprende tre componenti principali: (1) dati che guidano il comportamento algoritmico, (2) algoritmi di apprendimento che estraggono modelli da questi dati e (3) infrastruttura di elaborazione che consente sia il processo di apprendimento (ad esempio, addestramento) sia l’applicazione della conoscenza appresa (ad esempio, inferenza/servizio). Insieme, questi componenti creano un sistema di elaborazione in grado di fare previsioni, generare contenuti o intraprendere azioni in base a modelli appresi.\n\n\nIl nucleo di qualsiasi sistema di apprendimento automatico è costituito da tre componenti interrelati, come illustrato in Figura 1.4: modelli/algoritmi, dati e infrastruttura informatica. Questi componenti formano una dipendenza triangolare in cui ogni elemento plasma fondamentalmente le possibilità degli altri. L’architettura del modello detta sia le richieste computazionali per l’addestramento e l’inferenza, sia il volume e la struttura dei dati richiesti per un apprendimento efficace. La scala e la complessità dei dati influenzano l’infrastruttura necessaria per l’archiviazione e l’elaborazione, determinando contemporaneamente quali architetture del modello sono fattibili. Le capacità dell’infrastruttura stabiliscono limiti pratici sia sulla scala del modello che sulla capacità di elaborazione dei dati, creando un framework all’interno del quale devono operare gli altri componenti.\n\n\n\n\n\n\nFigura 1.4: I sistemi di apprendimento automatico coinvolgono algoritmi, dati e calcoli, tutti interconnessi tra loro.\n\n\n\nCiascuno di questi componenti ha uno scopo distinto ma interconnesso:\n\nAlgoritmi: Modelli matematici e metodi che apprendono pattern dai dati per fare previsioni o decisioni\nDati: Processi e infrastrutture per la raccolta, l’archiviazione, l’elaborazione, la gestione e la fornitura di dati sia per l’addestramento che per l’inferenza.\nCalcolo: Infrastruttura hardware e software che consente l’addestramento, la fornitura e il funzionamento efficienti di modelli su larga scala.\n\nL’interdipendenza di questi componenti significa che nessun singolo elemento può funzionare in modo isolato. L’algoritmo più sofisticato non può apprendere senza dati o risorse di elaborazione su cui eseguire. I set di dati più grandi sono inutili senza algoritmi per estrarre modelli o infrastrutture per elaborarli. E l’infrastruttura di elaborazione più potente non serve a nulla senza algoritmi da eseguire o dati da elaborare.\nPer illustrare queste relazioni, possiamo fare un paragone con l’esplorazione spaziale. Gli sviluppatori di algoritmi sono come gli astronauti: esplorano nuove frontiere e fanno scoperte. I team di data science funzionano come specialisti del controllo missione, assicurando il flusso costante di informazioni e risorse critiche necessarie per far funzionare la missione. Gli ingegneri delle infrastrutture informatiche sono come gli ingegneri missilistici: progettano e costruiscono i sistemi che rendono possibile la missione. Proprio come una missione spaziale richiede l’integrazione perfetta di astronauti, controllo missione e sistemi missilistici, un sistema di apprendimento automatico richiede l’attenta orchestrazione di algoritmi, dati e infrastrutture informatiche.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduzione</span>"
    ]
  },
  {
    "objectID": "contents/core/introduction/introduction.it.html#il-ciclo-di-vita-dei-sistemi-ml",
    "href": "contents/core/introduction/introduction.it.html#il-ciclo-di-vita-dei-sistemi-ml",
    "title": "1  Introduzione",
    "section": "1.5 Il ciclo di vita dei sistemi ML",
    "text": "1.5 Il ciclo di vita dei sistemi ML\nI sistemi software tradizionali seguono un ciclo di vita prevedibile in cui gli sviluppatori scrivono istruzioni esplicite che i computer devono eseguire. Questi sistemi sono basati su decenni di consolidate pratiche di ingegneria del software. I sistemi di controllo delle versioni mantengono cronologie precise delle modifiche del codice. Le pipeline di integrazione e distribuzione continue automatizzano i processi di test e rilascio. Gli strumenti di analisi statica misurano la qualità del codice e identificano potenziali problemi. Questa infrastruttura consente uno sviluppo, un test e una distribuzione affidabili di sistemi software, seguendo principi ben definiti di ingegneria del software.\nI sistemi di apprendimento automatico rappresentano una deviazione fondamentale da questo paradigma tradizionale. Mentre i sistemi tradizionali eseguono una logica di programmazione esplicita, i sistemi di apprendimento automatico derivano il loro comportamento da pattern nei dati. Questo passaggio dal codice ai dati come driver principale del comportamento del sistema introduce nuove complessità.\nCome illustrato in Figura 1.5, il ciclo di vita ML è costituito da fasi interconnesse dalla raccolta dati al monitoraggio del modello, con cicli di feedback per il miglioramento continuo quando le prestazioni si degradano o i modelli necessitano di miglioramenti.\n\n\n\n\n\n\nFigura 1.5: Il tipico ciclo di vita di un sistema di apprendimento automatico.\n\n\n\nA differenza del codice sorgente, che cambia solo quando gli sviluppatori lo modificano, i dati riflettono la natura dinamica del mondo reale. Le modifiche nelle distribuzioni dei dati possono alterare silenziosamente il comportamento del sistema. Gli strumenti tradizionali di ingegneria del software, progettati per sistemi basati su codice deterministico, si dimostrano insufficienti per la gestione di questi sistemi dipendenti dai dati. Ad esempio, i sistemi di controllo delle versioni che eccellono nel tracciare modifiche discrete del codice hanno difficoltà a gestire grandi set di dati in evoluzione. I framework di test progettati per output deterministici devono essere adattati per previsioni probabilistiche. Questa natura dipendente dai dati crea un ciclo di vita più dinamico, che richiede un monitoraggio e un adattamento continui per mantenere la pertinenza del sistema man mano che i modelli di dati del mondo reale si evolvono.\nPer comprendere il ciclo di vita del sistema di apprendimento automatico è necessario esaminarne le fasi distinte. Ogni fase presenta requisiti unici sia dal punto di vista dell’apprendimento che da quello dell’infrastruttura. Questa duplice considerazione, delle esigenze di apprendimento e del supporto dei sistemi, è estremamente importante per la creazione di sistemi di machine learning efficaci.\nTuttavia, le varie fasi del ciclo di vita ML in produzione non sono isolate; sono, infatti, profondamente interconnesse. Questa interconnessione può creare circoli virtuosi o viziosi. In un circolo virtuoso, dati di alta qualità consentono un apprendimento efficace, infrastrutture robuste supportano un’elaborazione efficiente e sistemi ben progettati facilitano la raccolta di dati ancora migliori. Tuttavia, in un circolo vizioso, una scarsa qualità dei dati mina l’apprendimento, infrastrutture inadeguate ostacolano l’elaborazione e le limitazioni del sistema impediscono il miglioramento della raccolta dati: ogni problema aggrava gli altri.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduzione</span>"
    ]
  },
  {
    "objectID": "contents/core/introduction/introduction.it.html#lo-spettro-dei-sistemi-ml",
    "href": "contents/core/introduction/introduction.it.html#lo-spettro-dei-sistemi-ml",
    "title": "1  Introduzione",
    "section": "1.6 Lo Spettro dei Sistemi ML",
    "text": "1.6 Lo Spettro dei Sistemi ML\nLa complessità della gestione dei sistemi di machine learning diventa ancora più evidente se consideriamo l’ampio spettro in cui il ML viene distribuito oggi. I sistemi di ML esistono su scale molto diverse e in ambienti diversi, ognuno dei quali presenta sfide e vincoli unici.\nDa un lato, abbiamo sistemi di ML basati su cloud in esecuzione in enormi data center. Questi sistemi, come i grandi modelli linguistici o i motori di raccomandazione, elaborano petabyte di dati e servono milioni di utenti contemporaneamente. Possono sfruttare risorse di elaborazione virtualmente illimitate, ma devono gestire un’enorme complessità operativa e costi.\nDall’altro lato, troviamo sistemi TinyML in esecuzione su microcontrollori e dispositivi embedded. Questi sistemi devono eseguire attività di ML con gravi vincoli di memoria, potenza di elaborazione e consumo energetico. Si immagini un dispositivo per la casa intelligente, come Alexa o Google Assistant, che deve riconoscere i comandi vocali utilizzando meno energia di una lampadina a LED, o un sensore che deve rilevare anomalie mentre funziona a batteria per mesi o addirittura anni.\nTra questi estremi, troviamo una ricca varietà di sistemi di ML adattati a diversi contesti. I sistemi Edge ML avvicinano il calcolo alle fonti dei dati, riducendo i requisiti di latenza e larghezza di banda e gestendo al contempo le risorse di elaborazione locali. I sistemi ML mobili devono bilanciare capacità sofisticate con limitazioni di durata della batteria e del processore su smartphone e tablet. I sistemi ML aziendali spesso operano entro vincoli aziendali specifici, concentrandosi su attività particolari e integrandosi con l’infrastruttura esistente. Alcune organizzazioni impiegano approcci ibridi, distribuendo le capacità ML su più livelli per bilanciare vari requisiti.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduzione</span>"
    ]
  },
  {
    "objectID": "contents/core/introduction/introduction.it.html#implicazioni-del-sistema-di-ml-sul-ciclo-di-vita-ml",
    "href": "contents/core/introduction/introduction.it.html#implicazioni-del-sistema-di-ml-sul-ciclo-di-vita-ml",
    "title": "1  Introduzione",
    "section": "1.7 Implicazioni del Sistema di ML sul Ciclo di Vita ML",
    "text": "1.7 Implicazioni del Sistema di ML sul Ciclo di Vita ML\nLa diversità dei sistemi ML nell’intero spettro rappresenta una complessa interazione di requisiti, vincoli e compromessi. Queste decisioni hanno un impatto fondamentale su ogni fase del ciclo di vita ML di cui abbiamo parlato in precedenza, dalla raccolta dati al funzionamento continuo.\nI requisiti di prestazioni spesso guidano le decisioni architettoniche iniziali. Le applicazioni sensibili alla latenza, come i veicoli autonomi o il rilevamento delle frodi in tempo reale, potrebbero richiedere architetture edge o embedded nonostante i loro vincoli di risorse. Al contrario, le applicazioni che richiedono un’enorme potenza di calcolo per l’addestramento, come i grandi modelli linguistici, gravitano naturalmente verso architetture cloud centralizzate. Tuttavia, le mere prestazioni sono solo una considerazione in uno spazio decisionale complesso.\nLa gestione delle risorse varia notevolmente tra le architetture. I sistemi cloud devono ottimizzare l’efficienza dei costi su larga scala, bilanciando costosi cluster GPU, sistemi di archiviazione e larghezza di banda di rete. I sistemi edge affrontano limiti di risorse fissate e devono gestire attentamente l’elaborazione e l’archiviazione locali. I sistemi mobili ed embedded operano con i vincoli più rigorosi, in cui ogni byte di memoria e milliwatt di potenza sono importanti. Queste considerazioni sulle risorse influenzano direttamente sia la progettazione del modello che l’architettura del sistema.\nLa complessità operativa aumenta con la distribuzione del sistema. Mentre le architetture cloud centralizzate traggono vantaggio da strumenti di distribuzione maturi e servizi gestiti, i sistemi edge e ibridi devono gestire la complessità della gestione del sistema distribuito. Questa complessità si manifesta durante l’intero ciclo di vita del ML, dalla raccolta dati e dal controllo delle versioni alla distribuzione e al monitoraggio del modello. Come discusso nell’esame del debito tecnico, questa complessità operativa può aumentare nel tempo se non gestita attentamente.\nLe considerazioni sui dati spesso introducono pressioni concorrenti. I requisiti sulla privacy o le normative sulla sovranità dei dati potrebbero spingere verso architetture edge o integrate, mentre la necessità di dati di formazione su larga scala potrebbe favorire approcci cloud. Anche la velocità e il volume dei dati influenzano le scelte architettoniche: i dati dei sensori in tempo reale potrebbero richiedere l’elaborazione edge per gestire la larghezza di banda, mentre l’analisi batch potrebbe essere più adatta all’elaborazione cloud.\nI requisiti di evoluzione e manutenzione devono essere considerati fin dall’inizio. Le architetture cloud offrono flessibilità per l’evoluzione del sistema, ma possono comportare costi continui significativi. I sistemi edge ed embedded potrebbero essere più difficili da aggiornare, ma potrebbero offrire un sovraccarico operativo inferiore. Il ciclo continuo dei sistemi ML di cui abbiamo parlato in precedenza diventa particolarmente impegnativo nelle architetture distribuite, dove l’aggiornamento dei modelli e il mantenimento dello stato di salute del sistema richiedono un’attenta orchestrazione su più piani.\nQuesti compromessi sono raramente semplici scelte binarie. I moderni sistemi ML adottano spesso approcci ibridi, bilanciando attentamente queste considerazioni in base a casi d’uso e vincoli specifici. La chiave è comprendere come queste decisioni influenzeranno il sistema durante tutto il suo ciclo di vita, dallo sviluppo iniziale al funzionamento continuo e all’evoluzione.\n\n1.7.1 Tendenze Emergenti\nSiamo solo all’inizio. Man mano che i sistemi di apprendimento automatico continuano a evolversi, diverse tendenze chiave stanno rimodellando il panorama della progettazione e dell’implementazione dei sistemi ML.\nL’ascesa dei “agentic system” [sistemi agenti] segna una profonda evoluzione nei sistemi ML. I sistemi ML tradizionali erano principalmente reattivi: facevano previsioni o classificazioni basate sui dati di input. Al contrario, i “agentic system” possono intraprendere azioni, imparare dai loro risultati e adattare il loro comportamento di conseguenza. Questi sistemi, esemplificati da agenti autonomi in grado di pianificare, ragionare ed eseguire attività complesse, introducono nuove sfide architettoniche. Richiedono framework sofisticati per il processo decisionale, vincoli di sicurezza e interazione in tempo reale con l’ambiente.\nL’evoluzione architettonica è guidata da nuovi pattern di hardware e di distribuzione. Acceleratori AI specializzati stanno emergendo in tutto lo spettro, dai potenti chip dei data center agli efficienti processori edge alle minuscole unità di elaborazione neurale nei dispositivi mobili. Questo panorama informatico eterogeneo sta abilitando nuove possibilità architettoniche, come la distribuzione dinamica dei modelli tra livelli in base alle capacità di elaborazione e alle condizioni attuali. I confini tradizionali tra cloud, edge e sistemi embedded stanno diventando sempre più fluidi.\nL’efficienza delle risorse sta acquisendo importanza man mano che i costi ambientali ed economici del ML su larga scala diventano più evidenti. Ciò ha innescato l’innovazione nella compressione dei modelli, nelle tecniche di addestramento efficienti e nell’elaborazione consapevole dei consumi energetici. I sistemi futuri dovranno probabilmente bilanciare la spinta verso modelli più potenti con le crescenti preoccupazioni per la sostenibilità. Questa enfasi sull’efficienza è particolarmente rilevante data la nostra precedente discussione sul debito tecnico e sui costi operativi.\nL’intelligenza di sistema si sta muovendo verso un funzionamento più autonomo. I futuri sistemi ML probabilmente incorporeranno un auto-monitoraggio più sofisticato, una gestione automatizzata delle risorse e strategie di distribuzione adattive. Questa evoluzione si basa sul ciclo continuo discusso in precedenza, ma con una maggiore automazione nella gestione dei turni di distribuzione dei dati, degli aggiornamenti dei modelli e dell’ottimizzazione del sistema.\nLe sfide dell’integrazione stanno diventando più complesse man mano che i sistemi ML interagiscono con ecosistemi tecnologici più ampi. La necessità di integrarsi con i sistemi software esistenti, gestire diverse fonti di dati e operare oltre i confini organizzativi sta guidando nuovi approcci alla progettazione del sistema. Questa complessità di integrazione aggiunge nuove dimensioni alle considerazioni sul debito tecnico esplorato in precedenza.\nQueste tendenze suggeriscono che i futuri sistemi ML dovranno essere sempre più adattabili ed efficienti, gestendo al contempo una crescente complessità. Comprendere queste direzioni è importante per costruire sistemi che possano evolversi con il settore, evitando al contempo l’accumulo di debito tecnico di cui abbiamo parlato in precedenza.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduzione</span>"
    ]
  },
  {
    "objectID": "contents/core/introduction/introduction.it.html#applicazioni-e-impatto-nel-mondo-reale",
    "href": "contents/core/introduction/introduction.it.html#applicazioni-e-impatto-nel-mondo-reale",
    "title": "1  Introduzione",
    "section": "1.8 Applicazioni e Impatto nel Mondo Reale",
    "text": "1.8 Applicazioni e Impatto nel Mondo Reale\nLa capacità di creare e rendere operativi sistemi ML su diverse scale e ambienti ha portato a cambiamenti trasformativi in numerosi settori. Questa sezione presenta alcuni esempi in cui i concetti teorici e le considerazioni pratiche di cui abbiamo discusso si manifestano in applicazioni tangibili e di impatto nel mondo reale.\nThis section showcases a few examples where theoretical concepts and practical considerations we have discussed manifest in tangible, impactful applications and real-world impact.\n\n1.8.1 Caso di Studio: FarmBeats: ML Edge ed Embedded per l’Agricoltura\nFarmBeats, un progetto sviluppato da Microsoft Research, mostrato in Figura 1.6, è un progresso significativo nell’applicazione dell’apprendimento automatico all’agricoltura. Questo sistema mira ad aumentare la produttività agricola e ridurre i costi sfruttando le tecnologie AI e IoT. FarmBeats esemplifica come i sistemi ML edge ed embedded possono essere distribuiti in ambienti reali e difficili per risolvere problemi pratici. Portando le capacità ML direttamente in azienda agricola, FarmBeats dimostra il potenziale dei sistemi AI distribuiti nella trasformazione delle industrie tradizionali.\n\n\n\n\n\n\nFigura 1.6: Microsoft Farmbeats: AI, Edge e IoT per l’Agricoltura.\n\n\n\nAspetti dei Dati\nL’ecosistema di dati in FarmBeats è diversificato e distribuito. I sensori distribuiti nei campi raccolgono dati in tempo reale su umidità del suolo, temperatura e livelli di nutrienti. I droni dotati di telecamere multispettrali catturano immagini ad alta risoluzione delle colture, fornendo informazioni sulla salute delle piante e sui pattern di crescita. Le stazioni meteorologiche forniscono dati climatici locali, mentre le registrazioni agricole storiche offrono un contesto per le tendenze a lungo termine. La sfida non risiede solo nella raccolta di questi dati eterogenei, ma anche nella gestione del loro flusso da luoghi dispersi, spesso remoti, con connettività limitata. FarmBeats impiega tecniche innovative di trasmissione dati, come l’utilizzo di spazi TV vuoti (frequenze di trasmissione inutilizzate) per estendere la connettività Internet a sensori distanti. Questo approccio alla raccolta e alla trasmissione dei dati incarna i principi dell’edge computing discussi in precedenza, in cui l’elaborazione dei dati inizia alla fonte per ridurre i requisiti di larghezza di banda e consentire il processo decisionale in tempo reale.\nAspetti Algoritmo/Modello\nFarmBeats utilizza una varietà di algoritmi ML su misura per applicazioni agricole. Per la previsione dell’umidità del suolo, utilizza reti neurali temporali in grado di catturare le complesse dinamiche del movimento dell’acqua nel suolo. Gli algoritmi di visione artificiale elaborano le immagini dei droni per rilevare lo stress delle colture, le infestazioni di parassiti e le stime della resa. Questi modelli devono essere robusti ai dati rumorosi e in grado di funzionare con risorse computazionali limitate. I metodi di apprendimento automatico come il “transfer learning” consentono ai modelli di addestrarsi su aziende agricole ricche di dati per essere adattati all’uso in aree con dati storici limitati. Il sistema incorpora anche una combinazione di metodi che combinano gli output di più algoritmi per migliorare l’accuratezza e l’affidabilità delle previsioni. Una sfida chiave che FarmBeats affronta è la personalizzazione del modello, ovvero l’adattamento di modelli generali alle condizioni specifiche delle singole aziende agricole, che possono avere composizioni del suolo, microclimi e pratiche agricole uniche.\nAspetti dell’Infrastruttura Informatica\nFarmBeats esemplifica il paradigma di edge computing esplorato nella discussione sullo spettro del sistema ML. Al livello più basso, i modelli ML embedded vengono eseguiti direttamente su dispositivi e sensori IoT, eseguendo il filtraggio dei dati di base e il rilevamento delle anomalie. I dispositivi edge, come i “ruggedized field gateway” [gateway di campo rinforzati], aggregano i dati da più sensori ed eseguono modelli più complessi per il processo decisionale locale. Questi dispositivi edge operano in condizioni difficili, richiedendo progetti hardware robusti e una gestione efficiente dell’alimentazione per funzionare in modo affidabile in contesti agricoli remoti. Il sistema impiega un’architettura gerarchica, con attività più intensive dal punto di vista computazionale scaricate su server locali o sul cloud. Questo approccio a livelli consente a FarmBeats di bilanciare la necessità di elaborazione in tempo reale con i vantaggi dell’analisi centralizzata dei dati e del training del modello. L’infrastruttura include anche meccanismi per gli aggiornamenti dei modelli over-the-air, assicurando che i dispositivi edge possano ricevere modelli migliorati man mano che diventano disponibili più dati e gli algoritmi vengono perfezionati.\nImpatto e Implicazioni Future\nFarmBeats mostra come i sistemi ML possono essere distribuiti in ambienti reali con risorse limitate per guidare miglioramenti significativi nei settori tradizionali. Fornendo agli agricoltori informazioni basate sull’IA, il sistema ha dimostrato il potenziale per aumentare le rese delle colture, ridurre l’uso dell’acqua e ottimizzare l’allocazione delle risorse. Guardando al futuro, l’approccio FarmBeats potrebbe essere esteso per affrontare le sfide globali in materia di sicurezza alimentare e agricoltura sostenibile. Il successo di questo sistema evidenzia anche la crescente importanza dell’edge e dell’embedded ML nelle applicazioni IoT, dove avvicinare l’intelligenza alla sorgente dei dati può portare a soluzioni più reattive, efficienti e scalabili. Man mano che le capacità di edge computing continuano a progredire, possiamo aspettarci di vedere simili architetture ML distribuite applicate ad altri domini, dalle smart city al monitoraggio ambientale.\n\n\n1.8.2 Caso di Studio: AlphaFold: ML Scientifico su Larga Scala\nAlphaFold, sviluppato da DeepMind, è un traguardo storico nell’applicazione dell’apprendimento automatico a problemi scientifici complessi. Questo sistema di IA è progettato per prevedere la struttura tridimensionale delle proteine, come mostrato in Figura 1.7, dalle loro sequenze di amminoacidi, una sfida nota come “problema del ripiegamento delle proteine” che ha lasciato perplessi gli scienziati per decenni. Il successo di AlphaFold dimostra come i sistemi di ML su larga scala possano accelerare la scoperta scientifica e potenzialmente rivoluzionare campi come la biologia strutturale e la progettazione di farmaci. Questo studio di caso esemplifica l’uso di tecniche di ML avanzate e di enormi risorse computazionali per affrontare problemi alle frontiere della scienza.\n\n\n\n\n\n\nFigura 1.7: Esempi di target proteici nella categoria di modellazione libera. Fonte: Google DeepMind\n\n\n\nAspetti dei Dati\nI dati alla base del successo di AlphaFold sono vasti e sfaccettati. Il set di dati principale è il Protein Data Bank (PDB), che contiene le strutture determinate sperimentalmente di oltre 180.000 proteine. Questo è completato da database di sequenze proteiche, che assommano a centinaia di milioni. AlphaFold utilizza anche dati evolutivi sotto forma di allineamenti di sequenze multiple (MSA), che forniscono informazioni sui modelli di conservazione degli amminoacidi nelle proteine correlate. La sfida non risiede solo nel volume dei dati, ma anche nella loro qualità e rappresentazione. Le strutture proteiche sperimentali possono contenere errori o essere incomplete, il che richiede sofisticati processi di pulizia e convalida dei dati. Inoltre, la rappresentazione delle strutture e delle sequenze proteiche in una forma adatta all’apprendimento automatico è una sfida significativa di per sé. La pipeline di dati di AlphaFold prevede complessi passaggi di pre-elaborazione per convertire i dati grezzi di sequenza e strutturali in feature significative che catturano le proprietà fisiche e chimiche rilevanti per il ripiegamento delle proteine.\nAspetti Algoritmo/Modello\nL’approccio algoritmico di AlphaFold rappresenta un “tour de force” nell’applicazione del deep learning ai problemi scientifici. Al centro, AlphaFold utilizza una nuova architettura di rete neurale che si combina con tecniche di biologia computazionale. Il modello impara a prevedere distanze inter-residuo e angoli di torsione, che vengono poi utilizzati per costruire una struttura proteica 3D completa. Un’innovazione fondamentale è l’uso di layer di “attenzione equivariante” che rispettano le simmetrie intrinseche nelle strutture proteiche. Il processo di apprendimento coinvolge più fasi, tra cui il “pre-addestramento” iniziale su un ampio corpus di sequenze proteiche, seguito da una messa a punto su strutture note. AlphaFold incorpora anche la conoscenza del dominio sotto forma di vincoli basati sulla fisica e funzioni di punteggio, creando un sistema ibrido che sfrutta sia l’apprendimento basato sui dati sia la conoscenza scientifica pregressa. La capacità del modello di generare stime di confidenza accurate per le sue previsioni è fondamentale, consentendo ai ricercatori di valutare l’affidabilità delle strutture previste.\nAspetti dell’Infrastruttura Informatica\nLe esigenze computazionali di AlphaFold esemplificano le sfide dei sistemi ML scientifici su larga scala. L’addestramento del modello richiede enormi risorse di elaborazione parallela, sfruttando cluster di GPU o TPU (Tensor Processing Unit) in un ambiente di elaborazione distribuito. DeepMind ha utilizzato l’infrastruttura cloud di Google, con la versione finale di AlphaFold addestrata su 128 core TPUv3 per diverse settimane. Il processo di inferenza, sebbene meno intensivo dal punto di vista computazionale rispetto all’addestramento, richiede comunque risorse significative, soprattutto quando si prevedono strutture per proteine di grandi dimensioni o si elaborano molte proteine in parallelo. Per rendere AlphaFold più accessibile alla comunità scientifica, DeepMind ha collaborato con l’European Bioinformatics Institute per creare un database pubblico di strutture proteiche previste, che di per sé rappresenta una sfida sostanziale per l’elaborazione e la gestione dei dati. Questa infrastruttura consente ai ricercatori di tutto il mondo di accedere alle previsioni di AlphaFold senza dover eseguire il modello in proprio, dimostrando come le risorse di elaborazione centralizzate e ad alte prestazioni possano essere sfruttate per democratizzare l’accesso alle funzionalità ML avanzate.\nImpatto e Implicazioni Future\nL’impatto di AlphaFold sulla biologia strutturale è stato profondo, con il potenziale di accelerare la ricerca in aree che vanno dalla biologia fondamentale alla scoperta di farmaci. Fornendo previsioni strutturali accurate per proteine che hanno resistito ai metodi sperimentali, AlphaFold apre nuove strade per comprendere i meccanismi delle malattie e progettare terapie mirate. Il successo di AlphaFold serve anche come una potente dimostrazione di come il ML può essere applicato ad altri complessi problemi scientifici, portando potenzialmente a scoperte in campi come la scienza dei materiali o la modellazione climatica. Tuttavia, solleva anche importanti questioni sul ruolo dell’IA nella scoperta scientifica e sulla natura mutevole dell’indagine scientifica nell’era dei sistemi ML su larga scala. Mentre guardiamo al futuro, l’approccio AlphaFold suggerisce un nuovo paradigma per il ML scientifico, in cui enormi risorse computazionali vengono combinate con conoscenze specifiche del dominio per spingere i confini della comprensione umana.\n\n\n1.8.3 Caso di Studio: Veicoli Autonomi: Abbracciare lo Spettro ML\nWaymo, una sussidiaria di Alphabet Inc., è all’avanguardia nella tecnologia dei veicoli autonomi, rappresentando una delle applicazioni più ambiziose dei sistemi di apprendimento automatico fino ad oggi. Evoluzione del Google Self-Driving Car Project avviato nel 2009, l’approccio di Waymo alla guida autonoma esemplifica come i sistemi ML possano abbracciare l’intero spettro, dai sistemi embedded all’infrastruttura cloud. Questo caso di studio mostra l’implementazione pratica di sistemi ML complessi in un ambiente reale critico per la sicurezza, integrando il processo decisionale in tempo reale con l’addestramento e l’adattamento a lungo termine.\n\nAspetti dei Dati\nL’ecosistema di dati alla base della tecnologia Waymo è vasto e dinamico. Ogni veicolo funge da data center itinerante, la sua suite di sensori, composta da LiDAR, radar e telecamere ad alta risoluzione, genera circa un terabyte di dati per ora di guida. Questi dati del mondo reale sono integrati da un set di dati simulato ancora più esteso, con i veicoli Waymo che hanno percorso oltre 20 miliardi di miglia in simulazione e più di 20 milioni di miglia su strade pubbliche. La sfida non risiede solo nel volume di dati, ma nella loro eterogeneità e nella necessità di elaborazione in tempo reale. Waymo deve gestire contemporaneamente sia dati strutturati (ad esempio, coordinate GPS) che non strutturati (ad esempio, immagini della telecamera). La pipeline di dati si estende dall’elaborazione edge sul veicolo stesso a enormi sistemi di archiviazione ed elaborazione basati su cloud. Sono necessari sofisticati processi di validazione e pulizia dei dati, data la natura critica per la sicurezza dell’applicazione. Inoltre, la rappresentazione dell’ambiente del veicolo in una forma adatta all’apprendimento automatico presenta sfide significative, che richiedono una pre-elaborazione complessa per convertire i dati grezzi dei sensori in caratteristiche significative che catturino le dinamiche degli scenari di traffico.\nAspetti Algoritmo/Modello\nLo stack ML di Waymo rappresenta un sofisticato insieme di algoritmi su misura per la sfida multiforme della guida autonoma. Il sistema di percezione impiega tecniche di deep learning, tra cui reti neurali convoluzionali, per elaborare dati visivi per il rilevamento e il tracciamento di oggetti. I modelli di previsione, necessari per anticipare il comportamento di altri utenti della strada, sfruttano reti neurali ricorrenti per comprendere sequenze temporali. Waymo ha sviluppato modelli ML personalizzati come VectorNet per prevedere le traiettorie dei veicoli. I sistemi di pianificazione e decisionali possono incorporare tecniche di apprendimento per rinforzo o apprendimento per imitazione per navigare in scenari di traffico complessi. Un’innovazione fondamentale nell’approccio di Waymo è l’integrazione di questi diversi modelli in un sistema coerente in grado di funzionare in tempo reale. I modelli ML devono anche essere interpretabili in una certa misura, poiché comprendere il ragionamento alla base delle decisioni di un veicolo è fondamentale per la sicurezza e la conformità alle normative. Il processo di apprendimento di Waymo implica un continuo perfezionamento basato su esperienze di guida nel mondo reale e simulazioni approfondite, creando un ciclo di feedback che migliora costantemente le prestazioni del sistema.\nAspetti dell’Infrastruttura Informatica\nL’infrastruttura informatica che supporta i veicoli autonomi di Waymo incarna le sfide dell’implementazione di sistemi ML nell’intero spettro, dall’edge al cloud. Ogni veicolo è dotato di una piattaforma informatica personalizzata in grado di elaborare dati dei sensori e prendere decisioni in tempo reale, spesso sfruttando hardware specializzato come GPU o acceleratori AI personalizzati. Questo edge computing è completato da un ampio utilizzo dell’infrastruttura cloud, sfruttando la potenza dei data center di Google per la formazione di modelli, l’esecuzione di simulazioni su larga scala e l’esecuzione di apprendimento a livello di flotta. La connettività tra questi livelli è fondamentale, con veicoli che richiedono comunicazioni affidabili e ad alta larghezza di banda per aggiornamenti in tempo reale e caricamento dati. L’infrastruttura di Waymo deve essere progettata per robustezza e tolleranza agli errori, garantendo un funzionamento sicuro anche in caso di guasti hardware o interruzioni di rete. La portata delle operazioni di Waymo presenta sfide significative nella gestione dei dati, nell’implementazione dei modelli e nel monitoraggio del sistema in una flotta di veicoli distribuita geograficamente.\nImpatto e Implicazioni Future\nL’impatto di Waymo si estende oltre il progresso tecnologico, rivoluzionando potenzialmente i trasporti, la pianificazione urbana e numerosi aspetti della vita quotidiana. Il lancio di Waymo One, un servizio di “ride-hailing” [a chiamata] commerciale che utilizza veicoli autonomi a Phoenix, in Arizona, rappresenta una pietra miliare significativa nell’implementazione pratica dei sistemi di IA in applicazioni critiche per la sicurezza. I progressi di Waymo hanno implicazioni più ampie per lo sviluppo di sistemi di IA solidi e reali, guidando innovazioni nella tecnologia dei sensori, nell’edge computing e nella sicurezza dell’IA che hanno applicazioni ben oltre l’industria automobilistica. Tuttavia, solleva anche importanti questioni su responsabilità, etica e interazione tra sistemi di IA e società umana. Mentre Waymo continua ad espandere le sue operazioni ed esplorare applicazioni nei trasporti su camion e nelle consegne dell’ultimo miglio, funge da importante banco di prova per sistemi di ML avanzati, guidando i progressi in aree come l’apprendimento continuo, la percezione robusta e l’interazione uomo-IA. Il caso di studio di Waymo sottolinea sia l’enorme potenziale dei sistemi di ML per trasformare i settori sia le complesse sfide implicate nell’implementazione dell’IA nel mondo reale.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduzione</span>"
    ]
  },
  {
    "objectID": "contents/core/introduction/introduction.it.html#sfide-e-considerazioni",
    "href": "contents/core/introduction/introduction.it.html#sfide-e-considerazioni",
    "title": "1  Introduzione",
    "section": "1.9 Sfide e Considerazioni",
    "text": "1.9 Sfide e Considerazioni\nLa creazione e l’implementazione di sistemi di apprendimento automatico presentano sfide uniche che vanno oltre lo sviluppo software tradizionale. Queste sfide aiutano a spiegare perché la creazione di sistemi ML efficaci non riguarda solo la scelta dell’algoritmo giusto o la raccolta di dati sufficienti. Esploriamo le aree chiave in cui i professionisti del ML affrontano ostacoli significativi.\n\n1.9.1 Sfide dei Dati\nIl fondamento di qualsiasi sistema ML sono i suoi dati e la gestione di questi dati presenta diverse sfide fondamentali. Innanzitutto, c’è la questione di base della qualità dei dati: i dati del mondo reale sono spesso disordinati e incoerenti. Si immagini un’applicazione sanitaria che deve elaborare le cartelle cliniche dei pazienti di diversi ospedali. Ogni ospedale potrebbe registrare le informazioni in modo diverso, utilizzare unità di misura diverse o avere standard diversi per i dati da raccogliere. Alcune cartelle potrebbero avere informazioni mancanti, mentre altre potrebbero contenere errori o incongruenze che devono essere eliminate prima che i dati possano essere utili.\nMan mano che i sistemi ML crescono, spesso devono gestire quantità di dati sempre maggiori. Un servizio di streaming video come Netflix, ad esempio, deve elaborare miliardi di interazioni con gli spettatori per alimentare il suo sistema di raccomandazione. Questa scala introduce nuove sfide su come archiviare, elaborare e gestire in modo efficiente set di dati così grandi.\nUn’altra sfida critica è il modo in cui i dati cambiano nel tempo. Questo fenomeno, noto come “data drift”, si verifica quando i modelli nei nuovi dati iniziano a differire dai modelli da cui il sistema ha appreso originariamente. Ad esempio, molti modelli predittivi hanno avuto difficoltà durante la pandemia di COVID-19 perché il comportamento dei consumatori è cambiato così drasticamente che i modelli storici sono diventati meno rilevanti. I sistemi ML hanno bisogno di modi per rilevare quando ciò accade e adattarsi di conseguenza.\n\n\n1.9.2 Sfide del Modello\nLa creazione e la manutenzione dei modelli ML stessi presentano un’altra serie di sfide. I moderni modelli ML, in particolare nel deep learning, possono essere estremamente complessi. Si consideri un modello linguistico come GPT-3, che ha centinaia di miliardi di parametri (le singole impostazioni che il modello apprende durante l’addestramento). Questa complessità crea sfide pratiche: questi modelli richiedono un’enorme potenza di calcolo per l’addestramento e l’esecuzione, rendendo difficile la loro distribuzione in situazioni con risorse limitate, come su telefoni cellulari o dispositivi IoT.\nAddestrare questi modelli in modo efficace è di per sé una sfida significativa. A differenza della programmazione tradizionale in cui scriviamo istruzioni esplicite, i modelli ML imparano dagli esempi. Questo processo di apprendimento comporta molte scelte: come dovremmo strutturare il modello? Per quanto tempo dovremmo addestrarlo? Come possiamo sapere se sta imparando le cose giuste? Prendere queste decisioni spesso richiede sia competenza tecnica che notevoli tentativi ed errori.\nUna sfida particolarmente importante è garantire che i modelli funzionino bene in condizioni reali. Un modello potrebbe funzionare in modo eccellente sui suoi dati di addestramento ma fallire quando si trova di fronte a situazioni leggermente diverse nel mondo reale. Questo divario tra le prestazioni di formazione e le prestazioni nel mondo reale è una sfida centrale nell’apprendimento automatico, specialmente per applicazioni critiche come veicoli autonomi o sistemi di diagnosi medica.\n\n\n1.9.3 Sfide di Sistema\nFar funzionare i sistemi ML in modo affidabile nel mondo reale presenta una serie di sfide. A differenza dei software tradizionali che seguono regole fisse, i sistemi ML devono gestire l’incertezza e la variabilità nei loro input e output. In genere necessitano sia di sistemi di formazione (per apprendere dai dati) sia di sistemi di servizio (per fare previsioni), ognuno con requisiti e vincoli diversi.\nSi consideri un’azienda che costruisce un sistema di riconoscimento vocale. Hanno bisogno di infrastrutture per raccogliere e archiviare dati audio, sistemi per addestrare modelli su questi dati e quindi sistemi separati per elaborare effettivamente il parlato degli utenti in tempo reale. Ogni parte di questa pipeline deve funzionare in modo affidabile ed efficiente e tutte le parti devono funzionare insieme senza problemi.\nQuesti sistemi necessitano anche di monitoraggio e aggiornamento costanti. Come facciamo a sapere se il sistema funziona correttamente? Come aggiorniamo i modelli senza interrompere il servizio? Come gestiamo errori o input imprevisti? Queste sfide operative diventano particolarmente complesse quando i sistemi ML servono milioni di utenti.\n\n\n1.9.4 Considerazioni Etiche e Sociali\nMan mano che i sistemi ML diventano più diffusi nella nostra vita quotidiana, il loro impatto più ampio sulla società diventa sempre più importante da considerare. Una delle principali preoccupazioni è l’equità: i sistemi ML a volte possono imparare a prendere decisioni che discriminano determinati gruppi di persone. Ciò accade spesso in modo involontario, poiché i sistemi rilevano pregiudizi presenti nei loro dati di formazione. Ad esempio, un sistema di screening delle domande di lavoro potrebbe imparare inavvertitamente a favorire determinati dati demografici se tali gruppi avevano storicamente maggiori probabilità di essere assunti.\nUn’altra considerazione importante è la trasparenza. Molti modelli ML moderni, in particolare i modelli di deep learning, funzionano come “scatole nere”: sebbene possano fare previsioni, è spesso difficile capire come sono arrivati alle loro decisioni. Ciò diventa particolarmente problematico quando i sistemi ML prendono decisioni importanti sulla vita delle persone, come nell’assistenza sanitaria o nei servizi finanziari.\nAnche la privacy è una delle principali preoccupazioni. I sistemi ML spesso necessitano di grandi quantità di dati per funzionare in modo efficace, ma questi dati potrebbero contenere informazioni personali sensibili. Come bilanciamo la necessità di dati con la necessità di proteggere la privacy individuale? Come possiamo garantire che i modelli non memorizzino e rivelino inavvertitamente informazioni private?\nQueste sfide non sono semplicemente problemi tecnici da risolvere, ma considerazioni in corso che modellano il nostro approccio alla progettazione e all’implementazione del sistema ML. In questo libro, esploreremo queste sfide in dettaglio ed esamineremo le strategie per affrontarle in modo efficace.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduzione</span>"
    ]
  },
  {
    "objectID": "contents/core/introduction/introduction.it.html#direzioni-future",
    "href": "contents/core/introduction/introduction.it.html#direzioni-future",
    "title": "1  Introduzione",
    "section": "1.10 Direzioni Future",
    "text": "1.10 Direzioni Future\nGuardando al futuro dei sistemi di apprendimento automatico, diverse tendenze entusiasmanti stanno plasmando il campo. Questi sviluppi promettono sia di risolvere le sfide esistenti sia di aprire nuove possibilità per ciò che i sistemi ML possono realizzare.\nUna delle tendenze più significative è la democratizzazione della tecnologia AI. Proprio come i personal computer hanno trasformato l’informatica da mainframe specializzati a strumenti di uso quotidiano, i sistemi ML stanno diventando più accessibili a sviluppatori e organizzazioni di tutte le dimensioni. I provider cloud ora offrono modelli pre-addestrati e piattaforme ML automatizzate che riducono le competenze necessarie per implementare soluzioni AI. Questa democratizzazione sta abilitando nuove applicazioni in tutti i settori, dalle piccole imprese che utilizzano l’AI per il servizio clienti ai ricercatori che applicano l’ML a problemi precedentemente intrattabili.\nCon l’aumento delle preoccupazioni sui costi computazionali e sull’impatto ambientale, c’è una crescente attenzione nel rendere i sistemi ML più efficienti. I ricercatori stanno sviluppando nuove tecniche per addestrare modelli con meno dati e potenza di calcolo. L’innovazione nell’hardware specializzato, dalle GPU migliorate ai chip AI personalizzati, sta rendendo i sistemi ML più veloci e più efficienti dal punto di vista energetico. Questi progressi potrebbero rendere disponibili sofisticate capacità AI su più dispositivi, dagli smartphone ai sensori IoT.\nForse la tendenza più trasformativa è lo sviluppo di sistemi ML più autonomi in grado di adattarsi e migliorarsi. Questi sistemi stanno iniziando a gestire le proprie attività di manutenzione, rilevando quando hanno bisogno di essere riqualificati, trovando e correggendo automaticamente gli errori e ottimizzando le proprie prestazioni. Questa automazione potrebbe ridurre drasticamente le spese generali operative dei sistemi ML in esecuzione, migliorandone al contempo l’affidabilità.\nSebbene queste tendenze siano promettenti, è importante riconoscere i limiti del campo. La creazione di un’intelligenza generale veramente artificiale rimane un obiettivo lontano. Gli attuali sistemi ML eccellono in attività specifiche, ma mancano della flessibilità e della comprensione che gli esseri umani danno per scontate. Le sfide relative a pregiudizi, trasparenza e privacy continuano a richiedere un’attenta considerazione. Man mano che i sistemi ML diventano più diffusi, sarà fondamentale affrontare queste limitazioni sfruttando al contempo nuove capacità.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduzione</span>"
    ]
  },
  {
    "objectID": "contents/core/introduction/introduction.it.html#percorso-di-apprendimento-e-struttura-del-libro",
    "href": "contents/core/introduction/introduction.it.html#percorso-di-apprendimento-e-struttura-del-libro",
    "title": "1  Introduzione",
    "section": "1.11 Percorso di Apprendimento e Struttura del Libro",
    "text": "1.11 Percorso di Apprendimento e Struttura del Libro\nQuesto libro è progettato per guidare dalla comprensione dei fondamenti dei sistemi ML alla progettazione e all’implementazione efficaci. Per affrontare le complessità e le sfide dell’ingegneria dei Sistemi di Machine Learning, abbiamo organizzato il contenuto attorno a cinque pilastri fondamentali che comprendono il ciclo di vita dei sistemi ML. Questi pilastri forniscono un framework per comprendere, sviluppare e mantenere sistemi ML robusti.\n\n\n\n\n\n\nFigura 1.8: Panoramica dei cinque pilastri fondamentali del sistema dell’ingegneria dei Sistemi di Machine Learning.\n\n\n\nCome illustrato nella Figura Figura 1.8, i cinque pilastri centrali del framework sono:\n\nDati: Enfatizzazione dell’ingegneria dei dati e dei principi fondamentali fondamentali per il funzionamento dell’IA in relazione ai dati.\nAddestramento: Esplorazione delle metodologie per l’addestramento dell’IA, concentrandosi su efficienza, ottimizzazione e tecniche di accelerazione per migliorare le prestazioni del modello.\nDistribuzione: Inclusione di benchmark, strategie di addestramento su dispositivo e operazioni di apprendimento automatico per garantire un’applicazione efficace del modello.\nOperazioni: Evidenziazione delle sfide di manutenzione uniche per i sistemi di apprendimento automatico, che richiedono approcci specializzati distinti dai sistemi di ingegneria tradizionali.\nEtica e Governance: Affrontare preoccupazioni quali sicurezza, privacy, pratiche di IA responsabili e le più ampie implicazioni sociali delle tecnologie di IA.\n\nOgni pilastro rappresenta una fase critica nel ciclo di vita dei sistemi ML ed è composto da elementi fondamentali che si basano l’uno sull’altro. Questa struttura garantisce una comprensione completa di “Machine Learning in Science and Engineering” (MLSE), dai principi di base alle applicazioni avanzate e alle considerazioni etiche.\nPer informazioni più dettagliate sulla panoramica del libro, sui contenuti, sui risultati di apprendimento, sul pubblico target, sui prerequisiti e sulla guida alla navigazione, fare riferimento alla sezione Informazioni sul libro. Lì si troveranno anche preziosi dettagli sulla nostra comunità di apprendimento e su come massimizzare l’esperienza con questa risorsa.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduzione</span>"
    ]
  },
  {
    "objectID": "contents/core/ml_systems/ml_systems.it.html",
    "href": "contents/core/ml_systems/ml_systems.it.html",
    "title": "2  Sistemi di ML",
    "section": "",
    "text": "2.1 Panoramica\nIl ML si sta evolvendo rapidamente, con nuovi paradigmi che plasmano il modo in cui i modelli vengono sviluppati, addestrati e implementati. Il settore sta vivendo un’innovazione significativa guidata dai progressi in hardware, software e tecniche algoritmiche. Questi sviluppi stanno consentendo l’applicazione dell’apprendimento automatico in diversi contesti, dalle infrastrutture cloud su larga scala ai dispositivi edge e persino ad ambienti minuscoli e con risorse limitate.\nI moderni sistemi di apprendimento automatico abbracciano uno spettro di opzioni di distribuzione, ciascuna con il proprio set di caratteristiche e casi d’uso. Da un lato, abbiamo l’apprendimento automatico basato su cloud, che sfrutta potenti risorse di elaborazione centralizzate per attività complesse e ad alta intensità di dati. Muovendoci lungo lo spettro, incontriamo l’apprendimento automatico edge, che avvicina l’elaborazione alla fonte dei dati per una latenza ridotta e una migliore privacy. All’estremità opposta, troviamo TinyML, che consente l’apprendimento automatico su dispositivi a bassissimo consumo energetico con pesanti vincoli di memoria ed elaborazione.\nQuesto capitolo esplora il panorama dei sistemi di apprendimento automatico contemporanei, coprendo tre approcci chiave: Cloud ML, Edge ML e TinyML. Figura 2.1 illustra lo spettro dell’intelligenza distribuita attraverso questi approcci, fornendo un confronto visivo delle loro caratteristiche. Esamineremo le caratteristiche uniche, i vantaggi e le sfide di ogni approccio, come illustrato nella figura. Inoltre, discuteremo le tendenze e le tecnologie emergenti che stanno plasmando il futuro dell’implementazione dell’apprendimento automatico, considerando come potrebbero influenzare l’equilibrio tra questi tre paradigmi.\nL’evoluzione dei sistemi di apprendimento automatico può essere vista come una progressione dai paradigmi di elaborazione centralizzati a quelli distribuiti:\nOgnuno di questi paradigmi ha i suoi punti di forza ed è adatto a diversi casi d’uso:\nLa progressione da Cloud a Edge a TinyML riflette una tendenza più ampia nell’informatica verso un’elaborazione più distribuita e localizzata. Questa evoluzione è guidata dalla necessità di tempi di risposta più rapidi, migliore privacy, ridotto utilizzo della larghezza di banda e capacità di operare in ambienti con connettività limitata o assente.\nFigura 2.2 illustra le principali differenze tra Cloud ML, Edge ML e TinyML in termini di hardware, latenza, connettività, requisiti di alimentazione e complessità del modello. Passando da Cloud a Edge a TinyML, assistiamo a una drastica riduzione delle risorse disponibili, che presenta sfide significative per l’implementazione di modelli di apprendimento automatico sofisticati. Questa disparità di risorse diventa particolarmente evidente quando si tenta di implementare modelli di deep learning su microcontrollori, la piattaforma hardware principale per TinyML. Questi piccoli dispositivi hanno capacità di memoria e archiviazione fortemente limitate, che sono spesso insufficienti per i modelli di deep learning convenzionali. Impareremo a mettere queste cose in prospettiva in questo capitolo.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Sistemi di ML</span>"
    ]
  },
  {
    "objectID": "contents/core/ml_systems/ml_systems.it.html#panoramica",
    "href": "contents/core/ml_systems/ml_systems.it.html#panoramica",
    "title": "2  Sistemi di ML",
    "section": "",
    "text": "Figura 2.1: Cloud vs. Edge vs. TinyML: Lo Spettro dell’Intelligenza Distribuita. Fonte: ABI Research – TinyML.\n\n\n\n\n\nCloud ML: Inizialmente, il ML era prevalentemente basato sul cloud. Per addestrare ed eseguire grandi modelli ML venivano utilizzati server potenti nei data center. Questo approccio sfrutta vaste risorse di elaborazione e capacità di archiviazione, consentendo lo sviluppo di modelli complessi addestrati su enormi set di dati. Cloud ML eccelle nelle attività che richiedono un’ampia potenza di elaborazione ed è ideale per applicazioni in cui la reattività in tempo reale non è critica.\nEdge ML: Con l’aumento della necessità di elaborazione in tempo reale e a bassa latenza, è emerso Edge ML. Questo paradigma avvicina le capacità di inferenza alla fonte dei dati, in genere su dispositivi edge come smartphone, telecamere intelligenti o gateway IoT. Edge ML riduce la latenza, migliora la privacy mantenendo i dati locali e può funzionare con connettività cloud intermittente. È particolarmente utile per applicazioni che richiedono risposte rapide o che gestiscono dati sensibili.\nTinyML: L’ultimo sviluppo in questa direzione è TinyML, che consente l’esecuzione di modelli ML su microcontrollori con risorse estremamente limitate e piccoli sistemi embedded. TinyML consente l’inferenza sul dispositivo senza fare affidamento sulla connettività al cloud o all’edge, aprendo nuove possibilità per dispositivi intelligenti alimentati a batteria. Questo approccio è fondamentale per le applicazioni in cui dimensioni, consumo energetico e costi sono fattori critici.\n\n\n\nCloud ML rimane essenziale per le attività che richiedono un’enorme potenza di calcolo o analisi di dati su larga scala.\nEdge ML è ideale per le applicazioni che necessitano di risposte a bassa latenza o elaborazione di dati locali.\nTinyML abilita le funzionalità di intelligenza artificiale in dispositivi piccoli e a basso consumo energetico, espandendo la portata di ML a nuovi domini.\n\n\n\n\n\n\n\n\n\nFigura 2.2: Dalle GPU cloud ai microcontrollori: Navigazione nel panorama della memoria e dell’archiviazione tra dispositivi di elaborazione. Fonte: (Lin et al. 2023)\n\n\nLin, Ji, Ligeng Zhu, Wei-Ming Chen, Wei-Chen Wang, e Song Han. 2023. «Tiny Machine Learning: Progress and Futures Feature». IEEE Circuits Syst. Mag. 23 (3): 8–34. https://doi.org/10.1109/mcas.2023.3302182.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Sistemi di ML</span>"
    ]
  },
  {
    "objectID": "contents/core/ml_systems/ml_systems.it.html#cloud-ml",
    "href": "contents/core/ml_systems/ml_systems.it.html#cloud-ml",
    "title": "2  Sistemi di ML",
    "section": "2.2 Cloud ML",
    "text": "2.2 Cloud ML\nCloud ML sfrutta potenti server nel cloud per il training e l’esecuzione di modelli ML complessi e di grandi dimensioni e si basa sulla connettività Internet. Figura 2.3 fornisce una panoramica delle capacità di Cloud ML che discuteremo più in dettaglio in questa sezione.\n\n\n\n\n\n\nFigura 2.3: Panoramica della sezione per Cloud ML.\n\n\n\n\n2.2.1 Caratteristiche\nDefinizione di Cloud ML\nIl Cloud Machine Learning (Cloud ML) è un sottocampo del machine learning che sfrutta la potenza e la scalabilità dell’infrastruttura di cloud computing per sviluppare, addestrare e distribuire modelli di machine learning. Utilizzando le vaste risorse computazionali disponibili nel cloud, Cloud ML consente la gestione efficiente di set di dati su larga scala e algoritmi di machine learning complessi.\nInfrastruttura Centralizzata\nUna delle caratteristiche principali di Cloud ML è la sua infrastruttura centralizzata. Figura 2.4 illustra questo concetto con un esempio dal data center Cloud TPU di Google. I provider di servizi cloud offrono una piattaforma virtuale composta da server ad alta capacità, soluzioni di storage espansive e architetture di rete robuste, tutte ospitate in data center distribuiti in tutto il mondo. Come mostrato nella figura, queste strutture centralizzate possono essere di grandi dimensioni, ospitando file e file di hardware specializzato. Questa configurazione centralizzata consente la messa in comune e la gestione efficiente delle risorse computazionali, semplificando la scalabilità dei progetti di machine learning in base alle esigenze.\n\n\n\n\n\n\nFigura 2.4: Data center Cloud TPU presso Google. Fonte: Google.\n\n\n\nElaborazione Dati Scalabile e Addestramento dei Modelli\nIl Cloud ML eccelle nella sua capacità di elaborare e analizzare enormi volumi di dati. L’infrastruttura centralizzata è progettata per gestire calcoli complessi e attività di model training che richiedono una notevole potenza di calcolo. Sfruttando la scalabilità del cloud, i modelli di apprendimento automatico possono essere addestrati su grandi quantità di dati, con conseguente miglioramento delle capacità di apprendimento e delle prestazioni predittive.\nDeployment Flessibile e Accessibilità\nUn altro vantaggio di Cloud ML è la flessibilità che offre in termini di deployment [distribuzione] e accessibilità. Una volta che un modello di machine learning è stato addestrato e convalidato, può essere facilmente distribuito e reso accessibile agli utenti tramite servizi basati su cloud. Ciò consente un’integrazione perfetta delle funzionalità di apprendimento automatico in varie applicazioni e servizi, indipendentemente dalla posizione o dal dispositivo dell’utente.\nCollaborazione e Condivisione delle Risorse\nIl Cloud ML promuove la collaborazione e la condivisione delle risorse tra team e organizzazioni. La natura centralizzata dell’infrastruttura cloud consente a più utenti di accedere e lavorare contemporaneamente sugli stessi progetti di apprendimento automatico. Questo approccio collaborativo facilita la condivisione delle conoscenze, accelera il processo di sviluppo e ottimizza l’utilizzo delle risorse.\nEfficacia dei Costi e Scalabilità\nSfruttando il modello di prezzo “pay-as-you-go” offerto dai provider di servizi cloud, Cloud ML consente alle organizzazioni di evitare i costi iniziali associati alla creazione e alla manutenzione della propria infrastruttura di machine learning. La capacità di aumentare o diminuire le risorse in base alla domanda garantisce economicità e flessibilità nella gestione dei progetti di apprendimento automatico.\nIl Cloud ML ha rivoluzionato il modo in cui ci si approccia all’apprendimento automatico, rendendolo più accessibile, scalabile ed efficiente. Ha aperto nuove possibilità per le organizzazioni di sfruttare la potenza dell’apprendimento automatico senza la necessità di investimenti significativi in hardware e infrastruttura.\n\n\n2.2.2 Vantaggi\nIl Cloud ML offre diversi vantaggi significativi che lo rendono una scelta potente per i progetti di apprendimento automatico:\nImmensa Potenza di Calcolo\nUno dei principali vantaggi del Cloud ML è la sua capacità di fornire vaste risorse di calcolo. L’infrastruttura cloud è progettata per gestire algoritmi complessi ed elaborare grandi set di dati in modo efficiente. Ciò è particolarmente vantaggioso per i modelli di apprendimento automatico che richiedono una notevole potenza di calcolo, come reti di deep learning o modelli addestrati su enormi set di dati. Sfruttando le capacità di calcolo del cloud, le organizzazioni possono superare i limiti delle configurazioni hardware locali e ridimensionare i loro progetti di apprendimento automatico per soddisfare requisiti esigenti.\nScalabilità Dinamica\nIl Cloud ML offre scalabilità dinamica, consentendo alle organizzazioni di adattarsi facilmente alle mutevoli esigenze di calcolo. Man mano che il volume dei dati aumenta o la complessità dei modelli di apprendimento automatico aumenta, l’infrastruttura cloud può essere ridimensionata senza problemi verso l’alto o verso il basso per adattarsi a questi cambiamenti. Questa flessibilità garantisce prestazioni costanti e consente alle organizzazioni di gestire carichi di lavoro variabili senza la necessità di ingenti investimenti hardware. Col Cloud ML, le risorse possono essere allocate su richiesta, fornendo una soluzione conveniente ed efficiente per la gestione di progetti di machine learning.\nAccesso a Strumenti e Algoritmi Avanzati\nLe piattaforme Cloud ML forniscono accesso a un’ampia gamma di strumenti e algoritmi avanzati specificamente progettati per l’apprendimento automatico. Questi strumenti spesso includono librerie, framework e API predefiniti che semplificano lo sviluppo e l’implementazione di modelli di apprendimento automatico. Gli sviluppatori possono sfruttare queste risorse per accelerare la creazione, il training e l’ottimizzazione di modelli sofisticati. Utilizzando gli ultimi progressi negli algoritmi e nelle tecniche di apprendimento automatico, le organizzazioni possono rimanere all’avanguardia dell’innovazione e ottenere risultati migliori nei loro progetti di apprendimento automatico.\nAmbiente Collaborativo\nIl Cloud ML promuove un ambiente collaborativo che consente ai team di lavorare insieme senza problemi. La natura centralizzata dell’infrastruttura cloud consente a più utenti di accedere e contribuire agli stessi progetti di apprendimento automatico contemporaneamente. Questo approccio collaborativo facilita la condivisione delle conoscenze, promuove la collaborazione interfunzionale e accelera lo sviluppo e l’iterazione dei modelli di apprendimento automatico. I team possono condividere facilmente codice, set di dati e risultati, consentendo una collaborazione efficiente e guidando l’innovazione in tutta l’organizzazione.\nEfficacia in Termini di Costi\nL’adozione del Cloud ML può essere una soluzione conveniente per le organizzazioni, soprattutto rispetto alla creazione e alla manutenzione di un’infrastruttura di apprendimento automatico in sede. I provider di servizi cloud offrono modelli di prezzo flessibili, come piani pay-as-you-go o basati su abbonamento, consentendo alle organizzazioni di pagare solo per le risorse che consumano. Ciò elimina la necessità di investimenti di capitale iniziali in hardware e infrastruttura, riducendo il costo complessivo dell’implementazione di progetti di apprendimento automatico. Inoltre, la scalabilità di Cloud ML garantisce che le organizzazioni possano ottimizzare l’utilizzo delle risorse ed evitare l’eccesso di provisioning [fornitura], migliorando ulteriormente l’efficienza in termini di costi.\nI vantaggi di Cloud ML, tra cui l’immensa potenza di calcolo, la scalabilità dinamica, l’accesso a strumenti e algoritmi avanzati, l’ambiente collaborativo e la convenienza, lo rendono una scelta interessante per le organizzazioni che desiderano sfruttare il potenziale del machine learning. Sfruttando le capacità del cloud, le organizzazioni possono accelerare le proprie iniziative di machine learning, guidare l’innovazione e ottenere un vantaggio competitivo nell’attuale panorama basato sui dati.\n\n\n2.2.3 Sfide\nSebbene il Cloud ML offra numerosi vantaggi, presenta anche alcune sfide che le organizzazioni devono considerare:\nProblemi di Latenza\nUna delle principali sfide del Cloud ML è il potenziale di problemi della latenza, in particolare nelle applicazioni che richiedono risposte in tempo reale. Poiché i dati devono essere inviati dall’origine dei dati ai server cloud centralizzati per l’elaborazione e quindi di nuovo all’applicazione, potrebbero verificarsi ritardi dovuti alla trasmissione in rete. Questa latenza può rappresentare un notevole svantaggio in scenari sensibili al fattore tempo, come veicoli autonomi, rilevamento delle frodi in tempo reale o sistemi di controllo industriale, in cui è fondamentale prendere decisioni immediate. Gli sviluppatori devono progettare attentamente i propri sistemi per ridurre al minimo la latenza e garantire tempi di risposta accettabili.\nProblemi di Sicurezza e Privacy dei Dati\nLa centralizzazione dell’elaborazione e dell’archiviazione dei dati nel cloud può sollevare preoccupazioni sulla privacy e sulla sicurezza dei dati. Quando i dati sensibili vengono trasmessi e archiviati in data center remoti, diventano vulnerabili a potenziali attacchi informatici e accessi non autorizzati. I data center cloud possono diventare obiettivi interessanti per gli hacker che cercano di sfruttare le vulnerabilità e ottenere l’accesso a informazioni preziose. Le organizzazioni devono investire in misure di sicurezza robuste, come crittografia, controlli di accesso e monitoraggio continuo, per proteggere i propri dati nel cloud. Anche la conformità alle normative sulla privacy dei dati, come GDPR o HIPAA, diventa una considerazione critica quando si gestiscono dati sensibili nel cloud.\nConsiderazioni sui Costi\nCon l’aumento delle esigenze di elaborazione dei dati, i costi associati all’utilizzo dei servizi cloud possono aumentare. Mentre il Cloud ML offre scalabilità e flessibilità, le organizzazioni che gestiscono grandi volumi di dati potrebbero dover affrontare costi crescenti man mano che consumano più risorse cloud. Il modello di prezzo pay-as-you-go dei servizi cloud implica che i costi possono aumentare rapidamente, soprattutto per attività ad alta intensità di elaborazione come l’addestramento e l’inferenza dei modelli. Le organizzazioni devono monitorare e ottimizzare attentamente l’utilizzo del cloud per garantirne la convenienza. Potrebbero dover prendere in considerazione strategie come la compressione dei dati, la progettazione efficiente degli algoritmi e l’ottimizzazione dell’allocazione delle risorse per ridurre al minimo i costi pur ottenendo le prestazioni desiderate.\nDipendenza dalla Connettività Internet\nIl Cloud ML si basa su una connettività Internet stabile e affidabile per funzionare in modo efficace. Poiché i dati devono essere trasmessi da e verso il cloud, eventuali interruzioni o limitazioni nella connettività di rete possono influire sulle prestazioni e sulla disponibilità del sistema di apprendimento automatico. Questa dipendenza dalla connettività Internet può rappresentare un problema in scenari in cui l’accesso alla rete è limitato, inaffidabile o costoso. Le organizzazioni devono garantire un’infrastruttura di rete solida e considerare meccanismi di “failover” o capacità offline per mitigare l’impatto dei problemi di connettività.\nVendor Lock-In\nQuando si adotta il Cloud ML, le organizzazioni spesso diventano dipendenti dagli strumenti, dalle API e dai servizi specifici forniti dal fornitore cloud prescelto. Questo vendor lock-in [blocco da fornitore] può rendere difficile cambiare fornitore o migrare verso piattaforme diverse in futuro. Le organizzazioni possono affrontare sfide in termini di portabilità, interoperabilità e costi quando prendono in considerazione un cambiamento nel loro fornitore di Cloud ML. È importante valutare attentamente le offerte del fornitore, considerare obiettivi strategici a lungo termine e pianificare potenziali scenari di migrazione per ridurre al minimo i rischi associati al vendor lock-in.\nAffrontare queste sfide richiede un’attenta pianificazione, progettazione architettonica e strategie di mitigazione del rischio. Le organizzazioni devono soppesare i vantaggi del Cloud ML rispetto ai potenziali problemi e prendere decisioni informate in base ai loro requisiti specifici, alla sensibilità dei dati e agli obiettivi aziendali. Affrontando proattivamente queste sfide, le organizzazioni possono sfruttare efficacemente la potenza del Cloud ML garantendo al contempo la privacy dei dati, la sicurezza, l’economicità e l’affidabilità complessiva del sistema.\n\n\n2.2.4 Casi d’Uso di Esempio\nIl Cloud ML ha trovato ampia adozione in vari domini, rivoluzionando il modo in cui le aziende operano e gli utenti interagiscono con la tecnologia. Esploriamo alcuni esempi notevoli del Cloud ML in azione:\nAssistenti Virtuali\nIl Cloud ML svolge un ruolo cruciale nel potenziamento di assistenti virtuali come Siri e Alexa. Questi sistemi sfruttano le immense capacità computazionali del cloud per elaborare e analizzare gli input vocali in tempo reale. Sfruttando la potenza dell’elaborazione del linguaggio naturale e degli algoritmi di apprendimento automatico, gli assistenti virtuali possono comprendere le domande degli utenti, estrarre informazioni rilevanti e generare risposte intelligenti e personalizzate. La scalabilità e la potenza di elaborazione del cloud consentono a questi assistenti di gestire un vasto numero di interazioni utente contemporaneamente, offrendo un’esperienza utente fluida e reattiva.\nSistemi di Raccomandazione Commerciali\nIl Cloud ML costituisce la spina dorsale dei sistemi di raccomandazione avanzati utilizzati da piattaforme come Netflix e Amazon. Questi sistemi sfruttano la capacità del cloud di elaborare e analizzare enormi set di dati per scoprire pattern, preferenze e comportamenti degli utenti. Sfruttando il filtraggio collaborativo e altre tecniche di apprendimento automatico, i sistemi di raccomandazione possono offrire contenuti personalizzati o suggerimenti di prodotti su misura per gli interessi di ciascun utente. La scalabilità del cloud consente a questi sistemi di aggiornare e perfezionare continuamente le proprie raccomandazioni in base alla quantità sempre crescente di dati utente, migliorandone il coinvolgimento e la soddisfazione.\nRilevamento delle Frodi\nNel settore finanziario, il Cloud ML ha rivoluzionato i sistemi di rilevamento delle frodi. Sfruttando la potenza di calcolo del cloud, questi sistemi possono analizzare grandi quantità di dati transazionali in tempo reale per identificare potenziali attività fraudolente. Gli algoritmi di apprendimento automatico addestrati su pattern di frode storici possono rilevare anomalie e comportamenti sospetti, consentendo agli istituti finanziari di adottare misure proattive per prevenire le frodi e ridurre al minimo le perdite finanziarie. La capacità del cloud di elaborare e archiviare grandi volumi di dati lo rende una piattaforma ideale per implementare sistemi di rilevamento delle frodi robusti e scalabili.\nEsperienze Utente Personalizzate\nIl Cloud ML è profondamente integrato nelle nostre esperienze online, plasmando il modo in cui interagiamo con le piattaforme digitali. Dagli annunci personalizzati sui feed dei social media alle funzionalità di testo predittivo nei servizi di posta elettronica, il Cloud ML alimenta algoritmi intelligenti che migliorano il coinvolgimento e la praticità dell’utente. Consente ai siti di e-commerce di consigliare prodotti in base alla cronologia di navigazione e acquisto di un utente, ottimizza i motori di ricerca per fornire risultati accurati e pertinenti e automatizza il tagging e la categorizzazione delle foto su piattaforme come Facebook. Sfruttando le risorse di calcolo del cloud, questi sistemi possono apprendere e adattarsi continuamente alle preferenze dell’utente, offrendo un’esperienza utente più intuitiva e personalizzata.\nSicurezza e Rilevamento delle Anomalie\nIl Cloud ML svolge un ruolo nel rafforzare la sicurezza dell’utente alimentando i sistemi di rilevamento delle anomalie. Questi sistemi monitorano costantemente le attività dell’utente e i log di sistema per identificare pattern insoliti o comportamenti sospetti. Analizzando grandi quantità di dati in tempo reale, gli algoritmi Cloud ML possono rilevare potenziali minacce informatiche, come tentativi di accesso non autorizzati, infezioni da malware o violazioni dei dati. La scalabilità e la potenza di elaborazione del cloud consentono a questi sistemi di gestire la crescente complessità e il volume dei dati di sicurezza, fornendo un approccio proattivo alla protezione di utenti e sistemi da potenziali minacce.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Sistemi di ML</span>"
    ]
  },
  {
    "objectID": "contents/core/ml_systems/ml_systems.it.html#edge-ml",
    "href": "contents/core/ml_systems/ml_systems.it.html#edge-ml",
    "title": "2  Sistemi di ML",
    "section": "2.3 Edge ML",
    "text": "2.3 Edge ML\n\n2.3.1 Caratteristiche\nDefinizione di Edge ML\nL’Edge Machine Learning (Edge ML) esegue algoritmi di apprendimento automatico direttamente sui dispositivi endpoint o più vicini al luogo in cui vengono generati i dati anziché affidarsi a server cloud centralizzati. Questo approccio avvicina l’elaborazione alla fonte dei dati, riducendo la necessità di inviarne grandi volumi sulle reti, con conseguente riduzione della latenza e miglioramento della privacy dei dati. Figura 2.5 fornisce una panoramica di questa sezione.\n\n\n\n\n\n\nFigura 2.5: Panoramica della sezione per Edge ML.\n\n\n\nElaborazione Dati Decentralizzata\nIn Edge ML, l’elaborazione dei dati avviene in modo decentralizzato, come illustrato in Figura 2.6. Invece di inviare dati a server remoti, i dati vengono elaborati localmente su dispositivi come smartphone, tablet o dispositivi Internet of Things (IoT). La figura mostra vari esempi di questi dispositivi edge, tra cui dispositivi indossabili, sensori industriali ed elettrodomestici intelligenti. Questa elaborazione locale consente ai dispositivi di prendere decisioni rapide in base ai dati che raccolgono senza affidarsi pesantemente alle risorse di un server centrale.\n\n\n\n\n\n\nFigura 2.6: Esempi di Edge ML. Fonte: Edge Impulse.\n\n\n\nArchiviazione e Calcolo dei Dati Locali\nL’archiviazione e il calcolo dei dati locali sono caratteristiche chiave di Edge ML. Questa configurazione garantisce che i dati possano essere archiviati e analizzati direttamente sui dispositivi, mantenendo così la privacy dei dati e riducendo la necessità di una connettività Internet costante. Inoltre, questo spesso porta a un calcolo più efficiente, poiché i dati non devono percorrere lunghe distanze e i calcoli vengono eseguiti con una comprensione più consapevole del contesto locale, che a volte può portare ad analisi più approfondite.\n\n\n2.3.2 Vantaggi\nLatenza Ridotta\nUno dei principali vantaggi di Edge ML è la significativa riduzione della latenza rispetto al Cloud ML. Questa ridotta latenza può essere un vantaggio fondamentale in situazioni in cui i millisecondi contano, come nei veicoli autonomi, dove un rapido processo decisionale può fare la differenza tra sicurezza e incidente.\nPrivacy dei Dati Migliorata\nEdge ML offre anche una migliore privacy dei dati, poiché i dati vengono principalmente archiviati ed elaborati localmente. Ciò riduce al minimo il rischio di violazioni dei dati, più comuni nelle soluzioni di archiviazione dati centralizzate. Le informazioni sensibili possono essere mantenute più sicure, poiché non vengono inviate su reti che potrebbero essere intercettate.\nMinore Utilizzo della Larghezza di Banda\nOperare più vicino alla fonte dei dati significa che meno dati devono essere inviati sulle reti, riducendo l’utilizzo della larghezza di banda. Ciò può comportare risparmi sui costi e guadagni di efficienza, soprattutto in ambienti in cui la larghezza di banda è limitata o costosa.\n\n\n2.3.3 Sfide\nRisorse di Calcolo Limitate Rispetto al Cloud ML\nTuttavia, Edge ML presenta le sue sfide. Una delle principali preoccupazioni sono le risorse di calcolo limitate rispetto alle soluzioni basate su cloud. I dispositivi endpoint possono avere una potenza di elaborazione o una capacità di archiviazione diverse rispetto ai server cloud, limitando la complessità dei modelli di apprendimento automatico che possono essere distribuiti.\nComplessità nella Gestione dei Nodi Edge\nLa gestione di una rete di nodi Edge può introdurre complessità, soprattutto per quanto riguarda coordinamento, aggiornamenti e manutenzione. Garantire che tutti i nodi funzionino senza problemi e siano aggiornati con gli algoritmi e i protocolli di sicurezza più recenti può essere una sfida logistica.\nProblemi di Sicurezza nei Nodi Edge\nSebbene Edge ML offra una maggiore privacy dei dati, i nodi Edge possono talvolta essere più vulnerabili ad attacchi fisici e informatici. Sviluppare protocolli di sicurezza affidabili che proteggano i dati su ogni nodo senza compromettere l’efficienza del sistema, resta una sfida significativa nell’implementazione di soluzioni Edge ML.\n\n\n2.3.4 Casi d’Uso di Esempio\nEdge ML ha molte applicazioni, dai veicoli autonomi e dalle case intelligenti all’IoT industriale. Questi esempi sono stati scelti per evidenziare scenari in cui l’elaborazione dei dati in tempo reale, la latenza ridotta e la privacy migliorata non sono solo vantaggiose, ma spesso fondamentali per il funzionamento e il successo di queste tecnologie. Dimostrano il ruolo che Edge ML può svolgere nel guidare i progressi in vari settori, promuovendo l’innovazione e aprendo la strada a sistemi più intelligenti, reattivi e adattabili.\nVeicoli Autonomi\nI veicoli autonomi sono un esempio lampante del potenziale di Edge ML. Questi veicoli si affidano in larga misura all’elaborazione dei dati in tempo reale per navigare e prendere decisioni. I modelli di apprendimento automatico localizzati aiutano ad analizzare rapidamente i dati da vari sensori per prendere decisioni di guida immediate, garantendo sicurezza e funzionamento regolare.\nCase ed Edifici Intelligenti\nEdge ML svolge un ruolo cruciale nella gestione efficiente di vari sistemi in case ed edifici intelligenti, dall’illuminazione e dal riscaldamento alla sicurezza. Elaborando i dati localmente, questi sistemi possono funzionare in modo più reattivo e armonioso con le abitudini e le preferenze degli occupanti, creando un ambiente di vita più confortevole.\nIoT industriale\nL’IoT industriale sfrutta Edge ML per monitorare e controllare processi industriali complessi. Qui, i modelli di apprendimento automatico possono analizzare i dati da numerosi sensori in tempo reale, consentendo la manutenzione predittiva, ottimizzando le operazioni e migliorando le misure di sicurezza. Questa rivoluzione nell’automazione e nell’efficienza industriale sta trasformando la manifattura e la produzione in vari settori.\nL’applicabilità di Edge ML è vasta e non si limita a questi esempi. Vari altri settori, tra cui sanità, agricoltura e pianificazione urbana, stanno esplorando e integrando l’Edge ML per sviluppare soluzioni innovative che rispondono alle esigenze e alle sfide del mondo reale, annunciando una nuova era di sistemi intelligenti e interconnessi.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Sistemi di ML</span>"
    ]
  },
  {
    "objectID": "contents/core/ml_systems/ml_systems.it.html#tiny-ml",
    "href": "contents/core/ml_systems/ml_systems.it.html#tiny-ml",
    "title": "2  Sistemi di ML",
    "section": "2.4 Tiny ML",
    "text": "2.4 Tiny ML\n\n2.4.1 Caratteristiche\nDefinizione di TinyML\nTinyML si colloca all’incrocio tra sistemi embedded e apprendimento automatico, rappresentando un campo in rapida crescita che porta algoritmi intelligenti direttamente a microcontrollori e sensori minuscoli. Questi microcontrollori operano con gravi limitazioni di risorse, in particolare per quanto riguarda memoria, archiviazione e potenza di calcolo. Figura 2.7 incapsula gli aspetti chiave di TinyML discussi in questa sezione.\n\n\n\n\n\n\nFigura 2.7: Panoramica della sezione per Tiny ML.\n\n\n\nMachine Learning On-Device\nIn TinyML, l’attenzione è rivolta all’apprendimento automatico sul dispositivo. Ciò significa che i modelli di apprendimento automatico vengono distribuiti e addestrati sul dispositivo, eliminando la necessità di server esterni o infrastrutture cloud. Ciò consente a TinyML di abilitare un processo decisionale intelligente proprio dove vengono generati i dati, rendendo possibili approfondimenti e azioni in tempo reale, anche in contesti in cui la connettività è limitata o non disponibile.\nAmbienti a Basso Consumo Energetico e con Risorse Limitate\nTinyML eccelle in contesti a basso consumo energetico e con risorse limitate. Questi ambienti richiedono soluzioni altamente ottimizzate che funzionino entro le risorse disponibili. Figura 2.8 mostra un kit di dispositivi TinyML di esempio, che illustra la natura compatta di questi sistemi. Questi dispositivi possono solitamente stare nel palmo di una mano o, in alcuni casi, sono persino piccoli come un’unghia. TinyML soddisfa l’esigenza di efficienza tramite algoritmi e modelli specializzati progettati per offrire prestazioni decenti consumando al contempo un’energia minima, garantendo così periodi operativi prolungati, anche in dispositivi alimentati a batteria come quelli mostrati.\n\n\n\n\n\n\nFigura 2.8: Esempi di kit di dispositivi TinyML. Fonte: Widening Access to Applied Machine Learning with TinyML.\n\n\n\n\n\n\n\n\n\nEsercizio 2.1: TinyML con Arduino\n\n\n\n\n\nPrepararsi a portare l’apprendimento automatico sui dispositivi più piccoli! Nel mondo dell’apprendimento automatico embedded, TinyML è il luogo in cui i vincoli di risorse incontrano l’ingegnosità. Questo notebook Colab guiderà nella creazione di un modello di riconoscimento dei gesti progettato su una scheda Arduino. Si imparerà come addestrare una piccola ma efficace rete neurale, ottimizzarla per un utilizzo minimo di memoria e distribuirla al proprio microcontrollore. Se si è entusiasti di rendere più intelligenti gli oggetti di uso quotidiano, è qui che si inizia!\n\n\n\n\n\n\n2.4.2 Vantaggi\nLatenza Estremamente Bassa\nUno dei vantaggi più importanti di TinyML è la sua capacità di offrire una latenza estremamente bassa. Poiché il calcolo avviene direttamente sul dispositivo, il tempo necessario per inviare dati a server esterni e ricevere una risposta viene eliminato. Ciò è fondamentale nelle applicazioni che richiedono un processo decisionale immediato, consentendo risposte rapide a condizioni mutevoli.\nElevata Sicurezza dei Dati\nTinyML migliora intrinsecamente la sicurezza dei dati. Poiché l’elaborazione e l’analisi dei dati avvengono sul dispositivo, il rischio di intercettazione dei dati durante la trasmissione viene praticamente eliminato. Questo approccio localizzato alla gestione dei dati garantisce che le informazioni sensibili rimangano sul dispositivo, rafforzando la sicurezza dei dati dell’utente.\nEfficienza Energetica\nTinyML opera all’interno di un framework efficiente dal punto di vista energetico, una necessità dati i suoi ambienti con risorse limitate. Utilizzando algoritmi snelli e metodi di calcolo ottimizzati, TinyML garantisce che i dispositivi possano eseguire attività complesse senza esaurire rapidamente la durata della batteria, il che lo rende un’opzione sostenibile per le distribuzioni a lungo termine.\n\n\n2.4.3 Sfide\nCapacità di Calcolo Limitate\nTuttavia, il passaggio a TinyML comporta una serie di ostacoli. La limitazione principale sono le capacità di calcolo limitate dei dispositivi. La necessità di operare entro tali limiti implica che i modelli distribuiti debbano essere semplificati, il che potrebbe influire sull’accuratezza e la complessità delle soluzioni.\nCiclo di Sviluppo Complesso\nTinyML introduce anche un ciclo di sviluppo complicato. La creazione di modelli leggeri ed efficaci richiede una profonda comprensione dei principi di apprendimento automatico e competenza nei sistemi embedded. Questa complessità richiede un approccio di sviluppo collaborativo, in cui la competenza multi-dominio è essenziale per il successo.\nOttimizzazione e Compressione del Modello\nUna sfida centrale in TinyML è l’ottimizzazione e la compressione del modello. La creazione di modelli di machine learning in grado di operare efficacemente all’interno della memoria limitata e della potenza di calcolo dei microcontrollori richiede approcci innovativi alla progettazione del modello. Gli sviluppatori si trovano spesso ad affrontare la sfida di trovare un delicato equilibrio e ottimizzare i modelli per mantenere l’efficacia, pur rispettando rigidi vincoli di risorse.\n\n\n2.4.4 Casi d’Uso di Esempio\nDispositivi Indossabili\nNei dispositivi indossabili, TinyML apre le porte a gadget più intelligenti e reattivi. Dai fitness tracker che offrono feedback in tempo reale sugli allenamenti agli occhiali intelligenti che elaborano dati visivi al volo, TinyML trasforma il modo in cui interagiamo con la tecnologia indossabile, offrendo esperienze personalizzate direttamente dal dispositivo.\nManutenzione Predittiva\nNegli ambienti industriali, TinyML svolge un ruolo significativo nella manutenzione predittiva. Implementando algoritmi TinyML su sensori che monitorano lo stato di salute delle apparecchiature, le aziende possono identificare preventivamente potenziali problemi, riducendo i tempi di inattività e prevenendo costosi guasti. L’analisi dei dati in loco garantisce risposte rapide, impedendo potenzialmente a piccoli problemi di diventare problemi gravi.\nRilevamento delle Anomalie\nTinyML può essere impiegato per creare modelli di rilevamento delle anomalie che identificano pattern di dati insoliti. Ad esempio, una fabbrica intelligente potrebbe usare TinyML per monitorare i processi industriali e individuare anomalie, aiutando a prevenire incidenti e migliorare la qualità del prodotto. Allo stesso modo, un’azienda di sicurezza potrebbe usare TinyML per monitorare il traffico di rete per pattern insoliti, aiutando a rilevare e prevenire attacchi informatici. TinyML potrebbe monitorare i dati dei pazienti per anomalie nell’assistenza sanitaria, aiutando a rilevare precocemente le malattie e a migliorare il trattamento dei pazienti.\nMonitoraggio Ambientale\nNel monitoraggio ambientale, TinyML consente l’analisi dei dati in tempo reale da vari sensori distribuiti sul campo. Questi potrebbero spaziare dal monitoraggio della qualità dell’aria in città al tracciamento della fauna selvatica nelle aree protette. Tramite TinyML, i dati possono essere elaborati localmente, consentendo risposte rapide alle mutevoli condizioni e fornendo una comprensione adeguata dei modelli pattern, cruciale per un processo decisionale informato.\nIn sintesi, TinyML funge da pioniere nell’evoluzione dell’apprendimento automatico, promuovendo l’innovazione in vari campi portando l’intelligenza direttamente nell’Edge. Il suo potenziale di trasformare la nostra interazione con la tecnologia e il mondo è immenso, promettendo un futuro in cui i dispositivi sono connessi, intelligenti e capaci di prendere decisioni e rispondere in tempo reale.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Sistemi di ML</span>"
    ]
  },
  {
    "objectID": "contents/core/ml_systems/ml_systems.it.html#confronto",
    "href": "contents/core/ml_systems/ml_systems.it.html#confronto",
    "title": "2  Sistemi di ML",
    "section": "2.5 Confronto",
    "text": "2.5 Confronto\nMettiamo insieme le diverse varianti di ML che abbiamo esplorato individualmente per una visione completa. Figura 2.9 illustra le relazioni e le sovrapposizioni tra Cloud ML, Edge ML e TinyML utilizzando un diagramma di Venn. Questa rappresentazione visiva evidenzia efficacemente le caratteristiche uniche di ciascun approccio, mostrando anche le aree in comune. Ogni paradigma di ML ha le sue caratteristiche distinte, ma ci sono anche intersezioni in cui questi approcci condividono determinati attributi o capacità. Questo diagramma ci aiuta a capire come queste varianti si relazionano tra loro nel più ampio panorama delle implementazioni di apprendimento automatico.\n\n\n\n\n\n\nFigura 2.9: Diagramma di Venn ML. Fonte: arXiv\n\n\n\nPer un confronto più dettagliato di queste varianti di ML, possiamo fare riferimento a Tabella 2.1. Questa tabella offre un’analisi completa di Cloud ML, Edge ML e TinyML in base a varie caratteristiche e aspetti. Esaminando queste diverse caratteristiche una accanto all’altra, otteniamo una prospettiva più chiara sui vantaggi unici e sui fattori distintivi di ogni approccio. Questo confronto dettagliato, combinato con la panoramica visiva fornita dal diagramma di Venn, aiuta a prendere decisioni informate in base alle esigenze e ai vincoli specifici di una determinata applicazione o progetto.\n\n\n\nTabella 2.1: Confronto degli aspetti delle funzionalità tra Cloud ML, Edge ML e TinyML.\n\n\n\n\n\n\n\n\n\n\n\nAspetto\nCloud ML\nEdge ML\nTinyML\n\n\n\n\nUbicazione della elaborazione\nServer centralizzati (Data Center)\nDispositivi locali (più vicini alle fonti di dati)\nSul dispositivo (microcontrollori, sistemi embedded)\n\n\nLatenza\nAlta (dipende dalla connettività Internet)\nModerata (latenza ridotta rispetto a Cloud ML)\nBassa (elaborazione immediata senza ritardo di rete)\n\n\nPrivacy dei dati\nModerata (dati trasmessi tramite reti)\nAlta (i dati rimangono sulle reti locali)\nMolto alta (dati elaborati sul dispositivo, non trasmessi)\n\n\nPotenza di calcolo\nAlta (usa una potente infrastruttura del data center)\nModerata (utilizza le capacità del dispositivo locale)\nBassa (limitata alla potenza del sistema embedded )\n\n\nConsumo energetico\nAlto (i data center consumano molta energia)\nModerato (meno dei data center, più di TinyML)\nBasso (alta efficienza energetica, progettato per bassi consumi)\n\n\nScalabilità\nAlto (facile da scalare con risorse server aggiuntive)\nModerato (dipende dalle capacità del dispositivo locale)\nBasso (limitato dalle risorse hardware del dispositivo)\n\n\nCosto\nAlto (costi ricorrenti per l’uso del server, manutenzione)\nVariabile (dipende dalla complessità della configurazione locale)\nBasso (principalmente costi iniziali per i componenti hardware)\n\n\nConnettività\nAlto (richiede una connettività Internet stabile)\nBasso (può funzionare con connettività intermittente)\nMolto basso (può funzionare senza alcuna connettività di rete)\n\n\nElaborazione in tempo reale\nModerata (può essere influenzata dalla latenza di rete)\nAlta (capace di elaborazione in tempo reale localmente)\nMolto alta (elaborazione immediata con latenza minima)\n\n\nEsempi di applicazione\nAnalisi di Big Data, Assistenti virtuali\nVeicoli autonomi, Case intelligenti\nDispositivi indossabili, Reti di sensori\n\n\nComplessità\nDa moderata ad alta (richiede conoscenza del cloud computing)\nModerata (richiede conoscenza della configurazione della rete locale)\nDa moderata ad alta (richiede competenza nei sistemi embedded)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Sistemi di ML</span>"
    ]
  },
  {
    "objectID": "contents/core/ml_systems/ml_systems.it.html#conclusione",
    "href": "contents/core/ml_systems/ml_systems.it.html#conclusione",
    "title": "2  Sistemi di ML",
    "section": "2.6 Conclusione",
    "text": "2.6 Conclusione\nIn questo capitolo, abbiamo offerto una panoramica in evoluzione dell’apprendimento automatico, che copre i paradigmi cloud, edge e tiny ML. L’apprendimento automatico basato su cloud sfrutta le immense risorse computazionali delle piattaforme cloud per abilitare modelli potenti e accurati, ma presenta delle limitazioni, tra cui problemi di latenza e privacy. Edge ML mitiga queste limitazioni portando l’inferenza direttamente sui dispositivi edge, offrendo una latenza inferiore e ridotte esigenze di connettività. TinyML va oltre, miniaturizzando i modelli ML per eseguirli direttamente su dispositivi con risorse altamente limitate, aprendo una nuova categoria di applicazioni intelligenti.\nOgni approccio ha i suoi compromessi, tra cui complessità del modello, latenza, privacy e costi dell’hardware. Nel tempo, prevediamo la convergenza di questi approcci ML embedded, col pre-training cloud che facilita implementazioni edge e tiny ML più sofisticate. Progressi come l’apprendimento federato e l’apprendimento “on-device” consentiranno ai dispositivi embedded di perfezionare i propri modelli imparando dai dati del mondo reale.\nIl panorama ML embedded si sta evolvendo rapidamente ed è pronto a consentire applicazioni intelligenti su un ampio spettro di dispositivi e casi d’uso. Questo capitolo funge da “istantanea” dello stato attuale del ML embedded. Man mano che algoritmi, hardware e connettività continuano a migliorare, possiamo aspettarci che i dispositivi embedded di tutte le dimensioni diventino sempre più capaci, sbloccando nuove applicazioni trasformative per l’intelligenza artificiale.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Sistemi di ML</span>"
    ]
  },
  {
    "objectID": "contents/core/ml_systems/ml_systems.it.html#sec-ml-systems-resource",
    "href": "contents/core/ml_systems/ml_systems.it.html#sec-ml-systems-resource",
    "title": "2  Sistemi di ML",
    "section": "2.7 Risorse",
    "text": "2.7 Risorse\nEcco un elenco curato di risorse per supportare studenti e insegnanti nei loro percorsi di apprendimento e insegnamento. Stiamo lavorando costantemente per espandere questa raccolta e presto aggiungeremo nuovi esercizi.\n\n\n\n\n\n\nSlide\n\n\n\n\n\nQueste slide sono uno strumento prezioso per gli insegnanti per tenere lezioni e per gli studenti per rivedere il materiale secondo il proprio ritmo. Incoraggiamo studenti e docenti a sfruttare queste slide per migliorare la loro comprensione e facilitare un trasferimento efficace delle conoscenze.\n\nEmbedded Systems Overview.\nEmbedded Computer Hardware.\nEmbedded I/O.\nEmbedded systems software.\nEmbedded ML software.\nEmbedded Inference.\nTinyML on Microcontrollers.\nTinyML as a Service (TinyMLaaS):\n\nTinyMLaaS: Introduction.\nTinyMLaaS: Design Overview.\n\n\n\n\n\n\n\n\n\n\n\nVideo\n\n\n\n\n\n\nProssimamente.\n\n\n\n\n\n\n\n\n\n\nEsercizi\n\n\n\n\n\nPer rafforzare i concetti trattati in questo capitolo, abbiamo curato una serie di esercizi che sfidano gli studenti ad applicare le proprie conoscenze e ad approfondire la propria comprensione.\n\nProssimamente.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Sistemi di ML</span>"
    ]
  },
  {
    "objectID": "contents/core/dl_primer/dl_primer.it.html",
    "href": "contents/core/dl_primer/dl_primer.it.html",
    "title": "3  Avvio al Deep Learning",
    "section": "",
    "text": "3.1 Panoramica",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Avvio al Deep Learning</span>"
    ]
  },
  {
    "objectID": "contents/core/dl_primer/dl_primer.it.html#panoramica",
    "href": "contents/core/dl_primer/dl_primer.it.html#panoramica",
    "title": "3  Avvio al Deep Learning",
    "section": "",
    "text": "3.1.1 Definizione e Importanza\nIl deep learning, un’area specializzata nell’apprendimento automatico e nell’intelligenza artificiale (IA), utilizza algoritmi modellati sulla struttura e la funzione del cervello umano, noti come reti neurali artificiali. Questo campo è un elemento fondamentale nell’IA, che guida il progresso in diversi settori come la visione artificiale, l’elaborazione del linguaggio naturale e i veicoli a guida autonoma. La sua importanza nei sistemi di IA embedded è evidenziata dalla sua capacità di gestire calcoli e previsioni intricati, ottimizzando le risorse limitate nelle impostazioni embedded.\nLa Figura 3.1 fornisce una rappresentazione visiva di come il deep learning si inserisce nel contesto più ampio dell’IA e del “machine learning” [apprendimento automatico]. Il diagramma illustra lo sviluppo cronologico e la relativa segmentazione di questi tre campi interconnessi, mostrando il deep learning come un sottoinsieme specializzato dell’apprendimento automatico, che a sua volta è un sottoinsieme dell’IA.\n\n\n\n\n\n\nFigura 3.1: Il diagramma illustra l’intelligenza artificiale come campo onnicomprensivo che comprende tutti i metodi computazionali che imitano le funzioni cognitive umane. Il Machine learning [apprendimento automatico] è un sottoinsieme dell’IA che include algoritmi in grado di apprendere dai dati. Il deep learning, un ulteriore sottoinsieme del ML, coinvolge specificamente reti neurali in grado di apprendere pattern [schemi] più complessi in grandi volumi di dati. Fonte: NVIDIA.\n\n\n\nCome mostrato nella figura, l’IA rappresenta il campo sovraordinato, che comprende tutti i metodi computazionali che imitano le funzioni cognitive umane. Il machine learning è mostrato come un sottoinsieme dell’IA che include algoritmi in grado di apprendere dai dati. Il deep learning, il sottoinsieme più piccolo nel diagramma, coinvolge specificamente reti neurali in grado di apprendere modelli più complessi da grandi volumi di dati.\n\n\n3.1.2 Breve Storia del Deep Learning\nL’idea del deep learning ha origine nelle prime reti neurali artificiali. Ha vissuto diversi cicli di interesse, a partire dall’introduzione del Perceptron negli anni ’50 (Rosenblatt 1957), seguita dall’invenzione degli algoritmi di backpropagation negli anni ’80 (Rumelhart, Hinton, e Williams 1986).\n\nRosenblatt, Frank. 1957. The perceptron, a perceiving and recognizing automaton Project Para. Cornell Aeronautical Laboratory.\n\nRumelhart, David E., Geoffrey E. Hinton, e Ronald J. Williams. 1986. «Learning representations by back-propagating errors». Nature 323 (6088): 533–36. https://doi.org/10.1038/323533a0.\n\nKrizhevsky, Alex, Ilya Sutskever, e Geoffrey E. Hinton. 2012. «ImageNet Classification with Deep Convolutional Neural Networks». In Advances in Neural Information Processing Systems 25: 26th Annual Conference on Neural Information Processing Systems 2012. Proceedings of a meeting held December 3-6, 2012, Lake Tahoe, Nevada, United States, a cura di Peter L. Bartlett, Fernando C. N. Pereira, Christopher J. C. Burges, Léon Bottou, e Kilian Q. Weinberger, 1106–14. https://proceedings.neurips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html.\nIl termine “deep learning” è diventato importante negli anni 2000, caratterizzato da progressi nella potenza di calcolo e nell’accessibilità dei dati. Traguardi importanti includono l’addestramento di successo di reti profonde come AlexNet (Krizhevsky, Sutskever, e Hinton 2012) da parte di Geoffrey Hinton, una figura di spicco nell’intelligenza artificiale, e il rinnovato focus sulle reti neurali come strumenti efficaci per l’analisi e la modellazione dei dati.\nIl deep learning ha recentemente registrato una crescita esponenziale, trasformando vari settori. La Figura 3.2 illustra questa notevole progressione, evidenziando due tendenze chiave nel settore. Innanzitutto, il grafico mostra che la crescita computazionale ha seguito un modello di raddoppio di 18 mesi dal 1952 al 2010. Questa tendenza ha poi accelerato drasticamente fino a un ciclo di raddoppio di 6 mesi dal 2010 al 2022, indicando un balzo significativo nelle capacità computazionali.\nIn secondo luogo, la figura raffigura l’emergere di modelli su larga scala tra il 2015 e il 2022. Questi modelli sono apparsi da 2 a 3 ordini di grandezza più veloci rispetto alla tendenza generale, seguendo un ciclo di raddoppio di 10 mesi ancora più aggressivo. Questo rapido ridimensionamento delle dimensioni del modello rappresenta un cambiamento di paradigma nelle capacità di deep learning.\n\n\n\n\n\n\nFigura 3.2: Crescita dei modelli di deep learning.\n\n\n\nMolteplici fattori hanno contribuito a questa impennata, tra cui i progressi nella potenza computazionale, l’abbondanza di big data e i miglioramenti nei progetti algoritmici. In primo luogo, la crescita delle capacità computazionali, in particolare l’arrivo delle Graphics Processing Units (GPU) [unità di elaborazione grafica] e delle Tensor Processing Units (TPU) [unità di elaborazione tensoriale] (Jouppi et al. 2017), ha accelerato notevolmente i tempi di training e inferenza dei modelli di apprendimento profondo. Questi miglioramenti hardware hanno consentito la costruzione e il training di reti più complesse e profonde di quanto fosse possibile negli anni precedenti.\n\nJouppi, Norman P., Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal, Raminder Bajwa, Sarah Bates, et al. 2017. «In-Datacenter Performance Analysis of a Tensor Processing Unit». In Proceedings of the 44th Annual International Symposium on Computer Architecture, 1–12. ISCA ’17. New York, NY, USA: ACM. https://doi.org/10.1145/3079856.3080246.\nIn secondo luogo, la rivoluzione digitale ha prodotto una grande quantità di big data, offrendo materiale ricco da cui i modelli di deep learning possono imparare e distinguersi in attività quali il riconoscimento di immagini e parlato, la traduzione linguistica e il gioco. I grandi set di dati etichettati sono stati fondamentali per perfezionare e distribuire con successo applicazioni di deep learning in contesti reali.\nInoltre, le collaborazioni e gli sforzi open source hanno alimentato una comunità dinamica di ricercatori e professionisti, accelerando i progressi nelle tecniche di deep learning. Innovazioni come il “deep reinforcement learning”, il “transfer learning” e l’intelligenza artificiale generativa hanno ampliato la portata di ciò che è realizzabile col deep learning, aprendo nuove possibilità in vari settori, tra cui sanità, finanza, trasporti e intrattenimento.\nLe organizzazioni di tutto il mondo riconoscono il potenziale trasformativo del deep learning e investono molto in ricerca e sviluppo per sfruttare le sue capacità nel fornire soluzioni innovative, ottimizzare le operazioni e creare nuove opportunità di business. Mentre il deep learning continua la sua traiettoria ascendente, è destinato a ridefinire il modo in cui interagiamo con la tecnologia, migliorando la praticità, la sicurezza e la connettività nelle nostre vite.\n\n\n3.1.3 Applicazioni del Deep Learning\nIl deep learning è ampiamente utilizzato in numerosi settori oggi, con il suo impatto trasformativo evidente in vari settori, come illustrato in Figura 3.3. Nella finanza, alimenta la previsione del mercato azionario, la valutazione del rischio e il rilevamento delle frodi, guidando le strategie di investimento e migliorando le decisioni finanziarie. Il marketing sfrutta il deep learning per la segmentazione e la personalizzazione dei clienti, consentendo pubblicità altamente mirate e l’ottimizzazione dei contenuti in base all’analisi del comportamento dei consumatori. Nella produzione, semplifica i processi e migliora il controllo di qualità, consentendo alle aziende di aumentare la produttività e ridurre al minimo gli sprechi. L’assistenza sanitaria trae vantaggio dal deep learning nella diagnosi, nella pianificazione del trattamento e nel monitoraggio dei pazienti, salvando potenzialmente vite umane attraverso migliori previsioni mediche.\n\n\n\n\n\n\nFigura 3.3: Applicazioni, vantaggi e implementazioni del deep learning in vari settori, tra cui finanza, marketing, produzione e assistenza sanitaria. Fonte: Leeway Hertz\n\n\n\nOltre a questi settori principali, il deep learning migliora i prodotti e i servizi di tutti i giorni. Netflix lo utilizza per rafforzare i suoi sistemi di raccomandazione, fornendo agli utenti più raccomandazioni personalizzate. Google ha migliorato notevolmente il suo servizio di traduzione, gestendo ora oltre 100 lingue con maggiore accuratezza, come evidenziato nei suoi recenti progressi. I veicoli autonomi di aziende come Waymo, Cruise e Motional sono diventati realtà grazie al deep learning nel loro perception system. Inoltre, Amazon impiega il deep learning edge nei dispositivi Alexa per attività come individuazione di parole chiave. Queste applicazioni dimostrano come il machine learning spesso predice ed elabora le informazioni con maggiore accuratezza e velocità rispetto agli esseri umani, rivoluzionando vari aspetti della nostra vita quotidiana.\n\n\n3.1.4 Rilevanza per l’IA Embedded\nL’IA embedded, l’integrazione di algoritmi di intelligenza artificiale direttamente nei dispositivi hardware, trae naturalmente vantaggio dalle capacità del deep learning. La combinazione di algoritmi di deep learning e sistemi embedded ha gettato le basi per dispositivi intelligenti e autonomi in grado di analisi avanzate on-device [sul dispositivo]. Il deep learning aiuta a estrarre pattern e informazioni complesse dai dati di input, il che è essenziale nello sviluppo di sistemi embedded intelligenti, dagli elettrodomestici ai macchinari industriali. Questa collaborazione inaugura una nuova era di dispositivi intelligenti e interconnessi, in grado di apprendere e adattarsi al comportamento dell’utente e alle condizioni ambientali, ottimizzando le prestazioni e offrendo praticità ed efficienza senza precedenti.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Avvio al Deep Learning</span>"
    ]
  },
  {
    "objectID": "contents/core/dl_primer/dl_primer.it.html#reti-neurali",
    "href": "contents/core/dl_primer/dl_primer.it.html#reti-neurali",
    "title": "3  Avvio al Deep Learning",
    "section": "3.2 Reti Neurali",
    "text": "3.2 Reti Neurali\nIl deep learning trae ispirazione dalle reti neurali del cervello umano per creare pattern decisionali. Questa sezione approfondisce i concetti fondamentali del deep learning, offrendo approfondimenti sugli argomenti più complessi trattati più avanti in questa introduzione.\nLe reti neurali fungono da fondamento del deep learning, ispirate alle reti neurali biologiche nel cervello umano per elaborare e analizzare i dati in modo gerarchico. Le reti neurali sono composte da unità di base chiamate perceptron, che sono solitamente organizzate in layer [strati]. Ogni layer è costituito da diversi perceptron e più layer sono impilati per formare l’intera rete. Le connessioni tra questi layer sono definite da insiemi di pesi o parametri che determinano come i dati vengono elaborati mentre fluiscono dall’input all’output della rete.\nDi seguito, esaminiamo i componenti e le strutture primarie nelle reti neurali.\n\n3.2.1 Perceptron\nIl Perceptron è l’unità di base o il nodo che costituisce la base per strutture più complesse. Funziona prendendo più input, ognuno dei quali rappresenta una feature dell’oggetto in analisi, come le caratteristiche di una casa per prevederne il prezzo o gli attributi di una canzone per prevederne la popolarità nei servizi di streaming musicale. Questi input sono indicati come \\(x_1, x_2, ..., x_n\\). Un perceptron può essere configurato per eseguire attività di regressione o classificazione. Per la regressione, viene utilizzato l’output numerico effettivo \\(\\hat{y}\\). Per la classificazione, l’output dipende dal fatto che \\(\\hat{y}\\) superi una determinata soglia. Se \\(\\hat{y}\\) supera questa soglia, il perceptron potrebbe restituire una classe (ad esempio, ‘yes’) e, in caso contrario, un’altra classe (ad esempio, ‘no’).\nFigura 3.4 illustra gli elementi fondamentali di un perceptron, che funge da fondamento per reti neurali più complesse. Un perceptron può essere pensato come un decisore in miniatura, che utilizza i suoi pesi, il sui bias [polarizzazione] e la sua funzione di attivazione per elaborare input e generare output in base ai parametri appresi. Questo concetto costituisce la base per comprendere architetture di reti neurali più complesse, come i perceptron multilayer [multistrato]. In queste strutture avanzate, i layer di perceptron lavorano di concerto, con l’output di ogni layer che funge da input per il layer successivo. Questa disposizione gerarchica crea un modello di deep learning in grado di comprendere e modellare pattern complessi e astratti all’interno dei dati. Impilando queste semplici unità, le reti neurali acquisiscono la capacità di affrontare attività sempre più sofisticate, dal riconoscimento delle immagini all’elaborazione del linguaggio naturale.\n\n\n\n\n\n\nFigura 3.4: Perceptron. Concepiti negli anni ’50, i perceptron hanno aperto la strada allo sviluppo di reti neurali più complesse e sono stati un elemento fondamentale nel deep learning. Fonte: Wikimedia - Chrislb.\n\n\n\nCiascun input \\(x_i\\) ha un peso corrispondente \\(w_{ij}\\) e il perceptron moltiplica semplicemente ogni input per il suo peso corrispondente. Questa operazione è simile alla regressione lineare, dove l’output intermedio, \\(z\\), è calcolato come la somma dei prodotti degli input e dei loro pesi:\n\\[\nz = \\sum (x_i \\cdot w_{ij})\n\\]\nA questo calcolo intermedio, viene aggiunto un termine di bias \\(b\\), che consente al modello di adattarsi meglio ai dati spostando la funzione di output lineare verso l’alto o verso il basso. Pertanto, la combinazione lineare intermedia calcolata dal perceptron, incluso il bias, diventa:\n\\[\nz = \\sum (x_i \\cdot w_{ij}) + b\n\\]\nQuesta forma base di un perceptron può modellare solo relazioni lineari tra input e output. I pattern trovati in natura sono spesso complessi e si estendono oltre le relazioni lineari. Per consentire al perceptron di gestire relazioni non lineari, una funzione di attivazione viene applicata all’output lineare \\(z\\).\n\\[\n\\hat{y} = \\sigma(z)\n\\]\nFigura 3.5 illustra un esempio in cui i dati presentano un pattern non lineare che non potrebbe essere modellato adeguatamente con un approccio lineare. La funzione di attivazione, come la sigmoide, la tanh o la ReLU, trasforma la somma di input lineare in un output non lineare. L’obiettivo principale di questa funzione è introdurre la non linearità nel modello, consentendogli di apprendere ed eseguire attività più sofisticate. Pertanto, l’output finale del perceptron, inclusa la funzione di attivazione, può essere espresso come:\n\n\n\n\n\n\nFigura 3.5: Le funzioni di attivazione consentono la modellazione di relazioni non lineari complesse. Fonte: Medium - Sachin Kaushik.\n\n\n\n\n\n3.2.2 Perceptron Multilayer\nI “Multilayer perceptron” (MLP) sono un’evoluzione del modello del perceptron a singolo layer, caratterizzato da più layer di nodi collegati in modo “feedforward”. Figura 3.6 fornisce una rappresentazione visiva di questa struttura. Come illustrato nella figura, le informazioni in una rete “feedforward” si muovono in una sola direzione: dal livello di input a sinistra, attraverso i livelli nascosti al centro, fino al livello di output a destra, senza cicli o loop.\n\n\n\n\n\n\nFigura 3.6: Perceptron Multilayer. Fonte: Wikimedia - Charlie.\n\n\n\nMentre un singolo perceptron è limitato nella sua capacità di modellare pattern complessi, la vera forza delle reti neurali emerge dall’assemblaggio di più layer. Ciascun layer è costituito da numerosi perceptron che lavorano insieme, consentendo alla rete di catturare relazioni intricate e non lineari all’interno dei dati. Con sufficiente profondità e ampiezza, queste reti possono approssimare praticamente qualsiasi funzione, indipendentemente da quanto sia complessa.\n\n\n3.2.3 Processo di Training\nUna rete neurale riceve un input, esegue un calcolo e produce una previsione. La previsione è determinata dai calcoli eseguiti all’interno dei set di perceptron trovati tra i layer di input e output. Questi calcoli dipendono principalmente dall’input e dai pesi. Poiché non si ha il controllo sull’input, l’obiettivo durante il training [addestramento] è quello di regolare i pesi in modo tale che l’output della rete fornisca la previsione più accurata.\nIl processo di addestramento prevede diversi passaggi chiave, a partire dal passaggio in avanti (forward), in cui i pesi esistenti della rete vengono utilizzati per calcolare l’output per un dato input. Questo output viene poi confrontato con i veri valori target per calcolare un errore, che misura quanto bene la previsione della rete corrisponde al risultato previsto. In seguito, viene eseguito un passaggio all’indietro (backward). Ciò comporta l’utilizzo dell’errore per apportare modifiche ai pesi della rete tramite un processo chiamato “backpropagation”. Questa regolazione riduce l’errore nelle previsioni successive. Il ciclo di passaggio forward [in avanti], calcolo dell’errore e passaggio backward [all’indietro] viene ripetuto iterativamente. Questo processo continua finché le previsioni della rete non sono sufficientemente accurate o non viene raggiunto un numero predefinito di iterazioni, riducendo al minimo la “funzione di perdita” utilizzata per misurare l’errore.\n\nForward Pass\nIl forward pass è la fase iniziale in cui i dati si spostano attraverso la rete dal livello di input a quello di output, come illustrato in Figura 3.7. All’inizio dell’addestramento, i pesi della rete vengono inizializzati in modo casuale, impostando le condizioni iniziali. Durante il “forward pass”, ogni layer esegue calcoli specifici sui dati di input utilizzando questi pesi e il bias, e i risultati vengono poi passati al layer successivo. L’output finale di questa fase è la previsione della rete. Questa “prediction” viene confrontata con i valori target effettivi presenti nel set di dati per calcolare la “loss” [perdita], che può essere considerata come la differenza tra gli output previsti e i valori target. La perdita quantifica le prestazioni della rete in questa fase, fornendo una metrica cruciale per la successiva regolazione dei pesi durante il backward pass.\n\n\n\n\n\n\nFigura 3.7: Reti neurali: propagazione forward e backward. Fonte: Linkedin\n\n\n\n\n\nBackward Pass (Backpropagation)\nDopo aver completato il forward pass e calcolato la perdita, che misura quanto le previsioni del modello si discostano dai valori target effettivi, il passo successivo è migliorare le prestazioni del modello regolando i pesi della rete. Poiché non possiamo controllare gli input del modello, la regolazione dei pesi diventa il nostro metodo principale per perfezionare il modello.\nDeterminiamo come regolare i pesi del nostro modello tramite un algoritmo chiave chiamato “backpropagation”. La backpropagation utilizza la perdita calcolata per determinare il gradiente di ciascun peso. Questi gradienti descrivono la direzione e l’entità in cui i pesi devono essere regolati. Regolando i pesi in base a questi gradienti, il modello è meglio posizionato per fare previsioni più vicine ai valori target effettivi nel successivo “forward pass”.\nComprendere questi concetti fondamentali apre la strada alla comprensione di architetture e tecniche di deep learning più complesse, favorendo lo sviluppo di applicazioni più sofisticate e produttive, in particolare all’interno di sistemi di intelligenza artificiale embedded.\nVideo 3.1 and Video 3.2 build upon Video 3.3. Riguardano la “gradient descent” [discesa del gradiente] e la backpropagation nelle reti neurali.\n\n\n\n\n\n\nVideo 3.1: Gradient descent\n\n\n\n\n\n\n\n\n\n\n\n\nVideo 3.2: Backpropagation\n\n\n\n\n\n\n\n\n\n3.2.4 Architetture dei Modelli\nLe architetture di deep learning si riferiscono ai vari approcci strutturati che stabiliscono come i neuroni e i layer sono organizzati e interagiscono nelle reti neurali. Queste architetture si sono evolute per affrontare efficacemente diversi problemi e diversi tipi di dati. Questa sezione fornisce una panoramica di alcune note architetture di deep learning e delle loro caratteristiche.\n\nMultilayer Perceptron (MLP)\nGli MLP sono architetture di deep learning di base che comprendono tre layer: uno di input, uno o più layer nascosti e un layer di output. Questi layer sono completamente connessi, il che significa che ogni neurone in uno layer è collegato a ogni neurone nei layer precedenti e successivi. Gli MLP possono modellare funzioni complesse e sono utilizzati in varie attività, come regressione, classificazione e riconoscimento di pattern. La loro capacità di apprendere relazioni non lineari tramite backpropagation li rende uno strumento versatile nel toolkit di deep learning.\nNei sistemi di intelligenza artificiale embedded, gli MLP possono funzionare come modelli compatti per attività più semplici come l’analisi dei dati dei sensori o il riconoscimento di pattern di base, in cui le risorse computazionali sono limitate. La loro capacità di apprendere relazioni non lineari con una complessità relativamente minore li rende una scelta adatta per i sistemi embedded.\n\n\n\n\n\n\nEsercizio 3.1: Multilayer Perceptron (MLP)\n\n\n\n\n\nAbbiamo appena scalfito la superficie delle reti neurali. Ora, proveremo ad applicare questi concetti in esempi pratici. Nei notebook Colab forniti, si esploreranno:\nPrevisione dei prezzi delle case: Scoprire come le reti neurali possono analizzare i dati sugli alloggi per stimare i valori delle proprietà. \nClassificazione delle immagini: Scoprire come creare una rete per comprendere il famoso set di dati di cifre scritte a mano MNIST. \nDiagnosi medica nel mondo reale: Usare il deep learning per affrontare l’importante compito della classificazione del cancro al seno. \n\n\n\n\n\nConvolutional Neural Networks (CNNs)\nLe CNN [reti neurali convoluzionali] sono utilizzate principalmente in attività di riconoscimento di immagini e video. Questa architettura è composta da due parti principali: la base convoluzionale e i layer completamente connessi. Nella base convoluzionale, i layer convoluzionali filtrano i dati di input per identificare feature come bordi, angoli e texture [trame]. Dopo ogni layer convoluzionale, è possibile applicare un layer di pooling [raggruppamento] per ridurre le dimensioni spaziali dei dati, diminuendo così il carico computazionale e concentrando le feature estratte. A differenza degli MLP, che trattano le feature di input come entità piatte e indipendenti, le CNN mantengono le relazioni spaziali tra i pixel, rendendole particolarmente efficaci per i dati di immagini e video. Le feature estratte dalla base convoluzionale vengono poi passate ai layer completamente connessi, simili a quelli utilizzati negli MLP, che eseguono la classificazione in base alle feature estratte dai layer di convoluzione. Le CNN si sono dimostrate altamente efficaci nel riconoscimento delle immagini, nel rilevamento di oggetti e in altre applicazioni di visione artificiale.\nVideo 3.3 spiega come funzionano le reti neurali usando il riconoscimento di cifre scritte a mano come applicazione di esempio. Affronta anche la matematica alla base delle reti neurali.\n\n\n\n\n\n\nVideo 3.3: Reti MLP & CNN\n\n\n\n\n\n\nLe CNN sono fondamentali per le attività di riconoscimento di immagini e video, in cui spesso è necessaria l’elaborazione in tempo reale. Possono essere ottimizzate per i sistemi embedded utilizzando tecniche come la quantizzazione e il “pruning” [potatura] per ridurre al minimo l’utilizzo della memoria e le richieste computazionali, consentendo funzionalità efficienti di rilevamento di oggetti e riconoscimento facciale in dispositivi con risorse computazionali limitate.\n\n\n\n\n\n\nEsercizio 3.2: Convolutional Neural Networks (CNNs)\n\n\n\n\n\nAbbiamo discusso del fatto che le CNN [Reti neurali convoluzionali] sono eccellenti nell’identificare le caratteristiche delle immagini, il che le rende ideali per attività come la classificazione degli oggetti. Ora, si potrà mettere in pratica questa conoscenza! Questo notebook Colab si concentra sulla creazione di una CNN per classificare le immagini dal set di dati CIFAR-10, che include oggetti come aeroplani, automobili e animali. Si impareranno le principali differenze tra CIFAR-10 e il set di dati MNIST che abbiamo esplorato in precedenza e come queste differenze influenzano la scelta del modello. Alla fine di questo notebook si avrà una conoscenza delle CNN per il riconoscimento delle immagini.\n\n\n\n\n\n\nRecurrent Neural Networks (RNN)\nLe RNN [Reti Neurali Ricorrenti] sono adatte per l’analisi di dati sequenziali, come la previsione di serie temporali e l’elaborazione del linguaggio naturale. In questa architettura, le connessioni tra i nodi formano un grafo diretto lungo una sequenza temporale, consentendo il trasporto delle informazioni attraverso le sequenze tramite vettori di stato nascosti. Le varianti delle RNN includono le Long Short-Term Memory (LSTM) e le Gated Recurrent Units (GRU), progettate per catturare dipendenze più lunghe nei dati sequenziali.\nQueste reti possono essere utilizzate nei sistemi di riconoscimento vocale, nella manutenzione predittiva o nei dispositivi IoT in cui sono comuni i pattern di dati sequenziali. Le ottimizzazioni specifiche per le piattaforme embedded possono aiutare a gestirne i requisiti di elaborazione e memoria tipicamente elevati.\n\n\nGenerative Adversarial Network (GAN)\nLe GAN [Reti Generative Avversarie] sono costituite da due reti, un generatore e un discriminatore, addestrate simultaneamente tramite l’addestramento adversarial [avversario] (Goodfellow et al. 2020). Il generatore produce dati che tentano di imitare la distribuzione di quelli reali, mentre il discriminatore distingue tra dati reali e dati generati. Le GAN sono ampiamente utilizzate nella generazione di immagini, nel trasferimento di stile e nell’aumento dei dati.\n\nGoodfellow, Ian, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, e Yoshua Bengio. 2020. «Generative adversarial networks». Commun. ACM 63 (11): 139–44. https://doi.org/10.1145/3422622.\nIn contesti embedded, le reti GAN potrebbero essere utilizzate per l’aumento dei dati sul dispositivo per migliorare il training dei modelli direttamente sul dispositivo embedded, consentendo un apprendimento continuo e un adattamento ai nuovi dati senza la necessità di risorse di cloud computing.\n\n\nAutoencoder\nGli autoencoder sono reti neurali per la compressione dei dati e la riduzione del rumore (Bank, Koenigstein, e Giryes 2023). Sono strutturati per codificare i dati di input in una rappresentazione a dimensione inferiore e quindi decodificarli nella loro forma originale. Varianti come gli Variational Autoencoders (VAE) [Autoencoder Variazionali] introducono livelli probabilistici che consentono proprietà generative, trovando applicazioni nella generazione di immagini e nel rilevamento di anomalie.\n\nBank, Dor, Noam Koenigstein, e Raja Giryes. 2023. «Autoencoders». Machine Learning for Data Science Handbook: Data Mining and Knowledge Discovery Handbook, 353–74.\nL’uso degli autoencoder può aiutare nella trasmissione e nell’archiviazione efficiente dei dati, migliorando le prestazioni complessive dei sistemi embedded con risorse di calcolo e di memoria limitate.\n\n\nTransformer Network\nLe “Transformer network” [reti di trasformatori] sono emerse come un’architettura potente, specialmente nell’elaborazione del linguaggio naturale (Vaswani et al. 2017). Queste reti utilizzano meccanismi di auto-attenzione per soppesare l’influenza di diverse parole di input su ogni parola di output, consentendo il calcolo parallelo e catturando pattern intricati nei dati. Le reti di trasformatori hanno portato a risultati all’avanguardia in attività come la traduzione linguistica, la sintesi e la generazione di testo.\n\nVaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, e Illia Polosukhin. 2017. «Attention is all you need». Adv Neural Inf Process Syst 30.\nQueste reti possono essere ottimizzate per eseguire attività correlate alla lingua direttamente sul dispositivo. Ad esempio, i trasformatori possono essere utilizzati nei sistemi embedded per servizi di traduzione in tempo reale o interfacce assistite dalla voce, dove latenza ed efficienza computazionale sono cruciali. Tecniche come la distillazione del modello possono essere impiegate per distribuire queste reti su dispositivi embedded con risorse limitate.\nQueste architetture servono a scopi specifici ed eccellono in diversi domini, offrendo un ricco toolkit per affrontare diversi problemi nei sistemi di intelligenza artificiale embedded. Comprendere le sfumature di queste architetture è fondamentale nella progettazione di modelli di deep learning efficaci ed efficienti per varie applicazioni.\n\n\n\n3.2.5 ML Tradizionale vs Deep Learning\nIl deep learning estende il machine learning tradizionale utilizzando reti neurali per discernere i pattern nei dati. Al contrario, il machine learning tradizionale si basa su un set di algoritmi consolidati come alberi decisionali, k-nearest neighbor e macchine a vettori di supporto, ma non coinvolge le reti neurali. Figura 3.8 fornisce un confronto visivo tra Machine Learning e Deep Learning, evidenziandone le principali differenze di approccio e capacità.\n\n\n\n\n\n\nFigura 3.8: Confronto tra Machine Learning e Deep Learning. Fonte: Medium\n\n\n\nCome mostrato nella figura, i modelli di deep learning possono elaborare dati grezzi direttamente ed estrarre automaticamente le feature rilevanti, mentre il machine learning tradizionale richiede spesso l’ingegneria manuale delle feature. La figura illustra anche come i modelli di deep learning possono gestire attività più complesse e set di dati più grandi rispetto ai tradizionali approcci di machine learning.\nPer evidenziare ulteriormente le differenze, Tabella 3.1 fornisce un confronto più dettagliato delle caratteristiche contrastanti tra ML tradizionale e deep learning. Questa tabella integra la rappresentazione visiva in Figura 3.8 offrendo punti di confronto specifici tra vari aspetti di questi due approcci.\n\n\n\nTabella 3.1: Confronto tra machine learning tradizionale e deep learning.\n\n\n\n\n\n\n\n\n\n\nAspetto\nML tradizionale\nDeep Learning\n\n\n\n\nRequisiti dei dati\nDa basso a moderato (efficiente con set di dati più piccoli)\nAlto (richiede set di dati di grandi dimensioni per un apprendimento adeguato)\n\n\nComplessità del modello\nModerata (adatta a problemi ben definiti)\nAlta (rileva pattern intricati, adatta a compiti complessi)\n\n\nRisorse di calcolo\nDa basse a moderate (economiche, meno dispendiose in termini di risorse)\nAlta (richiede una potenza di calcolo e risorse sostanziali)\n\n\nVelocità di distribuzione\nVeloce (cicli di training e distribuzione più rapidi)\nLento (tempi di training più lunghi, in particolare con set di dati più grandi)\n\n\nInterpretabilità\nAlta (chiare intuizioni sui percorsi decisionali)\nBassa (strutture complesse a layer, natura “scatola nera”)\n\n\nManutenzione\nPiù facile (semplice da aggiornare e mantenere)\nComplesso (richiede più sforzi nella manutenzione e negli aggiornamenti)\n\n\n\n\n\n\n\n\n3.2.6 Scelta tra ML tradizionale e DL\n\nDisponibilità e Volume dei Dati\nQuantità di Dati: Gli algoritmi di machine learning tradizionali, come gli alberi decisionali o Naive Bayes, sono spesso più adatti quando la disponibilità dei dati è limitata. Offrono previsioni affidabili anche con set di dati più piccoli. Ciò è particolarmente vero nella diagnostica medica per la previsione delle malattie e nella segmentazione dei clienti nel marketing.\nDiversità e Qualità dei Dati: Gli algoritmi di machine learning tradizionali spesso funzionano bene con dati strutturati (l’input del modello è un set di funzionalità, idealmente indipendenti l’una dall’altra) ma possono richiedere un notevole sforzo di pre-elaborazione (ad esempio, la “feature engineering” [progettazione delle funzionalità]). D’altro canto, il deep learning adotta l’approccio di eseguire automaticamente la progettazione delle funzionalità come parte dell’architettura del modello. Questo approccio consente la costruzione di modelli end-to-end in grado di mappare direttamente da dati di input non strutturati (come testo, audio e immagini) all’output desiderato senza fare affidamento su euristiche semplicistiche con efficacia limitata. Tuttavia, ciò si traduce in modelli più grandi che richiedono più dati e risorse computazionali. Nei dati rumorosi, la necessità di set di dati più grandi è ulteriormente enfatizzata quando si utilizza il Deep Learning.\n\n\nComplessità del Problema\nGranularità del Problema: I problemi che sono semplici o moderatamente complessi, che possono coinvolgere relazioni lineari o polinomiali tra variabili, spesso trovano una migliore aderenza ai metodi tradizionali di apprendimento automatico.\nRappresentazione Gerarchica delle Feature: I modelli di deep learning sono eccellenti in attività che richiedono una rappresentazione gerarchica delle feature [caratteristiche], come il riconoscimento di immagini e voce. Tuttavia, non tutti i problemi richiedono questa complessità e gli algoritmi tradizionali di apprendimento automatico possono talvolta offrire soluzioni più semplici e ugualmente efficaci.\n\n\nRisorse Hardware e Computazionali\nVincoli di Risorse: La disponibilità di risorse computazionali spesso influenza la scelta tra ML tradizionale e deep learning. Il primo è generalmente meno dispendioso in termini di risorse e quindi preferibile in ambienti con limitazioni hardware o vincoli di budget.\nScalabilità e Velocità: Gli algoritmi tradizionali di apprendimento automatico, come le Support Vector Machines (SVM) [macchine a vettori di supporto ], spesso consentono tempi di training più rapidi e una scalabilità più semplice, il che è particolarmente vantaggioso nei progetti con tempistiche ristrette e volumi di dati in crescita.\n\n\nNormativa di Conformità\nLa conformità normativa è fondamentale in vari settori, e richiede l’aderenza a linee guida e “best practice” come il General Data Protection Regulation (GDPR) [Regolamento generale sulla protezione dei dati] nell’UE. I modelli ML tradizionali, grazie alla loro intrinseca interpretabilità, spesso si allineano meglio a queste normative, soprattutto in settori come la finanza e l’assistenza sanitaria.\n\n\nInterpretabilità\nComprendere il processo decisionale è più facile con le tecniche tradizionali di apprendimento automatico rispetto ai modelli di deep learning, che funzionano come “scatole nere”, rendendo difficile tracciare i percorsi decisionali.\n\n\n\n3.2.7 Fare una Scelta Informata\nConsiderati i vincoli dei sistemi di intelligenza artificiale embedded, comprendere le differenze tra le tecniche di ML tradizionali e il deep learning diventa essenziale. Entrambe le strade offrono vantaggi unici e le loro caratteristiche distintive spesso determinano la scelta dell’una rispetto all’altra in diversi scenari.\nNonostante ciò, il deep learning ha costantemente superato i metodi tradizionali di apprendimento automatico in diverse aree chiave grazie all’abbondanza di dati, ai progressi computazionali e alla comprovata efficacia in attività complesse. Ecco alcuni motivi specifici per cui ci concentriamo sul deep learning:\n\nPrestazioni Superiori in Attività Complesse: I modelli di deep learning, in particolare le reti neurali profonde, eccellono in attività in cui le relazioni tra i punti dati sono incredibilmente intricate. Attività come il riconoscimento di immagini e parlato, la traduzione linguistica e la riproduzione di giochi complessi come Go e Scacchi hanno visto progressi significativi principalmente attraverso algoritmi di deep learning.\nGestione Efficiente dei Dati non Strutturati: A differenza dei metodi tradizionali di apprendimento automatico, il deep learning può elaborare in modo più efficace i dati non strutturati. Ciò è fondamentale nel panorama dei dati odierno, in cui la stragrande maggioranza dei dati, come testo, immagini e video, non è strutturata.\nSfruttamento dei Big Data: Con la disponibilità dei Big Data, i modelli di deep learning possono apprendere e migliorare continuamente. Questi modelli eccellono nell’utilizzare grandi set di dati per migliorare la loro accuratezza predittiva, un limite degli approcci tradizionali di machine-learning.\nProgressi Hardware e Calcolo Parallelo: L’avvento di potenti GPU e la disponibilità di piattaforme di cloud computing hanno consentito il rapido training di modelli di deep learning. Questi progressi hanno affrontato una delle sfide significative del deep learning: la necessità di risorse computazionali sostanziali.\nAdattabilità Dinamica e Apprendimento Continuo: I modelli di deep learning possono adattarsi dinamicamente a nuove informazioni o dati. Possono essere addestrati per generalizzare il loro apprendimento a nuovi dati mai visti, cruciali in campi in rapida evoluzione come la guida autonoma o la traduzione linguistica in tempo reale.\n\nSebbene il deep learning abbia guadagnato una notevole popolarità, è essenziale comprendere che il machine learning tradizionale è ancora rilevante. Man mano che ci addentriamo nei meandri del deep learning, evidenzieremo anche le situazioni in cui i metodi tradizionali di machine learning potrebbero essere più appropriati, grazie alla loro semplicità, efficienza e interpretabilità. Concentrandoci in questo testo sul deep learning, intendiamo fornire ai lettori le conoscenze e gli strumenti per affrontare problemi moderni e complessi in vari ambiti, fornendo al contempo approfondimenti sui vantaggi comparativi e sugli scenari applicativi appropriati per il deep learning e le tecniche tradizionali di machine learning.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Avvio al Deep Learning</span>"
    ]
  },
  {
    "objectID": "contents/core/dl_primer/dl_primer.it.html#conclusione",
    "href": "contents/core/dl_primer/dl_primer.it.html#conclusione",
    "title": "3  Avvio al Deep Learning",
    "section": "3.3 Conclusione",
    "text": "3.3 Conclusione\nIl deep learning è diventato un potente set di tecniche per affrontare le complesse sfide del riconoscimento di pattern e della previsione. Iniziando con una panoramica, abbiamo delineato i concetti e i principi fondamentali che governano il deep learning, gettando le basi per studi più avanzati.\nAl centro del deep learning, abbiamo esplorato le idee di base delle reti neurali, potenti modelli computazionali ispirati alla struttura neuronale interconnessa del cervello umano. Questa esplorazione ci ha permesso di apprezzare le capacità e il potenziale delle reti neurali nella creazione di algoritmi sofisticati in grado di apprendere e adattarsi dai dati.\nComprendere il ruolo delle librerie e dei framework è stata una parte fondamentale della nostra discussione. Abbiamo offerto approfondimenti sugli strumenti che possono facilitare lo sviluppo e l’implementazione di modelli di deep learning. Queste risorse semplificano l’implementazione delle reti neurali e aprono strade all’innovazione e all’ottimizzazione.\nSuccessivamente, abbiamo affrontato le sfide che si potrebbero incontrare quando si racchiudono algoritmi di deep learning nei sistemi embedded, fornendo una prospettiva critica sulle complessità e sulle considerazioni relative all’introduzione dell’intelligenza artificiale nei dispositivi edge.\nInoltre, abbiamo esaminato i limiti del deep learning. Attraverso le discussioni, abbiamo svelato le sfide affrontate nelle applicazioni del deep learning e delineato scenari in cui l’apprendimento automatico tradizionale potrebbe superare il deep learning. Queste sezioni sono fondamentali per promuovere una visione equilibrata delle capacità e dei limiti del deep learning.\nIn questo “Avviamento”, abbiamo fornito le conoscenze per fare scelte informate tra l’implementazione dell’apprendimento automatico tradizionale o delle tecniche di deep learning, a seconda delle esigenze e dei vincoli unici di un problema specifico.\nConcludendo questo capitolo, ci auguriamo che sia stato acquisito il “linguaggio” di base del deep learning e si sia pronti ad approfondire i capitoli successivi con una solida comprensione e una prospettiva critica. Il viaggio che è pieno di entusiasmanti opportunità e sfide nel racchiudere l’intelligenza artificiale nei sistemi.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Avvio al Deep Learning</span>"
    ]
  },
  {
    "objectID": "contents/core/dl_primer/dl_primer.it.html#sec-deep-learning-primer-resource",
    "href": "contents/core/dl_primer/dl_primer.it.html#sec-deep-learning-primer-resource",
    "title": "3  Avvio al Deep Learning",
    "section": "3.4 Risorse",
    "text": "3.4 Risorse\nEcco un elenco curato di risorse per supportare studenti e insegnanti nei loro percorsi di apprendimento e insegnamento. Stiamo lavorando costantemente per espandere questa raccolta e presto aggiungeremo nuovi esercizi.\n\n\n\n\n\n\nSlide\n\n\n\n\n\nQueste slide sono uno strumento prezioso per gli insegnanti per tenere lezioni e per gli studenti per rivedere il materiale secondo il proprio ritmo. Incoraggiamo studenti e docenti a sfruttare queste slide per migliorare la loro comprensione e facilitare un trasferimento efficace delle conoscenze.\n\nPast, Present, and Future of ML.\nThinking About Loss.\nMinimizing Loss.\nFirst Neural Network.\nUnderstanding Neurons.\nIntro to CLassification.\nTraining, Validation, and Test Data.\nIntro to Convolutions.\n\n\n\n\n\n\n\n\n\n\nVideo\n\n\n\n\n\n\nVideo 3.3\nVideo 3.1\nVideo 3.2\n\n\n\n\n\n\n\n\n\n\nEsercizi\n\n\n\n\n\nPer rafforzare i concetti trattati in questo capitolo, abbiamo curato una serie di esercizi che sfidano gli studenti ad applicare le proprie conoscenze e ad approfondire la propria comprensione.\n\nEsercizio 3.1\nEsercizio 3.2",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Avvio al Deep Learning</span>"
    ]
  },
  {
    "objectID": "contents/core/workflow/workflow.it.html",
    "href": "contents/core/workflow/workflow.it.html",
    "title": "4  Workflow dell’IA",
    "section": "",
    "text": "4.1 Panoramica\nFigura 4.1 illustra il flusso di lavoro sistematico necessario per sviluppare un modello di machine learning di successo. Questo processo end-to-end, comunemente denominato ciclo di vita del machine learning, consente di creare, distribuire e gestire i modelli in modo efficace. Solitamente comporta i seguenti passaggi chiave:\nSeguire questo flusso di lavoro ML strutturato ci guida attraverso le fasi chiave dello sviluppo. Garantisce di creare modelli efficaci e robusti pronti per la distribuzione nel mondo reale, con conseguenti modelli di qualità superiore che risolvono le varie esigenze.\nIl flusso di lavoro ML è iterativo, richiede un monitoraggio continuo e potenziali aggiustamenti. Ulteriori considerazioni includono:",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Workflow dell'IA</span>"
    ]
  },
  {
    "objectID": "contents/core/workflow/workflow.it.html#panoramica",
    "href": "contents/core/workflow/workflow.it.html#panoramica",
    "title": "4  Workflow dell’IA",
    "section": "",
    "text": "Figura 4.1: Metodologia di progettazione multi-step per lo sviluppo di un modello di machine learning. Comunemente denominato ciclo di vita del machine learning\n\n\n\n\n\nDefinizione del Problema - Si inizia articolando chiaramente il problema specifico da risolvere. Questo si concentra sui problemi durante la raccolta dati e la creazione del modello.\nRaccolta e Preparazione dei Dati: Raccogliere dati di training pertinenti e di alta qualità che catturino tutti gli aspetti del problema. Pulire e pre-elaborare i dati per prepararli alla modellazione.\nSelezione e Training del Modello: Scegliere un algoritmo di apprendimento automatico adatto al tipo di problema e ai dati. Considerare i pro e i contro dei diversi approcci. Inserire i dati preparati nel modello per addestrarlo. Il tempo di addestramento varia in base alle dimensioni dei dati e alla complessità del modello.\nValutazione del Modello: Testare il modello addestrato su nuovi dati non ancora esaminati per misurarne l’accuratezza predittiva. Identificare eventuali limitazioni.\nDistribuzione del Modello: Integrare il modello convalidato in applicazioni o sistemi per avviarne l’operatività.\nMonitoraggio e Manutenzione: Tenere traccia delle prestazioni del modello in produzione. Ri-addestrare periodicamente su nuovi dati per mantenerli aggiornati.\n\n\n\n\nControllo della Versione: Tenere traccia delle modifiche al codice e ai dati per riprodurre i risultati e ripristinare le versioni precedenti se necessario.\nDocumentazione: Mantenere una documentazione dettagliata per la comprensione e la riproduzione del flusso di lavoro.\nTest: Testare rigorosamente il flusso di lavoro per garantirne la funzionalità.\nSicurezza: Proteggere il flusso di lavoro e i dati quando si distribuiscono modelli in contesti di produzione.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Workflow dell'IA</span>"
    ]
  },
  {
    "objectID": "contents/core/workflow/workflow.it.html#ia-tradizionale-o-embedded",
    "href": "contents/core/workflow/workflow.it.html#ia-tradizionale-o-embedded",
    "title": "4  Workflow dell’IA",
    "section": "4.2 IA Tradizionale o Embedded",
    "text": "4.2 IA Tradizionale o Embedded\nIl flusso di lavoro ML è una guida universale applicabile su diverse piattaforme, tra cui soluzioni basate su cloud, edge computing e TinyML. Tuttavia, il flusso di lavoro per l’IA Embedded introduce complessità e sfide uniche, rendendolo un dominio accattivante e aprendo la strada a innovazioni straordinarie. Figura 4.2 illustra le differenze tra Machine Learning e Deep Learning.\n\n\n\n\n\n\nFigura 4.2: Confronto tra Machine Learning tradizionale e Deep Learning. Fonte: BBN Times\n\n\n\nFigura 4.3 illustra gli utilizzi dell’intelligenza artificiale embedded in vari settori.\n\n\n\n\n\n\nFigura 4.3: Applicazioni di IA Embedded. Fonte: Rinf.tech\n\n\n\n\n4.2.1 Ottimizzazione delle Risorse\n\nFlusso di Lavoro ML Tradizionale: Questo workflow dà priorità all’accuratezza e alle prestazioni del modello, spesso sfruttando abbondanti risorse di calcolo in ambienti cloud o data center.\nFlusso di Lavoro IA Embedded: Dati i vincoli di risorse dei sistemi embedded, questo flusso di lavoro richiede un’attenta pianificazione per ottimizzare le dimensioni del modello e le richieste di calcolo. Tecniche come la quantizzazione e il pruning [potatura] del modello sono fondamentali.\n\n\n\n4.2.2 Elaborazione in Real-time\n\nFlusso di Lavoro ML Tradizionale: Meno enfasi sull’elaborazione in tempo reale, spesso basata sull’elaborazione di dati in batch.\nFlusso di Lavoro IA Embedded: Dà priorità all’elaborazione dei dati in tempo reale, rendendo essenziali bassa latenza ed esecuzione rapida, soprattutto in applicazioni come veicoli autonomi e automazione industriale.\n\n\n\n4.2.3 Gestione dei Dati e Privacy\n\nFlusso di Lavoro ML Tradizionale: Elabora i dati in posizioni centralizzate, spesso richiedendo un ampio trasferimento di dati e concentrandosi sulla sicurezza dei dati durante il transito e l’archiviazione.\nFlusso di Lavoro IA Embedded: Questo workflow sfrutta l’edge computing per elaborare i dati più vicino alla fonte, riducendo la trasmissione dei dati e migliorando la privacy tramite la localizzazione dei dati.\n\n\n\n4.2.4 Integrazione Hardware-Software\n\nFlusso di Lavoro ML Tradizionale: In genere funziona su hardware generico, con sviluppo di software indipendente.\nFlusso di Lavoro IA Embedded: Questo flusso di lavoro prevede un approccio più integrato allo sviluppo hardware e software, spesso incorporando chip personalizzati o acceleratori hardware per ottenere prestazioni ottimali.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Workflow dell'IA</span>"
    ]
  },
  {
    "objectID": "contents/core/workflow/workflow.it.html#ruoli-e-responsabilità",
    "href": "contents/core/workflow/workflow.it.html#ruoli-e-responsabilità",
    "title": "4  Workflow dell’IA",
    "section": "4.3 Ruoli e responsabilità",
    "text": "4.3 Ruoli e responsabilità\nLa creazione di una soluzione ML, in particolare per l’intelligenza artificiale embedded, è uno sforzo multidisciplinare che coinvolge vari specialisti. A differenza dello sviluppo software tradizionale, la creazione di una soluzione ML richiede un approccio multidisciplinare a causa della natura sperimentale dello sviluppo del modello e dei requisiti ad alta intensità di risorse per il training e l’implementazione di questi modelli.\nC’è una forte necessità di ruoli incentrati sui dati per il successo delle pipeline di apprendimento automatico. Gli scienziati dei dati e gli ingegneri dei dati gestiscono la raccolta dei dati, creano pipeline di dati e ne garantiscono la qualità. Poiché la natura dei modelli di apprendimento automatico dipende dai dati che consumano, i modelli sono unici e variano a seconda delle diverse applicazioni, il che richiede un’ampia sperimentazione. I ricercatori e gli ingegneri di apprendimento automatico guidano questa fase sperimentale attraverso test continui, convalida e iterazione per ottenere prestazioni ottimali.\nLa fase di implementazione richiede spesso hardware e infrastrutture specializzati, poiché i modelli di machine learning possono essere ad alta intensità di risorse, richiedendo un’elevata potenza di calcolo e una gestione efficiente delle risorse. Ciò richiede la collaborazione con gli ingegneri hardware per garantire che l’infrastruttura possa supportare le esigenze computazionali di training e inferenza del modello.\nPoiché i modelli prendono decisioni che possono avere un impatto sugli individui e sulla società, gli aspetti etici e legali dell’apprendimento automatico stanno diventando sempre più importanti. Sono necessari esperti di etica e consulenti legali per garantire la conformità agli standard etici e alle normative legali.\nComprendere i vari ruoli coinvolti in un progetto ML è fondamentale per il suo completamento con successo. Tabella 4.1 fornisce una panoramica generale di questi ruoli tipici, anche se è importante notare che i confini tra loro a volte possono essere confusi. Esaminiamo questa ripartizione:\n\n\n\nTabella 4.1: Ruoli e responsabilità delle persone coinvolte in Operazioni di ML.\n\n\n\n\n\n\n\n\n\nRuolo\nResponsabilità\n\n\n\n\nProject Manager\nSupervisiona il progetto, assicurando che le tempistiche e le milestone siano rispettate.\n\n\nEsperti di Dominio\nOffrono approfondimenti specifici del dominio per definire i requisiti del progetto.\n\n\nData Scientist\nSpecializzati nell’analisi dei dati e nello sviluppo di modelli.\n\n\nIngegneri di Apprendimento Automatico\nConcentrati sullo sviluppo e l’implementazione del modello.\n\n\nData Scientist\nSpecializzati nell’analisi dei dati e nello sviluppo di modelli.\n\n\nEmbedded Systems Engineer\nIntegra modelli ML in sistemi embedded.\n\n\nSoftware Developer\nSviluppa componenti software per l’integrazione del sistema IA.\n\n\nHardware Engineer\nProgetta e ottimizza l’hardware per il sistema AI embedded.\n\n\nUI/UX Designer\nConcentrato sulla progettazione incentrata sull’utente.\n\n\nQA Engineer\nAssicura che il sistema soddisfi gli standard di qualità.\n\n\nEticisti e Consulenti Legali\nConsulenti sulla conformità etica e legale.\n\n\nPersonale Operativo e di Manutenzione\nMonitora e mantiene il sistema distribuito.\n\n\nSpecialisti della sicurezza\nGarantiscono la sicurezza del sistema.\n\n\n\n\n\n\nQuesta visione olistica facilita una collaborazione senza soluzione di continuità e alimenta un ambiente maturo per innovazione e scoperte. Man mano che procederemo nei prossimi capitoli, esploreremo l’essenza e le competenze di ciascun ruolo e favoriremo una comprensione più profonda delle complessità coinvolte nei progetti di intelligenza artificiale. Per una discussione più dettagliata degli strumenti e delle tecniche specifici utilizzati da questi ruoli, nonché per un’analisi approfondita delle loro responsabilità, fare riferimento a Sezione 13.5.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Workflow dell'IA</span>"
    ]
  },
  {
    "objectID": "contents/core/workflow/workflow.it.html#conclusione",
    "href": "contents/core/workflow/workflow.it.html#conclusione",
    "title": "4  Workflow dell’IA",
    "section": "4.4 Conclusione",
    "text": "4.4 Conclusione\nQuesto capitolo ha gettato le basi per comprendere il flusso di lavoro dell’apprendimento automatico, un approccio strutturato fondamentale per lo sviluppo, l’implementazione e la manutenzione dei modelli ML. Abbiamo esplorato le sfide uniche affrontate nei flussi di lavoro ML, dove l’ottimizzazione delle risorse, l’elaborazione in tempo reale, la gestione dei dati e l’integrazione hardware-software sono fondamentali. Queste distinzioni sottolineano l’importanza di adattare i flussi di lavoro per soddisfare le esigenze specifiche dell’ambiente applicativo.\nInoltre, abbiamo sottolineato l’importanza della collaborazione multidisciplinare nei progetti ML. Esaminando i diversi ruoli coinvolti, dai data scientist agli ingegneri del software, abbiamo ottenuto una panoramica del lavoro di squadra necessario per affrontare la natura sperimentale e ad alta intensità di risorse dello sviluppo ML. Questa comprensione è fondamentale per promuovere una comunicazione e una collaborazione efficaci tra diversi settori di competenza.\nMentre passiamo a discussioni più dettagliate nei capitoli successivi, questa panoramica di alto livello ci fornisce una prospettiva olistica sul flusso di lavoro ML e sui vari ruoli coinvolti. Queste basi si riveleranno importanti quando approfondiremo aspetti specifici dell’apprendimento automatico, che ci consentiranno di contestualizzare concetti avanzati nel quadro più ampio dello sviluppo e dell’implementazione del machine learning.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Workflow dell'IA</span>"
    ]
  },
  {
    "objectID": "contents/core/workflow/workflow.it.html#sec-ai-workflow-resource",
    "href": "contents/core/workflow/workflow.it.html#sec-ai-workflow-resource",
    "title": "4  Workflow dell’IA",
    "section": "4.5 Risorse",
    "text": "4.5 Risorse\nEcco un elenco curato di risorse per supportare studenti e insegnanti nei loro percorsi di apprendimento e insegnamento. Lavoriamo continuamente per espandere questa raccolta e presto aggiungeremo nuovi esercizi.\n\n\n\n\n\n\nSlide\n\n\n\n\n\nQueste slide sono uno strumento prezioso per gli insegnanti per tenere lezioni e per gli studenti per rivedere il materiale secondo il proprio ritmo. Incoraggiamo studenti e docenti a sfruttare queste slide per migliorare la loro comprensione e facilitare un trasferimento efficace delle conoscenze.\n\nML Workflow.\nML Lifecycle.\n\n\n\n\n\n\n\n\n\n\nVideo\n\n\n\n\n\n\nProssimamente.\n\n\n\n\n\n\n\n\n\n\nEsercizi\n\n\n\n\n\nPer rafforzare i concetti trattati in questo capitolo, abbiamo curato una serie di esercizi che sfidano gli studenti ad applicare le proprie conoscenze e ad approfondire la propria comprensione.\n\nProssimamente.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Workflow dell'IA</span>"
    ]
  },
  {
    "objectID": "contents/core/data_engineering/data_engineering.it.html",
    "href": "contents/core/data_engineering/data_engineering.it.html",
    "title": "5  Data Engineering",
    "section": "",
    "text": "5.1 Panoramica\nSi immagini un mondo in cui l’intelligenza artificiale può diagnosticare malattie con una precisione senza precedenti, ma solo se i dati utilizzati per addestrarla sono imparziali e affidabili. È qui che entra in gioco il “data engineering” [ingegneria dei dati]. Sebbene oltre il 90% dei dati mondiali sia stato creato negli ultimi due decenni, questa enorme quantità di informazioni è utile solo per creare modelli di intelligenza artificiale efficaci con un’elaborazione e una preparazione adeguate. L’ingegneria dei dati colma questa lacuna trasformando i dati grezzi in un formato di alta qualità che alimenta l’innovazione dell’intelligenza artificiale. Nel mondo odierno basato sui dati, proteggere la privacy degli utenti è fondamentale. Che siano obbligatorie per legge o guidate dalle preoccupazioni degli utenti, le tecniche di anonimizzazione come la privacy differenziale e l’aggregazione sono fondamentali per mitigare i rischi per la privacy. Tuttavia, un’implementazione attenta è fondamentale per garantire che questi metodi non compromettano l’utilità dei dati. I creatori di set di dati affrontano complesse sfide di privacy e rappresentazione quando creano dati di addestramento di alta qualità, in particolare per domini sensibili come l’assistenza sanitaria. Dal punto di vista legale, i creatori potrebbero dover rimuovere identificatori diretti come nomi ed età. Anche senza obblighi legali, la rimozione di tali informazioni può aiutare a creare fiducia negli utenti. Tuttavia, un’eccessiva anonimizzazione può compromettere l’utilità del set di dati. Tecniche come la privacy differenziale\\(^{1}\\), l’aggregazione e la riduzione dei dettagli forniscono alternative per bilanciare privacy e utilità, ma hanno degli svantaggi. I creatori devono trovare un equilibrio ponderato in base al caso d’uso.\nSebbene la privacy sia fondamentale, garantire modelli di intelligenza artificiale equi e solidi richiede di affrontare le lacune (gap) della rappresentazione nei dati. È fondamentale ma non sufficiente garantire la diversità tra variabili individuali come genere, razza e accento. Queste combinazioni, a volte chiamate lacune (gap) di ordine superiore, possono influire in modo significativo sulle prestazioni del modello. Ad esempio, un set di dati medico potrebbe avere dati bilanciati su genere, età e diagnosi individualmente, ma non ha abbastanza casi per catturare donne anziane con una condizione specifica. Tali higher-order gaps [lacune di ordine superiore] non sono immediatamente evidenti, ma possono influire in modo critico sulle prestazioni del modello.\nLa creazione di dati di training utili ed etici richiede una considerazione globale dei rischi per la privacy e delle lacune di rappresentazione. Le soluzioni perfette elusive necessitano di pratiche di ingegneria dei dati coscienziose come l’anonimizzazione, l’aggregazione, il sotto-campionamento di gruppi sovrarappresentati e la generazione di dati sintetizzati per bilanciare esigenze contrastanti. Ciò facilita modelli che sono sia accurati che socialmente responsabili. La collaborazione interfunzionale e i controlli esterni possono anche rafforzare i dati di training. Le sfide sono molteplici ma superabili con uno sforzo ponderato.\nIniziamo discutendo della raccolta dati: Dove reperiamo i dati e come li raccogliamo? Le opzioni spaziano dall’estrazione di dati dal web, all’accesso alle API e all’utilizzo di sensori e dispositivi IoT, fino alla conduzione di sondaggi e alla raccolta di input dagli utenti. Questi metodi riflettono pratiche del mondo reale. Successivamente, approfondiremo l’etichettatura dei dati, tenendo conto anche del coinvolgimento umano. Discuteremo i compromessi e le limitazioni dell’etichettatura umana ed esploreremo i metodi emergenti per l’etichettatura automatizzata. Successivamente, affronteremo la pulizia e la preelaborazione dei dati, un passaggio cruciale ma spesso sottovalutato nella preparazione dei dati grezzi per l’addestramento del modello di intelligenza artificiale. Segue l’aumento dei dati, una strategia per migliorare set di dati limitati generando campioni sintetici. Ciò è particolarmente pertinente per i sistemi embedded, poiché molti casi d’uso necessitano di ampi repository di dati prontamente disponibili per la cura [https://it.wikipedia.org/wiki/Data_curation]. La generazione di dati sintetici emerge come un’alternativa praticabile con vantaggi e svantaggi. Parleremo anche del versioning del dataset, sottolineando l’importanza di tracciare le modifiche dei dati nel tempo. I dati sono in continua evoluzione; quindi, è fondamentale ideare strategie per gestire e archiviare dataset espansivi. Alla fine di questa sezione, si avrà una comprensione completa dell’intera pipeline di dati, dalla raccolta all’archiviazione, essenziale per rendere operativi i sistemi di intelligenza artificiale. Intraprendiamo questo viaggio!",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Engineering</span>"
    ]
  },
  {
    "objectID": "contents/core/data_engineering/data_engineering.it.html#definizione-del-problema",
    "href": "contents/core/data_engineering/data_engineering.it.html#definizione-del-problema",
    "title": "5  Data Engineering",
    "section": "5.2 Definizione del Problema",
    "text": "5.2 Definizione del Problema\nIn molti domini di machine learning, algoritmi sofisticati sono al centro dell’attenzione, mentre l’importanza fondamentale della qualità dei dati viene spesso trascurata. Questa negligenza dà origine a “Data Cascades” di Sambasivan et al. (2021), eventi in cui le lacune nella qualità dei dati si sommano, portando a conseguenze negative a valle come previsioni errate, cessazioni di progetti e persino potenziali danni alle comunità.\nFigura 5.1 illustra queste potenziali insidie nei dati in ogni fase e come influenzano l’intero processo lungo la linea. L’influenza degli errori nella raccolta dei dati è particolarmente pronunciata. Come illustrato nella figura, qualsiasi lacuna in questa fase iniziale diventerà evidente nelle fasi successive (nella valutazione e nell’implementazione del modello) e potrebbe portare a conseguenze costose, come l’abbandono dell’intero modello e il riavvio da zero. Pertanto, investire in tecniche di ingegneria dei dati fin dall’inizio ci aiuterà a rilevare gli errori in anticipo, mitigando gli effetti a cascata illustrati nella figura.\n\n\n\n\n\n\nFigura 5.1: Data cascades: costi composti. Fonte: Sambasivan et al. (2021).\n\n\nSambasivan, Nithya, Shivani Kapania, Hannah Highfill, Diana Akrong, Praveen Paritosh, e Lora M Aroyo. 2021. «“Everyone wants to do the model work, not the data work”: Data Cascades in High-Stakes AI». In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems, 1–15.\n\n\nNonostante molti professionisti del ML riconoscano l’importanza dei dati, altri segnalano di dover affrontare queste “cascate”. Ciò evidenzia un problema sistemico: mentre il fascino dello sviluppo di modelli avanzati rimane, i dati spesso devono essere maggiormente apprezzati.\nKeyword Spotting (KWS) fornisce un esempio eccellente di TinyML in azione, come illustrato in Figura 5.2. Questa tecnologia è fondamentale per le interfacce abilitate alla voce su dispositivi endpoint come gli smartphone. In genere funzionando come motori di wake-word leggeri, i sistemi KWS sono costantemente attivi, in ascolto di una frase specifica per attivare ulteriori azioni. Come illustrato nella figura, quando diciamo “OK, Google” o “Alexa”, questo avvia un processo su un microcontrollore embedded nel dispositivo. Nonostante le loro risorse limitate, questi microcontrollori svolgono un ruolo importante nel consentire interazioni vocali senza interruzioni con i dispositivi, spesso operando in ambienti con elevato rumore ambientale. L’unicità della wake-word, come mostrato nella figura, aiuta a ridurre al minimo i falsi positivi, assicurando che il sistema non venga attivato inavvertitamente.\n\n\n\n\n\n\nFigura 5.2: Esempio di individuazione delle “Keyword Spotting”: interazione con Alexa. Fonte: Amazon.\n\n\n\nÈ importante comprendere che queste tecnologie di individuazione delle “parole chiave” non sono isolate; si integrano perfettamente in sistemi più grandi, elaborando segnali in modo continuo e gestendo al contempo un basso consumo energetico. Questi sistemi vanno oltre il semplice riconoscimento delle parole chiave, evolvendosi per facilitare diversi rilevamenti di suoni, come la rottura di un vetro. Questa evoluzione è orientata alla creazione di dispositivi intelligenti in grado di comprendere e rispondere ai comandi vocali, annunciando un futuro in cui anche gli elettrodomestici possono essere controllati tramite interazioni vocali.\nCreare un modello KWS affidabile è un compito complesso. Richiede una profonda comprensione dello scenario di distribuzione, che comprenda dove e come funzioneranno questi dispositivi. Ad esempio, l’efficacia di un modello KWS non riguarda solo il riconoscimento di una parola; riguarda la sua distinzione tra vari accenti e rumori di sottofondo, che si tratti di un bar affollato o del suono stridulo di una televisione in un soggiorno o in una cucina dove questi dispositivi sono comunemente presenti. Riguarda la garanzia che un sussurrato “Alexa” nel cuore della notte o un urlato “OK Google” in un mercato rumoroso vengano riconosciuti con la stessa precisione.\nInoltre, molti degli attuali assistenti vocali KWS supportano un numero limitato di lingue, lasciando una parte sostanziale della diversità linguistica mondiale non rappresentata. Questa limitazione è in parte dovuta alla difficoltà di raccogliere e monetizzare i dati per le lingue parlate da popolazioni più piccole. La distribuzione “long-tail” [https://it.wikipedia.org/wiki/Coda_lunga] delle lingue implica che molte lingue hanno dati limitati, rendendo difficile lo sviluppo di tecnologie di supporto.\nQuesto livello di accuratezza e robustezza dipende dalla disponibilità e dalla qualità dei dati, dalla capacità di etichettare correttamente i dati e dalla trasparenza dei dati per l’utente finale prima che vengano utilizzati per addestrare il modello. Tuttavia, tutto inizia con una chiara comprensione della dichiarazione o definizione del problema.\nIn genere, in ML, la definizione del problema ha alcuni passaggi chiave:\n\nIdentificare chiaramente la definizione del problema\nDefinire obiettivi chiari\nStabilire un benchmark [riferimento] di successo\nComprendere l’impegno/l’uso dell’utente finale\nComprendere i vincoli e le limitazioni dell’implementazione\nSeguito infine dalla raccolta dati.\n\nUna solida base di progetto è essenziale per la sua traiettoria e il suo successo finale. Al centro di questa base c’è innanzitutto l’identificazione di un problema chiaro, come garantire che i comandi vocali nei sistemi di assistenza vocale siano riconosciuti in modo coerente in diversi ambienti. Obiettivi chiari, come la creazione di set di dati rappresentativi per scenari diversi, forniscono una direzione unificata. I benchmark, come l’accuratezza del sistema nel rilevamento delle parole chiave, offrono risultati misurabili per valutare i progressi. Il coinvolgimento delle parti interessate, dagli utenti finali agli investitori, fornisce informazioni preziose e garantisce l’allineamento con le esigenze del mercato. Inoltre, quando si esplorano ambiti come l’assistenza vocale, è importante comprendere i limiti della piattaforma. I sistemi embedded, come i microcontrollori, sono dotati di limitazioni intrinseche di potenza di elaborazione, memoria ed efficienza energetica. Riconoscere queste limitazioni garantisce che le funzionalità, come il rilevamento delle parole chiave, siano personalizzate per funzionare in modo ottimale, bilanciando le prestazioni col risparmio delle risorse.\nIn questo contesto, usando KWS come esempio, possiamo suddividere ciascuno dei passaggi come segue:\n\nIdentificazione del Problema: In sostanza, KWS rileva parole chiave specifiche tra suoni ambientali e altre parole pronunciate. Il problema principale è progettare un sistema in grado di riconoscere queste parole chiave con elevata accuratezza, bassa latenza e minimi falsi positivi o negativi, soprattutto se distribuito su dispositivi con risorse di elaborazione limitate.\nImpostazione di Obiettivi Chiari: Gli obiettivi per un sistema KWS potrebbero includere:\n\nRaggiungimento di un tasso di accuratezza specifico (ad esempio, accuratezza del 98% nel rilevamento delle parole chiave).\nGaranzia di bassa latenza (ad esempio, rilevamento delle parole chiave e risposta entro 200 millisecondi).\nRiduzione al minimo del consumo di energia per estendere la durata della batteria sui dispositivi embedded.\nGaranzia che le dimensioni del modello siano ottimizzate per la memoria disponibile sul dispositivo.\n\nBenchmark per il successo: Stabilire metriche chiare per misurare il successo del sistema KWS. Questo potrebbe includere:\n\nTasso di Veri Positivi: La percentuale di parole chiave identificate correttamente.\nTasso di Falsi Positivi: La percentuale di parole chiave non identificate erroneamente come parole chiave.\nTempo di Risposta: Il tempo impiegato dall’enunciazione della parola chiave alla risposta del sistema.\nConsumo Energetico: Potenza media utilizzata durante il rilevamento della parola chiave.\n\nCoinvolgimento e Comprensione delle Parti Interessate:: Coinvolgere le parti interessate, tra cui produttori di dispositivi, sviluppatori di hardware e software e utenti finali. Comprendere le loro esigenze, capacità e vincoli. Ad esempio:\n\nI produttori di dispositivi potrebbero dare priorità al basso consumo energetico.\nGli sviluppatori di software potrebbero enfatizzare la facilità di integrazione.\nGli utenti finali darebbero priorità all’accuratezza e alla reattività.\n\nComprensione dei Vincoli e delle Limitazioni dei Sistemi Embedded: I dispositivi embedded presentano una serie di problematiche:\n\nLimiti della Memoria: I modelli KWS devono essere leggeri per adattarsi ai vincoli di memoria dei dispositivi embedded. In genere, i modelli KWS devono essere piccoli quanto 16 KB per adattarsi alla “isola always-on” [porzione sempre attiva] del SoC. Inoltre, questa è solo la dimensione del modello. Anche il codice applicativo aggiuntivo per la pre-elaborazione potrebbe dover rientrare nei vincoli di memoria.\nPotenza di Elaborazione: Le capacità di calcolo dei dispositivi embedded sono limitate (alcune centinaia di MHz di velocità di clock), quindi il modello KWS deve essere ottimizzato per l’efficienza.\nConsumo Energetico: Poiché molti dispositivi embedded sono alimentati a batteria, il sistema KWS deve essere efficiente dal punto di vista energetico.\nVincoli Ambientali: I dispositivi potrebbero essere distribuiti in vari ambienti, dalle silenziose camere da letto agli ambienti industriali rumorosi. Il sistema KWS deve essere sufficientemente robusto per funzionare efficacemente in questi scenari.\n\nRaccolta e Analisi dei Dati: Per un sistema KWS, la qualità e la diversità dei dati sono fondamentali. Le considerazioni potrebbero includere:\n\nVarietà di Accenti: Raccogliere dati da parlanti con accenti diversi per garantire un riconoscimento ad ampio raggio.\nRumori di Sottofondo: Includere campioni di dati con diversi rumori ambientali per addestrare il modello per scenari del mondo reale.\nVariazioni delle Parole Chiave: Le persone potrebbero pronunciare le parole chiave in modo diverso o avere leggere variazioni nella parola di attivazione stessa. Assicurarsi che il set di dati catturi queste sfumature.\n\nFeedback e Perfezionamento Iterativo: Una volta sviluppato un prototipo di sistema KWS, è fondamentale testarlo in scenari del mondo reale, raccogliere feedback e perfezionare iterativamente il modello. Ciò garantisce che il sistema rimanga allineato con il problema e gli obiettivi definiti. Ciò è importante perché gli scenari di distribuzione cambiano nel tempo man mano che le cose si evolvono.\n\n\n\n\n\n\n\nEsercizio 5.1: Keyword Spotting con TensorFlow Lite Micro\n\n\n\n\n\nEsplorare una guida pratica per la creazione e l’implementazione di sistemi Keyword Spotting utilizzando TensorFlow Lite Micro. Seguire i passaggi dalla raccolta dati all’addestramento del modello e all’implementazione nei microcontrollori. Imparare a creare modelli KWS efficienti che riconoscono parole chiave specifiche in mezzo al rumore di fondo. Perfetto per chi è interessato all’apprendimento automatico sui sistemi embedded. Sbloccare il potenziale dei dispositivi “voice-enabled” con TensorFlow Lite Micro!\n\n\n\n\nIl capitolo corrente sottolinea il ruolo essenziale della qualità dei dati nell’apprendimento automatico, utilizzando come esempio i sistemi Keyword Spotting. Descrive i passaggi chiave, dalla definizione del problema al coinvolgimento delle parti interessate, sottolineando il feedback iterativo. Il prossimo capitolo approfondirà la gestione della qualità dei dati, discutendone le conseguenze e le tendenze future, concentrandosi sull’importanza di dati diversificati e di alta qualità nello sviluppo di sistemi di intelligenza artificiale, affrontando considerazioni etiche e metodi di reperimento dei dati.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Engineering</span>"
    ]
  },
  {
    "objectID": "contents/core/data_engineering/data_engineering.it.html#ricerca-dei-dati.",
    "href": "contents/core/data_engineering/data_engineering.it.html#ricerca-dei-dati.",
    "title": "5  Data Engineering",
    "section": "5.3 Ricerca dei Dati.",
    "text": "5.3 Ricerca dei Dati.\nLa qualità e la diversità dei dati raccolti sono importanti per sviluppare sistemi di intelligenza artificiale accurati e robusti. Il reperimento di dati di training di alta qualità richiede un’attenta considerazione degli obiettivi, delle risorse e delle implicazioni etiche. I dati possono essere ottenuti da varie fonti a seconda delle esigenze del progetto:\n\n5.3.1 Dataset preesistenti\nPiattaforme come Kaggle e UCI Machine Learning Repository forniscono un comodo punto di partenza. I dataset preesistenti sono preziosi per ricercatori, sviluppatori e aziende. Uno dei loro principali vantaggi è l’efficienza dei costi. Creare un set di dati da zero può richiedere molto tempo ed essere costoso, quindi accedere a dati già pronti può far risparmiare risorse significative. Inoltre, molti set di dati, come ImageNet, sono diventati parametri di riferimento standard nella comunità di apprendimento automatico, consentendo confronti di prestazioni coerenti tra diversi modelli e algoritmi. Questa disponibilità di dati significa che gli esperimenti possono essere avviati immediatamente senza ritardi nella raccolta e nella preelaborazione dei dati. In un campo in rapida evoluzione come il ML, questa praticità è importante.\nLa garanzia di qualità che deriva dai dataset preesistenti più diffusi è importante da considerare perché diversi set di dati contengono errori. Ad esempio, nel set di dati ImageNet è stato riscontrato oltre il 6,4% di errori. Dato il loro uso diffuso, la comunità spesso identifica e corregge eventuali errori o distorsioni in questi set di dati. Questa garanzia è particolarmente utile per studenti e nuovi arrivati nel campo, in quanto possono concentrarsi sull’apprendimento e sulla sperimentazione senza preoccuparsi dell’integrità dei dati. La documentazione di supporto che spesso accompagna i set di dati esistenti è inestimabile, sebbene ciò si applichi generalmente solo a quelli ampiamente utilizzati. Una buona documentazione fornisce approfondimenti sul processo di raccolta dati e sulle definizioni delle variabili e talvolta offre persino prestazioni del modello di base. Queste informazioni non solo aiutano la comprensione, ma promuovono anche la riproducibilità nella ricerca, un pilastro dell’integrità scientifica; attualmente, c’è una crisi attorno al miglioramento della riproducibilità nei sistemi di apprendimento automatico. Quando altri ricercatori hanno accesso agli stessi dati, possono convalidare i risultati, testare nuove ipotesi o applicare metodologie diverse, consentendoci così di basarci più rapidamente sul lavoro reciproco.\nSebbene piattaforme come Kaggle e UCI Machine Learning Repository siano risorse inestimabili, è essenziale comprendere il contesto in cui sono stati raccolti i dati. I ricercatori dovrebbero fare attenzione al potenziale “overfitting” quando utilizzano set di dati popolari, poiché potrebbero essere stati addestrati più modelli su di essi, portando a metriche di prestazioni gonfiate. A volte, questi set di dati non riflettono i dati del mondo reale.\nNegli ultimi anni, si è sviluppata una crescente consapevolezza dei problemi di “bias” [distorsione], validità e riproducibilità che possono esistere nei set di dati di apprendimento automatico. Figura 5.3 illustra un’altra preoccupazione critica: il potenziale di disallineamento quando si utilizza lo stesso set di dati per addestrare modelli diversi.\n\n\n\n\n\n\nFigura 5.3: Addestrare modelli diversi con lo stesso set di dati. Fonte: (icons from left to right: Becris; Freepik; Freepik; Paul J; SBTS2018).\n\n\n\nCome mostrato in Figura 5.3, addestrare più modelli utilizzando lo stesso set di dati può comportare un “disallineamento” tra i modelli e il mondo. Questo disallineamento crea un intero ecosistema di modelli che riflette solo un sottoinsieme ristretto di dati del mondo reale. Un tale scenario può portare a una generalizzazione limitata e a risultati potenzialmente distorti in varie applicazioni che utilizzano questi modelli.\n\n\n5.3.2 Web Scraping\nIl “web scraping” si riferisce a tecniche automatizzate per l’estrazione di dati dai siti Web. In genere comporta l’invio di richieste HTTP ai server Web, il recupero di contenuti HTML e l’analisi di tali contenuti per estrarre informazioni rilevanti. Gli strumenti e i framework più diffusi per il web scraping includono Beautiful Soup, Scrapy e Selenium. Questi strumenti offrono diverse funzionalità, dall’analisi dei contenuti HTML all’automazione delle interazioni con i browser Web, in particolare per i siti Web che caricano i contenuti in modo dinamico tramite JavaScript.\nIl web scraping può raccogliere efficacemente grandi set di dati per l’addestramento di modelli di apprendimento automatico, in particolare quando i dati etichettati da esseri umani sono scarsi. Per la ricerca sulla visione artificiale, il web scraping consente la raccolta di enormi volumi di immagini e video. I ricercatori hanno utilizzato questa tecnica per creare set di dati influenti come ImageNet e OpenImages. Ad esempio, si potrebbero effettuare scraping di siti di e-commerce per accumulare foto di prodotti per il riconoscimento di oggetti o piattaforme di social media per raccogliere caricamenti di utenti per l’analisi facciale. Anche prima di ImageNet, il progetto LabelMe di Stanford ha raschiato (scraped) Flickr per oltre 63.000 immagini annotate che coprono centinaia di categorie di oggetti.\nOltre alla visione artificiale, lo scraping web supporta la raccolta di dati testuali per il linguaggio naturale. I ricercatori possono “raschiare” siti di notizie per dati di analisi del “sentiment”, forum e siti di recensioni per la ricerca sui sistemi di dialogo o social media per la modellazione di argomenti. Ad esempio, i dati di training per il chatbot ChatGPT sono stati ottenuti tramite scraping di gran parte dell’Internet pubblico. I repository GitHub sono stati sottoposti a scraping per addestrare l’assistente di codifica Copilot AI di GitHub.\nIl web scraping può anche raccogliere dati strutturati, come prezzi delle azioni, dati meteorologici o informazioni sui prodotti, per applicazioni analitiche. Una volta che i dati sono stati “raschiati”, è essenziale archiviarli in modo strutturato, spesso utilizzando database o data warehouse. Una corretta gestione dei dati garantisce l’usabilità dei dati raccolti per analisi e applicazioni future.\nTuttavia, mentre il web scraping offre numerosi vantaggi, ci sono limitazioni significative e considerazioni etiche da sostenere. Non tutti i siti Web consentono lo scraping e la violazione di queste restrizioni può portare a ripercussioni legali. Anche lo scraping di materiale protetto da copyright o comunicazioni private è immorale e potenzialmente illegale. Il web scraping etico impone l’aderenza al file ‘robots.txt’ di un sito web, che delinea le sezioni del sito a cui è possibile accedere e che possono essere scansionate dai bot automatizzati.\nPer scoraggiare lo scraping automatizzato, molti siti web implementano limiti di velocità. Se un bot invia troppe richieste in un breve periodo, potrebbe essere temporaneamente bloccato, limitando la velocità di accesso ai dati. Inoltre, la natura dinamica dei contenuti web implica che i dati estratti a intervalli diversi potrebbero richiedere maggiore coerenza, ponendo sfide per gli studi a lungo termine. Tuttavia, ci sono tendenze emergenti come la Web Navigation in cui gli algoritmi di apprendimento automatico possono navigare automaticamente nel sito web per accedere ai contenuti dinamici.\nIl volume di dati pertinenti disponibili per lo scraping potrebbe essere limitato per argomenti di nicchia. Ad esempio, mentre lo scraping per argomenti comuni come immagini di gatti e cani potrebbe produrre dati abbondanti, la ricerca di condizioni mediche rare potrebbe essere meno fruttuosa. Inoltre, i dati ottenuti tramite scraping sono spesso non strutturati e rumorosi, il che richiede un’accurata pre-elaborazione e pulizia. È fondamentale comprendere che non tutti i dati raccolti saranno di alta qualità o accuratezza. L’impiego di metodi di verifica, come il riferimento incrociato con fonti alternative di dati, può migliorare l’affidabilità dei dati.\nQuando si esegue lo scraping di dati personali, sorgono problemi di privacy, sottolineando la necessità di anonimizzazione. Pertanto, è fondamentale aderire ai “Termini del Servizio” di un sito Web, limitare la raccolta di dati a quelli di dominio pubblico e garantire l’anonimato di tutti i dati personali acquisiti.\nMentre il web scraping può essere un metodo scalabile per accumulare grandi set di dati di training per sistemi di intelligenza artificiale, la sua applicabilità è limitata a tipi di dati specifici. Ad esempio, il web scraping rende più complessa la ricerca di dati per unità di misura inerziali (IMU) per il riconoscimento dei gesti. Al massimo, si può effettuare lo scraping di un set di dati esistente.\nLa raccolta dal Web può produrre dati incoerenti o imprecisi. Ad esempio, la foto in Figura 5.4 viene visualizzata quando si cerca “semaforo” su Google Images. È un’immagine del 1914 che mostra semafori obsoleti, che sono anche appena distinguibili a causa della scarsa qualità dell’immagine. Questo può essere problematico per i set di dati estratti dal Web, poiché lo inquina con campioni di dati non applicabili (vecchi).\n\n\n\n\n\n\nFigura 5.4: Un’immagine di vecchi semafori (1914). Fonte: Vox.\n\n\n\n\n\n\n\n\n\nEsercizio 5.2: Web Scraping\n\n\n\n\n\nScoprire la potenza del web scraping con Python usando librerie come Beautiful Soup e Pandas. Questo esercizio estrarrà la documentazione Python per i nomi e le descrizioni delle funzioni ed esplorerà le statistiche dei giocatori NBA. Alla fine, si avranno le competenze per estrarre e analizzare dati da siti Web reali. Pronti all’immersione? Accedere al notebook Google Colab qui sotto e iniziare a fare pratica!\n\n\n\n\n\n\n5.3.3 Crowdsourcing\nIl crowdsourcing per i dataset è la pratica di ottenere dati utilizzando i servizi di molte persone, sia da una comunità specifica che dal pubblico in generale, in genere tramite Internet. Invece di affidarsi a un piccolo team o a un’organizzazione specifica per raccogliere o etichettare i dati, il crowdsourcing sfrutta lo sforzo collettivo di un vasto gruppo distribuito di partecipanti. Servizi come Amazon Mechanical Turk consentono la distribuzione di attività di annotazione a una forza lavoro ampia e diversificata. Questo facilita la raccolta di etichette per attività complesse come l’analisi del “sentiment” o il riconoscimento delle immagini che richiedono il giudizio umano.\nIl crowdsourcing è emerso come un approccio efficace per la raccolta di dati e la risoluzione dei problemi. Uno dei principali vantaggi del crowdsourcing è la scalabilità: distribuendo le attività a un ampio pool globale di collaboratori su piattaforme digitali, i progetti possono elaborare rapidamente enormi volumi di dati. Ciò rende il crowdsourcing ideale per l’etichettatura, la raccolta e l’analisi di dati su larga scala.\nInoltre, il crowdsourcing attinge a un gruppo eterogeneo di partecipanti, apportando un’ampia gamma di prospettive, intuizioni culturali e capacità linguistiche che possono arricchire i dati e migliorare la risoluzione creativa dei problemi in modi che un gruppo più omogeneo potrebbe non fare. Poiché il crowdsourcing attinge da un vasto pubblico oltre i canali tradizionali, è più conveniente rispetto ai metodi convenzionali, soprattutto per microattività più semplici.\nLe piattaforme di crowdsourcing consentono anche una grande flessibilità, poiché i parametri delle attività possono essere modificati in tempo reale in base ai risultati iniziali. Ciò crea un ciclo di feedback per miglioramenti iterativi al processo di raccolta dati. I lavori complessi possono essere suddivisi in microattività e distribuiti a più persone, con risultati convalidati in modo incrociato assegnando versioni ridondanti della stessa attività. Se gestito in modo ponderato, il crowdsourcing consente il coinvolgimento della comunità attorno a un progetto collaborativo, in cui i partecipanti trovano una ricompensa nel contribuire.\nTuttavia, mentre il crowdsourcing offre numerosi vantaggi, è essenziale affrontarlo con una strategia chiara. Mentre fornisce l’accesso a un set diversificato di annotatori, introduce anche variabilità nella qualità delle annotazioni. Inoltre, piattaforme come Mechanical Turk potrebbero non sempre catturano uno spettro demografico completo; spesso, gli individui esperti di tecnologia sono sovra-rappresentati, mentre i bambini e gli anziani potrebbero essere sotto-rappresentati. Fornire istruzioni chiare e formazione per gli annotatori è fondamentale. Controlli periodici e convalide dei dati etichettati aiutano a mantenere la qualità. Ciò si ricollega all’argomento della chiara definizione del problema di cui abbiamo discusso in precedenza. Il crowdsourcing per i set di dati richiede anche una particolare attenzione alle considerazioni etiche. È fondamentale assicurarsi che i partecipanti siano informati su come verranno utilizzati i loro dati e che la loro privacy sia protetta. Il controllo di qualità tramite protocolli dettagliati, trasparenza nell’approvvigionamento e verifica è essenziale per garantire risultati affidabili.\nPer TinyML, il crowdsourcing può presentare alcune sfide uniche. I dispositivi TinyML sono altamente specializzati per attività particolari entro vincoli rigorosi. Di conseguenza, i dati di cui hanno bisogno tendono a essere molto specifici. Ottenere tali dati specializzati da un pubblico generico può essere difficile tramite crowdsourcing. Ad esempio, le applicazioni TinyML spesso si basano su dati raccolti da determinati sensori o hardware. Il crowdsourcing richiederebbe ai partecipanti di avere accesso a dispositivi molto specifici e coerenti, come i microfoni, con le stesse frequenze di campionamento. Queste sfumature hardware presentano ostacoli anche per semplici attività audio come l’individuazione di parole chiave.\nOltre all’hardware, i dati stessi necessitano di elevata granularità e qualità, dati i limiti di TinyML. Può essere difficile garantire ciò quando si fa crowdsourcing da chi non ha familiarità con il contesto e i requisiti dell’applicazione. Ci sono anche potenziali problemi relativi alla privacy, alla raccolta in tempo reale, alla standardizzazione e alle competenze tecniche. Inoltre, la natura ristretta di molte attività TinyML semplifica l’etichettatura accurata dei dati con la giusta comprensione. I partecipanti potrebbero aver bisogno di un contesto completo per fornire annotazioni affidabili.\nPertanto, mentre il crowdsourcing può funzionare bene in molti casi, le esigenze specializzate di TinyML introducono sfide uniche per i dati. È richiesta un’attenta pianificazione per linee guida, targeting e controllo di qualità. Per alcune applicazioni, il crowdsourcing potrebbe essere fattibile, ma altre potrebbero richiedere più lavoro per la raccolta dati più mirati per ottenere dati di training pertinenti e di alta qualità.\n\n\n5.3.4 Dati Sintetici\nLa generazione di dati sintetici può essere una soluzione preziosa per affrontare le limitazioni della raccolta dati. Figura 5.5 illustra come funziona questo processo: i dati sintetici vengono uniti ai dati storici per creare un set di dati più ampio e diversificato per l’addestramento del modello.\n\n\n\n\n\n\nFigura 5.5: Aumento delle dimensioni dei dati di training con la generazione di dati sintetici. Fonte: AnyLogic.\n\n\n\nCome mostrato nella figura, i dati sintetici implicano la creazione di informazioni che non sono state originariamente catturate o osservate, ma vengono generate utilizzando algoritmi, simulazioni o altre tecniche per assomigliare ai dati del mondo reale. Questo approccio è diventato particolarmente prezioso in campi in cui i dati del mondo reale sono scarsi, costosi o eticamente difficili da ottenere, come nelle applicazioni TinyML. Varie tecniche, tra cui le “Generative Adversarial Networks (GAN)”, possono produrre dati sintetici di alta qualità quasi indistinguibili dai dati reali. Questi metodi hanno fatto notevoli progressi, rendendo la generazione di dati sintetici sempre più realistica e affidabile.\nPotrebbe essere necessario disporre di più dati del mondo reale per l’analisi o l’addestramento di modelli di apprendimento automatico in molti domini, in particolare quelli emergenti. I dati sintetici possono colmare questa lacuna producendo grandi volumi di dati che imitano scenari del mondo reale. Ad esempio, rilevare il suono di un vetro che si rompe potrebbe essere difficile nelle applicazioni di sicurezza in cui un dispositivo TinyML sta cercando di identificare le effrazioni. La raccolta di dati del mondo reale richiederebbe la rottura di numerose finestre, il che è poco pratico e costoso.\nInoltre, avere un set di dati diversificato è fondamentale nell’apprendimento automatico, in particolare nel deep learning. I dati sintetici possono aumentare i set di dati esistenti introducendo varianti, migliorando così la robustezza dei modelli. Ad esempio, SpecAugment è un’eccellente tecnica di aumento dei dati per i sistemi di “Automatic Speech Recognition” (ASR).\nAnche la privacy e la riservatezza sono grandi problemi. I set di dati contenenti informazioni sensibili o personali sollevano problemi di privacy quando vengono condivisi o utilizzati. I dati sintetici, essendo generati artificialmente, non hanno questi legami diretti con individui reali, consentendo un utilizzo più sicuro preservando al contempo le proprietà statistiche essenziali.\nLa generazione di dati sintetici, in particolare una volta stabiliti i meccanismi di generazione, può essere un’alternativa più conveniente. I dati sintetici eliminano la necessità di rompere più finestre per raccogliere dati rilevanti nello scenario applicativo di sicurezza di cui sopra.\nMolti casi d’uso embedded riguardano situazioni uniche, come gli impianti di produzione, che sono difficili da simulare. I dati sintetici consentono ai ricercatori il controllo completo sul processo di generazione dei dati, consentendo la creazione di scenari o condizioni specifici che sono difficili da catturare nella vita reale.\nSebbene i dati sintetici offrano numerosi vantaggi, è essenziale utilizzarli giudiziosamente. Bisogna fare attenzione a garantire che i dati generati rappresentino accuratamente le distribuzioni sottostanti del mondo reale e non introducano distorsioni indesiderate.\n\n\n\n\n\n\nEsercizio 5.3: Dati Sintetici\n\n\n\n\n\nScopriamo la generazione di dati sintetici utilizzando le Generative Adversarial Network (GAN) su dati tabellari. Adotteremo un approccio pratico, immergendoci nel funzionamento del modello CTGAN e applicandolo al set di dati Synthea dal dominio sanitario. Dalla pre-elaborazione dei dati al training e valutazione del modello, procederemo passo dopo passo, imparando come creare dati sintetici, valutarne la qualità e sbloccare il potenziale delle GAN per l’aumento dei dati e le applicazioni del mondo reale.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Engineering</span>"
    ]
  },
  {
    "objectID": "contents/core/data_engineering/data_engineering.it.html#archiviazione-dati",
    "href": "contents/core/data_engineering/data_engineering.it.html#archiviazione-dati",
    "title": "5  Data Engineering",
    "section": "5.4 Archiviazione Dati",
    "text": "5.4 Archiviazione Dati\nL’approvvigionamento e l’archiviazione dei dati vanno di pari passo e i dati devono essere archiviati in un formato che faciliti l’accesso e l’elaborazione. A seconda del caso d’uso, possono essere utilizzati vari tipi di sistemi di archiviazione dati per archiviare i set di dati. Alcuni esempi sono mostrati in Tabella 5.1.\n\n\n\nTabella 5.1: Panoramica comparativa del database, del data warehouse e del data lake.\n\n\n\n\n\n\n\n\n\n\nDatabase\nData Warehouse\nData Lake\n\n\n\n\nScopo\nOperativo e transazionale\nAnalitico\n\n\nTipo di dati\nStrutturato\nStrutturato, semi-strutturato e/o non strutturato\n\n\nScala\nDa piccoli a grandi volumi di dati\nGrandi volumi di dati integrati Grandi volumi di dati diversi\n\n\nEsempi\nMySQL\nGoogle BigQuery, Amazon Redshift, Microsoft Azure Synapse, Google Cloud Storage, AWS S3, Azure Data Lake Storage\n\n\n\n\n\n\nI dati archiviati sono spesso accompagnati da metadati, definiti come “dati sui dati”. Forniscono informazioni contestuali dettagliate sui dati, come mezzi di creazione dei dati, ora di creazione, licenza di utilizzo dei dati allegata, ecc. Figura 5.6 illustra i pilastri chiave della raccolta dati e i metodi associati, evidenziando l’importanza della gestione dei dati strutturati. Ad esempio, Hugging Face ha implementato le Dataset Cards per promuovere un utilizzo responsabile dei dati. Queste “card” [schede], che si allineano al pilastro della documentazione mostrato in Figura 5.6, consentono ai creatori di dataset di rivelare potenziali bias e di istruire gli utenti sui contenuti e le limitazioni di un dataset.\nLe “dataset card” forniscono un contesto importante sull’utilizzo appropriato del dataset evidenziando “bia” [pregiudizi] e altri dettagli importanti. Avere questo tipo di metadati strutturati può anche consentire un rapido recupero, allineandosi ai principi di gestione efficiente dei dati illustrati nella figura. Una volta sviluppato e distribuito il modello sui dispositivi edge, i sistemi di storage possono continuare a memorizzare dati in arrivo, aggiornamenti del modello o risultati analitici, utilizzando potenzialmente metodi da più pilastri mostrati in Figura 5.6. Questo processo continuo di raccolta e gestione dei dati garantisce che il modello rimanga aggiornato e pertinente nel suo ambiente operativo.\n\n\n\n\n\n\nFigura 5.6: Pilastri della raccolta dati. Fonte: Alexsoft\n\n\n\nData Governance: Con una grande quantità di archiviazione dati, è anche fondamentale disporre di policy e pratiche (ad esempio, “governance” [gestione] dei dati) che aiutino a gestire i dati durante il loro ciclo di vita, dall’acquisizione allo smaltimento. La governance dei dati descrive il modo in cui i dati vengono gestiti e include l’adozione di decisioni chiave in merito al loro accesso e controllo. Figura 5.7 illustra i diversi domini coinvolti nella governance dei dati. Implica l’esercizio dell’autorità e l’assunzione di decisioni sui dati per mantenerne la qualità, garantire la conformità, mantenere la sicurezza e ricavarne valore. La governance dei dati è resa operativa sviluppando politiche, incentivi e sanzioni, coltivando una cultura che percepisce i dati come un bene prezioso. Procedure specifiche e autorità assegnate vengono implementate per salvaguardare la qualità dei dati e monitorarne l’utilizzo e i rischi correlati.\n\n\n\n\n\n\nFigura 5.7: Una panoramica del framework di governance dei dati. Fonte: StarCIO..\n\n\n\nLa governance dei dati utilizza tre approcci integrativi: pianificazione e controllo, organizzativo e basato sul rischio.\n\nL’approccio di pianificazione e controllo, comune nell’IT, allinea business e tecnologia attraverso cicli annuali e continui aggiustamenti, concentrandosi su una governance verificabile e basata su policy.\nL’approccio organizzativo enfatizza la struttura, stabilendo ruoli autorevoli come Chief Data Officer e garantendo responsabilità e rendicontazione nella governance.\nL’approccio basato sul rischio, intensificato dai progressi dell’IA, si concentra sull’identificazione e la gestione dei rischi intrinseci nei dati e negli algoritmi. Affronta in particolare i problemi specifici dell’IA attraverso valutazioni regolari e strategie di gestione proattiva del rischio, consentendo azioni incidentali e preventive per mitigare gli impatti indesiderati degli algoritmi.\n\nEcco alcuni esempi di governance dei dati in diversi settori:\n\nMedicina: Gli Health Information Exchanges (HIE) [scambi di informazioni sanitarie] consentono la condivisione di informazioni sanitarie tra diversi operatori sanitari per migliorare l’assistenza ai pazienti. Implementano rigorose pratiche di governance dei dati per mantenere l’accuratezza, l’integrità, la privacy e la sicurezza dei dati, rispettando normative come l’Health Insurance Portability and Accountability Act (HIPAA). Le policy di governance assicurano che i dati dei pazienti siano condivisi solo con entità autorizzate e che i pazienti possano controllare l’accesso alle proprie informazioni.\nFinanza: Basilea III Framework è un quadro normativo internazionale per le banche. Garantisce che le banche stabiliscano policy, pratiche e responsabilità chiare per la gestione dei dati, assicurandone accuratezza, completezza e tempestività. Non solo consente alle banche di soddisfare la conformità normativa, ma previene anche le crisi finanziarie gestendo i rischi in modo più efficace.\nGoverno: Le agenzie governative che gestiscono i dati dei cittadini, i registri pubblici e le informazioni amministrative implementano la governance dei dati per gestire i dati in modo trasparente e sicuro. Il sistema di previdenza sociale negli Stati Uniti e il sistema Aadhar in India sono buoni esempi di tali sistemi di governance.\n\nConsiderazioni speciali sull’archiviazione dei dati per TinyML\nFormati di Archiviazione Audio Efficienti: I sistemi di individuazione delle parole chiave necessitano di formati di archiviazione audio specializzati per consentire una rapida ricerca delle parole chiave nei dati audio. I formati tradizionali come WAV e MP3 archiviano forme d’onda audio complete, che richiedono un’elaborazione estesa per la ricerca. L’individuazione delle parole chiave utilizza un archivio compresso ottimizzato per la ricerca basata su frammenti. Un approccio consiste nell’archiviazione di feature acustiche compatte anziché audio grezzo. Tale flusso di lavoro implicherebbe:\n\nEstrazione di Feature Acustiche: I coefficienti Mel-frequency cepstral (MFCC) rappresentano comunemente importanti caratteristiche audio.\nCreazione di Embedding: Gli “embedding” trasformano le feature acustiche estratte in spazi vettoriali continui, consentendo un’archiviazione dei dati più compatta e rappresentativa. Questa rappresentazione è essenziale per convertire dati ad alta dimensionalità, come l’audio, in un formato più gestibile ed efficiente per l’elaborazione e l’archiviazione.\nQuantizzazione vettoriale: Questa tecnica rappresenta dati ad alta dimensionalità, come gli embedding, con vettori a bassa dimensionalità, riducendo le esigenze di archiviazione. Inizialmente, un codebook viene generato dai dati di training per definire un set di vettori di codice che rappresentano i vettori di dati originali. Successivamente, ogni vettore di dati viene abbinato alla “codeword” più vicina in base al codebook, garantendo una perdita minima di informazioni.\nArchiviazione sequenziale: L’audio viene frammentato in frame brevi e le feature [caratteristiche] quantizzate (o embedded) per ogni frame vengono archiviate in sequenza per mantenere l’ordine temporale, preservando la coerenza e il contesto dei dati audio.\n\nQuesto formato consente di decodificare le feature frame per frame per la corrispondenza delle parole chiave. La ricerca delle feature è più rapida della decompressione dell’audio completo.\nSelective Network Output Storage: [Archiviazione selettiva dell’output di rete] Un’altra tecnica per ridurre l’archiviazione consiste nell’eliminare le caratteristiche audio intermedie archiviate durante l’addestramento ma non richieste durante l’inferenza. La rete viene eseguita su audio completo durante l’addestramento. Tuttavia, solo gli output finali vengono archiviati durante l’inferenza.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Engineering</span>"
    ]
  },
  {
    "objectID": "contents/core/data_engineering/data_engineering.it.html#elaborazione-dei-dati",
    "href": "contents/core/data_engineering/data_engineering.it.html#elaborazione-dei-dati",
    "title": "5  Data Engineering",
    "section": "5.5 Elaborazione dei Dati",
    "text": "5.5 Elaborazione dei Dati\nIl “Data processing” elaborazione dei dati si riferisce ai passaggi necessari per trasformare i dati grezzi in un formato adatto per l’inserimento negli algoritmi di apprendimento automatico. È una fase cruciale in qualsiasi flusso di lavoro ML, ma spesso trascurata. Con un’elaborazione dei dati adeguata, è probabile che i modelli ML raggiungano prestazioni ottimali. Figura 5.8 mostra una ripartizione dell’allocazione del tempo di uno scienziato dei dati, evidenziando la parte significativa spesa per la pulizia e l’organizzazione dei dati (%60).\n\n\n\n\n\n\nFigura 5.8: Ripartizione delle attività dei “Data scientist” in base al tempo impiegato. Fonte: Forbes.\n\n\n\nUna corretta pulizia dei dati è un passaggio cruciale che influisce direttamente sulle prestazioni del modello. I dati del mondo reale sono spesso sporchi, contengono errori, valori mancanti, rumore, anomalie e incongruenze. La pulizia dei dati comporta il rilevamento e la correzione di questi problemi per preparare dati di alta qualità per la modellazione. Selezionando attentamente le tecniche appropriate, i data scientist possono migliorare l’accuratezza del modello, ridurre l’overfitting e addestrare gli algoritmi per apprendere pattern più solidi. Nel complesso, un’elaborazione dei dati ponderata consente ai sistemi di apprendimento automatico di scoprire meglio le informazioni e di fare previsioni dai dati del mondo reale.\nI dati spesso provengono da fonti diverse e possono essere non strutturati o semi-strutturati. Pertanto, elaborarli e standardizzarli è essenziale, assicurando che aderiscano a un formato uniforme. Tali trasformazioni possono includere:\n\nNormalizzazione di variabili numeriche\nCodifica di variabili categoriali\nUtilizzo di tecniche come la riduzione della dimensionalità\n\nLa convalida dei dati svolge un ruolo più ampio rispetto alla garanzia di aderenza a determinati standard, come impedire che i valori di temperatura scendano sotto lo zero assoluto. Questi problemi si verificano in TinyML perché i sensori potrebbero funzionare male o produrre temporaneamente letture errate; tali transienti non sono rari. Pertanto, è fondamentale rilevare gli errori nei dati in anticipo prima che si propaghino attraverso la pipeline dei dati. Rigorosi processi di convalida, tra cui la verifica delle pratiche di annotazione iniziali, il rilevamento di valori anomali e la gestione dei valori mancanti tramite tecniche come l’imputazione della media, contribuiscono direttamente alla qualità dei set di dati. Ciò, a sua volta, influisce sulle prestazioni, la correttezza e la sicurezza dei modelli addestrati su di essi.\nDiamo un’occhiata a Figura 5.9 per un esempio di pipeline di elaborazione dei dati. Nel contesto di TinyML, il Multilingual Spoken Words Corpus (MSWC) è un esempio di pipeline di elaborazione dei dati, flussi di lavoro sistematici e automatizzati per la trasformazione, l’archiviazione e l’elaborazione dei dati. I dati di input (che sono una raccolta di brevi registrazioni) attraversano diverse fasi di elaborazione, come l’allineamento audio-parola e l’estrazione di parole chiave.\nMSWC semplifica il flusso di dati, dai dati grezzi ai set di dati utilizzabili, le pipeline di dati migliorano la produttività e facilitano lo sviluppo rapido di modelli di apprendimento automatico. MSWC è una raccolta ampia e in continua espansione di registrazioni audio di parole pronunciate in 50 lingue diverse, utilizzate collettivamente da oltre 5 miliardi di persone. Questo set di dati è destinato allo studio accademico e all’uso aziendale in aree come l’identificazione di parole chiave e la ricerca basata sul parlato. È concesso in licenza aperta con Creative Commons Attribution 4.0 per un ampio utilizzo.\n\n\n\n\n\n\nFigura 5.9: Una panoramica della pipeline di elaborazione dati del Multilingual Spoken Words Corpus (MSWC). Fonte: Mazumder et al. (2021).\n\n\nMazumder, Mark, Sharad Chitlangia, Colby Banbury, Yiping Kang, Juan Manuel Ciro, Keith Achorn, Daniel Galvez, et al. 2021. «Multilingual spoken words corpus». In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2).\n\n\nIl MSWC ha utilizzato un metodo di allineamento forzato per estrarre automaticamente singole registrazioni di parole per addestrare modelli di individuazione delle parole chiave dal progetto Common Voice, che presenta registrazioni a livello di frase in crowdsourcing. L’allineamento forzato si riferisce a metodi di lunga data nell’elaborazione del parlato che prevedono quando fenomeni del parlato come sillabe, parole o frasi iniziano e finiscono all’interno di una registrazione audio. Nei dati MSWC, le registrazioni in crowdsourcing spesso presentano rumori di sottofondo, come elettricità statica e vento. A seconda dei requisiti del modello, questi rumori possono essere rimossi o mantenuti intenzionalmente.\nMantenere l’integrità dell’infrastruttura dati è uno lavoro continuo. Ciò comprende archiviazione dei dati, sicurezza, gestione degli errori e rigoroso controllo delle versioni. Gli aggiornamenti periodici sono fondamentali, soprattutto in ambiti dinamici come l’individuazione delle parole chiave, per adattarsi alle tendenze linguistiche in evoluzione e alle integrazioni dei dispositivi.\nC’è un boom nelle pipeline di elaborazione dati, comunemente presenti nelle toolchain delle operazioni ML, di cui parleremo nel capitolo MLOps. In breve, questi includono framework come MLOps di Google Cloud. Fornisce metodi per l’automazione e il monitoraggio in tutte le fasi della costruzione del sistema ML, tra cui integrazione, test, rilascio, distribuzione e gestione dell’infrastruttura. Diversi meccanismi si concentrano sull’elaborazione dati, parte integrante di questi sistemi.\n\n\n\n\n\n\nEsercizio 5.4: Elaborazione dei Dati\n\n\n\n\n\nEsploriamo due progetti significativi nell’elaborazione dei dati vocali e nell’apprendimento automatico. MSWC è un vasto set di dati audio con oltre 340.000 parole chiave e 23,4 milioni di esempi parlati di 1 secondo. Viene utilizzato in varie applicazioni come dispositivi voice-enabled e automazione dei call center. Il progetto Few-Shot Keyword Spotting introduce un nuovo approccio per l’individuazione delle parole chiave in diverse lingue, ottenendo risultati impressionanti con dati di addestramento minimi. Esamineremo il set di dati MSWC, impareremo come strutturarlo in modo efficace e poi addestreremo un modello di individuazione di parole chiave con la tecnica “few-shot” [https://www.ibm.com/it-it/topics/few-shot-learning]. Cominciamo!",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Engineering</span>"
    ]
  },
  {
    "objectID": "contents/core/data_engineering/data_engineering.it.html#etichettatura-dei-dati",
    "href": "contents/core/data_engineering/data_engineering.it.html#etichettatura-dei-dati",
    "title": "5  Data Engineering",
    "section": "5.6 Etichettatura dei Dati",
    "text": "5.6 Etichettatura dei Dati\nIl “Data labeling” etichettatura dei dati è importante per creare set di dati di training di alta qualità per modelli di apprendimento automatico. Le etichette forniscono informazioni di base, consentendo ai modelli di apprendere relazioni tra input e output desiderati. Questa sezione copre considerazioni chiave per la selezione di tipi di etichette, formati e contenuti per acquisire le informazioni necessarie per le attività. Discute approcci di annotazione comuni, dall’etichettatura manuale al crowdsourcing ai metodi assistiti dall’intelligenza artificiale, e le “best practice” per garantire la qualità delle etichette tramite formazione, linee guida e controlli di qualità. Sottolineiamo anche il trattamento etico degli annotatori umani. Viene anche esplorata l’integrazione dell’intelligenza artificiale per accelerare e aumentare l’annotazione umana. Comprendere le esigenze, le sfide e le strategie di etichettatura è essenziale per costruire dataset affidabili e utili per addestrare sistemi di apprendimento automatico performanti e affidabili.\n\n5.6.1 Tipi di Etichette\nLe etichette contengono informazioni su attività o concetti chiave. Figura 5.10 include alcuni tipi di etichette comuni: una “classification label” [etichetta di classificazione] viene utilizzata per categorizzare le immagini con etichette (etichettando un’immagine con “dog” [cane] e presenta un cane); un “bounding box” [riquadro delimitatore] identifica la posizione dell’oggetto (disegnando un riquadro attorno al cane); una “segmentation map” [mappa di segmentazione] classifica gli oggetti a livello di pixel (evidenziando il cane con un colore distinto); una “caption” [didascalia] fornisce annotazioni descrittive (descrivendo le azioni, la posizione, il colore, ecc. del cane); e una “transcript” [trascrizione] denota il contenuto audio. La scelta del formato dell’etichetta dipende dal caso d’uso e dai vincoli di risorse, poiché etichette più dettagliate richiedono un lavoro maggiore per la raccolta (Johnson-Roberson et al. 2017).\n\nJohnson-Roberson, Matthew, Charles Barto, Rounak Mehta, Sharath Nittur Sridhar, Karl Rosaen, e Ram Vasudevan. 2017. «Driving in the Matrix: Can virtual worlds replace human-generated annotations for real world tasks?» In 2017 IEEE International Conference on Robotics and Automation (ICRA), 746–53. Singapore, Singapore: IEEE. https://doi.org/10.1109/icra.2017.7989092.\n\n\n\n\n\n\nFigura 5.10: Una panoramica dei tipi di etichette comuni.\n\n\n\nA meno che non si concentri sull’apprendimento auto-supervisionato, un set di dati fornirà probabilmente etichette che affrontano una o più attività di interesse. Date le loro limitazioni di risorse uniche, i creatori di set di dati devono considerare quali informazioni le etichette dovrebbero catturare e come possono ottenere praticamente le etichette necessarie. I creatori devono prima decidere quali tipi di etichette di contenuto dovrebbero catturare. Ad esempio, un creatore interessato al rilevamento delle auto vorrebbe etichettare le auto nel suo dataset. Tuttavia, potrebbe anche considerare se raccogliere simultaneamente etichette per altre attività per cui il set di dati potrebbe essere potenzialmente utilizzato, come il rilevamento dei pedoni.\nInoltre, gli annotatori possono fornire metadati per le informazioni su come il set di dati rappresenta diverse caratteristiche di interesse (cfr. Sezione 5.9). Il dataset Common Voice, ad esempio, include vari tipi di metadati che forniscono informazioni sugli oratori, sulle registrazioni e sulla qualità del set di dati per ciascuna lingua rappresentata (Ardila et al. 2020). Includono suddivisioni demografiche che mostrano il numero di registrazioni per fascia di età e genere del parlante. Questo ci consente di vedere chi ha contribuito alle registrazioni per ogni lingua. Includono anche statistiche come la durata media delle registrazioni e il numero totale di ore di registrazioni convalidate. Queste forniscono informazioni sulla natura e le dimensioni dei set di dati per ogni lingua.\n\nArdila, Rosana, Megan Branson, Kelly Davis, Michael Kohler, Josh Meyer, Michael Henretty, Reuben Morais, Lindsay Saunders, Francis Tyers, e Gregor Weber. 2020. «Common Voice: A Massively-Multilingual Speech Corpus». In Proceedings of the Twelfth Language Resources and Evaluation Conference, 4218–22. Marseille, France: European Language Resources Association. https://aclanthology.org/2020.lrec-1.520.\nInoltre, le metriche di controllo qualità come la percentuale di registrazioni convalidate sono utili per sapere quanto siano completi e puliti i set di dati. I metadati includono anche suddivisioni demografiche normalizzate scalate al 100% per il confronto tra le lingue. Questo evidenzia le differenze di rappresentazione tra lingue con risorse più elevate e più basse.\nSuccessivamente, i creatori devono determinare il formato di tali etichette. Ad esempio, un creatore interessato al rilevamento delle auto potrebbe scegliere tra etichette di classificazione binaria che indicano se è presente un’auto, riquadri di delimitazione che mostrano le posizioni generali di tutte le auto o etichette di segmentazione pixel per pixel che mostrano la posizione esatta di ogni auto. La scelta del formato dell’etichetta può dipendere dal caso d’uso e dai vincoli di risorse, poiché le etichette più dettagliate sono in genere più costose e richiedono più tempo per essere acquisite.\n\n\n5.6.2 Metodi di Annotazione\nGli approcci comuni all’annotazione includono etichettatura manuale, crowdsourcing e tecniche semi-automatiche. L’etichettatura manuale da parte di esperti produce alta qualità ma necessita di maggiore scalabilità. Il crowdsourcing consente ai non esperti di distribuire annotazioni, spesso tramite piattaforme dedicate (Sheng e Zhang 2019). Metodi debolmente supervisionati e programmatici possono ridurre il lavoro manuale generando etichette in modo euristico o automatico (Ratner et al. 2018).\n\nSheng, Victor S., e Jing Zhang. 2019. «Machine Learning with Crowdsourcing: A Brief Summary of the Past Research and Future Directions». Proceedings of the AAAI Conference on Artificial Intelligence 33 (01): 9837–43. https://doi.org/10.1609/aaai.v33i01.33019837.\n\nRatner, Alex, Braden Hancock, Jared Dunnmon, Roger Goldman, e Christopher Ré. 2018. «Snorkel MeTaL: Weak Supervision for Multi-Task Learning». In Proceedings of the Second Workshop on Data Management for End-To-End Machine Learning. ACM. https://doi.org/10.1145/3209889.3209898.\nDopo aver deciso il contenuto e il formato desiderati per le etichette, i creatori iniziano il processo di annotazione. Per raccogliere un gran numero di etichette da annotatori umani, i creatori si affidano spesso a piattaforme di annotazione dedicate, che possono metterli in contatto con team di annotatori umani. Quando utilizzano queste piattaforme, i creatori potrebbero aver bisogno di maggiori informazioni sui background degli annotatori e sui livelli di esperienza con argomenti di interesse. Tuttavia, alcune piattaforme offrono l’accesso ad annotatori con competenze specifiche (ad esempio, medici).\n\n\n\n\n\n\nEsercizio 5.5: Etichette Autoprodotte\n\n\n\n\n\nEsploriamo Wake Vision, un set di dati completo progettato per il rilevamento di persone con TinyML. Questo set di dati deriva da un set di dati più ampio e generico, Open Images (Kuznetsova et al. 2020), e specificamente adattato per il rilevamento binario di persone.\nIl processo di trasformazione comporta il filtraggio e la rietichettatura delle etichette e dei riquadri di delimitazione esistenti in Open Images utilizzando una pipeline automatizzata. Questo metodo non solo consente di risparmiare tempo e risorse, ma garantisce anche che il set di dati soddisfi i requisiti specifici delle applicazioni TinyML.\nInoltre, generiamo metadati per confrontare la correttezza e la robustezza dei modelli in scenari difficili.\nCominciamo!\n\n\n\n\n\nKuznetsova, Alina, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, et al. 2020. «The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale». International journal of computer vision 128 (7): 1956–81.\n\n\n5.6.3 Garantire la Qualità dell’Etichetta\nNon vi è alcuna garanzia che le etichette dei dati siano effettivamente corrette. Figura 5.11 mostra alcuni esempi di casi di etichettatura rigida: alcuni errori derivano da immagini sfocate che le rendono difficili da identificare (l’immagine della rana), e altri derivano da una mancanza di conoscenza del dominio (il caso della cicogna nera). È possibile che nonostante le migliori istruzioni fornite agli etichettatori, etichettino ancora in modo errato alcune immagini (Northcutt, Athalye, e Mueller 2021). Strategie come controlli di qualità, formazione degli annotatori e raccolta di più etichette per ciascun elemento possono aiutare a garantire la qualità delle etichette. Per attività ambigue, più annotatori possono aiutare a identificare i punti dati controversi e quantificare i livelli di disaccordo.\n\n\n\n\n\n\nFigura 5.11: Alcuni esempi di casi di etichettatura rigida. Fonte: Northcutt, Athalye, e Mueller (2021).\n\n\nNorthcutt, Curtis G, Anish Athalye, e Jonas Mueller. 2021. «Pervasive Label Errors in Test Sets Destabilize Machine Learning Benchmarks». arXiv. https://doi.org/https://doi.org/10.48550/arXiv.2103.14749 arXiv-issued DOI via DataCite.\n\n\nQuando si lavora con annotatori umani, è importante offrire un compenso equo e dare priorità al trattamento etico, poiché gli annotatori possono essere sfruttati o danneggiati durante il processo di etichettatura (Perrigo, 2023). Ad esempio, se è probabile che un set di dati contenga contenuti inquietanti, gli annotatori potrebbero trarre vantaggio dall’avere la possibilità di visualizzare le immagini in scala di grigi (Google, s.d.).\n\nGoogle. s.d. «Information quality content moderation». https://blog.google/documents/83/.\n\n\n5.6.4 Annotazione assistita dall’intelligenza artificiale\nIl ML ha una domanda insaziabile di dati. Pertanto, sono necessari più dati. Ciò solleva la questione di come possiamo ottenere più dati etichettati. Invece di generare e curare sempre i dati manualmente, possiamo fare affidamento sui modelli di intelligenza artificiale esistenti per etichettare i set di dati in modo più rapido ed economico, anche se spesso con una qualità inferiore rispetto all’annotazione umana. Questo può essere fatto in vari modi come mostrato in Figura 5.12, tra cui i seguenti:\n\nPre-annotazione: I modelli di intelligenza artificiale possono generare etichette preliminari per un set di dati utilizzando metodi come l’apprendimento semi-supervisionato (Chapelle, Scholkopf, e Zien 2009), che gli esseri umani possono poi esaminare e correggere. Questo può far risparmiare una notevole quantità di tempo, soprattutto per set di dati di grandi dimensioni.\nApprendimento attivo: I modelli di intelligenza artificiale possono identificare i dati più informativi in un dataset, che possono quindi essere riordinati per priorità per l’annotazione umana. Questo può aiutare a migliorare la qualità del set di dati etichettato riducendo al contempo il tempo di annotazione complessivo.\nControllo qualità: I modelli di intelligenza artificiale possono identificare e segnalare potenziali errori nelle annotazioni umane, contribuendo a garantire l’accuratezza e la coerenza del set di dati etichettato.\n\n\nChapelle, O., B. Scholkopf, e A. Zien Eds. 2009. «Semi-Supervised Learning (Chapelle, O. et al., Eds.; 2006) [Book reviews]». IEEE Trans. Neural Networks 20 (3): 542–42. https://doi.org/10.1109/tnn.2009.2015974.\n\n\n\n\n\n\nFigura 5.12: Strategie per acquisire ulteriori dati di addestramento etichettati. Fonte: Standford AI Lab.\n\n\n\nEcco alcuni esempi di come l’annotazione assistita dall’intelligenza artificiale è stata proposta come utile:\n\nImmagini mediche: L’annotazione assistita dall’intelligenza artificiale etichetta le immagini mediche, come scansioni MRI (Magnetic Resonance Imaging) e raggi X (Krishnan, Rajpurkar, e Topol 2022). Annotare attentamente i set di dati medici è estremamente impegnativo, soprattutto su larga scala, poiché gli esperti del settore sono scarsi e diventano costosi. Ciò può aiutare ad addestrare i modelli di intelligenza artificiale per diagnosticare malattie e altre condizioni mediche in modo più accurato ed efficiente.\nAuto a guida autonoma: L’annotazione assistita dall’intelligenza artificiale viene utilizzata per etichettare immagini e video di auto a guida autonoma. Ciò può aiutare ad addestrare i modelli di intelligenza artificiale per identificare oggetti sulla strada, come altri veicoli, pedoni e segnali stradali.\nSocial media: L’annotazione assistita dall’intelligenza artificiale etichetta i post sui social media come immagini e video. Ciò può aiutare ad addestrare i modelli di intelligenza artificiale a identificare e classificare diversi tipi di contenuti, come notizie, pubblicità e post personali.\n\n\nKrishnan, Rayan, Pranav Rajpurkar, e Eric J. Topol. 2022. «Self-supervised learning in medicine and healthcare». Nat. Biomed. Eng. 6 (12): 1346–52. https://doi.org/10.1038/s41551-022-00914-1.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Engineering</span>"
    ]
  },
  {
    "objectID": "contents/core/data_engineering/data_engineering.it.html#controllo-della-versione-dei-dati",
    "href": "contents/core/data_engineering/data_engineering.it.html#controllo-della-versione-dei-dati",
    "title": "5  Data Engineering",
    "section": "5.7 Controllo della Versione dei Dati",
    "text": "5.7 Controllo della Versione dei Dati\nI sistemi di produzione sono costantemente inondati da volumi di dati fluttuanti e in aumento, che determinano la rapida comparsa di numerose repliche di dati. Questi dati in aumento servono come base per l’addestramento di modelli di apprendimento automatico. Ad esempio, un’azienda di vendita globale impegnata nella previsione delle vendite riceve continuamente dati sul comportamento dei consumatori. Allo stesso modo, i sistemi sanitari che formulano modelli predittivi per la diagnosi delle malattie acquisiscono costantemente nuovi dati sui pazienti. Le applicazioni TinyML, come l’individuazione delle parole chiave, sono molto affamate di dati per quanto riguarda la quantità di dati generati. Di conseguenza, è fondamentale un monitoraggio meticoloso delle versioni dei dati e delle prestazioni del modello corrispondente.\nIl “Data Version Control” [controllo delle versioni dei dati] offre una metodologia strutturata per gestire in modo efficiente alterazioni e versioni di set di dati. Facilita il monitoraggio delle modifiche, conserva più versioni e garantisce riproducibilità e tracciabilità nei progetti incentrati sui dati. Inoltre, il controllo delle versioni dei dati offre la versatilità di rivedere e utilizzare versioni specifiche in base alle necessità, garantendo che ogni fase dell’elaborazione dei dati e dello sviluppo del modello possa essere riesaminata e verificata in modo preciso e semplice. Ha una varietà di usi pratici -\nGestione del Rischio: Il controllo della versione dei dati consente trasparenza e responsabilità monitorando le versioni del set di dati.\nCollaborazione ed Efficienza: Un facile accesso a diverse versioni del set di dati in un unico posto può migliorare la condivisione dei dati di controllo specifici e consentire una collaborazione efficiente.\nRiproducibilità: Il controllo della versione dei dati consente di monitorare le prestazioni dei modelli riguardanti diverse versioni dei dati, e quindi di abilitare la riproducibilità.\nConcetti Chiave\n\nCommit: È un’istantanea immutabile dei dati in un momento specifico, che rappresenta una versione univoca. Ogni commit è associato a un identificatore univoco per consentire\nBranch: I “branch” [rami] consentono a sviluppatori e specialisti dei data di discostarsi dalla linea di sviluppo principale e continuare a lavorare in modo indipendente senza influenzare altri rami. Ciò è particolarmente utile quando si sperimentano nuove funzionalità o modelli, consentendo sviluppo e sperimentazione paralleli senza il rischio di danneggiare il ramo principale stabile.\nMerge: I “Merge” [unioni] aiutano a integrare le modifiche da rami diversi mantenendo l’integrità dei dati.\n\nCon il controllo della versione dei dati in atto, possiamo tracciare le modifiche mostrate in Figura 5.13, riprodurre i risultati precedenti ripristinando le versioni precedenti e collaborare in modo sicuro ramificando e isolando le modifiche.\n\n\n\n\n\n\nFigura 5.13: Data versioning.\n\n\n\nSistemi di Data Version Control più Diffusi\n[DVC]: È l’acronimo di Data Version Control ed è uno strumento open source e leggero che funziona su Git Hub e supporta tutti i tipi di formati di dati. Può integrarsi perfettamente nel flusso di lavoro se Git viene utilizzato per gestire il codice. Cattura le versioni dei dati e dei modelli nei commit Git mentre li archivia in locale o sul cloud (ad esempio, AWS, Google Cloud, Azure). Questi dati e modelli (ad esempio, artefatti di ML) sono definiti nei file di metadati, che vengono aggiornati a ogni commit. Può consentire il monitoraggio delle metriche dei modelli su diverse versioni dei dati.\nlakeFS: È uno strumento open source che supporta il controllo della versione dei dati sui “data lake”. Supporta molte operazioni simili a git, come i “branch” e il “merge” dei dati, nonché il ripristino delle versioni precedenti dei dati. Ha anche una funzionalità UI unica, che semplifica notevolmente l’esplorazione e la gestione dei dati.\nGit LFS: È utile per il controllo della versione dei dataset di dimensioni ridotte. Utilizza le funzionalità di “branch” e “merge” native di Git, ma è limitato nel tracciamento delle metriche, nel ripristino delle versioni precedenti o nell’integrazione con i “data lake”.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Engineering</span>"
    ]
  },
  {
    "objectID": "contents/core/data_engineering/data_engineering.it.html#ottimizzazione-dei-dati-per-lia-embedded",
    "href": "contents/core/data_engineering/data_engineering.it.html#ottimizzazione-dei-dati-per-lia-embedded",
    "title": "5  Data Engineering",
    "section": "5.8 Ottimizzazione dei Dati per l’IA Embedded",
    "text": "5.8 Ottimizzazione dei Dati per l’IA Embedded\nI creatori che lavorano su sistemi embedded potrebbero avere priorità insolite quando puliscono i loro dataset. Da un lato, i modelli potrebbero essere sviluppati per casi d’uso insolitamente specifici, che richiedono un filtraggio intensivo dei dataset. Mentre altri modelli di linguaggio naturale possono essere in grado di trasformare qualsiasi discorso in testo, un modello per un sistema embedded può essere incentrato su un singolo compito limitato, come il rilevamento di una parola chiave. Di conseguenza, i creatori possono filtrare in modo aggressivo grandi quantità di dati perché devono affrontare un determinato compito. Un sistema di intelligenza artificiale embedded può anche essere legato a specifici dispositivi hardware o ambienti. Ad esempio, un modello video potrebbe dover elaborare immagini da un singolo tipo di telecamera, che verrà montata solo sui campanelli nei quartieri residenziali. In questo scenario, i creatori possono scartare le immagini se provengono da un diverso tipo di telecamera, mostrano il tipo sbagliato di scenario o sono state scattate dall’altezza o dall’angolazione sbagliate.\nD’altra parte, ci si aspetta spesso che i sistemi di IA embedded forniscano prestazioni particolarmente accurate in contesti imprevedibili del mondo reale. Ciò può portare i creatori a progettare set di dati per rappresentare variazioni nei potenziali input e promuovere la robustezza del modello. Di conseguenza, possono definire un ambito ristretto per il loro progetto ma poi puntare a una copertura approfondita entro quei limiti. Ad esempio, i creatori del modello del campanello menzionato sopra potrebbero provare a coprire le variazioni nei dati derivanti da:\n\nQuartieri geograficamente, socialmente e architettonicamente diversi\nDiversi tipi di illuminazione artificiale e naturale\nDiverse stagioni e condizioni meteorologiche\nOstruzioni (ad esempio gocce di pioggia o scatole di consegna che oscurano la visuale della telecamera)\n\nCome descritto sopra, i creatori possono prendere in considerazione il crowdsourcing o la generazione sintetica di dati per includere queste varianti.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Engineering</span>"
    ]
  },
  {
    "objectID": "contents/core/data_engineering/data_engineering.it.html#sec-data-transparency",
    "href": "contents/core/data_engineering/data_engineering.it.html#sec-data-transparency",
    "title": "5  Data Engineering",
    "section": "5.9 Trasparenza dei Dati",
    "text": "5.9 Trasparenza dei Dati\nFornendo una documentazione chiara e dettagliata, i creatori possono aiutare gli sviluppatori a capire come utilizzare al meglio i loro set di dati. Diversi gruppi hanno suggerito formati di documentazione standardizzati per i set di dati, come Data Cards (Pushkarna, Zaldivar, e Kjartansson 2022), datasheet (Gebru et al. 2021), data statement (Bender e Friedman 2018), o Data Nutrition Labels (Holland et al. 2020). Quando rilasciano un dataset, i creatori possono descrivere quali tipi di dati hanno raccolto, come li hanno raccolti ed etichettati e quali tipi di casi d’uso potrebbero essere adatti o meno al set di dati. Quantitativamente, potrebbe essere opportuno mostrare quanto bene il set di dati rappresenti gruppi diversi (ad esempio, gruppi di genere diversi, telecamere diverse).\n\nGebru, Timnit, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daumé III, e Kate Crawford. 2021. «Datasheets for datasets». Commun. ACM 64 (12): 86–92. https://doi.org/10.1145/3458723.\n\nBender, Emily M., e Batya Friedman. 2018. «Data Statements for Natural Language Processing: Toward Mitigating System Bias and Enabling Better Science». Transactions of the Association for Computational Linguistics 6 (dicembre): 587–604. https://doi.org/10.1162/tacl_a_00041.\n\nHolland, Sarah, Ahmed Hosny, Sarah Newman, Joshua Joseph, e Kasia Chmielinski. 2020. «The Dataset Nutrition Label: A Framework to Drive Higher Data Quality Standards». In Data Protection and Privacy. Hart Publishing. https://doi.org/10.5040/9781509932771.ch-001.\nFigura 5.14 mostra un esempio di una scheda dati per un set di dati di computer vision (CV). Include alcune informazioni di base sul set di dati e istruzioni su come utilizzarlo, inclusi i “bias” noti.\n\n\n\n\n\n\nFigura 5.14: Data card che descrive un dataset CV. Fonte: Pushkarna, Zaldivar, e Kjartansson (2022).\n\n\nPushkarna, Mahima, Andrew Zaldivar, e Oddur Kjartansson. 2022. «Data Cards: Purposeful and Transparent Dataset Documentation for Responsible AI». In 2022 ACM Conference on Fairness, Accountability, and Transparency. ACM. https://doi.org/10.1145/3531146.3533231.\n\n\nTenere traccia della provenienza dei dati, essenzialmente le origini e il viaggio di ogni dato attraverso la pipeline dei dati, non è solo una buona pratica, ma un requisito essenziale per la qualità. La provenienza dei dati contribuisce in modo significativo alla trasparenza dei sistemi di machine learning. I sistemi trasparenti semplificano l’analisi dei dati, consentendo una migliore identificazione e rettifica di errori, bias o incongruenze. Ad esempio, se un modello di ML addestrato su dati medici non è performante in aree specifiche, tracciare la provenienza può aiutare a identificare se il problema riguarda i metodi di raccolta dati, i gruppi demografici rappresentati nei dati o altri fattori. Questo livello di trasparenza non aiuta solo a eseguire il debug del sistema, ma svolge anche un ruolo cruciale nel migliorare la qualità complessiva dei dati. Migliorando l’affidabilità e la credibilità del set di dati, la provenienza dei dati migliora anche le prestazioni del modello e la sua accettabilità tra gli utenti finali.\nQuando si produce la documentazione, i creatori devono anche specificare come gli utenti possono accedere al dataset e come questo verrà mantenuto nel tempo. Ad esempio, gli utenti potrebbero dover sottoporsi a una formazione o ricevere un’autorizzazione speciale dai creatori prima di accedere a un set di dati di informazioni protette, come con molti dataset medici. In alcuni casi, gli utenti potrebbero non accedere direttamente ai dati. Devono invece inviare il loro modello per essere addestrato sull’hardware dei creatori del set di dati, seguendo una configurazione di apprendimento “federato” (Aledhari et al. 2020). I creatori possono anche descrivere per quanto tempo il dataset rimarrà accessibile, come gli utenti possono inviare feedback su eventuali errori che scoprono e se ci sono piani per aggiornare il set di dati.\n\nAledhari, Mohammed, Rehma Razzak, Reza M. Parizi, e Fahad Saeed. 2020. «Federated Learning: A Survey on Enabling Technologies, Protocols, and Applications». #IEEE_O_ACC# 8: 140699–725. https://doi.org/10.1109/access.2020.3013541.\nAlcune leggi e normative promuovono anche la trasparenza dei dati attraverso nuovi requisiti per le organizzazioni:\n\nIl “General Data Protection Regulation (GDPR)” nell’Unione Europea: Stabilisce requisiti rigorosi per l’elaborazione e la protezione dei dati personali dei cittadini dell’UE. Impone policy sulla privacy in linguaggio semplice che spiegano chiaramente quali dati vengono raccolti, perché vengono utilizzati, per quanto tempo vengono archiviati e con chi vengono condivisi. Il GDPR impone inoltre che le informative sulla privacy debbano includere dettagli sulla base giuridica per l’elaborazione, i trasferimenti di dati, i periodi di conservazione, i diritti di accesso e cancellazione e le informazioni di contatto per i responsabili del trattamento dei dati.\nIl “California’s Consumer Privacy Act” (CCPA): Il CCPA richiede policy sulla privacy chiare e diritti di esclusione per vendere dati personali. In modo significativo, stabilisce anche i diritti dei consumatori di essere interpellati per la divulgazione dei propri dati specifici. Le aziende devono fornire copie delle informazioni personali raccolte e dettagli su come vengono utilizzate, quali categorie vengono raccolte e cosa ricevono le terze parti. I consumatori possono identificare dati che ritengono debbano essere più accurati. La legge rappresenta un importante passo avanti nel potenziamento dell’accesso ai dati personali.\n\nGarantire la trasparenza dei dati presenta diverse sfide, soprattutto perché richiede molto tempo e risorse finanziarie. I sistemi di dati sono anche piuttosto complessi e la trasparenza completa può richiedere tempo. La trasparenza completa può anche sopraffare i consumatori con troppi dettagli. Infine, è anche importante bilanciare il compromesso tra trasparenza e privacy.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Engineering</span>"
    ]
  },
  {
    "objectID": "contents/core/data_engineering/data_engineering.it.html#licenze",
    "href": "contents/core/data_engineering/data_engineering.it.html#licenze",
    "title": "5  Data Engineering",
    "section": "5.10 Licenze",
    "text": "5.10 Licenze\nMolti dataset di alta qualità provengono da fonti proprietarie o contengono informazioni protette da copyright. Ciò introduce le licenze come una competenza legale impegnativa. Le aziende desiderose di addestrare sistemi di ML devono impegnarsi in trattative per ottenere licenze che garantiscano l’accesso legale a questi dataset. Inoltre, i termini delle licenze possono imporre restrizioni sulle applicazioni dei dati e sui metodi di condivisione. Il mancato rispetto di queste licenze può avere gravi conseguenze.\nAd esempio, ImageNet, uno dei dataset più ampiamente utilizzati per la ricerca sulla visione artificiale, è un caso emblematico. La maggior parte delle sue immagini è stata ottenuta da fonti online pubbliche senza esplicita autorizzazione, suscitando preoccupazioni etiche (Prabhu e Birhane, 2020). L’accesso al set di dati ImageNet per le aziende richiede la registrazione e l’adesione ai suoi termini di utilizzo, che limitano l’uso commerciale (ImageNet, 2021). I principali attori come Google e Microsoft investono in modo significativo nella concessione di licenze per i set di dati per migliorare i loro sistemi di visione di ML. Tuttavia, il fattore costo limita l’accessibilità per i ricercatori di aziende più piccole con budget limitati.\nIl dominio legale della concessione di licenze per i dati ha visto casi importanti che aiutano a definire i parametri di utilizzo corretto. Un esempio importante è Authors Guild, Inc. contro Google, Inc. Questa causa del 2005 sosteneva che il progetto di scansione di libri di Google violava i diritti d’autore visualizzando frammenti senza autorizzazione. Tuttavia, i tribunali alla fine si sono pronunciati a favore di Google, sostenendo il “fair use” [correttezza] in base alla natura trasformativa della creazione di un indice ricercabile e della visualizzazione di estratti limitati di testo. Questo precedente fornisce alcune basi legali per sostenere che le protezioni del “fair use” si applicano all’indicizzazione di set di dati e alla generazione di campioni rappresentativi per l’apprendimento automatico. Tuttavia, le restrizioni di licenza rimangono vincolanti, quindi un’analisi completa dei termini di licenza è fondamentale. Il caso dimostra perché le negoziazioni con i fornitori di dati sono importanti per consentire un utilizzo legale entro limiti accettabili.\nNuove Normative sui Dati e le Loro Implicazioni\nAnche le nuove normative sui dati hanno un impatto sulle pratiche di licenza. Il panorama legislativo si sta evolvendo con normative come l’Artificial Intelligence Act dell’UE, che è pronto a regolamentare lo sviluppo e l’uso dei sistemi di intelligenza artificiale all’interno dell’Unione Europea (UE). Questa legislazione:\n\nClassifica i sistemi di IA in base al rischio.\nImpone prerequisiti di sviluppo e utilizzo.\nSottolinea la qualità dei dati, la trasparenza, la supervisione umana e la responsabilità.\n\nInoltre, l’EU Act affronta le dimensioni etiche e le sfide operative in settori quali sanità e finanza. Gli elementi chiave includono il divieto di sistemi di intelligenza artificiale che presentano rischi “inaccettabili”, condizioni rigorose per sistemi ad alto rischio e obblighi minimi per sistemi di intelligenza artificiale a “rischio limitato”. Il proposto “European AI Board” supervisionerà e garantirà l’implementazione di una regolamentazione efficiente.\nProblemi nell’Assemblaggio di Dataset di Training ML\nProblemi complessi di licenza relativi a dati proprietari, leggi sul copyright e normative sulla privacy limitano le opzioni per l’assemblaggio dei set di dati di training ML. Tuttavia, espandere l’accessibilità tramite licenze più aperte o collaborazioni di dati pubblico-private potrebbe accelerare notevolmente il progresso del settore e gli standard etici.\nA volte, alcune parti di un dataset potrebbero dover essere rimosse o oscurate per rispettare gli accordi di utilizzo dei dati o proteggere informazioni sensibili. Ad esempio, un set di dati di informazioni utente potrebbe contenere nomi, dettagli di contatto e altri dati identificativi che potrebbero dover essere rimossi dal set di dati; questo avviene molto tempo dopo che il set di dati è già stato attivamente reperito e utilizzato per l’addestramento dei modelli. Analogamente, un dataset che include contenuti protetti da copyright o segreti commerciali potrebbe dover filtrare tali parti prima di essere distribuito. Leggi come il General Data Protection Regulation (GDPR), il California Consumer Privacy Act (CCPA) e L’Amended Act on the Protection of Personal Information (APPI) sono state approvate per garantire il diritto all’oblio. Queste normative impongono legalmente ai fornitori di modelli di cancellare i dati degli utenti su richiesta.\nI raccoglitori e i fornitori di dati devono essere in grado di adottare misure appropriate per de-identificare o filtrare qualsiasi informazione proprietaria, concessa in licenza, riservata o regolamentata, se necessario. A volte, gli utenti possono richiedere esplicitamente che i loro dati vengano rimossi.\nLa possibilità di aggiornare il set di dati rimuovendo i dati consentirà ai creatori di rispettare gli obblighi legali ed etici relativi al loro utilizzo e alla privacy. Tuttavia, la capacità di rimuovere i dati presenta alcune limitazioni importanti. Dobbiamo considerare che alcuni modelli potrebbero essere già stati addestrati sul dataset e non esiste un modo chiaro o noto per eliminare l’effetto di un particolare campione di dati dalla rete addestrata. Non esiste un meccanismo di cancellazione. Quindi, ciò solleva la questione: il modello dovrebbe essere riaddestrato da zero ogni volta che viene rimosso un campione? Questa è un’opzione costosa. Una volta che i dati sono stati utilizzati per addestrare un modello, la semplice rimozione dal set di dati originale potrebbe non eliminare completamente il suo impatto sul comportamento del modello. Sono necessarie nuove ricerche sugli effetti della rimozione dei dati sui modelli già addestrati e se sia necessario un ri-addestramento completo per evitare di conservare artefatti di dati eliminati. Ciò presenta una considerazione importante quando si bilanciano gli obblighi di licenza dei dati con l’efficienza e la praticità in un sistema di ML in evoluzione e distribuito.\nLa licenza del dataset è un dominio poliedrico che interseca tecnologia, etica e legge. Comprendere queste complessità diventa fondamentale per chiunque crei set di dati durante l’ingegneria dei dati, man mano che il mondo si evolve.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Engineering</span>"
    ]
  },
  {
    "objectID": "contents/core/data_engineering/data_engineering.it.html#conclusione",
    "href": "contents/core/data_engineering/data_engineering.it.html#conclusione",
    "title": "5  Data Engineering",
    "section": "5.11 Conclusione",
    "text": "5.11 Conclusione\nI dati sono il componente fondamentale dei sistemi di intelligenza artificiale. Senza dati di qualità, anche gli algoritmi di apprendimento automatico più avanzati falliranno. L’ingegneria dei dati comprende il processo end-to-end di raccolta, archiviazione, elaborazione e gestione dei dati per alimentare lo sviluppo di modelli di apprendimento automatico. Si inizia con la definizione chiara del problema principale e degli obiettivi, che guidano una raccolta dati efficace. I dati possono essere reperiti da diversi mezzi, tra cui dataset esistenti, web scraping, crowdsourcing e generazione di dati sintetici. Ogni approccio comporta compromessi tra costi, velocità, privacy e specificità. Una volta raccolti i dati, un’etichettatura ponderata tramite annotazione manuale o assistita dall’intelligenza artificiale consente la creazione di set di dati di training di alta qualità. Un’archiviazione adeguata in database, “warehouse” o “lake” facilita l’accesso e l’analisi. I metadati forniscono dettagli contestuali sui dati. L’elaborazione dei dati trasforma i dati grezzi in un formato pulito e coerente per lo sviluppo di modelli di apprendimento automatico. In tutta questa pipeline, la trasparenza attraverso la documentazione e il tracciamento della provenienza è fondamentale per l’etica, la verificabilità e la riproducibilità. I protocolli di licenza dei dati regolano anche l’accesso e l’uso legale dei dati. Le principali sfide nell’ingegneria dei dati includono rischi per la privacy, lacune di rappresentazione, restrizioni legali sui dati proprietari e la necessità di bilanciare vincoli concorrenti come velocità e qualità. Progettando attentamente dati di training di alta qualità, i professionisti dell’apprendimento automatico possono sviluppare sistemi di intelligenza artificiale accurati, robusti e responsabili, tra cui applicazioni embedded e TinyML.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Engineering</span>"
    ]
  },
  {
    "objectID": "contents/core/data_engineering/data_engineering.it.html#sec-data-engineering-resource",
    "href": "contents/core/data_engineering/data_engineering.it.html#sec-data-engineering-resource",
    "title": "5  Data Engineering",
    "section": "5.12 Risorse",
    "text": "5.12 Risorse\nEcco un elenco curato di risorse per supportare studenti e insegnanti nei loro percorsi di apprendimento e insegnamento. Lavoriamo continuamente per espandere questa raccolta e presto aggiungeremo nuovi esercizi.\n\n\n\n\n\n\nSlide\n\n\n\n\n\nQueste slide sono uno strumento prezioso per gli insegnanti per tenere lezioni e per gli studenti per rivedere il materiale secondo il proprio ritmo. Incoraggiamo studenti e docenti a sfruttare queste slide per migliorare la loro comprensione e facilitare un trasferimento efficace delle conoscenze.\n\nData Engineering: Overview.\nFeature engineering.\nData Standards: Speech Commands.\nCrowdsourcing Data for the Long Tail.\nReusing and Adapting Existing Datasets.\nResponsible Data Collection.\nRilevamento Dati Anomali:\n\nAnomaly Detection: Overview.\nAnomaly Detection: Challenges.\nAnomaly Detection: Datasets.\nAnomaly Detection: Using Autoencoders.\n\n\n\n\n\n\n\n\n\n\n\nVideo\n\n\n\n\n\n\nProssimamente.\n\n\n\n\n\n\n\n\n\n\nEsercizi\n\n\n\n\n\nPer rafforzare i concetti trattati in questo capitolo, abbiamo curato una serie di esercizi che sfidano gli studenti ad applicare le proprie conoscenze e ad approfondire la propria comprensione.\n\nEsercizio 5.1\nEsercizio 5.2\nEsercizio 5.3\nEsercizio 5.4\nEsercizio 5.5",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Engineering</span>"
    ]
  },
  {
    "objectID": "contents/core/frameworks/frameworks.it.html",
    "href": "contents/core/frameworks/frameworks.it.html",
    "title": "6  Framework di IA",
    "section": "",
    "text": "6.1 Panoramica\nI framework di machine learning [apprendimento automatico] forniscono gli strumenti e l’infrastruttura per creare, addestrare e distribuire in modo efficiente modelli di apprendimento automatico. In questo capitolo esploreremo l’evoluzione e le capacità chiave dei principali framework come TensorFlow (TF), PyTorch e framework specializzati per dispositivi embedded. Ci immergeremo nei componenti come grafi computazionali, algoritmi di ottimizzazione, accelerazione hardware e altro che consentono agli sviluppatori di creare rapidamente modelli performanti. Comprendere questi framework è essenziale per sfruttare la potenza del deep learning in tutto lo spettro, dal cloud ai dispositivi edge [periferici].\nI framework di apprendimento automatico gestiscono gran parte della complessità dello sviluppo di modelli tramite API di alto livello e linguaggi specifici per dominio che consentono ai professionisti di creare rapidamente modelli combinando componenti e astrazioni predefiniti. Ad esempio, framework come TensorFlow e PyTorch forniscono API Python per definire architetture di reti neurali utilizzando livelli, ottimizzatori, set di dati e altro. Ciò consente un’iterazione rapida rispetto alla codifica di ogni dettaglio del modello partendo da zero.\nUna capacità chiave offerta da questi framework è rappresentata dai motori di training distribuiti che possono scalare l’addestramento del modello su cluster di GPU e TPU. Ciò rende possibile il training di modelli all’avanguardia con miliardi o trilioni di parametri su vasti set di dati. I framework si integrano anche con hardware specializzato come le GPU NVIDIA per accelerare ulteriormente il training tramite ottimizzazioni come la parallelizzazione ed efficienti operazioni matriciali.\nInoltre, i framework semplificano il deploy [distribuzione] di modelli finiti in produzione tramite strumenti come TensorFlow Serving per il model serving scalabile e TensorFlow Lite per l’ottimizzazione su dispositivi mobili ed edge. Altre capacità preziose includono visualizzazione, tecniche di ottimizzazione del modello come quantizzazione e potatura e monitoraggio delle metriche durante il training.\nI principali framework open source come TensorFlow, PyTorch e MXNet alimentano gran parte della ricerca e dello sviluppo dell’IA oggi. Offerte commerciali come Amazon SageMaker e Microsoft Azure Machine Learning integrano questi framework open source con funzionalità proprietarie e strumenti aziendali.\nGli ingegneri e i professionisti del machine learning sfruttano questi framework robusti per concentrarsi su attività di alto valore come architettura del modello, progettazione delle feature e ottimizzazione degli iperparametri anziché sull’infrastruttura. L’obiettivo è creare e distribuire modelli performanti che risolvano in modo efficiente i problemi del mondo reale.\nIn questo capitolo, esploreremo i principali framework cloud odierni e il modo in cui hanno adattato modelli e strumenti specificamente per la distribuzione embedded ed edge. Confronteremo modelli di programmazione, hardware supportato, capacità di ottimizzazione e altro ancora per comprendere appieno in che modo i framework consentono un apprendimento automatico scalabile dal cloud all’edge.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Framework di IA</span>"
    ]
  },
  {
    "objectID": "contents/core/frameworks/frameworks.it.html#evoluzione-dei-framework",
    "href": "contents/core/frameworks/frameworks.it.html#evoluzione-dei-framework",
    "title": "6  Framework di IA",
    "section": "6.2 Evoluzione dei Framework",
    "text": "6.2 Evoluzione dei Framework\nI framework di apprendimento automatico si sono evoluti in modo significativo per soddisfare le diverse esigenze dei professionisti del machine learning e i progressi nelle tecniche di intelligenza artificiale. Qualche decennio fa, la creazione e l’addestramento di modelli di apprendimento automatico richiedevano un’ampia codifica e infrastruttura di basso livello. Oltre alla necessità di una codifica di basso livello, la prima ricerca sulle reti neurali era limitata da dati e potenza di calcolo insufficienti. Tuttavia, i framework di apprendimento automatico si sono evoluti notevolmente nell’ultimo decennio per soddisfare le crescenti esigenze dei professionisti e i rapidi progressi nelle tecniche di deep learning [apprendimento profondo]. Il rilascio di grandi set di dati come ImageNet (Deng et al. 2009) e i progressi nel calcolo parallelo con GPU hanno sbloccato il potenziale per reti neurali molto più profonde.\n\nDeng, Jia, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, e Fei-Fei Li. 2009. «ImageNet: A large-scale hierarchical image database». In 2009 IEEE Conference on Computer Vision and Pattern Recognition, 248–55. IEEE. https://doi.org/10.1109/cvpr.2009.5206848.\n\nTeam, The Theano Development, Rami Al-Rfou, Guillaume Alain, Amjad Almahairi, Christof Angermueller, Dzmitry Bahdanau, Nicolas Ballas, et al. 2016. «Theano: A Python framework for fast computation of mathematical expressions». https://arxiv.org/abs/1605.02688.\n\nJia, Yangqing, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross Girshick, Sergio Guadarrama, e Trevor Darrell. 2014. «Caffe: Convolutional Architecture for Fast Feature Embedding». In Proceedings of the 22nd ACM international conference on Multimedia, 675–78. ACM. https://doi.org/10.1145/2647868.2654889.\n\nKrizhevsky, Alex, Ilya Sutskever, e Geoffrey E. Hinton. 2012. «ImageNet Classification with Deep Convolutional Neural Networks». In Advances in Neural Information Processing Systems 25: 26th Annual Conference on Neural Information Processing Systems 2012. Proceedings of a meeting held December 3-6, 2012, Lake Tahoe, Nevada, United States, a cura di Peter L. Bartlett, Fernando C. N. Pereira, Christopher J. C. Burges, Léon Bottou, e Kilian Q. Weinberger, 1106–14. https://proceedings.neurips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html.\n\nChollet, François. 2018. «Introduction to keras». March 9th.\n\nTokui, Seiya, Ryosuke Okuta, Takuya Akiba, Yusuke Niitani, Toru Ogawa, Shunta Saito, Shuji Suzuki, Kota Uenishi, Brian Vogel, e Hiroyuki Yamazaki Vincent. 2019. «Chainer: A Deep Learning Framework for Accelerating the Research Cycle». In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, 5:1–6. ACM. https://doi.org/10.1145/3292500.3330756.\n\nSeide, Frank, e Amit Agarwal. 2016. «Cntk: Microsoft’s Open-Source Deep-Learning Toolkit». In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2135–35. ACM. https://doi.org/10.1145/2939672.2945397.\n\nAnsel, Jason, Edward Yang, Horace He, Natalia Gimelshein, Animesh Jain, Michael Voznesensky, Bin Bao, et al. 2024. «PyTorch 2: Faster Machine Learning Through Dynamic Python Bytecode Transformation and Graph Compilation». In Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2, a cura di Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d’Alché-Buc, Emily B. Fox, e Roman Garnett, 8024–35. ACM. https://doi.org/10.1145/3620665.3640366.\nI primi framework di apprendimento automatico, Theano di Team et al. (2016) e Caffe di Jia et al. (2014), sono stati sviluppati da istituzioni accademiche. Theano è stato creato dal Montreal Institute for Learning Algorithms, mentre Caffe è stato sviluppato dal Berkeley Vision and Learning Center. Nel crescente interesse per il deep learning dovuto alle prestazioni all’avanguardia di AlexNet Krizhevsky, Sutskever, e Hinton (2012) sul dataset ImageNet, aziende private e singole persone hanno iniziato a sviluppare framework di ML, dando vita a Keras di Chollet (2018), Chainer di Tokui et al. (2019), TensorFlow di Google (Yu et al. 2018), CNTK di Microsoft (Seide e Agarwal 2016) e PyTorch di Facebook (Ansel et al. 2024).\nMolti di questi framework ML possono essere suddivisi in framework di alto livello, di basso livello e di grafi computazionali statici e dinamici. I framework di alto livello forniscono un livello di astrazione più elevato rispetto a quelli di basso livello. I framework di alto livello hanno funzioni e moduli predefiniti per attività ML comuni, come la creazione, l’addestramento e la valutazione di modelli ML comuni, la preelaborazione dei dati, le funzionalità di progettazione e la visualizzazione dei dati, che i framework di basso livello non hanno. Pertanto, i framework di alto livello possono risultare più facili da usare ma sono meno personalizzabili rispetto a quelli di basso livello (ad esempio, gli utenti di framework di basso livello possono definire livelli personalizzati, funzioni “loss” [di perdita], algoritmi di ottimizzazione, ecc.). Esempi di framework di alto livello sono TensorFlow/Keras e PyTorch. Esempi di framework ML di basso livello includono TensorFlow con API di basso livello, Theano, Caffe, Chainer e CNTK.\nFramework come Theano e Caffe utilizzavano grafi computazionali statici, che richiedevano la definizione anticipata dell’architettura completa del modello, limitandone così la flessibilità. Al contrario, i grafici dinamici vengono costruiti al volo per uno sviluppo più iterativo. Intorno al 2016, framework come PyTorch e TensorFlow 2.0 hanno iniziato ad adottare grafici dinamici, offrendo maggiore flessibilità per lo sviluppo del modello. Discuteremo di questi concetti e dettagli più avanti nella sezione Training dell’IA.\nLo sviluppo di questi framework ha suscitato un’esplosione di dimensioni e complessità del modello nel tempo, dai primi perceptron multilayer e reti convoluzionali ai moderni trasformatori con miliardi o trilioni di parametri. Nel 2016, i modelli ResNet di He et al. (2016) hanno raggiunto un’accuratezza ImageNet record con oltre 150 livelli e 25 milioni di parametri. Poi, nel 2020, il modello linguistico GPT-3 di OpenAI (Brown et al. 2020) ha spinto i parametri a un sorprendente numero di 175 miliardi utilizzando il parallelismo del modello nei framework per addestrare migliaia di GPU e TPU.\n\nHe, Kaiming, Xiangyu Zhang, Shaoqing Ren, e Jian Sun. 2016. «Deep Residual Learning for Image Recognition». In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 770–78. IEEE. https://doi.org/10.1109/cvpr.2016.90.\n\nBrown, Tom B., Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, et al. 2020. «Language Models are Few-Shot Learners». In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, a cura di Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, e Hsuan-Tien Lin. https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html.\nOgni generazione di framework ha sbloccato nuove capacità che hanno alimentato il progresso:\n\nTheano e TensorFlow (2015) hanno introdotto grafi computazionali e differenziazione automatica per semplificare la creazione di modelli.\nCNTK (2016) ha aperto la strada a un addestramento distribuito efficiente combinando parallelismo di modelli e dati.\nPyTorch (2016) ha fornito programmazione imperativa e grafici dinamici per una sperimentazione flessibile.\nTensorFlow 2.0 (2019) ha impostato di default l’esecuzione Eager per intuitività e debug.\nTensorFlow Graphics (2020) ha aggiunto strutture dati 3D per gestire nuvole di punti e mesh.\n\nNegli ultimi anni, il panorama dei framework di apprendimento automatico si è notevolmente consolidato. Figura 6.2 illustra questa convergenza, mostrando che TensorFlow e PyTorch sono diventati i framework ML prevalentemente dominanti, rappresentando collettivamente oltre il 95% dei framework ML utilizzati nella ricerca e nella produzione. Sebbene entrambi i framework siano diventati importanti, presentano caratteristiche distinte. Figura 6.1 traccia un contrasto tra gli attributi di TensorFlow e PyTorch, contribuendo a spiegare il loro predominio complementare nel settore.\n\n\n\n\n\n\nFigura 6.1: PyTorch e TensorFlow: Caratteristiche e Funzioni. Fonte: K&C\n\n\n\n\n\n\n\n\n\nFigura 6.2: Popolarità dei framework ML negli Stati Uniti misurata dalle ricerche web di Google. Fonte: Google.\n\n\n\nUn approccio unico non funziona bene in tutto lo spettro, dal cloud ai piccoli dispositivi edge. Diversi framework rappresentano varie filosofie sull’esecuzione di grafici, API dichiarative rispetto a quelle imperative e altro ancora. Le dichiarative definiscono cosa dovrebbe fare il programma, mentre le imperative si concentrano su come dovrebbe essere fatto passo dopo passo. Ad esempio, TensorFlow utilizza l’esecuzione di grafici e la modellazione in stile dichiarativo, mentre PyTorch adotta l’esecuzione rapida e la modellazione imperativa per una maggiore flessibilità con Python. Ogni approccio comporta dei compromessi che discuteremo in Sezione 6.3.7.\nGli attuali framework avanzati consentono ai professionisti di sviluppare e distribuire modelli sempre più complessi, un fattore chiave dell’innovazione nel campo dell’intelligenza artificiale. Questi framework continuano a evolversi ed espandere le loro capacità per la prossima generazione di machine learning. Per capire come questi sistemi continuano a evolversi, approfondiremo TensorFlow come esempio di come il framework sia cresciuto in complessità nel tempo.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Framework di IA</span>"
    ]
  },
  {
    "objectID": "contents/core/frameworks/frameworks.it.html#sec-deep_dive_into_tensorflow",
    "href": "contents/core/frameworks/frameworks.it.html#sec-deep_dive_into_tensorflow",
    "title": "6  Framework di IA",
    "section": "6.3 Approfondimento su TensorFlow",
    "text": "6.3 Approfondimento su TensorFlow\nTensorFlow è stato sviluppato dal team di Google Brain ed è stato rilasciato come libreria software open source il 9 novembre 2015. È stato progettato per il calcolo numerico utilizzando grafici di flusso di dati e da allora è diventato popolare per un’ampia gamma di applicazioni di apprendimento automatico e deep learning.\nTensorFlow è un framework di training e inferenza che fornisce funzionalità integrate per gestire tutto, dalla creazione e training del modello alla distribuzione, come mostrato in Figura 6.3. Sin dal suo sviluppo iniziale, l’ecosistema TensorFlow è cresciuto fino a includere molte diverse “varietà” di TensorFlow, ciascuna pensata per consentire agli utenti di supportare ML su diverse piattaforme. In questa sezione, discuteremo principalmente solo del pacchetto core.\n\n6.3.1 Ecosistema TF\n\nTensorFlow Core: pacchetto principale con cui interagiscono la maggior parte degli sviluppatori. Fornisce una piattaforma completa e flessibile per definire, addestrare e distribuire modelli di apprendimento automatico. Include tf.keras come API di alto livello.\nTensorFlow Lite: progettato per distribuire modelli leggeri su dispositivi mobili, embedded ed edge. Offre strumenti per convertire i modelli TensorFlow in un formato più compatto adatto a dispositivi con risorse limitate e fornisce modelli pre-addestrati ottimizzati per dispositivi mobili.\nTensorFlow Lite Micro: progettato per eseguire modelli di apprendimento automatico su microcontrollori con risorse minime. Funziona senza la necessità di supporto del sistema operativo, librerie C o C++ standard o allocazione dinamica della memoria, utilizzando solo pochi kilobyte di memoria.\nTensorFlow.js: libreria JavaScript che consente l’addestramento e la distribuzione di modelli di apprendimento automatico direttamente nel browser o su Node.js. Fornisce inoltre strumenti per il porting di modelli TensorFlow pre-addestrati nel formato browser-friendly.\nTensorFlow su dispositivi edge (Coral): piattaforma di componenti hardware e strumenti software di Google che consente l’esecuzione di modelli TensorFlow su dispositivi edge, sfruttando Edge TPU per l’accelerazione.\nTensorFlow Federated (TFF): framework per l’apprendimento automatico e altri calcoli su dati decentralizzati. TFF facilita l’apprendimento “federato”, consentendo l’addestramento del modello su molti dispositivi senza centralizzare i dati.\nTensorFlow Graphics: libreria per l’utilizzo di TensorFlow per svolgere attività correlate alla grafica, tra cui l’elaborazione di forme 3D e nuvole di punti, utilizzando il deep learning.\nTensorFlow Hub: repository di componenti di modelli di apprendimento automatico riutilizzabili che consente agli sviluppatori di riutilizzare componenti di modelli pre-addestrati, facilitando l’apprendimento per trasferimento e la composizione del modello.\nTensorFlow Serving: framework progettato per servire e distribuire modelli di apprendimento automatico per l’inferenza in ambienti di produzione. Fornisce strumenti per il versioning e l’aggiornamento dinamico dei modelli distribuiti senza interruzione del servizio.\nTensorFlow Extended (TFX): piattaforma end-to-end progettata per distribuire e gestire pipeline di apprendimento automatico in ambienti di produzione. TFX comprende validazione dei dati, pre-elaborazione, addestramento del modello, convalida e componenti di servizio.\n\n\n\n\n\n\n\nFigura 6.3: Panoramica dell’architettura di TensorFlow 2.0. Fonte: Tensorflow.\n\n\n\nTensorFlow è stato sviluppato per affrontare le limitazioni di DistBelief (Yu et al. 2018)—il framework in uso presso Google dal 2011 al 2015—offrendo flessibilità lungo tre direttrici: 1) definizione di nuovi livelli [livelli], 2) perfezionamento degli algoritmi di training e 3) definizione di nuovi algoritmi di training. Per comprendere quali limitazioni di DistBelief hanno portato allo sviluppo di TensorFlow, faremo prima una breve panoramica dell’architettura del server dei parametri utilizzata da DistBelief (Dean et al. 2012).\n\nYu, Yuan, Martı́n Abadi, Paul Barham, Eugene Brevdo, Mike Burrows, Andy Davis, Jeff Dean, et al. 2018. «Dynamic control flow in large-scale machine learning». In Proceedings of the Thirteenth EuroSys Conference, 265–83. ACM. https://doi.org/10.1145/3190508.3190551.\n\nDean, Jeffrey, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Quoc V. Le, Mark Z. Mao, et al. 2012. «Large Scale Distributed Deep Networks». In Advances in Neural Information Processing Systems 25: 26th Annual Conference on Neural Information Processing Systems 2012. Proceedings of a meeting held December 3-6, 2012, Lake Tahoe, Nevada, United States, a cura di Peter L. Bartlett, Fernando C. N. Pereira, Christopher J. C. Burges, Léon Bottou, e Kilian Q. Weinberger, 1232–40. https://proceedings.neurips.cc/paper/2012/hash/6aca97005c68f1206823815f66102863-Abstract.html.\nL’architettura Parameter Server (PS) è un design popolare per distribuire il training di modelli di apprendimento automatico, in particolare reti neurali profonde, su più macchine. L’idea fondamentale è di separare l’archiviazione e la gestione dei parametri del modello dal calcolo utilizzato per aggiornare tali parametri. In genere, i server dei parametri gestiscono l’archiviazione e la gestione dei parametri del modello, suddividendoli su più server. I processi worker eseguono le attività di calcolo, tra cui l’elaborazione dei dati e il calcolo dei gradienti, che vengono poi inviati ai server dei parametri per l’aggiornamento.\nStorage: I processi del server dei parametri stateful [con stato] gestivano l’archiviazione e la gestione dei parametri del modello. Data l’ampia scala dei modelli e la natura distribuita del sistema, questi parametri erano condivisi tra più server dei parametri. Ogni server manteneva una parte dei parametri del modello, rendendolo \"stateful\" poiché doveva mantenere e gestire questo stato durante il processo di training.\nComputation: I processi worker, che potevano essere eseguiti in parallelo, erano senza stato e puramente computazionali. Elaboravano dati e calcolavano gradienti senza mantenere alcuno stato o memoria a lungo termine (M. Li et al. 2014). I worker non conservavano informazioni tra le diverse attività. Invece, comunicavano periodicamente con i server dei parametri per recuperare i parametri più recenti e restituire i gradienti calcolati.\n\nLi, Mu, David G. Andersen, Alexander J. Smola, e Kai Yu. 2014. «Communication Efficient Distributed Machine Learning with the Parameter Server». In Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014, December 8-13 2014, Montreal, Quebec, Canada, a cura di Zoubin Ghahramani, Max Welling, Corinna Cortes, Neil D. Lawrence, e Kilian Q. Weinberger, 19–27. https://proceedings.neurips.cc/paper/2014/hash/1ff1de774005f8da13f42943881c655f-Abstract.html.\n\n\n\n\n\n\nEsercizio 6.1: TensorFlow Core\n\n\n\n\n\nAndiamo a comprendere in modo completo gli algoritmi di apprendimento automatico di base utilizzando TensorFlow e le loro applicazioni pratiche nell’analisi dei dati e nella modellazione predittiva. Inizieremo con la regressione lineare per prevedere i tassi di sopravvivenza dal set di dati del Titanic. Poi, utilizzando TensorFlow, costruiremo classificatori per identificare diverse specie di fiori in base ai loro attributi. Successivamente, utilizzeremo l’algoritmo K-Means e la sua applicazione nella segmentazione dei set di dati in cluster coesi. Infine, applicheremo modelli hidden [nascosti] di Markov (HMM) per prevedere i pattern meteorologici.\n\n\n\n\n\n\n\n\n\n\nEsercizio 6.2: TensorFlow Lite\n\n\n\n\n\nQui vedremo come costruire un modello di apprendimento automatico in miniatura per microcontrollori. Costruiremo una mini rete neurale semplificata per apprendere dai dati anche con risorse limitate e ottimizzata per l’implementazione riducendo il nostro modello per un uso efficiente sui microcontrollori. TensorFlow Lite, una potente tecnologia derivata da TensorFlow, riduce i modelli per dispositivi minuscoli e aiuta ad abilitare funzionalità sul dispositivo come il riconoscimento delle immagini nei dispositivi smart [intelligenti]. Viene utilizzato nell’edge computing per consentire analisi e decisioni più rapide nei dispositivi che elaborano i dati localmente.\n\n\n\n\nDistBelief e la sua architettura definita sopra sono stati fondamentali per abilitare il deep learning distribuito in Google, ma hanno anche introdotto delle limitazioni che hanno motivato lo sviluppo di TensorFlow:\n\n\n6.3.2 Grafico di Calcolo Statico\nI parametri del modello sono distribuiti su vari server di parametri nell’architettura del server di parametri. Poiché DistBelief è stato progettato principalmente per il paradigma della rete neurale, i parametri corrispondevano a una struttura di rete neurale fissa. Se il computation graph [grafico di calcolo] fosse dinamico, la distribuzione e il coordinamento dei parametri diventerebbero significativamente più complicati. Ad esempio, una modifica nel grafico potrebbe richiedere l’inizializzazione di nuovi parametri o la rimozione di quelli esistenti, complicando le attività di gestione e sincronizzazione dei server di parametri. Ciò ha reso più difficile implementare modelli al di fuori del framework neurale o modelli che richiedevano grafici di calcolo dinamici.\nTensorFlow è stato progettato come un framework di calcolo più generale che esprime il calcolo come un grafico del flusso di dati. Ciò consente una più ampia varietà di modelli e algoritmi di apprendimento automatico al di fuori delle reti neurali e fornisce flessibilità nel perfezionamento dei modelli.\n\n\n6.3.3 Usabilità & Distribuzione\nIl modello del server dei parametri delinea i ruoli (nodi worker e server dei parametri) ed è ottimizzato per i deployment [distribuzioni] dei data center, che potrebbero essere ottimali solo per alcuni casi d’uso. Ad esempio, questa divisione introduce overhead o complessità sui dispositivi edge o in altri ambienti non data center.\nTensorFlow è stato creato per funzionare su più piattaforme, dai dispositivi mobili e edge all’infrastruttura cloud. Mirava anche a essere più leggero e intuitivo per gli sviluppatori e a fornire facilità d’uso tra il training locale e quello distribuito.\n\n\n6.3.4 Progettazione dell’Architettura\nInvece di utilizzare l’architettura del server dei parametri, TensorFlow distribuisce i task [attività] su un cluster. Queste attività sono processi denominati che possono comunicare su una rete e ciascuna può eseguire la struttura principale di TensorFlow, il grafico del flusso di dati e l’interfaccia con vari dispositivi di elaborazione (come CPU o GPU). Questo grafico [grafo] è una rappresentazione diretta in cui i nodi simboleggiano le operazioni di elaborazione e gli edge rappresentano i tensori (dati) che scorrono tra queste operazioni.\nNonostante l’assenza di server di parametri tradizionali, alcuni “task PS” memorizzano e gestiscono parametri che ricordano i server di parametri di altri sistemi. I task rimanenti, che di solito gestiscono calcoli, elaborazione dati e gradienti, sono denominati “task worker”. I task PS di TensorFlow possono eseguire qualsiasi calcolo rappresentabile dal grafico del flusso di dati, il che significa che non sono limitati solo all’archiviazione dei parametri e il calcolo può essere distribuito. Questa capacità li rende significativamente più versatili e offre agli utenti il potere di programmare i task PS utilizzando l’interfaccia TensorFlow standard, la stessa che userebbero per definire i loro modelli. Come accennato in precedenza, la struttura dei grafici del flusso di dati li rende anche intrinsecamente buoni per il parallelismo, consentendo l’elaborazione di grandi set di dati.\n\n\n6.3.5 Funzionalità Native & Keras\nTensorFlow include librerie per aiutare gli utenti a sviluppare e distribuire più modelli specifici per i casi d’uso e, poiché questo framework è open source, questo elenco continua a crescere. Queste librerie affrontano l’intero ciclo di vita dello sviluppo ML: preparazione dei dati, creazione di modelli, distribuzione e IA responsabile.\nUno dei maggiori vantaggi di TensorFlow è la sua integrazione con Keras, anche se, come vedremo nella prossima sezione, Pytorch ha recentemente aggiunto un’integrazione Keras. Keras è un altro framework ML creato per essere estremamente intuitivo e, di conseguenza, ha un alto livello di astrazione. Parleremo di Keras più approfonditamente più avanti in questo capitolo. Tuttavia, quando si discute della sua integrazione con TensorFlow, è importante notare che era stato originariamente creato per essere indipendente dal backend. Ciò significa che gli utenti potrebbero astrarre queste complessità, offrendo un modo più pulito e intuitivo per definire e addestrare modelli senza preoccuparsi di problemi di compatibilità con diversi backend. Gli utenti di TensorFlow hanno evidenziato alcuni problemi sull’usabilità e la leggibilità dell’API di TensorFlow, quindi, man mano che TF acquisiva importanza, ha integrato Keras come API di alto livello. Questa integrazione ha offerto grandi vantaggi agli utenti di TensorFlow poiché ha introdotto una leggibilità e una portabilità più intuitive dei modelli, sfruttando comunque le potenti funzionalità di backend, il supporto di Google e l’infrastruttura per distribuire i modelli su varie piattaforme.\n\n\n\n\n\n\nEsercizio 6.3: Esplorazione di Keras: Creazione, Addestramento e Valutazione di Reti Neurali\n\n\n\n\n\nQui, impareremo come utilizzare Keras, un’API di reti neurali di alto livello, per lo sviluppo e l’addestramento (training) di modelli. Esploreremo l’API funzionale per la creazione di modelli concisi, comprenderemo le classi “loss” e metriche per la valutazione dei modelli e utilizzeremo gli ottimizzatori nativi per aggiornare i parametri del modello durante l’addestramento. Inoltre, scopriremo come definire layer e metriche personalizzati su misura per le nostre esigenze. Infine, esamineremo i cicli di addestramento di Keras per semplificare il processo di addestramento delle reti neurali su grandi set di dati. Questa conoscenza ci consentirà di costruire e ottimizzare modelli di reti neurali in varie applicazioni di machine learning e intelligenza artificiale.\n\n\n\n\n\n\n6.3.6 Limitazioni e Sfide\nTensorFlow è uno dei framework di deep learning più popolari, ma ha dovuto affrontare critiche e debolezze, principalmente legate all’usabilità e all’utilizzo delle risorse. Sebbene vantaggioso, il ritmo rapido degli aggiornamenti tramite il supporto di Google ha talvolta portato a problemi di retrocompatibilità, funzioni deprecate e documentazione instabile. Inoltre, anche con l’implementazione di Keras, la sintassi e la curva di apprendimento di TensorFlow possono risultare difficili per i nuovi utenti. Un’altra critica importante di TensorFlow è il suo elevato overhead e consumo di memoria dovuto alla gamma di librerie integrate e al supporto. Sebbene le versioni ridotte possano risolvere alcuni di questi problemi, potrebbero comunque essere limitate in ambienti con risorse limitate.\n\n\n6.3.7 PyTorch & TensorFlow\nPyTorch e TensorFlow si sono affermati come leader nel settore. Entrambi i framework offrono funzionalità robuste ma differiscono per filosofie di progettazione, facilità d’uso, ecosistema e capacità di distribuzione.\nFilosofia di Progettazione e Paradigma di Programmazione: PyTorch utilizza un grafo computazionale dinamico denominato eager execution [esecuzione rapida]. Ciò lo rende intuitivo e facilita il debug poiché le operazioni vengono eseguite immediatamente e possono essere ispezionate al volo. Al contrario, le versioni precedenti di TensorFlow erano incentrate su un grafo computazionale statico, che richiedeva la definizione completa del grafico prima dell’esecuzione. Tuttavia, TensorFlow 2.0 ha introdotto la “eager execution” per default, rendendolo più allineato con PyTorch. La natura dinamica di PyTorch e l’approccio basato su Python hanno consentito la sua semplicità e flessibilità, in particolare per la prototipazione rapida. L’approccio grafico statico di TensorFlow nelle sue versioni precedenti aveva una curva di apprendimento più ripida; l’introduzione di TensorFlow 2.0, con la sua integrazione Keras come API di alto livello, ha semplificato notevolmente il processo di sviluppo.\nDeployment: PyTorch è fortemente favorito negli ambienti di ricerca, ma la distribuzione dei modelli PyTorch in contesti di produzione è sempre stata un problema. Tuttavia, la distribuzione è diventata più fattibile con l’introduzione di TorchScript, lo strumento TorchServe e PyTorch Mobile. TensorFlow si distingue per la sua forte scalabilità e capacità di distribuzione, in particolare su piattaforme embedded e mobili con TensorFlow Lite. TensorFlow Serving e TensorFlow.js facilitano ulteriormente la distribuzione in vari ambienti, conferendogli così una portata più ampia nell’ecosistema.\nPrestazioni: Entrambi i framework offrono un’accelerazione hardware efficiente per le loro operazioni. Tuttavia, TensorFlow ha un flusso di lavoro di ottimizzazione leggermente più robusto, come il compilatore XLA (Accelerated Linear Algebra), che può aumentare ulteriormente le prestazioni. Il suo grafo computazionale statico era anche vantaggioso per alcune ottimizzazioni nelle prime versioni.\nEcosistema: PyTorch ha un ecosistema in crescita con strumenti come TorchServe per servire modelli e librerie come TorchVision, TorchText e TorchAudio per domini specifici. Come abbiamo detto prima, TensorFlow ha un ecosistema ampio e maturo. TensorFlow Extended (TFX) fornisce una piattaforma end-to-end per distribuire pipeline di apprendimento automatico di produzione. Altri strumenti e librerie includono TensorFlow Lite, TensorFlow Lite Micro, TensorFlow.js, TensorFlow Hub e TensorFlow Serving. Tabella 6.1 fornisce un’analisi comparativa:\n\n\n\nTabella 6.1: Confronto tra PyTorch e TensorFlow.\n\n\n\n\n\n\n\n\n\n\nAspetto\nPytorch\nTensorFlow\n\n\n\n\nFilosofia di Progettazione\nGrafo computazionale dinamico (eager execution)\nGrafo computazionale statico (prime versioni); Esecuzione rapida in TensorFlow 2.0\n\n\nDeployment\nTradizionalmente impegnativa; Migliorata con TorchScript e TorchServe\nScalabile, specialmente su piattaforme embedded con TensorFlow Lite\n\n\nPrestazioni e Ottimizzazione\nAccelerazione GPU efficiente\nOttimizzazione robusta con compilatore XLA\n\n\nEcosistema\nTorchServe, TorchVision, TorchText, TorchAudio, PyTorch Mobile\nTensorFlow Extended (TFX), TensorFlow Lite, TensorFlow Lite Micro TensorFlow.js, TensorFlow Hub, TensorFlow Serving\n\n\nFacilità d’uso\nPreferito per il suo approccio Pythonic e la prototipazione rapida\nCurva di apprendimento inizialmente ripida; Semplificato con Keras in TensorFlow 2.0",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Framework di IA</span>"
    ]
  },
  {
    "objectID": "contents/core/frameworks/frameworks.it.html#componenti-di-base-del-framework",
    "href": "contents/core/frameworks/frameworks.it.html#componenti-di-base-del-framework",
    "title": "6  Framework di IA",
    "section": "6.4 Componenti di Base del Framework",
    "text": "6.4 Componenti di Base del Framework\nDopo aver introdotto i popolari framework di machine learning e aver fornito un confronto di alto livello, questa sezione presenterà le funzionalità principali che formano la struttura di questi framework. Tratterà la struttura speciale chiamata tensori, che questi framework utilizzano per gestire più facilmente dati multidimensionali complessi. Si vedrà anche come questi framework rappresentano diversi tipi di architetture di reti neurali e le loro operazioni richieste tramite grafi computazionali. Inoltre, si vedrà come offrono strumenti che rendono lo sviluppo di modelli di machine learning più astratto ed efficiente, come caricatori di dati, algoritmi di ottimizzazione delle perdite implementate, tecniche di differenziazione efficienti e la capacità di accelerare il processo di training su acceleratori hardware.\n\n6.4.1 Strutture Dati Tensoriali\nCome mostrato nella figura, i vettori possono essere rappresentati come una pila di numeri in un array unidimensionale. Le matrici seguono la stessa idea e si possono pensare a loro come a molti vettori impilati l’uno sull’altro, rendendoli bidimensionali. I tensori di dimensioni superiori funzionano allo stesso modo. Un tensore tridimensionale, come illustrato in Figura 6.4, è semplicemente un set di matrici impilate l’una sull’altra in un’altra direzione. Pertanto, vettori e matrici possono essere considerati casi speciali di tensori con dimensioni 1D e 2D, rispettivamente.\n\n\n\n\n\n\nFigura 6.4: Visualizzazione della Struttura Dati Tensore.\n\n\n\nI tensori offrono una struttura flessibile che può rappresentare dati in dimensioni superiori. Figura 6.5 illustra come questo concetto si applica ai dati immagine. Come mostrato nella figura, le immagini non sono rappresentate da una sola matrice di valori pixel. Invece, hanno in genere tre canali, dove ogni canale è una matrice contenente valori pixel che rappresentano l’intensità di rosso, verde o blu. Insieme, questi canali creano un’immagine colorata. Senza i tensori, archiviare tutte queste informazioni da più matrici può risultare complesso. Tuttavia, come illustra Figura 6.5, i tensori facilitano il contenimento dei dati dell’immagine in un’unica struttura tridimensionale, in cui ciascun numero rappresenta un determinato valore di colore in una posizione specifica nell’immagine.\n\n\n\n\n\n\nFigura 6.5: Visualizzazione della struttura dell’immagine colorata che può essere facilmente memorizzata come un Tensore 3D. Credito: Niklas Lang\n\n\n\nNon finisce qui. Se volessimo archiviare una serie di immagini, potremmo usare un tensore quadridimensionale, in cui la nuova dimensione rappresenta immagini diverse. Ciò significa che si stanno archiviando più immagini, ciascuna con tre matrici che rappresentano i tre canali del colore. Questo dà un’idea dell’utilità dei tensori quando si gestiscono dati multidimensionali in modo efficiente.\nI tensori hanno anche un attributo unico che consente ai framework di calcolare automaticamente i gradienti, semplificando l’implementazione di modelli complessi e algoritmi di ottimizzazione. Nel machine learning, come discusso nel Capitolo 3, la backpropagation richiede di prendere la derivata delle equazioni. Una delle caratteristiche principali dei tensori in PyTorch e TensorFlow è la loro capacità di tracciare i calcoli e calcolare i gradienti. Ciò è fondamentale per la backpropagation nelle reti neurali. Ad esempio, in PyTorch, si può usare l’attributo requires_grad, che consente di calcolare e memorizzare automaticamente i gradienti durante il “backward pass”, facilitando il processo di ottimizzazione. Analogamente, in TensorFlow, tf.GradientTape registra le operazioni per la differenziazione automatica.\nSi consideri questa semplice equazione matematica che si vuole differenziare. Matematicamente, il calcolo del gradiente si effettua nel modo seguente:\nDato: \\[\ny = x^2\n\\]\nLa derivata di \\(y\\) rispetto a \\(x\\) è: \\[\n\\frac{dy}{dx} = 2x\n\\]\nQuando \\(x = 2\\): \\[\n\\frac{dy}{dx} = 2*2 = 4\n\\]\nIl gradiente di \\(y\\) rispetto a \\(x\\), con \\(x = 2\\), è 4.\nUna potente caratteristica dei tensori in PyTorch e TensorFlow è la loro capacità di calcolare facilmente le derivate (gradienti). Ecco gli esempi di codice corrispondenti in PyTorch e TensorFlow:\n\nPyTorchTensorFlow\n\n\nimport torch\n\n# Create a tensor with gradient tracking\nx = torch.tensor(2.0, requires_grad=True)\n\n# Define a simple function\ny = x ** 2\n\n# Compute the gradient\ny.backward()\n\n# Print the gradient\nprint(x.grad)\n\n# Output\ntensor(4.0)\n\n\nimport tensorflow as tf\n\n# Create a tensor with gradient tracking\nx = tf.Variable(2.0)\n\n# Define a simple function\nwith tf.GradientTape() as tape:\n    y = x ** 2\n\n# Compute the gradient\ngrad = tape.gradient(y, x)\n\n# Print the gradient\nprint(grad)\n\n# Output\ntf.Tensor(4.0, shape=(), dtype=float32)\n\n\n\nQuesta differenziazione automatica è una potente funzionalità dei tensori in framework come PyTorch e TensorFlow, che semplifica l’implementazione e l’ottimizzazione di modelli complessi di apprendimento automatico.\n\n\n6.4.2 Grafi computazionali\n\nDefinizione di Grafico\nI grafi computazionali sono una componente chiave di framework di deep learning come TensorFlow e PyTorch. Ci consentono di esprimere architetture di reti neurali complesse in modo efficiente e differenziato. Un grafo computazionale è costituito da un grafo aciclico diretto (directed acyclic graph, DAG) in cui ogni nodo rappresenta un’operazione o una variabile e gli spigoli rappresentano le dipendenze dei dati tra di essi.\nÈ importante distinguere i grafici computazionali dai diagrammi delle reti neurali, come quelli dei “multilayer perceptrons (MLP)”, che rappresentano nodi e layer. I diagrammi di reti neurali, come illustrato nel Capitolo 3, visualizzano l’architettura e il flusso di dati attraverso nodi e layer, fornendo una comprensione intuitiva della struttura del modello. Al contrario, i grafi computazionali forniscono una rappresentazione di basso livello delle operazioni matematiche sottostanti e delle dipendenze dei dati necessarie per implementare e addestrare queste reti.\nAd esempio, un nodo potrebbe rappresentare un’operazione di moltiplicazione di matrici, prendendo due matrici di input (o tensori) e producendo una matrice di output (o tensore). Per visualizzarlo, si consideri il semplice esempio in Figura 6.6. Il grafo aciclico orientato calcola \\(z = x \\times y\\), dove ogni variabile è solo un numero.\n\n\n\n\n\n\nFigura 6.6: Esempio di base di un grafo computazionale.\n\n\n\nFramework come TensorFlow e PyTorch creano grafi computazionali per implementare le architetture delle reti neurali che in genere rappresentiamo con diagrammi. Quando si definisce un layer di rete neurale nel codice (ad esempio, un “layer denso” in TensorFlow), il framework costruisce un grafo computazionale che include tutte le operazioni necessarie (come moltiplicazione di matrici, addizione e funzioni di attivazione) e le relative dipendenze dai dati. Questo grafo consente al framework di gestire in modo efficiente il flusso di dati, ottimizzare l’esecuzione delle operazioni e calcolare automaticamente i gradienti per l’addestramento. Internamente, i grafi computazionali rappresentano astrazioni per layer comuni come quelli convoluzionali, di pooling, ricorrenti e densi, con dati che includono attivazioni, pesi e bias rappresentati in tensori. Questa rappresentazione consente un calcolo efficiente, sfruttando la struttura del grafico per parallelizzare le operazioni e applicare ottimizzazioni.\nAlcuni livelli comuni che i grafi computazionali potrebbero implementare includono layer convoluzionali, di attenzione, ricorrenti e densi. I layer fungono da astrazioni di livello superiore che definiscono calcoli specifici in cima alle operazioni di base rappresentate nel grafo. Ad esempio, un layer Denso esegue la moltiplicazione e l’addizione di matrici tra tensori di input, peso e bias. È importante notare che un layer opera su tensori come input e output; il layer stesso non è un tensore. Alcune differenze chiave tra layer e tensori sono:\n\nI layer contengono stati come pesi e bias. I tensori sono senza stato, contengono solo dati.\nI layer possono modificare lo stato interno durante l’addestramento. I tensori sono immutabili/di sola lettura.\nI layer sono astrazioni di livello superiore. I tensori sono a un livello inferiore e rappresentano direttamente dati e operazioni matematiche.\nI layer definiscono pattern di calcolo fissi. I tensori scorrono tra i livelli durante l’esecuzione.\nI layer vengono utilizzati indirettamente durante la creazione di modelli. I tensori scorrono tra i livelli durante l’esecuzione.\n\nQuindi, mentre i tensori sono una struttura dati fondamentale che i layer consumano e producono, i layer hanno funzionalità aggiuntive per definire operazioni parametrizzate e addestramento. Mentre un layer configura le operazioni tensoriali in background, il layer rimane distinto dagli oggetti tensoriali. L’astrazione del layer rende la creazione e l’addestramento di reti neurali molto più intuitive. Questa astrazione consente agli sviluppatori di creare modelli impilando insieme questi layer senza implementare la logica del layer. Ad esempio, la chiamata di tf.keras.layers.Conv2D in TensorFlow crea un layer convoluzionale. Il framework gestisce il calcolo delle convoluzioni, la gestione dei parametri, ecc. Ciò semplifica lo sviluppo del modello, consentendo agli sviluppatori di concentrarsi sull’architettura anziché sulle implementazioni di basso livello. Le astrazioni dei layer utilizzano implementazioni altamente ottimizzate per le prestazioni. Consentono inoltre la portabilità, poiché la stessa architettura può essere eseguita su backend hardware diversi come GPU e TPU.\nInoltre, i grafi computazionali includono funzioni di attivazione come ReLU, sigmoide e tanh che sono essenziali per le reti neurali e molti framework le forniscono come astrazioni standard. Queste funzioni introducono non linearità che consentono ai modelli di approssimare funzioni complesse. I framework le forniscono come operazioni semplici e predefinite che possono essere utilizzate durante la costruzione di modelli, ad esempio if.nn.relu in TensorFlow. Questa astrazione consente flessibilità, poiché gli sviluppatori possono facilmente scambiare le funzioni di attivazione per ottimizzare le prestazioni. Le attivazioni predefinite sono inoltre ottimizzate dal framework per un’esecuzione più rapida.\nNegli ultimi anni, modelli come ResNets e MobileNets sono emersi come architetture popolari, con i framework attuali che li pre-confezionano come grafi computazionali. Invece di preoccuparsi dei dettagli, gli sviluppatori possono utilizzarli come punto di partenza, personalizzandoli secondo necessità sostituendo i layer. Ciò semplifica e velocizza lo sviluppo del modello, evitando di reinventare le architetture da zero. I modelli predefiniti includono implementazioni ben collaudate e ottimizzate che garantiscono buone prestazioni. Il loro design modulare consente inoltre di trasferire le funzionalità apprese a nuove attività tramite apprendimento tramite trasferimento. Queste architetture predefinite forniscono i mattoni ad alte prestazioni per creare rapidamente modelli robusti.\nQueste astrazioni di layer, funzioni di attivazione e architetture predefinite fornite dai framework costituiscono un grafo computazionale. Quando un utente definisce un layer in un framework (ad esempio, tf.keras.layers.Dense()), il framework configura nodi e bordi del grafo computazionale per rappresentare tale layer. I parametri del layer come pesi e bias diventano variabili nel grafo. I calcoli del layer diventano nodi operativi (come x e y nella figura sopra). Quando si chiama una funzione di attivazione come tf.nn.relu(), il framework aggiunge un nodo operativo ReLU al grafo. Le architetture predefinite sono solo sottografi preconfigurati che possono essere inseriti nel grafo del modello. Quindi, la definizione del modello tramite astrazioni di alto livello crea un grafo computazionale: i livelli, le attivazioni e le architetture che utilizziamo diventano nodi e rami del grafo.\nCostruiamo implicitamente un grafo computazionale quando definiamo un’architettura di rete neurale in un framework. Il framework utilizza questo grafo per determinare le operazioni da eseguire durante l’addestramento e l’inferenza. I grafi computazionali offrono diversi vantaggi rispetto al codice grezzo e questa è una delle funzionalità principali offerte da un buon framework di ML:\n\nRappresentazione esplicita del flusso di dati e delle operazioni\nCapacità di ottimizzare il grafo prima dell’esecuzione\nDifferenziazione automatica per il training\nAgnosticismo linguistico: il grafo può essere tradotto per essere eseguito su GPU, TPU, ecc.\nPortabilità: il grafo può essere serializzato, salvato e ripristinato in seguito\n\nI grafi computazionali sono i componenti fondamentali dei framework di ML. La definizione del modello tramite astrazioni di alto livello crea un grafo computazionale: i layer, le attivazioni e le architetture che utilizziamo diventano nodi e rami del grafico. I compilatori e gli ottimizzatori del framework operano su questo grafo per generare codice eseguibile. Le astrazioni forniscono un’API intuitiva per gli sviluppatori per la creazione di grafi computazionali. Sotto, ci sono ancora grafi! Quindi, anche se non si possono manipolare direttamente i grafi come utente del framework, consentono di eseguire ad alto livello e in modo efficiente le specifiche del modello. Le astrazioni semplificano la creazione del modello, mentre i grafi computazionali la rendono possibile.\n\n\nGrafi Statici vs. Dinamici\nI framework di deep learning hanno tradizionalmente seguito uno dei due approcci per esprimere grafi computazionali.\nGrafi statici (declare-then-execute): Con questo modello, l’intero grafo computazionale deve essere definito in anticipo prima di eseguirlo. Tutte le operazioni e le dipendenze dei dati devono essere specificate durante la fase di dichiarazione. TensorFlow originariamente seguiva questo approccio statico: i modelli venivano definiti in un contesto separato e poi veniva creata una sessione per eseguirli. Il vantaggio dei grafi statici è che consentono un’ottimizzazione più aggressiva poiché il framework può vedere il grafo completo. Tuttavia, tende anche a essere meno flessibile per la ricerca e l’interattività. Le modifiche al grafo richiedono la nuova dichiarazione del modello completo.\nPer esempio:\nx = tf.placeholder(tf.float32)\ny = tf.matmul(x, weights) + biases\nIn questo esempio, x è un segnaposto per i dati di input e y è il risultato di un’operazione di moltiplicazione di matrici seguita da un’addizione. Il modello è definito in questa fase di dichiarazione, in cui tutte le operazioni e le variabili devono essere specificate in anticipo.\nUna volta definito l’intero grafo, il framework lo compila e lo ottimizza. Ciò significa che i passaggi computazionali sono definitivamente “scolpiti” e il framework può applicare varie ottimizzazioni per migliorare l’efficienza e le prestazioni. Quando in seguito si esegue il grafo, si forniscono i tensori di input effettivi e le operazioni predefinite vengono eseguite nella sequenza ottimizzata.\nQuesto approccio è simile alla creazione di un progetto in cui ogni dettaglio è pianificato prima dell’inizio della costruzione. Sebbene ciò consenta potenti ottimizzazioni, significa anche che qualsiasi modifica al modello richiede la ridefinizione dell’intero grafo da zero.\nGrafi dinamici (define-by-run): A differenza della dichiarazione (di tutto) prima e dell’esecuzione poi, il grafo viene creato dinamicamente durante l’esecuzione. Non esiste una fase di dichiarazione separata: le operazioni vengono eseguite immediatamente come definite. Questo stile è imperativo e flessibile, facilitando la sperimentazione.\nPyTorch utilizza grafi dinamici, creandoli al volo mentre avviene l’esecuzione. Ad esempio, si consideri il seguente frammento di codice, in cui il grafo viene creato durante l’esecuzione:\nx = torch.randn(4,784)\ny = torch.matmul(x, weights) + biases\nL’esempio sopra non ha fasi separate di compilazione/build/esecuzione. Le operazioni definiscono ed eseguono immediatamente. Con i grafi dinamici, la definizione è intrecciata con l’esecuzione, fornendo un flusso di lavoro più intuitivo e interattivo. Tuttavia, lo svantaggio è che c’è meno potenziale di ottimizzazione poiché il framework vede solo il grafo mentre viene creato. Figura 6.7 mostra le differenze tra un grafo di calcolo statico e uno dinamico.\n\n\n\n\n\n\nFigura 6.7: Confronto tra grafi statici e dinamici. Fonte: Dev\n\n\n\nDi recente, la distinzione si è offuscata poiché i framework adottano entrambe le modalità. TensorFlow 2.0 passa automaticamente alla modalità di grafo dinamico, consentendo agli utenti di lavorare con quelli statici quando necessario. La dichiarazione dinamica offre flessibilità e facilità d’uso, rendendo i framework più intuitivi, mentre i grafi statici forniscono vantaggi di ottimizzazione a scapito dell’interattività. Il framework ideale bilancia questi approcci. Tabella 6.2 confronta i pro e i contro dei grafi di esecuzione statici e dinamici:\n\n\n\nTabella 6.2: Confronto tra Grafi di Esecuzione Statici (Declare-then-execute) e Dinamici (Define-by-run), evidenziandone i rispettivi pro e contro.\n\n\n\n\n\n\n\n\n\n\nGrafo di esecuzione\nPro\nContro\n\n\n\n\nStatico (Declare-then-execute)\n\nAbilita le ottimizzazioni del grafo visualizzando il modello completo in anticipo\nPuò esportare e distribuire grafici congelati\nIl grafo è impacchettato indipendentemente dal codice\n\n\nMeno flessibile per la ricerca e l’iterazione\nLe modifiche richiedono la ricostruzione del grafo\nL’esecuzione ha fasi di compilazione ed esecuzione separate\n\n\n\nDinamico (Define-by-run)\n\nStile imperativo intuitivo come il codice Python\nAlterna la creazione del grafo con l’esecuzione\nFacile da modificare i grafi\nIl debug si adatta perfettamente al flusso di lavoro\n\n\nPiù difficile da ottimizzare senza un grafo completo\nPossibili rallentamenti dalla creazione del grafo durante l’esecuzione\nPuò richiedere più memoria\n\n\n\n\n\n\n\n\n\n\n6.4.3 Tool della Pipeline dei Dati\nI grafi computazionali possono essere validi solo quanto i dati da cui apprendono e su cui lavorano. Pertanto, alimentare i dati di training in modo efficiente è fondamentale per ottimizzare le prestazioni della “deep neural network” [rete neurale profonda], sebbene spesso venga trascurata come una delle funzionalità principali. Molti framework di IA moderni forniscono pipeline specializzate per acquisire, elaborare e aumentare i set di dati per il training del modello.\n\nData Loader\nAl centro di queste pipeline ci sono i “data loader”, che gestiscono esempi di training di lettura da fonti come file, database e storage di oggetti. I data loader facilitano il caricamento e la pre-elaborazione efficienti dei dati, cruciali per i modelli di deep learning. Ad esempio, la pipeline di caricamento dati tf.data di TensorFlow è progettata per gestire questo processo. A seconda dell’applicazione, i modelli di deep learning richiedono diversi formati di dati come file CSV o cartelle di immagini. Alcuni formati popolari includono:\n\nCSV: Un formato versatile e semplice spesso utilizzato per dati tabulari.\nTFRecord: Formato proprietario di TensorFlow, ottimizzato per le prestazioni.\nParquet: Archiviazione a colonne, che offre compressione e recupero dati efficienti.\nJPEG/PNG: Comunemente utilizzato per dati immagine.\nWAV/MP3: Formati prevalenti per dati audio.\n\nEsempi di batch di data loader per sfruttare il supporto di vettorizzazione nell’hardware. Il “batching” si riferisce al raggruppamento di più dati per l’elaborazione simultanea, sfruttando le capacità di calcolo vettorizzate di hardware come le GPU. Sebbene le dimensioni tipiche dei batch siano comprese tra 32 e 512 esempi, la dimensione ottimale spesso dipende dall’ingombro di memoria dei dati e dai vincoli hardware specifici. I loader avanzati possono trasmettere in streaming set di dati virtualmente illimitati da dischi e archivi cloud. Trasmettono in streaming grandi dataset da dischi o reti anziché caricarli completamente in memoria, consentendo dimensioni illimitate.\nI data loader possono anche mescolare i dati tra “epoche” per la randomizzazione e le funzionalità di preelaborazione in parallelo con l’addestramento del modello per accelerarne il processo. Mescolare casualmente l’ordine degli esempi tra epoche di training riduce il bias e migliora la generalizzazione.\nI data loader supportano anche strategie di “caching” e “prefetching” per ottimizzare la distribuzione dei dati per un addestramento del modello rapido e fluido. Il caching [memorizzazione nella cache] dei batch preelaborati consente di riutilizzarli in modo efficiente durante più fasi di addestramento ed elimina l’elaborazione ridondante. Il prefetching, al contrario, comporta il precaricamento dei batch successivi, assicurando che il modello non resti mai inattivo in attesa di dati.\n\n\n\n6.4.4 Data Augmentation\nFramework di apprendimento automatico come TensorFlow e PyTorch forniscono strumenti per semplificare e snellire il processo di “data augmentation” [aumento dei dati], migliorando l’efficienza dell’espansione sintetica dei set di dati. Questi framework offrono funzionalità integrate per applicare trasformazioni casuali, come capovolgimento, ritaglio, rotazione, modifica del colore e aggiunta di rumore per le immagini. Per i dati audio, gli aumenti comuni comportano la miscelazione di clip con rumore di fondo o la modulazione di velocità, tono e volume.\nIntegrando gli strumenti di “augmentation” nella pipeline di dati, i framework consentono di applicare queste trasformazioni al volo durante ogni epoca di addestramento. Questo approccio incrementa la variazione nella distribuzione dei dati di addestramento, riducendo così l’overfitting e migliorando la generalizzazione del modello. Figura 6.8 mostra i casi di overfitting e underfitting. L’uso di “data loader” performanti in combinazione con ampie capacità di “augmentation” consente ai professionisti di alimentare in modo efficiente set di dati massicci e vari alle reti neurali.\n\n\n\n\n\n\nFigura 6.8: Overfitting e underfitting. Fonte: Aquarium Learning\n\n\n\nQueste pipeline di dati “hands-off” rappresentano un miglioramento significativo in termini di usabilità e produttività. Consentono agli sviluppatori di concentrarsi maggiormente sull’architettura del modello e meno sulla manipolazione dei dati durante l’addestramento di modelli di deep learning.\n\n\n6.4.5 Funzioni Loss e Algoritmi di Ottimizzazione\nL’addestramento di una rete neurale è fondamentalmente un processo iterativo che cerca di minimizzare una funzione di loss [perdita]. L’obiettivo è di mettere a punto i pesi e i parametri del modello per produrre previsioni vicine alle vere etichette target. I framework di apprendimento automatico hanno notevolmente semplificato questo processo offrendo funzioni di loss [perdita] e algoritmi di ottimizzazione.\nI framework di apprendimento automatico forniscono funzioni di perdita implementate che sono necessarie per quantificare la differenza tra le previsioni del modello e i valori reali. Diversi set di dati richiedono una diversa funzione di perdita per funzionare correttamente, poiché tale funzione indica al computer l’“obiettivo” a cui mirare. Le funzioni di perdita comunemente utilizzate includono il “Mean Squared Error (MSE)” [errore quadratico medio] per le attività di regressione, la “Cross-Entropy Loss” per le attività di classificazione, e la Kullback-Leibler (KL) per i modelli probabilistici. Ad esempio, tf.keras.losses di TensorFlow contiene una serie di queste funzioni di perdita comunemente utilizzate.\nGli algoritmi di ottimizzazione vengono utilizzati per trovare in modo efficiente il set di parametri del modello che minimizzano la funzione di perdita, assicurando che il modello funzioni bene sui dati di training e si generalizzi a nuovi dati. I framework moderni sono dotati di implementazioni efficienti di diversi algoritmi di ottimizzazione, molti dei quali sono varianti della “discesa del gradiente” con metodi stocastici e tassi di apprendimento adattivo. Alcuni esempi di queste varianti sono Stochastic Gradient Descent, Adagrad, Adadelta e Adam. L’implementazione di tali varianti è fornita in tf.keras.optimizers. Ulteriori informazioni con esempi chiari sono disponibili nella sezione Training dell’IA.\n\n\n6.4.6 Supporto al Training del Modello\nÈ richiesta una fase di compilazione prima di addestrare un modello di rete neurale definito. Durante questa fase, l’architettura di alto livello della rete neurale viene trasformata in un formato eseguibile ottimizzato. Questo processo comprende diverse fasi. La prima fase consiste nel costruire il grafo computazionale, che rappresenta tutte le operazioni matematiche e il flusso di dati all’interno del modello. Ne abbiamo discusso in precedenza.\nDurante l’addestramento, l’attenzione è rivolta all’esecuzione del grafo computazionale. A ogni parametro all’interno del grafo, come pesi e bias, viene assegnato un valore iniziale. A seconda del metodo di inizializzazione scelto, questo valore potrebbe essere casuale o basato su una logica predefinita.\nIl passaggio critico successivo è l’allocazione della memoria. La memoria essenziale è riservata alle operazioni del modello sia su CPU che su GPU, garantendo un’elaborazione efficiente dei dati. Le operazioni del modello vengono poi mappate sulle risorse hardware disponibili, in particolare GPU o TPU, per accelerare l’elaborazione. Una volta completata la compilazione, il modello viene preparato per l’addestramento.\nIl processo di addestramento impiega vari strumenti per migliorare l’efficienza. L’elaborazione batch è comunemente utilizzata per massimizzare la produttività computazionale. Tecniche come la vettorizzazione consentono operazioni su interi array di dati anziché procedere elemento per elemento, il che aumenta la velocità. Ottimizzazioni come la “kernel fusion” (fare riferimento al capitolo Ottimizzazioni) amalgamano più operazioni in un’unica azione, riducendo al minimo il sovraccarico computazionale. Le operazioni possono anche essere segmentate in fasi, facilitando l’elaborazione simultanea di diversi mini-batch in varie parti.\nI framework eseguono costantemente il checkpoint dello stato, preservando le versioni intermedie del modello durante l’addestramento. Ciò garantisce che i progressi vengano recuperati in caso di interruzione e che l’addestramento possa essere ripreso dall’ultimo checkpoint. Inoltre, il sistema monitora attentamente le prestazioni del modello rispetto a un set di dati di convalida. Se il modello inizia a sovradimensionarsi (se le sue prestazioni sul set di convalida diminuiscono), l’addestramento viene automaticamente interrotto, conservando risorse computazionali e tempo.\nI framework di ML incorporano una combinazione di compilazione del modello, metodi di elaborazione batch avanzati e utilità come il checkpoint e l’arresto anticipato. Queste risorse gestiscono gli aspetti complessi delle prestazioni, consentendo ai professionisti di concentrarsi sullo sviluppo e l’addestramento del modello. Di conseguenza, gli sviluppatori sperimentano sia velocità che facilità quando utilizzano le capacità delle reti neurali.\n\n\n6.4.7 Validazione e Analisi\nDopo aver addestrato i modelli di deep learning, i framework forniscono utilità per valutare le prestazioni e ottenere informazioni sul funzionamento dei modelli. Questi strumenti consentono una sperimentazione e un debug disciplinati.\n\nMetriche di Valutazione\nI framework includono implementazioni di comuni metriche di valutazione per la convalida:\n\nAccuratezza: Frazione di previsioni corrette complessive. Sono ampiamente utilizzate per la classificazione.\nPrecisione: Delle previsioni positive, quante erano positive. Utile per set di dati sbilanciati.\nRichiamo: Dei positivi effettivi, quanti ne abbiamo previsti correttamente? Misura della Completezza.\nPunteggio F1: Media armonica di precisione e richiamo. Combina entrambe le metriche.\nAUC-ROC - Area sotto la curva ROC. Sono utilizzate per l’analisi della soglia di classificazione.\nMAP - Mean Average Precision. Valuta le previsioni classificate nel recupero/rilevamento.\nMatrice di Confusione: Matrice che mostra i veri positivi, i veri negativi, i falsi positivi e i falsi negativi. Fornisce una visione più dettagliata delle prestazioni di classificazione.\n\nQueste metriche quantificano le prestazioni del modello sui dati di convalida per il confronto.\n\n\nVisualizzazione\nGli strumenti di visualizzazione forniscono informazioni sui modelli:\n\nCurve di perdita: Tracciano la perdita di training e validazione nel tempo per individuare l’overfitting.\nLoss curves [Griglie di attivazione]: Illustrano le funzionalità apprese dai filtri convoluzionali.\nProjection [Proiezione]: Riduce la dimensionalità per una visualizzazione intuitiva.\nPrecision-recall curves [Curve di richiamo della precisione]: Valutano i compromessi di classificazione. Figura 6.9 mostra un esempio di una curva di precisione-richiamo.\n\n\n\n\n\n\n\nFigura 6.9: Lettura di una curva “precision-recall”. Fonte: AIM\n\n\n\nStrumenti come TensorBoard per TensorFlow e TensorWatch per PyTorch consentono metriche e visualizzazioni in tempo reale durante il training.\n\n\n\n6.4.8 Programmazione differenziabile\nI metodi di addestramento per il machine learning come la backpropagation si basano sulla modifica della funzione di perdita rispetto alla modifica dei pesi (che essenzialmente è la definizione di derivata). Pertanto, la capacità di addestrare rapidamente ed efficientemente grandi modelli di machine learning si basa sulla capacità del computer di prendere derivate. Ciò rende la programmazione differenziabile uno degli elementi più importanti di un framework di apprendimento automatico.\nPossiamo utilizzare quattro metodi principali per far sì che i computer prendano derivate. Innanzitutto, possiamo calcolare manualmente le derivate a mano e inserirle nel computer. Questo diventerebbe rapidamente un incubo con molti layer di reti neurali se dovessimo calcolare manualmente tutte le derivate nei passaggi di backpropagation. Un altro metodo è la differenziazione simbolica utilizzando sistemi di computer algebrici come Mathematica, che può introdurre un layer di inefficienza, poiché è necessario un livello di astrazione per prendere le derivate. Le derivate numeriche, la pratica di approssimare i gradienti utilizzando metodi di differenze finite, soffrono di molti problemi, tra cui elevati costi computazionali e dimensioni della griglia più grandi, che portano a molti errori. Ciò porta alla differenziazione automatica, che sfrutta le funzioni primitive che i computer utilizzano per rappresentare le operazioni per ottenere una derivata esatta. Con la differenziazione automatica, la complessità computazionale del calcolo del gradiente è proporzionale al calcolo della funzione stessa. Le complessità della differenziazione automatica non sono gestite dagli utenti finali al momento, ma le risorse per saperne di più possono essere trovate ampiamente, ad esempio qui. La differenziazione automatica e la programmazione differenziabile di oggi sono onnipresenti e vengono eseguite in modo efficiente e automatico dai moderni framework di machine learning.\n\n\n6.4.9 Accelerazione Hardware\nLa tendenza a formare e distribuire continuamente modelli di apprendimento automatico più grandi ha reso necessario il supporto dell’accelerazione hardware per le piattaforme di machine-learning. Figura 6.10 mostra il gran numero di aziende che offrono acceleratori hardware in diversi domini, come il machine learning “Very Low Power” e quello “Embedded”. I “deep layer” delle reti neurali richiedono molte moltiplicazioni di matrici, che attraggono hardware in grado di calcolare rapidamente e in parallelo tali operazioni. In questo panorama, due architetture hardware, GPU e TPU, sono emerse come scelte principali per l’addestramento di modelli di apprendimento automatico.\nL’uso di acceleratori hardware è iniziato con AlexNet, che ha aperto la strada a lavori futuri per utilizzare le GPU come acceleratori hardware per l’addestramento di modelli di visione artificiale. Le GPU, o “Graphics Processing Units” [unità di elaborazione grafica], eccellono nella gestione di molti calcoli contemporaneamente, il che le rende ideali per le operazioni matriciali centrali per l’addestramento delle reti neurali. La loro architettura, progettata per il rendering della grafica, è perfetta per le operazioni matematiche richieste nell’apprendimento automatico. Sebbene siano molto utili per le attività di apprendimento automatico e siano state implementate in molte piattaforme hardware, le GPU sono comunque di uso generale in quanto possono essere utilizzate per altre applicazioni.\nD’altro canto, le Tensor Processing Units (TPU) sono unità hardware progettate specificamente per le reti neurali. Si concentrano sull’operazione di “moltiplicazione e accumulazione” (MAC) e il loro hardware è costituito da una grande matrice hardware che contiene elementi che calcolano in modo efficiente l’operazione MAC. Questo concetto, chiamato systolic array architecture, è stato ideato da Kung e Leiserson (1979), ma ha dimostrato di essere una struttura utile per calcolare in modo efficiente i prodotti matriciali e altre operazioni all’interno delle reti neurali (come le convoluzioni).\n\nKung, Hsiang Tsung, e Charles E Leiserson. 1979. «Systolic arrays (for VLSI)». In Sparse Matrix Proceedings 1978, 1:256–82. Society for industrial; applied mathematics Philadelphia, PA, USA.\nSebbene le TPU possano ridurre drasticamente i tempi di addestramento, presentano anche degli svantaggi. Ad esempio, molte operazioni all’interno dei framework di apprendimento automatico (principalmente TensorFlow in questo caso, poiché la TPU si integra direttamente con esso) non sono supportate dalle TPU. Non possono inoltre supportare operazioni personalizzate dai framework di apprendimento automatico e la progettazione della rete deve essere strettamente allineata alle capacità hardware.\nOggi, le GPU NVIDIA dominano il training, supportate da librerie software come CUDA, cuDNN e TensorRT. I framework includono anche ottimizzazioni per massimizzare le prestazioni su questi tipi di hardware, come l’eliminazione di connessioni non importanti e la fusione di layer. La combinazione di queste tecniche con l’accelerazione hardware fornisce una maggiore efficienza. Per l’inferenza, l’hardware si sta spostando sempre di più verso ASIC e SoC ottimizzati. Le TPU di Google accelerano i modelli nei data center, mentre Apple, Qualcomm, la famiglia NVIDIA Jetson e altri ora producono chip “mobili” incentrati sull’intelligenza artificiale.\n\n\n\n\n\n\nFigura 6.10: Aziende che offrono acceleratori hardware di ML. Fonte: Gradient Flow.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Framework di IA</span>"
    ]
  },
  {
    "objectID": "contents/core/frameworks/frameworks.it.html#sec-ai_frameworks-advanced",
    "href": "contents/core/frameworks/frameworks.it.html#sec-ai_frameworks-advanced",
    "title": "6  Framework di IA",
    "section": "6.5 Funzionalità Avanzate",
    "text": "6.5 Funzionalità Avanzate\nOltre a fornire gli strumenti essenziali per il training di modelli di apprendimento automatico, i framework offrono anche funzionalità avanzate. Queste funzionalità includono la distribuzione del training su diverse piattaforme hardware, la facile messa a punto di grandi modelli pre-addestrati e l’esemplificazione del “federated learning”. L’implementazione di queste funzionalità in modo indipendente sarebbe altamente complessa e richiederebbe molte risorse, ma i framework semplificano questi processi, rendendo le tecniche avanzate di apprendimento automatico più accessibili.\n\n6.5.1 Training distribuito\nPoiché i modelli di apprendimento automatico sono diventati più grandi nel corso degli anni, è diventato essenziale per i modelli di grandi dimensioni utilizzare più nodi di elaborazione nel processo di training. Questo processo, l’apprendimento distribuito, ha consentito maggiori capacità di training, ma ha anche imposto problemi nell’implementazione.\nPossiamo considerare tre diversi modi per distribuire il lavoro di training dei modelli di apprendimento automatico su più nodi di elaborazione. Il partizionamento dei dati di input (o parallelismo dei dati) si riferisce a più processori che eseguono lo stesso modello su diverse partizioni di input. Questa è l’implementazione più semplice ed è disponibile per molti framework di machine learning. La distribuzione più impegnativa del lavoro è rappresentata dal parallelismo del modello, che si riferisce a più nodi di elaborazione che lavorano su parti diverse del modello, e dal parallelismo del modello pipelined, che si riferisce a più nodi di elaborazione che lavorano su diversi layer del modello sullo stesso input. Gli ultimi due menzionati qui sono aree di ricerca attive.\nI framework di ML che supportano l’apprendimento distribuito includono TensorFlow (tramite il suo modulo tf.distribute), PyTorch (tramite i suoi moduli torch.nn.DataParallel e torch.nn.DistributedDataParallel) e MXNet (tramite la sua API gluon).\n\n\n6.5.2 Conversione del Modello\nI modelli di machine learning hanno vari metodi per essere rappresentati e utilizzati in diversi framework e per diversi tipi di dispositivi. Ad esempio, un modello può essere convertito per essere compatibile con i framework di inferenza all’interno del dispositivo mobile. Il formato di default per i modelli TensorFlow sono i file di checkpoint contenenti pesi e architetture, necessari per riaddestrare i modelli. Tuttavia, i modelli vengono in genere convertiti nel formato TensorFlow Lite per la distribuzione mobile. TensorFlow Lite utilizza una rappresentazione compatta del “flat buffer” e ottimizzazioni per un’inferenza rapida su hardware mobile, eliminando tutto il bagaglio non necessario associato ai metadati di addestramento, come le strutture dei file di checkpoint.\nLe ottimizzazioni del modello come la quantizzazione (vedere il capitolo Ottimizzazioni) possono ottimizzare ulteriormente i modelli per architetture target come i dispositivi mobili. Ciò riduce la precisione di pesi e attivazioni a uint8 o a int8 per un ingombro ridotto e un’esecuzione più rapida con acceleratori hardware supportati. Per la quantizzazione post-training, il convertitore di TensorFlow gestisce automaticamente analisi e conversione.\nFramework come TensorFlow semplificano la distribuzione di modelli addestrati su dispositivi IoT mobili ed embedded tramite API di conversione semplici per il formato TFLite e la quantizzazione. La conversione pronta all’uso consente un’inferenza ad alte prestazioni su dispositivi mobili senza l’onere dell’ottimizzazione manuale. Oltre a TFLite, altri target comuni includono TensorFlow.js per la distribuzione Web, TensorFlow Serving per i servizi cloud e TensorFlow Hub per l’apprendimento tramite trasferimento. Le utility di conversione di TensorFlow gestiscono questi scenari per semplificare i flussi di lavoro end-to-end.\nUlteriori informazioni sulla conversione dei modelli in TensorFlow sono linkate qui.\n\n\n6.5.3 AutoML, No-Code/Low-Code ML\nIn molti casi, l’apprendimento automatico può avere una barriera d’ingresso relativamente alta rispetto ad altri campi. Per addestrare e distribuire con successo modelli, è necessario avere una comprensione critica di una varietà di discipline, dalla scienza dei dati (elaborazione dei dati, pulizia dei dati), strutture di modelli (ottimizzazione degli iperparametri, architettura delle reti neurali), hardware (accelerazione, elaborazione parallela) e altro a seconda del problema in questione. La complessità di questi problemi ha portato all’introduzione di framework come AutoML, che cerca di rendere “l’apprendimento automatico disponibile anche a chi non è esperto di apprendimento automatico” e di “automatizzare la ricerca nell’apprendimento automatico”. Hanno creato AutoWEKA, che aiuta nel complesso processo di selezione degli iperparametri, e Auto-sklearn e Auto-pytorch, un’estensione di AutoWEKA nelle popolari librerie sklearn e PyTorch.\nMentre questi sforzi per automatizzare parti delle attività di apprendimento automatico sono in corso, altri si sono concentrati sulla semplificazione dei modelli tramite l’implementazione di apprendimento automatico “no-code” [senza codice]/low-code [a basso codice], utilizzando un’interfaccia drag-and-drop con un’interfaccia utente di facile navigazione. Aziende come Apple, Google e Amazon hanno già creato queste piattaforme di facile utilizzo per consentire agli utenti di costruire modelli di apprendimento automatico che possono essere integrati nel loro ecosistema.\nQuesti passaggi per rimuovere le barriere all’ingresso continuano a democratizzare il machine learning, semplificano l’accesso per i principianti e semplificano il flusso di lavoro per gli esperti.\n\n\n6.5.4 Metodi di Apprendimento Avanzati\n\nIl Transfer Learning\nIl “transfer learning” è la pratica di utilizzare le conoscenze acquisite da un modello pre-addestrato per addestrare e migliorare le prestazioni di un modello per un’attività diversa. Ad esempio, modelli come MobileNet e ResNet vengono addestrati sul set di dati ImageNet. Per fare ciò, si può congelare il modello pre-addestrato, utilizzandolo come un estrattore di feature per addestrare un modello molto più piccolo costruito sopra l’estrazione di feature. Si può anche mettere a punto l’intero modello per adattarlo al nuovo compito. I framework di apprendimento automatico semplificano il caricamento di modelli pre-addestrati, il congelamento di layer specifici e l’addestramento di layer personalizzati in cima. Semplificano questo processo fornendo API intuitive e un facile accesso a grandi repository di modelli pre-addestrati.\nIl transfer learning, pur essendo potente, presenta delle sfide. Un problema significativo è la potenziale incapacità del modello modificato di svolgere le sue attività originali dopo il transfer learning. Per affrontare queste sfide, i ricercatori hanno proposto varie soluzioni. Ad esempio, Z. Li e Hoiem (2018) ha introdotto il concetto di “Imparare senza Dimenticare.” nel suo articolo “Learning without Forgetting”, che da allora è stato implementato nelle moderne piattaforme di machine learning. Figura 6.11 fornisce un’illustrazione semplificata del concetto di “transfer learning” [apprendimento per trasferimento]:\n\nLi, Zhizhong, e Derek Hoiem. 2018. «Learning without Forgetting». IEEE Trans. Pattern Anal. Mach. Intell. 40 (12): 2935–47. https://doi.org/10.1109/tpami.2017.2773081.\n\n\n\n\n\n\nFigura 6.11: Trasferimento dell’apprendimento. Fonte: Tech Target\n\n\n\nCome mostrato in Figura 6.11, l’apprendimento per trasferimento implica l’assunzione di un modello addestrato su un’attività (l’attività di origine) e l’adattamento per eseguire un’attività nuova e correlata (l’attività di target). Questo processo consente al modello di sfruttare le conoscenze acquisite dall’attività di origine, migliorando potenzialmente le prestazioni e riducendo i tempi di addestramento sull’attività di destinazione. Tuttavia, come accennato in precedenza, è necessario prestare attenzione per garantire che il modello non “dimentichi” la sua capacità di eseguire l’attività originale durante questo processo.\n\n\nIl Federated Learning\nIl “Federated learning” di McMahan et al. (2017) è una forma di elaborazione distribuita che prevede l’addestramento di modelli su dispositivi personali anziché la centralizzazione dei dati su un singolo server (Figura 12.6). Inizialmente, un modello globale di base viene addestrato su un server centrale per essere distribuito a tutti i dispositivi. Utilizzando questo modello di base, i dispositivi calcolano individualmente i gradienti e li inviano all’hub centrale. Intuitivamente, questo trasferisce i parametri del modello anziché i dati stessi. L’apprendimento federato migliora la privacy mantenendo i dati sensibili sui dispositivi locali e condividendo gli aggiornamenti del modello solo con un server centrale. Questo metodo è particolarmente utile quando si gestiscono dati sensibili o quando un’infrastruttura su larga scala non è praticabile.\n\nMcMahan, Brendan, Eider Moore, Daniel Ramage, Seth Hampson, e Blaise Agüera y Arcas. 2017. «Communication-Efficient Learning of Deep Networks from Decentralized Data». In Proceedings of the 20th International Conference on Artificial Intelligence and Statistics, AISTATS 2017, 20-22 April 2017, Fort Lauderdale, FL, USA, a cura di Aarti Singh e Xiaojin (Jerry) Zhu, 54:1273–82. Proceedings of Machine Learning Research. PMLR. http://proceedings.mlr.press/v54/mcmahan17a.html.\n\n\n\n\n\n\nFigura 6.12: Un approccio con server centralizzato al “federated learning”. Fonte: NVIDIA.\n\n\n\nTuttavia, il federated learning deve affrontare sfide come garantire l’accuratezza dei dati, gestire dati non-IID (independent and identically distributed) [indipendenti e distribuiti in modo identico], gestire la produzione di dati non bilanciata e superare il sovraccarico della comunicazione e l’eterogeneità dei dispositivi. Anche i problemi di privacy e sicurezza, come gli attacchi di inversione del gradiente, pongono sfide significative.\nI framework di apprendimento automatico semplificano l’implementazione dell’apprendimento federato fornendo gli strumenti e le librerie necessarie. Ad esempio, TensorFlow Federated (TFF) offre un framework open source per supportare l’apprendimento federato. TFF consente agli sviluppatori di simulare e implementare algoritmi di apprendimento federato, offrendo un core federato per operazioni di basso livello e API di alto livello per attività federate comuni. Si integra perfettamente con TensorFlow, consentendo l’uso di modelli e ottimizzatori TensorFlow in un ambiente federato. TFF supporta tecniche di aggregazione sicure per migliorare la privacy e consente la personalizzazione degli algoritmi di apprendimento federato. Sfruttando questi strumenti, gli sviluppatori possono distribuire in modo efficiente il training, perfezionare i modelli pre-addestrati e gestire le complessità intrinseche dell’apprendimento federato.\nSono stati sviluppati anche altri programmi open source come Flower per semplificare l’implementazione dell’apprendimento federato con vari framework di machine learning.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Framework di IA</span>"
    ]
  },
  {
    "objectID": "contents/core/frameworks/frameworks.it.html#specializzazione-del-framework",
    "href": "contents/core/frameworks/frameworks.it.html#specializzazione-del-framework",
    "title": "6  Framework di IA",
    "section": "6.6 Specializzazione del Framework",
    "text": "6.6 Specializzazione del Framework\nFinora abbiamo parlato in generale dei framework di ML. Tuttavia, in genere, i framework sono ottimizzati in base alle capacità computazionali e ai requisiti applicativi dell’ambiente target, che vanno dal cloud all’edge ai dispositivi minuscoli. La scelta del framework giusto è fondamentale in base all’ambiente target per la distribuzione. Questa sezione fornisce una panoramica dei principali tipi di framework di IA su misura per ambienti cloud, edge e TinyML per aiutare a comprendere le somiglianze e le differenze tra questi ecosistemi.\n\n6.6.1 Cloud\nI framework di IA basati su cloud presuppongono l’accesso a un’ampia potenza di calcolo, memoria e risorse di archiviazione nel cloud. In genere supportano sia il training che l’inferenza. I framework di IA basati su cloud sono adatti per applicazioni in cui i dati possono essere inviati al cloud per l’elaborazione, come servizi di IA basati su cloud, analisi di dati su larga scala e applicazioni Web. I framework di IA cloud più diffusi includono quelli che abbiamo menzionato in precedenza, come TensorFlow, PyTorch, MXNet, Keras, ecc. Questi framework utilizzano GPU, TPU, training distribuito e AutoML per fornire IA scalabile. Concetti come model serving, MLOps e AIOps sono correlati all’operatività dell’IA nel cloud. L’IA cloud alimenta servizi come Google Cloud AI e consente il “transfer learning” tramite modelli pre-addestrati.\n\n\n6.6.2 Edge\nI framework Edge AI sono pensati per distribuire modelli di IA su dispositivi IoT, smartphone e server edge. I framework Edge AI sono ottimizzati per dispositivi con risorse di calcolo moderate, bilanciando potenza e prestazioni. I framework Edge AI sono ideali per applicazioni che richiedono elaborazione in tempo reale o quasi reale, tra cui robotica, veicoli autonomi e dispositivi intelligenti. I principali framework Edge AI includono TensorFlow Lite, PyTorch Mobile, CoreML e altri. Impiegano ottimizzazioni come compressione del modello, quantizzazione ed architetture di reti neurali efficienti. Il supporto hardware include CPU, GPU, NPU e acceleratori come Edge TPU. Edge AI consente casi d’uso come visione mobile, riconoscimento vocale e rilevamento di anomalie in tempo reale.\n\n\n6.6.3 Embedded\nI framework TinyML sono specializzati per distribuire modelli AI su dispositivi con risorse estremamente limitate, in particolare microcontrollori e sensori all’interno dell’ecosistema IoT. I framework TinyML sono progettati per dispositivi con risorse limitate, enfatizzando memoria minima e consumo energetico. I framework TinyML sono specializzati per casi d’uso su dispositivi IoT con risorse limitate per applicazioni di manutenzione predittiva, riconoscimento dei gesti e monitoraggio ambientale. I principali framework TinyML includono TensorFlow Lite Micro, uTensor e ARM NN. Ottimizzano modelli complessi per adattarli a kilobyte di memoria tramite tecniche come l’addestramento consapevole della quantizzazione e la precisione ridotta. TinyML consente il rilevamento intelligente su dispositivi alimentati a batteria, consentendo l’apprendimento collaborativo tramite apprendimento federato. La scelta del framework implica il bilanciamento delle prestazioni del modello e dei vincoli computazionali della piattaforma target, che sia cloud, edge o TinyML. Tabella 6.3 confronta i principali framework di IA negli ambienti cloud, edge e TinyML:\n\n\n\nTabella 6.3: Confronto dei tipi di framework per Cloud AI, Edge AI e TinyML.\n\n\n\n\n\n\n\n\n\n\n\nTipo di framework\nEsempi\nTecnologie chiave\nCasi d’uso\n\n\n\n\nCloud AI\nTensorFlow, PyTorch, MXNet, Keras\nGPU, TPU, addestramento distribuito, AutoML, MLOps\nServizi cloud, app Web, analisi di big data\n\n\nEdge AI\nTensorFlow Lite, PyTorch Mobile, Core ML\nOttimizzazione del modello, compressione, quantizzazione, architetture NN efficienti\nApp mobili, sistemi autonomi, elaborazione in tempo reale\n\n\nTinyML\nTensorFlow Lite Micro, uTensor, ARM NN\nTraining consapevole della quantizzazione, precisione ridotta, ricerca di architettura neurale\nSensori IoT, dispositivi indossabili, manutenzione predittiva, riconoscimento dei gesti\n\n\n\n\n\n\nDifferenze principali:\n\nCloud AI sfrutta un’enorme potenza di calcolo per modelli complessi utilizzando GPU/TPU e training distribuito.\nEdge AI ottimizza i modelli per l’esecuzione locale su dispositivi edge con risorse limitate.\nTinyML adatta i modelli a una memoria estremamente bassa e calcola ambienti come i microcontrollori.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Framework di IA</span>"
    ]
  },
  {
    "objectID": "contents/core/frameworks/frameworks.it.html#sec-ai_frameworks_embedded",
    "href": "contents/core/frameworks/frameworks.it.html#sec-ai_frameworks_embedded",
    "title": "6  Framework di IA",
    "section": "6.7 Framework di IA Embedded",
    "text": "6.7 Framework di IA Embedded\n\n6.7.1 Vincoli di Risorse\nI sistemi embedded affrontano gravi limitazioni di risorse che pongono sfide uniche quando si distribuiscono modelli di machine learning rispetto alle piattaforme di elaborazione tradizionali. Ad esempio, le unità microcontrollore (MCU) comunemente utilizzate nei dispositivi IoT hanno spesso:\n\nRAM varia da decine di kilobyte a pochi megabyte. Il popolare MCU ESP8266 ha circa 80 KB di RAM a disposizione degli sviluppatori. Ciò contrasta con 8 GB o più su laptop e desktop tipici odierni.\nMemoria Flash varia da centinaia di kilobyte a pochi megabyte. Il microcontrollore Arduino Uno fornisce solo 32 KB di archiviazione del codice. I computer standard odierni hanno un’archiviazione su disco nell’ordine dei terabyte.\nPotenza di elaborazione da pochi MHz a circa 200 MHz. L’ESP8266 funziona a 80 MHz. Questo è di diversi ordini di grandezza più lento delle CPU multi-core multi-GHz nei server e nei laptop di fascia alta.\n\nQuesti vincoli rigorosi spesso rendono impossibile l’addestramento di modelli di apprendimento automatico direttamente sui microcontrollori. La RAM limitata impedisce la gestione di grandi set di dati per il training. L’uso di energia per l’addestramento esaurirebbe rapidamente anche i dispositivi alimentati a batteria. Al contrario, i modelli vengono addestrati su sistemi ricchi di risorse e distribuiti su microcontrollori per un’inferenza ottimizzata. Ma anche l’inferenza pone delle sfide:\n\nDimensioni del Modello: I modelli di intelligenza artificiale sono troppo grandi per adattarsi a dispositivi IoT ed embedded. Ciò richiede tecniche di compressione del modello, come quantizzazione, potatura e “knowledge distillation” [distillazione della conoscenza]. Inoltre, come vedremo, molti dei framework utilizzati dagli sviluppatori di intelligenza artificiale hanno grandi quantità di overhead e librerie integrate che i sistemi embedded non possono supportare.\nComplessità delle Attività: Con solo decine di KB o pochi MB di RAM, i dispositivi IoT e i sistemi embedded sono limitati nella complessità delle attività che possono gestire. Le attività che richiedono grandi set di dati o algoritmi sofisticati, ad esempio LLM, che verrebbero eseguiti senza problemi su piattaforme di elaborazione tradizionali potrebbero non essere fattibili su sistemi embedded senza compressione o altre tecniche di ottimizzazione a causa delle limitazioni di memoria.\nArchiviazione ed Elaborazione dei Dati: I sistemi embedded spesso elaborano i dati in tempo reale e potrebbero archiviarne solo piccole quantità localmente. Al contrario, i sistemi di elaborazione tradizionali possono contenere ed elaborare grandi set di dati in memoria, consentendo un’analisi più rapida delle operazioni sui dati e aggiornamenti in tempo reale.\nSicurezza e Privacy: La poca memoria limita anche la complessità degli algoritmi e dei protocolli di sicurezza, la crittografia dei dati, le protezioni da reverse engineering e altro che può essere implementato sul dispositivo. Ciò potrebbe rendere alcuni dispositivi IoT più vulnerabili agli attacchi.\n\nDi conseguenza, le ottimizzazioni software specializzate e i framework ML su misura per i microcontrollori devono funzionare entro questi stretti limiti delle risorse. Tecniche di ottimizzazione intelligenti come quantizzazione, potatura e distillazione della conoscenza comprimono i modelli per adattarli alla memoria limitata (vedere la sezione Ottimizzazioni). Gli insegnamenti tratti dalla ricerca di architettura neurale aiutano a guidare la progettazione dei modelli.\nI miglioramenti hardware come gli acceleratori ML dedicati sui microcontrollori aiutano anche ad alleviare i vincoli. Ad esempio, Hexagon DSP di Qualcomm accelera i modelli TensorFlow Lite sui chip mobili Snapdragon. Google Edge TPU racchiude le prestazioni ML in un piccolo ASIC per dispositivi edge. ARM Ethos-U55 offre un’inferenza efficiente sui microcontrollori di classe Cortex-M. Questi chip ML personalizzati sbloccano funzionalità avanzate per applicazioni con risorse limitate.\nA causa della potenza di elaborazione limitata, è quasi sempre impossibile addestrare modelli di intelligenza artificiale su IoT o sistemi embedded. Invece, i modelli vengono addestrati su potenti computer tradizionali (spesso con GPU) e poi distribuiti sul dispositivo embedded per l’inferenza. TinyML si occupa specificamente di questo, assicurando che i modelli siano sufficientemente leggeri per l’inferenza in tempo reale su questi dispositivi limitati.\n\n\n6.7.2 Framework e Librerie\nI framework di intelligenza artificiale embedded sono strumenti software e librerie progettati per abilitare funzionalità di intelligenza artificiale e ML su sistemi embedded. Questi framework sono essenziali per portare l’intelligenza artificiale su dispositivi IoT, robotica e altre piattaforme di edge computing e sono progettati per funzionare dove risorse di elaborazione, memoria e consumo energetico sono limitati.\n\n\n6.7.3 Sfide\nSebbene i sistemi embedded rappresentino un’enorme opportunità per l’implementazione dell’apprendimento automatico per abilitare capacità intelligenti in edge, questi ambienti con risorse limitate pongono sfide significative. A differenza dei tipici ambienti cloud o desktop ricchi di risorse computazionali, i dispositivi embedded introducono gravi limitazioni in termini di memoria, potenza di elaborazione, efficienza energetica e hardware specializzato. Di conseguenza, le tecniche e i framework di apprendimento automatico esistenti progettati per cluster di server con risorse abbondanti non si traducono direttamente nei sistemi embedded. Questa sezione svela alcune delle sfide e delle opportunità per i sistemi embedded e i framework ML.\n\nEcosistema Frammentato\nLa mancanza di un framework ML unificato ha portato a un ecosistema altamente frammentato. Gli ingegneri di aziende come STMicroelectronics, NXP Semiconductors e Renesas hanno dovuto sviluppare soluzioni personalizzate su misura per le loro specifiche architetture di microcontrollori e DSP. Questi framework ad hoc richiedevano un’ampia ottimizzazione manuale per ogni piattaforma hardware di basso livello. Ciò ha reso estremamente difficile il porting dei modelli, richiedendo la riqualificazione per nuove architetture Arm, RISC-V o proprietarie.\n\n\nEsigenze Hardware Disparate\nSenza un framework condiviso, non esisteva un modo standard per valutare le capacità dell’hardware. Fornitori come Intel, Qualcomm e NVIDIA crearono soluzioni integrate, combinando modelli e migliorando software e hardware. Ciò rese difficile discernere i motivi del guadagni di prestazioni, se fosse merito dei nuovi progetti di chip come i core x86 a basso consumo di Intel o le ottimizzazioni software. Era necessario un framework standard affinché i fornitori potessero valutare le capacità del loro hardware in modo equo e riproducibile.\n\n\nMancanza di Portabilità\nCon strumenti standardizzati, adattare modelli addestrati in framework comuni come TensorFlow o PyTorch per funzionare in modo efficiente sui microcontrollori era più facile. Richiedeva una traduzione manuale dispendiosa, in termini di tempo, dei modelli per l’esecuzione su DSP specializzati di aziende come CEVA o core Arm M-series a basso consumo. Nessuno strumento immediato consentiva l’implementazione portatile su diverse architetture.\n\n\nInfrastruttura Incompleta\nL’infrastruttura per supportare i flussi di lavoro di sviluppo dei modelli chiave doveva essere migliorata. È necessario un maggiore supporto per le tecniche di compressione per adattare modelli di grandi dimensioni a budget di memoria limitati. Mancavano strumenti per la quantizzazione per ridurre la precisione per un’inferenza più rapida. Le API standardizzate per l’integrazione nelle applicazioni erano incomplete. Mancavano funzionalità essenziali come il debugging sul dispositivo, le metriche e la profilazione delle prestazioni. Queste lacune hanno aumentato i costi e la difficoltà dello sviluppo ML embedded.\n\n\nNessun Benchmark Standard\nSenza benchmark unificati, non esisteva un modo standard per valutare e confrontare le capacità di diverse piattaforme hardware di fornitori come NVIDIA, Arm e Ambiq Micro. Le valutazioni esistenti si basavano su benchmark proprietari pensati per mostrare i punti di forza di specifici chip. Ciò rendeva impossibile misurare i miglioramenti hardware in modo oggettivo, imparziale e imparziale. Il capitolo Benchmarking dell’IA affronta questo argomento in modo più dettagliato.\n\n\nTest Minimi del Mondo Reale\nGran parte dei benchmark si basava su dati sintetici. Testare rigorosamente i modelli su applicazioni embedded nel mondo reale era difficile senza set di dati e benchmark standardizzati, sollevando dubbi su come le dichiarazioni sulle prestazioni si sarebbero tradotte in un utilizzo nel mondo reale. Erano necessari test più approfonditi per convalidare i chip in casi di utilizzo reali.\nLa mancanza di framework e infrastrutture condivisi ha rallentato l’adozione di TinyML, ostacolandone l’integrazione nei prodotti embedded. I recenti framework standard hanno iniziato ad affrontare questi problemi attraverso una migliore portabilità, profilazione delle prestazioni e supporto per il benchmarking. Tuttavia, è ancora necessaria un’innovazione continua per consentire un’implementazione fluida e conveniente dell’IA nei dispositivi edge.\n\n\nRiepilogo\nL’assenza di framework, benchmark e infrastrutture standardizzati per ML embedded ne ha tradizionalmente ostacolato l’adozione. Tuttavia, sono stati compiuti recenti progressi nello sviluppo di framework condivisi come TensorFlow Lite Micro e suite di benchmark come MLPerf Tiny che mirano ad accelerare la proliferazione di soluzioni TinyML. Tuttavia, superare la frammentazione e la difficoltà dell’implementazione embedded rimane un processo in corso.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Framework di IA</span>"
    ]
  },
  {
    "objectID": "contents/core/frameworks/frameworks.it.html#esempi",
    "href": "contents/core/frameworks/frameworks.it.html#esempi",
    "title": "6  Framework di IA",
    "section": "6.8 Esempi",
    "text": "6.8 Esempi\nIl deployment [distribuzione] di machine learning su microcontrollori e altri dispositivi embedded richiede spesso librerie software e framework appositamente ottimizzati per funzionare entro vincoli rigorosi di memoria, elaborazione e potenza. Esistono diverse opzioni per eseguire l’inferenza su hardware con risorse limitate, ciascuna con il proprio approccio all’ottimizzazione dell’esecuzione del modello. Questa sezione esplorerà le caratteristiche chiave e i principi di progettazione alla base di TFLite Micro, TinyEngine e CMSIS-NN, fornendo informazioni su come ogni framework affronta il complesso problema dell’esecuzione di reti neurali molto accurata ma efficiente sui microcontrollori. Mostrerà inoltre diversi approcci per l’implementazione di framework TinyML efficienti.\nTabella 6.4 riassume le principali differenze e somiglianze tra questi tre framework di inferenza di apprendimento automatico specializzati per sistemi embedded e microcontrollori.\n\n\n\nTabella 6.4: Confronto dei framework: TensorFlow Lite Micro, TinyEngine e CMSIS-NN\n\n\n\n\n\n\n\n\n\n\n\nFramework\nTensorFlow Lite Micro\nTinyEngine\nCMSIS-NN\n\n\n\n\nApproccio\nBasato su interprete\nCompilazione statica\nKernel di reti neurali ottimizzati\n\n\nFocus sull’hardware\nDispositivi embedded generali\nMicrocontrollori\nProcessori ARM Cortex-M\n\n\nSupporto aritmetico\nVirgola mobile\nVirgola mobile, virgola fissa\nVirgola mobile, virgola fissa\n\n\nSupporto del modello\nModelli di rete neurale generale\nModelli co-progettati con TinyNAS\nTipi di livelli di rete neurale comuni\n\n\nImpronta del codice\nPiù grande grazie all’inclusione di interprete e operazioni\nPiccola, include solo le operazioni necessarie per il modello\nNativamente leggera\n\n\nLatenza\nPiù alta grazie a overhead di interpretazione\nMolto bassa grazie al modello compilato\nfocalizzato sulla bassa latenza\n\n\nGestione della memoria\nGestita dinamicamente da interprete\nOttimizzazione a livello di modello\nStrumenti per un’allocazione efficiente\n\n\nApproccio di ottimizzazione\nAlcune funzionalità di e generazione del codice\nKernel specializzati, fusione di operatori\nOttimizzazioni di assemblaggio specifiche dell’architettura\n\n\nPrincipali vantaggi\nFlessibilità, portabilità, facile aggiornamento dei modelli\nMassimizza le prestazioni, ottimizza l’utilizzo della memoria\nAccelerazione hardware, API standardizzata, portabilità\n\n\n\n\n\n\nNe comprenderemo ciascuno in modo più dettagliato nelle sezioni seguenti.\n\n6.8.1 Interprete\nTensorFlow Lite Micro (TFLM) è un framework di inferenza di apprendimento automatico progettato per dispositivi embedded con risorse limitate. Utilizza un interprete per caricare ed eseguire modelli di apprendimento automatico, il che fornisce flessibilità e facilità di aggiornamento dei modelli sul campo (David et al. 2021).\n\nDavid, Robert, Jared Duke, Advait Jain, Vijay Janapa Reddi, Nat Jeffries, Jian Li, Nick Kreeger, et al. 2021. «Tensorflow lite micro: Embedded machine learning for tinyml systems». Proceedings of Machine Learning and Systems 3: 800–811.\nGli interpreti tradizionali spesso hanno un overhead di branching [diramazione] significativo, che può ridurre le prestazioni. Tuttavia, l’interpretazione del modello di machine learning trae vantaggio dall’efficienza dei kernel di lunga durata, in cui ogni runtime del kernel è relativamente grande e aiuta a mitigare l’overhead dell’interprete.\nUn’alternativa a un motore di inferenza basato su interprete è quella di generare codice nativo da un modello durante l’esportazione. Ciò può migliorare le prestazioni, ma sacrifica portabilità e flessibilità, poiché il codice generato deve essere ricompilato per ogni piattaforma target e deve essere sostituito completamente per modificare un modello.\nTFLM bilancia la semplicità della compilazione del codice e la flessibilità di un approccio basato su interprete includendo alcune funzionalità di generazione del codice. Ad esempio, la libreria può essere costruita esclusivamente da file sorgenti, offrendo gran parte della semplicità della compilazione associata alla generazione di codice, pur mantenendo i vantaggi di un framework che esegue il modello interpretandolo.\nUn approccio basato su interprete offre diversi vantaggi rispetto alla generazione di codice per l’inferenza di apprendimento automatico su dispositivi embedded:\n\nFlessibilità: I modelli possono essere aggiornati sul campo senza ricompilare l’intera applicazione.\nPortabilità: L’interprete può essere utilizzato per eseguire modelli su diverse piattaforme target senza dover effettuare il porting del codice.\nEfficienza della Memoria: L’interprete può condividere il codice su più modelli, riducendo l’utilizzo della memoria.\nFacilità di sviluppo: Gli interpreti sono più facili da sviluppare e gestire rispetto ai generatori di codice.\n\nTensorFlow Lite Micro è un framework potente e flessibile per l’inferenza di apprendimento automatico su dispositivi embedded. Il suo approccio basato su interprete offre diversi vantaggi rispetto alla generazione di codice, tra cui flessibilità, portabilità, efficienza della memoria e facilità di sviluppo.\n\n\n6.8.2 Basati su Compilatore\nTinyEngine è un framework di inferenza ML progettato specificamente per microcontrollori con risorse limitate. Utilizza diverse ottimizzazioni per consentire l’esecuzione di reti neurali molto accurate entro i vincoli rigorosi di memoria, elaborazione e archiviazione sui microcontrollori (Lin et al. 2020).\n\nLin, Ji, Wei-Ming Chen, Yujun Lin, John Cohn, Chuang Gan, e Song Han. 2020. «MCUNet: Tiny Deep Learning on IoT Devices». In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, a cura di Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, e Hsuan-Tien Lin. https://proceedings.neurips.cc/paper/2020/hash/86c51678350f656dcc7f490a43946ee5-Abstract.html.\nMentre framework di inferenza come TFLite Micro utilizzano interpreti per eseguire il grafo della rete neurale in modo dinamico in fase di esecuzione, ciò aggiunge un overhead significativo per quanto riguarda l’utilizzo della memoria per archiviare metadati, latenza di interpretazione e mancanza di ottimizzazioni. Tuttavia, TFLite sostiene che l’overhead è piccolo. TinyEngine elimina questo overhead utilizzando un approccio di generazione del codice. Analizza il grafo di rete durante la compilazione e genera codice specializzato per eseguire solo quel modello. Questo codice viene compilato in modo nativo nel binario dell’applicazione, evitando i costi di interpretazione in fase di esecuzione.\nI framework ML convenzionali pianificano la memoria per layer, cercando di ridurre al minimo l’utilizzo per ogni layer separatamente. TinyEngine esegue la pianificazione a livello di modello anziché analizzare l’utilizzo della memoria tra i layer. Assegna una dimensione di buffer comune in base alle esigenze massime di memoria di tutti i layer. Questo buffer viene quindi condiviso in modo efficiente tra i layer per aumentare il riutilizzo dei dati.\nTinyEngine è inoltre specializzato nei kernel per ogni layer tramite tecniche come operatori di tiling, unrolling e fusing. Ad esempio, genererà kernel di calcolo unrolled [srotolato] con il numero di loop necessari per una convoluzione 3x3 o 5x5. Questi kernel specializzati estraggono le massime prestazioni dall’hardware del microcontrollore. Utilizza convoluzioni depthwise [in profondità] ottimizzate per ridurre al minimo le allocazioni di memoria calcolando l’output di ogni canale posizionato sui dati del canale di input. Questa tecnica sfrutta la natura separabile dei canali delle convoluzioni depthwise per ridurre le dimensioni di picco della memoria.\nCome TFLite Micro, il binario TinyEngine compilato include solo le operazioni necessarie per un modello specifico anziché tutte le operazioni possibili. Ciò si traduce in un footprint binario molto piccolo, mantenendo basse le dimensioni del codice per i dispositivi con limiti di memoria.\nUna differenza tra TFLite Micro e TinyEngine è che quest’ultimo è co-progettato con “TinyNAS”, un metodo di ricerca di architettura per modelli di microcontrollori simile al NAS differenziale per microcontrollori. L’efficienza di TinyEngine consente di esplorare modelli più grandi e accurati tramite NAS. Fornisce inoltre feedback a TinyNAS su quali modelli possono rientrare nei vincoli hardware.\nAttraverso varie tecniche personalizzate, come la compilazione statica, la pianificazione basata sul modello, kernel specializzati e la co-progettazione con NAS, TinyEngine consente un’inferenza di deep learning ad alta precisione entro i vincoli di risorse rigorosi dei microcontrollori.\n\n\n6.8.3 Libreria\nCMSIS-NN, acronimo di Cortex Microcontroller Software Interface Standard for Neural Networks, è una libreria software ideata da ARM. Offre un’interfaccia standardizzata per distribuire l’inferenza di reti neurali su microcontrollori e sistemi embedded, concentrandosi sull’ottimizzazione per i processori ARM Cortex-M (Lai, Suda, e Chandra 2018).\n\nLai, Liangzhen, Naveen Suda, e Vikas Chandra. 2018. «Cmsis-nn: Efficient neural network kernels for arm cortex-m cpus». ArXiv preprint abs/1801.06601. https://arxiv.org/abs/1801.06601.\nKernel di Reti Neurali: CMSIS-NN ha kernel altamente efficienti che gestiscono operazioni fondamentali di reti neurali come convoluzione, pooling, layer completamente connessi e funzioni di attivazione. Si rivolge a un’ampia gamma di modelli di reti neurali supportando l’aritmetica a virgola mobile e fissa. Quest’ultima è particolarmente utile per i dispositivi con risorse limitate in quanto riduce i requisiti di memoria e di calcolo (Quantization).\nAccelerazione Hardware: CMSIS-NN sfrutta la potenza delle istruzioni SIMD (Single Instruction, Multiple Data) disponibili su molti processori Cortex-M. Ciò consente l’elaborazione parallela di più elementi di dati all’interno di una singola istruzione, aumentando così l’efficienza computazionale. Alcuni processori Cortex-M dispongono di estensioni di Digital Signal Processing (DSP) che CMSIS-NN può sfruttare per l’esecuzione accelerata della rete neurale. La libreria include anche ottimizzazioni a livello di assembly su misura per specifiche architetture di microcontrollori per migliorare ulteriormente le prestazioni.\nAPI standardizzata: CMSIS-NN offre un’API coerente e astratta che protegge gli sviluppatori dalle complessità dei dettagli hardware di basso livello. Ciò semplifica l’integrazione dei modelli di rete neurale nelle applicazioni. Può anche comprendere strumenti o utilità per convertire i formati di modelli di rete neurale più diffusi in un formato compatibile con CMSIS-NN.\nGestione della Memoria: CMSIS-NN fornisce funzioni per un’allocazione e una gestione efficienti della memoria, il che è fondamentale nei sistemi embedded in cui le risorse di memoria sono scarse. Garantisce un utilizzo ottimale della memoria durante l’inferenza e, in alcuni casi, consente operazioni in loco per ridurre il sovraccarico di memoria.\nPortabilità: CMSIS-NN è progettato per la portabilità su vari processori Cortex-M. Questo consente agli sviluppatori di scrivere codice che possa funzionare su diversi microcontrollori senza modifiche significative.\nBassa Latenza: CMSIS-NN riduce al minimo la latenza di inferenza, rendendolo una scelta ideale per applicazioni in tempo reale in cui è fondamentale prendere decisioni rapide.\nEfficienza Energetica: La libreria è progettata con un focus sull’efficienza energetica, rendendola adatta per dispositivi alimentati a batteria e con vincoli energetici.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Framework di IA</span>"
    ]
  },
  {
    "objectID": "contents/core/frameworks/frameworks.it.html#scelta-del-framework-giusto",
    "href": "contents/core/frameworks/frameworks.it.html#scelta-del-framework-giusto",
    "title": "6  Framework di IA",
    "section": "6.9 Scelta del Framework Giusto",
    "text": "6.9 Scelta del Framework Giusto\nLa scelta del framework di machine learning giusto per una determinata applicazione richiede un’attenta valutazione di modelli, hardware e considerazioni software. Figura 6.13 fornisce un confronto tra diversi framework TensorFlow, che discuteremo più in dettaglio:\n\n\n\n\n\n\nFigura 6.13: Confronto tra Framework TensorFlow - Generale. Fonte: TensorFlow.\n\n\n\nAnalizzando questi tre aspetti, modelli, hardware e software, come illustrato in Figura 6.13, gli ingegneri ML possono selezionare il framework ottimale e personalizzarlo in base alle esigenze per applicazioni ML su dispositivo efficienti e performanti. L’obiettivo è bilanciare complessità del modello, limitazioni hardware e integrazione software per progettare una pipeline ML su misura per dispositivi embedded e edge. Mentre esaminiamo le differenze mostrate in Figura 6.13, acquisiremo informazioni su come scegliere il framework giusto e comprenderemo cosa causa le variazioni tra i framework.\n\n6.9.1 Modello\nFigura 6.13 illustra le principali differenze tra le varianti di TensorFlow, in particolare in termini di operazioni supportate (op) e funzionalità. TensorFlow supporta molte più operazioni rispetto a TensorFlow Lite e TensorFlow Lite Micro, poiché viene in genere utilizzato per la ricerca o l’implementazione cloud, che richiedono un numero elevato di operatori e una maggiore flessibilità.\nLa figura dimostra chiaramente questa differenza nel supporto op tra i framework. TensorFlow Lite supporta operazioni selezionate per il training sul dispositivo, mentre TensorFlow Micro no. Inoltre, la figura mostra che TensorFlow Lite supporta forme dinamiche e training consapevole della quantizzazione, funzionalità assenti in TensorFlow Micro. Al contrario, sia TensorFlow Lite che TensorFlow Micro offrono strumenti e supporto di quantizzazione nativi. Qui, la quantizzazione si riferisce alla trasformazione di un programma ML in una rappresentazione approssimata con operazioni di precisione inferiore disponibili, una funzionalità cruciale per dispositivi embedded e edge con risorse computazionali limitate.\n\n\n6.9.2 Software\nCome mostrato in Figura 6.14, TensorFlow Lite Micro non supporta il sistema operativo, mentre TensorFlow e TensorFlow Lite sì. Questa scelta di progettazione per TensorFlow Lite Micro aiuta a ridurre il sovraccarico di memoria, a rendere i tempi di avvio più rapidi e a consumare meno energia. Invece, TensorFlow Lite Micro può essere utilizzato insieme a sistemi operativi in tempo reale (RTOS) come FreeRTOS, Zephyr e Mbed OS.\nLa figura evidenzia anche un’importante funzionalità di gestione della memoria: TensorFlow Lite e TensorFlow Lite Micro supportano la mappatura della memoria del modello, consentendo l’accesso diretto ai modelli dall’archiviazione flash anziché caricarli nella RAM. Al contrario, TensorFlow non offre questa capacità.\n\n\n\n\n\n\nFigura 6.14: Confronto tra Framework TensorFlow - Software. Fonte: TensorFlow.\n\n\n\nUn’altra differenza fondamentale è la “accelerator delegation” [delega dell’acceleratore]. TensorFlow e TensorFlow Lite supportano questa funzionalità, consentendo loro di pianificare il codice su acceleratori diversi. Tuttavia, TensorFlow Lite Micro non offre la delega dell’acceleratore, poiché i sistemi embedded tendono ad avere una gamma limitata di acceleratori specializzati.\nQueste differenze dimostrano come ogni variante di TensorFlow sia ottimizzata per il suo ambiente di distribuzione di target, dai potenti server cloud ai dispositivi embedded con risorse limitate.\n\n\n6.9.3 Hardware\nTensorFlow Lite e TensorFlow Lite Micro hanno dimensioni binarie di base e footprint di memoria significativamente più piccoli rispetto a TensorFlow (vedere Figura 6.15). Ad esempio, un tipico binario TensorFlow Lite Micro è inferiore a 200 KB, mentre TensorFlow è molto più grande. Ciò è dovuto agli ambienti con risorse limitate dei sistemi embedded. TensorFlow supporta x86, TPU e GPU come NVIDIA, AMD e Intel.\n\n\n\n\n\n\nFigura 6.15: Confronto tra Framework TensorFlow - Hardware. Fonte: TensorFlow.\n\n\n\nTensorFlow Lite supporta i processori Arm Cortex-A e x86 comunemente utilizzati su telefoni cellulari e tablet. Quest’ultimo è privo di tutta la logica di training non necessaria per l’implementazione sul dispositivo. TensorFlow Lite Micro fornisce supporto per core Arm Cortex M focalizzati sui microcontrollori come M0, M3, M4 e M7, nonché DSP come Hexagon e SHARC e MCU come STM32, NXP Kinetis, Microchip AVR.\n\n\n6.9.4 Altri Fattori\nLa selezione del framework di IA appropriato è essenziale per garantire che i sistemi embedded possano eseguire in modo efficiente i modelli di IA. Diversi fattori chiave oltre a modelli, hardware e software dovrebbero essere presi in considerazione quando si valutano i framework IA per i sistemi embedded.\nAltri fattori chiave da considerare quando si sceglie un framework di apprendimento automatico sono prestazioni, scalabilità, facilità d’uso, integrazione con strumenti di ingegneria dei dati, integrazione con strumenti di ottimizzazione dei modelli e supporto della community. Gli sviluppatori possono prendere decisioni informate e massimizzare il potenziale delle iniziative di apprendimento automatico comprendendo questi diversi fattori.\n\nPrestazioni\nLe prestazioni sono fondamentali nei sistemi embedded in cui le risorse di calcolo sono limitate. Valutare la capacità del framework di ottimizzare l’inferenza del modello per l’hardware embedded. La quantizzazione del modello e il supporto dell’accelerazione hardware sono cruciali per ottenere un’inferenza efficiente.\n\n\nScalabilità\nLa scalabilità è essenziale quando si considera la potenziale crescita di un progetto di IA embedded. Il framework dovrebbe supportare l’implementazione di modelli su vari dispositivi embedded, dai microcontrollori ai processori più potenti. Dovrebbe inoltre gestire senza problemi sia le distribuzioni su piccola che su larga scala.\n\n\nIntegrazione con Strumenti di Data Engineering\nGli strumenti di ingegneria dei dati sono essenziali per la pre-elaborazione dei dati e la gestione della pipeline. Un framework di intelligenza artificiale ideale per sistemi embedded dovrebbe integrarsi perfettamente con questi strumenti, consentendo un’efficiente acquisizione dei dati, trasformazione e addestramento del modello.\n\n\nIntegrazione con Strumenti di Ottimizzazione del Modello\nL’ottimizzazione del modello garantisce che i modelli di intelligenza artificiale siano adatti per la distribuzione embedded. Valutare se il framework si integra con strumenti di ottimizzazione del modello come TensorFlow Lite Converter o ONNX Runtime per facilitare la quantizzazione del modello e la riduzione delle dimensioni.\n\n\nFacilità d’Uso\nLa facilità d’uso di un framework di IA ha un impatto significativo sull’efficienza dello sviluppo. Un framework con un’interfaccia intuitiva e una documentazione chiara riduce la curva di apprendimento degli sviluppatori. Si dovrebbe considerare se il framework supporta API di alto livello, consentendo agli sviluppatori di concentrarsi sulla progettazione del modello piuttosto che sui dettagli di implementazione di basso livello. Questo fattore è incredibilmente importante per i sistemi embedded, che hanno meno funzionalità di quelle a cui gli sviluppatori tipici potrebbero essere abituati.\n\n\nSupporto della Community\nIl supporto della community gioca un altro fattore essenziale. I framework con community attive e coinvolte spesso hanno basi di codice ben mantenute, ricevono aggiornamenti regolari e forniscono forum preziosi per la risoluzione dei problemi. Di conseguenza, anche il supporto della community gioca un ruolo nella facilità d’uso perché garantisce che gli sviluppatori abbiano accesso a una vasta gamma di risorse, tra cui tutorial e progetti di esempio. Il supporto della community fornisce una certa garanzia che il framework continuerà a essere supportato per futuri aggiornamenti. Ci sono solo pochi framework che soddisfano le esigenze di TinyML. TensorFlow Lite Micro è il più popolare e ha il maggior supporto della comunità.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Framework di IA</span>"
    ]
  },
  {
    "objectID": "contents/core/frameworks/frameworks.it.html#tendenze-future-nei-framework-ml",
    "href": "contents/core/frameworks/frameworks.it.html#tendenze-future-nei-framework-ml",
    "title": "6  Framework di IA",
    "section": "6.10 Tendenze Future nei Framework ML",
    "text": "6.10 Tendenze Future nei Framework ML\n\n6.10.1 Decomposizione\nAttualmente, lo stack del sistema ML è costituito da quattro astrazioni come mostrato in Figura 6.16, vale a dire (1) grafi computazionali, (2) programmi tensoriali, (3) librerie e runtime e (4) primitive hardware.\n\n\n\n\n\n\nFigura 6.16: Quattro astrazioni negli attuali stack dei sistemi ML. Fonte: TVM.\n\n\n\nCiò ha portato a confini verticali (ad esempio, tra i livelli di astrazione) e orizzontali (ad esempio, approcci basati sulla libreria rispetto a quelli basati sulla compilazione per il calcolo dei tensori), che ostacolano l’innovazione per il ML. Il lavoro futuro nei framework ML può guardare alla rottura di questi confini. A dicembre 2021 è stato proposto Apache TVM Unity, che mirava a facilitare le interazioni tra i diversi livelli di astrazione (nonché le persone dietro di essi, come scienziati ML, ingegneri ML e ingegneri hardware) e a co-ottimizzare le decisioni in tutti e quattro i livelli di astrazione.\n\n\n6.10.2 Compilatori e Librerie ad Alte Prestazioni\nCon l’ulteriore sviluppo dei framework ML, continueranno a emergere compilatori e librerie ad alte prestazioni. Alcuni esempi attuali includono TensorFlow XLA e CUTLASS di Nvidia, che accelerano le operazioni di algebra lineare nei grafi computazionali, e TensorRT di Nvidia, che accelera e ottimizza l’inferenza.\n\n\n6.10.3 ML per Framework ML\nPossiamo anche usare il ML per migliorare i framework di ML in futuro. Alcuni usi correnti di ML per framework ML includono:\n\nOttimizzazione degli iperparametri tramite tecniche quali ottimizzazione bayesiana, ricerca casuale e ricerca a griglia\nNeural Architecture Search (NAS) per cercare automaticamente architetture di rete ottimali\nAutoML, che come descritto in Sezione 6.5, automatizza la pipeline ML.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Framework di IA</span>"
    ]
  },
  {
    "objectID": "contents/core/frameworks/frameworks.it.html#conclusione",
    "href": "contents/core/frameworks/frameworks.it.html#conclusione",
    "title": "6  Framework di IA",
    "section": "6.11 Conclusione",
    "text": "6.11 Conclusione\nIn sintesi, la selezione del framework di machine learning ottimale richiede una valutazione approfondita di varie opzioni in base a criteri quali usabilità, supporto della community, prestazioni, compatibilità hardware e capacità di conversione del modello. Non esiste una soluzione adatta a tutti, poiché il framework giusto dipende da vincoli e casi d’uso specifici.\nAbbiamo prima introdotto la necessità di framework di apprendimento automatico come TensorFlow e PyTorch. Questi framework offrono funzionalità quali tensori per la gestione di dati multidimensionali, grafi computazionali per la definizione e l’ottimizzazione delle operazioni del modello e una suite di strumenti tra cui funzioni di perdita, ottimizzatori e caricatori di dati che semplificano lo sviluppo del modello.\nLe funzionalità avanzate migliorano ulteriormente l’usabilità di questi framework, consentendo attività come la messa a punto di grandi modelli pre-addestrati e la facilitazione del “federated learning”. Queste funzionalità sono fondamentali per sviluppare modelli di apprendimento automatico sofisticati in modo efficiente.\nI framework di intelligenza artificiale embedded o TinyML, come TensorFlow Lite Micro, forniscono strumenti specializzati per distribuire modelli su piattaforme con risorse limitate. TensorFlow Lite Micro, ad esempio, offre strumenti di ottimizzazione completi, tra cui la mappatura della quantizzazione e le ottimizzazioni del kernel, per garantire prestazioni elevate su piattaforme basate su microcontrollori come i processori Arm Cortex-M e RISC-V. I framework creati appositamente per hardware specializzato come CMSIS-NN su processori Cortex-M possono massimizzare ulteriormente le prestazioni ma sacrificare la portabilità. I framework integrati dei fornitori di processori adattano lo stack alle loro architetture, liberando il pieno potenziale dei loro chip ma ci si blocca nel loro ecosistema.\nIn definitiva, la scelta del framework giusto implica la ricerca della migliore corrispondenza tra le sue capacità e i requisiti della piattaforma target. Ciò richiede un bilanciamento tra esigenze di prestazioni, vincoli hardware, complessità del modello e altri fattori. Una valutazione approfondita dei modelli e dei casi d’uso previsti e la valutazione delle opzioni rispetto alle metriche chiave guideranno gli sviluppatori nella selezione del framework ideale per le loro applicazioni di machine learning.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Framework di IA</span>"
    ]
  },
  {
    "objectID": "contents/core/frameworks/frameworks.it.html#sec-ai-frameworks-resource",
    "href": "contents/core/frameworks/frameworks.it.html#sec-ai-frameworks-resource",
    "title": "6  Framework di IA",
    "section": "6.12 Risorse",
    "text": "6.12 Risorse\nEcco un elenco curato di risorse per supportare studenti e insegnanti nei loro percorsi di apprendimento e insegnamento. Lavoriamo continuamente per espandere questa raccolta e presto aggiungeremo nuovi esercizi.\n\n\n\n\n\n\nSlide\n\n\n\n\n\nQueste slide sono uno strumento prezioso per gli insegnanti per tenere lezioni e per gli studenti per rivedere il materiale secondo il proprio ritmo. Incoraggiamo studenti e docenti a sfruttare queste slide per migliorare la loro comprensione e facilitare un trasferimento efficace delle conoscenze.\n\nFrameworks overview.\nEmbedded systems software.\nInference engines: TF vs. TFLite.\nTF flavors: TF vs. TFLite vs. TFLite Micro.\nTFLite Micro:\n\nTFLite Micro Big Picture.\nTFLite Micro Interpreter.\nTFLite Micro Model Format.\nTFLite Micro Memory Allocation.\nTFLite Micro NN Operations.\n\n\n\n\n\n\n\n\n\n\n\nVideo\n\n\n\n\n\n\nProssimamente.\n\n\n\n\n\n\n\n\n\n\nEsercizi\n\n\n\n\n\nPer rafforzare i concetti trattati in questo capitolo, abbiamo curato una serie di esercizi che sfidano gli studenti ad applicare le proprie conoscenze e ad approfondire la propria comprensione.\n\nEsercizio 6.1\nEsercizio 6.2\nEsercizio 6.3",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Framework di IA</span>"
    ]
  },
  {
    "objectID": "contents/core/training/training.it.html",
    "href": "contents/core/training/training.it.html",
    "title": "7  Addestramento dell’IA",
    "section": "",
    "text": "7.1 Panoramica\nIl training è fondamentale per sviluppare sistemi di intelligenza artificiale accurati e utili tramite il machine learning. Il training crea un modello di apprendimento automatico che può essere generalizzato a dati nuovi e inediti anziché memorizzare gli esempi dell’addestramento. Ciò avviene inserendo dati di training in algoritmi che apprendono pattern da questi esempi regolando i parametri interni.\nGli algoritmi riducono al minimo una “funzione loss” [perdita], che confronta le loro previsioni sui dati di training con le etichette o le soluzioni note, guidando l’apprendimento. Un training efficace richiede spesso set di dati rappresentativi di alta qualità sufficientemente grandi da catturare la variabilità nei casi d’uso del mondo reale.\nRichiede inoltre la scelta di un algoritmo adatto all’attività, che si tratti di una rete neurale per la visione artificiale, un algoritmo di apprendimento di rinforzo per il controllo robotico o un metodo basato su alberi per la previsione categoriale. È necessaria un’attenta messa a punto per la struttura del modello, come la profondità e la larghezza della rete neurale e i parametri di apprendimento come la dimensione del passo e la forza della regolarizzazione.\nSono importanti anche le tecniche per prevenire l’overfitting, come le penalità di regolarizzazione nonché la convalida con dati trattenuti. L’overfitting può verificarsi quando un modello si adatta troppo ai dati di training, non riuscendo a generalizzare con i nuovi dati. Ciò può accadere se il modello è troppo complesso o è stato addestrato troppo a lungo.\nPer evitare l’overfitting, le tecniche di regolarizzazione possono aiutare a vincolare il modello. Un metodo di regolarizzazione consiste nell’aggiungere un termine di penalità alla funzione di perdita che scoraggia la complessità, come la norma L2 dei pesi. Questo penalizza i valori dei parametri elevati. Un’altra tecnica è il dropout, in cui una percentuale di neuroni viene impostata casualmente a zero durante l’addestramento. Ciò riduce il co-adattamento dei neuroni.\nI metodi di validazione aiutano anche a rilevare ed evitare l’overfitting. Una parte dei dati di training viene tenuta fuori dal ciclo di training come un set di validazione. Il modello viene valutato su questi dati. Se l’errore di convalida aumenta mentre l’errore di training diminuisce, si verifica un overfitting. Il training può quindi essere interrotto in anticipo o regolarizzato in modo più forte. La regolarizzazione e la convalida consentono ai modelli di addestrarsi alla massima capacità senza overfitting [sovra-adattare] i dati di training.\nIl training richiede notevoli risorse di elaborazione, in particolare per le reti neurali profonde (deep) utilizzate nella visione artificiale, nell’elaborazione del linguaggio naturale e in altre aree. Queste reti hanno milioni di pesi regolabili che devono essere regolati tramite un training esteso. I miglioramenti hardware e le tecniche di training distribuite hanno consentito di addestrare reti neurali sempre più grandi che possono raggiungere prestazioni di livello umano in alcune attività.\nIn sintesi, alcuni punti chiave sul training:\nGuideremo attraverso questi dettagli nelle restanti sezioni. Comprendere come sfruttare in modo efficace dati, algoritmi, ottimizzazione dei parametri e generalizzazione attraverso il training è essenziale per sviluppare sistemi di intelligenza artificiale capaci e distribuibili che funzionino in modo robusto nel mondo reale.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Addestramento dell'IA</span>"
    ]
  },
  {
    "objectID": "contents/core/training/training.it.html#panoramica",
    "href": "contents/core/training/training.it.html#panoramica",
    "title": "7  Addestramento dell’IA",
    "section": "",
    "text": "I Dati sono cruciali: I modelli di machine learning apprendono dagli esempi nei dati di training. Dati più rappresentativi e di qualità elevata portano a migliori prestazioni del modello. I dati devono essere elaborati e formattati per il training.\nGli algoritmi imparano dai dati: Diversi algoritmi (reti neurali, alberi decisionali, ecc.) hanno approcci diversi per trovare dei pattern nei dati. È importante scegliere l’algoritmo giusto per l’attività.\nL’addestramento affina i parametri del modello: L’addestramento del modello regola i parametri interni per trovare pattern nei dati. I modelli avanzati come le reti neurali hanno molti pesi regolabili. L’addestramento regola iterativamente i pesi per ridurre al minimo una funzione di perdita.\nLa generalizzazione è l’obiettivo: Un modello che sovra-adatta i dati di addestramento non generalizzerà bene. Le tecniche di regolarizzazione (dropout, early stopping arresto anticipato, ecc.) riducono il sovra-adattamento. I dati di validazione vengono utilizzati per valutare la generalizzazione.\nL’addestramento richiede risorse di elaborazione: L’addestramento di modelli complessi richiede una notevole potenza di elaborazione e tempo. I miglioramenti hardware e il training distribuito su GPU/TPU hanno consentito dei progressi.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Addestramento dell'IA</span>"
    ]
  },
  {
    "objectID": "contents/core/training/training.it.html#matematica-delle-reti-neurali",
    "href": "contents/core/training/training.it.html#matematica-delle-reti-neurali",
    "title": "7  Addestramento dell’IA",
    "section": "7.2 Matematica delle Reti Neurali",
    "text": "7.2 Matematica delle Reti Neurali\nIl deep learning ha rivoluzionato l’apprendimento automatico e l’intelligenza artificiale, consentendo ai computer di apprendere pattern complessi e prendere decisioni intelligenti. La rete neurale è al centro della rivoluzione del deep learning e, come discusso nella sezione 3, “Avvio al Deep Learning”, è un pilastro in alcuni di questi progressi.\nLe reti neurali sono costituite da semplici funzioni stratificate l’una sull’altra. Ogni layer acquisisce alcuni dati, esegue alcuni calcoli e li passa al layer successivo. Questi layer apprendono progressivamente funzionalità di alto livello utili per le attività che la rete è addestrata a svolgere. Ad esempio, in una rete addestrata per il riconoscimento delle immagini, il layer di input può acquisire valori di pixel, mentre i layer successivi possono rilevare forme semplici come i bordi. I layer successivi possono rilevare forme più complesse come nasi, occhi, ecc. Il layer di output finale classifica l’immagine nel suo complesso.\nLa rete, in una rete neurale, si riferisce al modo in cui questi layer sono connessi. L’output di ogni layer è considerato un set di neuroni, che sono collegati ai neuroni nei layer successivi, formando una “rete”. Il modo in cui questi neuroni interagiscono è determinato dai pesi tra di loro, che modellano le forze sinaptiche simili a quelle di un neurone del cervello. La rete neurale viene addestrata regolando questi pesi. Concretamente, i pesi vengono inizialmente impostati in modo casuale, quindi viene immesso l’input, l’output viene confrontato con il risultato desiderato e, infine, i pesi vengono modificati per migliorare la rete. Questo processo viene ripetuto finché la rete non riduce al minimo in modo affidabile la perdita (loss), indicando di aver appreso i pattern nei dati.\nCome viene definito matematicamente questo processo? Formalmente, le reti neurali sono modelli matematici costituiti da operazioni lineari e non lineari alternate, parametrizzate da un set di pesi apprendibili che vengono addestrati per minimizzare una qualche funzione di perdita (loss). Questa funzione di perdita misura quanto è buono il nostro modello per quanto riguarda l’adattamento dei nostri dati di addestramento e produce un valore numerico quando viene valutato sul nostro modello rispetto ai dati di addestramento. L’addestramento delle reti neurali comporta la valutazione ripetuta della funzione di perdita su molti dati diversi per misurare quanto è buono il nostro modello, quindi la modifica continua dei pesi del nostro modello utilizzando la backpropagation in modo che la perdita diminuisca, ottimizzando infine il modello per adattarlo ai nostri dati.\n\n7.2.1 Notazione delle Reti Neurali\nIl nucleo di una rete neurale può essere visto come una sequenza di operazioni lineari e non lineari alternate, come mostrato in Figura 7.1.\n\n\n\n\n\n\nFigura 7.1: Diagramma della rete neurale. Fonte: astroML.\n\n\n\nLe reti neurali sono strutturate con layer [strati] di neuroni collegati da pesi (che rappresentano operazioni lineari) e funzioni di attivazione (che rappresentano operazioni non lineari). Esaminando la figura, vediamo come le informazioni fluiscono attraverso la rete, partendo dal layer di input, passando attraverso uno o più layer nascosti, e infine raggiungendo il layer di output. Ogni connessione tra neuroni rappresenta un peso, mentre ciascun neurone applica tipicamente una funzione di attivazione non lineare ai suoi input.\nLa rete neurale funziona prendendo un vettore di input \\(x_i\\) e passandolo attraverso una serie di layer, ognuno dei quali esegue operazioni lineari e non lineari. L’output della rete a ogni layer \\(A_j\\) può essere rappresentato come:\n\\[\nA_j = f\\left(\\sum_{i=1}^{N} w_{ij} x_i\\right)\n\\]\nDove:\n\n\\(N\\) - Il numero totale di feature di input.\n\\(x_{i}\\) - La singola feature di input, dove \\(i\\) varia da \\(1\\) a \\(N\\).\n\\(w_{ij}\\) - I pesi che collegano il neurone \\(i\\) in uno layer al neurone \\(j\\) nel layer successivo, che vengono aggiustati durante l’addestramento.\n\\(f(\\theta)\\) - La funzione di attivazione non lineare applicata a ogni layer (ad esempio, ReLU, softmax, ecc.).\n\\(A_{j}\\) - L’output della rete neurale a ogni layer \\(j\\), dove \\(j\\) indica il numero del layer.\n\nNel contesto di Figura 7.1, \\(x_1, x_2, x_3, x_4,\\) e \\(x_5\\) rappresentano le caratteristiche di input. Ogni neurone di input \\(x_i\\) corrisponde a una feature dei dati di input. Le frecce dal layer di input al layer nascosto indicano le connessioni tra i neuroni di input e i neuroni nascosti, con ogni connessione associata a un peso \\(w_{ij}\\).\nIl layer nascosto è costituito dai neuroni \\(a_1, a_2, a_3,\\) e \\(a_4\\), ognuno dei quali riceve input da tutti i neuroni nello layer di input. I pesi \\(w_{ij}\\) collegano i neuroni di input ai neuroni nascosti. Ad esempio, \\(w_{11}\\) è il peso che collega l’input \\(x_1\\) al neurone nascosto \\(a_1\\).\nIl numero di nodi in ogni layer e il numero totale di layer insieme definiscono l’architettura della rete neurale. Nel primo layer (layer di input), il numero di nodi corrisponde alla dimensionalità dei dati di input, mentre nell’ultimo layer (layer di output), il numero di nodi corrisponde alla dimensionalità dell’output. Il numero di nodi nei layer intermedi può essere impostato arbitrariamente, consentendo flessibilità nella progettazione dell’architettura di rete.\nI pesi, che determinano il modo in cui ogni layer della rete neurale interagisce con gli altri, sono matrici di numeri reali. Inoltre, ogni layer in genere include un vettore di bias [polarizzazione], ma qui lo ignoriamo per semplicità. La matrice dei pesi \\(W_j\\) che collega il layer \\(j-1\\) al layer \\(j\\) ha le dimensioni:\n\\[\nW_j \\in \\mathbb{R}^{d_j \\times d_{j-1}}\n\\]\ndove \\(d_j\\) è il numero di nodi nel layer \\(j\\) e \\(d_{j-1}\\) è il numero di nodi nel layer precedente \\(j-1\\).\nL’output finale \\(y_k\\) della rete si ottiene applicando un’altra funzione di attivazione \\(g(\\theta)\\) alla somma ponderata degli output del layer nascosto:\n\\[\ny = g\\left(\\sum_{j=1}^{M} w_{jk} A_j\\right)\n\\]\nDove:\n\n\\(M\\) - Il numero di neuroni nascosti nel layer finale prima dell’output.\n\\(w_{jk}\\) - Il peso tra il neurone nascosto \\(a_j\\) e il neurone di output \\(y_k\\).\n\\(g(\\theta)\\) - La funzione di attivazione applicata alla somma ponderata degli output del layer nascosto.\n\nLa nostra rete neurale, come definita, esegue una sequenza di operazioni lineari e non lineari sui dati di input (\\(x_{i}\\)) per ottenere previsioni (\\(y_{i}\\)), che si spera siano una buona risposta a ciò che vogliamo che la rete neurale faccia sull’input (ad esempio, classificare se l’immagine di input è un gatto o meno). La nostra rete neurale può quindi essere rappresentata succintamente come una funzione \\(N\\) che accetta un input \\(x \\in \\mathbb{R}^{d_0}\\) parametrizzato da \\(W_1, ..., W_n\\) e produce l’output finale \\(y\\):\n\\[\ny = N(x; W_1, ..., W_n) \\quad \\text{where } A_0 = x\n\\]\nQuesta equazione indica che la rete inizia con l’input \\(A_0 = x\\) e calcola iterativamente \\(A_j\\) a ogni layer utilizzando i parametri \\(W_j\\) fino a produrre l’output finale \\(y\\) al layer di output.\nSuccessivamente vedremo come valutare questa rete neurale rispetto ai dati di addestramento introducendo una funzione di perdita.\n\n\n\n\n\n\nNota\n\n\n\nPerché sono necessarie le operazioni non lineari? Se avessimo solo layer lineari, l’intera rete sarebbe equivalente a un singolo layer lineare costituito dal prodotto degli operatori lineari. Quindi, le funzioni non lineari svolgono un ruolo chiave nella potenza delle reti neurali poiché migliorano la capacità della rete neurale di adattare le funzioni.\n\n\n\n\n\n\n\n\nNota\n\n\n\nAnche le convoluzioni sono operatori lineari e possono essere convertite in una moltiplicazione di matrici.\n\n\n\n\n7.2.2 Funzione Loss come Misura della Bontà di Adattamento Rispetto ai Dati di Addestramento\nDopo aver definito la nostra rete neurale, ci vengono forniti alcuni dati di addestramento, ovvero un set di punti \\({(x_j, y_j)}\\) per \\(j=1 \\rightarrow M\\), dove \\(M\\) è il numero totale di campioni nel set di dati e \\(j\\) indicizza ogni campione. Vogliamo valutare quanto è buona la nostra rete neurale nell’adattare questi dati. Per fare ciò, introduciamo una funzione di perdita, ovvero una funzione che prende l’output della rete neurale su un particolare punto dati \\(\\hat{y_j} = N(x_j; W_1, ..., W_n)\\) e lo confronta con la “etichetta” di quel particolare dato (il corrispondente \\(y_j\\)) e restituisce un singolo scalare numerico (ovvero un numero reale) che rappresenta quanto è “bene” la rete neurale adatta quel particolare dato; la misura finale di quanto è buona la rete neurale sull’intero set di dati è quindi solo la media delle perdite su tutti i dati.\nEsistono molti tipi diversi di funzioni di perdita; ad esempio, nel caso della classificazione delle immagini, potremmo usare la funzione di “cross-entropy loss” [perdita di entropia incrociata], che ci dice quanto bene si confrontano due vettori che rappresentano le previsioni di classificazione (ad esempio, se la nostra previsione prevede che un’immagine sia più probabilmente un cane, ma l’etichetta dice che è un gatto, restituirà una “perdita” elevata, che indica un cattivo adattamento).\nMatematicamente, una funzione di perdita è una funzione che prende due vettori con valori reali, uno che rappresenta gli output previsti della rete neurale e l’altro che rappresenta le etichette vere, e restituisce un singolo scalare numerico che rappresenta l’errore o la “perdita”.\n\\[\nL: \\mathbb{R}^{d_{n}} \\times \\mathbb{R}^{d_{n}} \\longrightarrow \\mathbb{R}\n\\]\nPer un singolo esempio di training, la perdita è data da:\n\\[\nL(N(x_j; W_1, ..., W_n), y_j)\n\\]\ndove \\(\\hat{y}_j = N(x_j; W_1, ..., W_n)\\) è l’output previsto della rete neurale per l’input \\(x_j\\), and \\(y_j\\) è la vera etichetta.\nLa perdita totale nell’intero set di dati, \\(L_{full}\\), viene quindi calcolata come la perdita media in tutti i dati di training:\n\nFunzione di Perdita per l’Ottimizzazione del Modello di Rete Neurale su Dataset \\[\nL_{full} = \\frac{1}{M} \\sum_{j=1}^{M} L(N(x_j; W_1,...W_n), y_j)\n\\]\n\n\n\n7.2.3 Addestramento di Reti Neurali con Discesa del Gradiente\nOra che possiamo misurare quanto bene la nostra rete si adatta ai dati di training, possiamo ottimizzare i pesi della rete neurale per ridurre al minimo questa perdita. In questo contesto, stiamo denotando \\(W_i\\) come pesi per ogni layer \\(i\\) nella rete. Ad alto livello, modifichiamo i parametri delle matrici a valori reali \\(W_i\\) per ridurre al minimo la funzione di perdita \\(L_{full}\\). Nel complesso, il nostro obiettivo matematico è\n\nObiettivo dell’Addestramento della Rete Neurale \\[\nmin_{W_1, ..., W_n} L_{full}\n\\] \\[\n= min_{W_1, ..., W_n} \\frac{1}{M} \\sum_{j=1}^{M} L(N(x_j; W_1,...W_n), y_j)\n\\]\n\nQuindi, come ottimizziamo questo obiettivo? Ricordiamo dal calcolo che la minimizzazione di una funzione può essere eseguita prendendo la derivata della funzione relativa ai parametri di input e modificando i parametri nella direzione del gradiente. Questa tecnica è chiamata a “discesa del gradiente” e concretamente comporta il calcolo della derivata della funzione di perdita \\(L_{full}\\) relativa a \\(W_1, ..., W_n\\) per ottenere un gradiente per questi parametri per fare un passo avanti, poi aggiornare questi parametri nella direzione del gradiente. Quindi, possiamo addestrare la nostra rete neurale utilizzando la discesa del gradiente, che applica ripetutamente la regola di aggiornamento.\n\nRegola di Aggiornamento della Discesa del Gradiente \\[\nW_i := W_i - \\lambda \\frac{\\partial L_{full}}{\\partial W_i} \\mbox{ for } i=1..n\n\\]\n\n\n\n\n\n\n\nNota\n\n\n\nIn pratica, il gradiente viene calcolato su un mini-batch di punti dati per migliorare l’efficienza computazionale. Questo processo è chiamato “discesa del gradiente stocastico” o “discesa del gradiente batch”.\n\n\nDove \\(\\lambda\\) è la dimensione del passo o il tasso di apprendimento delle nostre modifiche, nell’addestramento della nostra rete neurale, eseguiamo ripetutamente il passaggio precedente fino alla convergenza, o quando la perdita non diminuisce più. Figura 7.2 illustra questo processo: vogliamo raggiungere il punto minimo, il che si ottiene seguendo il gradiente (come illustrato con le frecce blu nella figura). Questo precedente approccio è noto come discesa del gradiente completa poiché stiamo calcolando la derivata relativa a tutti i dati di addestramento e solo dopo eseguiamo un singolo passaggio del gradiente; un approccio più efficiente è quello di calcolare il gradiente relativo solo a un batch casuale di dati e poi eseguire un passaggio, un processo noto come discesa del gradiente batch o discesa del gradiente stocastica (Robbins e Monro 1951), che è più efficiente poiché ora eseguiamo molti più passi per passaggio di tutti i dati di addestramento. Successivamente, tratteremo la matematica alla base del calcolo del gradiente della funzione di perdita relativa a \\(W_i\\), un processo noto come backpropagation.\n\nRobbins, Herbert, e Sutton Monro. 1951. «A Stochastic Approximation Method». The Annals of Mathematical Statistics 22 (3): 400–407. https://doi.org/10.1214/aoms/1177729586.\n\n\n\n\n\n\nFigura 7.2: Discesa del gradiente. Fonte: Towards Data Science.\n\n\n\n\n\n7.2.4 Backpropagation\nL’addestramento delle reti neurali comporta ripetute applicazioni dell’algoritmo di discesa del gradiente, che prevede il calcolo della derivata della funzione di perdita rispetto alle \\(W_i\\). Come calcoliamo la derivata della perdita relativa alle \\(W_i\\), dato che le \\(W_i\\) sono funzioni annidate l’una dell’altra in una rete neurale profonda? Il trucco è sfruttare la regola della catena: possiamo calcolare la derivata della perdita relativa alle \\(W_i\\) applicando ripetutamente la regola della catena in un processo completo noto come backpropagation. In particolare, possiamo calcolare i gradienti calcolando la derivata della perdita relativa agli output dell’ultimo layer, poi usarla progressivamente per calcolare la derivata della perdita relativa a ciascun layer precedente a quello di input. Questo processo inizia dalla fine della rete (il layer più vicino all’output) e procede all’indietro, e quindi prende il nome di backpropagation.\nAnalizziamolo. Possiamo calcolare la derivata della perdita relativa agli output di ciascun layer della rete neurale utilizzando applicazioni ripetute della regola della catena.\n\\[\n\\frac{\\partial L_{full}}{\\partial L_{n}} = \\frac{\\partial A_{n}}{\\partial L_{n}} \\frac{\\partial L_{full}}{\\partial A_{n}}\n\\]\n\\[\n\\frac{\\partial L_{full}}{\\partial L_{n-1}} = \\frac{\\partial A_{n-1}}{\\partial L_{n-1}} \\frac{\\partial L_{n}}{\\partial A_{n-1}} \\frac{\\partial A_{n}}{\\partial L_{n}} \\frac{\\partial L_{full}}{\\partial A_{n}}  \n\\]\no più in generale\n\\[\n\\frac{\\partial L_{full}}{\\partial L_{i}} = \\frac{\\partial A_{i}}{\\partial L_{i}} \\frac{\\partial L_{i+1}}{\\partial A_{i}} ... \\frac{\\partial A_{n}}{\\partial L_{n}} \\frac{\\partial L_{full}}{\\partial A_{n}}  \n\\]\n\n\n\n\n\n\nNota\n\n\n\nIn quale ordine dovremmo eseguire questo calcolo? Da una prospettiva computazionale, è preferibile eseguire i calcoli dalla fine alla parte frontale. (ad esempio: prima si calcola \\(\\frac{\\partial L_{full}}{\\partial A_{n}}\\), poi i termini precedenti, anziché iniziare dal centro) poiché ciò evita di materializzare e calcolare grandi jacobiani. Questo perché \\(\\ \\frac {\\partial L_{full}}{\\partial A_{n}}\\) è un vettore; quindi, qualsiasi operazione di matrice che include questo termine ha un output che è compresso per essere un vettore. Quindi, eseguire il calcolo dalla fine evita grandi moltiplicazioni matrice-matrice assicurando che i prodotti intermedi siano vettori.\n\n\n\n\n\n\n\n\nNota\n\n\n\nNella nostra notazione, assumiamo che le attivazioni intermedie \\(A_{i}\\) siano vettori colonna, anziché vettori riga, quindi la regola della catena è \\(\\frac{\\partial L}{\\partial L_{i}} = \\frac{\\partial L_{i+1}}{\\partial L_{i}} ... \\frac{\\partial L}{\\partial L_{n}}\\) piuttosto che \\(\\frac{\\partial L}{\\partial L_{i}} = \\frac{\\partial L}{\\partial L_{n}} ... \\frac{\\partial L_{i+1}}{\\partial L_{i}}\\)\n\n\nDopo aver calcolato la derivata della perdita relativa all’output di ogni layer, possiamo facilmente ottenere la derivata della perdita relativa ai parametri, utilizzando di nuovo la regola della catena:\n\\[\n\\frac{\\partial L_{full}}{W_{i}} = \\frac{\\partial L_{i}}{\\partial W_{i}} \\frac{\\partial L_{full}}{\\partial L_{i}}\n\\]\nEd è in definitiva così che le derivate dei pesi dei layer vengono calcolate usando la backpropagation! Come appare concretamente in un esempio specifico? Di seguito, esaminiamo un esempio specifico di una semplice rete neurale a 2 layer su un’attività di regressione usando una funzione di perdita MSE con input a 100 dimensioni e uno layer nascosto a 30 dimensioni:\n\nEsempio di backpropagation\nSupponiamo di avere una rete neurale a due layer \\[\nL_1 = W_1 A_{0}\n\\] \\[\nA_1 = ReLU(L_1)\n\\] \\[\nL_2 = W_2 A_{1}\n\\] \\[\nA_2 = ReLU(L_2)\n\\] \\[\nNN(x) = \\mbox{Let } A_{0} = x \\mbox{ then output } A_2\n\\] dove \\(W_1 \\in \\mathbb{R}^{30 \\times 100}\\) e \\(W_2 \\in \\mathbb{R}^{1 \\times 30}\\). Inoltre, supponiamo di utilizzare la funzione di perdita MSE: \\[\nL(x, y) = (x-y)^2\n\\] Vogliamo calcolare \\[\n\\frac{\\partial L(NN(x), y)}{\\partial W_i} \\mbox{ for } i=1,2\n\\] Notare quanto segue: \\[\n\\frac{\\partial L(x, y)}{\\partial x} = 2 \\times (x-y)\n\\] \\[\n\\frac{\\partial ReLU(x)}{\\partial x} \\delta  = \\left\\{\\begin{array}{lr}\n0 & \\text{for } x \\leq 0 \\\\\n1 & \\text{for } x \\geq 0 \\\\\n\\end{array}\\right\\} \\odot \\delta\n\\] \\[\n\\frac{\\partial WA}{\\partial A} \\delta = W^T \\delta\n\\] \\[\n\\frac{\\partial WA}{\\partial W} \\delta = \\delta A^T\n\\] Quindi abbiamo \\[\n\\frac{\\partial L(NN(x), y)}{\\partial W_2} = \\frac{\\partial L_2}{\\partial W_2} \\frac{\\partial A_2}{\\partial L_2} \\frac{\\partial L(NN(x), y)}{\\partial A_2}\n\\] \\[\n= (2L(NN(x) - y) \\odot ReLU'(L_2)) A_1^T\n\\] e \\[\n\\frac{\\partial L(NN(x), y)}{\\partial W_1} = \\frac{\\partial L_1}{\\partial W_1} \\frac{\\partial A_1}{\\partial L_1} \\frac{\\partial L_2}{\\partial A_1} \\frac{\\partial A_2}{\\partial L_2} \\frac{\\partial L(NN(x), y)}{\\partial A_2}\n\\] \\[\n= [ReLU'(L_1) \\odot (W_2^T [2L(NN(x) - y) \\odot ReLU'(L_2)])] A_0^T\n\\]\n\n\n\n\n\n\n\nConsiglio\n\n\n\nRicontrollare il lavoro assicurandosi che le forme siano corrette!\n\nTutti i prodotti di Hadamard (\\(\\odot\\)) dovrebbero operare su tensori della stessa forma\nTutte le moltiplicazioni di matrici dovrebbero operare su matrici che condividono una dimensione comune (ad esempio, m per n, n per k)\nTutti i gradienti relativi ai pesi dovrebbero avere la stessa forma delle stesse matrici dei pesi\n\n\n\nL’intero processo di backpropagation può essere complesso, specialmente per reti molto profonde. Fortunatamente, framework di machine learning come PyTorch supportano la differenziazione automatica, che esegue la backpropagation. In questi framework, dobbiamo semplicemente specificare il passaggio in avanti e le derivate ci verranno calcolate automaticamente. Tuttavia, è utile comprendere il processo teorico che avviene internamente in questi framework di apprendimento automatico.\n\n\n\n\n\n\nNota\n\n\n\nCome visto sopra, le attivazioni intermedie \\(A_i\\) vengono riutilizzate nella backpropagation. Per migliorare le prestazioni, queste attivazioni vengono memorizzate nella cache dal passaggio in avanti per evitare di essere ricalcolate. Tuttavia, le attivazioni devono essere mantenute in memoria tra i passaggi in avanti e indietro, il che comporta un maggiore utilizzo della memoria. Se la rete e le dimensioni del batch sono grandi, ciò potrebbe causare problemi di memoria. Analogamente, le derivate rispetto agli output di ogni layer vengono memorizzate nella cache per evitare il ricalcolo.\n\n\n\n\n\n\n\n\nEsercizio 7.1: Reti Neurali con Backpropagation e Discesa del Gradiente\n\n\n\n\n\nScoprire la matematica dietro le potenti reti neurali! Il deep learning potrebbe sembrare magico, ma è radicato nei principi matematici. In questo capitolo, abbiamo scomposto la notazione delle reti neurali, le funzioni di perdita e la potente tecnica della backpropagation. Ora, prepariamoci a implementare questa teoria con questi notebook Colab. Immergersi nel cuore di come le reti neurali apprendono. Si vedrà la matematica dietro la backpropagation e la discesa del gradiente, aggiornando quei pesi passo dopo passo.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Addestramento dell'IA</span>"
    ]
  },
  {
    "objectID": "contents/core/training/training.it.html#grafi-del-calcolo-differenziabili",
    "href": "contents/core/training/training.it.html#grafi-del-calcolo-differenziabili",
    "title": "7  Addestramento dell’IA",
    "section": "7.3 Grafi del Calcolo Differenziabili",
    "text": "7.3 Grafi del Calcolo Differenziabili\nIn generale, la discesa del gradiente stocastico mediante backpropagation può essere eseguita su qualsiasi grafo computazionale che un utente può definire, a condizione che le operazioni del calcolo siano differenziabili. Pertanto, le librerie generiche di deep learning come PyTorch e Tensorflow consentono agli utenti di specificare il loro processo computazionale (ad esempio, reti neurali) come grafo computazionale. La backpropagation viene eseguita automaticamente tramite differenziazione automatica quando la discesa del gradiente stocastico viene eseguita su questi grafi computazionali. Inquadrare l’addestramento dell’IA come un problema di ottimizzazione su grafi di calcolo differenziabili è un modo generale per comprendere cosa sta accadendo internamente con i sistemi di deep learning.\nLa struttura raffigurata in Figura 7.3 mostra un segmento di un grafo computazionale differenziabile. In questo grafo, l’input ‘x’ viene elaborato tramite una serie di operazioni: viene prima moltiplicato per una matrice di pesi ‘W’ (MatMul), poi aggiunto a un bias ‘b’ (Add) e infine passato a una funzione di attivazione, Rectified Linear Unit (ReLU). Questa sequenza di operazioni ci fornisce l’output C. La natura differenziabile del grafo significa che ogni operazione ha un gradiente ben definito. La differenziazione automatica, come implementata nei framework ML, sfrutta questa proprietà per calcolare in modo efficiente i gradienti della perdita rispetto a ciascun parametro nella rete (ad esempio, ‘W’ e ‘b’).\n\n\n\n\n\n\nFigura 7.3: Grafo Computazionale. Fonte: TensorFlow.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Addestramento dell'IA</span>"
    ]
  },
  {
    "objectID": "contents/core/training/training.it.html#dati-di-training",
    "href": "contents/core/training/training.it.html#dati-di-training",
    "title": "7  Addestramento dell’IA",
    "section": "7.4 Dati di Training",
    "text": "7.4 Dati di Training\nPer consentire un training efficace della rete neurale, i dati disponibili devono essere suddivisi in set di training, di validazione e di test. Il set di training viene utilizzato per addestrare i parametri del modello. Il set di validazione valuta il modello durante il training per ottimizzare gli iperparametri e prevenire l’overfitting. Il set di test fornisce una valutazione finale imparziale delle prestazioni del modello addestrato.\nMantenere chiare suddivisioni tra training, validation e test con dati rappresentativi è fondamentale per addestrare, ottimizzare e valutare correttamente i modelli per ottenere le migliori prestazioni nel mondo reale. A tal fine, scopriremo le insidie o gli errori comuni che le persone commettono quando creano queste suddivisioni dei dati.\nTabella 7.1 confronta le differenze tra le suddivisioni dei dati di training, validazione e test:\n\n\n\nTabella 7.1: Confronto tra suddivisioni di dati di training, validazione e test.\n\n\n\n\n\n\n\n\n\n\nSuddivisione\nScopo\nDimensioni tipiche\n\n\n\n\nSet di addestramento\nAddestrare i parametri del modello\n60-80% dei dati totali\n\n\nSet di validazione\nValutare il modello durante l’addestramento per ottimizzare gli iperparametri e prevenire l’overfitting\n∼20% dei dati totali\n\n\nSet di test\nFornire una valutazione imparziale del modello finale addestrato\n∼20% dei dati totali\n\n\n\n\n\n\n\n7.4.1 Suddivisioni di Dataset\n\nSet di Training\nIl set di training viene utilizzato per addestrare il modello. È il sottoinsieme più grande, in genere il 60-80% dei dati totali. Il modello vede e impara dai dati di addestramento per fare previsioni. È necessario un set di training sufficientemente grande e rappresentativo affinché il modello apprenda efficacemente i pattern sottostanti.\n\n\nSet di Validazione\nIl set di validazione valuta il modello durante l’addestramento, in genere dopo ogni epoca. Solitamente, il 20% dei dati viene assegnato a questo set. Il modello non impara né aggiorna i suoi parametri in base ai dati di validazione. Vengono usati per ottimizzare gli iperparametri e apportare altre modifiche per migliorare l’addestramento. Il monitoraggio di metriche come perdita e accuratezza sul set di validazione impedisce l’overfitting solo sui dati di addestramento.\n\n\nSet di Test\nIl set di test agisce come un dataset che il modello non ha visto durante l’addestramento. Viene utilizzato per fornire una valutazione imparziale del modello addestrato finale. In genere, il 20% dei dati è riservato ai test. Mantenere un set di test “hold-out” [esterno] è fondamentale per ottenere una stima accurata di come il modello addestrato si comporterebbe su dati non ancora visti del mondo reale. La mancanza di dati dal set di test deve essere evitata a tutti i costi.\nLe proporzioni relative dei set di training, validazione e test possono variare in base alle dimensioni dei dati e all’applicazione. Tuttavia, seguire le linee guida generali per una suddivisione 60/20/20 è un buon punto di partenza. Un’attenta suddivisione dei dati garantisce che i modelli siano adeguatamente addestrati, ottimizzati e valutati per ottenere le prestazioni migliori.\nVideo 7.1 spiega come suddividere correttamente il dataset in set di training, validazione e test, assicurando un processo di training ottimale.\n\n\n\n\n\n\nVideo 7.1: Train/Dev/Test Sets\n\n\n\n\n\n\n\n\n\n7.4.2 Errori e Insidie Comuni\n\nDati di Training Insufficienti\nAssegnare troppo pochi dati al set di training è un errore comune quando si suddividono i dati, il che può avere un impatto significativo sulle prestazioni del modello. Se il set di training è troppo piccolo, il modello non avrà campioni sufficienti per apprendere in modo efficace i veri pattern nei dati. Ciò comporta un’elevata varianza e impedisce al modello di generalizzare bene ai nuovi dati.\nAd esempio, se si addestra un modello di classificazione delle immagini per riconoscere cifre scritte a mano, fornire solo 10 o 20 immagini per classe di cifre sarebbe del tutto inadeguato. Il modello avrebbe bisogno di più esempi per catturare le ampie varianze negli stili di scrittura, rotazioni, larghezze dei tratti e altre varianti.\nCome regola generale, la dimensione del training set dovrebbe essere di almeno centinaia o migliaia di esempi affinché la maggior parte degli algoritmi di apprendimento automatico funzioni in modo efficace. A causa dell’elevato numero di parametri, il set di training spesso deve essere di decine o centinaia di migliaia per le reti neurali profonde, in particolare quelle che utilizzano layer convoluzionali.\nDati di training insufficienti si manifestano in genere in sintomi quali alti tassi di errore su set di validazione/test, bassa accuratezza del modello, alta varianza e overfitting su campioni di set di training di piccole dimensioni. La soluzione è raccogliere più dati di training di qualità. Le tecniche di data augmentation possono anche aiutare ad aumentare virtualmente le dimensioni dei dati di training per immagini, audio, ecc.\nÈ importante considerare attentamente la complessità del modello e la difficoltà del problema quando si assegnano i campioni di training per garantire che siano disponibili dati sufficienti affinché il modello possa apprendere correttamente. Si consiglia inoltre di seguire le linee guida sulle dimensioni minime dei set di training per diversi algoritmi. Sono necessari più dati di training per mantenere il successo complessivo di qualsiasi applicazione di machine learning.\nSi consideri Figura 7.4 dove proviamo a classificare/suddividere i dati in due categorie (qui, per colore): a sinistra, l’overfitting è rappresentato da un modello che ha appreso troppo bene le sfumature nei dati di training (o il set di dati era troppo piccolo o abbiamo eseguito il modello per troppo tempo), facendo sì che segua il rumore insieme al segnale, come indicato dalle eccessive curve della linea. Il lato destro mostra l’underfitting, dove la semplicità del modello gli impedisce di catturare la struttura sottostante del dataset, con conseguente linea che non si adatta bene ai dati. Il grafico centrale rappresenta un adattamento ideale, dove il modello bilancia bene tra generalizzazione e adattamento, catturando la tendenza principale dei dati senza essere influenzato da valori anomali. Sebbene il modello non sia un adattamento perfetto (manca di alcuni punti), ci interessa di più la sua capacità di riconoscere pattern generali piuttosto che valori anomali idiosincratici.\n\n\n\n\n\n\nFigura 7.4: Adattamento dei dati: overfitting, right fit e underfitting. Fonte: MathWorks.\n\n\n\nFigura 7.5 il processo di adattamento dei dati nel tempo. Durante l’addestramento, cerchiamo il “punto ottimale” tra underfitting e overfitting. Inizialmente, quando il modello non ha avuto abbastanza tempo per apprendere i pattern nei dati, ci troviamo nella zona di underfitting, indicata da alti tassi di errore sul set di convalida (da ricordare che il modello è addestrato sul set di addestramento e testiamo la sua generalizzabilità sul set di convalida o sui dati che non ha mai visto prima). A un certo punto, raggiungiamo un minimo globale per i tassi di errore e idealmente vogliamo interrompere l’addestramento lì. Se continuiamo l’addestramento, il modello inizierà a “memorizzare” o a conoscere i dati troppo bene, tanto che il tasso di errore inizierà a risalire, poiché il modello non riuscirà a generalizzare a dati che non ha mai visto prima.\n\n\n\n\n\n\nFigura 7.5: Adattamento dei dati nel tempo. Fonte: IBM.\n\n\n\nIl Video 7.2 fornisce una panoramica di bias e varianza e la relazione tra i due concetti e l’accuratezza del modello.\n\n\n\n\n\n\nVideo 7.2: Bias/Varianza\n\n\n\n\n\n\n\n\nPerdita di Dati Tra Set\nIl “data leakage” [perdita di dati] si riferisce al trasferimento involontario di informazioni tra i set di training, convalida e test. Ciò viola il presupposto fondamentale che le divisioni siano reciprocamente esclusive. La perdita di dati porta a risultati di valutazione seriamente compromessi e metriche di prestazioni gonfiate.\nUn modo comune in cui si verifica la perdita di dati è se alcuni campioni del set di test vengono inavvertitamente inclusi nei dati di training. Quando si valuta il set di test, il modello ha già visto alcuni dati, il che fornisce punteggi eccessivamente ottimistici. Ad esempio, se il 2% dei dati di test trapelano nel set di training di un classificatore binario, può comportare un aumento della precisione fino al 20%!\nSe le divisioni dei dati non vengono eseguite con attenzione, possono verificarsi forme di perdita più sottili. Se le divisioni non vengono randomizzate e mescolate correttamente, i campioni che sono vicini tra loro nel set di dati potrebbero finire nella stessa divisione, portando a distorsioni della distribuzione. Ciò crea una fuga di informazioni basata sulla prossimità nel set di dati.\nUn altro caso è quando i set di dati hanno campioni collegati, intrinsecamente connessi, come grafici, reti o dati di serie temporali. La suddivisione “ingenua” può isolare nodi o intervalli di tempo connessi in set diversi. I modelli possono fare ipotesi non valide basate su informazioni parziali.\nPer prevenire la perdita di dati è necessario creare una solida separazione tra le suddivisioni: nessun campione dovrebbe esistere in più di una suddivisione. Il mescolamento e la suddivisione randomizzata aiutano a creare divisioni robuste. Le tecniche di “cross-validation” [validazione incrociata] possono essere utilizzate per una valutazione più rigorosa. Rilevare la perdita è difficile, ma i segnali rivelatori includono modelli che funzionano molto meglio sui dati di test rispetto a quelli di validazione.\nLa perdita di dati compromette gravemente la validità della valutazione perché il modello ha già visto parzialmente i dati di test. Nessuna quantità di messa a punto o architetture complesse può sostituire le suddivisioni nette dei dati. È meglio essere prudenti e creare una separazione completa tra le suddivisioni per evitare questo errore fondamentale nelle pipeline di machine learning.\n\n\nSet di Validazione Piccolo o Non Rappresentativo\nIl set di validazione viene utilizzato per valutare le prestazioni del modello durante l’addestramento e per ottimizzare gli iperparametri. Per valutazioni affidabili e stabili, il set di validazione deve essere sufficientemente ampio e rappresentativo della distribuzione dei dati reali. Tuttavia, ciò può rendere più impegnativa la selezione e l’ottimizzazione del modello.\nAd esempio, se il set di validazione contiene solo 100 campioni, le metriche calcolate avranno un’elevata varianza. A causa del rumore, l’accuratezza può variare fino al 5-10% tra le epoche. Questo rende difficile sapere se un calo nell’accuratezza della validazione è dovuto a un overfitting o a una varianza naturale. Con un set di validazione più ampio, diciamo 1000 campioni, le metriche saranno molto più stabili.\nInoltre, se il set di validazione non è rappresentativo, forse mancano alcune sottoclassi, la capacità stimata del modello potrebbe essere gonfiata. Ciò potrebbe portare a scelte di iperparametri scadenti o a interruzioni premature dell’addestramento. I modelli selezionati in base a tali set di validazione distorti non si generalizzano bene ai dati reali.\nUna buona regola pratica è che la dimensione del set di convalida dovrebbe essere di almeno diverse centinaia di campioni e fino al 10-20% del set di addestramento, lasciando comunque campioni sufficienti per l’addestramento. Le divisioni dovrebbero anche essere stratificate, il che significa che le proporzioni di classe nel set di validazione dovrebbero corrispondere a quelle nel set di dati completo, soprattutto se si lavora con set di dati sbilanciati. Un set di validazione più ampio che rappresenti le caratteristiche dei dati originali è essenziale per una corretta selezione e messa a punto del modello.\n\n\nRiutilizzo del Set di Test Più Volte\nIl set di test è progettato per fornire una valutazione imparziale del modello completamente addestrato solo una volta alla fine del processo di sviluppo del modello. Riutilizzare il set di test più volte durante lo sviluppo per la valutazione del modello, la messa a punto degli iperparametri, la selezione del modello, ecc., può causare un overfitting sui dati di test. Invece, si deve riservare il set di test per una valutazione finale del modello completamente addestrato, trattandolo come una scatola nera per simularne le prestazioni su dati reali. Questo approccio fornisce metriche affidabili per determinare se il modello è pronto per la distribuzione in produzione.\nSe il set di test viene riutilizzato come parte del processo di validazione, il modello potrebbe iniziare a vedere e imparare dai campioni di test. Questo, insieme all’ottimizzazione intenzionale o meno delle prestazioni del modello sul set di test, può gonfiare artificialmente metriche come l’accuratezza.\nAd esempio, supponiamo che il set di test venga utilizzato ripetutamente per la selezione del modello su 5 architetture. In tal caso, il modello potrebbe raggiungere il 99% di accuratezza del test memorizzando i campioni anziché apprendere pattern generalizzabili. Tuttavia, quando implementati nel mondo reale, l’accuratezza dei nuovi dati potrebbe scendere del 60%.\nLa prassi migliore è interagire con il set di test solo una volta alla fine per segnalare metriche imparziali su come il modello finale ottimizzato si comporterebbe nel mondo reale. Durante lo sviluppo del modello, il set di convalida dovrebbe essere utilizzato per tutte le attività di ottimizzazione dei parametri, selezione del modello, arresto anticipato e simili. È importante riservare una parte, come il 20-30% dell’intero set di dati, esclusivamente per la valutazione finale del modello. Questi dati non dovrebbero essere utilizzati per la convalida, l’ottimizzazione o la selezione del modello durante lo sviluppo.\nNon mantenere un set “hold-out” non visto per la convalida finale rischia di ottimizzare i risultati e trascurare potenziali errori prima del rilascio del modello. Avere alcuni dati nuovi fornisce un controllo di integrità finale sull’efficacia nel mondo reale. Mantenere la completa separazione di addestramento/validazione dal set di test è essenziale per ottenere stime accurate delle prestazioni del modello. Anche piccole deviazioni da un singolo utilizzo del set di test potrebbero falsare positivamente i risultati e le metriche, fornendo una visione eccessivamente ottimistica dell’efficacia nel mondo reale.\n\n\nStesse Suddivisioni dei Dati negli Esperimenti\nQuando si confrontano diversi modelli di machine learning o si sperimentano varie architetture e iperparametri, utilizzare le stesse suddivisioni dei dati per l’addestramento, la validazione e il test nei diversi esperimenti può introdurre distorsioni e invalidare le comparazioni.\nSe le stesse suddivisioni vengono riutilizzate, i risultati della valutazione potrebbero essere più bilanciati e misurare accuratamente quale modello funziona meglio. Ad esempio, una certa suddivisione casuale dei dati potrebbe favorire il modello A rispetto al modello B indipendentemente dagli algoritmi. Riutilizzare questa suddivisione causerà quindi distorsioni a favore del modello A.\nInvece, le suddivisioni dei dati dovrebbero essere randomizzate o mescolate per ogni iterazione sperimentale. Ciò garantisce che la casualità nel campionamento delle suddivisioni non conferisca un vantaggio ingiusto a nessun modello.\nCon diverse suddivisioni per esperimento, la valutazione diventa più solida. Ogni modello viene testato su un’ampia gamma di set di test estratti casualmente dalla popolazione complessiva, attenuando la variazione e rimuovendo la correlazione tra i risultati.\nLa prassi corretta è quella di impostare un “seed” casuale prima di suddividere i dati per ogni esperimento. La suddivisione dovrebbe avvenire dopo il rimescolamento/ricampionamento come parte della pipeline sperimentale. Eseguire confronti sulle stesse suddivisioni viola l’ipotesi i.i.d (indipendenti e identicamente distribuite) richiesta per la validità statistica.\nLe suddivisioni univoche sono essenziali per confronti di modelli equi. Sebbene richieda un’elaborazione più intensiva, l’allocazione randomizzata per esperimento rimuove la distorsione del campionamento e consente un benchmarking valido. Ciò evidenzia le vere differenze nelle prestazioni del modello indipendentemente dalle caratteristiche di una particolare suddivisione.\n\n\nMancata Stratificazione delle Suddivisioni\nQuando si suddividono i dati in set di training, validazione e test, la mancata stratificazione delle suddivisioni può comportare una rappresentazione non uniforme delle classi target tra le suddivisioni e introdurre un bias di campionamento. Ciò è particolarmente problematico per i set di dati sbilanciati.\nLa suddivisione stratificata implica il campionamento dei dati in modo che la proporzione di classi di output sia approssimativamente preservata in ogni suddivisione. Ad esempio, se si esegue una suddivisione training-test 70/30 su un set di dati con campioni negativi al 60% e positivi al 40%, la stratificazione garantisce esempi negativi al ~60% e positivi al ~40% sia nei set di training che nei set di test.\nSenza stratificazione, la casualità potrebbe comportare che la suddivisione di training abbia campioni positivi al 70% mentre il test ha campioni positivi al 30%. Il modello addestrato su questa distribuzione di training distorta non si generalizzerà bene. Lo squilibrio delle classi compromette anche le metriche del modello come l’accuratezza.\nLa stratificazione funziona meglio quando viene eseguita utilizzando etichette, sebbene proxy come il clustering possano essere utilizzati per l’apprendimento non supervisionato. Diventa essenziale per set di dati altamente distorti con classi rare che potrebbero essere facilmente omesse dalle suddivisioni.\nLibrerie come Scikit-Learn hanno metodi di suddivisione stratificati nativi. Non utilizzarli potrebbe inavvertitamente introdurre bias di campionamento e danneggiare le prestazioni del modello sui gruppi minoritari. Dopo aver eseguito le suddivisioni, il bilanciamento complessivo delle classi dovrebbe essere esaminato per garantire una rappresentazione uniforme tra le suddivisioni.\nLa stratificazione fornisce un set di dati bilanciato sia per l’addestramento del modello che per la valutazione. Sebbene la semplice suddivisione casuale sia facile, tenendo conto delle esigenze di stratificazione, specialmente per dati sbilanciati nel mondo reale, si traduce in uno sviluppo e una valutazione del modello più solidi.\n\n\nIgnorare le Dipendenze delle Serie Temporali\nI dati delle serie temporali hanno una struttura temporale intrinseca con osservazioni dipendenti dal contesto passato. Suddividere ingenuamente i dati delle serie temporali in set di training e test senza tenere conto di questa dipendenza porta a perdite di dati e bias di lookahead.\nAd esempio, suddividere semplicemente una serie temporale nel primo 70% di training e nell’ultimo 30% come dati di test contaminerà i dati di training con dati futuri. Il modello può usare queste informazioni per “sbirciare” in avanti durante il training.\nCiò si traduce in una valutazione eccessivamente ottimistica delle prestazioni del modello. Il modello può sembrare che preveda il futuro in modo accurato, ma in realtà ha appreso implicitamente in base ai dati futuri, il che non si traduce in prestazioni nel mondo reale.\nDovrebbero essere utilizzate tecniche di validazione incrociata delle serie temporali appropriate, come il concatenamento in avanti, per preservare l’ordine e la dipendenza. Il set di test dovrebbe contenere solo dati da una finestra temporale futura a cui il modello non è stato esposto per il training.\nNon tenere conto delle relazioni temporali porta a ipotesi di causalità non valide. Se i dati di training contengono dati futuri, il modello potrebbe anche dover imparare come estrapolare ulteriormente le previsioni.\nMantenere il flusso temporale degli eventi ed evitare il bias di lookahead è fondamentale per addestrare e testare correttamente i modelli di serie temporali. Ciò garantisce che possano davvero prevedere pattern futuri e non solo memorizzare i dati di training passati.\n\n\nNessun Dato Non Visto per la Valutazione Finale\nUn errore comune quando si suddividono i dati è non metterne da parte una porzione solo per la valutazione finale del modello completato. Tutti i dati vengono utilizzati per training, validazione e set di test durante lo sviluppo.\nQuesto non lascia dati non visti per ottenere una stima imparziale di come il modello finale ottimizzato si comporterebbe nel mondo reale. Le metriche sul set di test utilizzate durante lo sviluppo potrebbero riflettere solo parzialmente le reali capacità del modello.\nAd esempio, scelte come l’arresto anticipato e l’ottimizzazione degli iperparametri sono spesso ottimizzate in base alle prestazioni del set di test. Questo accoppia il modello ai dati di test. È necessario un set di dati non visto per interrompere questo accoppiamento e ottenere metriche reali del mondo reale.\nLa “best practice” è quella di riservare una parte, come il 20-30% del set di dati completo, esclusivamente per la valutazione finale del modello. Questi dati non dovrebbero essere utilizzati per la convalida, l’ottimizzazione o la selezione del modello durante lo sviluppo.\nIl salvataggio di alcuni dati non visti consente di valutare il modello completamente addestrato come una scatola nera su dati del mondo reale. Questo fornisce metriche affidabili per decidere se il modello è pronto per la distribuzione in produzione.\nNon mantenere un set “hold-out” non visto per la convalida finale rischia di ottimizzare i risultati e trascurare potenziali errori prima del rilascio del modello. Avere alcuni dati nuovi fornisce un controllo di integrità finale sull’efficacia nel mondo reale.\n\n\nSovra-ottimizzazione del Set di Validazione\nIl set di validazione è pensato per guidare il processo di training del modello, non per fungere da dati di training aggiuntivi. L’eccessiva ottimizzazione del set di validazione per massimizzare le metriche delle prestazioni lo tratta più come un set di training secondario, portando a metriche gonfiate e scarsa generalizzazione.\nAd esempio, tecniche come l’ottimizzazione estensiva degli iperparametri o l’aggiunta di incrementi di dati mirati a migliorare l’accuratezza della convalida possono far sì che il modello si adatti troppo ai dati di validazione. Il modello può raggiungere un’accuratezza di validazione del 99% ma solo un’accuratezza di test del 55%.\nAnalogamente, riutilizzare il set di validazione per un arresto anticipato può anche ottimizzare il modello specificamente per quei dati. L’arresto alle migliori prestazioni di validazione sovra-adatta il rumore e le fluttuazioni causate dalle piccole dimensioni di validazione.\nIl set di validazione funge da proxy per ottimizzare e selezionare i modelli. Tuttavia, l’obiettivo rimane massimizzare le prestazioni dei dati del mondo reale, non il set di validazione. Ridurre al minimo la perdita o l’errore sui dati di validazione non si traduce automaticamente in una buona generalizzazione.\nUn buon approccio è quello di mantenere l’uso del set di validazione al minimo: gli iperparametri possono essere regolati grossolanamente prima sui dati di training, ad esempio. Il set di validazione guida il training ma non dovrebbe influenzare o alterare il modello stesso. È uno strumento diagnostico, non di ottimizzazione.\nQuando si valutano le prestazioni sul set di validazione, bisogna fare attenzione a non sovra-adattare. Sono necessari dei compromessi per costruire modelli che funzionino bene sulla popolazione complessiva e non siano eccessivamente regolati sui campioni di validazione.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Addestramento dell'IA</span>"
    ]
  },
  {
    "objectID": "contents/core/training/training.it.html#algoritmi-di-ottimizzazione",
    "href": "contents/core/training/training.it.html#algoritmi-di-ottimizzazione",
    "title": "7  Addestramento dell’IA",
    "section": "7.5 Algoritmi di Ottimizzazione",
    "text": "7.5 Algoritmi di Ottimizzazione\nStochastic gradient descent (SGD) è un algoritmo di ottimizzazione semplice ma potente per l’addestramento di modelli di machine learning. Funziona stimando il gradiente della funzione di perdita relativa ai parametri del modello utilizzando un singolo esempio di addestramento e poi aggiornando i parametri nella direzione che riduce la perdita.\nSebbene concettualmente semplice, SGD necessita di alcune aree di miglioramento. Innanzitutto, scegliere un tasso di apprendimento appropriato può essere difficile: troppo piccolo e i progressi sono molto lenti; troppo grande e i parametri possono oscillare e non convergere. In secondo luogo, SGD tratta tutti i parametri in modo uguale e indipendente, il che potrebbe non essere l’ideale in tutti i casi. Infine, SGD vanilla [standard] utilizza solo informazioni sul gradiente di primo ordine, il che si traduce in progressi lenti su problemi mal condizionati.\n\n7.5.1 Ottimizzazioni\nNel corso degli anni, sono state proposte varie ottimizzazioni per accelerare e migliorare l’SGD vanilla. Ruder (2016) fornisce un’eccellente panoramica dei diversi ottimizzatori. In breve, diverse tecniche di ottimizzazione SGD comunemente utilizzate includono:\n\nRuder, Sebastian. 2016. «An overview of gradient descent optimization algorithms». ArXiv preprint abs/1609.04747 (settembre). http://arxiv.org/abs/1609.04747v2.\nMomentum: Accumula un vettore di velocità in direzioni di gradiente persistente attraverso le iterazioni. Ciò aiuta ad accelerare i progressi smorzando le oscillazioni e mantiene i progressi in direzioni coerenti.\nNesterov Accelerated Gradient (NAG): Una variante di momentum che calcola i gradienti in “look ahead” anziché nella posizione del parametro corrente. Questo aggiornamento anticipatorio impedisce l’overshooting mentre il momentum mantiene il progresso accelerato.\nAdagrad: Un algoritmo di velocità di apprendimento adattivo che mantiene una velocità di apprendimento per parametro ridotta proporzionalmente alla somma storica dei gradienti di ciascun parametro. Aiuta a eliminare la necessità di regolare manualmente i tassi di apprendimento (Duchi, Hazan, e Singer 2010).\n\nDuchi, John C., Elad Hazan, e Yoram Singer. 2010. «Adaptive Subgradient Methods for Online Learning and Stochastic Optimization». In COLT 2010 - The 23rd Conference on Learning Theory, Haifa, Israel, June 27-29, 2010, a cura di Adam Tauman Kalai e Mehryar Mohri, 257–69. Omnipress. http://colt2010.haifa.il.ibm.com/papers/COLT2010proceedings.pdf#page=265.\n\nZeiler, Matthew D. 2012. «ADADELTA: An Adaptive Learning Rate Method», dicembre, 119–49. https://doi.org/10.1002/9781118266502.ch6.\nAdadelta: Una modifica ad Adagrad limita la finestra dei gradienti passati accumulati, riducendo così il decadimento aggressivo dei tassi di apprendimento (Zeiler 2012).\nRMSProp: Divide il tasso di apprendimento per una media esponenzialmente decrescente dei gradienti quadrati. Ciò ha un effetto di normalizzazione simile ad Adagrad ma non accumula i gradienti nel tempo, evitando un rapido decadimento dei tassi di apprendimento (Hinton 2017).\n\nHinton, Geoffrey. 2017. «Overview of Minibatch Gradient Descent». University of Toronto; University Lecture.\n\nKingma, Diederik P., e Jimmy Ba. 2014. «Adam: A Method for Stochastic Optimization». A cura di Yoshua Bengio e Yann LeCun, dicembre. http://arxiv.org/abs/1412.6980v9.\nAdam: Combinazione di momentum e rmsprop dove rmsprop modifica il tasso di apprendimento in base alla media delle recenti ampiezze dei gradienti. Mostra un progresso iniziale molto rapido e regola automaticamente le dimensioni dei passi (Kingma e Ba 2014).\nAMSGrad: Una variante di Adam che assicura una convergenza stabile mantenendo il massimo dei gradienti quadratici passati, impedendo al tasso di apprendimento di aumentare durante l’addestramento (Reddi, Kale, e Kumar 2019).\n\nReddi, Sashank J., Satyen Kale, e Sanjiv Kumar. 2019. «On the Convergence of Adam and Beyond». arXiv preprint arXiv:1904.09237, aprile. http://arxiv.org/abs/1904.09237v1.\nTra questi metodi, Adam è ampiamente considerato l’algoritmo di ottimizzazione di riferimento per molte attività di deep-learning. Supera costantemente SGD vanilla in termini di velocità di addestramento e prestazioni. Altri ottimizzatori potrebbero essere più adatti in alcuni casi, in particolare per modelli più semplici.\n\n\n7.5.2 Compromessi\nTabella 7.2 è una tabella di pro e contro per alcuni dei principali algoritmi di ottimizzazione per l’addestramento di reti neurali:\n\n\n\nTabella 7.2: Confronto dei pro e dei contro di diversi algoritmi di ottimizzazione.\n\n\n\n\n\n\n\n\n\n\nAlgoritmo\nPro\nContro\n\n\n\n\nMomentum\n\nConvergenza più rapida dovuta all’accelerazione lungo i gradienti\nMinore oscillazione rispetto a SGD vanilla\n\n\nRichiede la messa a punto del parametro momentum\n\n\n\nNesterov Accelerated Gradient (NAG)\n\nPiù veloce dello slancio standard in alcuni casi\nGli aggiornamenti anticipati impediscono il superamento\n\n\nPiù complesso da comprendere intuitivamente\n\n\n\nAdagrad\n\nElimina la necessità di regolare manualmente i tassi di apprendimento\nFunziona bene su gradienti radi\n\n\nIl tasso di apprendimento potrebbe decadere troppo rapidamente su gradienti densi\n\n\n\nAdadelta\n\nDecadimento del tasso di apprendimento meno aggressivo rispetto ad Adagrad\n\n\nAncora sensibile al valore iniziale del tasso di apprendimento\n\n\n\nRMSProp\n\nRegola automaticamente i tassi di apprendimento\nFunziona bene nella pratica\n\n\nNessun aspetto negativo importante\n\n\n\nAdam\n\nCombinazione di momentum e tassi di apprendimento adattivo\nConvergenza efficiente e veloce\n\n\nPrestazioni di generalizzazione leggermente peggiori in alcuni casi\n\n\n\nAMSGrad\n\nMiglioramento di Adam che affronta il problema della generalizzazione\n\n\nNon è stato utilizzato/testato così ampiamente come Adam\n\n\n\n\n\n\n\n\n\n7.5.3 Algoritmi di Benchmarking\nNon esiste un singolo metodo migliore per tutti i tipi di problemi. Ciò significa che abbiamo bisogno di un benchmarking completo per identificare l’ottimizzatore più efficace per set di dati e modelli specifici. Le prestazioni di algoritmi come Adam, RMSProp e Momentum variano a seconda delle dimensioni del batch, dei programmi di apprendimento, dell’architettura del modello, della distribuzione dei dati e della regolarizzazione. Queste variazioni sottolineano l’importanza di valutare ogni ottimizzatore in diverse condizioni.\nPrendiamo ad esempio Adam, che spesso eccelle nelle attività di visione artificiale, a differenza di RMSProp, che potrebbe mostrare una migliore generalizzazione in determinate attività di elaborazione del linguaggio naturale. La forza di Momentum risiede nella sua accelerazione in scenari con direzioni di gradiente coerenti, mentre i tassi di apprendimento adattivo di Adagrad sono più adatti per problemi di gradiente sparso.\nQuesta vasta gamma di interazioni tra ottimizzatori dimostra la difficoltà di dichiarare un singolo algoritmo universalmente superiore. Ogni ottimizzatore ha punti di forza unici, rendendo fondamentale valutare vari metodi per scoprire empiricamente le loro condizioni di applicazione ottimali.\nUn approccio di benchmarking completo dovrebbe valutare la velocità di convergenza e fattori come errore di generalizzazione, stabilità, sensibilità degli iperparametri ed efficienza computazionale, tra gli altri. Ciò comporta il monitoraggio delle curve di apprendimento di training e convalida su più esecuzioni e il confronto degli ottimizzatori su vari set di dati e modelli per comprenderne i punti di forza e di debolezza.\nAlgoPerf, introdotto da Dürr et al. (2021), risponde alla necessità di un sistema di benchmarking robusto. Questa piattaforma valuta le prestazioni dell’ottimizzatore utilizzando criteri quali curve di loss [perdita] di training, errore di generalizzazione, sensibilità agli iperparametri ed efficienza computazionale. AlgoPerf testa vari metodi di ottimizzazione, tra cui Adam, LAMB e Adafactor, su diversi tipi di modelli come CNN e RNN/LSTM su set di dati stabiliti. Utilizza la “containerizzazione” e la raccolta automatica di metriche per ridurre al minimo le incongruenze e consente esperimenti controllati su migliaia di configurazioni, fornendo una base affidabile per confrontare gli ottimizzatori.\n\nDürr, Marc, Gunnar Nissen, Kurt-Wolfram Sühs, Philipp Schwenkenbecher, Christian Geis, Marius Ringelstein, Hans-Peter Hartung, et al. 2021. «CSF Findings in Acute NMDAR and LGI1 Antibody–Associated Autoimmune Encephalitis». Neurology Neuroimmunology &amp; Neuroinflammation 8 (6). https://doi.org/10.1212/nxi.0000000000001086.\nLe informazioni ottenute da AlgoPerf e benchmark simili sono inestimabili per guidare la scelta ottimale o la messa a punto degli ottimizzatori. Abilitando valutazioni riproducibili, questi benchmark contribuiscono a una comprensione più approfondita delle prestazioni di ciascun ottimizzatore, aprendo la strada a innovazioni future e progressi accelerati nel settore.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Addestramento dell'IA</span>"
    ]
  },
  {
    "objectID": "contents/core/training/training.it.html#ottimizzazione-degli-iperparametri",
    "href": "contents/core/training/training.it.html#ottimizzazione-degli-iperparametri",
    "title": "7  Addestramento dell’IA",
    "section": "7.6 Ottimizzazione degli Iperparametri",
    "text": "7.6 Ottimizzazione degli Iperparametri\nGli iperparametri sono impostazioni importanti nei modelli di machine learning che incidono notevolmente sulle prestazioni finali dei modelli. A differenza di altri parametri del modello che vengono appresi durante l’addestramento, gli iperparametri vengono specificati dai “data scientist” o dagli ingegneri del machine learning prima dell’addestramento del modello.\nLa scelta dei valori degli iperparametri corretti consente ai modelli di apprendere pattern dai dati in modo efficace. Alcuni esempi di iperparametri chiave negli algoritmi di apprendimento automatico includono:\n\nReti neurali: Velocità di apprendimento, dimensione del batch, numero di unità nascoste, funzioni di attivazione\nMacchine a vettori di supporto: Forza di regolarizzazione, tipo di kernel e parametri\nRandom forest: Numero di alberi, profondità dell’albero\nK-means: Numero di cluster\n\nIl problema è che non ci sono regole pratiche affidabili per scegliere configurazioni ottimali degli iperparametri: in genere si devono provare valori diversi e valutare le prestazioni. Questo processo è chiamato “hyperparameter tuning” ottimizzazione degli iperparametri.\nNei primi anni del moderno deep learning, i ricercatori erano ancora alle prese con problemi di convergenza instabile e lenta. I punti dolenti comuni includevano perdite di training che fluttuavano selvaggiamente, gradienti che esplodevano o svanivano e un’ampia serie di tentativi ed errori necessari per addestrare le reti in modo affidabile. Di conseguenza, un punto focale iniziale era l’utilizzo di iperparametri per controllare l’ottimizzazione del modello. Ad esempio, tecniche seminali come la normalizzazione batch consentivano una convergenza più rapida del modello regolando gli aspetti dello spostamento interno delle covariate. I metodi di velocità di apprendimento adattivo hanno anche mitigato la necessità di estese pianificazioni manuali. Questi affrontavano problemi di ottimizzazione durante l’addestramento, come la divergenza incontrollata del gradiente. Le velocità di apprendimento adattate con attenzione sono anche il fattore di controllo primario per ottenere una convergenza rapida e stabile anche oggi.\nCon l’espansione esponenziale della capacità computazionale negli anni successivi, modelli molto più grandi potevano essere addestrati senza cadere preda di problemi di pura ottimizzazione numerica. L’attenzione si è spostata verso la generalizzazione, sebbene una convergenza efficiente fosse un prerequisito fondamentale. Tecniche all’avanguardia come “Transformers” hanno introdotto miliardi di parametri. A tali dimensioni, gli iperparametri relativi a capacità, regolarizzazione, ensembling [raggruppamento], ecc., hanno assunto un ruolo centrale per la messa a punto, anziché solo le metriche di convergenza grezze.\nLa lezione è che comprendere l’accelerazione e la stabilità del processo di ottimizzazione stesso costituisce il lavoro di base. Schemi di inizializzazione, dimensioni dei batch, decadimenti di peso e altri iperparametri di training rimangono indispensabili oggi. Dominare una convergenza rapida e impeccabile consente ai professionisti di espandere la propria attenzione sulle esigenze emergenti relative alla messa a punto di parametri quali accuratezza, robustezza ed efficienza su larga scala.\n\n7.6.1 Algoritmi di Ricerca\nQuando si tratta del processo critico di ottimizzazione degli iperparametri, ci sono diversi algoritmi sofisticati su cui gli specialisti del machine learning si affidano per cercare sistematicamente nel vasto spazio di possibili configurazioni dei modelli. Alcuni degli algoritmi di ricerca degli iperparametri più importanti includono:\n\nGrid Search: Il metodo di ricerca più elementare, in cui si definisce manualmente una griglia di valori da controllare per ogni iperparametro. Ad esempio, controllando velocità di apprendimento = [0.01, 0.1, 1] e dimensioni batch = [32, 64, 128]. Il vantaggio principale è la semplicità, ma può portare a un’esplosione esponenziale nello spazio di ricerca, rendendolo dispendioso in termini di tempo. È più adatto per l’ottimizzazione di un piccolo numero di parametri.\nRandom Search: Invece di definire una griglia, si selezionano casualmente valori per ogni iperparametro da un intervallo o set predefinito. Questo metodo è più efficiente nell’esplorazione di un vasto spazio di iperparametri perché non richiede una ricerca esaustiva. Tuttavia, potrebbe comunque non trovare parametri ottimali poiché non esplora sistematicamente tutte le possibili combinazioni.\nBayesian Optimization: Questo è un approccio probabilistico avanzato per l’esplorazione adattiva basato su una funzione surrogata per modellare le prestazioni su iterazioni. È semplice ed efficiente: trova iperparametri altamente ottimizzati in meno passaggi di valutazione. Tuttavia, richiede un maggiore investimento nella configurazione (Snoek, Larochelle, e Adams 2012).\nEvolutionary Algorithms: Questi algoritmi imitano i principi della selezione naturale. Generano popolazioni di combinazioni di iperparametri e le evolvono nel tempo in base alle prestazioni. Questi algoritmi offrono solide capacità di ricerca più adatte per superfici di risposta complesse. Tuttavia, sono necessarie molte iterazioni per una convergenza ragionevole.\nPopulation Based Training (PBT): Un metodo che ottimizza gli iperparametri addestrando più modelli in parallelo, consentendo loro di condividere e adattare configurazioni di successo durante l’addestramento, combinando elementi di ricerca casuale e algoritmi evolutivi (Jaderberg et al. 2017).\nNeural Architecture Search: Un approccio alla progettazione di architetture ad alte prestazioni per reti neurali. Tradizionalmente, gli approcci NAS utilizzano una qualche forma di apprendimento di rinforzo per proporre architetture di reti neurali, che vengono poi ripetutamente valutate (Zoph e Le 2016).\n\n\nSnoek, Jasper, Hugo Larochelle, e Ryan P. Adams. 2012. «Practical Bayesian Optimization of Machine Learning Algorithms». In Advances in Neural Information Processing Systems 25: 26th Annual Conference on Neural Information Processing Systems 2012. Proceedings of a meeting held December 3-6, 2012, Lake Tahoe, Nevada, United States, a cura di Peter L. Bartlett, Fernando C. N. Pereira, Christopher J. C. Burges, Léon Bottou, e Kilian Q. Weinberger, 2960–68. https://proceedings.neurips.cc/paper/2012/hash/05311655a15b75fab86956663e1819cd-Abstract.html.\n\nJaderberg, Max, Valentin Dalibard, Simon Osindero, Wojciech M. Czarnecki, Jeff Donahue, Ali Razavi, Oriol Vinyals, et al. 2017. «Population Based Training of Neural Networks». arXiv preprint arXiv:1711.09846, novembre. http://arxiv.org/abs/1711.09846v2.\n\nZoph, Barret, e Quoc V. Le. 2016. «Neural Architecture Search with Reinforcement Learning», novembre, 367–92. https://doi.org/10.1002/9781394217519.ch17.\n\n\n7.6.2 Implicazioni di Sistema\nLa messa a punto degli iperparametri può avere un impatto significativo sul tempo di convergenza durante l’addestramento del modello, influenzando direttamente il runtime complessivo. I valori corretti per gli iperparametri chiave di training sono cruciali per un’efficiente convergenza del modello. Ad esempio, la velocità di apprendimento dell’iperparametro controlla la dimensione del passo durante l’ottimizzazione della discesa del gradiente. Impostando correttamente uno scheduling della velocità di apprendimento assicura che l’algoritmo di ottimizzazione converga rapidamente verso un buon minimo. Una velocità di apprendimento troppo bassa porta a una convergenza dolorosamente lenta, mentre un valore troppo grande causa una fluttuazione selvaggia delle perdite. Una messa a punto corretta assicura un rapido movimento verso pesi e bias ottimali.\nAnalogamente, la dimensione del batch per la discesa del gradiente stocastica influisce sulla stabilità della convergenza. La giusta dimensione del batch attenua le fluttuazioni negli aggiornamenti dei parametri per avvicinarsi più rapidamente al minimo. Sono necessarie più dimensioni del batch per evitare una convergenza rumorosa, mentre le dimensioni maggiori del batch non riescono a generalizzare e rallentano la convergenza a causa di aggiornamenti dei parametri meno frequenti. La messa a punto degli iperparametri per una convergenza più rapida e una durata di addestramento ridotta ha implicazioni dirette sui costi e sui requisiti di risorse per il ridimensionamento dei sistemi di machine learning:\n\nCosti computazionali inferiori: Tempi di convergenza più brevi significano costi computazionali inferiori per i modelli di training. Il training ML sfrutta spesso grandi istanze di cloud computing come cluster GPU e TPU che comportano pesanti costi orari. Ridurre al minimo i tempi di training riduce direttamente questo costo di noleggio delle risorse, che tende a dominare i budget ML per le organizzazioni. Un’iterazione più rapida consente inoltre agli esperti di dati di sperimentare più liberamente all’interno dello stesso budget.\nTempo di training ridotto: Un tempo di training ridotto sblocca opportunità per addestrare più modelli utilizzando lo stesso budget computazionale. Gli iperparametri ottimizzati estendono ulteriormente le risorse disponibili, consentendo alle aziende di sviluppare e sperimentare più modelli con vincoli di risorse per massimizzare le prestazioni.\nEfficienza delle risorse: Un training più rapido consente di allocare istanze di calcolo più piccole nel cloud poiché i modelli richiedono l’accesso alle risorse per una durata più breve. Ad esempio, un job di training di un’ora consente di utilizzare istanze GPU meno potenti rispetto a un training di più ore, che richiede un accesso di elaborazione sostenuto su intervalli più lunghi. Ciò consente di risparmiare sui costi, soprattutto per carichi di lavoro di grandi dimensioni.\n\nCi sono anche altri vantaggi. Ad esempio, una convergenza più rapida riduce la pressione sui team di ingegneria ML in merito al provisioning delle risorse di training. Le semplici routine di riaddestramento del modello possono utilizzare risorse meno potenti anziché richiedere l’accesso a code ad alta priorità per cluster GPU di livello di produzione vincolati, liberando risorse di distribuzione per altre applicazioni.\n\n\n7.6.3 Gli Auto Tuner\nData la sua importanza, esiste un’ampia gamma di offerte commerciali per aiutare con l’ottimizzazione degli iperparametri. Toccheremo brevemente due esempi: uno incentrato sull’ottimizzazione per ML su scala cloud e l’altro per modelli di apprendimento automatico mirati ai microcontrollori. Tabella 7.3 delinea le principali differenze:\n\n\n\nTabella 7.3: Confronto di piattaforme di ottimizzazione per diversi casi d’uso di machine learning.\n\n\n\n\n\n\n\n\n\n\n\nPiattaforma\nCaso d’Uso Target\nTecniche di ottimizzazione\nVantaggi\n\n\n\n\nVertex AI di Google\nApprendimento automatico su scala cloud\nOttimizzazione bayesiana, addestramento Population-Based\nNasconde la complessità, consentendo modelli rapidi e pronti per l’implementazione con ottimizzazione iperparametrica all’avanguardia\n\n\nEON Tuner di Edge Impulse\nModelli di microcontrollori (TinyML)\nOttimizzazione bayesiana\nAdatta i modelli per dispositivi con risorse limitate, semplifica l’ottimizzazione per l’implementazione embedded\n\n\n\n\n\n\n\nBigML\nSono disponibili diverse piattaforme commerciali di auto-tuning per risolvere questo problema. Una soluzione è Vertex AI Cloud di Google, che offre un ampio supporto integrato per tecniche di ottimizzazione all’avanguardia.\nUna delle funzionalità più importanti della piattaforma di apprendimento automatico gestita da Vertex AI di Google è l’ottimizzazione efficiente e integrata degli iperparametri per lo sviluppo del modello. Per addestrare con successo modelli ML performanti è necessario identificare configurazioni ottimali per un set di iperparametri esterni che determinano il comportamento del modello, ponendo un problema di ricerca ad alta dimensione impegnativo. Vertex AI semplifica questo processo tramite strumenti di Automated Machine Learning (AutoML).\nIn particolare, gli scienziati dei dati possono sfruttare i motori di ottimizzazione degli iperparametri di Vertex AI fornendo un set di dati etichettato e scegliendo un tipo di modello come un classificatore di reti neurali o Random Forest. Vertex avvia un job di “Hyperparameter Search” in modo trasparente sul backend, gestendo completamente il provisioning delle risorse, l’addestramento del modello, il monitoraggio delle metriche e l’analisi dei risultati automaticamente utilizzando algoritmi di ottimizzazione avanzati.\nInternamente, Vertex AutoML impiega varie strategie di ricerca per esplorare in modo intelligente le configurazioni di iperparametri più promettenti in base ai risultati delle valutazioni precedenti. Tra queste, l’ottimizzazione bayesiana è offerta in quanto fornisce un’efficienza di campionamento superiore, richiedendo meno iterazioni di training per ottenere una qualità del modello ottimizzata rispetto ai metodi standard di Grid Search o di Random Search. Per spazi di ricerca di architettura neurale più complessi, Vertex AutoML utilizza il Population-Based Training, che addestra simultaneamente più modelli e regola dinamicamente i loro iperparametri sfruttando le prestazioni di altri modelli nella popolazione, analogamente ai principi di selezione naturale.\nVertex AI democratizza le tecniche di ricerca di iperparametri all’avanguardia su scala cloud per tutti gli sviluppatori ML, astraendo la complessità di esecuzione e di orchestrazione sottostante. Gli utenti si concentrano esclusivamente sul loro set di dati, sui requisiti del modello e sugli obiettivi di accuratezza, mentre Vertex gestisce il ciclo di ottimizzazione, l’allocazione delle risorse, il training del modello, il monitoraggio dell’accuratezza e l’archiviazione degli artefatti internamente. Il risultato è ottenere modelli ML ottimizzati e pronti per la distribuzione più velocemente per il problema target.\n\n\nTinyML\nEdge Impulse’s Efficient On-device Neural Network Tuner (EON Tuner) è uno strumento di ottimizzazione automatizzata degli iperparametri progettato per sviluppare modelli di apprendimento automatico per microcontrollori. Semplifica il processo di sviluppo del modello trovando automaticamente la migliore configurazione di rete neurale per un’implementazione efficiente e accurata su dispositivi con risorse limitate.\nLa funzionalità chiave di EON Tuner è la seguente. Innanzitutto, gli sviluppatori definiscono gli iperparametri del modello, come numero di layer, nodi per layer, funzioni di attivazione e pianificazione della velocità di “annealing” [https://it.wikipedia.org/wiki/Ricottura_simulata] dell’apprendimento. Questi parametri costituiscono lo spazio di ricerca che verrà ottimizzato. Successivamente, viene selezionata la piattaforma del microcontrollore target, fornendo vincoli hardware embedded. L’utente può anche specificare obiettivi di ottimizzazione, come la riduzione dell’ingombro di memoria, la riduzione della latenza, la riduzione del consumo energetico o la massimizzazione della precisione.\nCon lo spazio di ricerca definito e gli obiettivi di ottimizzazione, EON Tuner sfrutta l’ottimizzazione degli iperparametri bayesiani per esplorare in modo intelligente possibili configurazioni. Ogni configurazione potenziale viene automaticamente implementata come specifica di modello completa, addestrata e valutata per metriche di qualità. Il processo continuo bilancia esplorazione e sfruttamento per arrivare a impostazioni ottimizzate su misura per l’architettura del chip scelta dallo sviluppatore e i requisiti di prestazioni.\nEON Tuner libera gli esperti di machine learning dal processo iterativo esigente di messa a punto manuale dei modelli, regolando automaticamente i modelli per il deployment embedded. Lo strumento si integra perfettamente nel flusso di lavoro Edge Impulse, portando i modelli dal concetto a implementazioni ottimizzate in modo efficiente sui microcontrollori. L’esperienza racchiusa in EON Tuner per quanto riguarda l’ottimizzazione del modello ML per i microcontrollori garantisce che sia gli sviluppatori principianti che quelli esperti possano rapidamente iterare per ottenere modelli adatti alle esigenze del loro progetto.\n\n\n\n\n\n\nEsercizio 7.2: Ottimizzazione degli Iperparametri\n\n\n\n\n\nPrepariamoci a scoprire i segreti della messa a punto degli iperparametri e portiamo i modelli PyTorch al livello successivo! Gli iperparametri sono come i quadranti e le manopole nascosti che controllano i superpoteri di apprendimento del modello. In questo notebook Colab, si collaborerà con Ray Tune per trovare le combinazioni perfette di iperparametri. Scopriamo come definire quali valori cercare, impostare il codice di training per l’ottimizzazione e lasciare che Ray Tune faccia il grosso del lavoro. Alla fine, si diventerà professionisti della messa a punto degli iperparametri!\n\n\n\n\nVideo 7.3 spiega l’organizzazione sistematica del processo di ottimizzazione degli iperparametri.\n\n\n\n\n\n\nVideo 7.3: Iperparametro",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Addestramento dell'IA</span>"
    ]
  },
  {
    "objectID": "contents/core/training/training.it.html#regolarizzazione",
    "href": "contents/core/training/training.it.html#regolarizzazione",
    "title": "7  Addestramento dell’IA",
    "section": "7.7 Regolarizzazione",
    "text": "7.7 Regolarizzazione\nLa regolarizzazione è una tecnica critica per migliorare le prestazioni e la generalizzabilità dei modelli di machine learning in impostazioni applicate. Si riferisce alla limitazione matematica o alla penalizzazione della complessità del modello per evitare il sovra-adattamento dei dati di training. Senza regolarizzazione, i modelli ML complessi sono inclini al sovra-adattamento del set di dati e alla memorizzazione di peculiarità e rumore nel set di training anziché all’apprendimento di pattern significativi. Possono raggiungere un’elevata accuratezza di training ma hanno prestazioni scadenti quando valutano nuovi input non ancora visti.\nLa regolarizzazione aiuta ad affrontare questo problema ponendo vincoli che favoriscono modelli più semplici e più generalizzabili che non si agganciano a errori di campionamento. Tecniche come la regolarizzazione L1/L2 penalizzano direttamente valori di parametri elevati durante il training, costringendo il modello a utilizzare i parametri più piccoli che possono spiegare adeguatamente il segnale. Le regole di arresto anticipato interrompono il training quando le prestazioni del set di validazione smettono di migliorare, prima che il modello inizi a sovra-adattarsi.\nUna regolarizzazione appropriata è fondamentale quando si distribuiscono modelli a nuove popolazioni di utenti e ambienti in cui sono probabili cambiamenti di distribuzione. Ad esempio, un modello irregolare di rilevamento delle frodi addestrato presso una banca potrebbe funzionare inizialmente, ma accumulare debiti tecnici nel tempo man mano che emergono nuovi pattern di frode.\nLa regolarizzazione di reti neurali complesse offre anche vantaggi computazionali: modelli più piccoli richiedono meno “data augmentation”, potenza di calcolo e archiviazione dei dati. La regolarizzazione consente anche sistemi di intelligenza artificiale più efficienti, in cui accuratezza, robustezza e gestione delle risorse sono attentamente bilanciate rispetto alle limitazioni del set di addestramento.\nDiverse potenti tecniche di regolarizzazione sono comunemente utilizzate per migliorare la generalizzazione del modello. L’architettura della strategia ottimale richiede la comprensione di come ogni metodo influisce sull’apprendimento e sulla complessità del modello.\n\n7.7.1 L1 e L2\nDue delle forme di regolarizzazione più ampiamente utilizzate sono la regolarizzazione L1 e la L2. Entrambe penalizzano la complessità del modello aggiungendo un termine extra alla funzione di costo ottimizzata durante l’addestramento. Questo termine cresce all’aumentare dei parametri del modello.\nLa regolarizzazione L2, nota anche come “ridge regression” [https://it.wikipedia.org/wiki/Regolarizzazione_di_Tichonov], aggiunge la somma delle grandezze al quadrato di tutti i parametri moltiplicata per un coefficiente α. Questa penalità quadratica riduce i valori dei parametri estremi in modo più aggressivo rispetto alle tecniche L1. L’implementazione richiede solo la modifica della funzione di costo e la messa a punto di α.\n\\[R_{L2}(\\Theta) = \\alpha \\sum_{i=1}^{n}\\theta_{i}^2\\]\nDove:\n\n\\(R_{L2}(\\Theta)\\) - Il termine di regolarizzazione L2 che viene aggiunto alla funzione di costo\n\\(\\alpha\\) - L’iperparametro di regolarizzazione L2 che controlla la forza della regolarizzazione\n\\(\\theta_{i}\\) - L’i-esimo parametro del modello\n\\(n\\) - Il numero di parametri nel modello\n\\(\\theta_{i}^2\\) - Il quadrato di ciascun parametro\n\nE la funzione di costo regolarizzata L2 completa è:\n\\[J(\\theta) = L(\\theta) + R_{L2}(\\Theta)\\]\nDove:\n\n\\(L(\\theta)\\) - La funzione di costo non regolarizzata originale\n\\(J(\\theta)\\) - La nuova funzione di costo regolarizzata\n\nSia la regolarizzazione L1 che L2 penalizzano i pesi elevati nella rete neurale. Tuttavia, la differenza fondamentale tra la regolarizzazione L1 e L2 è che la regolarizzazione L2 penalizza i quadrati dei parametri anziché i valori assoluti. Questa differenza fondamentale ha un impatto considerevole sui pesi regolarizzati risultanti. La regolarizzazione L1, o regressione LASSO [https://it.wikipedia.org/wiki/Regolarizzazione_(matematica)], utilizza la somma assoluta delle grandezze anziché il quadrato moltiplicato per α. La penalizzazione del valore assoluto dei pesi induce scarsità poiché il gradiente degli errori estrapola linearmente mentre i termini dei pesi tendono a zero; questo è diverso dalla penalizzazione del valore al quadrato dei pesi, dove la penalità si riduce man mano che i pesi tendono a 0. Inducendo scarsità nel vettore dei parametri, la regolarizzazione L1 esegue automaticamente la selezione delle feature, impostando i pesi delle feature irrilevanti a zero. A differenza della regolarizzazione L2, la L1 porta alla scarsità poiché i pesi sono impostati su 0; nella regolarizzazione L2, i pesi sono impostati su un valore molto vicino a 0 ma generalmente non raggiungono mai esattamente 0. La regolarizzazione L1 incoraggia la scarsità ed è stata utilizzata in alcuni lavori per addestrare reti sparse che potrebbero essere più efficienti in termini di hardware (Hoefler et al. 2021).\n\nHoefler, Torsten, Dan Alistarh, Tal Ben-Nun, Nikoli Dryden, e Alexandra Peste. 2021. «Sparsity in Deep Learning: Pruning and growth for efficient inference and training in neural networks», gennaio. http://arxiv.org/abs/2102.00554v1.\n\\[R_{L1}(\\Theta) = \\alpha \\sum_{i=1}^{n}||\\theta_{i}||\\]\nDove:\n\n\\(R_{L1}(\\Theta)\\) - Il termine di regolarizzazione L1 che viene aggiunto alla funzione di costo\n\\(\\alpha\\) - L’iperparametro di regolarizzazione L1 che controlla la forza della regolarizzazione\n\\(\\theta_{i}\\) - L’i-esimo parametro del modello\n\\(n\\) - Il numero di parametri nel modello\n\\(||\\theta_{i}||\\) - La norma L1, che assume il valore assoluto di ciascun parametro\n\nE la funzione di costo regolarizzata L1 completa è:\n\\[J(\\theta) = L(\\theta) + R_{L1}(\\Theta)\\]\nDove:\n\n\\(L(\\theta)\\) - La funzione di costo non regolarizzata originale\n\\(J(\\theta)\\) - La nuova funzione di costo regolarizzata\n\nLa scelta tra L1 e L2 dipende dalla complessità del modello prevista e dalla necessità o meno di una selezione di feature intrinseche. Entrambi richiedono una messa a punto iterativa su un set di validazione per selezionare l’iperparametro α ottimale.\nVideo 7.4 e Video 7.5 spiegano come funziona la regolarizzazione.\n\n\n\n\n\n\nVideo 7.4: Regolarizzazione\n\n\n\n\n\n\nVideo 7.5 spiega come la regolarizzazione può aiutare a ridurre l’overfitting del modello per migliorare le prestazioni.\n\n\n\n\n\n\nVideo 7.5: Perché la Regolarizzazione Riduce l’Overfitting\n\n\n\n\n\n\n\n\n7.7.2 Dropout\nUn altro metodo di regolarizzazione ampiamente adottato è “dropout” (Srivastava et al. 2014). Durante l’addestramento, dropout imposta casualmente una frazione \\(p\\) di output del nodo o attivazioni nascoste a zero. Questo incoraggia una maggiore distribuzione delle informazioni su più nodi anziché affidarsi a un piccolo numero di nodi. Al momento della previsione, viene utilizzata l’intera rete neurale, con attivazioni intermedie scalate di \\(1 - p\\) per mantenere le ampiezze di output. Le ottimizzazioni GPU semplificano l’implementazione efficiente di dropout tramite framework come PyTorch e TensorFlow.\n\nSrivastava, Nitish, Geoffrey E. Hinton, Alex Krizhevsky, Ilya Sutskever, e Ruslan Salakhutdinov. 2014. «Dropout: a simple way to prevent neural networks from overfitting.» J. Mach. Learn. Res. 15 (1): 1929–58. https://doi.org/10.5555/2627435.2670313.\nSiamo più precisi. Durante l’addestramento con dropout, l’output di ogni nodo \\(a_i\\) viene passato attraverso una maschera di dropout \\(r_i\\) prima di essere utilizzato dal layer successivo:\n\\[ ã_i = r_i \\odot a_i \\]\nDove:\n\n\\(a_i\\) - output del nodo \\(i\\)\n\\(ã_i\\) - output del nodo \\(i\\) dopo il dropout\n\\(r_i\\) - variabile casuale di Bernoulli indipendente con probabilità \\(1 - p\\) di essere 1\n\\(\\odot\\) - moltiplicazione elemento per elemento\n\nPer capire come funziona il dropout, è importante sapere che la maschera di dropout \\(r_i\\) è basata sulle variabili casuali di Bernoulli. Una variabile casuale di Bernoulli assume un valore di 1 con probabilità \\(1-p\\) (mantenendo l’attivazione) e un valore di 0 con probabilità \\(p\\) (dropping [perdendo] l’attivazione). Ciò significa che l’attivazione di ciascun nodo viene mantenuta o eliminata indipendentemente durante l’addestramento. Questa maschera di dropout \\(r_i\\) imposta casualmente una frazione \\(p\\) di attivazioni a 0 durante l’addestramento, costringendo la rete a creare rappresentazioni ridondanti.\nAl momento del test, la maschera di dropout viene rimossa e le attivazioni vengono ridimensionate di \\(1 - p\\) per mantenere le ampiezze di output previste:\n\\[ a_i^{test} = (1 - p)  a_i\\]\nDove:\n\n\\(a_i^{test}\\) - output del nodo al momento del test\n\\(p\\) - la probabilità di effettuare il dropping [eliminare] di un nodo.\n\nL’iperparametro chiave è \\(p\\), la probabilità di eliminare ogni nodo, spesso impostata tra 0.2 e 0.5. Le reti più grandi tendono a trarre vantaggio da un dropout maggiore, mentre le reti più piccole rischiano di non adattarsi se vengono eliminati troppi nodi. Tentativi ed errori combinati con il monitoraggio delle prestazioni di validazione aiutano a regolare il livello di dropout.\nVideo 7.6 discute l’intuizione alla base della tecnica di regolarizzazione del dropout e il suo funzionamento.\n\n\n\n\n\n\nVideo 7.6: Dropout\n\n\n\n\n\n\n\n\n7.7.3 Arresto Anticipato\nL’intuizione alla base di “early stopping” arresto anticipato implica il monitoraggio delle prestazioni del modello su un set di validazione “held-out” [esterno] in epoche di addestramento. Inizialmente, gli aumenti nell’idoneità del set di addestramento accompagnano i guadagni nell’accuratezza della validazione man mano che il modello rileva pattern generalizzabili. Dopo un certo punto, tuttavia, il modello inizia a sovradimensionarsi, agganciandosi a peculiarità e rumore nei dati di addestramento che non si applicano più in generale. Le prestazioni di validazione raggiungono il picco e poi si degradano se l’addestramento continua. Le regole di “arresto anticipato” interrompono l’addestramento a questo picco per evitare il sovradimensionamento. Questa tecnica dimostra come le pipeline ML debbano monitorare il feedback del sistema, non solo massimizzare incondizionatamente le prestazioni su un set di addestramento statico. Lo stato del sistema evolve e gli endpoint ottimali cambiano.\nPertanto, i metodi formali di arresto anticipato richiedono il monitoraggio di una metrica come l’accuratezza o la perdita di validazione dopo ogni epoca. Le curve comuni mostrano rapidi guadagni iniziali che si riducono gradualmente, alla fine raggiungendo un plateau e diminuiscono leggermente man mano che si verifica il sovradimensionamento. Il punto di arresto ottimale è spesso compreso tra 5 e 15 epoche oltre il picco, a seconda dei “patient threshold” [limiti della pazienza!]. Il monitoraggio di più metriche può migliorare il segnale poiché esiste una varianza tra le misure.\nLe semplici regole di arresto anticipato si interrompono immediatamente alla prima degradazione post-picco. Metodi più robusti introducono un parametro di “pazienza”, ovvero il numero di epoche di degradazione consentite prima dell’arresto. Ciò evita di interrompere prematuramente l’addestramento a causa di fluttuazioni transitorie. Le finestre di “pazienza” tipiche vanno da 50 a 200 batch di validazione. Finestre più ampie comportano il rischio di overfitting. Le strategie di ottimizzazione formali possono determinare la “pazienza” ottimale.\n\n\n\n\n\n\nEsercizio 7.3: Regolarizzazione\n\n\n\n\n\nCombattere l’Overfitting: Scoprire i Segreti della Regolarizzazione! L’overfitting è come se il modello memorizzasse le risposte a un test, per poi fallire l’esame reale. Le tecniche di regolarizzazione sono le guide di studio che aiutano il modello a generalizzare e ad affrontare nuovi problemi. In questo notebook Colab, impareremo come ottimizzare i parametri di regolarizzazione per risultati ottimali utilizzando la regolarizzazione L1 e L2, il dropout e l’arresto anticipato.\n\n\n\n\nVideo 7.7 tratta alcuni altri metodi di regolarizzazione che possono ridurre l’overfitting del modello.\n\n\n\n\n\n\nVideo 7.7: Altri Metodi di Regolarizzazione",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Addestramento dell'IA</span>"
    ]
  },
  {
    "objectID": "contents/core/training/training.it.html#funzioni-di-attivazione",
    "href": "contents/core/training/training.it.html#funzioni-di-attivazione",
    "title": "7  Addestramento dell’IA",
    "section": "7.8 Funzioni di Attivazione",
    "text": "7.8 Funzioni di Attivazione\nLe funzioni di attivazione svolgono un ruolo cruciale nelle reti neurali. Introducono comportamenti non lineari che consentono alle reti neurali di modellare pattern complessi. Le funzioni di attivazione elemento per elemento vengono applicate alle somme ponderate che arrivano a ciascun neurone nella rete. Senza funzioni di attivazione, le reti neurali sarebbero ridotte a modelli di regressione lineare.\nIdealmente, le funzioni di attivazione possiedono alcune qualità desiderabili:\n\nNon lineari: Consentono di modellare relazioni complesse tramite trasformazioni non lineari della somma degli input.\nDifferenziabili: Devono avere derivate prime ben definite per abilitare la retropropagazione e l’ottimizzazione basata sul gradiente durante l’addestramento.\nLimitazione dell’Intervallo: Limitano il segnale di output, impedendo un’esplosione. Ad esempio, la sigmoide schiaccia gli input a (0,1).\n\nInoltre, proprietà come efficienza computazionale, monotonicità e fluidità rendono alcune attivazioni più adatte di altre in base all’architettura di rete e alla complessità del problema.\nEsamineremo brevemente alcune delle funzioni di attivazione più ampiamente adottate e i loro punti di forza e limiti. Forniremo anche linee guida per la selezione di funzioni appropriate abbinate ai vincoli del sistema ML e alle esigenze dei casi d’uso.\n\n7.8.1 Sigmoide\nL’attivazione sigmoide applica una curva a forma di S schiacciante che lega strettamente l’output tra 0 e 1. Ha la forma matematica:\n\\[ sigmoid(x) = \\frac{1}{1+e^{-x}} \\]\nLa trasformazione esponenziale consente alla funzione di passare gradualmente da quasi 0 a quasi 1 quando l’input passa da molto negativo a molto positivo. L’aumento monotono copre l’intero intervallo (0,1).\nLa funzione sigmoide presenta diversi vantaggi. Fornisce sempre un gradiente uniforme per la retropropagazione e il suo output è limitato tra 0 e 1, il che aiuta a prevenire valori “esplosivi” durante l’addestramento. Inoltre, ha una semplice formula matematica che è facile da calcolare.\nTuttavia, la funzione sigmoide presenta anche alcuni svantaggi. Tende a saturarsi a valori di input estremi, il che può causare la “scomparsa” dei gradienti, rallentando o addirittura interrompendo il processo di apprendimento. Inoltre, la funzione non è centrata sullo zero, il che significa che i suoi output non sono distribuiti simmetricamente attorno allo zero, il che può portare ad aggiornamenti inefficienti durante l’addestramento.\n\n\n7.8.2 Tanh\nAnche Tanh o “tangente iperbolica” assume una forma a S ma è centrata sullo zero, il che significa che il valore medio dell’output è 0.\n\\[ tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}} \\]\nLa trasformazione numeratore/denominatore sposta l’intervallo da (0,1) in Sigmoide a (-1, 1) in tanh.\nLa maggior parte dei pro/contro sono condivisi con la Sigmoide, ma Tanh evita alcuni problemi di saturazione dell’output essendo centrata. Tuttavia, soffre ancora di gradienti che svaniscono con molti layer.\n\n\n7.8.3 ReLU\nLa Rectified Linear Unit (ReLU) introduce un semplice comportamento di soglia con la sua forma matematica:\n\\[ ReLU(x) = max(0, x) \\]\nLascia tutti gli input positivi invariati mentre taglia tutti i valori negativi a 0. Questa attivazione sparsa e il calcolo economico rendono ReLU ampiamente favorito rispetto a sigmoide/tanh.\nFigura 7.6 dimostra le 3 funzioni di attivazione di cui abbiamo discusso sopra in confronto a una funzione lineare:\n\n\n\n\n\n\nFigura 7.6: Funzioni di Attivazione Comuni. Fonte: AI Wiki.\n\n\n\n\n\n7.8.4 Softmax\nLa funzione di attivazione softmax è generalmente utilizzata come ultimo layer per le attività di classificazione per normalizzare il vettore del valore di attivazione in modo che i suoi elementi sommino a 1. Questo è utile per le attività di classificazione in cui vogliamo imparare a prevedere probabilità specifiche per classe di un input particolare, nel qual caso la probabilità cumulativa tra le classi è uguale a 1. La funzione di attivazione softmax è definita come\n\\[\\sigma(z_i) = \\frac{e^{z_{i}}}{\\sum_{j=1}^K e^{z_{j}}} \\ \\ \\ for\\ i=1,2,\\dots,K\\]\n\n\n7.8.5 Pro e Contro\nTabella 7.4 sono i pro e i contro riassuntivi di queste varie funzioni di attivazione standard:\n\n\n\nTabella 7.4: Confronto dei pro e dei contro di diversi algoritmi di ottimizzazione.\n\n\n\n\n\n\n\n\n\n\nAttivazione\nPro\nContro\n\n\n\n\nSigmoide\n\nGradiente uniforme per il backdrop [sfondo]\nOutput limitato tra 0 e 1\n\n\nLa saturazione elimina i gradienti\nNon centrato sullo zero\n\n\n\nTanh\n\nGradiente più uniforme della sigmoide\nOutput centrato sullo zero [-1, 1]\n\n\nSoffre ancora di problemi di gradiente evanescente\n\n\n\nReLU\n\nEfficiente dal punto di vista computazionale\nIntroduce la “sparsity” [scarsità]\nEvita gradienti evanescenti\n\n\nUnità “ReLU morenti”\nNon limitato\n\n\n\nSoftmax\n\nUtilizzato per l’ultimo livello per normalizzare gli output in modo che siano una distribuzione\nIn genere utilizzato per attività di classificazione\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEsercizio 7.4: Funzioni di Attivazione\n\n\n\n\n\nSblocchiamo la potenza delle funzioni di attivazione! Questi piccoli “muletti” matematici sono ciò che rende le reti neurali così incredibilmente flessibili. In questo notebook Colab, ci si cimenterà con funzioni come Sigmoid, tanh e la superstar ReLU. Guardiamo come trasformano gli input e scopriamo quale funziona meglio in diverse situazioni. È la chiave per costruire reti neurali in grado di affrontare problemi complessi!",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Addestramento dell'IA</span>"
    ]
  },
  {
    "objectID": "contents/core/training/training.it.html#inizializzazione-dei-pesi",
    "href": "contents/core/training/training.it.html#inizializzazione-dei-pesi",
    "title": "7  Addestramento dell’IA",
    "section": "7.9 Inizializzazione dei Pesi",
    "text": "7.9 Inizializzazione dei Pesi\nLa corretta inizializzazione dei pesi in una rete neurale prima dell’addestramento è un passaggio fondamentale che ha un impatto diretto sulle prestazioni del modello. L’inizializzazione casuale dei pesi a valori molto grandi o molto piccoli può portare a problemi come gradienti che svaniscono/esplodono, convergenza lenta dell’addestramento o intrappolati in minimi locali scadenti. La corretta inizializzazione del peso accelera la convergenza del modello durante l’addestramento e comporta implicazioni per le prestazioni del sistema al momento dell’inferenza negli ambienti di produzione. Alcuni aspetti chiave sono:\n\nTempo di Accuratezza più Rapido: Un’inizializzazione attentamente calibrata porta a una convergenza più rapida, che si traduce nel raggiungimento da parte dei modelli di traguardi di accuratezza target in anticipo nel ciclo di training. Ad esempio, l’inizializzazione Xavier potrebbe ridurre il tempo di accuratezza del 20% rispetto a un’inizializzazione casuale errata. Poiché l’addestramento è in genere la fase più dispendiosa in termini di tempo e calcolo, ciò migliora direttamente la velocità e la produttività del sistema ML.\nEfficienza del Ciclo di Iterazione del Modello: Se i modelli vengono addestrati più rapidamente, il tempo di risposta complessivo per le iterazioni di sperimentazione, valutazione e progettazione del modello diminuisce in modo significativo. I sistemi hanno maggiore flessibilità per esplorare architetture, pipeline di dati, ecc., entro determinati intervalli di tempo.\nImpatto sulle Epoche di Addestramento Necessarie: Il processo di addestramento viene eseguito per più epoche, con ogni passaggio completo attraverso i dati che rappresenta un’epoca. Una buona inizializzazione può ridurre le epoche necessarie per far convergere le curve di perdita e accuratezza sul set di addestramento del 10-30%. Ciò significa risparmi tangibili sui costi di risorse e infrastruttura.\nEffetto sugli Iperparametri di Addestramento: I parametri di inizializzazione del peso interagiscono fortemente con determinati iperparametri di regolarizzazione che governano le dinamiche di addestramento, come i programmi di velocità di apprendimento e le probabilità di abbandono. Trovare la giusta combinazione di impostazioni non è banale. Un’inizializzazione appropriata semplifica questa ricerca.\n\nL’inizializzazione dei pesi ha vantaggi a cascata per l’efficienza ingegneristica dell’apprendimento automatico e un overhead di risorse di sistema ridotto al minimo. È una tattica facilmente trascurata che ogni professionista dovrebbe padroneggiare. La scelta di quale tecnica di inizializzazione del peso utilizzare dipende da fattori come l’architettura del modello (numero di layer, pattern di connettività, ecc.), le funzioni di attivazione e il problema specifico da risolvere. Nel corso degli anni, i ricercatori hanno sviluppato e verificato empiricamente diverse strategie di inizializzazione mirate alle comuni architetture di reti neurali, di cui parleremo qui.\n\n7.9.1 Inizializzazione Uniforme e Normale\nQuando si inizializzano pesi in modo casuale, vengono comunemente utilizzate due distribuzioni di probabilità standard: uniforme e Gaussiana (normale). La distribuzione uniforme imposta una probabilità uguale che i parametri di peso iniziali rientrino in qualsiasi punto entro i limiti minimi e massimi impostati. Ad esempio, i limiti potrebbero essere -1 e 1, portando a una distribuzione uniforme dei pesi tra questi limiti. La distribuzione gaussiana, d’altra parte, concentra la probabilità attorno a un valore medio, seguendo la forma di una curva a campana. La maggior parte dei valori di peso si raggrupperà nella regione della media specificata, con meno campioni verso le estremità. Il parametro di deviazione standard controlla la distribuzione attorno alla media.\nLa scelta tra inizializzazione uniforme o normale dipende dall’architettura di rete e dalle funzioni di attivazione. Per reti poco profonde, si consiglia una distribuzione normale con una deviazione standard relativamente piccola (ad esempio, 0.01). La curva a campana impedisce valori di peso elevati che potrebbero innescare l’instabilità di addestramento in reti piccole. Per reti più profonde, una distribuzione normale con deviazione standard più elevata (diciamo 0.5 o superiore) o una distribuzione uniforme può essere preferita per tenere conto dei problemi di gradiente evanescente su molti layer. La maggiore diffusione determina una maggiore differenziazione tra i comportamenti dei neuroni. La messa a punto dei parametri di distribuzione di inizializzazione è fondamentale per una convergenza stabile e rapida del modello. Il monitoraggio dei trend di “loss” [perdita] di addestramento può diagnosticare i problemi per modificare i parametri in modo iterativo.\n\n\n7.9.2 Inizializzazione Xavier\nProposta da Glorot e Bengio (2010), questa tecnica di inizializzazione è progettata appositamente per le funzioni di attivazione sigmoide e tanh. Queste attivazioni saturate possono causare gradienti evanescenti o esplosivi durante la retro-propagazione su molti layer.\n\nGlorot, Xavier, e Yoshua Bengio. 2010. «Understanding the difficulty of training deep feedforward neural networks.» In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, 249–56. http://proceedings.mlr.press/v9/glorot10a.html.\nIl metodo Xavier imposta in modo intelligente la varianza della distribuzione dei pesi in base al numero di input e output per ciascun layer. L’intuizione è che questo bilancia il flusso di informazioni e gradienti in tutta la rete. Ad esempio, si consideri un layer con 300 unità di input e 100 unità di output. Inserendo questo nella formula varianza = 2/(#inputs + #outputs) si ottiene una varianza di 2/(300+100) = 0.01.\nIl campionamento dei pesi iniziali da una distribuzione uniforme o normale centrata su 0 con questa varianza fornisce una convergenza di addestramento molto più fluida per reti sigmoide/tanh profonde. I gradienti sono ben condizionati, impedendo la scomparsa o la crescita esponenziale.\n\n\n7.9.3 Inizializzazione He\nCome proposto da He et al. (2015), questa tecnica di inizializzazione è adattata alle funzioni di attivazione ReLU (Rectified Linear Unit). Le ReLU introducono il problema del neurone morente in cui le unità rimangono bloccate e producono solo 0 se inizialmente ricevono forti input negativi. Ciò rallenta e ostacola l’addestramento.\n\nHe, Kaiming, Xiangyu Zhang, Shaoqing Ren, e Jian Sun. 2015. «Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification». In 2015 IEEE International Conference on Computer Vision (ICCV), 1026–34. IEEE. https://doi.org/10.1109/iccv.2015.123.\n“He” supera questo problema campionando i pesi da una distribuzione con un set di varianza basato solo sul numero di input per layer, ignorando gli output. Ciò mantiene i segnali in arrivo sufficientemente piccoli da attivare le ReLU nel loro regime lineare dall’inizio, evitando unità morte. Per un layer con 1024 input, la formula varianza = 2/1024 = 0.002 mantiene la maggior parte dei pesi concentrati strettamente attorno a 0.\nQuesta inizializzazione specializzata consente alle reti ReLU di convergere in modo efficiente fin dall’inizio. La scelta tra Xavier e He deve corrispondere alla funzione di attivazione della rete prevista.\n\n\n\n\n\n\nEsercizio 7.5: Inizializzazione dei Pesi\n\n\n\n\n\nFacciamo partire la rete neurale col piede giusto con l’inizializzazione dei pesi! Il modo in cui si impostano quei pesi iniziali può fare la differenza nell’addestramento del modello. Si immagini di accordare gli strumenti di un’orchestra prima del concerto. In questo notebook Colab, si imparerà che la giusta strategia di inizializzazione può far risparmiare tempo, migliorare le prestazioni del modello e rendere il percorso di deep-learning molto più fluido.\n\n\n\n\nVideo 7.8 sottolinea l’importanza di selezionare deliberatamente i valori di peso iniziale rispetto a scelte casuali.\n\n\n\n\n\n\nVideo 7.8: Inizializzazione dei Pesi",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Addestramento dell'IA</span>"
    ]
  },
  {
    "objectID": "contents/core/training/training.it.html#colli-di-bottiglia-del-sistema",
    "href": "contents/core/training/training.it.html#colli-di-bottiglia-del-sistema",
    "title": "7  Addestramento dell’IA",
    "section": "7.10 “Colli di Bottiglia” del Sistema",
    "text": "7.10 “Colli di Bottiglia” del Sistema\nCome introdotto in precedenza, le reti neurali comprendono operazioni lineari (moltiplicazioni di matrici) intervallate da funzioni di attivazione non lineari elemento per elemento. La parte computazionalmente più costosa delle reti neurali sono le trasformazioni lineari, in particolare le moltiplicazioni di matrici tra ogni layer. Questi layer lineari mappano le attivazioni dal layer precedente a uno spazio dimensionale superiore che funge da input per la funzione di attivazione del layer successivo.\n\n7.10.1 Complessità a Runtime della Moltiplicazione di Matrici\n\nMoltiplicazioni di Layer vs. Attivazioni\nLa maggior parte del calcolo nelle reti neurali deriva dalle moltiplicazioni di matrici tra layer. Si consideri un layer di rete neurale con una dimensione di input di \\(M\\) = 500 e una dimensione di output di \\(N\\) = 1000; la moltiplicazione di matrici richiede \\(O(N \\cdot M) = O(1000 \\cdot 500) = 500,000\\) operazioni di moltiplicazione-accumulazione (MAC) tra quei layer.\nConfronta questo col layer precedente, che aveva \\(M\\) = 300 input, che richiedevano \\(O(500 \\cdot 300) = 150,000\\) operazioni. Man mano che le dimensioni dei layer aumentano, i requisiti computazionali aumentano quadraticamente con la dimensione del layer. I calcoli totali su \\(L\\) layer possono essere espressi come \\(\\sum_{l=1}^{L-1} O\\big(N^{(l)} \\cdot M^{(l-1)}\\big)\\), dove il calcolo richiesto per ogni layer dipende dal prodotto delle dimensioni di input e output delle matrici che vengono moltiplicate.\nOra, confrontando la moltiplicazione della matrice con la funzione di attivazione, che richiede solo \\(O(N) = 1000\\) non linearità elemento per elemento per \\(N = 1000\\) output, possiamo vedere le trasformazioni lineari che dominano le attivazioni computazionalmente.\nQueste grandi moltiplicazioni di matrici influiscono sulle scelte hardware, sulla latenza dell’inferenza e sui vincoli di potenza per le applicazioni di reti neurali nel mondo reale. Ad esempio, un tipico layer DNN potrebbe richiedere 500,000 moltiplicazioni-accumulazioni rispetto a solo 1000 attivazioni non lineari, dimostrando un aumento di 500x nelle operazioni matematiche.\nQuando si addestrano reti neurali, in genere utilizziamo la discesa del gradiente in mini-batch, operando su piccoli batch di dati contemporaneamente. Considerando una dimensione batch di \\(B\\) esempi di addestramento, l’input per la moltiplicazione di matrice diventa una matrice \\(M \\times B\\), mentre l’output è una matrice \\(N \\times B\\).\n\n\nMini-batch\nNell’addestramento delle reti neurali, dobbiamo stimare ripetutamente il gradiente della funzione di perdita rispetto ai parametri di rete (ad esempio, pesi e bias). Questo gradiente indica in quale direzione i parametri devono essere aggiornati per ridurre al minimo la perdita. Come introdotto in precedenza, eseguiamo aggiornamenti su un batch di dati a ogni aggiornamento, noto anche come discesa del gradiente stocastico o “discesa del gradiente mini-batch”.\nL’approccio più semplice è stimare il gradiente in base a un singolo esempio di addestramento, calcolare l’aggiornamento dei parametri, riassettare tutto e ripetere per l’esempio successivo. Tuttavia, ciò comporta aggiornamenti dei parametri molto piccoli e frequenti che possono essere computazionalmente inefficienti e potrebbero dover essere più accurati in termini di convergenza a causa della stocasticità dell’utilizzo di un solo dato per un aggiornamento del modello.\nInvece, la discesa del gradiente in mini-batch bilancia la stabilità della convergenza e l’efficienza computazionale. Invece di calcolare il gradiente su singoli esempi, stimiamo il gradiente in base a piccoli “mini-batch” di dati, solitamente tra 8 e 256 esempi in pratica.\nCiò fornisce una stima del gradiente rumorosa ma coerente che porta a una convergenza più stabile. Inoltre, l’aggiornamento dei parametri deve essere eseguito solo una volta per mini-batch anziché una volta per ogni esempio, riducendo il sovraccarico computazionale.\nRegolando la dimensione del mini-batch, possiamo controllare il compromesso tra la fluidità della stima (i batch più grandi sono generalmente migliori) e la frequenza degli aggiornamenti (i batch più piccoli consentono aggiornamenti più frequenti). Le dimensioni del mini-batch sono solitamente potenze di 2, quindi possono sfruttare in modo efficiente il parallelismo tra i core GPU.\nQuindi, il calcolo totale esegue una moltiplicazione di matrici \\(N \\times M\\) per \\(M \\times B\\), producendo \\(O(N \\cdot M \\cdot B)\\) operazioni in virgola mobile. Come esempio numerico, \\(N=1000\\) unità nascoste, \\(M=500\\) unità di input e una dimensione del batch \\(B=64\\) equivale a 1000 x 500 x 64 = 32 milioni di moltiplicazioni-accumulazioni per iterazione di training!\nAl contrario, le funzioni di attivazione vengono applicate elemento per elemento alla matrice di output \\(N \\times B\\), richiedendo solo \\(O(N \\cdot B)\\) calcoli. Per \\(N=1000\\) e \\(B=64\\), si tratta di sole 64,000 non linearità, ovvero 500 volte meno lavoro della moltiplicazione di matrici.\nMan mano che aumentiamo le dimensioni del batch per sfruttare appieno hardware parallelo come le GPU, la discrepanza tra la moltiplicazione di matrici e il costo della funzione di attivazione aumenta ulteriormente. Ciò rivela come l’ottimizzazione delle operazioni di algebra lineare offra enormi guadagni di efficienza.\nPertanto, la moltiplicazione di matrici è fondamentale nell’analisi di dove e come le reti neurali impiegano i calcoli. Ad esempio, le moltiplicazioni di matrici spesso rappresentano oltre il 90% della latenza di inferenza e del tempo di addestramento nelle comuni reti neurali convoluzionali e ricorrenti.\n\n\nOttimizzazione della Moltiplicazione di Matrici\nDiverse tecniche migliorano l’efficienza delle operazioni generali matrice-matrice densa/sparsa e matrice-vettore per migliorare l’efficienza complessiva. Alcuni metodi chiave sono:\n\nSfruttamento di librerie matematiche ottimizzate come cuBLAS per l’accelerazione GPU\nAbilitazione di formati di precisione inferiore come FP16 o INT8 dove l’accuratezza lo consente\nUtilizzo di Tensor Processing Unit con moltiplicazione di matrici in hardware\nCalcoli consapevoli della sparsità e formati di archiviazione dati per sfruttare i parametri zero\nApprossimazione delle moltiplicazioni di matrici con algoritmi come le Fast Fourier Transform\nProgettazione dell’architettura del modello per ridurre le larghezze e le attivazioni degli layer\nQuantizzazione, pruning [potatura], distillazione e altre tecniche di compressione\nParallelizzazione del calcolo sull’hardware disponibile\nRisultati di caching/pre-calcolo ove possibile per ridurre le operazioni ridondanti\n\nLe potenziali tecniche di ottimizzazione sono vaste, data la porzione sproporzionata di tempo che i modelli trascorrono nella matematica di matrici e vettori. Anche i miglioramenti incrementali velocizzano i tempi di esecuzione e riducono il consumo di energia. Trovare nuovi modi per migliorare queste primitive di algebra lineare rimane un’area di ricerca attiva allineata con le future esigenze di machine learning. Ne parleremo in dettaglio nei capitoli Ottimizzazioni e Accelerazione IA.\n\n\n\n7.10.2 Calcolo vs. Collo di Bottiglia della Memoria\nA questo punto, la moltiplicazione matrice-matrice è l’operazione matematica fondamentale alla base delle reti neurali. Sia l’addestramento che l’inferenza per le reti neurali utilizzano ampiamente queste operazioni di moltiplicazione di matrici. L’analisi mostra che oltre il 90% dei requisiti computazionali nelle reti neurali attuali derivano da moltiplicazioni di matrici. Di conseguenza, le prestazioni della moltiplicazione di matrici hanno un’enorme influenza sul tempo complessivo di addestramento o inferenza del modello.\n\nAddestramento vs. Inferenza\nMentre l’addestramento e l’inferenza si basano ampiamente sulle prestazioni della moltiplicazione di matrici, i loro profili computazionali precisi differiscono. In particolare, l’inferenza della rete neurale tende a essere più legata al calcolo rispetto all’addestramento per una dimensione di batch equivalente. La differenza fondamentale risiede nel passaggio di backpropagation, che è richiesto solo durante l’addestramento. La backpropagation implica una sequenza di operazioni di moltiplicazione di matrici per calcolare i gradienti rispetto alle attivazioni su ogni layer della rete. Tuttavia, è fondamentale che qui non sia necessaria alcuna larghezza di banda di memoria aggiuntiva: gli input, gli output e i gradienti vengono letti/scritti dalla cache o dai registri.\nDi conseguenza, l’addestramento mostra intensità aritmetiche inferiori, con calcoli del gradiente limitati dall’accesso alla memoria anziché dai FLOP (Floating Point Operations Per Second), una misura delle prestazioni computazionali che indica quanti calcoli in virgola mobile un sistema può eseguire al secondo. Al contrario, la propagazione in avanti domina l’inferenza della rete neurale, che corrisponde a una serie di moltiplicazioni matrice-matrice. Senza una retrospettiva del gradiente che richiede molta memoria, le dimensioni dei batch più grandi spingono facilmente l’inferenza a essere estremamente limitata dal calcolo. Le elevate intensità aritmetiche misurate mostrano questo. I tempi di risposta possono essere critici per alcune applicazioni di inferenza, costringendo il fornitore dell’applicazione a utilizzare una dimensione di batch inferiore per soddisfare questi requisiti di tempo di risposta, riducendo così l’efficienza dell’hardware; quindi, le inferenze potrebbero vedere un utilizzo inferiore dell’hardware.\nLe implicazioni sono che il provisioning hardware e i compromessi tra larghezza di banda e FLOP differiscono a seconda che un sistema miri al training o all’inferenza. I server ad alta produttività e bassa latenza per l’inferenza dovrebbero enfatizzare la potenza di calcolo anziché la memoria, mentre i cluster di training richiedono un’architettura più bilanciata.\nTuttavia, la moltiplicazione di matrici mostra un’interessante tensione: la larghezza di banda della memoria dell’hardware sottostante o le capacità di throughput aritmetico possono vincolarla. La capacità del sistema di recuperare e fornire dati matriciali rispetto alla sua capacità di eseguire operazioni di calcolo determina questa direzione.\nQuesto fenomeno ha impatti profondi; l’hardware deve essere progettato giudiziosamente e devono essere prese in considerazione le ottimizzazioni del software. Ottimizzare e bilanciare il calcolo rispetto alla memoria per alleviare questo collo di bottiglia della moltiplicazione di matrici è fondamentale per un training un deployment efficienti del modello.\nInfine, la dimensione del batch può avere un impatto sui tassi di convergenza durante l’addestramento della rete neurale, un’altra considerazione importante. Ad esempio, ci sono generalmente rendimenti decrescenti nei benefici della convergenza con dimensioni di batch estremamente grandi (ad esempio, &gt; 16384). Al contrario, dimensioni di batch estremamente grandi possono essere sempre più vantaggiose da una prospettiva di intensità hardware/aritmetica; l’utilizzo di batch così grandi potrebbe non tradursi in una convergenza più rapida rispetto al tempo a causa dei loro benefici decrescenti per la convergenza. Questi compromessi fanno parte delle decisioni di progettazione fondamentali per i sistemi per il tipo di ricerca basata sull’apprendimento automatico.\n\n\nDimensione del Batch\nLa dimensione del batch utilizzata durante l’addestramento e l’inferenza della rete neurale ha un impatto significativo sul fatto che la moltiplicazione di matrici rappresenti un collo di bottiglia computazionale o di memoria. In concreto, la dimensione del batch si riferisce al numero di campioni propagati assieme attraverso la rete in un passaggio avanti/indietro. La moltiplicazione di matrici equivale a dimensioni di matrice maggiori.\nIn particolare, diamo un’occhiata all’intensità aritmetica della moltiplicazione di matrici durante l’addestramento della rete neurale. Questa misura il rapporto tra operazioni computazionali e trasferimenti di memoria. La moltiplicazione di due matrici di dimensione \\(N \\times M\\) e \\(M \\times B\\) richiede \\(N \\times M \\times B\\) operazioni di moltiplicazione-accumulo, ma solo trasferimenti di \\(N \\times M + M \\times B\\) elementi di matrice.\nMan mano che aumentiamo la dimensione del batch \\(B\\), il numero di operazioni aritmetiche cresce più velocemente dei trasferimenti di memoria. Ad esempio, con una dimensione del batch di 1, abbiamo bisogno di \\(N \\times M\\) operazioni e \\(N + M\\) trasferimenti, dando un rapporto di intensità aritmetica di circa \\(\\frac{N \\times M}{N+M}\\). Ma con una dimensione del batch di grandi dimensioni di 128, il rapporto di intensità diventa \\(\\frac{128 \\times N \\times M}{N \\times M + M \\times 128} \\approx 128\\).\nL’utilizzo di una dimensione del batch più grande sposta il calcolo complessivo da vincolato alla memoria a più vincolato al calcolo. L’addestramento IA utilizza grandi dimensioni del batch ed è generalmente limitato dalle massime prestazioni di calcolo aritmetiche, ovvero l’Applicazione 3 in Figura 7.7. Pertanto, la moltiplicazione di matrici in batch è molto più intensiva dal punto di vista computazionale rispetto al limite di accesso alla memoria. Ciò ha implicazioni per la progettazione hardware e le ottimizzazioni software, che tratteremo in seguito. L’intuizione chiave è che possiamo modificare in modo significativo il profilo computazionale e i colli di bottiglia posti dall’addestramento e dall’inferenza della rete neurale regolando la dimensione del batch.\n\n\n\n\n\n\nFigura 7.7: Modello a profilo a di tetto per il training di IA.\n\n\n\n\n\nCaratteristiche Hardware\nL’hardware moderno come CPU e GPU è altamente ottimizzato per la produttività computazionale piuttosto che per la larghezza di banda della memoria. Ad esempio, le GPU H100 Tensor Core di fascia alta possono fornire oltre 60 TFLOPS di prestazioni a doppia precisione, ma forniscono solo fino a 3 TB/s di larghezza di banda della memoria. Ciò significa che c’è uno squilibrio di quasi 20 volte tra unità aritmetiche e accesso alla memoria; di conseguenza, per hardware come gli acceleratori GPU, i carichi di lavoro di addestramento della rete neurale devono essere resi il più intensivi possibile dal punto di vista computazionale per utilizzare appieno le risorse disponibili.\nCiò motiva ulteriormente la necessità di utilizzare batch di grandi dimensioni durante l’addestramento. Quando si utilizza un batch di piccole dimensioni, la moltiplicazione della matrice è limitata dalla larghezza di banda della memoria, sottoutilizzando le abbondanti risorse di elaborazione. Tuttavia, possiamo spostare il collo di bottiglia verso l’elaborazione e ottenere un’intensità aritmetica molto più elevata con batch sufficientemente grandi. Ad esempio, potrebbero essere necessari batch di 256 o 512 campioni per saturare una GPU di fascia alta. Lo svantaggio è che batch più grandi forniscono aggiornamenti dei parametri meno frequenti, il che può influire sulla convergenza. Tuttavia, il parametro funge da importante manopola di sintonizzazione per bilanciare le limitazioni di memoria e quelle di elaborazione.\nPertanto, date le architetture di elaborazione-memoria sbilanciate dell’hardware moderno, l’impiego di batch di grandi dimensioni è essenziale per alleviare i colli di bottiglia e massimizzare la produttività. Come accennato, anche il software e gli algoritmi successivi devono adattarsi a tali dimensioni di batch, poiché dimensioni di batch più grandi possono avere rendimenti decrescenti verso la convergenza della rete. L’utilizzo di dimensioni di batch molto piccole può portare a un utilizzo non ottimale dell’hardware, limitando in ultima analisi l’efficienza del training. L’aumento di dimensioni dei batch di grandi dimensioni è un argomento di ricerca esplorato in vari lavori che mirano a eseguire una training su larga scala (You et al. 2017).\n\nYou, Yang, Zhao Zhang, Cho-Jui Hsieh, James Demmel, e Kurt Keutzer. 2017. «ImageNet Training in Minutes», settembre. http://arxiv.org/abs/1709.05011v10.\n\n\nArchitetture dei Modelli\nL’architettura della rete neurale influisce anche sul fatto che la moltiplicazione di matrici rappresenti un collo di bottiglia computazionale o di memoria maggiore durante l’esecuzione. I trasformatori e gli MLP sono molto più vincolati al calcolo rispetto alle reti neurali convoluzionali CNN. Ciò deriva dai tipi di operazioni di moltiplicazione di matrici coinvolte in ciascun modello. I trasformatori si basano sull’auto-attenzione, moltiplicando grandi matrici di attivazione per enormi matrici di parametri per correlare gli elementi. Gli MLP impilano layer completamente connessi, richiedendo anche grandi moltiplicazioni matriciali.\nAl contrario, i layer convoluzionali nelle CNN hanno una finestra scorrevole che riutilizza attivazioni e parametri nell’input, il che significa che sono necessarie meno operazioni matriciali univoche. Tuttavia, le convoluzioni richiedono l’accesso ripetuto a piccole parti di input e lo spostamento di somme parziali per popolare ciascuna finestra. Sebbene le operazioni aritmetiche nelle convoluzioni siano intense, questo spostamento di dati e la manipolazione del buffer impongono enormi overhead di accesso alla memoria. Le CNN comprendono diverse fasi a strati, quindi gli output intermedi devono materializzarsi frequentemente nella memoria.\nDi conseguenza, l’addestramento CNN tende a essere più vincolato alla larghezza di banda della memoria rispetto al limite aritmetico in confronto a Transformers e MLP. Pertanto, il profilo di moltiplicazione della matrice e, a sua volta, il collo di bottiglia posto, varia in modo significativo in base alla scelta del modello. Hardware e sistemi devono essere progettati con un appropriato equilibrio di larghezza di banda di elaborazione-memoria a seconda dell’implementazione del modello target. I modelli che si basano maggiormente sull’attenzione e sui layer MLP richiedono una maggiore produttività aritmetica rispetto alle CNN, il che richiede un’elevata larghezza di banda della memoria.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Addestramento dell'IA</span>"
    ]
  },
  {
    "objectID": "contents/core/training/training.it.html#parallelizzazione-del-training",
    "href": "contents/core/training/training.it.html#parallelizzazione-del-training",
    "title": "7  Addestramento dell’IA",
    "section": "7.11 Parallelizzazione del Training",
    "text": "7.11 Parallelizzazione del Training\nL’addestramento delle reti neurali comporta richieste di calcolo e memoria intensive. L’algoritmo di backpropagation per il calcolo dei gradienti e l’aggiornamento dei pesi consiste in ripetute moltiplicazioni di matrici e operazioni aritmetiche sull’intero set di dati. Ad esempio, un passaggio di backpropagation scala in complessità temporale con \\(O(num\\_parameters \\times batch\\_size \\times sequence\\_length)\\).\nI requisiti di calcolo aumentano rapidamente con l’aumento delle dimensioni del modello in parametri e layer. Inoltre, l’algoritmo richiede l’archiviazione di output di attivazione e parametri del modello per la fase di backward, che cresce con le dimensioni del modello.\nI modelli più grandi non possono adattarsi e addestrarsi su un singolo dispositivo acceleratore come una GPU e l’ingombro di memoria diventa proibitivo. Pertanto, dobbiamo parallelizzare l’addestramento del modello su più dispositivi per fornire elaborazione e memoria sufficienti per addestrare reti neurali all’avanguardia.\nCome mostrato in Figura 7.8, i due approcci principali sono il parallelismo dei dati, che replica il modello su più dispositivi suddividendo i dati di input in batch, e il parallelismo del modello, che suddivide l’architettura del modello stesso su diversi dispositivi. Tramite il training in parallelo, possiamo sfruttare maggiori risorse aggregate di elaborazione e memoria per superare le limitazioni del sistema e accelerare i carichi di lavoro di deep learning.\n\n\n\n\n\n\nFigura 7.8: Parallelismo dei dati e parallelismo del modello.\n\n\n\n\n7.11.1 Parallelismo dei Dati\nLa parallelizzazione dei dati è un approccio comune per parallelizzare il training di apprendimento automatico su più unità di elaborazione, come GPU o risorse di elaborazione distribuite. Il set di dati di addestramento è suddiviso in batch nel parallelismo dei dati e un’unità di elaborazione separata elabora ogni batch. I parametri del modello vengono poi aggiornati in base ai gradienti calcolati dall’elaborazione di ogni batch. Ecco una descrizione dettagliata della parallelizzazione dei dati per il training ML:\n\nDivisione del Dataset: Il set di dati di addestramento è suddiviso in batch più piccoli, ciascuno contenente un sottoinsieme degli esempi di training.\nReplica del Modello: Il modello di rete neurale è replicato su tutte le unità di elaborazione e ogni unità di elaborazione ha la sua copia.\nCalcolo Parallelo: Ogni unità di elaborazione prende un batch diverso e calcola in modo indipendente i passaggi in forward e backward. Durante il passaggio forward [in avanti], il modello fa delle previsioni sui dati di input. La funzione di loss [perdita] determina i gradienti per i parametri del modello durante il passaggio backward [all’indietro].\nAggregazione dei Gradienti: Dopo l’elaborazione dei rispettivi batch, i gradienti di ogni unità di elaborazione vengono aggregati. I metodi di aggregazione comuni includono la sommatoria o la media dei gradienti.\nAggiornamento dei Parametri: I gradienti aggregati aggiornano i parametri del modello. L’aggiornamento può essere eseguito utilizzando algoritmi di ottimizzazione come SGD o varianti come Adam.\nSincronizzazione: Dopo l’aggiornamento, tutte le unità di elaborazione sincronizzano i parametri del modello, assicurandosi che ciascuna ne abbia la versione più recente.\n\nI passaggi precedenti vengono ripetuti per diverse iterazioni o fino alla convergenza.\nPrendiamo un esempio specifico. Abbiamo 256 dimensioni di batch e 8 GPU; ogni GPU riceverà un micro-batch di 32 campioni. I loro passaggi forward e backward calcolano perdite e gradienti solo in base ai 32 campioni locali. I gradienti vengono aggregati tra i dispositivi con un server dei parametri o una libreria di comunicazioni collettiva per ottenere il gradiente effettivo per il batch globale. Gli aggiornamenti dei pesi avvengono indipendentemente su ogni GPU in base a questi gradienti. Dopo un numero configurato di iterazioni, i pesi aggiornati si sincronizzano e si equalizzano tra i dispositivi prima di passare alle iterazioni successive.\nIl parallelismo dei dati è efficace quando il modello è grande e il set di dati è sostanziale, poiché consente l’elaborazione parallela di diverse parti dei dati. È ampiamente utilizzato in framework e librerie di deep learning che supportano il training distribuito, come TensorFlow e PyTorch. Tuttavia, per garantire una parallelizzazione efficiente, è necessario prestare attenzione a gestire problemi come l sovraccarico della comunicazione, bilanciamento del carico e sincronizzazione.\n\n\n7.11.2 Parallelismo del Modello\nIl parallelismo del modello si riferisce alla distribuzione del modello di rete neurale su più dispositivi anziché alla replica del modello completo come il parallelismo dei dati. Ciò è particolarmente utile quando un modello è troppo grande per essere inserito nella memoria di una singola GPU o di un dispositivo acceleratore. Sebbene ciò potrebbe non essere specificamente applicabile per casi d’uso embedded o TinyML poiché la maggior parte dei modelli è relativamente piccola, è comunque utile saperlo.\nNell’addestramento parallelo del modello, diverse parti o layer del modello vengono assegnati a dispositivi separati. Le attivazioni di input e gli output intermedi vengono partizionati e passati tra questi dispositivi durante i passaggi forward e backward per coordinare i calcoli del gradiente tra le partizioni del modello.\nIl “footprint” [impronta] di memoria e le operazioni di calcolo vengono distribuite suddividendo l’architettura del modello su più dispositivi anziché concentrarsi su uno. Ciò consente l’addestramento di modelli molto grandi con miliardi di parametri che altrimenti supererebbero la capacità di un singolo dispositivo. Esistono diversi modi in cui possiamo eseguire il partizionamento:\n\nParallelismo di Layer: I layer consecutivi sono distribuiti su dispositivi diversi. Ad esempio, il dispositivo 1 contiene i layer 1-3; il dispositivo 2 contiene i layer 4-6. Le attivazioni di output dal layer 3 verrebbero trasferite al dispositivo 2 per avviare i layer successivi per i calcoli della fase di forward.\nParallelismo a Livello di Filtro: Nei layer convoluzionali, i filtri di output possono essere suddivisi tra più dispositivi. Ogni dispositivo calcola gli output di attivazione per un sottoinsieme di filtri, che vengono concatenati prima di propagarsi ulteriormente.\nParallelismo Spaziale: Le immagini di input vengono divise spazialmente, quindi ogni dispositivo elabora una determinata regione come il quarto in alto a sinistra delle immagini. Le regioni di output si combinano poi per formare l’output completo.\n\nInoltre, le combinazioni ibride possono suddividere il modello a livello di layer e i dati in batch. Il tipo appropriato di parallelismo del modello dipende dai vincoli specifici dell’architettura neurale e dalla configurazione hardware. Ottimizzare il partizionamento e la comunicazione per la topologia del modello è fondamentale per ridurre al minimo il sovraccarico.\nTuttavia, poiché le parti del modello vengono eseguite su dispositivi fisicamente separati, devono comunicare e sincronizzare i loro parametri durante ogni fase di addestramento. La fase di backward deve garantire che gli aggiornamenti del gradiente si propaghino accuratamente tra le partizioni del modello. Quindi, il coordinamento e l’interconnessione ad alta velocità tra i dispositivi sono fondamentali per ottimizzare le prestazioni dell’addestramento parallelo. Sono necessari dei buoni protocolli di partizionamento e comunicazione per ridurre al minimo il sovraccarico di trasferimento.\n\n\n7.11.3 Confronto\nRiassumendo, Tabella 7.5 illustra alcune delle caratteristiche chiave per confrontare il parallelismo dei dati e quello dei modelli.\n\n\n\nTabella 7.5: Confronto tra parallelismo dei dati e parallelismo del modello.\n\n\n\n\n\n\n\n\n\n\nCaratteristica\nParallelismo dei dati\nParallelismo del modello\n\n\n\n\nDefinizione\nDistribuisce i dati tra i dispositivi con repliche\nDistribuisce il modello tra i dispositivi\n\n\nObiettivo\nAccelera il training tramite il ridimensionamento del calcolo\nAbilita un training del modello più ampio\n\n\nMetodo di Ridimensionamento\nDispositivi/workers in scala\nDimensioni modello in scala\n\n\nVincolo Principale\nDimensione del modello per ogni dispositivo\nOverhead di coordinamento dispositivo\n\n\nRequisiti Hardware\nPiù GPU/TPU\nSpesso interconnessione specializzata\n\n\nDifficoltà Principale\nSincronizzazione dei parametri\nPartizionamento e comunicazione complicati\n\n\nTipi\nN/D\nPer livello, per filtro, spaziale\n\n\nComplessità del Codice\nModifiche minime\nIntervento più significativa sul modello\n\n\nLibrerie Popolari\nHorovod, PyTorch Distributed\nMesh TensorFlow",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Addestramento dell'IA</span>"
    ]
  },
  {
    "objectID": "contents/core/training/training.it.html#conclusione",
    "href": "contents/core/training/training.it.html#conclusione",
    "title": "7  Addestramento dell’IA",
    "section": "7.12 Conclusione",
    "text": "7.12 Conclusione\nIn questo capitolo abbiamo trattato le basi fondamentali che consentono un training efficace dei modelli di intelligenza artificiale. Abbiamo esplorato concetti matematici come funzioni di perdita, backpropagation e discesa del gradiente che rendono possibile l’ottimizzazione delle reti neurali. Abbiamo anche discusso tecniche pratiche per sfruttare i dati di training, la regolarizzazione, la messa a punto degli iperparametri, l’inizializzazione dei pesi e strategie di parallelizzazione distribuita che migliorano convergenza, generalizzazione e scalabilità.\nQueste metodologie costituiscono il fondamento attraverso cui è stato raggiunto il successo del deep learning nell’ultimo decennio. Padroneggiare questi fondamenti prepara i professionisti a progettare sistemi e perfezionare modelli su misura per il loro contesto. Tuttavia, man mano che modelli e set di dati crescono in modo esponenziale, i sistemi di training devono ottimizzare parametri come tempo, costo e “carbon footprint” [impatto ambientale]. Il ridimensionamento hardware tramite grosse warehouse consente un throughput computazionale enorme, ma le ottimizzazioni relative a efficienza e specializzazione saranno fondamentali. Tecniche software come compressione e sfruttamento delle matrici sparse possono aumentare i guadagni hardware. Ne discuteremo diverse nei prossimi capitoli.\nNel complesso, i fondamenti trattati in questo capitolo preparano i professionisti a costruire, perfezionare e distribuire modelli. Tuttavia, le competenze interdisciplinari che abbracciano teoria, sistemi e hardware differenzieranno gli esperti in grado di portare l’IA al livello successivo in modo sostenibile e responsabile, come richiesto dalla società. Comprendere l’efficienza insieme all’accuratezza costituisce l’approccio ingegneristico bilanciato necessario per addestrare sistemi intelligenti che si integrano senza problemi in molti contesti del mondo reale.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Addestramento dell'IA</span>"
    ]
  },
  {
    "objectID": "contents/core/training/training.it.html#sec-ai-training-resource",
    "href": "contents/core/training/training.it.html#sec-ai-training-resource",
    "title": "7  Addestramento dell’IA",
    "section": "7.13 Risorse",
    "text": "7.13 Risorse\nEcco un elenco curato di risorse per supportare studenti e insegnanti nei loro percorsi di apprendimento e insegnamento. Stiamo lavorando costantemente per espandere questa raccolta e presto aggiungeremo nuovi esercizi.\n\n\n\n\n\n\nSlide\n\n\n\n\n\nQueste slide sono uno strumento prezioso per gli insegnanti per tenere lezioni e per gli studenti per rivedere il materiale secondo il proprio ritmo. Incoraggiamo studenti e docenti a sfruttare queste slide per migliorare la loro comprensione e facilitare un trasferimento efficace delle conoscenze.\n\nThinking About Loss.\nMinimizing Loss.\nTraining, Validation, and Test Data.\nContinuous Training:\n\nRetraining Trigger.\nData Processing Overview.\nData Ingestion.\nData Validation.\nData Transformation.\nTraining with AutoML.\nContinuous Training with Transfer Learning.\nContinuous Training Use Case Metrics.\nContinuous Training Impact on MLOps.\n\n\n\n\n\n\n\n\n\n\n\nVideo\n\n\n\n\n\n\nVideo 7.1\nVideo 7.2\nVideo 7.3\nVideo 7.4\nVideo 7.5\nVideo 7.6\nVideo 7.7\nVideo 7.8\n\n\n\n\n\n\n\n\n\n\nEsercizi\n\n\n\n\n\nPer rafforzare i concetti trattati in questo capitolo, abbiamo curato una serie di esercizi che sfidano gli studenti ad applicare le proprie conoscenze e ad approfondire la propria comprensione.\n\nEsercizio 7.1\nEsercizio 7.2\nEsercizio 7.3\nEsercizio 7.5\nEsercizio 7.4\n\n\n\n\n\n\n\n\n\n\nLaboratori\n\n\n\n\n\nOltre agli esercizi, offriamo una serie di laboratori pratici che consentono agli studenti di acquisire esperienza pratica con le tecnologie di intelligenza artificiale embedded. Questi laboratori forniscono una guida passo dopo passo, consentendo agli studenti di sviluppare le proprie competenze in un ambiente strutturato e di supporto. Siamo lieti di annunciare che presto saranno disponibili nuovi laboratori, che arricchiranno ulteriormente l’esperienza di apprendimento.\n\nProssimamente.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Addestramento dell'IA</span>"
    ]
  },
  {
    "objectID": "contents/core/efficient_ai/efficient_ai.it.html",
    "href": "contents/core/efficient_ai/efficient_ai.it.html",
    "title": "8  IA Efficiente",
    "section": "",
    "text": "8.1 Panoramica\nI modelli di training possono consumare molta energia, a volte equivalente all’impatto ambientale di processi industriali considerevoli. Tratteremo alcuni di questi dettagli sulla sostenibilità nel capitolo Sostenibilità dell’IA. Dal punto di vista dell’implementazione, se questi modelli non sono ottimizzati per l’efficienza, possono esaurire rapidamente le batterie dei dispositivi, richiedere una memoria eccessiva o non soddisfare le esigenze di elaborazione in tempo reale. In questo capitolo, miriamo a chiarire le sfumature dell’efficienza, gettando le basi per un’esplorazione completa nei capitoli successivi.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>IA Efficiente</span>"
    ]
  },
  {
    "objectID": "contents/core/efficient_ai/efficient_ai.it.html#la-necessità-di-unia-efficiente",
    "href": "contents/core/efficient_ai/efficient_ai.it.html#la-necessità-di-unia-efficiente",
    "title": "8  IA Efficiente",
    "section": "8.2 La Necessità di un’IA Efficiente",
    "text": "8.2 La Necessità di un’IA Efficiente\nL’efficienza assume connotazioni diverse a seconda di dove si verificano i calcoli dell’IA. Rivediamo Cloud, Edge e TinyML (come discusso in Sistemi di ML) e distinguiamoli in termini di efficienza. Figura 8.1 fornisce un confronto generale delle tre diverse piattaforme.\n\n\n\n\n\n\nFigura 8.1: Cloud, Mobile e TinyML. Fonte: Schizas et al. (2022).\n\n\nSchizas, Nikolaos, Aristeidis Karras, Christos Karras, e Spyros Sioutas. 2022. «TinyML for Ultra-Low Power AI and Large Scale IoT Deployments: A Systematic Review». Future Internet 14 (12): 363. https://doi.org/10.3390/fi14120363.\n\n\nIA Cloud: I modelli IA tradizionali vengono spesso eseguiti in data center su larga scala dotati di potenti GPU e TPU (Barroso, Hölzle, e Ranganathan 2019). Qui, l’efficienza riguarda l’ottimizzazione delle risorse di calcolo, la riduzione dei costi e la garanzia di elaborazione e restituzione tempestive dei dati. Tuttavia, fare affidamento sul cloud introduce latenza, soprattutto quando si ha a che fare con flussi di dati di grandi dimensioni che richiedono caricamento, elaborazione e download.\n\nBarroso, Luiz André, Urs Hölzle, e Parthasarathy Ranganathan. 2019. The Datacenter as a Computer: Designing Warehouse-Scale Machines. Springer International Publishing. https://doi.org/10.1007/978-3-031-01761-2.\n\nLi, En, Liekang Zeng, Zhi Zhou, e Xu Chen. 2020. «Edge AI: On-demand Accelerating Deep Neural Network Inference via Edge Computing». IEEE Trans. Wireless Commun. 19 (1): 447–57. https://doi.org/10.1109/twc.2019.2946140.\nIA Edge: L’edge computing avvicina l’intelligenza artificiale alla fonte dei dati, elaborando le informazioni direttamente su dispositivi locali come smartphone, fotocamere o macchine industriali (Li et al. 2020). Qui, l’efficienza comprende risposte rapide in tempo reale e ridotte esigenze di trasmissione dei dati. Tuttavia, i vincoli sono più severi: questi dispositivi, sebbene più potenti dei microcontrollori, hanno una potenza di calcolo limitata rispetto alle configurazioni cloud.\nTinyML: TinyML supera i limiti consentendo ai modelli di intelligenza artificiale di funzionare su microcontrollori o ambienti con risorse estremamente limitate. La differenza di prestazioni del processore e della memoria tra TinyML e i sistemi cloud o mobili può essere di diversi ordini di grandezza (Warden e Situnayake 2019). L’efficienza in TinyML consiste nell’assicurare che i modelli siano sufficientemente leggeri da adattarsi a questi dispositivi, consumino il minimo di energia (fondamentale per i dispositivi alimentati a batteria) e continuino a svolgere le loro attività in modo efficace.\n\nWarden, Pete, e Daniel Situnayake. 2019. Tinyml: Machine learning with tensorflow lite on arduino and ultra-low-power microcontrollers. O’Reilly Media.\nLo spettro da Cloud a TinyML rappresenta un passaggio da vaste risorse di elaborazione centralizzate ad ambienti distribuiti, localizzati e limitati. Passando dall’uno all’altro, i problemi e le strategie relative all’efficienza evolvono, sottolineando la necessità di approcci specializzati su misura per ogni scenario. Dopo aver stabilito la necessità di un’intelligenza artificiale efficiente, in particolare nel contesto di TinyML, passeremo all’esplorazione delle metodologie ideate per rispondere a queste sfide. Le sezioni seguenti delineano i concetti principali che approfondiremo in seguito. Dimostreremo l’ampiezza e la profondità dell’innovazione necessarie per ottenere un’intelligenza artificiale efficiente mentre esploriamo queste strategie.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>IA Efficiente</span>"
    ]
  },
  {
    "objectID": "contents/core/efficient_ai/efficient_ai.it.html#architetture-di-modelli-efficienti",
    "href": "contents/core/efficient_ai/efficient_ai.it.html#architetture-di-modelli-efficienti",
    "title": "8  IA Efficiente",
    "section": "8.3 Architetture di Modelli Efficienti",
    "text": "8.3 Architetture di Modelli Efficienti\nSelezionare un’architettura del modello ottimale è tanto cruciale quanto ottimizzarla. Negli ultimi anni, i ricercatori hanno compiuto passi da gigante nell’esplorazione di architetture innovative che possono avere intrinsecamente meno parametri pur mantenendo prestazioni elevate.\nMobileNet: MobileNet sono modelli di applicazioni di visione mobile ed embedded efficienti (Howard et al. 2017). L’idea chiave che ha portato al loro successo sono le convoluzioni separabili in profondità, che riducono significativamente il numero di parametri e calcoli nella rete. MobileNetV2 e V3 migliorano ulteriormente questo design introducendo residui invertiti e colli di bottiglia lineari.\n\nHoward, Andrew G., Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, e Hartwig Adam. 2017. «MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications». ArXiv preprint. https://arxiv.org/abs/1704.04861.\n\nIandola, Forrest N, Song Han, Matthew W Moskewicz, Khalid Ashraf, William J Dally, e Kurt Keutzer. 2016. «SqueezeNet: Alexnet-level accuracy with 50x fewer parameters and 0.5 MB model size». ArXiv preprint abs/1602.07360. https://arxiv.org/abs/1602.07360.\nSqueezeNet: SqueezeNet è una classe di modelli ML noti per le sue dimensioni ridotte senza sacrificare la precisione. Ciò si ottiene utilizzando un “modulo fire” che riduce il numero di canali di input a filtri 3x3, riducendo così i parametri (Iandola et al. 2016). Inoltre, impiega il downsampling [sottocampionamento] ritardato per aumentare la precisione mantenendo una mappa delle feature più ampia.\nVarianti ResNet: L’architettura Residual Network (ResNet) consente l’introduzione di connessioni skip o scorciatoie (He et al. 2016). Alcune varianti di ResNet sono progettate per essere più efficienti. Ad esempio, ResNet-SE incorpora il meccanismo “squeeze and excitation” per ricalibrare le feature map (Hu, Shen, e Sun 2018), mentre ResNeXt offre convoluzioni raggruppate per l’efficienza (Xie et al. 2017).\n\nHe, Kaiming, Xiangyu Zhang, Shaoqing Ren, e Jian Sun. 2016. «Deep Residual Learning for Image Recognition». In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 770–78. IEEE. https://doi.org/10.1109/cvpr.2016.90.\n\nHu, Jie, Li Shen, e Gang Sun. 2018. «Squeeze-and-Excitation Networks». In 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, 7132–41. IEEE. https://doi.org/10.1109/cvpr.2018.00745.\n\nXie, Saining, Ross Girshick, Piotr Dollar, Zhuowen Tu, e Kaiming He. 2017. «Aggregated Residual Transformations for Deep Neural Networks». In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 1492–1500. IEEE. https://doi.org/10.1109/cvpr.2017.634.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>IA Efficiente</span>"
    ]
  },
  {
    "objectID": "contents/core/efficient_ai/efficient_ai.it.html#sec-efficient-model-compression",
    "href": "contents/core/efficient_ai/efficient_ai.it.html#sec-efficient-model-compression",
    "title": "8  IA Efficiente",
    "section": "8.4 Compressione Efficiente del Modello",
    "text": "8.4 Compressione Efficiente del Modello\nI metodi di compressione dei modelli sono essenziali per portare i modelli di apprendimento profondo su dispositivi con risorse limitate. Queste tecniche riducono le dimensioni dei modelli, il consumo energetico e le richieste di elaborazione senza perdere significativamente la precisione. Ad alto livello, i metodi possono essere categorizzati nei seguenti metodi fondamentali:\nPruning: L’Abbiamo menzionato un paio di volte nei capitoli precedenti, ma non l’abbiamo ancora formalmente introdotta. Il pruning è simile alla potatura dei rami di un albero. Questo è stato pensato per la prima volta nel documento Optimal Brain Damage (LeCun, Denker, e Solla 1989) ed è stato successivamente reso popolare nel contesto del deep learning da Han, Mao, e Dally (2016). Determinati pesi o interi neuroni vengono rimossi dalla rete nella potatura in base a criteri specifici. Questo può ridurre significativamente le dimensioni del modello. In Sezione 9.2.1 esploreremo due delle principali strategie di potatura, quella strutturata e quella non-strutturata. Figura 8.2 è un esempio di potatura della rete neurale, in cui la rimozione di alcuni nodi negli strati interni (in base a criteri specifici) riduce il numero di rami tra i nodi e, a sua volta, le dimensioni del modello.\n\nLeCun, Yann, John Denker, e Sara Solla. 1989. «Optimal brain damage». Adv Neural Inf Process Syst 2.\n\nHan, Song, Huizi Mao, e William J. Dally. 2016. «Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding». https://arxiv.org/abs/1510.00149.\n\n\n\n\n\n\nFigura 8.2: Neural Network Pruning.\n\n\n\nQuantizzazione: La quantizzazione è il processo di limitazione di un input da un set ampio a un output in un set più piccolo, principalmente nel deep learning; ciò significa ridurre il numero di bit che rappresentano i pesi e i bias del modello. Ad esempio, l’utilizzo di rappresentazioni a 16 o 8 bit anziché a 32 bit può ridurre la dimensione del modello e velocizzare i calcoli, con un piccolo compromesso in termini di accuratezza. Esploreremo questi aspetti più in dettaglio in Sezione 9.3.4. Figura 8.3 mostra un esempio di quantizzazione mediante arrotondamento al numero più vicino. La conversione da virgola mobile a 32 bit a 16 bit riduce l’utilizzo della memoria del 50%. Passare da un intero a 32 bit a uno a 8 bit riduce l’utilizzo della memoria del 75%. Mentre la perdita di precisione numerica e, di conseguenza, di prestazioni del modello è minima, l’efficienza nell’utilizzo della memoria è significativa.\n\n\n\n\n\n\nFigura 8.3: Diverse forme di quantizzazione.\n\n\n\nKnowledge Distillation: La “distillazione della conoscenza” comporta l’addestramento di un modello più piccolo (studente) per replicare il comportamento di un modello più grande (insegnante). L’idea è quella di trasferire la conoscenza dal modello ingombrante a quello leggero. Quindi, il modello più piccolo raggiunge prestazioni vicine alla sua controparte più grande ma con parametri significativamente inferiori. Figura 8.4 mostra la struttura tutor-studente per la distillazione della conoscenza. Esploreremo la “distillazione della conoscenza” in modo più dettagliato in Sezione 9.2.2.1.\n\n\n\n\n\n\nFigura 8.4: Il framework tutor-studente per la distillazione della conoscenza. Fonte: Medium",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>IA Efficiente</span>"
    ]
  },
  {
    "objectID": "contents/core/efficient_ai/efficient_ai.it.html#hardware-di-inferenza-efficiente",
    "href": "contents/core/efficient_ai/efficient_ai.it.html#hardware-di-inferenza-efficiente",
    "title": "8  IA Efficiente",
    "section": "8.5 Hardware di Inferenza Efficiente",
    "text": "8.5 Hardware di Inferenza Efficiente\nNel capitolo Training, abbiamo discusso il processo di training dei modelli di intelligenza artificiale. Ora, dal punto di vista dell’efficienza, è importante notare che il training è un’attività che richiede molte risorse e molto tempo, spesso richiede hardware potente e impiega da ore a settimane per essere completato. L’inferenza, d’altra parte, deve essere il più veloce possibile, soprattutto nelle applicazioni in tempo reale. È qui che entra in gioco un hardware di inferenza efficiente. Ottimizzando l’hardware specificamente per le attività di inferenza, possiamo ottenere tempi di risposta rapidi e un funzionamento efficiente dal punto di vista energetico, il che è particolarmente cruciale per i dispositivi edge e i sistemi embedded.\nTPU (Tensor Processing Unit): Le TPU sono ASIC (Application-Specific Integrated Circuits) personalizzati da Google per accelerare i carichi di lavoro di apprendimento automatico (Jouppi et al. 2017). Sono ottimizzate per le operazioni tensoriali, offrono un throughput elevato per l’aritmetica a bassa precisione e sono progettate specificamente per il machine learning delle reti neurali. Le TPU accelerano significativamente l’addestramento e l’inferenza del modello rispetto alle GPU/CPU generiche. Questo potenziamento si traduce in un addestramento più rapido dei modelli e in capacità di inferenza in tempo reale o quasi reale, fondamentali per applicazioni come la ricerca vocale e la realtà aumentata.\n\nJouppi, Norman P., Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal, Raminder Bajwa, Sarah Bates, et al. 2017. «In-Datacenter Performance Analysis of a Tensor Processing Unit». In Proceedings of the 44th Annual International Symposium on Computer Architecture, 1–12. ISCA ’17. New York, NY, USA: ACM. https://doi.org/10.1145/3079856.3080246.\nLe Edge TPU sono una versione più piccola e a basso consumo delle TPU di Google, studiate appositamente per i dispositivi edge. Forniscono un’inferenza ML veloce sul dispositivo per i modelli TensorFlow Lite. Le Edge TPU consentono un’inferenza a bassa latenza e ad alta efficienza su dispositivi edge come smartphone, dispositivi IoT e sistemi embedded. Le capacità di IA possono essere implementate in applicazioni in tempo reale senza comunicare con un server centrale, risparmiando così larghezza di banda e riducendo la latenza. Si consideri la tabella in Figura 8.5. Mostra le differenze di prestazioni tra l’esecuzione di modelli diversi su CPU rispetto a un acceleratore Coral USB. L’acceleratore Coral USB è un accessorio della piattaforma Coral AI di Google che consente agli sviluppatori di collegare le Edge TPU ai computer Linux. L’esecuzione dell’inferenza sulle Edge TPU è stata da 70 a 100 volte più veloce rispetto alle CPU.\n\n\n\n\n\n\nFigura 8.5: Confronto delle prestazioni tra acceleratore e CPU in diverse configurazioni hardware. Desktop CPU: 64-bit Intel(R) Xeon(R) E5-1650 v4 @ 3.60GHz. Embedded CPU: Quad-core Cortex-A53 @ 1.5GHz, †Dev Board: Quad-core Cortex-A53 @ 1.5GHz + Edge TPU. Fonte: TensorFlow Blog.\n\n\n\nAcceleratori NN (Neural Network): Gli acceleratori di reti neurali a funzione fissa sono acceleratori hardware progettati esplicitamente per i calcoli di reti neurali. Possono essere chip standalone o far parte di una soluzione di system-on-chip (SoC) più ampia. Ottimizzando l’hardware per le operazioni specifiche richieste dalle reti neurali, come moltiplicazioni di matrici e convoluzioni, gli acceleratori NN possono ottenere tempi di inferenza più rapidi e consumi energetici inferiori rispetto alle CPU e alle GPU per uso generico. Sono particolarmente utili nei dispositivi TinyML con vincoli di potenza o termici, come smartwatch, micro-droni o robotica.\nMa questi sono solo gli esempi più comuni. Stanno emergendo diversi altri tipi di hardware che hanno il potenziale per offrire vantaggi significativi per l’inferenza. Questi includono, ma non solo, hardware neuromorfico, elaborazione fotonica, ecc. In Sezione 10.3, esploreremo questi aspetti in modo più dettagliato.\nUn hardware efficiente per l’inferenza velocizza il processo, risparmia energia, prolunga la durata della batteria e può funzionare in condizioni di tempo reale. Man mano che l’intelligenza artificiale viene integrata in innumerevoli applicazioni, dalle telecamere intelligenti agli assistenti vocali, il ruolo dell’hardware ottimizzato diventerà sempre più importante. Sfruttando questi componenti hardware specializzati, sviluppatori e ingegneri possono portare la potenza dell’intelligenza artificiale a dispositivi e situazioni che prima erano impensabili.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>IA Efficiente</span>"
    ]
  },
  {
    "objectID": "contents/core/efficient_ai/efficient_ai.it.html#sec-efficient-numerics",
    "href": "contents/core/efficient_ai/efficient_ai.it.html#sec-efficient-numerics",
    "title": "8  IA Efficiente",
    "section": "8.6 Matematica Efficiente",
    "text": "8.6 Matematica Efficiente\nL’apprendimento automatico, e in particolare il deep learning, comporta enormi quantità di elaborazione. I modelli possono avere milioni o miliardi di parametri, spesso addestrati su vasti set di dati. Ogni operazione, ogni moltiplicazione o addizione, richiede risorse di elaborazione. Pertanto, la precisione dei numeri utilizzati in queste operazioni può avere un impatto significativo sulla velocità di elaborazione, sul consumo di energia e sui requisiti di memoria. È qui che entra in gioco il concetto di numeri efficienti.\n\n8.6.1 Formati Numerici\nEsistono molti tipi diversi di numeri. I numeri hanno una lunga storia nei sistemi di elaborazione.\nFloating point: Noto come “virgola mobile” a precisione singola, FP32 utilizza 32 bit per rappresentare un numero, incorporandone segno, esponente e mantissa. Comprendere come i numeri in virgola mobile sono rappresentati in modo approfondito è fondamentale per comprendere le varie ottimizzazioni possibili nei calcoli numerici. Il bit del segno determina se il numero è positivo o negativo, l’esponente controlla l’intervallo di valori che possono essere rappresentati e la mantissa determina la precisione del numero. La combinazione di questi componenti consente ai numeri in virgola mobile di rappresentare un’ampia gamma di valori con vari gradi di precisione.\nVideo 8.1 fornisce una panoramica completa di questi tre componenti principali, segno, esponente e mantissa, e di come funzionano insieme per rappresentare i numeri in virgola mobile.\n\n\n\n\n\n\nVideo 8.1: Numeri in Virgola Mobile\n\n\n\n\n\n\nFP32 è ampiamente adottato in molti framework di deep learning e bilancia accuratezza e requisiti computazionali. È prevalente nella fase di training per molte reti neurali grazie alla sua sufficiente precisione nel catturare dettagli minuti durante gli aggiornamenti dei pesi. Noto anche come virgola mobile a mezza precisione, FP16 utilizza 16 bit per rappresentare un numero, inclusi il segno, l’esponente e la frazione. Offre un buon equilibrio tra precisione e risparmio di memoria. FP16 è particolarmente popolare nella training di deep learning su GPU che supportano l’aritmetica a precisione mista, combinando i vantaggi di velocità di FP16 con la precisione di FP32 quando necessario.\nFigura 8.6 mostra tre diversi formati in virgola mobile: Float32, Float16 e BFloat16.\n\n\n\n\n\n\nFigura 8.6: Tre formati a virgola mobile.\n\n\n\nDiversi altri formati numerici rientrano in una classe esotica. Un esempio esotico è BF16 o Brain Floating Point. È un formato numerico a 16 bit progettato esplicitamente per applicazioni di deep learning. È un compromesso tra FP32 e FP16, che mantiene l’esponente a 8 bit di FP32 riducendo la mantissa a 7 bit (rispetto alla mantissa a 23 bit di FP32). Questa struttura dà priorità al range rispetto alla precisione. BF16 ha ottenuto risultati di training paragonabili in accuratezza a FP32, utilizzando significativamente meno memoria e risorse computazionali (Kalamkar et al. 2019). Ciò lo rende adatto non solo per l’inferenza, ma anche per il training di reti neurali profonde.\n\nKalamkar, Dhiraj, Dheevatsa Mudigere, Naveen Mellempudi, Dipankar Das, Kunal Banerjee, Sasikanth Avancha, Dharma Teja Vooturi, et al. 2019. «A Study of BFLOAT16 for Deep Learning Training». https://arxiv.org/abs/1905.12322.\nMantenendo l’esponente a 8 bit di FP32, BF16 offre un range simile, che è fondamentale per le attività di deep learning in cui determinate operazioni possono generare numeri molto grandi o molto piccoli. Allo stesso tempo, troncando la precisione, BF16 consente requisiti di memoria e computazionali ridotti rispetto a FP32. BF16 è emerso come una promettente via di mezzo nel panorama dei formati numerici per il deep learning, fornendo un’alternativa efficiente ed efficace ai formati FP32 e FP16 più tradizionali.\nIntero: Si tratta di rappresentazioni di numeri interi che utilizzano 8, 4 e 2 bit. Vengono spesso utilizzati durante la fase di inferenza delle reti neurali, in cui i pesi e le attivazioni del modello sono quantizzati a queste precisioni inferiori. Le rappresentazioni intere sono deterministiche e offrono notevoli vantaggi in termini di velocità e memoria rispetto alle rappresentazioni in virgola mobile. Per molte attività di inferenza, in particolare sui dispositivi edge, la leggera perdita di accuratezza dovuta alla quantizzazione è spesso accettabile, dati i guadagni di efficienza. Una forma estrema di numeri interi è per le reti neurali binarie (BNN), in cui pesi e attivazioni sono vincolati a uno dei due valori: +1 o -1.\nLarghezze di bit variabili: Oltre alle larghezze standard, sono in corso ricerche su numeri con larghezze di bit estremamente basse, persino fino a rappresentazioni binarie o ternarie. Le operazioni con larghezze di bit estremamente basse possono offrire accelerazioni significative e ridurre ulteriormente il consumo di energia. Sebbene permangano dei problemi nel mantenere l’accuratezza del modello con una quantizzazione così drastica, si continuano a fare progressi in quest’area.\nL’efficienza numerica non riguarda solo la riduzione della larghezza di bit dei numeri, ma anche la comprensione dei compromessi tra accuratezza ed efficienza. Man mano che i modelli di apprendimento automatico diventano più pervasivi, soprattutto in ambienti reali con risorse limitate, l’attenzione su una numerica efficiente continuerà a crescere. Selezionando e sfruttando attentamente la precisione numerica appropriata, è possibile ottenere prestazioni di modello robuste ottimizzando al contempo velocità, memoria ed energia. Tabella 8.1 riassume questi compromessi.\n\n\n\nTabella 8.1: Confronto dei livelli di precisione nel deep learning.\n\n\n\n\n\n\n\n\n\n\nPrecisione\nPro\nContro\n\n\n\n\nFP32 (virgola mobile a 32 bit)\n\nPrecisione standard utilizzata nella maggior parte dei framework di deep learning.\nElevata accuratezza grazie all’ampia capacità di rappresentazione.\nAdatto per il training\n\n\nElevato utilizzo di memoria.\nTempi di inferenza più lenti rispetto ai modelli quantizzati.\nMaggiore consumo energetico.\n\n\n\nFP16 (virgola mobile a 16 bit)\n\nRiduce l’utilizzo di memoria rispetto a FP32.\nVelocizza i calcoli su hardware che supporta FP16.\nSpesso utilizzato nel training a precisione mista per bilanciare velocità e accuratezza.\n\n\nMinore capacità di rappresentazione rispetto a FP32.\nRischio di instabilità numerica in alcuni modelli o livelli.\n\n\n\nINT8 (intero a 8 bit)\n\nImpronta di memoria notevolmente ridotta rispetto alle rappresentazioni in virgola mobile.\nInferenza più rapida se l’hardware supporta i calcoli INT8.\nAdatto a molti scenari di quantizzazione post-training.\n\n\nLa quantizzazione può comportare una certa perdita di accuratezza.\nRichiede una calibrazione attenta durante la quantizzazione per ridurre al minimo il degrado della precisione.\n\n\n\nINT4 (intero a 4 bit)\n\nUtilizzo di memoria ancora inferiore rispetto a INT8.\nUlteriore potenziale di accelerazione per l’inferenza.\n\n\nRischio di perdita di precisione più elevato rispetto a INT8.\nLa calibrazione durante la quantizzazione diventa più critica.\n\n\n\nBinario\n\nIngombro di memoria minimo (solo 1 bit per parametro).\nInferenza estremamente rapida grazie alle operazioni bit a bit.\nEfficienza energetica.\n\n\nCalo significativo della precisione per molte attività.\nDinamiche di training complesse grazie alla quantizzazione estrema.\n\n\n\nTernario\n\nBasso utilizzo di memoria ma leggermente superiore a quello binario.\nOffre una via di mezzo tra rappresentazione ed efficienza.\n\n\nL’accuratezza potrebbe essere ancora inferiore a quella dei modelli di precisione più elevata.\nLe dinamiche di addestramento possono essere complesse.\n\n\n\n\n\n\n\n\n\n8.6.2 Vantaggi dell’Efficienza\nL’efficienza numerica è importante per i carichi di lavoro di machine learning per diversi motivi:\nEfficienza Computazionale: I calcoli ad alta precisione (come FP32 o FP64) possono essere lenti e richiedere molte risorse. Ridurre la precisione numerica può ottenere tempi di calcolo più rapidi, specialmente su hardware specializzato che supporta una precisione inferiore.\nEfficienza della Memoria: I requisiti di archiviazione diminuiscono con una precisione numerica ridotta. Ad esempio, FP16 richiede metà della memoria di FP32. Ciò è fondamentale quando si distribuiscono modelli su dispositivi edge con memoria limitata o si lavora con modelli di grandi dimensioni.\nEfficienza Energetica: I calcoli a precisione inferiore spesso consumano meno energia, il che è particolarmente importante per i dispositivi alimentati a batteria.\nIntroduzione del Rumore: È interessante notare che il rumore introdotto utilizzando una precisione inferiore può talvolta fungere da regolarizzatore, contribuendo a prevenire l’overfitting in alcuni modelli.\nAccelerazione Hardware: Molti acceleratori di IA e GPU moderni sono ottimizzati per operazioni di precisione inferiore, sfruttando i vantaggi dell’efficienza di tali numeri.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>IA Efficiente</span>"
    ]
  },
  {
    "objectID": "contents/core/efficient_ai/efficient_ai.it.html#valutazione-dei-modelli",
    "href": "contents/core/efficient_ai/efficient_ai.it.html#valutazione-dei-modelli",
    "title": "8  IA Efficiente",
    "section": "8.7 Valutazione dei Modelli",
    "text": "8.7 Valutazione dei Modelli\nVale la pena notare che i vantaggi e i compromessi effettivi possono variare in base all’architettura specifica della rete neurale, al set di dati, all’attività e all’hardware utilizzato. Prima di decidere una precisione numerica, è consigliabile eseguire esperimenti per valutare l’impatto sull’applicazione desiderata.\n\n8.7.1 Metriche di Efficienza\nUna profonda comprensione dei metodi di valutazione dei modelli è importante per guidare questo processo in modo sistematico. Quando si valuta l’efficacia e l’idoneità dei modelli di intelligenza artificiale per varie applicazioni, le metriche di efficienza vengono in primo piano.\nI FLOP (Floating Point Operations), introdotti in Training, misurano le esigenze computazionali di un modello. Ad esempio, una moderna rete neurale come BERT ha miliardi di FLOP, che potrebbero essere gestibili su un potente server cloud ma sarebbero gravosi su uno smartphone. FLOP più elevati possono portare a tempi di inferenza più prolungati e a un notevole consumo di energia, soprattutto su dispositivi senza acceleratori hardware specializzati. Quindi, per applicazioni in tempo reale come lo streaming video o i giochi, potrebbero essere più desiderabili modelli con FLOP più bassi.\nL’Utilizzo della Memoria riguarda la quantità di spazio di archiviazione richiesta dal modello, che influisce sia sullo spazio di archiviazione del dispositivo che sulla RAM. Si prenda in considerazione l’implementazione di un modello su uno smartphone: un modello che occupa diversi gigabyte di spazio non solo consuma prezioso spazio di archiviazione, ma potrebbe anche essere più lento a causa della necessità di caricare grandi pesi nella memoria. Ciò diventa particolarmente cruciale per dispositivi edge come telecamere di sicurezza o droni, dove impronte di memoria minime sono vitali per l’archiviazione e l’elaborazione rapida dei dati.\nIl Consumo Energetico diventa particolarmente cruciale per i dispositivi che si basano sulle batterie. Ad esempio, un monitor sanitario indossabile che utilizza un modello ad alto consumo energetico potrebbe esaurire la batteria in poche ore, rendendolo poco pratico per il monitoraggio continuo. L’ottimizzazione dei modelli per un basso consumo energetico diventa essenziale mentre ci muoviamo verso un’era dominata dai dispositivi IoT, dove molti dispositivi funzionano a batteria.\nIl Tempo di Inferenza riguarda la rapidità con cui un modello può produrre risultati. In applicazioni come la guida autonoma, dove decisioni in frazioni di secondo fanno la differenza tra sicurezza e calamità, i modelli devono funzionare rapidamente. Se il modello di un’auto a guida autonoma impiega anche solo pochi secondi in più per riconoscere un ostacolo, le conseguenze potrebbero essere disastrose. Quindi, garantire che il tempo di inferenza di un modello sia allineato con le richieste in tempo reale della sua applicazione è fondamentale.\nIn sostanza, queste metriche di efficienza sono più che dei numeri che stabiliscono dove e come un modello può essere distribuito in modo efficace. Un modello potrebbe vantare un’elevata accuratezza, ma se i suoi FLOP, l’utilizzo della memoria, il consumo energetico o il tempo di inferenza lo rendono inadatto alla piattaforma prevista o agli scenari del mondo reale, la sua utilità pratica diventa limitata.\n\n\n8.7.2 Confronti di Efficienza\nIl panorama dei modelli di machine learning è vasto, con ogni modello che offre un set unico di punti di forza e considerazioni di implementazione. Sebbene le cifre di accuratezza grezza o le velocità di training e inferenza possano essere parametri di riferimento allettanti, forniscono un quadro incompleto. Un’analisi comparativa più approfondita rivela diversi fattori critici che influenzano l’idoneità di un modello per le applicazioni TinyML. Spesso, incontriamo il delicato equilibrio tra accuratezza ed efficienza. Ad esempio, mentre un modello di deep learning e denso e una variante MobileNet leggera potrebbero eccellere nella classificazione delle immagini, le loro richieste di calcolo potrebbero essere ad estremi opposti. Questa differenziazione è particolarmente pronunciata quando si confrontano le distribuzioni su server cloud con risorse abbondanti rispetto ai limitati dispositivi TinyML. In molti scenari del mondo reale, i guadagni marginali in termini di accuratezza potrebbero essere oscurati dalle inefficienze di un modello ad alta intensità di risorse richieste.\nInoltre, la scelta del modello ottimale non è sempre universale, ma spesso dipende dalle specifiche di un’applicazione. Ad esempio, un modello che eccelle in scenari di rilevamento di oggetti generali potrebbe avere difficoltà in ambienti di nicchia, come il rilevamento di difetti di fabbricazione in una fabbrica. Questa adattabilità, o la sua mancanza, può influenzare l’utilità reale di un modello.\nUn’altra considerazione importante è la relazione tra la complessità del modello e i suoi vantaggi pratici. Prendiamo gli assistenti attivati tramite comando vocale, come “Alexa” o “OK Google”. Mentre un modello complesso potrebbe dimostrare una comprensione marginalmente superiore del parlato dell’utente se è più lento a rispondere rispetto a una controparte più semplice, l’esperienza utente potrebbe essere compromessa. Pertanto, l’aggiunta di layer o parametri solo a volte equivale a risultati migliori nel mondo reale.\nUn’altra considerazione importante è la relazione tra la complessità del modello e i suoi vantaggi pratici. Prendiamo gli assistenti vocali come “Alexa” o “OK Google”. Mentre un modello complesso potrebbe dimostrare una comprensione leggermente superiore del parlato dell’utente se è più lento a rispondere rispetto a una controparte più semplice, l’esperienza utente potrebbe essere compromessa. Pertanto, l’aggiunta di layer o parametri solo a volte equivale a risultati migliori nel mondo reale.\nInoltre, mentre i set di dati di riferimento, come ImageNet (Russakovsky et al. 2015), COCO (Lin et al. 2014), Visual Wake Words (Wang e Zhan 2019), Google Speech Commands (Warden 2018), ecc. forniscono una metrica di prestazioni standardizzata, potrebbero non catturare la diversità e l’imprevedibilità dei dati del mondo reale. Due modelli di riconoscimento facciale con punteggi di riferimento simili potrebbero mostrare competenze diverse quando si trovano di fronte a background etnici diversi o condizioni di illuminazione difficili. Tali disparità sottolineano l’importanza di robustezza e coerenza tra dati diversi. Ad esempio, Figura 8.7 dal set di dati Dollar Street mostra immagini di stufe su redditi mensili estremi. Le stufe hanno forme e livelli tecnologici diversi in diverse regioni e livelli di reddito. Un modello che non è addestrato su set di dati diversi potrebbe funzionare bene su un benchmark ma fallire nelle applicazioni del mondo reale. Quindi, se un modello fosse addestrato solo su immagini di stufe trovate nei paesi ricchi, non riuscirebbe a riconoscere le stufe delle regioni più povere.\n\nRussakovsky, Olga, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, et al. 2015. «ImageNet Large Scale Visual Recognition Challenge». Int. J. Comput. Vision 115 (3): 211–52. https://doi.org/10.1007/s11263-015-0816-y.\n\nLin, Tsung-Yi, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, e C Lawrence Zitnick. 2014. «Microsoft coco: Common objects in context». In Computer VisionECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, 740–55. Springer.\n\nWang, LingFeng, e YaQing Zhan. 2019. «A conceptual peer review model for arXiv and other preprint databases». Learn. Publ. 32 (3): 213–19. https://doi.org/10.1002/leap.1229.\n\nWarden, Pete. 2018. «Speech commands: A dataset for limited-vocabulary speech recognition». arXiv preprint arXiv:1804.03209.\n\n\n\n\n\n\nFigura 8.7: Diversi tipi di stufe. Fonte: Immagini di stufe di Dollar Street.\n\n\n\nIn sostanza, un’analisi comparativa approfondita trascende le metriche numeriche. È una valutazione olistica intrecciata con applicazioni del mondo reale, costi e le intricate sottigliezze che ogni modello porta con sé. Ecco perché avere parametri di riferimento e metriche standard ampiamente stabiliti e adottati dalla comunità diventa importante.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>IA Efficiente</span>"
    ]
  },
  {
    "objectID": "contents/core/efficient_ai/efficient_ai.it.html#conclusione",
    "href": "contents/core/efficient_ai/efficient_ai.it.html#conclusione",
    "title": "8  IA Efficiente",
    "section": "8.8 Conclusione",
    "text": "8.8 Conclusione\nL’intelligenza artificiale efficiente è fondamentale mentre ci spingiamo verso un’implementazione più ampia e diversificata del machine learning nel mondo reale. Questo capitolo ha fornito una panoramica, esplorando le varie metodologie e considerazioni alla base del raggiungimento di un’intelligenza artificiale efficiente, a partire dall’esigenza fondamentale, dalle somiglianze e dalle differenze tra i sistemi cloud, Edge e TinyML.\nAbbiamo esaminato le architetture dei modelli efficienti e la loro utilità per l’ottimizzazione. Le tecniche di compressione dei modelli come pruning, quantizzazione e distillazione della conoscenza esistono per aiutare a ridurre le richieste di calcolo e l’ingombro della memoria senza influire in modo significativo sulla precisione. Hardware specializzati come TPU e acceleratori NN offrono chip ottimizzati per le operazioni di rete neurale e il flusso di dati. I numeri efficienti bilanciano precisione ed efficienza, consentendo ai modelli di ottenere prestazioni robuste utilizzando risorse minime. Esploreremo questi argomenti in modo approfondito e dettagliato nei capitoli successivi.\nInsieme, questi formano un quadro olistico per un’intelligenza artificiale efficiente. Ma il viaggio non finisce qui. Il raggiungimento di un’intelligenza efficiente in modo ottimale richiede ricerca e innovazione continue. Man mano che i modelli diventano più sofisticati, i set di dati crescono e le applicazioni si diversificano in domini specializzati, l’efficienza deve evolversi di pari passo. La misura dell’impatto nel mondo reale richiede parametri di riferimento adatti e metriche standardizzate che vadano oltre le semplicistiche cifre dell’accuratezza.\nInoltre, l’intelligenza artificiale efficiente si espande oltre l’ottimizzazione tecnologica e comprende costi, impatto ambientale e considerazioni etiche per il bene della società in senso più ampio. Man mano che l’intelligenza artificiale permea i settori e la vita quotidiana, una prospettiva completa sull’efficienza sostiene il suo progresso sostenibile e responsabile. I capitoli successivi si baseranno su questi concetti fondamentali, fornendo approfondimenti concreti e norme pratiche per lo sviluppo e l’implementazione di soluzioni di intelligenza artificiale efficienti.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>IA Efficiente</span>"
    ]
  },
  {
    "objectID": "contents/core/efficient_ai/efficient_ai.it.html#sec-efficient-ai-resource",
    "href": "contents/core/efficient_ai/efficient_ai.it.html#sec-efficient-ai-resource",
    "title": "8  IA Efficiente",
    "section": "8.9 Risorse",
    "text": "8.9 Risorse\nEcco un elenco curato di risorse per supportare studenti e insegnanti nei loro percorsi di apprendimento e insegnamento. Lavoriamo continuamente per espandere questa raccolta e presto aggiungeremo nuovi esercizi.\n\n\n\n\n\n\nSlide\n\n\n\n\n\nQueste slide sono uno strumento prezioso per gli insegnanti per tenere lezioni e per gli studenti per rivedere il materiale secondo il proprio ritmo. Incoraggiamo studenti e docenti a sfruttare queste slide per migliorare la loro comprensione e facilitare un trasferimento efficace delle conoscenze.\n\nDeploying on Edge Devices: challenges and techniques.\nModel Evaluation.\nContinuous Evaluation Challenges for TinyML.\n\n\n\n\n\n\n\n\n\n\nVideo\n\n\n\n\n\n\nProssimamente.\n\n\n\n\n\n\n\n\n\n\nEsercizi\n\n\n\n\n\nPer rafforzare i concetti trattati in questo capitolo, abbiamo curato una serie di esercizi che sfidano gli studenti ad applicare le proprie conoscenze e ad approfondire la propria comprensione.\n\nProssimamente.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>IA Efficiente</span>"
    ]
  },
  {
    "objectID": "contents/core/optimizations/optimizations.it.html",
    "href": "contents/core/optimizations/optimizations.it.html",
    "title": "9  Ottimizzazioni dei Modelli",
    "section": "",
    "text": "9.1 Panoramica\nL’ottimizzazione dei modelli di apprendimento automatico per l’implementazione pratica è un aspetto critico dei sistemi di IA. Questo capitolo si concentra sull’esplorazione delle tecniche di ottimizzazione dei modelli in relazione allo sviluppo di sistemi di apprendimento automatico, che vanno dalle considerazioni di architettura del modello di alto livello agli adattamenti hardware di basso livello. Figura 9.1 Illustra i tre livelli dello stack di ottimizzazione che trattiamo.\nAl livello più alto, esaminiamo le metodologie per ridurre la complessità dei parametri del modello senza compromettere le capacità inferenziali. Tecniche come la potatura e la distillazione della conoscenza offrono approcci potenti per comprimere e perfezionare i modelli mantenendo o addirittura migliorandone le prestazioni, non solo in termini di qualità del modello ma anche nelle prestazioni effettive del runtime del sistema. Questi metodi sono fondamentali per creare modelli efficienti che possono essere implementati in ambienti con risorse limitate.\nInoltre, esploriamo il ruolo della precisione numerica nei calcoli del modello. Comprendere come diversi livelli di precisione numerica influenzino le dimensioni, la velocità e l’accuratezza del modello è essenziale per ottimizzare le prestazioni. Analizziamo vari formati numerici e l’applicazione dell’aritmetica a precisione ridotta, particolarmente rilevante per le distribuzioni di sistemi embedded in cui le risorse computazionali sono spesso limitate.\nAl livello più basso, esploriamo l’intricato panorama della progettazione congiunta hardware-software. Questa esplorazione rivela come i modelli possono essere personalizzati per sfruttare le caratteristiche e le capacità specifiche delle piattaforme hardware di destinazione. Allineando la progettazione del modello all’architettura hardware, possiamo migliorare significativamente le prestazioni e l’efficienza.\nQuesto approccio collettivo si concentra sull’aiutarci a sviluppare e distribuire modelli di apprendimento automatico efficienti, potenti e consapevoli dell’hardware. Dalla semplificazione delle architetture del modello alla messa a punto della precisione numerica e all’adattamento a hardware specifico, questo capitolo copre l’intero spettro di strategie di ottimizzazione. Alla conclusione di questo capitolo, i lettori avranno acquisito una conoscenza approfondita di varie tecniche di ottimizzazione e delle loro applicazioni pratiche in scenari del mondo reale. Questa conoscenza è importante per creare modelli di apprendimento automatico che non solo funzionino bene, ma siano anche ottimizzati per i vincoli e le opportunità offerti dagli ambienti informatici moderni.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Ottimizzazioni dei Modelli</span>"
    ]
  },
  {
    "objectID": "contents/core/optimizations/optimizations.it.html#panoramica",
    "href": "contents/core/optimizations/optimizations.it.html#panoramica",
    "title": "9  Ottimizzazioni dei Modelli",
    "section": "",
    "text": "Figura 9.1: Tre livelli da coprire.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Ottimizzazioni dei Modelli</span>"
    ]
  },
  {
    "objectID": "contents/core/optimizations/optimizations.it.html#sec-model_ops_representation",
    "href": "contents/core/optimizations/optimizations.it.html#sec-model_ops_representation",
    "title": "9  Ottimizzazioni dei Modelli",
    "section": "9.2 Rappresentazione Efficiente del Modello",
    "text": "9.2 Rappresentazione Efficiente del Modello\nIl primo passo per l’ottimizzazione del modello inizia in un territorio familiare per la maggior parte dei professionisti del ML: la rappresentazione efficiente del modello viene spesso affrontata per la prima volta al livello più alto di astrazione della parametrizzazione, ovvero l’architettura stessa del modello.\nLa maggior parte dei professionisti del ML tradizionali progetta modelli con un obiettivo generale di alto livello in mente, che si tratti di classificazione delle immagini, rilevamento di persone o individuazione di parole chiave come menzionato in precedenza in questo testo. I loro progetti in genere finiscono per adattarsi naturalmente ad alcuni vincoli soft dovuti a risorse di elaborazione limitate durante lo sviluppo, ma in genere questi progetti non sono a conoscenza di vincoli successivi, come quelli richiesti se il modello deve essere distribuito su un dispositivo più limitato anziché sul cloud.\nIn questa sezione, discuteremo di come i professionisti possono sfruttare i principi della progettazione congiunta hardware-software anche nell’architettura di alto livello di un modello per rendere i loro modelli compatibili con i dispositivi edge. Da quelli più consapevoli dell’hardware a quelli meno consapevoli a questo livello di modifica, discutiamo alcune delle strategie più comuni per una parametrizzazione efficiente del modello: pruning, compressione e architetture edge-friendly. Abbiamo già parlato di pruning e compressione del modello in Sezione 8.4; questa sezione andrà oltre le definizioni per fornire una comprensione tecnica del loro funzionamento.\n\n9.2.1 Il Pruning\n\nPanoramica\nIl model pruning [potatura] è una tecnica di apprendimento automatico che riduce le dimensioni e la complessità di un modello di rete neurale, mantenendone il più possibile le capacità predittive. L’obiettivo della potatura è quello di rimuovere componenti ridondanti o non essenziali del modello, tra cui connessioni tra neuroni, singoli neuroni o persino interi layer della rete.\nQuesto processo in genere comporta l’analisi del modello di machine learning per identificare e rimuovere pesi, nodi o layer che hanno scarso impatto sugli output del modello. Potando selettivamente un modello in questo modo, il numero totale di parametri può essere ridotto in modo significativo senza cali sostanziali nell’accuratezza del modello. Il modello compresso risultante richiede meno memoria e risorse di calcolo per l’addestramento e l’esecuzione, consentendo tempi di inferenza più rapidi.\nIl pruning del modello è particolarmente utile quando si distribuiscono modelli di apprendimento automatico su dispositivi con risorse di calcolo limitate, come telefoni cellulari o sistemi TinyML. La tecnica facilita la distribuzione di modelli più grandi e complessi su questi dispositivi riducendo le loro richieste di risorse. Inoltre, i modelli più piccoli richiedono meno dati per generalizzare bene e sono meno inclini all’overfitting [sovradattamento]. Fornendo un modo efficiente per semplificare i modelli, la potatura dei modelli è diventata una tecnica fondamentale per ottimizzare le reti neurali nell’apprendimento automatico.\nEsistono diverse tecniche di potatura comuni utilizzate nell’apprendimento automatico, tra cui la potatura strutturata, la potatura non strutturata, la potatura iterativa, la potatura bayesiana e persino la potatura casuale. Oltre a potare i pesi, si possono anche potare le attivazioni. La potatura di attivazioni prende di mira specificamente neuroni o filtri che si attivano raramente o hanno un’attivazione complessivamente bassa. Esistono numerosi altri metodi, come la potatura di sensibilità e movimento. Per un elenco completo dei metodi, si consiglia al lettore di leggere il seguente articolo: “A Survey on Deep Neural Network Pruning: Taxonomy, Comparison, Analysis, and Recommendations” (2023).\nQuindi, come si scelgono i metodi di potatura? Esistono molte varianti di tecniche di potatura, ciascuna delle quali varia l’euristica di ciò che dovrebbe essere mantenuto e potato dal modello, nonché il numero di volte in cui cui deve essere eseguita. Tradizionalmente, la potatura avviene dopo che il modello è completamente addestrato, dove il modello potato può subire una lieve perdita di accuratezza. Tuttavia, come discuteremo più avanti, recenti scoperte hanno trovato che la potatura può essere utilizzata durante l’addestramento (ad esempio, in modo iterativo) per identificare rappresentazioni del modello più efficienti e accurate.\n\n\nPotatura Strutturata\nIniziamo con la “potatura strutturata”, una tecnica che riduce le dimensioni di una rete neurale eliminando intere sotto-strutture specifiche del modello mantenendone la struttura generale. Rimuove interi neuroni/canali o layer in base a criteri di importanza. Ad esempio, per una rete neurale convoluzionale (CNN), potrebbero essere determinate istanze di filtro o canali. Per reti completamente connesse, potrebbero essere i neuroni stessi mantenendo la piena connettività o persino l’eliminazione di interi layer del modello che sono considerati insignificanti. Questo tipo di potatura spesso porta a reti sparse regolari e strutturate che sono compatibili con l’hardware.\nSono iniziate a emergere le “best practice” su come pensare alla potatura strutturata. Ci sono tre componenti principali:\n\n1. Strutture Candidate per il Pruning\nData la varietà di approcci, diverse strutture all’interno di una rete neurale vengono potate in base a criteri specifici. Le strutture primarie per la potatura includono neuroni, canali e talvolta interi layer, ognuno con le sue implicazioni e metodologie uniche. L’obiettivo di ogni approccio è garantire che il modello ridotto mantenga il più possibile la capacità predittiva del modello originale, migliorando al contempo l’efficienza computazionale e riducendo le dimensioni.\nQuando i neuroni vengono potati, rimuoviamo interi neuroni insieme ai loro pesi e bias associati, riducendo così la larghezza del layer. Questo tipo di potatura viene spesso utilizzato in layer completamente connessi.\nLa potatura del canale, che viene applicata prevalentemente nelle reti neurali convoluzionali (CNN), comporta l’eliminazione di interi canali o filtri, il che a sua volta riduce la profondità delle mappe delle feature e influisce sulla capacità della rete di estrarre determinate feature dai dati di input. Ciò è particolarmente cruciale nelle attività di elaborazione delle immagini in cui l’efficienza computazionale è fondamentale.\nInfine, la potatura dei layer adotta un approccio più aggressivo rimuovendo interi layer della rete. Ciò riduce significativamente la profondità della rete e quindi la sua capacità di plasmare pattern e gerarchie complesse nei dati. Questo approccio richiede un attento equilibrio per garantire che la capacità predittiva del modello non venga indebitamente compromessa.\nFigura 9.2 mostra la differenza tra la potatura di canale/filtro e quella del layer. Quando potiamo un canale, dobbiamo riconfigurare l’architettura del modello per adattarla ai cambiamenti strutturali. Una modifica consiste nel cambiare il numero di canali di input nel layer successivo (qui, il terzo e il layer più profondo): modificando le profondità dei filtri applicati al layer con il canale potato. D’altra parte, la potatura di un intero layer (rimuovendo tutti i canali nel layer) richiede modifiche più drastiche. Quella principale riguarda la modifica delle connessioni tra i layer rimanenti per sostituire o bypassare il layer potato. Nel nostro caso, riconfiguriamo per connettere il primo e l’ultimo layer. In tutti i casi di potatura, dobbiamo mettere a punto la nuova struttura per regolare i pesi.\n\n\n\n\n\n\nFigura 9.2: Potatura del canale e quella del layer.\n\n\n\n\n\n2. Stabilire un criterio per il Pruning\nStabilire criteri ben definiti per determinare quali strutture specifiche potare da un modello di rete neurale è una componente cruciale del processo di “pruning” del modello. L’obiettivo principale qui è identificare e rimuovere i componenti che contribuiscono meno alle capacità predittive del modello, mantenendo al contempo le strutture integrali per preservare l’accuratezza.\nUna strategia ampiamente adottata ed efficace per potare sistematicamente le strutture si basa sul calcolo di punteggi di importanza per singoli componenti come neuroni, filtri, canali o layer. Questi punteggi servono come metriche quantitative per valutare la significatività di ciascuna struttura e il suo effetto sull’output del modello.\nEsistono diverse tecniche per assegnare questi punteggi sull’importanza:\n\nPruning Basato sulla Magnitudo del peso: Questo approccio assegna punteggi di importanza a una struttura valutando la magnitudo aggregata dei pesi associati. Le strutture con magnitudo del peso complessivo inferiore sono considerate meno critiche per le prestazioni della rete.\nPruning Basato sul Bradiente: Questa tecnica utilizza i gradienti della funzione di los [perdita] rispetto ai pesi associati a una struttura. Le strutture con magnitudo del gradiente cumulativo basso, che indica un impatto minimo sulla perdita quando alterato, sono le candidate principali per la potatura.\nPruning Basato sull’Attivazione: Questo metodo tiene traccia della frequenza con cui un neurone o un filtro viene attivato memorizzando queste informazioni in un parametro chiamato contatore delle attivazioni. Ogni volta che la struttura viene attivata, il contatore viene incrementato. Un conteggio di attivazione basso suggerisce che la struttura è meno rilevante.\nPruning Basato sull’Espansione di Taylor: Questo approccio approssima la modifica nella funzione di perdita derivante dalla rimozione di un dato peso. Valutando la perturbazione della perdita cumulativa derivante dalla rimozione di tutti i pesi associati a una struttura, è possibile identificare le strutture con un impatto trascurabile sulla perdita, rendendole candidate idonee per la potatura.\n\nL’idea è di misurare, direttamente o indirettamente, il contributo di ogni componente all’output del modello. Le strutture con un’influenza minima in base ai criteri definiti vengono potate per prime. Ciò consente una potatura selettiva e ottimizzata che comprime al massimo i modelli preservando al contempo la capacità predittiva. In generale, è importante valutare l’impatto della rimozione di particolari strutture sull’output del modello, con lavori recenti come (Rachwan et al. 2022) e (Lubana e Dick 2020) che studiano combinazioni di tecniche come la potatura basata sulla magnitudine e la potatura basata sul gradiente.\n\nRachwan, John, Daniel Zügner, Bertrand Charpentier, Simon Geisler, Morgane Ayle, e Stephan Günnemann. 2022. «Winning the lottery ahead of time: Efficient early network pruning». In International Conference on Machine Learning, 18293–309. PMLR.\n\nLubana, Ekdeep Singh, e Robert P Dick. 2020. «A gradient flow framework for analyzing network pruning». arXiv preprint arXiv:2009.11839.\n\n\n3. Selezione di una Strategia di Potatura\nOra che abbiamo capito alcune tecniche per determinare l’importanza delle strutture all’interno di una rete neurale, il passo successivo è decidere come applicare queste intuizioni. Ciò comporta la selezione di una strategia di potatura appropriata, che stabilisce come e quando le strutture identificate vengono rimosse e come il modello viene messo a punto per mantenere le sue prestazioni. Esistono due principali strategie di potatura strutturata: quella iterativa e la one-shot.\nLa potatura iterativa rimuove gradualmente le strutture attraverso più cicli di potatura seguiti da messa a punto. In ogni ciclo, un piccolo set di strutture viene potato in base a criteri di importanza. Il modello viene poi messo a punto, consentendogli di adattarsi senza problemi ai cambiamenti strutturali prima della successiva iterazione di potatura. Questo approccio graduale e ciclico impedisce bruschi cali di accuratezza. Consente al modello di adattarsi lentamente man mano che le strutture vengono ridotte attraverso le iterazioni.\nConsideriamo una situazione in cui desideriamo potare i 6 canali meno efficaci (in base ad alcuni criteri specifici) da una rete neurale convoluzionale. In Figura 9.3, mostriamo un processo di potatura semplificato eseguito su 3 iterazioni. In ogni iterazione, eliminiamo solo 2 canali. La rimozione dei canali comporta un degrado della precisione. Nella prima iterazione, la precisione scende da 0.995 a 0.971. Tuttavia, dopo aver perfezionato il modello sulla nuova struttura, siamo in grado di recuperare dalla perdita di prestazioni, portando la precisione a 0.992. Poiché i cambiamenti strutturali sono minori e graduali, la rete può adattarsi più facilmente a essi. Eseguendo lo stesso processo altre 2 volte, finiamo con una precisione finale di 0.991 (una perdita di solo lo 0.4% rispetto all’originale) e una riduzione del 27% nel numero di canali. Pertanto, la potatura iterativa ci consente di mantenere le prestazioni beneficiando di una maggiore efficienza computazionale dovuta alla riduzione delle dimensioni del modello.\n\n\n\n\n\n\nFigura 9.3: Potatura iterativa.\n\n\n\nLa potatura one-shot adotta un approccio più aggressivo, potando una grande porzione di strutture simultaneamente in un’unica operazione in base a criteri di importanza predefiniti. Segue un’ampia messa a punto per recuperare l’accuratezza del modello. Sebbene più rapida, questa strategia aggressiva può degradare l’accuratezza se il modello non riesce a recuperare durante la messa a punto.\nLa scelta tra queste strategie comporta la valutazione di fattori quali dimensioni del modello, quanto è sparso il target, calcolo disponibile e perdite di accuratezza accettabili. La potatura one-shot può comprimere rapidamente i modelli, ma quella iterativa può consentire una migliore conservazione dell’accuratezza per un livello target di potatura. In pratica, la strategia è personalizzata in base ai vincoli del caso d’uso. L’obiettivo generale è quello di generare una strategia ottimale che rimuova la ridondanza, ottenga guadagni di efficienza tramite la potatura e metta a punto il modello per stabilizzare l’accuratezza a un livello accettabile per l’implementazione.\nOra si consideri la stessa rete che avevamo nell’esempio di potatura iterativa. Mentre nel processo iterativo abbiamo potato 2 canali alla volta, nel processo one-shot poteremo i 6 canali contemporaneamente, come mostrato in Figura 9.4. La rimozione simultanea del 27% del canale della rete altera significativamente la struttura, causando un calo della precisione da 0.995 a 0.914. Date le modifiche principali, la rete non è in grado di adattarsi correttamente durante la messa a punto e la precisione è salita a 0.943, un degrado del 5% rispetto alla precisione della rete non potata. Mentre le strutture finali nei processi di potatura iterativa e di potatura one-shot sono identiche, la prima è in grado di mantenere prestazioni elevate mentre la seconda subisce degradi significativi.\n\n\n\n\n\n\nFigura 9.4: Potatura one-shot.\n\n\n\n\n\n\nVantaggi della Potatura Strutturata\nLa potatura strutturata offre una miriade di vantaggi che soddisfano vari aspetti dell’implementazione e dell’utilizzo del modello, specialmente in ambienti in cui le risorse computazionali sono limitate.\n\nEfficienza Computazionale: Eliminando intere strutture, come neuroni o canali, si riduce significativamente il carico computazionale durante le fasi di training e inferenza, consentendo così previsioni più rapide del modello e convergenza del training. Inoltre, la rimozione delle strutture riduce intrinsecamente il “footprint” [impronta] di memoria del modello, assicurando che richieda meno spazio di archiviazione e memoria durante il funzionamento, il che è particolarmente vantaggioso in ambienti con limiti di memoria come i sistemi TinyML.\nEfficienza Hardware: La potatura strutturata spesso si traduce in modelli più adatti all’implementazione su hardware specializzato, come i Field-Programmable Gate Arrays (FPGA) o Application-Specific Integrated Circuits (ASIC), a causa della regolarità e la semplicità dell’architettura potata. Con requisiti di elaborazione ridotti, si traduce in un consumo energetico inferiore, fondamentale per i dispositivi alimentati a batteria e i metodi di elaborazione sostenibili.\nManutenzione e Distribuzione: Il modello ridotto, sebbene più piccolo, mantiene la sua forma architettonica originale, che può semplificare la pipeline di distribuzione e garantire la compatibilità con i sistemi e i framework esistenti. Inoltre, con meno parametri e strutture più semplici, il modello potato diventa più facile da gestire e monitorare negli ambienti di produzione, riducendo potenzialmente le spese generali associate alla manutenzione e agli aggiornamenti del modello. Più avanti, quando approfondiremo MLOps, questa necessità diventerà evidente.\n\n\n\nPotatura non Strutturata\nIl “pruning” non-strutturato è, come suggerisce il nome, la potatura del modello senza riguardo alla sotto-struttura specifica del modello. Come accennato in precedenza, offre una maggiore aggressività nella potatura e può raggiungere maggiori diradazione del modello mantenendo la precisione, dati meno vincoli su ciò che può e non può essere potato. In genere, la potatura non-strutturata post-training consiste in un criterio di importanza per i singoli parametri/pesi del modello, potatura/rimozione dei pesi che scendono al di sotto dei criteri e una successiva messa a punto facoltativa per provare a recuperare la precisione persa durante la rimozione dei pesi.\nLa potatura non-strutturata presenta alcuni vantaggi rispetto a quella strutturata: la rimozione di singoli pesi anziché di intere sotto-strutture del modello spesso porta in pratica a minori diminuzioni della precisione del modello. Inoltre, in genere determinare il criterio di importanza per un singolo peso è molto più semplice che per un’intera sotto-struttura di parametri nella potatura strutturata, rendendo la prima preferibile nei casi in cui tale overhead è difficile o poco chiaro da calcolare. Analogamente, il processo effettivo di potatura strutturata è generalmente meno flessibile, poiché la rimozione di singoli pesi è generalmente più semplice della rimozione di intere sotto-strutture e della garanzia che il modello funzioni ancora.\nLa potatura non strutturata, pur offrendo il potenziale per una significativa riduzione delle dimensioni del modello e una migliore implementabilità, porta con sé problemi legati alla gestione di rappresentazioni sparse e alla garanzia dell’efficienza computazionale. È particolarmente utile in scenari in cui è fondamentale ottenere la massima compressione possibile del modello e in cui l’ambiente di distribuzione può gestire in modo efficiente i calcoli sparsi.\nTabella 9.1 fornisce un confronto conciso tra potatura strutturata e la non-strutturata. In questa tabella, gli aspetti relativi alla natura e all’architettura del modello potato (Definizione, Regolarità del modello e Livello di compressione) sono raggruppati insieme, seguiti dagli aspetti relativi alle considerazioni computazionali (Efficienza computazionale e Compatibilità hardware) e terminando con gli aspetti relativi all’implementazione e all’adattamento del modello potato (Complessità di implementazione e Complessità di messa a punto). Entrambe le strategie di potatura offrono vantaggi e problemi unici, come mostrato in Tabella 9.1, e la selezione tra di esse dovrebbe essere influenzata da requisiti specifici del progetto e della distribuzione.\n\n\n\nTabella 9.1: Confronto tra potatura strutturata e non-strutturata.\n\n\n\n\n\n\n\n\n\n\nAspetto\nPotatura strutturata\nPotatura non strutturata\n\n\n\n\nDefinizione\nPotatura di intere strutture (ad esempio, neuroni, canali, layer) all’interno della rete\nPotatura di singoli pesi o neuroni, con conseguenti matrici sparse o strutture di rete non regolari\n\n\nRegolarità del Modello\nMantiene un’architettura di rete regolare e strutturata\nSi traduce in architetture di rete irregolari e sparse\n\n\nLivello di Compressione\nPuò offrire una compressione del modello limitata rispetto alla potatura non-strutturata\nPuò ottenere una compressione del modello più elevata grazie alla potatura a grana fine\n\n\nEfficienza Computazionale\nIn genere più efficiente computazionalmente grazie al mantenimento di strutture regolari\nPuò essere inefficiente dal punto di vista computazionale a causa di matrici di peso sparse, a meno che non venga utilizzato hardware/software specializzato\n\n\nCompatibilità Hardware\nIn genere più compatibile con vari hardware grazie alle strutture regolari\nPotrebbe richiedere hardware che gestisca in modo efficiente i calcoli sparsi per ottenere vantaggi\n\n\nComplessità di Implementazione\nSpesso più semplice da implementare e gestire grazie al mantenimento della struttura della rete\nPuò essere complesso da gestire e calcolare a causa delle rappresentazioni sparse\n\n\nComplessità di Messa a Punto Fine\nPotrebbe richiedere strategie di messa a punto fine meno complesse dopo la potatura\nPotrebbe richiedere strategie di riaddestramento o messa a punto fine più complesse dopo la potatura\n\n\n\n\n\n\nIn Figura 9.5 abbiamo esempi che illustrano le differenze tra potatura non-strutturata e strutturata. Osservare che la potatura non-strutturata può portare a modelli che non rispettano più le garanzie strutturali di alto livello delle loro controparti originali non potate: la rete di sinistra non è più una rete completamente connessa dopo la potatura. La potatura strutturata, d’altro canto, mantiene quelle invarianti: al centro, la rete completamente connessa viene potata in modo che resti ancora completamente connessa; allo stesso modo, la CNN mantiene la sua struttura convoluzionale, sebbene con meno filtri.\n\n\n\n\n\n\nFigura 9.5: Potatura non-strutturata e strutturata. Fonte: Qi et al. (2021).\n\n\nQi, Chen, Shibo Shen, Rongpeng Li, Zhifeng Zhao, Qing Liu, Jing Liang, e Honggang Zhang. 2021. «An efficient pruning scheme of deep neural networks for Internet of Things applications». EURASIP Journal on Advances in Signal Processing 2021 (1): 31. https://doi.org/10.1186/s13634-021-00744-4.\n\n\n\n\nIpotesi del Biglietto della Lotteria\nLa potatura si è evoluta da una tecnica puramente post-addestramento che comportava un costo per una certa accuratezza, a un potente approccio di meta-apprendimento applicato durante l’addestramento per ridurre la complessità del modello. Questo progresso a sua volta migliora l’efficienza di calcolo, memoria e latenza sia nell’addestramento che nell’inferenza.\nUna scoperta rivoluzionaria che ha catalizzato questa evoluzione è stata l’ipotesi del biglietto della lotteria di Frankle e Carbin (2019). Il loro lavoro afferma che all’interno di reti neurali dense esistono sotto-reti sparse, denominate “biglietti vincenti”, che possono eguagliare o addirittura superare le prestazioni del modello originale quando addestrate in isolamento. In particolare, questi biglietti vincenti, quando inizializzati utilizzando gli stessi pesi della rete originale, possono raggiungere una convergenza e un’accuratezza di addestramento altrettanto elevate su un dato compito. Vale la pena sottolineare che hanno scoperto empiricamente l’ipotesi del biglietto della lotteria, che è stata successivamente formalizzata.\n\nFrankle, Jonathan, e Michael Carbin. 2019. «The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks». In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net. https://openreview.net/forum?id=rJl-b3RcF7.\nL’intuizione alla base di questa ipotesi è che, durante il processo di addestramento di una rete neurale, molti neuroni e connessioni diventano ridondanti o non importanti, in particolare con l’inclusione di tecniche di addestramento che incoraggiano la ridondanza come il “dropout” [abbandono]. L’identificazione, la potatura e l’inizializzazione di questi “biglietti vincenti” consentono un addestramento più rapido e modelli più efficienti, poiché contengono le informazioni essenziali per la decisione del modello per l’attività. Inoltre, come generalmente noto con la teoria del “bias-variance tradeoff” [compromesso tra bias e varianza], questi biglietti soffrono meno di sovra-parametrizzazione e quindi si generalizzano meglio piuttosto che sovra-adattarsi all’attività.\nIn Figura 9.6 abbiamo un esempio che mostra esperimenti di potatura e addestramento su una LeNet completamente connessa su una varietà di rapporti di potatura. Nel grafico a sinistra, si nota come una potatura pesante riveli una sotto-rete più efficiente (in verde) che è il 21,1% delle dimensioni della rete originale (in blu). La sotto-rete raggiunge una maggiore accuratezza e in modo più rapido rispetto alla versione non potata (la linea verde è sopra la linea blu). Tuttavia, la potatura ha un limite (punto ottimale) e un’ulteriore potatura produrrà degradi delle prestazioni e alla fine scenderà al di sotto delle prestazioni della versione non potata (nota come le sotto-reti rossa, viola e marrone diminuiscono gradualmente nelle prestazioni di accuratezza) a causa della significativa perdita nel numero di parametri.\n\n\n\n\n\n\nFigura 9.6: Esperimenti sull’ipotesi del biglietto della lotteria.\n\n\n\nPer scoprire questi biglietti vincenti della lotteria all’interno di una rete neurale, viene seguito un processo sistematico. Questo processo, illustrato in Figura 9.7 (a sinistra), prevede l’addestramento iterativo, la potatura e la reinizializzazione della rete. I passaggi seguenti delineano questo approccio:\n\nInizializzare i pesi della rete a valori casuali.\nAddestrare la rete finché non converge alle prestazioni desiderate.\nEliminare una percentuale di rami con i valori di peso più bassi.\nReinizializzare la rete con gli stessi valori casuali del passaggio 1.\nRipetere i passaggi 2-4 più volte o finché la precisione non peggiora in modo significativo.\n\nAlla fine, ci si ritrova con una rete potata (Figura 9.7 lato destro), che è una sotto-rete di quella di partenza. La sotto-rete dovrebbe avere una struttura significativamente più piccola, pur mantenendo un livello di precisione comparabile.\n\n\n\n\n\n\nFigura 9.7: Trovare la sottorete del biglietto vincente.\n\n\n\n\n\nProblemi e Limitazioni\nNon c’è niente di gratuito con le ottimizzazioni di potatura, con alcune scelte che comportano sia miglioramenti che costi da considerare. Di seguito, discutiamo alcuni compromessi che gli esperti devono considerare.\n\nGestione di Matrici di Peso Sparse: Una matrice di peso sparsa è una matrice in cui molti degli elementi sono pari a zero. La potatura non strutturata spesso produce matrici di peso sparse, in cui molti pesi vengono potati a zero. Sebbene ciò riduca le dimensioni del modello, introduce anche diversi problemi. L’inefficienza computazionale può sorgere perché l’hardware standard è ottimizzato per operazioni di matrice densa. Senza ottimizzazioni che sfruttano la sparsità, i risparmi computazionali derivanti dalla potatura possono essere persi. Sebbene le matrici sparse possano essere archiviate senza formati specializzati, sfruttare efficacemente la loro sparsità richiede una gestione attenta per evitare di sprecare risorse. Algoritmicamente, la navigazione in strutture sparse richiede di saltare in modo efficiente le voci zero, il che aggiunge complessità al calcolo e agli aggiornamenti del modello.\nQualità vs. Riduzione delle Dimensioni: Una sfida fondamentale sia nella potatura strutturata che in quella non-strutturata è bilanciare la riduzione delle dimensioni con il mantenimento o il miglioramento delle prestazioni predittive. È essenziale stabilire criteri di potatura robusti, sia per rimuovere intere strutture (potatura strutturata) sia singoli pesi (potatura non strutturata). Questi criteri di potatura scelti devono identificare accuratamente gli elementi la cui rimozione ha un impatto minimo sulle prestazioni. Spesso è necessaria un’attenta sperimentazione per garantire che il modello potato rimanga efficiente mantenendo al contempo le sue prestazioni predittive.\nFine-Tuning e Riaddestramento: La messa a punto post-potatura è fondamentale sia nella potatura strutturata che in quella non-strutturata per recuperare le prestazioni perse e stabilizzare il modello. La sfida comprende la determinazione dell’estensione, della durata e della natura del processo di messa a punto, che può essere influenzato dal metodo di potatura e dal grado di potatura applicato.\nCompatibilità ed Efficienza Hardware: Particolarmente pertinenti alla potatura non-strutturata, la compatibilità e l’efficienza hardware diventano critiche. La potatura non strutturata spesso si traduce in matrici di peso sparse, che potrebbero non essere gestite in modo efficiente da un certo hardware, annullando potenzialmente i vantaggi computazionali della potatura (vedere Figura 9.8). Garantire che i modelli potati, in particolare quelli risultanti dall’eliminazione non-strutturata, siano scalabili, compatibili ed efficienti sull’hardware target è una considerazione importante.\nConsiderazioni Legali ed Etiche: Ultimo ma non meno importante, il rispetto delle linee guida legali ed etiche è importante, soprattutto in ambiti con conseguenze significative. I metodi di potatura devono essere sottoposti a rigorosi processi di validazione, test e potenzialmente certificazione per garantire la conformità alle normative e agli standard pertinenti, sebbene al momento non esistano standard formali e “best practice” che siano esaminati e convalidati da entità terze. Ciò è particolarmente cruciale in applicazioni ad alto rischio come l’intelligenza artificiale medica e la guida autonoma, dove i cali di qualità dovuti a ottimizzazioni simili alla potatura possono essere pericolosi per la vita. Inoltre, le considerazioni etiche si estendono oltre la sicurezza fino all’equità e all’uguaglianza; un recente lavoro di (Tran et al. 2022) ha rivelato che la potatura può avere un impatto sproporzionato sulle persone di colore, sottolineando la necessità di una valutazione etica completa nel processo di potatura.\n\n\nTran, Cuong, Ferdinando Fioretto, Jung-Eun Kim, e Rakshit Naidu. 2022. «Pruning has a disparate impact on model accuracy». Adv Neural Inf Process Syst 35: 17652–64.\n\n\n\n\n\n\nFigura 9.8: Matrice dei pesi sparsi.\n\n\n\n\n\n\n\n\n\nEsercizio 9.1: Pruning\n\n\n\n\n\nSi immagini che la rete neurale sia un cespuglio gigante e troppo cresciuto. La potatura è come tagliare strategicamente i rami per renderla più forte ed efficiente! Nel Colab, si imparerà come fare questa potatura in TensorFlow. La comprensione di questi concetti fornirà le basi per vedere come la potatura rende i modelli abbastanza piccoli da poter essere eseguiti sul telefono!\n\n\n\n\n\n\n\n9.2.2 Compressione del Modello\nLe tecniche di compressione del modello sono fondamentali per distribuire modelli di deep learning su dispositivi con risorse limitate. Queste tecniche mirano a creare modelli più piccoli ed efficienti che preservino le prestazioni predittive dei modelli originali.\n\nDistillazione della Conoscenza\nUna tecnica popolare è la knowledge distillation (KD) distillazione della conoscenza, che trasferisce la conoscenza da un modello “insegnante” ampio e complesso a un modello “studente” più piccolo. L’idea chiave è addestrare il modello studente a imitare gli output dell’insegnante. Il concetto di KD è stato reso popolare per la prima volta da Hinton (2005).\n\nHinton, Geoffrey. 2005. «Van Nostrand’s Scientific Encyclopedia». Wiley. https://doi.org/10.1002/0471743984.vse0673.\n\nPanoramica e Vantaggi\nLa distillazione della conoscenza implica il trasferimento della conoscenza da un modello insegnante ampio e complesso a un modello studente più piccolo. L’idea di base è quella di utilizzare gli output dell’insegnante, noti come soft targets, per guidare il training del modello studente. A differenza dei tradizionali “hard targets” (le vere etichette), quelli soft sono le distribuzioni di probabilità sulle classi che il modello insegnante prevede. Queste distribuzioni forniscono informazioni più complete sulle relazioni tra le classi, il che può aiutare il modello studente ad apprendere in modo più efficace.\nAbbiamo imparato che la funzione softmax converte gli output grezzi di un modello in una distribuzione di probabilità sulle classi. Una tecnica chiave in KD è la scalatura della temperatura, che viene applicata alla funzione softmax degli output del modello insegnante. Introducendo un parametro di temperatura, la distribuzione può essere regolata: una temperatura più alta produce probabilità più soft, il che significa che le differenze tra le probabilità di classe diventano meno estreme. Questo effetto di ammorbidimento determina una distribuzione più uniforme, in cui la fiducia del modello nella classe più probabile è ridotta e altre classi hanno probabilità più elevate, diverse da zero. Ciò è prezioso per il modello studente perché gli consente di apprendere non solo dalla classe più probabile, ma anche dalle probabilità relative di tutte le classi, catturando pattern sottili che potrebbero essere persi se addestrati solo su obiettivi difficili. Pertanto, la scalabilità della temperatura facilita il trasferimento di conoscenze più sfumate dal modello insegnante a quello studente.\nLa funzione di perdita nella distillazione della conoscenza in genere combina due componenti: una perdita di distillazione e una perdita di classificazione. La perdita di distillazione, spesso calcolata utilizzando la divergenza di Kullback-Leibler (KL), misura la differenza tra gli soft target prodotti dal modello insegnante e gli output del modello studente, incoraggiando lo studente a imitare le previsioni dell’insegnante. Nel frattempo, la perdita di classificazione assicura che il modello studente preveda correttamente le etichette vere in base ai dati originali. Insieme, queste due componenti aiutano lo studente modello a conservare le conoscenze dell’insegnante, rispettando al contempo le etichette di verità di base.\nQuesti componenti, quando configurati e armonizzati abilmente, consentono al modello studente di assimilare la conoscenza del modello insegnante, creando un percorso verso modelli più piccoli, efficienti e robusti, che mantengono la capacità predittiva delle loro controparti più grandi. Figura 9.9 visualizza la procedura di training della “knowledge distillation”. Notare come i logit o le soft label del modello insegnante vengono utilizzati per fornire una perdita di distillazione da cui il modello studente può imparare.\n\n\n\n\n\n\nFigura 9.9: Processo di training della distillazione della conoscenza. Fonte: IntelLabs (2023).\n\n\nIntelLabs. 2023. «Knowledge Distillation - Neural Network Distiller». https://intellabs.github.io/distiller/knowledge_distillation.html.\n\n\n\n\nSfide\nTuttavia, KD presenta una serie unica di sfide e considerazioni che ricercatori e professionisti devono affrontare attentamente. Una delle sfide è nella messa a punto meticolosa degli iperparametri, come il parametro “temperatura” nella funzione softmax e la ponderazione tra la distillazione e la perdita di classificazione nella funzione obiettivo. Raggiungere un equilibrio che sfrutti efficacemente gli output ammorbiditi del modello insegnante mantenendo al contempo la fedeltà alle etichette dei dati reali non è banale e può avere un impatto significativo sulle prestazioni e sulle capacità di generalizzazione del modello studente.\nInoltre, l’architettura del modello studente stesso pone un problema considerevole. Progettare un modello compatto per soddisfare i vincoli di calcolo e memoria, pur essendo in grado di assimilare le conoscenze essenziali dal modello insegnante, richiede una comprensione sfumata della capacità del modello e dei compromessi intrinseci coinvolti nella compressione. Il modello studente deve essere attentamente progettato per navigare nella dicotomia di dimensioni e prestazioni, assicurando che la conoscenza distillata venga catturata e utilizzata in modo significativo. Inoltre, la scelta del modello dell’insegnante, che influenza intrinsecamente la qualità e la natura della conoscenza da trasferire, è importante e introduce un ulteriore livello di complessità nel processo KD.\nQueste sfide sottolineano la necessità di un approccio completo e sfumato all’implementazione di KD, assicurando che i modelli degli studenti risultanti siano sia efficienti che efficaci nei loro contesti operativi.\n\n\n\nFattorizzazione di Matrici di Basso Rango\nSimile nel tema dell’approssimazione, la Low-Rank Matrix Factorization (LRMF) fattorizzazione di matrici di basso rango è una tecnica matematica utilizzata in algebra lineare e analisi dei dati per approssimare una matrice data scomponendola in due o più matrici di dimensione inferiore. L’idea fondamentale è di esprimere una matrice di grandi dimensioni come prodotto di matrici di rango inferiore, il che può aiutare a ridurre la complessità dei dati preservandone la struttura essenziale. Matematicamente, data una matrice \\(A \\in \\mathbb{R}^{m \\times n}\\), LRMF cercare le matrici \\(U \\in \\mathbb{R}^{m \\times k}\\) e \\(V \\in \\mathbb{R}^{k \\times n}\\) tali che \\(A \\approx UV\\), dove \\(k\\) è il rango ed è in genere molto più piccolo di \\(m\\) e \\(n\\).\n\nBackground e Benefici\nUno dei primo lavori nel campo della fattorizzazione di matrici, in particolare nel contesto dei sistemi di raccomandazione, è il documento di Koren, Bell, e Volinsky (2009). Gli autori esaminano vari modelli di fattorizzazione, fornendo approfondimenti sulla loro efficacia nel catturare i pattern sottostanti nei dati e nel migliorare l’accuratezza predittiva nel filtraggio collaborativo. LRMF è stato ampiamente applicato nei sistemi di raccomandazione (come Netflix, Facebook, ecc.), dove la matrice di interazione utente-elemento è fattorizzata per catturare fattori latenti corrispondenti alle preferenze dell’utente e agli attributi dell’elemento.\n\nKoren, Yehuda, Robert Bell, e Chris Volinsky. 2009. «Matrix Factorization Techniques for Recommender Systems». Computer 42 (8): 30–37. https://doi.org/10.1109/mc.2009.263.\nIl vantaggio principale della “fattorizzazione di matrici di basso rango” risiede nella sua capacità di ridurre la dimensionalità dei dati come mostrato in Figura 9.10, dove ci sono meno parametri da memorizzare, rendendola più efficiente dal punto di vista computazionale e riducendo i requisiti di archiviazione a costo di un po’ di elaborazione aggiuntiva. Ciò può portare a calcoli più rapidi e rappresentazioni di dati più compatte, il che è particolarmente prezioso quando si ha a che fare con grandi set di dati. Inoltre, può aiutare nella riduzione del rumore e può rivelare pattern e relazioni sottostanti nei dati.\nFigura 9.10 illustra la diminuzione della parametrizzazione abilitata dalla fattorizzazione di matrici di basso rango. Osservare come la matrice \\(M\\) può essere approssimata dal prodotto delle matrici \\(L_k\\) e \\(R_k^T\\). Per intuizione, la maggior parte dei layer completamente connessi nelle reti sono archiviati come matrice di proiezione \\(M\\), che richiede il caricamento di \\(m \\times n\\) parametri durante il calcolo. Tuttavia, scomponendola e approssimandola come prodotto di due matrici di rango inferiore, abbiamo bisogno di archiviare solo \\(m \\times k + k\\times n\\) parametri in termini di archiviazione, sostenendo al contempo un costo di calcolo aggiuntivo per la moltiplicazione delle matrici. Finché \\(k &lt; n/2\\), questa fattorizzazione ha meno parametri totali da archiviare, aggiungendo un calcolo di runtime \\(O(mkn)\\) (Gu 2023).\n\nGu, Ivy. 2023. «Deep Learning Model Compression (ii) by Ivy Gu Medium». https://ivygdy.medium.com/deep-learning-model-compression-ii-546352ea9453.\n\n\n\n\n\n\nFigura 9.10: Fattorizzazione di matrici di basso rango. Fonte: The Clever Machine.\n\n\n\n\n\nSfide\nMa professionisti e ricercatori incontrano una serie di problemi e considerazioni che richiedono una particolare attenzione e approcci strategici. Come con qualsiasi tecnica di compressione lossy [con perdita], potremmo perdere informazioni durante questo processo di approssimazione: scegliere il rango corretto che bilanci le informazioni perse e i costi computazionali è altrettanto complicato e aggiunge un ulteriore iperparametro da regolare.\nLa fattorizzazione di matrici di basso rango è uno strumento prezioso per la riduzione della dimensionalità e per adattare il calcolo ai dispositivi edge ma, come altre tecniche, deve essere attentamente regolata in base al modello e all’attività da svolgere. Una sfida fondamentale risiede nella gestione della complessità computazionale inerente a LRMF, soprattutto quando si hanno a che fare con dati ad alta dimensionalità e su larga scala. L’onere computazionale, in particolare nel contesto di applicazioni in tempo reale e set di dati massicci, rimane un ostacolo significativo per un utilizzo efficace di LRMF.\nInoltre, l’enigma della scelta del rango ottimale \\(k\\) per la fattorizzazione introduce un ulteriore livello di complessità. La selezione di \\(k\\) implica intrinsecamente un compromesso tra accuratezza dell’approssimazione e semplicità del modello, e l’identificazione di un rango che bilanci abilmente questi obiettivi contrastanti spesso richiede una combinazione di competenza di dominio, convalida empirica e, a volte, approcci euristici. La sfida è ulteriormente amplificata quando i dati comprendono rumore o quando la struttura intrinseca di basso rango non è pronunciata, rendendo la determinazione di un \\(k\\) adatto ancora più sfuggente.\nLa gestione di dati mancanti o sparsi, un evento comune in applicazioni come i sistemi di raccomandazione, pone un’altra sfida sostanziale. Le tecniche tradizionali di fattorizzazione delle matrici, come la Singular Value Decomposition (SVD), non sono direttamente applicabili alle matrici con voci mancanti, rendendo necessario lo sviluppo e l’applicazione di algoritmi specializzati in grado di fattorizzare matrici incomplete mitigando al contempo i rischi di overfitting alle voci osservate. Ciò spesso comporta l’incorporazione di termini di regolarizzazione o la limitazione della fattorizzazione in modi specifici, il che a sua volta introduce ulteriori iperparametri che devono essere selezionati giudiziosamente.\nInoltre, in scenari in cui i dati evolvono o crescono nel tempo, sviluppare modelli LRMF in grado di adattarsi a nuovi dati senza richiedere una completa rifattorizzazione è un’impresa critica ma impegnativa. Gli algoritmi di fattorizzazione di matrici incrementali e online cercano di risolvere questo problema consentendo l’aggiornamento delle matrici fattorizzate all’arrivo di nuovi dati, ma garantire stabilità, accuratezza ed efficienza computazionale in queste impostazioni dinamiche rimane un compito intricato. Ciò è particolarmente impegnativo nello spazio di TinyML, in cui la ridistribuzione dei rami per i modelli aggiornati può essere piuttosto impegnativa.\n\n\n\nDecomposizione dei Tensori\nAbbiamo visto in Sezione 6.4.1 che i tensori sono strutture flessibili, comunemente utilizzate dai framework ML, che possono rappresentare dati in dimensioni superiori. Similmente alla fattorizzazione di matrici di basso rango, i modelli più complessi possono memorizzare pesi in dimensioni superiori, come i tensori. La decomposizione tensoriale è l’analogo a più dimensioni della fattorizzazione della matrice, in cui un tensore modello viene scomposto in componenti di rango inferiore (Figura 9.11). Questi componenti di rango inferiore sono più facili da calcolare e memorizzare, ma possono soffrire degli stessi problemi menzionati sopra, come la perdita di informazioni e la necessità di una messa a punto sfumata degli iperparametri. Matematicamente, dato un tensore \\(\\mathcal{A}\\), la decomposizione tensoriale cerca di rappresentare \\(\\mathcal{A}\\) come una combinazione di tensori più semplici, facilitando una rappresentazione compressa che approssima i dati originali riducendo al minimo la perdita di informazioni.\n\n\n\n\n\n\nFigura 9.11: Decomposizione dei Tensori. Fonte: Xinyu (s.d.).\n\n\nXinyu, Chen. s.d.\n\n\nIl lavoro di Tamara G. Kolda e Brett W. Bader, “Tensor Decompositions and Applications” (2009), si distingue come un articolo fondamentale nel campo delle decomposizioni tensoriali. Gli autori forniscono una panoramica completa di vari metodi di decomposizione tensoriale, esplorandone i fondamenti matematici, gli algoritmi e un’ampia gamma di applicazioni, che vanno dall’elaborazione del segnale al data mining. Naturalmente, il motivo per cui ne stiamo discutendo è perché ha un enorme potenziale per i miglioramenti delle prestazioni del sistema, in particolare nello spazio di TinyML, dove la produttività e i risparmi di memoria sono fondamentali per la fattibilità delle distribuzioni.\n\n\n\n\n\n\nEsercizio 9.2: Compressione di Modelli Scalabili con TensorFlow\n\n\n\n\n\nQuesto Colab si addentra in una tecnica per comprimere i modelli mantenendo un’elevata accuratezza. L’idea chiave è quella di addestrare un modello con un termine di penalità extra che incoraggia il modello a essere più comprimibile. Quindi, il modello viene codificato utilizzando uno schema di codifica speciale che si allinea con questa penalità. Questo approccio consente di ottenere modelli compressi che funzionano altrettanto bene dei modelli originali ed è utile per distribuire modelli su dispositivi con risorse limitate come telefoni cellulari e dispositivi edge.\n\n\n\n\n\n\n\n9.2.3 Modelli Progettati per l’Edge\nOra raggiungiamo l’altro estremo del gradiente hardware-software, dove prendiamo decisioni specifiche sull’architettura del modello direttamente in base alla conoscenza dei dispositivi edge su cui desideriamo implementare.\nCome spiegato nelle sezioni precedenti, i dispositivi edge sono vincolati specificamente da limitazioni di memoria e calcoli parallelizzabili: in quanto tali, se ci sono requisiti critici di velocità di inferenza, i calcoli devono essere sufficientemente flessibili da soddisfare i vincoli hardware, qualcosa che può essere progettato a livello di architettura del modello. Inoltre, cercare di stipare grandi modelli SOTA ML su dispositivi edge anche dopo potatura e compressione è generalmente irrealizzabile puramente a causa delle dimensioni: la complessità del modello stesso deve essere scelta con più sfumature per adattarsi più fattibilmente al dispositivo. Gli sviluppatori di Edge ML hanno affrontato questa sfida architettonica sia attraverso la progettazione di architetture di modelli edge ML su misura sia attraverso la Neural Architecture Search (NAS) [ricerca di architettura neurale] avente il dispositivo come target, che può generare in modo più sistematico architetture fattibili di modelli su dispositivo.\n\nTecniche di Progettazione del Modello\nUn design di architettura edge friendly, comunemente utilizzato nel deep learning per l’elaborazione delle immagini, è quello delle convoluzioni separabili in profondità. Consiste in due fasi distinte: la prima è la convoluzione in profondità, in cui ogni canale di input viene convoluto in modo indipendente con il proprio set di filtri apprendibili, come mostrato in Figura 9.12. Questa fase riduce la complessità computazionale in modo significativo rispetto alle convoluzioni standard, poiché riduce drasticamente il numero di parametri e calcoli coinvolti. La seconda fase è la convoluzione puntuale, che combina l’output dei canali di convoluzione in profondità tramite una convoluzione 1x1, creando interazioni tra canali. Questo approccio offre diversi vantaggi. I vantaggi includono dimensioni ridotte del modello, tempi di inferenza più rapidi e spesso una migliore generalizzazione grazie al minor numero di parametri, rendendolo adatto ad applicazioni mobili ed embedded. Tuttavia, le convoluzioni separabili in profondità potrebbero non catturare interazioni spaziali complesse in modo efficace come le convoluzioni standard e potrebbero richiedere più profondità (livelli) per raggiungere lo stesso livello di potenza rappresentativa, portando potenzialmente a tempi di addestramento più lunghi. Tuttavia, la loro efficienza in termini di parametri e calcolo le rende una scelta popolare nelle moderne architetture di reti neurali convoluzionali.\n\n\n\n\n\n\nFigura 9.12: Convoluzioni separabili in profondità. Fonte: Hegde (2023).\n\n\nHegde, Sumant. 2023. «An Introduction to Separable Convolutions - Analytics Vidhya». https://www.analyticsvidhya.com/blog/2021/11/an-introduction-to-separable-convolutions/.\n\n\n\n\nArchitetture di Modello di Esempio\nIn quest’ottica, diverse architetture recenti sono state, fin dall’inizio, progettate specificamente per massimizzare la precisione in un’implementazione edge, in particolare SqueezeNet, MobileNet ed EfficientNet.\n\nSqueezeNet di Iandola et al. (2016), ad esempio, utilizza un’architettura compatta con convoluzioni 1x1 e moduli “fire” per ridurre al minimo il numero di parametri mantenendo al contempo una forte accuratezza.\nMobileNet di Howard et al. (2017), d’altra parte, impiega le suddette convoluzioni separabili in profondità per ridurre sia il calcolo che le dimensioni del modello.\nEfficientNet di Tan e Le (2023) adotta un approccio diverso ottimizzando il ridimensionamento della rete (ovvero variando la profondità, la larghezza e la risoluzione di una rete) e il ridimensionamento composto, una variazione più sfumata del ridimensionamento della rete, per ottenere prestazioni superiori con meno parametri.\n\n\nIandola, Forrest N, Song Han, Matthew W Moskewicz, Khalid Ashraf, William J Dally, e Kurt Keutzer. 2016. «SqueezeNet: Alexnet-level accuracy with 50x fewer parameters and 0.5 MB model size». ArXiv preprint abs/1602.07360. https://arxiv.org/abs/1602.07360.\n\nHoward, Andrew G., Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, e Hartwig Adam. 2017. «MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications». ArXiv preprint. https://arxiv.org/abs/1704.04861.\n\nTan, Mingxing, e Quoc V. Le. 2023. «Demystifying Deep Learning». Wiley. https://doi.org/10.1002/9781394205639.ch6.\nQuesti modelli sono essenziali nel contesto dell’edge computing in cui la limitazione di potenza di elaborazione e di memoria richiede modelli leggeri ma efficaci in grado di eseguire in modo efficiente attività quali il riconoscimento delle immagini, il rilevamento di oggetti e altro ancora. I loro principi di progettazione mostrano l’importanza di un’architettura di modelli intenzionalmente personalizzata per l’edge computing, in cui prestazioni ed efficienza devono rientrare nei vincoli.\n\n\nSemplificazione della Ricerca di Architetture di Modelli\nInfine, per affrontare la sfida di trovare architetture di modelli efficienti che siano compatibili con i dispositivi edge, i ricercatori hanno sviluppato pipeline sistematizzate che semplificano la ricerca di progetti performanti. Due framework degni di nota in questo spazio sono TinyNAS di J. Lin et al. (2020) e MorphNet di Gordon et al. (2018), che automatizzano il processo di ottimizzazione delle architetture di reti neurali per l’implementazione edge.\n\nGordon, Ariel, Elad Eban, Ofir Nachum, Bo Chen, Hao Wu, Tien-Ju Yang, e Edward Choi. 2018. «MorphNet: Fast &amp; Simple Resource-Constrained Structure Learning of Deep Networks». In 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, 1586–95. IEEE. https://doi.org/10.1109/cvpr.2018.00171.\nTinyNAS è un innovativo framework di ricerca di architetture neurali introdotto nel documento MCUNet, progettato per scoprire in modo efficiente architetture di reti neurali leggere per dispositivi edge con risorse computazionali limitate. Sfruttando l’apprendimento per rinforzo e uno spazio di ricerca compatto di micromoduli neurali, TinyNAS ottimizza sia l’accuratezza che la latenza, consentendo l’implementazione di modelli di deep learning su microcontrollori, dispositivi IoT e altre piattaforme con risorse limitate. Nello specifico, TinyNAS, in combinazione con un ottimizzatore di rete, TinyEngine, genera diversi spazi di ricerca ridimensionando la risoluzione di input e la larghezza del modello, poi raccoglie la distribuzione FLOP di calcolo delle reti soddisfacenti all’interno dello spazio di ricerca per valutarne la priorità. TinyNAS si basa sul presupposto che uno spazio di ricerca che ospita FLOP più elevati con vincoli di memoria possa produrre modelli di accuratezza più elevata, cosa che gli autori hanno verificato in pratica nel loro lavoro. In termini di prestazioni empiriche, TinyEngine ha ridotto l’utilizzo di memoria di picco dei modelli di circa 3.4 volte e ha accelerato l’inferenza da 1.7 a 3.3 volte rispetto a TFLite e a CMSIS-NN.\nAnalogamente, MorphNet è un framework di ottimizzazione delle reti neurali progettato per rimodellare e trasformare automaticamente l’architettura delle reti neurali profonde, ottimizzandole per requisiti di distribuzione specifici. Ciò avviene in due fasi: in primo luogo, sfrutta un set di operazioni di morphing della rete personalizzabili, come l’ampliamento o l’approfondimento dei layer, per regolare dinamicamente la struttura della rete. Queste operazioni consentono alla rete di adattarsi a vari vincoli computazionali, tra cui dimensioni del modello, latenza e obiettivi di accuratezza, che sono estremamente diffusi nell’utilizzo dell’edge computing. Nella seconda fase, MorphNet utilizza un approccio basato sull’apprendimento di rinforzo per cercare la permutazione ottimale delle operazioni di morphing, bilanciando efficacemente il compromesso tra dimensioni del modello e prestazioni. Questo metodo innovativo consente ai professionisti del deep learning di adattare automaticamente le architetture delle reti neurali a requisiti hardware e applicativi specifici, garantendo un’implementazione efficiente ed efficace su diverse piattaforme.\nTinyNAS e MorphNet rappresentano alcuni dei numerosi progressi significativi nel campo dell’ottimizzazione sistematica delle reti neurali, consentendo di scegliere e generare sistematicamente architetture per adattarsi perfettamente ai vincoli del problema.\n\n\n\n\n\n\nEsercizio 9.3: Modelli Progettati per l’Edge\n\n\n\n\n\nSi Immagini di costruire un piccolo robot in grado di identificare diversi fiori. Deve essere intelligente, ma anche piccolo ed efficiente dal punto di vista energetico! Nel mondo dell’“Edge-Aware Model Design”, abbiamo appreso tecniche come le convoluzioni separabili in base alla profondità e architetture come SqueezeNet, MobileNet ed EfficientNet, tutte progettate per concentrare l’intelligenza in modelli compatti. Ora, vediamo queste idee in azione con alcuni xColab:\nSqueezeNet in Action: Forse piacerebbe un Colab che mostra come addestrare un modello SqueezeNet su un set di dati di immagini di fiori. Ciò dimostrerebbe le sue piccole dimensioni e come impara a riconoscere i pattern nonostante la sua efficienza.\n\nMobileNet Exploration: Ci si è mai chiesto se quei piccoli modelli di immagini sono buoni quanto quelli grandi? Scopriamolo! In questo Colab, mettiamo a confronto MobileNet, il campione dei pesi leggeri, con un modello di classificazione delle immagini classico. Li faremo gareggiare per la velocità, misureremo le loro esigenze di memoria e vedremo chi vincerà per accuratezza. Preparatevi per una battaglia di cervelli di immagini!",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Ottimizzazioni dei Modelli</span>"
    ]
  },
  {
    "objectID": "contents/core/optimizations/optimizations.it.html#sec-model_ops_numerics",
    "href": "contents/core/optimizations/optimizations.it.html#sec-model_ops_numerics",
    "title": "9  Ottimizzazioni dei Modelli",
    "section": "9.3 Rappresentazione Numerica Efficiente",
    "text": "9.3 Rappresentazione Numerica Efficiente\nLa rappresentazione numerica implica una miriade di considerazioni, tra cui, ma non solo, la precisione dei numeri, i loro formati di codifica e le operazioni aritmetiche facilitate. Implica invariabilmente una vasta gamma di diversi compromessi, in cui i professionisti sono incaricati di destreggiarsi tra accuratezza numerica ed efficienza computazionale. Ad esempio, mentre i numeri a bassa precisione possono offrire il fascino di un utilizzo di memoria ridotto e calcoli accelerati, presentano contemporaneamente sfide relative alla stabilità numerica e al potenziale degrado dell’accuratezza del modello.\n\nMotivazione\nEmerge l’imperativo per una rappresentazione numerica efficiente, in particolare perché l’ottimizzazione efficiente del modello da sola non è sufficiente quando si adattano i modelli per l’implementazione su dispositivi edge a bassa potenza che operano con vincoli rigorosi.\nOltre a ridurre al minimo le richieste di memoria, l’enorme potenziale di una rappresentazione numerica efficiente risiede, ma non è limitato a, queste modalità fondamentali. Riducendo l’intensità computazionale, la matematica efficiente può amplificare la velocità computazionale, consentendo di elaborare modelli più complessi su dispositivi a bassa potenza. Ridurre la precisione in bit di pesi e attivazioni su modelli fortemente sovra-parametrizzati consente la condensazione delle dimensioni del modello per dispositivi edge senza danneggiare significativamente l’accuratezza predittiva del modello. Con l’onnipresenza delle reti neurali nei modelli, la matematica efficiente ha un vantaggio unico nello sfruttare la struttura a layer delle NN per variare la precisione numerica tra i layer, riducendo al minimo la precisione nei layer resistenti e preservando una maggiore precisione in quelli sensibili.\nIn questa sezione, approfondiremo il modo in cui i professionisti possono sfruttare i principi della progettazione congiunta hardware-software ai livelli più bassi di un modello per facilitare la compatibilità con i dispositivi edge. Iniziando con un’introduzione ai numeri, esamineremo le sue implicazioni per la memoria del dispositivo e la complessità computazionale. Successivamente, intraprenderemo una discussione sui compromessi implicati nell’adozione di questa strategia, seguita da un’analisi approfondita di un metodo fondamentale della matematica efficiente: la quantizzazione.\n\n\n9.3.1 Le Basi\n\nI Tipi\nI dati numerici, il fondamento su cui si basano i modelli di apprendimento automatico, si manifestano in due forme principali. Si tratta di numeri interi e numeri in virgola mobile.\nNumeri Interi: Numeri interi, privi di componenti frazionarie, (ad esempio, -3, 0, 42) sono fondamentali negli scenari che richiedono valori discreti. Ad esempio, in ML, le etichette di classe in un’attività di classificazione potrebbero essere rappresentate come numeri interi, dove “gatto”, “cane” e “uccello” potrebbero essere codificati rispettivamente come 0, 1 e 2.\nNumeri in virgola mobile: Comprendendo numeri reali, (ad esempio, -3.14, 0.01, 2.71828) consentono la rappresentazione di valori con componenti frazionarie. Nei parametri del modello ML, i pesi potrebbero essere inizializzati con piccoli valori a virgola mobile, ad esempio 0.001 o -0.045, per avviare il processo di training. Attualmente, ci sono 4 popolari formati di precisione discussi di seguito.\nLarghezze di bit variabili: Oltre alle larghezze standard, sono in corso ricerche su numeri con larghezze di bit estremamente basse, persino fino a rappresentazioni binarie o ternarie. Le operazioni con larghezza di bit estremamente ridotta possono offrire accelerazioni significative e ridurre ulteriormente il consumo energetico. Sebbene permangano dei problemi nel mantenere l’accuratezza del modello con una quantizzazione così drastica, si continuano a fare progressi in quest’area.\n\n\nPrecisione\nLa precisione, che delinea l’esattezza con cui un numero è rappresentato, si biforca tipicamente in singola, doppia, mezza e negli ultimi anni sono emerse numerose altre precisioni per supportare meglio e in modo efficiente le attività di apprendimento automatico sull’hardware sottostante.\nDoppia precisione (Float64): Allocando 64 bit, la doppia precisione (ad esempio, 3.141592653589793) fornisce una precisione elevata, sebbene richieda più memoria e più risorse di calcolo. Nei calcoli scientifici, dove la precisione è fondamentale, variabili come π potrebbero essere rappresentate con Float64.\nSingola precisione (Float32): Con 32 bit a disposizione, la singola precisione (ad esempio, 3.1415927) raggiunge un equilibrio tra precisione numerica e risparmio della memoria. In ML, Float32 potrebbe essere impiegato per memorizzare i pesi durante l’addestramento per mantenere un livello ragionevole di precisione.\nHalf Precision (Float16): Limitata a 16 bit, la half precision (ad esempio, 3.14) riduce l’utilizzo della memoria e può velocizzare i calcoli, sebbene sacrifichi l’accuratezza e l’intervallo numerico. In ML, specialmente durante l’inferenza su dispositivi con risorse limitate, Float16 potrebbe essere utilizzato per ridurre l’impronta di memoria del modello.\nBfloat16: Brain Floating-Point Format o Bfloat16, impiega anche 16 bit ma li alloca in modo diverso rispetto a FP16: 1 bit per il segno, 8 bit per l’esponente (che si traduce nello stesso intervallo numerico di float32) e 7 bit per la frazione. Questo formato, sviluppato da Google, dà priorità a un intervallo di esponenti più ampio rispetto alla precisione, rendendolo particolarmente utile nelle applicazioni di apprendimento profondo in cui l’intervallo dinamico è cruciale.\nFigura 9.13 illustra le differenze tra i tre formati a virgola mobile: Float32, Float16 e BFloat16.\n\n\n\n\n\n\nFigura 9.13: Tre formati a virgola mobile.\n\n\n\nIntero: Le rappresentazioni di numeri interi sono realizzate utilizzando 8, 4 e 2 bit. Vengono spesso utilizzati durante la fase di inferenza delle reti neurali, in cui i pesi e le attivazioni del modello sono quantizzati a queste precisioni inferiori. Le rappresentazioni intere sono deterministiche e offrono notevoli vantaggi in termini di velocità e memoria rispetto alle rappresentazioni in virgola mobile. Per molte attività di inferenza, in particolare su dispositivi edge, la leggera perdita di accuratezza dovuta alla quantizzazione è spesso accettabile dati i guadagni di efficienza. Una forma estrema di numeri interi è per le reti neurali binarie (BNN), in cui pesi e attivazioni sono vincolati a uno di due valori: +1 o -1.\nÈ possibile fare riferimento a Sezione 8.6.1 per una tabella di confronto tra i compromessi dei diversi tipi numerici.\n\n\nCodifica e Archiviazione Numerica\nLa codifica numerica, l’arte di trasformare i numeri in un formato utilizzabile dal computer e la loro successiva memorizzazione sono fondamentali per l’efficienza computazionale. Ad esempio, i numeri in virgola mobile potrebbero essere codificati utilizzando lo standard IEEE 754, che ripartisce i bit tra i componenti segno, esponente e frazione, consentendo così la rappresentazione di una vasta gamma di valori con un singolo formato. Esistono alcuni nuovi formati in virgola mobile IEEE che sono stati definiti specificamente per i carichi di lavoro AI:\n\nbfloat16- Un formato in virgola mobile a 16 bit introdotto da Google. Ha 8 bit per esponente, 7 bit per mantissa e 1 bit per segno. Offre un compromesso di precisione ridotto tra float a 32 bit e interi a 8 bit. Supportato su molti acceleratori hardware.\nposit - Un formato configurabile che può rappresentare diversi livelli di precisione in base ai bit esponente. È più efficiente dei numeri binari in virgola mobile IEEE 754. Ha una gamma dinamica e una precisione regolabili.\nFlexpoint - Un formato introdotto da Intel che può regolare dinamicamente la precisione tra livelli o all’interno di un layer. Consente di adattare la precisione all’accuratezza e ai requisiti hardware.\nBF16ALT - Un formato a 16 bit proposto da ARM come alternativa a bfloat16. Utilizza un bit aggiuntivo nell’esponente per evitare overflow/underflow.\nTF32 - Introdotto da Nvidia per le GPU Ampere. Utilizza 10 bit per l’esponente invece di 8 bit come FP32. Migliora le prestazioni di training del modello mantenendo l’accuratezza.\nFP8 - Formato a virgola mobile a 8 bit che mantiene 6 bit per la mantissa e 2 bit per l’esponente. Consente una gamma dinamica migliore rispetto agli interi.\n\nGli obiettivi principali di questi nuovi formati sono di fornire alternative di precisione inferiore ai float a 32 bit per una migliore efficienza computazionale e prestazioni sugli acceleratori AI, mantenendo al contempo l’accuratezza del modello. Offrono diversi compromessi in termini di precisione, portata e costo/complessità di implementazione.\n\n\n\n9.3.2 Vantaggi dell’Efficienza\nCome visto in Sezione 8.6.2, l’efficienza numerica è importante per i carichi di lavoro di apprendimento automatico per una serie di motivi. L’efficienza numerica non riguarda solo la riduzione della larghezza di bit dei numeri, ma anche la comprensione dei compromessi tra accuratezza ed efficienza. Man mano che i modelli di apprendimento automatico diventano più pervasivi, soprattutto in ambienti reali con risorse limitate, l’attenzione su una numerica efficiente continuerà a crescere. Selezionando e sfruttando attentamente la precisione numerica appropriata, è possibile ottenere prestazioni di modello robuste ottimizzando al contempo velocità, memoria ed energia.\n\n\n9.3.3 Sfumature della Rappresentazione Numerica\nCi sono diverse sfumature con le rappresentazioni numeriche per ML che richiedono di avere una comprensione sia degli aspetti teorici che pratici della rappresentazione numerica, nonché una profonda consapevolezza dei requisiti e dei vincoli specifici del dominio applicativo.\n\nUtilizzo della Memoria\nL’impronta di memoria dei modelli ML, in particolare quelli di notevole complessità e profondità, può essere sostanziale, ponendo quindi una sfida significativa sia nelle fasi di training che di deployment. Ad esempio, una rete neurale profonda con 100 milioni di parametri, rappresentata utilizzando Float32 (32 bit o 4 byte per parametro), richiederebbe circa 400 MB di memoria solo per l’archiviazione dei pesi del modello. Ciò non tiene conto dei requisiti di memoria aggiuntivi durante il training per l’archiviazione di gradienti, stati dell’ottimizzatore e cache di passaggio forward [in avanti], che possono amplificare ulteriormente l’utilizzo della memoria, potenzialmente mettendo a dura prova le risorse su determinati hardware, in particolare dispositivi edge con capacità di memoria limitata.\nLa scelta della rappresentazione numerica ha un impatto ulteriore sull’utilizzo della memoria e sull’efficienza computazionale. Ad esempio, l’utilizzo di Float64 per i pesi del modello raddoppierebbe i requisiti di memoria rispetto a Float32 e potrebbe potenzialmente aumentare anche il tempo di elaborazione. Per una matrice di peso con dimensioni [1000, 1000], Float64 consumerebbe circa 8 MB di memoria, mentre Float32 la ridurrebbe a circa 4 MB. Pertanto, la selezione di un formato numerico appropriato è fondamentale per ottimizzare sia la memoria che l’efficienza computazionale.\n\n\nComplessità Computazionale\nLa precisione numerica ha un impatto diretto sulla complessità computazionale, influenzando il tempo e le risorse necessarie per eseguire operazioni aritmetiche. Ad esempio, le operazioni che utilizzano Float64 generalmente consumano più risorse computazionali rispetto alle loro controparti Float32 o Float16 (vedere Figura 9.14). Nel regno del ML, dove i modelli potrebbero dover elaborare milioni di operazioni (ad esempio, moltiplicazioni e addizioni in operazioni di matrice durante passaggi in forward e backward), anche piccole differenze nella complessità computazionale per operazione possono aggregarsi in un impatto sostanziale sui tempi di training e inferenza. Come mostrato in Figura 9.15, i modelli quantizzati possono essere molte volte più veloci delle loro versioni non-quantizzate.\n\n\n\n\n\n\nFigura 9.14: Utilizzo di energia da parte di operazioni quantizzate. Fonte: Mark Horowitz, Stanford University.\n\n\n\n\n\n\n\n\n\nFigura 9.15: Velocità di tre diversi modelli in forma normale e quantizzata.\n\n\n\nOltre ai tempi di esecuzione puri, c’è anche una preoccupazione per l’efficienza energetica. Non tutti i calcoli numerici sono creati uguali dal punto di vista dell’hardware sottostante. Alcune operazioni numeriche sono più efficienti dal punto di vista energetico di altre. Ad esempio, Figura 9.16 di seguito mostra che l’addizione di interi è molto più efficiente dal punto di vista energetico della moltiplicazione di interi.\n\n\n\n\n\n\nFigura 9.16: Utilizzo di energia da parte di operazioni quantizzate. Fonte: Isscc (2014).\n\n\nIsscc. 2014. «Computing’s energy problem (and what we can do about it)». https://ieeexplore.ieee.org/document/6757323.\n\n\n\n\nCompatibilità Hardware\nGarantire la compatibilità e le prestazioni ottimizzate su diverse piattaforme hardware è un’altra sfida nella rappresentazione numerica. Hardware diversi, come CPU, GPU, TPU e FPGA, hanno capacità e ottimizzazioni diverse per gestire diverse precisioni numeriche. Ad esempio, alcune GPU potrebbero essere ottimizzate per i calcoli Float32, mentre altre potrebbero fornire accelerazioni per Float16. Sviluppare e ottimizzare modelli ML in grado di sfruttare le capacità numeriche specifiche di hardware diversi, garantendo al contempo che il modello mantenga la sua accuratezza e robustezza, richiede un’attenta considerazione e potenzialmente ulteriori sforzi di sviluppo e test.\n\n\nCompromessi di Precisione e Accuratezza\nIl compromesso tra precisione numerica e accuratezza del modello è una sfida “sfumata” nella rappresentazione numerica. L’utilizzo di numeri a bassa precisione, come Float16, potrebbe risparmiare memoria e velocizzare i calcoli, ma può anche introdurre problemi come errore di quantizzazione e intervallo numerico ridotto. Ad esempio, addestrare un modello con Float16 potrebbe introdurre problemi nella rappresentazione di valori di gradiente molto piccoli, potenzialmente influenzando la convergenza e la stabilità del processo di addestramento. Inoltre, in alcune applicazioni, come simulazioni scientifiche o calcoli finanziari, in cui l’elevata precisione è fondamentale, l’uso di numeri a bassa precisione potrebbe non essere consentito a causa del rischio di accumulare errori significativi.\n\n\nEsempi di Compromessi\nPer comprendere e apprezzare le sfumature, prendiamo in considerazione alcuni esempi di casi d’uso. Attraverso questi, ci renderemo conto che la scelta della rappresentazione numerica non è semplicemente una decisione tecnica, ma strategica, che influenza l’acume predittivo del modello, le sue esigenze computazionali e la sua implementabilità in diversi ambienti computazionali. In questa sezione esamineremo un paio di esempi per comprendere meglio i compromessi con i numeri e come si collegano al mondo reale.\n\nVeicoli Autonomi\nNel dominio dei veicoli autonomi, i modelli ML vengono impiegati per interpretare i dati dei sensori e prendere decisioni in tempo reale. I modelli devono elaborare dati ad alta dimensionalità da vari sensori (ad esempio, LiDAR, telecamere, radar) ed eseguire numerosi calcoli entro un intervallo di tempo limitato per garantire un funzionamento sicuro e reattivo del veicolo. Quindi i compromessi qui includerebbero:\n\nUtilizzo della Memoria: L’archiviazione e l’elaborazione di dati dei sensori ad alta risoluzione, specialmente in formati a virgola mobile, possono consumare una quantità di memoria sostanziale.\nComplessità Computazionale: L’elaborazione in tempo reale richiede calcoli efficienti, in cui numeri di precisione più elevata potrebbero impedire l’esecuzione tempestiva delle azioni di controllo.\n\n\n\nApplicazioni Sanitarie Mobili\nLe applicazioni sanitarie mobili spesso utilizzano modelli ML per attività come il riconoscimento delle attività, il monitoraggio della salute o l’analisi predittiva, operando nell’ambiente con risorse limitate dei dispositivi mobili. I compromessi in questo caso includerebbero:\n\nCompromessi di Precisione e Accuratezza: L’impiego di numeri a bassa precisione per conservare risorse potrebbe influire sull’accuratezza delle previsioni sanitarie o delle rilevazioni di anomalie, il che potrebbe avere implicazioni significative per la salute e la sicurezza degli utenti.\nCompatibilità Hardware: I modelli devono essere ottimizzati per diversi hardware mobili, garantendo un funzionamento efficiente su un’ampia gamma di dispositivi con diverse capacità di calcolo numerico.\n\n\n\nSistemi di Trading ad Alta Frequenza (HFT)\nI sistemi HFT sfruttano i modelli ML per prendere decisioni di trading rapide basate su dati di mercato in tempo reale. Questi sistemi richiedono risposte a bassissima latenza per capitalizzare le opportunità di trading di breve durata.\n\nComplessità Computazionale: I modelli devono elaborare e analizzare vasti flussi di dati di mercato con una latenza minima, dove anche lievi ritardi, potenzialmente introdotti da numeri a precisione più elevata, possono comportare opportunità perse.\nCompromessi di Precisione e Accuratezza: I calcoli finanziari spesso richiedono un’elevata precisione numerica per garantire valutazioni accurate dei prezzi e dei rischi, ponendo sfide nel bilanciamento tra efficienza computazionale e accuratezza numerica.\n\n\n\nSistemi di Sorveglianza Basati su Edge\nI sistemi di sorveglianza distribuiti su dispositivi edge, come le telecamere di sicurezza, utilizzano modelli ML per attività come rilevamento di oggetti, riconoscimento di attività e rilevamento di anomalie, spesso operando con vincoli di risorse rigorosi.\n\nUtilizzo della Memoria: L’archiviazione di modelli pre-addestrati e l’elaborazione di feed video in tempo reale richiedono un utilizzo efficiente della memoria, il che può essere impegnativo con numeri ad alta precisione.\nCompatibilità Hardware: Garantire che i modelli possano funzionare in modo efficiente su dispositivi edge con diverse capacità hardware e ottimizzazioni per diverse precisioni numeriche è fondamentale per una distribuzione diffusa.\n\n\n\nSimulazioni Scientifiche\nI modelli ML vengono sempre più utilizzati nelle simulazioni scientifiche, come la modellazione climatica o le simulazioni di dinamica molecolare, per migliorare le capacità predittive e ridurre le richieste di calcolo.\n\nCompromessi di Precisione e Accuratezza: Le simulazioni scientifiche spesso richiedono un’elevata precisione numerica per garantire risultati accurati e affidabili, il che può entrare in conflitto con il desiderio di ridurre le richieste di calcolo tramite numeri a bassa precisione.\nComplessità Computazionale: I modelli devono gestire ed elaborare dati di simulazione complessi e ad alta dimensionalità in modo efficiente per garantire risultati tempestivi e consentire simulazioni su larga scala o di lunga durata.\n\nQuesti esempi illustrano diversi scenari in cui le sfide della rappresentazione numerica nei modelli ML sono palesemente manifestate. Ogni sistema presenta un set unico di requisiti e vincoli, che richiedono strategie e soluzioni personalizzate per affrontare i problemi dell’utilizzo della memoria, della complessità computazionale, dei compromessi tra precisione e accuratezza e della compatibilità hardware.\n\n\n\n\n9.3.4 Quantizzazione\nLa quantizzazione è prevalente in vari domini scientifici e tecnologici e comporta essenzialmente la mappatura o la limitazione di un set o intervallo continuo in una controparte discreta per ridurre al minimo il numero di bit richiesti.\n\nAnalisi Iniziale\nIniziamo la nostra incursione nella quantizzazione con una breve analisi di un importante utilizzo della quantizzazione.\nNel signal processing [elaborazione del segnale], l’onda sinusoidale continua (mostrata in Figura 9.17) può essere quantizzata in valori discreti tramite un processo noto come campionamento. Questo è un concetto fondamentale nell’elaborazione del segnale digitale ed è cruciale per convertire segnali analogici (come l’onda sinusoidale continua) in una forma digitale che possa essere elaborata dai computer. L’onda sinusoidale è un esempio prevalente grazie alla sua natura periodica e regolare, il che la rende uno strumento utile per spiegare concetti come frequenza, ampiezza, fase e, naturalmente, quantizzazione.\n\n\n\n\n\n\nFigura 9.17: Onda Sinusoidale.\n\n\n\nNella versione quantizzata mostrata in Figura 9.18, l’onda sinusoidale continua (Figura 9.17) viene campionata a intervalli regolari (in questo caso, ogni \\(\\frac{\\pi}{4}\\) radianti) e solo questi valori campionati vengono rappresentati nella versione digitale del segnale. Le linee graduali tra i punti mostrano un modo per rappresentare il segnale quantizzato in una forma costante a tratti. Questo è un esempio semplificato di come funziona la conversione analogico-digitale, in cui un segnale continuo viene mappato su un set discreto di valori, consentendone la rappresentazione e l’elaborazione digitale.\n\n\n\n\n\n\nFigura 9.18: Onda Sinusoidale Quantizzata.\n\n\n\nTornando al contesto del Machine Learning (ML), la quantizzazione si riferisce al processo di limitazione dei possibili valori che i parametri numerici (come pesi e bias) possono assumere in un set discreto, riducendo così la precisione dei parametri e, di conseguenza, l’ingombro di memoria del modello. Se implementata correttamente, la quantizzazione può ridurre le dimensioni del modello fino a 4 volte e migliorare la latenza e la produttività dell’inferenza fino a 2-3 volte. Figura 9.19 illustra l’impatto che la quantizzazione ha sulle dimensioni di modelli diversi: ad esempio, un modello di classificazione delle immagini come ResNet-v2 può essere compresso da 180 MB a 45 MB con quantizzazione a 8 bit. In genere, la perdita di accuratezza del modello è inferiore all’1% con una quantizzazione ben fatta. L’accuratezza può spesso essere recuperata riaddestrando il modello quantizzato con tecniche di addestramento consapevoli della quantizzazione. Pertanto, questa tecnica è emersa come molto importante nell’implementazione di modelli ML in ambienti con risorse limitate, come dispositivi mobili, dispositivi IoT e piattaforme di edge computing, dove le risorse computazionali (memoria e potenza di elaborazione) sono limitate.\n\n\n\n\n\n\nFigura 9.19: Effetto della quantizzazione sulle dimensioni del modello. Fonte: HarvardX.\n\n\n\nEsistono diverse dimensioni della quantizzazione, come uniformità, stocasticità (o determinismo), simmetria, granularità (tra layer/canali/gruppi o persino all’interno dei canali), considerazioni sulla calibrazione dell’intervallo (statico o dinamico) e metodi di messa a punto (QAT, PTQ, ZSQ). Esaminiamo questi di seguito.\n\n\n\n9.3.5 I Tipi\n\nQuantizzazione Uniforme\nLa quantizzazione uniforme implica la mappatura di valori continui o ad alta precisione su una rappresentazione a precisione inferiore utilizzando una scala uniforme. Ciò significa che l’intervallo tra ogni possibile valore quantizzato è coerente. Ad esempio, se i pesi di un layer di rete neurale sono quantizzati su numeri interi a 8 bit (valori tra 0 e 255), un peso con un valore in virgola mobile di 0.56 potrebbe essere mappato su un valore intero di 143, presupponendo una mappatura lineare tra le scale originale e quantizzata. Grazie all’uso di pipeline matematiche intere o a virgola fissa, questa forma di quantizzazione consente il calcolo sul dominio quantizzato senza la necessità di dequantizzare in anticipo.\nIl processo per implementare la quantizzazione uniforme inizia con la scelta di un intervallo di numeri reali da quantizzare. Il passaggio successivo consiste nel selezionare una funzione di quantizzazione e mappare i valori reali sugli interi rappresentabili dalla larghezza di bit della rappresentazione quantizzata. Ad esempio, una scelta popolare per una funzione di quantizzazione è:\n\\[\nQ(r)=Int(r/S) - Z\n\\]\ndove \\(Q\\) è l’operatore di quantizzazione, \\(r\\) è un input a valore reale (nel nostro caso, un’attivazione o un peso), \\(S\\) è un fattore di scala a valore reale e \\(Z\\) è un punto zero intero. La funzione Int mappa un valore reale in un valore intero tramite un’operazione di arrotondamento. Tramite questa funzione, abbiamo mappato in modo efficace i valori reali \\(r\\) in alcuni valori interi, ottenendo livelli quantizzati uniformemente distanziati.\nQuando i professionisti hanno la necessità di recuperare i valori originali di precisione più elevata, i valori reali \\(r\\) possono essere recuperati dai valori quantizzati tramite un’operazione nota come dequantizzazione. Nell’esempio sopra, ciò significherebbe eseguire la seguente operazione sul nostro valore quantizzato:\n\\[\n\\bar{r} = S(Q(r) + Z)\n\\]\nCome discusso, una certa precisione nel valore reale viene persa dalla quantizzazione. In questo caso, il valore recuperato \\(\\bar{r}\\) non corrisponderà esattamente a \\(r\\) a causa dell’operazione di arrotondamento. Questo è un importante compromesso da notare; tuttavia, in molti utilizzi riusciti della quantizzazione, la perdita di precisione può essere trascurabile e l’accuratezza del test rimane elevata. Nonostante ciò, la quantizzazione uniforme continua a essere la scelta di fatto attuale per la sua semplicità e l’efficiente mappatura all’hardware.\n\n\nQuantizzazione Non-Uniforme\nLa quantizzazione non uniforme, d’altro canto, non mantiene un intervallo coerente tra i valori quantizzati. Questo approccio potrebbe essere utilizzato per allocare più possibili valori discreti in regioni in cui i valori dei parametri sono più densamente popolati, preservando così maggiori dettagli dove sono più necessari. Ad esempio, nelle distribuzioni a campana di pesi con lunghe code, un set di pesi in un modello si trova prevalentemente all’interno di un certo intervallo; quindi, più livelli di quantizzazione potrebbero essere assegnati a tale intervallo per preservare dettagli più fini, consentendoci di acquisire meglio le informazioni. Tuttavia, una delle principali debolezze della quantizzazione non uniforme è che richiede la dequantizzazione prima di calcoli di precisione più elevata a causa della sua non uniformità, limitando la sua capacità di accelerare il calcolo rispetto alla quantizzazione uniforme.\nIn genere, una quantizzazione non uniforme basata su regole utilizza una distribuzione logaritmica di passaggi e livelli esponenzialmente crescenti anziché linearmente. Un altra tipologia popolare risiede nella quantizzazione basata su codice binario in cui i vettori di numeri reali vengono quantizzati in vettori binari con un fattore di scala. In particolare, non esiste una soluzione in forma chiusa per minimizzare gli errori tra il valore reale e il valore non uniformemente quantizzato, quindi la maggior parte delle quantizzazioni in questo campo si basa su soluzioni euristiche. Ad esempio, un lavoro recente di Xu et al. (2018) formula la quantizzazione non uniforme come un problema di ottimizzazione in cui i passaggi/livelli di quantizzazione nel quantizzatore \\(Q\\) vengono regolati per ridurre al minimo la differenza tra il tensore originale e la controparte quantizzata.\n\nXu, Chen, Jianqiang Yao, Zhouchen Lin, Wenwu Ou, Yuanbin Cao, Zhirong Wang, e Hongbin Zha. 2018. «Alternating Multi-bit Quantization for Recurrent Neural Networks». In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net. https://openreview.net/forum?id=S19dR9x0b.\n\\[\n\\min_Q ||Q(r)-r||^2\n\\]\nInoltre, i quantizzatori addestrabili lo possono essere congiuntamente con parametri di modello e i passaggi/livelli di quantizzazione sono generalmente addestrati con ottimizzazione iterativa o discesa del gradiente. Inoltre, il clustering è stato utilizzato per alleviare la perdita di informazioni dalla quantizzazione. Sebbene in grado di catturare livelli di dettaglio più elevati, gli schemi di quantizzazione non uniformi possono essere difficili da implementare in modo efficiente su hardware di calcolo generale, rendendoli meno preferiti ai metodi che utilizzano la quantizzazione uniforme.\n\n\n\n\n\n\nFigura 9.20: Uniformità della Quantizzazione. Fonte: Gholami et al. (2021).\n\n\n\n\n\nQuantizzazione Stocastica\nA differenza dei due approcci precedenti che generano mappature deterministiche, c’è un po’ di lavoro che esplora l’idea della quantizzazione stocastica per l’addestramento consapevole della quantizzazione e l’addestramento a precisione ridotta. Questo approccio mappa numeri fluttuanti verso l’alto o verso il basso con una probabilità associata alla grandezza dell’aggiornamento del peso. La speranza generata dall’intuizione di alto livello è che un tale approccio probabilistico possa consentire a una rete neurale di esplorare di più, rispetto alla quantizzazione deterministica. Presumibilmente, abilitare un arrotondamento stocastico potrebbe consentire alle reti neurali di sfuggire agli ottimi locali, aggiornando così i propri parametri. Di seguito sono riportati due esempi di funzioni di mappatura stocastica:\n\n\n\n\n\n\n\nFigura 9.21: Funzioni di quantizzazione Intera e Binaria.\n\n\n\n\n\nQuantizzazione “Zero Shot”\nLa quantizzazione Zero-shot si riferisce al processo di conversione di un modello di deep learning a precisione completa direttamente in un modello quantizzato a bassa precisione senza la necessità di alcun riaddestramento o messa a punto sul modello quantizzato. Il vantaggio principale di questo approccio è la sua efficienza, in quanto elimina il processo, spesso dispendioso in termini di tempo e risorse, del riaddestramento post-quantizzazione. Sfruttando tecniche che anticipano e riducono al minimo gli errori di quantizzazione, la quantizzazione zero-shot mantiene l’accuratezza originale del modello anche dopo averne ridotto la precisione numerica. È particolarmente utile per i provider di “Machine Learning as a Service (MLaaS)” che mirano ad accelerare la distribuzione dei carichi di lavoro dei propri clienti senza dover accedere ai loro set di dati.\n\n\n\n9.3.6 Calibrazione\nLa calibrazione è il processo di selezione dell’intervallo di clipping [ritaglio] più efficace [\\(\\alpha\\), \\(\\beta\\)] per pesi e attivazioni da quantizzare. Ad esempio, si consideri la quantizzazione delle attivazioni che originariamente hanno un intervallo in virgola mobile tra -6 e 6 a interi a 8 bit. Prendere solo i valori minimi e massimi possibili di interi a 8 bit (da -128 a 127) come intervallo di quantizzazione, potrebbe non essere il più efficace. Invece, la calibrazione implicherebbe il passaggio di un set di dati rappresentativo e quindi l’utilizzo di questo intervallo osservato per la quantizzazione.\nEsistono molti metodi di calibrazione, ma alcuni comunemente utilizzati includono:\n\nMax: Utilizza il valore assoluto massimo visualizzato durante la calibrazione. Tuttavia, questo metodo è suscettibile di dati anomali. Notare come in Figura 9.22, abbiamo un cluster anomalo intorno a 2.1, mentre il resto è raggruppato attorno a valori più piccoli.\nEntropia: Utilizza la divergenza KL per ridurre al minimo la perdita di informazioni tra i valori originali in virgola mobile e i valori che potrebbero essere rappresentati dal formato quantizzato. Questo è il metodo predefinito utilizzato da TensorRT.\nPercentile: Imposta l’intervallo su un percentile della distribuzione dei valori assoluti osservati durante la calibrazione. Ad esempio, una calibrazione del 99% taglierebbe l’1% dei valori di magnitudine più grandi.\n\n\n\n\n\n\n\nFigura 9.22: Attivazioni di input nel layer 3 in ResNet50. Fonte: @Wu, Judd, e Isaev (2020).\n\n\n\nÈ importante notare che la qualità della calibrazione può fare la differenza tra un modello quantizzato che conserva la maggior parte della sua accuratezza e uno che si degrada in modo significativo. Quindi, è un passaggio essenziale nel processo di quantizzazione. Quando si sceglie un intervallo di calibrazione, ci sono due tipi: simmetrico e asimmetrico.\n\nQuantizzazione Simmetrica\nLa quantizzazione simmetrica mappa i valori reali su un intervallo di clipping simmetrico centrato su 0. Ciò comporta la scelta di un intervallo [\\(\\alpha\\), \\(\\beta\\)] dove \\(\\alpha = -\\beta\\). Ad esempio, un intervallo simmetrico si baserebbe sui valori min/max dei valori reali in modo tale che:\n\\[\n\\alpha = \\beta = max(abs(r_{max}), abs(r_{min}))\n\\]\nGli intervalli di clipping simmetrici sono i più ampiamente adottati nella pratica in quanto hanno il vantaggio di un’implementazione più semplice. In particolare, la mappatura da zero a zero nell’intervallo di clipping (talvolta chiamata “azzeramento del punto zero”) può portare a una riduzione del costo computazionale durante l’inferenza (Wu, Judd, e Isaev 2020).\n\n\nQuantizzazione Asimmetrica\nLa quantizzazione asimmetrica mappa i valori reali in un intervallo di clipping asimmetrico che non è necessariamente centrato sullo 0, come mostrato in Figura 9.23 a destra. Comporta la scelta di un intervallo [\\(\\alpha\\), \\(\\beta\\)] dove \\(\\alpha \\neq -\\beta\\). Ad esempio, selezionando un intervallo basato sui valori reali minimi e massimi, o dove \\(\\alpha = r_{min}\\) and \\(\\beta = r_{max}\\), si crea un intervallo asimmetrico. In genere, la quantizzazione asimmetrica produce intervalli di clipping più stretti rispetto a quella simmetrica, il che è importante quando i pesi e le attivazioni target sono sbilanciati, ad esempio, l’attivazione dopo la ReLU ha sempre valori non negativi. Nonostante produca intervalli di clipping più stretti, la quantizzazione asimmetrica è meno preferita di quella simmetrica in quanto non azzera sempre il valore dello zero reale.\n\n\n\n\n\n\nFigura 9.23: (a)simmetria della Quantizzazione. Fonte: Gholami et al. (2021).\n\n\n\n\n\nGranularità\nDopo aver deciso il tipo di intervallo di clipping, è essenziale restringerlo per consentire a un modello di mantenere la massima accuratezza possibile. Daremo un’occhiata alle reti neurali convoluzionali come nostro modo di esplorare metodi che ottimizzano la granularità degli intervalli di clipping per la quantizzazione. L’attivazione di input di un layer nella nostra CNN subisce una convoluzione con più filtri convoluzionali. Ogni filtro convoluzionale può possedere un intervallo di valori univoco. Si noti come in Figura 9.24 l’intervallo per il Filtro 1 sia molto più piccolo di quello per il Filtro 3. Di conseguenza, una caratteristica distintiva degli approcci di quantizzazione è la precisione con cui l’intervallo di clipping [α,β] viene determinato per i pesi.\n\n\n\n\n\n\nFigura 9.24: Granularità di quantizzazione: intervalli variabili. Fonte: Gholami et al. (2021).\n\n\n\n\nQuantizzazione a Layer: Questo approccio determina l’intervallo di clipping considerando tutti i pesi nei filtri convoluzionali di un layer. Quindi, lo stesso intervallo di clipping viene utilizzato per tutti i filtri convoluzionali. È il più semplice da implementare e, come tale, spesso si traduce in una precisione non ottimale a causa dell’ampia varietà di intervalli diversi tra i filtri. Ad esempio, un kernel convoluzionale con un intervallo di parametri più ristretto perde la sua risoluzione di quantizzazione a causa di un altro kernel nello stesso layer che ha un intervallo più ampio.\nGroupwise Quantization: Questo approccio raggruppa diversi canali all’interno di un layer per calcolare l’intervallo di clipping. Questo metodo può essere utile quando la distribuzione dei parametri su una singola convoluzione/attivazione varia molto. In pratica, questo metodo è stato utile in Q-BERT (Shen et al. 2020) per quantizzare i modelli Transformer (Vaswani et al. 2017) costituiti da layer di attenzione completamente connessi. Lo svantaggio di questo approccio è il costo aggiuntivo di contabilizzazione di diversi fattori di scala.\nChannelwise Quantization: Questo metodo popolare utilizza un intervallo fisso per ogni filtro convoluzionale che è indipendente dagli altri canali. Poiché a ogni canale viene assegnato un fattore di scala dedicato, questo metodo garantisce una risoluzione di quantizzazione più elevata e spesso si traduce in una maggiore accuratezza.\nSub-channelwise Quantization: Portando la quantizzazione canale per canale all’estremo, questo metodo determina l’intervallo di clipping rispetto a qualsiasi gruppo di parametri in una convoluzione o in un layer completamente connesso. Potrebbe comportare un overhead considerevole poiché è necessario tenere conto di diversi fattori di scala quando si elabora una singola convoluzione o un layer completamente connesso.\n\n\nShen, Sheng, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami, Michael W. Mahoney, e Kurt Keutzer. 2020. «Q-BERT: Hessian Based Ultra Low Precision Quantization of BERT». Proceedings of the AAAI Conference on Artificial Intelligence 34 (05): 8815–21. https://doi.org/10.1609/aaai.v34i05.6409.\n\nVaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, e Illia Polosukhin. 2017. «Attention is all you need». Adv Neural Inf Process Syst 30.\nTra questi, la quantizzazione canale per canale è lo standard corrente utilizzato per quantizzare i kernel convoluzionali, poiché consente la regolazione degli intervalli di clipping per ogni singolo kernel con overhead trascurabile.\n\n\nQuantizzazione Statica e Dinamica\nDopo aver determinato il tipo e la granularità dell’intervallo di clipping, gli esperti devono decidere quando gli intervalli vengono determinati nei loro algoritmi di calibrazione dell’intervallo. Esistono due approcci per quantizzare le attivazioni: quantizzazione statica e quella dinamica.\nLa quantizzazione statica è l’approccio più frequentemente utilizzato. In questo, l’intervallo di clipping è precalcolato e statico durante l’inferenza. Non aggiunge alcun sovraccarico computazionale, ma, di conseguenza, comporta una minore accuratezza rispetto alla quantizzazione dinamica. Un metodo popolare per implementarlo è eseguire una serie di input di calibrazione per calcolare l’intervallo tipico di attivazioni (Jacob et al. 2018; Yao et al. 2021).\n\nJacob, Benoit, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard, Hartwig Adam, e Dmitry Kalenichenko. 2018. «Quantization and training of neural networks for efficient integer-arithmetic-only inference». In Proceedings of the IEEE conference on computer vision and pattern recognition, 2704–13.\n\nYao, Zhewei, Zhen Dong, Zhangcheng Zheng, Amir Gholami, Jiali Yu, Eric Tan, Leyuan Wang, et al. 2021. «Hawq-v3: Dyadic neural network quantization». In International Conference on Machine Learning, 11875–86. PMLR.\nLa quantizzazione dinamica è un approccio alternativo che calcola dinamicamente l’intervallo per ogni mappa di attivazione durante il runtime. L’approccio richiede calcoli in tempo reale che potrebbero avere un sovraccarico molto elevato. In questo modo, la quantizzazione dinamica spesso raggiunge la massima accuratezza poiché l’intervallo viene calcolato specificamente per ogni input.\nTra i due, il calcolo dell’intervallo in modo dinamico è solitamente molto costoso, quindi la maggior parte dei professionisti utilizzerà spesso la quantizzazione statica.\n\n\n\n9.3.7 Tecniche\nQuando si ottimizzano i modelli di apprendimento automatico per l’implementazione, vengono utilizzate varie tecniche di quantizzazione per bilanciare efficienza, accuratezza e adattabilità del modello. Ogni metodo (quantizzazione post-addestramento, addestramento consapevole della quantizzazione e quantizzazione dinamica) offre vantaggi e compromessi unici, che incidono su fattori quali complessità di implementazione, sovraccarico computazionale e ottimizzazione delle prestazioni.\nTabella 9.2 fornisce una panoramica di questi metodi di quantizzazione, evidenziandone i rispettivi punti di forza, limiti e compromessi. Ci addentreremo più a fondo in ciascuno di questi metodi perché sono ampiamente distribuiti e utilizzati in tutti i sistemi ML di scale molto diverse.\n\n\n\nTabella 9.2: Confronto tra quantizzazione post-training, training sensibile alla quantizzazione e quantizzazione dinamica.\n\n\n\n\n\n\n\n\n\n\n\nAspetto\nQuantizzazione Post Training\nTraining Quantization-Aware\nQuantization Dinamica\n\n\n\n\nPro\n\n\n\n\n\nSemplicità\n✓\n✗\n✗\n\n\nPreservazione della Precisione\n✗\n✓\n✓\n\n\nAdattabilità\n✗\n✗\n✓\n\n\nPrestazioni Ottimizzate\n✗\n✓\nPotentialmente\n\n\nContro\n\n\n\n\n\nDegrado della Precisione\n✓\n✗\nPotentialmente\n\n\nOverhead Computazionale\n✗\n✓\n✓\n\n\nComplessità di Implementazione\n✗\n✓\n✓\n\n\nCompromessi\n\n\n\n\n\nVelocità vs. Precisione\n✓\n✗\n✗\n\n\nPrecisione vs. Costo\n✗\n✓\n✗\n\n\nAdaptability vs. Overhead\n✗\n✗\n✓\n\n\n\n\n\n\nPost Training Quantization: La quantizzazione post-addestramento (PTQ) è una tecnica di quantizzazione in cui il modello viene quantizzato dopo essere stato addestrato. Il modello viene addestrato in virgola mobile e poi i pesi e le attivazioni vengono quantizzati come fase di post-elaborazione. Questo è l’approccio più semplice e non richiede l’accesso ai dati di addestramento. Diversamente la “Quantization-Aware Training (QAT), PTQ” imposta direttamente i parametri di quantizzazione del peso e dell’attivazione, rendendolo poco costoso e adatto a situazioni con dati limitati o non etichettati. Tuttavia, non riaggiustare i pesi dopo la quantizzazione, specialmente nella quantizzazione a bassa precisione, può portare a un comportamento molto diverso e quindi a una minore accuratezza. Per affrontare questo problema, sono state sviluppate tecniche come la correzione della distorsione, l’equalizzazione degli intervalli di peso e i metodi di arrotondamento adattivo. PTQ può essere applicato anche in scenari zero-shot, in cui non sono disponibili dati di addestramento o di test. Questo metodo è stato reso ancora più efficiente per avvantaggiare modelli linguistici di grandi dimensioni che richiedono molta elaborazione e memoria. Di recente, è stata sviluppata SmoothQuant, una soluzione PTQ senza training, che preserva l’accuratezza ed è di uso generale che consente la quantizzazione di peso a 8 bit e attivazione a 8 bit per LLM, dimostrando un’accelerazione fino a 1.56x e una riduzione della memoria di 2x per LLM con una perdita trascurabile di accuratezza (Xiao et al. 2022).\nIn PTQ, un modello pre-addestrato subisce un processo di calibrazione, come mostrato in Figura 9.25. La calibrazione comporta l’utilizzo di un set di dati separato noto come dati di calibrazione, un sottoinsieme specifico dei dati di training riservato alla quantizzazione per aiutare a trovare gli intervalli di clipping e i fattori di scala appropriati.\n\n\n\n\n\n\nFigura 9.25: Quantizzazione e Calibrazione Post-Training. Fonte: Gholami et al. (2021).\n\n\n\nQuantization-Aware Training: L’addestramento consapevole della quantizzazione (QAT) è una messa a punto del modello PTQ. Il modello viene addestrato in modo consapevole della quantizzazione, consentendogli di adattarsi agli effetti della quantizzazione. Ciò produce una migliore accuratezza con l’inferenza quantizzata. La quantizzazione di un modello di rete neurale addestrato con metodi come PTQ introduce perturbazioni che possono deviare il modello dal suo punto di convergenza originale. Ad esempio, Krishnamoorthi ha dimostrato che anche con la quantizzazione per canale, reti come MobileNet non raggiungono la precisione di base con int8 PTQ e richiedono QAT (Krishnamoorthi 2018). Per risolvere questo problema, QAT riqualifica il modello con parametri quantizzati, utilizzando passaggi forward [avanti] e backward [indietro] in virgola mobile ma quantizzando i parametri dopo ogni aggiornamento del gradiente. La gestione dell’operatore di quantizzazione non differenziabile è fondamentale; un metodo ampiamente utilizzato è lo “Straight Through Estimator (STE)”, che approssima l’operazione di arrotondamento come una funzione identità. Sebbene esistano altri metodi e varianti, STE rimane il più comunemente utilizzato per la sua efficacia pratica. In QAT, un modello pre-addestrato viene quantizzato e poi messo a punto utilizzando i dati di addestramento per regolare i parametri e recuperare il degrado della precisione, come mostrato in Figura 9.26. Il processo di calibrazione viene spesso condotto parallelamente al processo di messa a punto per QAT.\n\n\n\n\n\n\nFigura 9.26: Quantization-Aware Training. Fonte: Gholami et al. (2021).\n\n\nGholami, Dong Kim, Mahoney Yao, e Keutzer. 2021. «A Survey of Quantization Methods for Efficient Neural Network Inference)». ArXiv preprint. https://arxiv.org/abs/2103.13630.\n\n\nLa “Quantization-Aware Training” funge da estensione naturale della “Post-Training Quantization”. Dopo la quantizzazione iniziale eseguita da PTQ, QAT viene utilizzata per perfezionare e mettere a punto ulteriormente i parametri quantizzati: vedere come in Figura 9.27, il modello PTQ subisce un ulteriore passaggio, QAT. Comporta un processo di riqualificazione in cui il modello viene esposto a ulteriori iterazioni di training utilizzando i dati originali. Questo approccio di training dinamico consente al modello di adattare e regolare i suoi parametri, compensando il degrado delle prestazioni causato dalla quantizzazione.\n\n\n\n\n\n\nFigura 9.27: PTQ e QAT. Fonte: «The Ultimate Guide to Deep Learning Model Quantization and Quantization-Aware Training» (s.d.).\n\n\n«The Ultimate Guide to Deep Learning Model Quantization and Quantization-Aware Training». s.d. https://deci.ai/quantization-and-quantization-aware-training/.\n\n\nFigura 9.28 mostra l’accuratezza relativa di diversi modelli dopo PTQ e QAT. In quasi tutti i casi, QAT produce un’accuratezza migliore di PTQ. Si consideri ad esempio EfficientNet b0. Dopo PTQ, l’accuratezza scende dal 76.85% a 72.06%. Ma quando applichiamo QAT, l’accuratezza rimbalza al 76.95% (con persino un leggero miglioramento rispetto all’accuratezza originale).\n\n\n\n\n\n\nFigura 9.28: Accuratezza relativa di PTQ e QAT. Fonte: Wu, Judd, e Isaev (2020).\n\n\n\n\n\n9.3.8 Pesi vs. Attivazioni\nQuantizzazione del peso: Comporta la conversione dei pesi continui o ad alta precisione di un modello in pesi a bassa precisione, come la conversione dei pesi Float32 in pesi INT8 (interi) quantizzati - in Figura 9.29, la quantizzazione del peso avviene nel secondo passaggio (quadrati rossi) quando moltiplichiamo gli input. Ciò riduce le dimensioni del modello, riducendo così la memoria richiesta per archiviare il modello e le risorse computazionali necessarie per eseguire l’inferenza. Ad esempio, si consideri una matrice di pesi in un layer di rete neurale con pesi Float32 come [0.215, -1.432, 0.902, …]. Attraverso la quantizzazione del peso, questi potrebbero essere mappati su valori INT8 come [27, -183, 115, …], riducendo significativamente la memoria richiesta per memorizzarli.\n\n\n\n\n\n\nFigura 9.29: Quantizzazione del peso e dell’attivazione. Fonte: HarvardX.\n\n\n\nQuantizzazione dell’Attivazione: Comporta la quantizzazione dei valori di attivazione (output dei livelli) durante l’inferenza del modello. Ciò può ridurre le risorse computazionali richieste durante l’inferenza, ma introduce ulteriori problemi nel mantenimento dell’accuratezza del modello a causa della ridotta precisione dei calcoli intermedi. Ad esempio, in una rete neurale convoluzionale (CNN), le mappe di attivazione (mappe delle feature) prodotte dai layer convoluzionali, originariamente in Float32, potrebbero essere quantizzate su INT8 durante l’inferenza per accelerare il calcolo, in particolare su hardware ottimizzato per l’aritmetica degli interi. Inoltre, un lavoro recente ha esplorato l’uso della quantizzazione del “Activation-aware Weight Quantization” per la compressione e l’accelerazione LLM, che comporta la protezione di solo l’1% dei pesi salienti più importanti osservando le attivazioni, non i pesi (Lin et al. 2023).\n\n\n9.3.9 Compromessi\nLa quantizzazione introduce invariabilmente un compromesso tra dimensioni/prestazioni del modello e accuratezza. Sebbene riduca significativamente l’ingombro della memoria e possa accelerare l’inferenza, specialmente su hardware ottimizzato per aritmetica a bassa precisione, la precisione ridotta può degradare l’accuratezza del modello.\nDimensioni del Modello: Un modello con pesi rappresentati come Float32 quantizzato a INT8 può teoricamente ridurre le dimensioni del modello di un fattore 4, consentendone l’implementazione su dispositivi con memoria limitata. Le dimensioni di grandi modelli linguistici si stanno sviluppando a un ritmo più veloce della memoria GPU negli ultimi anni, portando a un grande divario tra domanda e offerta di memoria. Figura 9.30 illustra la recente tendenza del divario crescente tra le dimensioni del modello (linea rossa) e la memoria dell’acceleratore (linea gialla). Le tecniche di quantizzazione e compressione del modello possono aiutare a colmare il divario\n\n\n\n\n\n\nFigura 9.30: Dimensioni del modello vs. memoria dell’acceleratore. Fonte: Xiao et al. (2022).\n\n\nXiao, Seznec Lin, Demouth Wu, e Han. 2022. «SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models». ArXiv preprint. https://arxiv.org/abs/2211.10438.\n\n\nVelocità di Inferenza: La quantizzazione può anche accelerare l’inferenza, poiché l’aritmetica a precisione inferiore è computazionalmente meno costosa. Ad esempio, alcuni acceleratori hardware, come Edge TPU di Google, sono ottimizzati per l’aritmetica INT8 e possono eseguire l’inferenza in modo significativamente più rapido con modelli quantizzati INT8 rispetto alle loro controparti in virgola mobile. La riduzione della memoria dalla quantizzazione aiuta a ridurre la quantità di trasmissione dei dati, risparmiando memoria e velocizzando il processo. Figura 9.31 confronta l’aumento della produttività e la riduzione della memoria della larghezza di banda per diversi tipi di dati sulla NVIDIA Turing GPU.\n\n\n\n\n\n\nFigura 9.31: Vantaggi dei tipi di dati a precisione inferiore. Fonte: Wu, Judd, e Isaev (2020).\n\n\nWu, Zhang Judd, e Micikevicius Isaev. 2020. «Integer Quantization for Deep Learning Inference: Principles and Empirical Evaluation)». ArXiv preprint. https://arxiv.org/abs/2004.09602.\n\n\nPrecisione: La riduzione della precisione numerica post-quantizzazione può portare a un degrado della precisione del modello, che potrebbe essere accettabile in alcune applicazioni (ad esempio, classificazione delle immagini) ma non in altre (ad esempio, diagnosi medica). Pertanto, dopo la quantizzazione, il modello richiede in genere una ricalibrazione o una messa a punto per mitigare la perdita di accuratezza. Inoltre, un lavoro recente ha esplorato l’uso di Activation-aware Weight Quantization (Lin et al. 2023) che si basa sull’osservazione che proteggere solo l’1% dei pesi salienti può ridurre notevolmente l’errore di quantizzazione.\n\n\n9.3.10 Quantizzazione e Potatura\nPruning [potatura] e quantizzazione funzionano bene insieme ed è stato scoperto che il pruning non ostacola la quantizzazione. In effetti, il pruning può aiutare a ridurre l’errore di quantizzazione. Intuitivamente, ciò è dovuto al pruning che riduce il numero di pesi da quantizzare, riducendo così l’errore accumulato dalla quantizzazione. Ad esempio, una AlexNet non potata ha 60 milioni di pesi da quantizzare mentre una AlexNet potata ha solo 6.7 milioni di pesi da quantizzare. Questa significativa riduzione dei pesi aiuta a ridurre l’errore tra la quantizzazione dell’AlexNet non potato rispetto all’AlexNet potato. Inoltre, studi recenti hanno scoperto che il pruning consapevole della quantizzazione genera modelli più efficienti dal punto di vista computazionale rispetto al pruning o alla quantizzazione da soli; in genere, ha prestazioni simili o migliori in termini di efficienza computazionale rispetto ad altre tecniche di ricerca dell’architettura neurale come l’ottimizzazione bayesiana (Hawks et al. 2021).\n\n\n\n\n\n\nFigura 9.32: Precisione rispetto al tasso di compressione con diversi metodi di compressione. Fonte: Han, Mao, e Dally (2015).\n\n\nHan, Song, Huizi Mao, e William J Dally. 2015. «Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding». arXiv preprint arXiv:1510.00149.\n\n\n\n\n9.3.11 Quantizzazione Edge-aware\nLa quantizzazione non solo riduce le dimensioni del modello, ma consente anche calcoli più rapidi e consuma meno energia, rendendola fondamentale per lo sviluppo per edge. I dispositivi edge in genere hanno vincoli di risorse rigidi con elaborazione, memoria e potenza, impossibili da soddisfare per molti dei modelli deep NN profondi odierni. Inoltre, i processori edge non supportano le operazioni in virgola mobile, rendendo la quantizzazione intera particolarmente importante per chip come GAP-8, un SoC RISC-V per l’inferenza edge con un acceleratore CNN dedicato, che supporta solo l’aritmetica intera.\nUna piattaforma hardware che utilizza la quantizzazione è il gruppo ARM Cortex-M di core di processori ARM RISC a 32 bit. Sfruttano la quantizzazione a virgola fissa con fattori di scala di potenza di due, in modo che la quantizzazione e la de-quantizzazione possano essere eseguite in modo efficiente tramite spostamento di bit. Inoltre, Google Edge TPU, la soluzione emergente di Google per l’esecuzione di inferenze in periferia, è progettata per dispositivi piccoli e a bassa potenza e può supportare solo l’aritmetica a 8 bit. Molti modelli di reti neurali complesse che potevano essere distribuiti solo su server a causa delle loro elevate esigenze di elaborazione possono ora essere eseguiti su dispositivi edge grazie ai recenti progressi (ad esempio metodi di quantizzazione) nel campo dell’edge computing.\nOltre a essere una tecnica indispensabile per molti processori edge, la quantizzazione ha anche apportato notevoli miglioramenti ai processori non edge, incoraggiando tali processori a soddisfare i requisiti del Service Level Agreement (SLA) come la latenza del 99° percentile.\nPertanto, la quantizzazione combinata con una logica efficiente a bassa precisione e acceleratori dedicati di deep learning, è stata una forza trainante cruciale per l’evoluzione di tali processori edge.\nVideo 9.1 è una lezione sulla quantizzazione e sui diversi metodi di quantizzazione.\n\n\n\n\n\n\nVideo 9.1: Quantizzazione",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Ottimizzazioni dei Modelli</span>"
    ]
  },
  {
    "objectID": "contents/core/optimizations/optimizations.it.html#sec-model_ops_hw",
    "href": "contents/core/optimizations/optimizations.it.html#sec-model_ops_hw",
    "title": "9  Ottimizzazioni dei Modelli",
    "section": "9.4 Implementazione Hardware Efficiente",
    "text": "9.4 Implementazione Hardware Efficiente\nL’implementazione hardware efficiente trascende la selezione di componenti adatti; richiede una comprensione olistica di come il software interagirà con le architetture sottostanti. L’essenza del raggiungimento delle massime prestazioni nelle applicazioni TinyML non risiede solo nell’affinare gli algoritmi per l’hardware, ma anche nell’assicurare che l’hardware sia strategicamente adattato per supportare questi algoritmi. Questa sinergia tra hardware e software è fondamentale. Mentre esaminiamo più a fondo le complessità dell’implementazione hardware efficiente, il significato di un approccio di progettazione congiunta, in cui hardware e software vengono sviluppati in tandem, diventa sempre più evidente. Questa sezione fornisce una panoramica delle tecniche di come l’hardware e le interazioni tra hardware e software possono essere ottimizzati per migliorare le prestazioni dei modelli.\n\n9.4.1 Ricerca di Architettura Neurale Basata sull’Hardware\nConcentrarsi solo sulla precisione durante l’esecuzione della ricerca di architettura neurale porta a modelli esponenzialmente complessi e che richiedono memoria e capacità di elaborazione crescenti. Ciò ha portato a vincoli hardware che limitano lo sfruttamento dei modelli di apprendimento profondo al loro pieno potenziale. Progettare manualmente l’architettura del modello è ancora più difficile se si considerano la varietà e le limitazioni dell’hardware. Ciò ha portato alla creazione di Hardware-aware Neural Architecture Search che incorpora le contrazioni hardware nella loro ricerca e ottimizza lo spazio di ricerca per un hardware e una precisione specifici. HW-NAS può essere categorizzato in base a come ottimizza per l’hardware. Esploreremo brevemente queste categorie e lasceremo dei link a documenti correlati per il lettore interessato.\n\nConfigurazione Single Target, Fixed Platfrom\nL’obiettivo qui è trovare la migliore architettura in termini di precisione ed efficienza hardware per un hardware target fisso. Per un hardware specifico, ad esempio Arduino Nicla Vision, questa categoria di HW-NAS cercherà l’architettura che ottimizza precisione, latenza, consumo energetico, ecc.\n\nStrategia di Ricerca Hardware-aware\nQui, la ricerca è un problema di ottimizzazione multi-obiettivo, in cui sia l’accuratezza che il costo dell’hardware guidano l’algoritmo di ricerca per trovare l’architettura più efficiente (Tan et al. 2019; Cai, Zhu, e Han 2019; B. Wu et al. 2019).\n\nTan, Mingxing, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew Howard, e Quoc V. Le. 2019. «MnasNet: Platform-aware Neural Architecture Search for Mobile». In 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2820–28. IEEE. https://doi.org/10.1109/cvpr.2019.00293.\n\nCai, Han, Ligeng Zhu, e Song Han. 2019. «ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware». In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net. https://openreview.net/forum?id=HylVB3AqYm.\n\nWu, Bichen, Kurt Keutzer, Xiaoliang Dai, Peizhao Zhang, Yanghan Wang, Fei Sun, Yiming Wu, Yuandong Tian, Peter Vajda, e Yangqing Jia. 2019. «FBNet: Hardware-aware Efficient ConvNet Design via Differentiable Neural Architecture Search». In 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 10734–42. IEEE. https://doi.org/10.1109/cvpr.2019.01099.\n\n\nSpazio di Ricerca Hardware-aware\nQui, lo spazio di ricerca è limitato alle architetture che funzionano bene sull’hardware specifico. Questo può essere ottenuto misurando le prestazioni degli operatori (operatore Conv, operatore Pool, …) o definendo un set di regole che limitano lo spazio di ricerca. (L. L. Zhang et al. 2020)\n\nZhang, Li Lyna, Yuqing Yang, Yuhang Jiang, Wenwu Zhu, e Yunxin Liu. 2020. «Fast Hardware-Aware Neural Architecture Search». In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW). IEEE. https://doi.org/10.1109/cvprw50498.2020.00354.\n\n\n\nConfigurazioni Single Target, Multiple Platform\nAlcuni hardware possono avere configurazioni diverse. Ad esempio, gli FPGA hanno blocchi logici configurabili (CLB) che possono essere configurati dal firmware. Questo metodo consente all’HW-NAS di esplorare diverse configurazioni. (Hu et al. 2023; Ho Yoon et al. 2012)\n\nHo Yoon, Jung, Hyung-Suk Jung, Min Hwan Lee, Gun Hwan Kim, Seul Ji Song, Jun Yeong Seok, Kyung Jean Yoon, et al. 2012. «Frontiers in Electronic Materials». Wiley. https://doi.org/10.1002/9783527667703.ch67.\n\n\nTarget Multipli\nQuesta categoria mira a ottimizzare un singolo modello per più hardware. Questo può essere utile per lo sviluppo di dispositivi mobili in quanto può ottimizzare diversi modelli di telefoni. (Chu et al. 2021; Hu et al. 2023)\n\nChu, Grace, Okan Arikan, Gabriel Bender, Weijun Wang, Achille Brighton, Pieter-Jan Kindermans, Hanxiao Liu, Berkin Akin, Suyog Gupta, e Andrew Howard. 2021. «Discovering Multi-Hardware Mobile Models via Architecture Search». In 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), 3022–31. IEEE. https://doi.org/10.1109/cvprw53098.2021.00337.\n\nHu, Yang, Jie Jiang, Lifu Zhang, Yunfeng Shi, e Jian Shi. 2023. «Halide Perovskite Semiconductors». Wiley. https://doi.org/10.1002/9783527829026.ch13.\n\n\nEsempi di “Hardware-Aware Neural Architecture Search”\n\nTinyNAS\nTinyNAS adotta un approccio in due fasi per trovare un’architettura ottimale per il modello tenendo a mente i vincoli del microcontrollore specifico.\nInnanzitutto, TinyNAS genera più spazi di ricerca variando la risoluzione di input del modello e il numero di canali dei layer. Quindi, TinyNAS sceglie uno spazio di ricerca in base ai FLOP (operazioni in virgola mobile al secondo) di ogni spazio di ricerca. Gli spazi con una probabilità maggiore di contenere architetture con un numero elevato di FLOP producono modelli con maggiore accuratezza: confrontare la linea rossa con la linea nera in Figura 9.33. Poiché un numero maggiore di FLOP significa che il modello ha una maggiore capacità di calcolo, è più probabile che il modello abbia una maggiore accuratezza.\nPoi, TinyNAS esegue un’operazione di ricerca sullo spazio scelto per trovare l’architettura ottimale per i vincoli specifici del microcontrollore. (J. Lin et al. 2020)\n\n\n\n\n\n\nFigura 9.33: Precisione degli spazi di ricerca. Fonte: J. Lin et al. (2020).\n\n\nLin, Ji, Wei-Ming Chen, Yujun Lin, John Cohn, Chuang Gan, e Song Han. 2020. «MCUNet: Tiny Deep Learning on IoT Devices». In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, a cura di Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, e Hsuan-Tien Lin. https://proceedings.neurips.cc/paper/2020/hash/86c51678350f656dcc7f490a43946ee5-Abstract.html.\n\n\n\n\n\nTopology-Aware NAS\nSi concentra sulla creazione e l’ottimizzazione di uno spazio di ricerca allineato alla topologia hardware del dispositivo. (T. Zhang et al. 2020)\n\nZhang, Tunhou, Hsin-Pai Cheng, Zhenwen Li, Feng Yan, Chengyu Huang, Hai Helen Li, e Yiran Chen. 2020. «AutoShrink: A Topology-Aware NAS for Discovering Efficient Neural Architecture». In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, 6829–36. AAAI Press. https://aaai.org/ojs/index.php/AAAI/article/view/6163.\n\n\n\n9.4.2 Sfide nella “Hardware-Aware Neural Architecture Search”\nSebbene HW-NAS abbia un potenziale elevato per trovare architetture ottimali per TinyML, presenta alcuni problemi. Le metriche hardware come latenza, consumo energetico e utilizzo dell’hardware sono più difficili da valutare rispetto alle metriche di accuratezza o di perdita. Spesso richiedono strumenti specializzati per misure precise. Inoltre, l’aggiunta di tutte queste metriche porta a uno spazio di ricerca molto più grande. Ciò fa sì che HW-NAS sia dispendioso in termini di tempo e denaro. Deve essere applicato a ogni hardware per risultati ottimali, tra le altre cose, il che significa che se si deve distribuire il modello su più dispositivi, la ricerca deve essere condotta più volte e produrrà modelli diversi, a meno che non si ottimizzi per tutti, il che significa una minore accuratezza. Infine, l’hardware cambia frequentemente e potrebbe essere necessario eseguire HW-NAS su ogni versione.\n\n\n9.4.3 Ottimizzazioni del Kernel\nLe ottimizzazioni del kernel sono modifiche apportate al kernel per migliorare le prestazioni dei modelli di apprendimento automatico su dispositivi con risorse limitate. Separeremo le ottimizzazioni del kernel in due tipi.\n\nOttimizzazioni del kernel Generali\nQueste sono ottimizzazioni del kernel da cui tutti i dispositivi possono trarre vantaggio. Forniscono tecniche per convertire il codice in istruzioni più efficienti.\n\n“Srotolamento” del Loop\nInvece di avere un loop con “loop control” (incrementando il contatore, si controlla la condizione di terminazione del loop), il loop può essere srotolato e il sovraccarico del “loop control” può essere omesso. Questo può anche fornire ulteriori opportunità di parallelismo che potrebbero non essere possibili con la struttura con loop. Questo può essere particolarmente utile per loop stretti, in cui il corpo del loop è un piccolo numero di istruzioni con molte iterazioni.\n\n\nBlocking\nIl Blocking viene utilizzato per rendere più efficienti i pattern di accesso alla memoria. Se abbiamo tre calcoli, il primo e l’ultimo devono accedere alla cache A e il secondo deve accedere alla cache B, il “blocking” ferma i primi due calcoli per ridurre il numero di letture di memoria necessarie.\n\n\nTiling\nAnalogamente al blocking, il tiling [piastrellatura] divide i dati e il calcolo in blocchi, ma si estende oltre i miglioramenti della cache. Il tiling crea partizioni di calcolo indipendenti che possono essere eseguite in parallelo, il che può comportare significativi miglioramenti delle prestazioni.\n\n\nLibrerie Kernel Ottimizzate\nQuesto comprende lo sviluppo di kernel ottimizzati che sfruttano appieno un hardware specifico. Un esempio è la libreria CMSIS-NN, che è una raccolta di kernel di reti neurali efficienti sviluppati per ottimizzare le prestazioni e ridurre al minimo l’ingombro di memoria dei modelli sui processori Arm Cortex-M, comuni sui dispositivi edge IoT. Il kernel sfrutta più capacità hardware dei processori Cortex-M come Single Instruction Multiple Data (SIMD), Floating Point Unit (FPU) e M-Profile Vector Extensions (MVE). Queste ottimizzazioni rendono più efficienti le operazioni comuni come le moltiplicazioni di matrici, aumentando le prestazioni delle operazioni del modello sui processori Cortex-M. (Lai, Suda, e Chandra 2018)\n\nLai, Liangzhen, Naveen Suda, e Vikas Chandra. 2018. «CMSIS-NN: Efficient Neural Network Kernels for Arm Cortex-M CPUs». https://arxiv.org/abs/1801.06601.\n\n\n\n\n9.4.4 Compute-in-Memory (CiM)\nQuesto è un esempio di progettazione congiunta di algoritmo e hardware. CiM è un paradigma di elaborazione che esegue calcoli all’interno della memoria. Pertanto, le architetture CiM consentono di eseguire operazioni direttamente sui dati archiviati, senza la necessità di spostare i dati avanti e indietro tra unità di elaborazione e memoria separate. Questo paradigma di progettazione è particolarmente utile in scenari in cui lo spostamento dei dati è una fonte primaria di consumo energetico e latenza, come nelle applicazioni TinyML su dispositivi edge. Figura 9.34 è un esempio di utilizzo di CiM in TinyML: l’individuazione delle parole chiave richiede un processo sempre attivo che cerca determinate parole di attivazione (come “Hey, Siri”). Data la natura ad alta intensità di risorse di questa attività, l’integrazione di CiM per il modello di rilevamento delle parole chiave sempre attivo può migliorare l’efficienza.\nAttraverso la progettazione congiunta di algoritmo e hardware, gli algoritmi possono essere ottimizzati per sfruttare le caratteristiche uniche delle architetture CiM e, l’hardware CiM può essere personalizzato o configurato per supportare meglio i requisiti di elaborazione e le caratteristiche degli algoritmi. Ciò si ottiene utilizzando le proprietà analogiche delle celle di memoria, come l’addizione e la moltiplicazione nella DRAM. (Zhou et al. 2021)\n\n\n\n\n\n\nFigura 9.34: CiM per l’individuazione delle parole chiave. Fonte: Zhou et al. (2021).\n\n\nZhou, Chuteng, Fernando Garcia Redondo, Julian Büchel, Irem Boybat, Xavier Timoneda Comas, S. R. Nandakumar, Shidhartha Das, Abu Sebastian, Manuel Le Gallo, e Paul N. Whatmough. 2021. «AnalogNets: Ml-hw Co-Design of Noise-robust TinyML Models and Always-On Analog Compute-in-Memory Accelerator». https://arxiv.org/abs/2111.06503.\n\n\n\n\n9.4.5 Ottimizzazione dell’Accesso alla Memoria\nDispositivi diversi possono avere gerarchie di memorie diverse. L’ottimizzazione per la gerarchia di memoria specifica nell’hardware specifico può portare a grandi miglioramenti delle prestazioni riducendo le costose operazioni di lettura e scrittura nella memoria. L’ottimizzazione del flusso di dati può essere ottenuta ottimizzando il riutilizzo dei dati all’interno di un singolo layer e tra più layer. Questa ottimizzazione del flusso di dati può essere adattata alla gerarchia di memoria specifica dell’hardware, il che può portare a maggiori vantaggi rispetto alle ottimizzazioni generali per diversi hardware.\n\nSfruttamento dei Dati Sparsi\nIl Pruning [potatura] è un approccio fondamentale per comprimere i modelli e renderli compatibili con dispositivi con risorse limitate. Ciò si traduce in modelli sparsi in cui molti pesi sono 0. Pertanto, sfruttare questa diradazione può portare a miglioramenti significativi nelle prestazioni. Sono stati creati degli strumenti per ottenere esattamente questo. RAMAN, è un acceleratore TinyML sparse progettato per l’inferenza su dispositivi edge. RAMAN sovrappone le attivazioni di input e output sullo stesso spazio di memoria, riducendo i requisiti di archiviazione fino al 50%. (Krishna et al. 2023)\n\nKrishna, Adithya, Srikanth Rohit Nudurupati, Chandana D G, Pritesh Dwivedi, André van Schaik, Mahesh Mehendale, e Chetan Singh Thakur. 2023. «RAMAN: A Re-configurable and Sparse TinyML Accelerator for Inference on Edge». https://arxiv.org/abs/2306.06493.\n\n\nFramework di Ottimizzazione\nI framework di ottimizzazione sono stati introdotti per sfruttare le capacità specifiche dell’hardware per accelerare il software. Un esempio di tale framework è hls4ml: Figura 9.35 fornisce una panoramica del flusso di lavoro del framework. Questo flusso di lavoro di co-progettazione software-hardware open source aiuta a interpretare e tradurre algoritmi di machine learning per l’implementazione con tecnologie FPGA e ASIC. Funzionalità quali ottimizzazione di rete, nuove API Python, potatura consapevole della quantizzazione e flussi di lavoro FPGA end-to-end sono integrate nel framework hls4ml, sfruttando unità di elaborazione parallele, gerarchie di memoria e set di istruzioni specializzati per ottimizzare i modelli per hardware edge. Inoltre, hls4ml è in grado di tradurre algoritmi di apprendimento automatico direttamente nel firmware FPGA.\n\n\n\n\n\n\nFigura 9.35: workflow del framework hls4ml. Fonte: Fahim et al. (2021).\n\n\nFahim, Farah, Benjamin Hawks, Christian Herwig, James Hirschauer, Sergo Jindariani, Nhan Tran, Luca P. Carloni, et al. 2021. «hls4ml: An Open-Source Codesign Workflow to Empower Scientific Low-Power Machine Learning Devices». https://arxiv.org/abs/2103.05579.\n\n\nUn altro framework per FPGA che si concentra su un approccio olistico è CFU Playground (Prakash et al. 2023)\n\nPrakash, Shvetank, Tim Callahan, Joseph Bushagour, Colby Banbury, Alan V. Green, Pete Warden, Tim Ansell, e Vijay Janapa Reddi. 2023. «CFU Playground: Full-stack Open-Source Framework for Tiny Machine Learning (TinyML) Acceleration on FPGAs». In 2023 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS). Vol. abs/2201.01863. IEEE. https://doi.org/10.1109/ispass57527.2023.00024.\n\n\nHardware Costruito Attorno al Software\nIn un approccio contrastante, l’hardware può essere progettato su misura attorno ai requisiti software per ottimizzare le prestazioni per un’applicazione specifica. Questo paradigma crea hardware specializzato per adattarsi meglio alle specifiche del software, riducendo così il sovraccarico computazionale e migliorando l’efficienza operativa. Un esempio di questo approccio è un’applicazione di riconoscimento vocale di (Kwon e Park 2021). Il documento propone una struttura in cui le operazioni di pre-elaborazione, tradizionalmente gestite dal software, sono assegnate ad un hardware progettato su misura. Questa tecnica è stata ottenuta introducendo la logica resistore-transistor in un modulo audio a circuito inter-integrato per il windowing e l’acquisizione di dati audio grezzi nell’applicazione di riconoscimento vocale. Di conseguenza, questa “delega” delle operazioni di pre-elaborazione ha portato a una riduzione del carico computazionale sul software, mostrando un’applicazione pratica della creazione di hardware attorno al software per migliorare l’efficienza e le prestazioni.\n\n\n\n\n\n\nFigura 9.36: Delega dell’elaborazione dei dati a un FPGA. Fonte: Kwon e Park (2021).\n\n\nKwon, Jisu, e Daejin Park. 2021. «Hardware/Software Co-Design for TinyML Voice-Recognition Application on Resource Frugal Edge Devices». Applied Sciences 11 (22): 11073. https://doi.org/10.3390/app112211073.\n\n\n\n\nSplitNet\nLi SplitNet sono state introdotte nel contesto dei sistemi Head-Mounted. Distribuiscono il carico di lavoro delle Deep Neural Network (DNN) tra i sensori della telecamera e un aggregatore. Ciò è particolarmente interessante nel contesto di TinyML. Il framework SplitNet è un NAS split-aware per trovare l’architettura di rete neurale ottimale per ottenere una buona accuratezza, dividere il modello tra i sensori e l’aggregatore e ridurre al minimo la comunicazione tra i sensori e l’aggregatore.\nFigura 9.37 dimostra come le SplitNet (in rosso) ottengano una maggiore accuratezza per una latenza inferiore (in esecuzione su ImageNet) rispetto ad altri approcci, come l’esecuzione del DNN sul sensore (All-on-sensor; in verde) o sul cellulare (All-on-aggregator; in blu). La comunicazione minima è importante in TinyML dove la memoria è fortemente limitata, in questo modo i sensori conducono parte dell’elaborazione sui loro chip e poi inviano solo le informazioni necessarie all’aggregatore. Durante i test su ImageNet, SplitNets è stato in grado di ridurre la latenza di un ordine di grandezza sui dispositivi di visione artificiale montati sulla testa [occhiali o visori]. Ciò può essere utile quando il sensore ha il suo chip. (Dong et al. 2022)\n\n\n\n\n\n\nFigura 9.37: Le SplitNet rispetto ad altri approcci. Fonte: Dong et al. (2022).\n\n\nDong, Xin, Barbara De Salvo, Meng Li, Chiao Liu, Zhongnan Qu, H. T. Kung, e Ziyun Li. 2022. «SplitNets: Designing Neural Architectures for Efficient Distributed Computing on Head-Mounted Systems». In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 12549–59. IEEE. https://doi.org/10.1109/cvpr52688.2022.01223.\n\n\n\n\nHardware Specifico per il “Data Augmentation”\nOgni dispositivo edge può possedere caratteristiche di sensore uniche, che portano a specifici pattern di rumore che possono influire sulle prestazioni del modello. Un esempio sono i dati audio, in cui sono prevalenti le variazioni derivanti dalla scelta del microfono. Applicazioni come le Keyword Spotting possono sperimentare miglioramenti sostanziali incorporando dati registrati da dispositivi simili a quelli destinati all’implementazione. La messa a punto dei modelli esistenti può essere impiegata per adattare i dati in modo preciso alle caratteristiche distintive del sensore.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Ottimizzazioni dei Modelli</span>"
    ]
  },
  {
    "objectID": "contents/core/optimizations/optimizations.it.html#supporto-software-e-framework",
    "href": "contents/core/optimizations/optimizations.it.html#supporto-software-e-framework",
    "title": "9  Ottimizzazioni dei Modelli",
    "section": "9.5 Supporto Software e Framework",
    "text": "9.5 Supporto Software e Framework\nSebbene tutte le tecniche sopra menzionate come pruning, quantizzazione e numeri efficienti siano ben note, rimarrebbero poco pratiche e inaccessibili senza un ampio supporto software. Ad esempio, la quantizzazione diretta di pesi e attivazioni in un modello richiederebbe la modifica manuale della definizione del modello e l’inserimento di operazioni di quantizzazione. Allo stesso modo, la potatura diretta dei pesi del modello richiede la manipolazione dei tensori dei pesi. Tali approcci noiosi diventano impraticabili su larga scala.\nSenza l’ampia innovazione software nei framework, negli strumenti di ottimizzazione e nell’integrazione hardware, la maggior parte di queste tecniche rimarrebbe teorica o praticabile solo per gli esperti. Senza API del framework e automazione per semplificare l’applicazione di queste ottimizzazioni, non verrebbero adottate. Il supporto software le rende accessibili al pubblico e sblocca vantaggi concreti. Inoltre, problemi come la messa a punto degli iperparametri per la potatura, la gestione del compromesso tra dimensioni del modello e accuratezza e la garanzia della compatibilità con i dispositivi target pongono ostacoli che gli sviluppatori devono superare.\n\n9.5.1 API Native di Ottimizzazione\nI principali framework di machine learning come TensorFlow, PyTorch e MXNet forniscono librerie e API per consentire l’applicazione di tecniche comuni di ottimizzazione dei modelli senza richiedere implementazioni personalizzate. Ad esempio, TensorFlow offre il TensorFlow Model Optimization Toolkit che contiene moduli come:\n\nQuantization: Applica un training che tiene conto della quantizzazione per convertire i modelli in virgola mobile in una precisione inferiore come int8 con una perdita di accuratezza minima. Gestisce la quantizzazione del peso e dell’attivazione.\nSparsity: Fornisce API di potatura per indurre la “sparsità” e rimuovere connessioni non necessarie in modelli come le reti neurali. Può potare pesi, livelli, ecc.\nClustering: Supporta la compressione del modello raggruppando i pesi per tassi di compressione più elevati.\n\nQueste API consentono agli utenti di abilitare tecniche di ottimizzazione come la quantizzazione e la potatura senza modificare direttamente il codice del modello. È possibile configurare parametri come i tassi di “sparsità” del target, le larghezze di bit di quantizzazione, ecc. Allo stesso modo, PyTorch fornisce torch.quantization per convertire i modelli in rappresentazioni di precisione inferiore. TorchTensor e TorchModule formano le classi di base per il supporto della quantizzazione. Offre inoltre torch.nn.utils.prune per la potatura nativa dei modelli. MXNet offre layer gluon.contrib che aggiungono funzionalità di quantizzazione come l’arrotondamento a punto fisso e l’arrotondamento stocastico di pesi/attivazioni durante l’addestramento. Ciò consente di includere facilmente la quantizzazione nei modelli gluon.\nIl vantaggio principale delle ottimizzazioni integrate è che gli utenti possono applicarle senza dover reimplementare tecniche complesse. Ciò rende i modelli ottimizzati accessibili a un’ampia gamma di professionisti. Garantisce inoltre che le best practice siano seguite basandosi sulla ricerca e sull’esperienza nell’implementazione dei metodi. Man mano che emergono nuove ottimizzazioni, i framework si sforzano di fornire supporto nativo e API ove possibile per abbassare ulteriormente la barriera verso un ML efficiente. La disponibilità di questi strumenti è fondamentale per un’adozione diffusa.\n\n\n9.5.2 Strumenti di Ottimizzazione Automatizzata\nGli strumenti di ottimizzazione automatizzati forniti dai framework possono analizzare i modelli e applicare automaticamente ottimizzazioni come quantizzazione, potatura e fusione degli operatori per rendere il processo più semplice e accessibile senza un’eccessiva messa a punto manuale. In effetti, questo si basa sulla sezione precedente. Ad esempio, TensorFlow fornisce il TensorFlow Model Optimization Toolkit che contiene moduli come:\n\nQuantizationAwareTraining: Quantizza automaticamente pesi e attivazioni in un modello per ridurre la precisione come UINT8 o INT8 con una perdita di accuratezza minima. Inserisce nodi di quantizzazione falsi durante l’addestramento in modo che il modello possa imparare a essere compatibile con la quantizzazione.\nPruning: Rimuove automaticamente le connessioni non necessarie in un modello in base all’analisi dell’importanza del peso. Può potare interi filtri in livelli convoluzionali o “attention head” [teste di attenzione] nei trasformatori. Gestisce il ri-addestramento iterativo per recuperare qualsiasi perdita di accuratezza.\nGraphOptimizer: Applica ottimizzazioni grafiche come la fusione degli operatori per consolidare le operazioni e ridurre la latenza di esecuzione, in particolare per l’inferenza. In Figura 9.38, si può vedere l’originale (Source Graph) a sinistra e come le sue operazioni vengono trasformate (consolidate) a destra. Notare come Block1 in Source Graph abbia 3 passaggi separati (Convolution, BiasAdd e Activation), che vengono poi consolidati insieme in Block1 su Optimized Graph.\n\n\n\n\n\n\n\nFigura 9.38: GraphOptimizer. Fonte: Wess et al. (2020).\n\n\nWess, Matthias, Matvey Ivanov, Christoph Unger, e Anvesh Nookala. 2020. «ANNETTE: Accurate Neural Network Execution Time Estimation with Stacked Models». IEEE. https://doi.org/10.1109/ACCESS.2020.3047259.\n\n\nQuesti moduli automatizzati richiedono solo all’utente di fornire il modello originale in virgola mobile e di gestire la pipeline di ottimizzazione end-to-end, inclusa qualsiasi riqualificazione per ripristinare la precisione. Anche altri framework come PyTorch offrono un crescente supporto all’automazione, ad esempio tramite torch.quantization.quantize_dynamic. L’ottimizzazione automatizzata rende l’apprendimento automatico efficiente accessibile ai professionisti senza competenze di ottimizzazione.\n\n\n9.5.3 Librerie di Ottimizzazione Hardware\nLibrerie hardware come TensorRT e TensorFlow XLA consentono di ottimizzare i modelli per l’hardware target tramite tecniche di cui abbiamo discusso in precedenza.\n\nQuantizzazione: Ad esempio, TensorRT e TensorFlow Lite supportano entrambi la quantizzazione dei modelli durante la conversione nel loro formato. Ciò fornisce accelerazioni sui SoC mobili con supporto INT8/INT4.\nOttimizzazione del Kernel: ad esempio, TensorRT esegue l’auto-tuning per ottimizzare i kernel CUDA in base all’architettura GPU per ogni layer nel grafo del modello. Ciò estrae la massima produttività.\nFusione degli Operatori: TensorFlow XLA esegue una fusione aggressiva per creare un binario ottimizzato per le TPU. Sui dispositivi mobili, framework come NCNN supportano anche operatori fusi [unificati].\nCodice Specifico per l’Hardware: Le librerie vengono utilizzate per generare codice binario ottimizzato specializzato per l’hardware target. Per esempio, TensorRT usa librerie Nvidia CUDA/cuDNN che sono ottimizzate manualmente per ogni architettura GPU. Questa codifica specifica per hardware è fondamentale per le prestazioni. Sui dispositivi TinyML, questo può significare codice assembly ottimizzato per una CPU Cortex M4, ad esempio. I fornitori forniscono CMSIS-NN e altre librerie.\nOttimizzazioni del Layout dei Dati: Possiamo sfruttare in modo efficiente la gerarchia della memoria di hardware come cache e registri tramite tecniche come riorganizzazione tensore/peso, tiling e riutilizzo. Ad esempio, TensorFlow XLA ottimizza i layout dei buffer per massimizzare l’utilizzo della TPU. Questo aiuta qualsiasi sistema con limiti di memoria.\nOttimizzazione Basata sulla Profilazione: Possiamo usare strumenti di profilazione per identificare i colli di bottiglia. Ad esempio, regolare i livelli di fusione del kernel in base alla profilazione della latenza. Sui SoC mobili, fornitori come Qualcomm forniscono profiler in SNPE per trovare opportunità di ottimizzazione nelle CNN. Questo approccio basato sui dati è importante per le prestazioni.\n\nIntegrando i modelli di framework con queste librerie hardware tramite pipeline di conversione ed esecuzione, gli sviluppatori di ML possono ottenere significativi incrementi di velocità e guadagni di efficienza da ottimizzazioni di basso livello su misura per l’hardware target. La stretta integrazione tra software e hardware è fondamentale per consentire un’implementazione performante delle applicazioni di ML, in particolare su dispositivi mobili e TinyML.\n\n\n9.5.4 Visualizzazione delle Ottimizzazioni\nL’implementazione di tecniche di ottimizzazione del modello senza visibilità degli effetti sul modello può essere impegnativa. Strumenti dedicati o strumenti di visualizzazione possono fornire informazioni critiche e utili sulle modifiche del modello e aiutano a tracciare il processo di ottimizzazione. Consideriamo le ottimizzazioni che abbiamo considerato in precedenza, come la potatura per la “sparsity” [diradazione] e la quantizzazione.\n\nSparsità\nAd esempio, si considerino le ottimizzazioni di sparsity. Gli strumenti di visualizzazione di sparsity possono fornire informazioni critiche sui modelli potati, mappando esattamente quali pesi sono stati rimossi. Ad esempio, le mappe di calore di sparsity possono utilizzare gradienti di colore per indicare la percentuale di pesi potati in ogni layer di una rete neurale. I layer con percentuali di potatura più elevate appaiono più scuri (cfr. Figura 9.39). Questo identifica quali layer sono stati semplificati di più tramite potatura (Souza 2020).\n\n\n\n\n\n\nFigura 9.39: Mappa “termica” della rete sparsa. Fonte: Numenta.\n\n\n\nI grafici di tendenza possono anche tracciare la scarsità nei successivi round di potatura: possono mostrare una rapida potatura iniziale seguita da incrementi più graduali. Il tracciamento della diradazione globale corrente insieme a statistiche come la diradazione media, minima e massima per ogli layer in tabelle o grafici fornisce una panoramica della composizione del modello. Per una rete convoluzionale di esempio, questi strumenti potrebbero rivelare che il primo layer di convoluzione viene potato del 20% mentre quello di classificazione finale viene potato del 70% data la sua ridondanza. La diradazione del modello globale può aumentare dal 10% dopo la potatura iniziale al 40% dopo cinque round.\nRendendo i dati di diradazione visivamente accessibili, i professionisti possono comprendere meglio esattamente come il loro modello viene ottimizzato e quali aree vengono interessate. La visibilità consente loro di mettere a punto e controllare il processo di potatura per una determinata architettura.\nLa visualizzazione della diradazione trasforma la potatura in una tecnica trasparente anziché in un’operazione “black-box”.\n\n\nQuantizzazione\nLa conversione di modelli in precisioni numeriche inferiori tramite quantizzazione introduce errori che possono influire sulla precisione del modello se non vengono monitorati e affrontati correttamente. La visualizzazione delle distribuzioni degli errori di quantizzazione fornisce informazioni preziose sugli effetti dei numeri di precisione ridotti applicati a diverse parti di un modello. Per questo, è possibile generare istogrammi degli errori di quantizzazione per pesi e attivazioni. Questi istogrammi possono rivelare la forma della distribuzione degli errori, se assomigliano a una distribuzione gaussiana o contengono valori anomali e picchi significativi. Figura 9.40 mostra le distribuzioni di diversi metodi di quantizzazione. Valori anomali elevati possono indicare problemi con particolari layer che gestiscono la quantizzazione. Il confronto degli istogrammi tra layer evidenzia eventuali aree problematiche che si distinguono con errori anormalmente elevati.\n\n\n\n\n\n\nFigura 9.40: Errori di Quantizzazione. Fonte: Kuzmin et al. (2022).\n\n\nKuzmin, Andrey, Mart Van Baalen, Yuwei Ren, Markus Nagel, Jorn Peters, e Tijmen Blankevoort. 2022. «FP8 Quantization: The Power of the Exponent». https://arxiv.org/abs/2208.09225.\n\n\nLe visualizzazioni di attivazione sono importanti anche per rilevare problemi di overflow. Con la mappatura a colori delle attivazioni prima e dopo la quantizzazione, tutti i valori spinti al di fuori degli intervalli previsti diventano visibili. Ciò rivela problemi di saturazione e troncamento che potrebbero alterare le informazioni che fluiscono attraverso il modello. Il rilevamento di questi errori consente di ricalibrare le attivazioni per evitare la perdita di informazioni (Mandal 2022). Figura 9.41 è una mappatura a colori dei kernel convoluzionali AlexNet.\n\n\n\n\n\n\nFigura 9.41: Mappatura a colori delle attivazioni. Fonte: Krizhevsky, Sutskever, e Hinton (2017).\n\n\nKrizhevsky, Alex, Ilya Sutskever, e Geoffrey E. Hinton. 2017. «ImageNet classification with deep convolutional neural networks». A cura di F. Pereira, C. J. Burges, L. Bottou, e K. Q. Weinberger. Commun. ACM 60 (6): 84–90. https://doi.org/10.1145/3065386.\n\n\nAltre tecniche, come il tracciamento dell’errore di quantizzazione quadratico medio complessivo a ogni passaggio del processo di addestramento consapevole della quantizzazione, identificano fluttuazioni e divergenze. Picchi improvvisi nel grafico di tracciamento possono indicare punti in cui la quantizzazione sta interrompendo l’addestramento del modello. Il monitoraggio di questa metrica crea intuizione sul comportamento del modello in fase di quantizzazione. Insieme, queste tecniche trasformano la quantizzazione in un processo trasparente. Le intuizioni empiriche consentono ai professionisti di valutare correttamente gli effetti della quantizzazione. Individuano le aree dell’architettura del modello o del processo di training da ricalibrare in base ai problemi di quantizzazione osservati. Ciò aiuta a ottenere modelli quantizzati numericamente stabili e accurati.\nFornire questi dati consente ai professionisti di valutare correttamente l’impatto della quantizzazione e identificare potenziali aree problematiche del modello da ricalibrare o riprogettare per renderlo più adatto alla quantizzazione. Questa analisi empirica sviluppa l’intuizione sul raggiungimento di una quantizzazione ottimale.\nGli strumenti di visualizzazione possono fornire approfondimenti che aiutano i professionisti a comprendere meglio gli effetti delle ottimizzazioni sui loro modelli. La visibilità consente di correggere i problemi in anticipo prima che l’accuratezza o le prestazioni siano influenzate in modo significativo. Aiuta anche ad applicare le ottimizzazioni in modo più efficace per modelli specifici. Queste analisi di ottimizzazione aiutano a sviluppare l’intuizione quando si trasferiscono i modelli a rappresentazioni più efficienti.\n\n\n\n9.5.5 Conversione e Distribuzione del Modello\nUna volta che i modelli sono stati ottimizzati con successo in framework come TensorFlow e PyTorch, sono necessarie piattaforme specializzate di conversione e deployment [distribuzione] del modello per colmare il divario con l’esecuzione sui dispositivi target.\nTensorFlow Lite - La piattaforma di TensorFlow per convertire i modelli in un formato leggero ottimizzato per dispositivi mobili, embedded ed edge. Supporta ottimizzazioni come quantizzazione, fusione del kernel e rimozione di operazioni inutilizzate. I modelli possono essere eseguiti utilizzando kernel TensorFlow Lite ottimizzati sull’hardware del dispositivo. Fondamentale per la distribuzione mobile e TinyML.\nONNX Runtime - Esegue la conversione e l’inferenza per i modelli nel formato “open ONNX”. Fornisce kernel ottimizzati, supporta acceleratori hardware come GPU e distribuzione multipiattaforma dal cloud all’edge. Consente la distribuzione indipendente dal framework. Figura 9.42 è una mappa di interoperabilità ONNX, inclusi i principali framework più diffusi.\n\n\n\n\n\n\nFigura 9.42: Interoperabilità di ONNX. Fonte: TowardsDataScience.\n\n\n\nPyTorch Mobile - Consente l’esecuzione dei modelli PyTorch su iOS e Android convertendoli in rappresentazioni ottimizzate per dispositivi mobili. Fornisce implementazioni mobili efficienti di operazioni come convoluzione e funzioni speciali ottimizzate per hardware mobile.\nQueste piattaforme si integrano con driver hardware, sistemi operativi e librerie di acceleratori sui dispositivi per eseguire modelli in modo efficiente utilizzando l’ottimizzazione hardware. Inoltre, delegano le operazioni ad acceleratori ML dedicati, ove presenti. La disponibilità di queste piattaforme di distribuzione collaudate e robuste colma il divario tra l’ottimizzazione dei modelli nei framework e la distribuzione effettiva su miliardi di dispositivi. Consentono agli utenti di concentrarsi sullo sviluppo del modello anziché sulla creazione di runtime mobili personalizzati. L’innovazione continua per supportare nuovi hardware e ottimizzazioni in queste piattaforme è fondamentale per le ottimizzazioni di ML diffuse.\nFornendo queste pipeline di distribuzione ottimizzate, l’intero flusso di lavoro, dal training al deployment [distribuzione] del dispositivo, può sfruttare le ottimizzazioni del modello per fornire applicazioni ML performanti. Questa infrastruttura software end-to-end ha contribuito a guidare l’adozione di ML sul dispositivo.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Ottimizzazioni dei Modelli</span>"
    ]
  },
  {
    "objectID": "contents/core/optimizations/optimizations.it.html#conclusione",
    "href": "contents/core/optimizations/optimizations.it.html#conclusione",
    "title": "9  Ottimizzazioni dei Modelli",
    "section": "9.6 Conclusione",
    "text": "9.6 Conclusione\nIn questo capitolo abbiamo discusso l’ottimizzazione del modello nell’ambito software-hardware. Ci siamo immersi in una rappresentazione efficiente del modello, dove abbiamo trattato le sfumature della potatura strutturata e non-strutturata e altre tecniche per la compressione del modello come la distillazione della conoscenza e la decomposizione di matrice e tensore. Ci siamo anche immersi brevemente nella progettazione del modello specifico per l’edge a livello di parametri e architettura del modello, esplorando argomenti come modelli specifici per l’edge e NAS basati sull’hardware.\nAbbiamo quindi esplorato rappresentazioni numeriche efficienti, dove abbiamo trattato le basi della matematica, codifiche numeriche e archiviazione, vantaggi della matematica efficiente e le sfumature della rappresentazione numerica con utilizzo della memoria, complessità computazionale, compatibilità hardware e scenari di compromesso. Abbiamo concluso concentrandoci su un elemento fondamentale della matematica efficiente: la quantizzazione, dove abbiamo esaminato la sua storia, calibrazione, tecniche e interazione con la potatura.\nInfine, abbiamo esaminato come possiamo apportare ottimizzazioni specifiche per l’hardware che abbiamo. Abbiamo esplorato come possiamo trovare architetture modello su misura per l’hardware, apportare ottimizzazioni nel kernel per gestire meglio il modello e framework creati per sfruttare al meglio l’hardware. Abbiamo anche esaminato come possiamo fare il contrario e creare hardware attorno al nostro software specifico e abbiamo parlato di come suddividere le reti per l’esecuzione su più processori disponibili sul dispositivo edge.\nComprendendo il quadro completo dei gradi di libertà all’interno dell’ottimizzazione del modello sia lontano che vicino all’hardware e i compromessi da considerare quando si implementano questi metodi, i professionisti possono sviluppare una pipeline più ponderata per comprimere i loro carichi di lavoro sui dispositivi edge.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Ottimizzazioni dei Modelli</span>"
    ]
  },
  {
    "objectID": "contents/core/optimizations/optimizations.it.html#sec-model-optimizations-resource",
    "href": "contents/core/optimizations/optimizations.it.html#sec-model-optimizations-resource",
    "title": "9  Ottimizzazioni dei Modelli",
    "section": "9.7 Risorse",
    "text": "9.7 Risorse\nEcco un elenco curato di risorse per supportare sia gli studenti che gli insegnanti nel loro percorso di apprendimento e insegnamento. Stiamo lavorando costantemente per espandere questa raccolta e aggiungeremo nuovi esercizi nel prossimo futuro.\n\n\n\n\n\n\nSlide\n\n\n\n\n\nQueste slide sono uno strumento prezioso per gli insegnanti per tenere lezioni e per gli studenti per rivedere il materiale al proprio ritmo. Incoraggiamo sia gli studenti che gli insegnanti a sfruttare queste slide per migliorare la loro comprensione e facilitare un trasferimento efficace delle conoscenze.\n\nQuantizzazione:\n\nQuantization: Part 1.\nQuantization: Part 2.\nPost-Training Quantization (PTQ).\nQuantization-Aware Training (QAT).\n\nPruning:\n\nPruning: Part 1.\nPruning: Part 2.\n\nKnowledge Distillation.\nClustering.\nNeural Architecture Search (NAS):\n\nNAS overview.\nNAS: Part 1.\nNAS: Part 2.\n\n\n\n\n\n\n\n\n\n\n\nVideo\n\n\n\n\n\n\nVideo 9.1\n\n\n\n\n\n\n\n\n\n\nEsercizi\n\n\n\n\n\nPer rafforzare i concetti trattati in questo capitolo, abbiamo curato una serie di esercizi che sfidano gli studenti ad applicare le proprie conoscenze e ad approfondire la propria comprensione.\n\nEsercizio 9.1\nEsercizio 9.2\nEsercizio 9.3",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Ottimizzazioni dei Modelli</span>"
    ]
  },
  {
    "objectID": "contents/core/hw_acceleration/hw_acceleration.it.html",
    "href": "contents/core/hw_acceleration/hw_acceleration.it.html",
    "title": "10  Accelerazione IA",
    "section": "",
    "text": "10.1 Panoramica\nProbabilmente avrete notato la crescente domanda di integrazione dell’apprendimento automatico nei dispositivi di uso quotidiano, come gli smartphone nelle nostre tasche, gli elettrodomestici intelligenti e persino i veicoli autonomi. Portare le funzionalità di ML in questi ambienti del mondo reale è entusiasmante, ma comporta una serie di sfide. A differenza dei potenti server dei data center, questi dispositivi edge hanno risorse di elaborazione limitate, il che rende difficile eseguire modelli complessi in modo efficace.\nL’accelerazione hardware specializzata è la chiave per rendere possibile l’apprendimento automatico ad alte prestazioni su dispositivi edge con risorse limitate. Quando parliamo di accelerazione hardware, ci riferiamo all’uso di chip e architetture personalizzati progettati per gestire il pesante lavoro delle operazioni di ML, alleggerendo il carico del processore principale. Nelle reti neurali, alcune delle attività più impegnative riguardano le moltiplicazioni di matrici durante l’inferenza. Gli acceleratori hardware sono progettati per ottimizzare queste operazioni, spesso offrendo accelerazioni da 10 a 100 volte superiori rispetto alle CPU per uso generico. Questo tipo di accelerazione è ciò che rende fattibile l’esecuzione di modelli di reti neurali avanzate su dispositivi limitati da dimensioni, peso e potenza, e di fare tutto in tempo reale.\nIn questo capitolo, esamineremo più da vicino le diverse tecniche di accelerazione hardware disponibili per l’apprendimento automatico embedded e i compromessi che derivano da ciascuna opzione. L’obiettivo è fornire una solida comprensione di come funzionano queste tecniche, in modo che si possano prendere decisioni informate quando si tratta di scegliere l’hardware giusto e ottimizzare il software. Alla fine, sarete ben equipaggiati per sviluppare capacità di apprendimento automatico ad alte prestazioni su dispositivi edge, anche con i loro vincoli.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Accelerazione IA</span>"
    ]
  },
  {
    "objectID": "contents/core/hw_acceleration/hw_acceleration.it.html#background-e-basi",
    "href": "contents/core/hw_acceleration/hw_acceleration.it.html#background-e-basi",
    "title": "10  Accelerazione IA",
    "section": "10.2 Background e Basi",
    "text": "10.2 Background e Basi\n\n10.2.1 Background Storico\nLe origini dell’accelerazione hardware risalgono agli anni ’60, con l’avvento dei coprocessori matematici in virgola mobile per eliminare i calcoli dalla CPU principale. Un primo esempio è stato il chip Intel 8087 rilasciato nel 1980 per accelerare le operazioni in virgola mobile per il processore 8086. Ciò ha stabilito la pratica di utilizzare processori specializzati per gestire in modo efficiente carichi di lavoro ad alta intensità di calcolo.\nNegli anni ’90, sono emerse le prime Graphics Processing Units (GPU) [Unità di elaborazione grafica] per elaborare rapidamente pipeline grafiche per rendering e giochi. La GeForce 256 di Nvidia nel 1999 è stata una delle prime GPU programmabili in grado di eseguire algoritmi software personalizzati. Le GPU esemplificano acceleratori a funzione fissa specifici per dominio e si sono evolute in acceleratori programmabili paralleli.\nNegli anni 2000, le GPU sono state applicate all’elaborazione generica in GPGPU. La loro elevata larghezza di banda di memoria e la produttività computazionale le hanno rese adatte a carichi di lavoro ad alta intensità di calcolo. Ciò ha incluso innovazioni nell’uso di GPU per accelerare il training di modelli di deep learning come AlexNet nel 2012.\nNegli ultimi anni, le Tensor Processing Unit (TPU) di Google rappresentano ASIC personalizzati specificamente progettati per la moltiplicazione di matrici nel deep learning. Durante l’inferenza, i loro core tensoriali ottimizzati raggiungono TeraOPS/watt più elevati rispetto a CPU o GPU. L’innovazione continua include tecniche di compressione del modello come pruning e quantizzazione per adattare reti neurali più grandi su dispositivi edge.\nQuesta evoluzione dimostra come l’accelerazione hardware si sia concentrata sulla risoluzione di colli di bottiglia ad alta intensità di calcolo, dalla matematica in virgola mobile alla grafica alla moltiplicazione di matrici per ML. Comprendere questa storia fornisce un contesto cruciale per gli acceleratori AI specializzati odierni.\n\n\n10.2.2 La Necessità di Accelerazione\nL’evoluzione dell’accelerazione hardware è strettamente legata alla storia più ampia dell’informatica. Centrale in questa storia è il ruolo dei transistor, i mattoni fondamentali dell’elettronica moderna. I transistor agiscono come piccoli interruttori che possono accendersi o spegnersi, consentendo i calcoli complessi che guidano tutto, dalle semplici calcolatrici ai modelli avanzati di apprendimento automatico. Nei primi decenni, la progettazione dei chip era governata dalla legge di Moore, che prevedeva che il numero di transistor su un circuito integrato sarebbe raddoppiato approssimativamente ogni due anni, e dal Dennard Scaling, che osservava che man mano che i transistor diventavano più piccoli, le loro prestazioni (velocità) aumentavano, mentre la densità di potenza (potenza per unità di area) rimaneva costante. Queste due leggi sono state mantenute durante l’era single-core. Figura 10.1 mostra le tendenze di diverse metriche dei microprocessori. Come indica la figura, il Dennard Scaling fallisce intorno alla metà degli anni 2000; si noti come la velocità di clock (frequenza) rimanga pressoché costante anche se il numero di transistor continua ad aumentare.\n\n\n\n\n\n\nFigura 10.1: Tendenze dei Microprocessori. Fonte: Karl Rupp.\n\n\n\nTuttavia, come descrive Patterson e Hennessy (2016), i vincoli tecnologici alla fine hanno imposto una transizione all’era multicore, con chip contenenti più core di elaborazione per offrire guadagni in termini di prestazioni. Le limitazioni di potenza hanno impedito un ulteriore ridimensionamento, il che ha portato al “silicio scuro” (Dark Silicon), in cui non tutte le aree del chip potevano essere attive simultaneamente (Xiu 2019).\n\nPatterson, David A, e John L Hennessy. 2016. Computer organization and design ARM edition: The hardware software interface. Morgan kaufmann.\n\nXiu, Liming. 2019. «Time Moore: Exploiting Moore’s Law From The Perspective of Time». IEEE Solid-State Circuits Mag. 11 (1): 39–55. https://doi.org/10.1109/mssc.2018.2882285.\n“Dark silicon” si riferisce a parti del chip che non possono essere alimentate simultaneamente a causa di limitazioni termiche e di potenza. In sostanza, con l’aumento della densità dei transistor, la quota del chip che poteva essere utilizzata attivamente senza surriscaldarsi o superare i budget di potenza si è ridotta.\nQuesto fenomeno ha comportato che, sebbene i chip avessero più transistor, non tutti potevano essere operativi simultaneamente, limitando i potenziali guadagni in termini di prestazioni. Questa crisi energetica ha reso necessario un passaggio all’era degli acceleratori, con unità hardware specializzate su misura per attività specifiche per massimizzare l’efficienza. L’esplosione dei carichi di lavoro dell’intelligenza artificiale ha ulteriormente spinto la domanda di acceleratori personalizzati. I fattori abilitanti includevano nuovi linguaggi di programmazione, strumenti software e progressi nella produzione.\nFondamentalmente, gli acceleratori hardware vengono valutati in base a Prestazioni, Potenza e Area di silicio (PPA); la natura dell’applicazione target, sia essa legata alla memoria o al calcolo, influenza notevolmente la progettazione. Ad esempio, i carichi di lavoro legati alla memoria richiedono un’elevata larghezza di banda e un accesso a bassa latenza, mentre le applicazioni legate al calcolo richiedono la massima produttività di elaborazione.\n\n\n10.2.3 Principi Generali\nLa progettazione di acceleratori hardware specializzati comporta la gestione di compromessi complessi tra prestazioni, efficienza energetica, area di silicio e ottimizzazioni specifiche del carico di lavoro. Questa sezione delinea considerazioni e metodologie fondamentali per raggiungere un equilibrio ottimale in base ai requisiti dell’applicazione e ai vincoli hardware.\n\nPrestazioni entro i Budget di Potenza\nPer capire come raggiungere il giusto equilibrio tra prestazioni e budget di potenza, è importante definire prima alcuni concetti chiave che svolgono un ruolo cruciale in questo processo. Le prestazioni si riferiscono in generale alla capacità complessiva di un sistema di completare efficacemente le attività di calcolo entro determinati vincoli. Uno dei componenti chiave delle prestazioni è il throughput, ovvero la velocità con cui vengono elaborate queste attività, comunemente misurata in “floating point operations per second (FLOPS)” [operazioni in virgola mobile al secondo ] o frame al secondo (FPS). Il throughput dipende fortemente dal parallelismo, ovvero la capacità dell’hardware di eseguire più operazioni contemporaneamente, e dalla frequenza di clock, ovvero la velocità con cui il processore esegue ciclicamente queste operazioni. Un throughput più elevato in genere comporta prestazioni migliori, ma aumenta anche il consumo di energia all’aumentare dell’attività.\nLa semplice massimizzazione del throughput non è sufficiente; anche l’efficienza dell’hardware è importante. L’efficienza è la misura di quante operazioni vengono eseguite per watt di potenza consumata, riflettendo la relazione tra lavoro di calcolo e consumo di energia. In scenari in cui la potenza è un fattore limitante, come nei dispositivi edge, ottenere un’elevata efficienza è fondamentale. Per aiutare a ricordare come questi concetti si interconnettono, considerare le seguenti relazioni:\n\nPrestazioni = Throughput * Efficienza\nThroughput ~= Parallelismo * Frequenza di Clock\nEfficienza = Operazioni / Watt\n\nGli acceleratori hardware mirano a massimizzare le prestazioni entro budget di potenza stabiliti. Ciò richiede un attento bilanciamento del parallelismo, della frequenza di clock del chip, della tensione di esercizio, dell’ottimizzazione del carico di lavoro e di altre tecniche per massimizzare le operazioni per watt.\nAd esempio, le GPU raggiungono un throughput elevato tramite architetture massivamente parallele. Tuttavia, la loro efficienza è inferiore a quella dei circuiti integrati specifici per applicazione (ASIC) personalizzati come il TPU di Google, che ottimizzano per un carico di lavoro specifico.\n\n\nGestione dell’Area e dei Costi del Silicio\nLa dimensione dell’area di un chip ha un impatto diretto sul suo costo di produzione. Per capirne il motivo, è utile conoscere un po’ il processo di produzione.\nI chip vengono creati da grandi e sottili fette di materiale semiconduttore note come wafer. Durante la produzione, ogni wafer viene suddiviso in blocchi più piccoli chiamati “die”, e ogni die contenente i circuiti per un singolo chip. Dopo che il wafer è stato elaborato, viene tagliato in questi singoli die, che vengono poi confezionati per formare i chip finali utilizzati nei dispositivi elettronici.\nI die più grandi richiedono più materiale e sono più inclini a difetti, il che può ridurre la resa, il che significa che vengono prodotti meno chip utilizzabili da ogni wafer. Mentre i produttori possono scalare i progetti combinando più die più piccoli in un singolo pacchetto (pacchetti multi-die), ciò aggiunge complessità e costi al processo di confezionamento e produzione.\nLa quantità di area di silicio necessaria su un die dipende da diversi fattori:\n\nRisorse di Calcolo, ad esempio numero di core, memoria, cache\nNodo del Processo di Produzione, transistor più piccoli consentono una maggiore densità\nModello di Programmazione, acceleratori programmati richiedono maggiore flessibilità\n\nLa progettazione dell’acceleratore implica la compressione delle massime prestazioni entro questi vincoli di area del silicio. Tecniche come la potatura e la compressione aiutano ad adattare modelli più grandi al chip senza superare lo spazio disponibile.\n\n\nOttimizzazioni Specifiche del Carico di Lavoro\nLa progettazione di acceleratori hardware efficaci richiede di adattare l’architettura alle esigenze specifiche del carico di lavoro target. Diversi tipi di carichi di lavoro, che siano in AI, grafica o robotica, hanno caratteristiche uniche che stabiliscono come l’acceleratore dovrebbe essere ottimizzato.\nAlcune delle considerazioni chiave quando si ottimizza l’hardware per carichi di lavoro specifici includono:\n\nMemoria vs Limiti di Calcolo: I carichi di lavoro vincolati alla memoria richiedono una maggiore larghezza di banda di memoria, mentre le app vincolate al calcolo necessitano di un throughput [produttività] aritmetico.\nLocalità dei Dati: Lo spostamento dei dati dovrebbe essere ridotto al minimo per l’efficienza. La memoria vicina al calcolo aiuta.\nOperazioni a Livello di Bit: I tipi di dati a bassa precisione come INT8/INT4 ottimizzano la densità di calcolo.\nParallelismo dei Dati: Più unità di calcolo replicate consentono l’esecuzione parallela.\nPipelining: L’esecuzione sovrapposta delle operazioni aumenta la produttività.\n\nLa comprensione delle caratteristiche del carico di lavoro consente un’accelerazione personalizzata. Ad esempio, le reti neurali convoluzionali utilizzano operazioni di “finestra scorrevole” mappate in modo ottimale su array spaziali di elementi di elaborazione.\nGrazie alla comprensione di questi compromessi architettonici, i progettisti possono prendere decisioni informate sull’architettura dell’acceleratore hardware, assicurandosi che fornisca le migliori prestazioni possibili per l’uso previsto.\n\n\nProgettazione Hardware Sostenibile\nNegli ultimi anni, la sostenibilità dell’IA è diventata una preoccupazione urgente, guidata da due fattori chiave: la scala crescente dei carichi di lavoro dell’IA e il consumo energetico associato.\nInnanzitutto, le dimensioni dei modelli e dei set di dati dell’IA sono cresciute rapidamente. Ad esempio, in base alle tendenze di elaborazione dell’IA di OpenAI, la quantità di elaborazione utilizzata per addestrare modelli all’avanguardia raddoppia ogni 3,5 mesi. Questa crescita esponenziale richiede enormi risorse di elaborazione nei data center.\nIn secondo luogo, l’uso di energia per l’addestramento e l’inferenza dell’IA presenta problemi di sostenibilità. I data center che eseguono applicazioni di IA consumano molta energia, contribuendo a elevate emissioni di carbonio. Si stima che l’addestramento di un grande modello di IA possa avere un’impronta di carbonio di 626.000 libbre di CO2 equivalente, quasi 5 volte le emissioni di un’auto media nel corso della sua vita.\nPer affrontare queste sfide, la progettazione hardware sostenibile si concentra sull’ottimizzazione dell’efficienza energetica senza compromettere le prestazioni. Ciò comporta lo sviluppo di acceleratori specializzati che riducono al minimo il consumo di energia massimizzando al contempo la produttività computazionale.\nParleremo di IA sostenibile in un capitolo successivo, dove ne discuteremo più in dettaglio.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Accelerazione IA</span>"
    ]
  },
  {
    "objectID": "contents/core/hw_acceleration/hw_acceleration.it.html#sec-aihw",
    "href": "contents/core/hw_acceleration/hw_acceleration.it.html#sec-aihw",
    "title": "10  Accelerazione IA",
    "section": "10.3 Tipi di acceleratori",
    "text": "10.3 Tipi di acceleratori\nGli acceleratori hardware possono assumere molte forme. Possono esistere come widget (come il Neural Engine nel chip Apple M1) o come interi chip appositamente progettati per svolgere molto bene determinate attività. Questa sezione esaminerà i processori per carichi di lavoro di apprendimento automatico lungo lo spettro che va dagli ASIC altamente specializzati alle CPU più generiche.\nCi concentriamo prima sull’hardware personalizzato appositamente progettato per l’intelligenza artificiale per comprendere le ottimizzazioni più estreme possibili quando vengono rimossi i vincoli di progettazione. Questo stabilisce un limite massimo per prestazioni ed efficienza. Poi prendiamo in considerazione progressivamente architetture più programmabili e adattabili, discutendo di GPU e FPGA. Queste fanno compromessi nella personalizzazione per mantenere la flessibilità. Infine, trattiamo le CPU generiche che sacrificano le ottimizzazioni per un carico di lavoro particolare in cambio di una programmabilità versatile tra le applicazioni.\nStrutturando l’analisi lungo questo spettro, miriamo a illustrare i compromessi fondamentali tra utilizzo, efficienza, programmabilità e flessibilità nella progettazione dell’acceleratore. Il punto di equilibrio ottimale dipende dai vincoli e dai requisiti dell’applicazione target. Questa prospettiva dello spettro fornisce un quadro per ragionare sulle scelte hardware per l’apprendimento automatico e sulle capacità richieste a ciascun livello di specializzazione.\nFigura 10.2 illustra la complessa interazione tra flessibilità, prestazioni, diversità funzionale e area di progettazione dell’architettura. Notare come l’ASIC si trovi nell’angolo in basso a destra, con area minima, flessibilità e consumo energetico e prestazioni massime, a causa della sua natura altamente specializzata per l’applicazione. Un compromesso chiave è la diversità funzionale rispetto alle prestazioni: le architetture per uso generico possono servire applicazioni diverse, ma le loro prestazioni applicative sono degradate rispetto alle architetture più personalizzate.\n\n\n\n\n\n\nFigura 10.2: Compromessi di Progettazione. Fonte: El-Rayis (2014).\n\n\nEl-Rayis, A. O. 2014. «Reconfigurable architectures for the next generation of mobile device telecommunications systems». : https://www.researchgate.net/publication/292608967.\n\n\nLa progressione inizia con l’opzione più specializzata, gli ASIC appositamente progettati per l’intelligenza artificiale, per basare la nostra comprensione sulle massime ottimizzazioni possibili prima di espanderci ad architetture più generalizzabili. Questo approccio strutturato chiarisce lo spazio di progettazione dell’acceleratore.\n\n10.3.1 Application-Specific Integrated Circuits (ASIC)\nUn “circuito integrato specifico per applicazione” (ASIC) è un tipo di circuito integrato (IC) progettato su misura per un’applicazione o un carico di lavoro specifico, anziché per un uso generico. A differenza di CPU e GPU, gli ASIC non supportano più applicazioni o carichi di lavoro. Piuttosto, sono ottimizzati per eseguire un singolo compito in modo estremamente efficiente. Google TPU è un esempio di ASIC.\nGli ASIC raggiungono questa efficienza adattando ogni aspetto del design del chip, ovvero le porte logiche sottostanti, i componenti elettronici, l’architettura, la memoria, l’I/O e il processo di produzione, specificamente per l’applicazione target. Questo livello di personalizzazione consente di rimuovere qualsiasi logica o funzionalità non necessaria richiesta per il calcolo generale. Il risultato è un IC che massimizza le prestazioni e l’efficienza energetica sul carico di lavoro desiderato. I guadagni di efficienza derivanti dall’hardware specifico per applicazione sono così sostanziali che queste aziende incentrate sul software dedicano enormi risorse ingegneristiche alla progettazione di ASIC personalizzati.\nL’ascesa di algoritmi di apprendimento automatico più complessi ha reso i vantaggi prestazionali abilitati dall’accelerazione hardware personalizzata un fattore di differenziazione competitiva chiave, anche per le aziende tradizionalmente concentrate sull’ingegneria del software. Gli ASIC sono diventati un investimento ad alta priorità per i principali provider cloud che mirano a offrire un calcolo AI più veloce.\n\nVantaggi\nGrazie alla loro natura personalizzata, gli ASIC offrono vantaggi significativi rispetto ai processori generici come CPU e GPU. I principali vantaggi includono quanto segue.\n\nPrestazioni ed efficienza massimizzate\nIl vantaggio più fondamentale degli ASIC è la massimizzazione delle prestazioni e dell’efficienza energetica personalizzando l’architettura hardware specificamente per l’applicazione target. Ogni transistor e aspetto della progettazione è ottimizzato per il carico di lavoro desiderato: non è necessaria alcuna logica o sovraccarico non necessario per supportare il calcolo generico.\nAd esempio, le Tensor Processing Units (TPU) di Google contengono architetture su misura esattamente per le operazioni di moltiplicazione di matrici utilizzate nelle reti neurali. Per progettare gli ASIC TPU, i team di ingegneria di Google devono definire chiaramente le specifiche del chip, scrivere la descrizione dell’architettura utilizzando linguaggi di descrizione hardware come Verilog, sintetizzare il design per mapparlo sui componenti hardware e posizionare e instradare con cura transistor e collegamenti in base alle regole di progettazione del processo di fabbricazione. Questo complesso processo di progettazione, noto come “very-large-scale integration” (VLSI) [integrazione su larga scala ], consente loro di creare un IC ottimizzato per carichi di lavoro di apprendimento automatico.\nDi conseguenza, gli ASIC TPU raggiungono un’efficienza di oltre un ordine di grandezza superiore nelle operazioni per watt rispetto alle GPU per uso generico sui carichi di lavoro di apprendimento automatico massimizzando le prestazioni e riducendo al minimo il consumo energetico tramite un design hardware full-stack personalizzato.\n\n\nMemoria On-Chip Specializzata\nGli ASIC incorporano memoria on-chip, come SRAM (Static Random Access Memory) e cache specificamente ottimizzate per fornire dati alle unità di elaborazione. La SRAM è un tipo di memoria più veloce e affidabile della DRAM (Dynamic Random Access Memory) perché non deve essere aggiornata periodicamente. Tuttavia, richiede più transistor per bit di dati, il che la rende più ingombrante e costosa da produrre rispetto alla DRAM.\nLa SRAM è ideale per la memoria on-chip, dove la velocità è fondamentale. Il vantaggio di avere grandi quantità di SRAM on-chip ad alta larghezza di banda è che i dati possono essere archiviati vicino agli elementi di elaborazione, consentendo un rapido accesso. Ciò fornisce enormi vantaggi in termini di velocità rispetto all’accesso alla DRAM off-chip, che, sebbene di capacità maggiore, può essere fino a 100 volte più lenta. Ad esempio, il system-on-a-chip M1 di Apple contiene una speciale SRAM a bassa latenza per accelerare le prestazioni del suo hardware di machine learning Neural Engine.\nLa località dei dati e l’ottimizzazione della gerarchia di memoria sono fondamentali per un throughput elevato e un basso consumo energetico. Tabella 10.1 mostra “Numeri che Tutti Dovrebbero Conoscere”, di Jeff Dean.\n\n\n\nTabella 10.1: Confronto della latenza delle operazioni di elaborazione e di rete.\n\n\n\n\n\n\n\n\n\nOperazione\nLatenza\n\n\n\n\nRiferimento alla cache L1\n0,5 ns\n\n\nBranch mispredict\n5 ns\n\n\nRiferimento alla cache L2\n7 ns\n\n\nBlocco/sblocco mutex\n25 ns\n\n\nRiferimento alla memoria principale\n100 ns\n\n\nComprimere 1K byte con Zippy\n3.000 ns (3 us)\n\n\nInviare 1 KB byte su una rete da 1 Gbps\n10.000 ns (10 us)\n\n\nLeggere 4 KB casualmente da SSD\n150.000 ns (150 us)\n\n\nLeggere 1 MB in sequenza dalla memoria\n250.000 ns (250 us)\n\n\nAndata e ritorno all’interno dello stesso data center\n500.000 ns (0,5 ms)\n\n\nLeggere 1 MB in sequenza da SSD\n1.000.000 ns (1 ms)\n\n\nRicerca su disco\n10.000.000 ns (10 ms)\n\n\nLeggere 1 MB in sequenza da disco\n20.000.000 ns (20 ms)\n\n\nInviare un pacchetto CA → Paesi Bassi → CA\n150.000.000 ns (150 ms)\n\n\n\n\n\n\n\n\nTipi di Dati e Operazioni Personalizzati\nA differenza dei processori generici, gli ASIC possono essere progettati per supportare in modo nativo tipi di dati personalizzati come INT4 o bfloat16, ampiamente utilizzati nei modelli di ML. Ad esempio, l’architettura GPU Ampere di Nvidia ha un bfloat16 dedicato ai Tensor Core per accelerare i carichi di lavoro AI. I tipi di dati a bassa precisione consentono una maggiore densità aritmetica e prestazioni. Per ulteriori dettagli fare riferimento a Sezione 8.6. Gli ASIC possono anche incorporare direttamente operazioni non standard negli algoritmi ML come operazioni primitive, ad esempio, il supporto nativo di funzioni di attivazione come ReLU rende l’esecuzione più efficiente.\n\n\nParallelismo Elevato\nLe architetture ASIC possono sfruttare un parallelismo più elevato ottimizzato per il carico di lavoro del target rispetto alle CPU o GPU generiche. Un maggior numero di unità di calcolo personalizzate per l’applicazione significa più operazioni eseguite simultaneamente. Gli ASIC altamente paralleli raggiungono un throughput enorme per carichi di lavoro paralleli di dati come l’inferenza di reti neurali.\n\n\nNodi di Processo Avanzati\nI processi di produzione all’avanguardia consentono di impacchettare più transistor in aree di die più piccole, aumentando la densità. Gli ASIC progettati specificamente per applicazioni ad alto volume possono ammortizzare meglio i costi dei nodi.\n\n\n\nSvantaggi\n\nTempistiche di Progettazione Lunghe\nIl processo di progettazione e validazione di un ASIC può richiedere 2-3 anni. La sintesi dell’architettura utilizzando linguaggi di descrizione hardware, la definizione del layout del chip e la fabbricazione del chip su nodi di processo avanzati comportano lunghi cicli di sviluppo. Ad esempio, per realizzare un chip da 7 nm, i team devono definire attentamente le specifiche, scrivere l’architettura in HDL, sintetizzare le porte logiche, posizionare i componenti, instradare tutte le interconnessioni e finalizzare il layout da inviare per la fabbricazione. Questa “Very Large-Scale Integration (VLSI)” significa che la progettazione e la produzione di ASIC possono tradizionalmente richiedere 2-5 anni.\nCi sono alcuni motivi chiave per cui le lunghe tempistiche di progettazione degli ASIC, spesso 2-3 anni, possono essere difficili per i carichi di lavoro di apprendimento automatico:\n\nGli algoritmi ML si evolvono rapidamente: Nuove architetture di modelli, tecniche di training e ottimizzazioni di rete emergono continuamente. Ad esempio, i Transformers sono diventati estremamente popolari nell’NLP negli ultimi anni. Quando un ASIC termina il tapeout, l’architettura ottimale per un carico di lavoro potrebbe essere cambiata.\nI dataset crescono rapidamente: Gli ASIC progettati per determinate dimensioni di modello o tipi di dati possono diventare sottodimensionati rispetto alla domanda. Ad esempio, i modelli di linguaggio naturale stanno aumentando esponenzialmente con più dati e parametri. Un chip progettato per BERT potrebbe non supportare GPT-3.\nLe applicazioni ML cambiano frequentemente: L’attenzione del settore cambia tra visione artificiale, parlato, NLP, sistemi di raccomandazione, ecc. Un ASIC ottimizzato per la classificazione delle immagini potrebbe avere meno rilevanza in pochi anni.\nCicli di progettazione più rapidi con GPU/FPGA: Gli acceleratori programmabili come le GPU possono adattarsi molto più rapidamente aggiornando le librerie software e i framework. I nuovi algoritmi possono essere implementati senza modifiche hardware.\nEsigenze di time-to-market: Ottenere un vantaggio competitivo in ML richiede di sperimentare e implementare rapidamente nuove idee. Attendere diversi anni per un ASIC è diverso da un’iterazione rapida.\n\nIl ritmo dell’innovazione in ML deve essere adattato meglio alla scala temporale pluriennale per lo sviluppo di ASIC. Sono necessari notevoli sforzi ingegneristici per estendere la durata di vita di ASIC tramite architetture modulari, ridimensionamento dei processi, compressione dei modelli e altre tecniche. Tuttavia, la rapida evoluzione di ML rende l’hardware a funzione fissa una sfida.\n\n\nElevati Costi di Progettazione Non Ricorrenti\nI costi fissi per portare un ASIC dalla progettazione alla produzione ad alto volume possono essere molto dispendiosi in termini di capitale, spesso decine di milioni di dollari. La fabbricazione di fotomaschere per il tape-out dei chip in nodi di processo avanzati, il packaging e il lavoro di progettazione una tantum sono costosi. Ad esempio, un solo tape-out del chip da 7 nm potrebbe costare milioni. L’elevato “non-recurring engineering (NRE)” [investimento di progettazione non ricorrente] riduce la fattibilità dell’ASIC ai casi di utilizzo della produzione ad alto volume in cui il costo iniziale può essere ammortizzato.\n\n\nIntegrazione e Programmazione Complesse\nGli ASIC richiedono un ampio lavoro di integrazione software, inclusi driver, compilatori, supporto del sistema operativo e strumenti di debug. Hanno anche bisogno di competenza nel packaging elettrico e termico. Inoltre, programmare in modo efficiente le architetture ASIC può comportare sfide come il partizionamento del carico di lavoro e la pianificazione su molte unità parallele. La natura personalizzata richiede notevoli sforzi di integrazione per trasformare l’hardware grezzo in acceleratori completamente operativi.\nMentre gli ASIC forniscono enormi guadagni di efficienza nelle applicazioni target adattando ogni aspetto della progettazione hardware a un’attività specifica, la loro natura fissa comporta compromessi in termini di flessibilità e costi di sviluppo rispetto agli acceleratori programmabili, che devono essere soppesati in base all’applicazione.\n\n\n\n\n10.3.2 Field-Programmable Gate Array (FPGA)\nGli FPGA sono circuiti integrati programmabili che possono essere riconfigurati per diverse applicazioni. La loro natura personalizzabile offre vantaggi per accelerare gli algoritmi AI rispetto agli ASIC fissi o alle GPU inflessibili. Mentre Google, Meta e NVIDIA stanno valutando di installare gli ASIC nei data center, Microsoft ha distribuito gli FPGA nei suoi data center (Putnam et al. 2014) nel 2011 per servire in modo efficiente diversi carichi di lavoro.\n\nXiong, Siyu, Guoqing Wu, Xitian Fan, Xuan Feng, Zhongcheng Huang, Wei Cao, Xuegong Zhou, et al. 2021. «MRI-based brain tumor segmentation using FPGA-accelerated neural network». BMC Bioinf. 22 (1): 421. https://doi.org/10.1186/s12859-021-04347-6.\nGli FPGA hanno trovato ampia applicazione in vari campi, tra cui l’imaging medico, la robotica e la finanza, dove eccellono nella gestione di attività di machine learning ad alta intensità di calcolo. Nell’imaging medico, un esempio illustrativo è l’applicazione degli FPGA per la segmentazione dei tumori cerebrali, un processo tradizionalmente dispendioso in termini di tempo e soggetto a errori. Rispetto alle implementazioni tradizionali di GPU e CPU, gli FPGA hanno dimostrato rispettivamente miglioramenti delle prestazioni di oltre 5 e 44 volte e guadagni di 11 e 82 volte in termini di efficienza energetica, evidenziando il loro potenziale per applicazioni esigenti (Xiong et al. 2021).\n\nVantaggi\nGli FPGA offrono diversi vantaggi rispetto alle GPU e agli ASIC per accelerare i carichi di lavoro di apprendimento automatico.\n\nFlessibilità Tramite “Reconfigurable Fabric”\nIl vantaggio principale degli FPGA è la capacità di riconfigurare il “fabric” [tessuto] sottostante per implementare architetture personalizzate ottimizzate per diversi modelli, a differenza degli ASIC a funzione fissa. Ad esempio, le società di trading quantitativo utilizzano gli FPGA per accelerare i loro algoritmi perché cambiano frequentemente e il basso costo NRE degli FPGA è più fattibile rispetto acquistare i nuovi ASIC. Figura 10.3 contiene una tabella che confronta tre diversi FPGA.\n\n\n\n\n\n\nFigura 10.3: Confronto di FPGA. Fonte: Gwennap (s.d.).\n\n\nGwennap, Linley. s.d. «Certus-NX Innovates General-Purpose FPGAs».\n\n\nGli FPGA comprendono blocchi di base: blocchi logici configurabili, blocchi RAM e interconnessioni. Vengono fornite una quantità base di queste risorse e gli ingegneri programmano i chip compilando il codice HDL in flussi di bit che riorganizzano la struttura in diverse configurazioni. Questo rende gli FPGA adattabili man mano che gli algoritmi evolvono.\nSebbene gli FPGA possano non raggiungere le massime prestazioni ed efficienza degli ASIC specifici per il carico di lavoro, la loro programmabilità offre maggiore flessibilità man mano che gli algoritmi cambiano. Questa adattabilità rende gli FPGA una scelta interessante per accelerare le applicazioni di machine learning in evoluzione.\n\n\nParallelismo e Pipeline Personalizzati\nLe architetture FPGA possono sfruttare il parallelismo spaziale e il pipelining adattando la progettazione hardware per rispecchiare il parallelismo nei modelli ML. Ad esempio, su una piattaforma FPGA HARPv2 di Intel è possibile suddividere i layer di una rete convoluzionale su elementi di elaborazione separati per massimizzare la produttività. Sugli FPGA sono possibili anche pattern paralleli unici come le valutazioni di “ensemble” ad albero. Pipeline profonde con buffering e flusso di dati ottimizzati possono essere personalizzate in base alla struttura e ai tipi di dati di ogni modello. Questo livello di parallelismo e pipeline su misura non è fattibile sulle GPU.\n\n\nMemoria On-Chip a Bassa Latenza\nGrandi quantità di memoria on-chip ad alta larghezza di banda consentono l’archiviazione localizzata per pesi e attivazioni. Ad esempio, gli FPGA Xilinx Versal contengono 32 MB di blocchi RAM a bassa latenza e interfacce DDR4 a doppio canale per la memoria esterna. Avvicinare fisicamente la memoria alle unità di elaborazione riduce la latenza di accesso. Ciò fornisce significativi vantaggi di velocità rispetto alle GPU che attraversano PCIe (Peripheral Component Interconnect Express) o altri bus di sistema per raggiungere la memoria GDDR6 off-chip.\n\n\nSupporto Nativo per Bassa Precisione\nUn vantaggio fondamentale degli FPGA è la capacità di implementare in modo nativo qualsiasi larghezza di bit per unità aritmetiche, come INT4 o bfloat16, utilizzate nei modelli ML quantizzati. Ad esempio, Intel Stratix 10 NX FPGA ha core INT8 dedicati che possono raggiungere fino a 143 INT8 TOPS (Tera Operations Per Second) a ~1 TOPS/W (Tera Operations Per Second per Watt). TOPS è una misura di prestazioni simile a FLOPS, ma mentre FLOPS misura calcoli in virgola mobile, TOPS misura il numero di operazioni intere che un sistema può eseguire al secondo. Le larghezze di bit inferiori, come INT8 o INT4, aumentano la densità aritmetica e le prestazioni. Gli FPGA possono persino supportare la sintonizzazione a precisione mista o dinamica in fase di esecuzione.\n\n\n\nSvantaggi\n\nThroughput di Picco Inferiore Rispetto agli ASIC\nGli FPGA non possono eguagliare i numeri di throughput grezzi degli ASIC, personalizzati per un modello e una precisione specifici. I sovraccarichi del “fabric” riconfigurabile rispetto all’hardware a funzione fissa comportano prestazioni di picco inferiori. Ad esempio, i pod TPU v5e consentono di connettere fino a 256 chip con oltre 100 PetaOps (Peta Operations Per Second) di prestazioni INT8, mentre gli FPGA possono offrire fino a 143 INT8 TOPS o 286 INT4 TOPS come sull’FPGA Intel Stratix 10 NX; PetaOps rappresenta quadrilioni di operazioni al secondo, mentre TOPS misura trilioni, evidenziando la capacità di elaborazione molto maggiore dei pod TPU rispetto agli FPGA.\nQuesto perché gli FPGA comprendono blocchi di base: blocchi logici configurabili, blocchi RAM e interconnessioni. Vengono forniti con una quantità stabilita di queste risorse. Per programmare gli FPGA, gli ingegneri scrivono codice HDL e lo compilano in flussi di bit che riorganizzano il “fabric”, che ha sovraccarichi intrinseci rispetto a un ASIC appositamente progettato per un calcolo.\n\n\nComplessità di Programmazione\nPer ottimizzare le prestazioni FPGA, gli ingegneri devono programmare le architetture in linguaggi di descrizione hardware di basso livello come Verilog o VHDL. Ciò richiede competenza nella progettazione hardware e cicli di sviluppo più lunghi rispetto a framework software di livello superiore come TensorFlow. Massimizzare l’utilizzo può essere difficile nonostante i progressi nella sintesi di alto livello da C/C++.\n\n\nSovraccarichi di Riconfigurazione\nLa modifica delle configurazioni FPGA richiede il ricaricamento di un nuovo flusso di bit, che ha costi di latenza e dimensioni di archiviazione considerevoli. Ad esempio, la riconfigurazione parziale su FPGA Xilinx può richiedere centinaia di millisecondi. Questo rende impossibile lo scambio dinamico di architetture in tempo reale. L’archiviazione del flusso di bit consuma anche memoria on-chip.\n\n\nGuadagni in diminuzione sui nodi avanzati\nSebbene i nodi di processo più piccoli siano molto vantaggiosi per gli ASIC, offrono meno vantaggi per gli FPGA. A 7 nm e al di sotto, effetti come variazione di processo, vincoli termici e invecchiamento hanno un impatto sproporzionato sulle prestazioni degli FPGA. Anche le spese generali della struttura configurabile riducono i guadagni rispetto agli ASIC a funzione fissa.\n\n\n\n\n10.3.3 Digital Signal Processor (DSP)\nIl primo core di elaborazione del segnale digitale è stato costruito nel 1948 da Texas Instruments (The Evolution of Audio DSPs). Tradizionalmente, i DSP avrebbero avuto una logica per accedere direttamente ai dati digitali/audio nella memoria, eseguire un’operazione aritmetica (moltiplica-addiziona-accumula-MAC era una delle operazioni più comuni) e quindi scrivere il risultato nella memoria. Il DSP avrebbe incluso componenti analogici specializzati per recuperare i dati digitali/audio.\nUna volta entrati nell’era degli smartphone, i DSP hanno iniziato a comprendere attività più sofisticate. Richiedevano Bluetooth, Wi-Fi e connettività cellulare. Anche i media sono diventati molto più complessi. Oggi, è raro avere chip interi dedicati solo al DSP, ma un System on Chip includerebbe DSP e CPU per uso generico. Ad esempio, l’Hexagon Digital Signal Processor di Qualcomm afferma di essere un “processore di livello mondiale con funzionalità sia CPU che DSP per supportare le esigenze di elaborazione profondamente integrate della piattaforma mobile per funzioni sia multimediali che modem”. Google Tensors, il chip nei telefoni Google Pixel, include anche CPU e motori DSP specializzati.\n\nVantaggi\nI DSP offrono vantaggi architettonici in termini di throughput della matematica vettoriale, accesso alla memoria a bassa latenza, efficienza energetica e supporto per diversi tipi di dati, rendendoli adatti all’accelerazione ML embedded.\n\nArchitettura Ottimizzata per la Matematica Vettoriale\nI DSP contengono percorsi dati specializzati, file di registro e istruzioni ottimizzati specificamente per le operazioni di matematica vettoriale comunemente utilizzate nei modelli di apprendimento automatico. Ciò include motori di prodotto scalare, unità MAC e funzionalità SIMD su misura per calcoli vettoriali/matriciali. Ad esempio, il DSP CEVA-XM6 (“Ceva SensPro fonde AI e Vector DSP”) ha unità vettoriali a 512 bit per accelerare le convoluzioni. Questa efficienza sui carichi di lavoro di matematica vettoriale va ben oltre le CPU generiche.\n\n\nMemoria On-Chip a Bassa Latenza\nI DSP integrano grandi quantità di memoria SRAM veloce su chip per conservare i dati localmente per l’elaborazione. Avvicinare fisicamente la memoria alle unità di calcolo riduce la latenza di accesso. Ad esempio, il DSP SHARC+ di Analog contiene 10 MB di SRAM su chip. Questa memoria locale ad alta larghezza di banda offre vantaggi di velocità per le applicazioni in tempo reale.\n\n\nEfficienza Energetica\nI DSP sono progettati per fornire elevate prestazioni per watt su carichi di lavoro di segnali digitali. Percorsi dati efficienti, parallelismo e architetture di memoria consentono trilioni di operazioni matematiche al secondo entro budget di potenza mobili ridotti. Ad esempio, l’Hexagon DSP di Qualcomm può fornire 4 TOPS consumando un numero minimo di watt.\n\n\nSupporto per Matematica a Virgola Mobile e Intera\nA differenza delle GPU che eccellono in precisione singola o dimezzata, i DSP possono supportare nativamente tipi di dati a virgola mobile e intera a 8/16 bit utilizzati nei modelli ML. Alcuni DSP supportano l’accelerazione del prodotto scalare a precisione INT8 per reti neurali quantizzate.\n\n\n\nSvantaggi\nI DSP fanno compromessi architettonici che limitano il throughput di picco, la precisione e la capacità del modello rispetto ad altri acceleratori AI. Tuttavia, i loro vantaggi in termini di efficienza energetica e matematica intera li rendono una valida opzione di edge computing. Quindi, mentre i DSP offrono alcuni vantaggi rispetto alle CPU, presentano anche delle limitazioni per i carichi di lavoro di apprendimento automatico:\n\nThroughput di Picco Inferiore Rispetto ad ASIC/GPU\nI DSP non possono eguagliare il throughput computazionale grezzo delle GPU o degli ASIC personalizzati progettati specificamente per l’apprendimento automatico. Ad esempio, l’ASIC Cloud AI 100 di Qualcomm fornisce 480 TOPS su INT8, mentre il loro DSP Hexagon fornisce 10 TOPS. I DSP non hanno il massiccio parallelismo delle unità GPU SM.\n\n\nPrestazioni a Doppia Precisione più Lente\nLa maggior parte dei DSP deve essere ottimizzata per la virgola mobile di precisione più elevata necessaria in alcuni modelli ML. I loro motori di prodotto scalare si concentrano su INT8/16 e FP32, che forniscono una migliore efficienza energetica. Tuttavia, la produttività in virgola mobile a 64 bit è molto più bassa, il che può limitare l’utilizzo nei modelli che richiedono un’elevata precisione.\n\n\nCapacità del Modello Limitata\nLa limitata memoria on-chip dei DSP limita le dimensioni del modello che possono eseguire. Grandi modelli di deep learning con centinaia di megabyte di parametri supererebbero la capacità delle SRAM on-chip. I DSP sono più adatti per modelli di piccole e medie dimensioni destinati a dispositivi edge.\n\n\nComplessità di Programmazione\nLa programmazione efficiente delle architetture DSP richiede competenza nella programmazione parallela e nell’ottimizzazione dei pattern di accesso ai dati. Le loro microarchitetture specializzate hanno una curva di apprendimento più ripida rispetto ai framework software di alto livello, rendendo lo sviluppo più complesso.\n\n\n\n\n10.3.4 Graphics Processing Unit (GPU)\nIl termine “graphics processing unit” [unità di elaborazione grafica] esiste almeno dagli anni ’80. C’è sempre stata una richiesta di hardware grafico nelle console per videogiochi (elevata richiesta, doveva avere un costo relativamente basso) e nelle simulazioni scientifiche (richiesta inferiore, ma risoluzione più alta, poteva avere un prezzo elevato).\nIl termine è stato reso popolare, tuttavia, nel 1999 quando NVIDIA ha lanciato la GeForce 256, mirando principalmente al settore di mercato dei giochi per PC (Lindholm et al. 2008). Man mano che i giochi per PC diventavano più sofisticati, le GPU NVIDIA diventavano più programmabili. Presto, gli utenti si resero conto che potevano sfruttare questa programmabilità, eseguire vari carichi di lavoro non correlati alla grafica sulle GPU e trarre vantaggio dall’architettura sottostante. E così, alla fine degli anni 2000, le GPU divennero unità di elaborazione grafica per uso generale o GP-GPU.\n\nLindholm, Erik, John Nickolls, Stuart Oberman, e John Montrym. 2008. «NVIDIA Tesla: A Unified Graphics and Computing Architecture». IEEE Micro 28 (2): 39–55. https://doi.org/10.1109/mm.2008.31.\nIn seguito a questo cambiamento, altri importanti attori come Intel con la sua Arc Graphics e AMD con la sua serie Radeon RX hanno anche evoluto le loro GPU per supportare una gamma più ampia di applicazioni oltre al rendering grafico tradizionale. Questa espansione delle capacità delle GPU ha aperto nuove possibilità, in particolare nei campi che richiedono un’enorme potenza di calcolo.\nUn esempio lampante di questo potenziale è la recente ricerca rivoluzionaria condotta da OpenAI (Brown et al. 2020) con GPT-3, un modello di linguaggio con 175 miliardi di parametri. L’addestramento di un modello così massiccio, che avrebbe richiesto mesi su CPU convenzionali, è stato completato in pochi giorni utilizzando potenti GPU, dimostrando l’impatto trasformativo delle GPU nell’accelerazione di complesse attività di apprendimento automatico.\n\nVantaggi\n\nElevata Capacità di Elaborazione\nIl vantaggio principale delle GPU è la loro capacità di eseguire calcoli in virgola mobile paralleli massivi ottimizzati per la computer grafica e l’algebra lineare (Raina, Madhavan, e Ng 2009). Le GPU moderne come la A100 di Nvidia offrono fino a 19,5 teraflop di prestazioni FP32 con 6912 core CUDA e 40 GB di memoria grafica strettamente accoppiati a 1,6 TB/s di larghezza di banda della memoria grafica.\n\nRaina, Rajat, Anand Madhavan, e Andrew Y. Ng. 2009. «Large-scale deep unsupervised learning using graphics processors». In Proceedings of the 26th Annual International Conference on Machine Learning, a cura di Andrea Pohoreckyj Danyluk, Léon Bottou, e Michael L. Littman, 382:873–80. ACM International Conference Proceeding Series. ACM. https://doi.org/10.1145/1553374.1553486.\nQuesta capacità di elaborazione grezza deriva dall’architettura “Streaming Multiprocessor” (SM) altamente parallela, pensata per carichi di lavoro paralleli ai dati (Zhihao Jia, Zaharia, e Aiken 2019). Ogni SM contiene centinaia di core scalari ottimizzati per la matematica float32/64. Con migliaia di SM su un chip, le GPU sono appositamente progettate per la moltiplicazione di matrici e le operazioni vettoriali utilizzate in tutte le reti neurali.\nAd esempio, l’ultima GPU H100 di Nvidia fornisce 4000 TFLOP di FP8, 2000 TFLOP di FP16, 1000 TFLOP di TF32, 67 TFLOP di FP32 e 34 TFLOP di prestazioni di elaborazione FP64, che possono accelerare notevolmente l’addestramento di grandi batch su modelli come BERT, GPT-3 e altre architetture di trasformatori. Il parallelismo scalabile delle GPU è fondamentale per accelerare il deep learning computazionalmente intensivo.\n\n\nEcosistema Software Maturo\nNvidia fornisce ampie librerie di runtime come cuDNN e cuBLAS che sono altamente ottimizzate per primitive di deep learning. Framework come TensorFlow e PyTorch si integrano con queste librerie per abilitare l’accelerazione GPU senza programmazione diretta. Queste librerie sono basate su CUDA, la piattaforma di elaborazione parallela e il modello di programmazione di Nvidia.\nCUDA (Compute Unified Device Architecture) è il framework sottostante che consente a queste librerie di alto livello di interagire con l’hardware della GPU. Fornisce agli sviluppatori un accesso di basso livello alle risorse della GPU, consentendo calcoli e ottimizzazioni personalizzate che sfruttano appieno le capacità di elaborazione parallela della GPU. Utilizzando CUDA, gli sviluppatori possono scrivere software che sfruttano l’architettura della GPU per attività di elaborazione ad alte prestazioni.\nQuesto ecosistema consente di sfruttare rapidamente le GPU ad alto livello tramite Python senza competenze di programmazione GPU. Flussi di lavoro e astrazioni noti forniscono una comoda rampa di accesso per scalare gli esperimenti di deep learning. La maturità del software integra i vantaggi della produttività.\n\n\nAmpia Disponibilità\nLe economie di scala dell’elaborazione grafica rendono le GPU ampiamente accessibili nei data center, nelle piattaforme cloud come AWS e GCP e nelle workstation desktop. La loro disponibilità negli ambienti di ricerca ha fornito una comoda piattaforma di sperimentazione e innovazione nel ML. Ad esempio, quasi tutti i risultati di deep learning all’avanguardia hanno coinvolto l’accelerazione GPU per merito di questa ubiquità. L’ampio accesso integra la maturità del software per rendere le GPU l’acceleratore ML standard.\n\n\nArchitettura Programmabile\nSebbene non siano flessibili come gli FPGA, le GPU offrono programmabilità tramite linguaggi CUDA e shader per personalizzare i calcoli. Gli sviluppatori possono ottimizzare i pattern di accesso ai dati, creare nuove operazioni e regolare le precisioni per modelli e algoritmi in evoluzione.\n\n\n\nSvantaggi\nSebbene le GPU siano diventate l’acceleratore standard per il deep learning, la loro architettura presenta alcuni svantaggi importanti.\n\nMeno Efficienti degli ASIC Custom\nL’affermazione “Le GPU sono meno efficienti degli ASIC” potrebbe scatenare un acceso dibattito nel campo ML/AI e far esplodere questo libro.\nIn genere, le GPU sono percepite come meno efficienti degli ASIC perché questi ultimi sono realizzati su misura per attività specifiche e quindi possono funzionare in modo più efficiente nativamente. Con la loro architettura generica, le GPU sono intrinsecamente più versatili e programmabili, soddisfacendo un ampio spettro di attività computazionali oltre a ML/AI.\nTuttavia, le GPU moderne si sono evolute per includere un supporto hardware specializzato per operazioni AI essenziali, come la moltiplicazione di matrici generalizzata (GEMM) e altre operazioni di matrice, supporto nativo per la quantizzazione e supporto nativo per la potatura, che sono fondamentali per l’esecuzione efficace dei modelli ML. Questi miglioramenti hanno notevolmente migliorato l’efficienza delle GPU per le attività AI al punto che possono rivaleggiare con le prestazioni degli ASIC per determinate applicazioni.\nDi conseguenza, le GPU contemporanee sono convergenti, incorporando capacità specializzate simili ad ASIC all’interno di un framework di elaborazione flessibile e di uso generale. Questa adattabilità ha offuscato i confini tra i due tipi di hardware. Le GPU offrono un forte equilibrio tra specializzazione e programmabilità che si adatta bene alle esigenze dinamiche della ricerca e sviluppo ML/AI.\n\n\nElevate Esigenze di Larghezza di Banda di Memoria\nL’architettura massicciamente parallela richiede un’enorme larghezza di banda di memoria per alimentare migliaia di core. Ad esempio, la GPU Nvidia A100 richiede 1.6 TB/sec per saturare completamente il suo computer. Le GPU si affidano ad ampi bus di memoria a 384 bit per RAM GDDR6 ad alta larghezza di banda, ma anche la GDDR6 più veloce raggiunge il massimo a circa 1 TB/sec. Questa dipendenza dalla DRAM esterna comporta latenza e sovraccarico di potenza.\n\n\nComplessità di Programmazione\nSebbene strumenti come CUDA siano utili, la mappatura e il partizionamento ottimali dei carichi di lavoro ML nell’architettura GPU massivamente parallela rimangono una sfida, il raggiungimento di un utilizzo elevato e della località della memoria richiede una messa a punto di basso livello (Zhe Jia et al. 2018). Astrazioni come TensorFlow possono tralasciare le prestazioni.\n\nJia, Zhe, Marco Maggioni, Benjamin Staiger, e Daniele P. Scarpazza. 2018. «Dissecting the NVIDIA Volta GPU Architecture via Microbenchmarking». ArXiv preprint. https://arxiv.org/abs/1804.06826.\n\n\nMemoria On-Chip Limitata\nLe GPU hanno cache di memoria on-chip relativamente piccole rispetto ai grandi requisiti di working set dei modelli ML durante l’addestramento. Si basano su un accesso ad alta larghezza di banda alla DRAM esterna, che gli ASIC riducono al minimo con una grande SRAM on-chip.\n\n\nArchitettura Fissa\nA differenza degli FPGA, l’architettura fondamentale della GPU non può essere modificata dopo la produzione. Questo vincolo limita l’adattamento a nuovi carichi di lavoro o layer ML. Il confine CPU-GPU crea anche overhead di spostamento dei dati.\n\n\n\n\n10.3.5 Central Processing Unit (CPU)\nIl termine CPU ha una lunga storia che risale al 1955 (Weik 1955) mentre la prima CPU a microprocessore, l’Intel 4004, è stata inventata nel 1971 (Chi ha inventato il microprocessore?). I compilatori traducono linguaggi di programmazione di alto livello come Python, Java o C per assemblare istruzioni (x86, ARM, RISC-V, ecc.) che le CPU devono elaborare. Il set di istruzioni che una CPU comprende è chiamato “instruction set architecture” (ISA), che definisce i comandi che il processore può eseguire direttamente. Deve essere concordato sia dall’hardware che dal software ci gira sopra.\n\nWeik, Martin H. 1955. A Survey of Domestic Electronic Digital Computing Systems. Ballistic Research Laboratories.\nUna panoramica degli sviluppi significativi nelle CPU:\n\nEra del Single-core (anni ’50-2000): Questa era è nota per i miglioramenti microarchitettonici aggressivi. Tecniche come l’esecuzione speculativa (esecuzione di un’istruzione prima che quella precedente fosse finita), “out-of-order execution” [esecuzione fuori ordine] (riordinamento delle istruzioni per renderle più efficaci) e “wider issue widths” [larghezze di emissione più ampie] (esecuzione di più istruzioni contemporaneamente) sono state implementate per aumentare la produttività delle istruzioni. Anche il termine “System on Chip” ha avuto origine in questa era, poiché diversi componenti analogici (componenti progettati con transistor) e componenti digitali (componenti progettati con linguaggi di descrizione hardware mappati su transistor) sono stati inseriti sulla stessa piattaforma per realizzare un’attività.\nEra Multicore (anni 2000): Guidata dalla diminuzione della legge di Moore, questa è caratterizzata dall’aumento del numero di core all’interno di una CPU. Ora, le attività possono essere suddivise su più core diversi, ognuno con il proprio percorso dati e unità di controllo. Molti dei problemi di quest’epoca riguardavano come condividere determinate risorse, quali risorse condividere e come mantenere coerenza e consistenza in tutti i core.\nUn Mare di acceleratori (anni 2010): Ancora una volta, spinta dalla diminuzione della legge di Moore, quest’epoca è caratterizzata dal delegare le attività più complicate su acceleratori (widget) collegati al datapath principale nelle CPU. È comune vedere acceleratori dedicati a vari carichi di lavoro di intelligenza artificiale, nonché elaborazione di immagini/digitali e crittografia. In queste progettazioni, le CPU sono spesso descritte più come giudici, che decidono quali attività devono essere elaborate piuttosto che eseguire l’elaborazione stessa. Qualsiasi attività potrebbe comunque essere eseguita sulla CPU anziché sugli acceleratori, ma la CPU sarebbe generalmente più lenta. Tuttavia, il costo di progettazione e programmazione dell’acceleratore è diventato un ostacolo non banale che ha suscitato interesse per le librerie specifiche per la progettazione (DSL).\nPresenza nei data center: Sebbene sentiamo spesso dire che le GPU dominano il mercato dei data center, le CPU sono comunque adatte per attività che non possiedono intrinsecamente un elevato grado di parallelismo. Le CPU spesso gestiscono attività seriali e di piccole dimensioni e coordinano il data center.\nSull’edge: Dati i vincoli più rigidi sulle risorse sull’edge, le CPU edge spesso implementano solo un sottoinsieme delle tecniche sviluppate nell’era single-core perché queste ottimizzazioni tendono a essere pesanti in termini di consumo di energia e area. Le CPU edge mantengono comunque un datapath relativamente semplice con capacità di memoria limitate.\n\nTradizionalmente, le CPU sono state sinonimo di elaborazione generica, un termine che è cambiato anche perché il carico di lavoro “medio” che un consumatore esegue cambia nel tempo. Ad esempio, i componenti in virgola mobile erano un tempo considerati riservati alla “elaborazione scientifica”, di solito venivano implementati come un coprocessore (un componente modulare che funzionava con il datapath) e raramente distribuiti ai consumatori medi. Confrontate questo atteggiamento con quello odierno, in cui le FPU sono integrate in ogni datapath.\n\nVantaggi\nSebbene la produttività in sé sia limitata, le CPU per uso generico offrono vantaggi pratici di accelerazione AI.\n\nProgrammabilità Generale\nLe CPU supportano carichi di lavoro diversi oltre al ML, offrendo una programmabilità flessibile per uso generico. Questa versatilità deriva dai loro set di istruzioni standardizzati e dagli ecosistemi di compilatori maturi, che consentono di eseguire qualsiasi applicazione, dai database e server Web alle pipeline analitiche (Hennessy e Patterson 2019).\n\nHennessy, John L., e David A. Patterson. 2019. «A new golden age for computer architecture». Commun. ACM 62 (2): 48–60. https://doi.org/10.1145/3282307.\nQuesto evita la necessità di acceleratori ML dedicati e consente di sfruttare l’infrastruttura basata su CPU esistenti per la distribuzione ML di base. Ad esempio, i server X86 di fornitori come Intel e AMD possono eseguire framework ML comuni utilizzando pacchetti Python e TensorFlow insieme ad altri carichi di lavoro aziendali.\n\n\nEcosistema Software Maturo\nPer decenni, librerie matematiche altamente ottimizzate come BLAS, LAPACK e FFTW hanno sfruttato istruzioni vettorializzate e multithreading su CPU (Dongarra 2009). I principali framework ML come PyTorch, TensorFlow e SciKit-Learn sono progettati per integrarsi perfettamente con questi kernel matematici di CPU.\n\nDongarra, Jack J. 2009. «The evolution of high performance computing on system z». IBM J. Res. Dev. 53: 3–4.\nI fornitori di hardware come Intel e AMD forniscono anche librerie di basso livello per ottimizzare completamente le prestazioni per primitive di deep learning (accelerazione dell’inferenza AI su CPU). Questo ecosistema software robusto e maturo consente di distribuire rapidamente ML su infrastrutture di CPU esistenti.\n\n\nAmpia Disponibilità\nLe economie di scala della produzione di CPU, guidate dalla domanda in molti mercati come PC, server e dispositivi mobili, le rendono disponibili ovunque. Le CPU Intel, ad esempio, hanno alimentato la maggior parte dei server per decenni (Ranganathan 2011). Questa ampia disponibilità nei data center riduce i costi hardware per l’implementazione di ML di base.\n\nRanganathan, Parthasarathy. 2011. «From Microprocessors to Nanostores: Rethinking Data-Centric Systems». Computer 44 (1): 39–48. https://doi.org/10.1109/mc.2011.18.\nAnche i piccoli dispositivi embedded in genere integrano una certa CPU, consentendo l’inferenza edge. L’ubiquità riduce la necessità di acquistare acceleratori ML specializzati in molte situazioni.\n\n\nBasso Consumo per L’inferenza\nOttimizzazioni come ARM Neon e le estensioni vettoriali Intel AVX forniscono un throughput di numeri interi e in virgola mobile a basso consumo ottimizzato per carichi di lavoro “a raffica” come l’inferenza (Ignatov et al. 2018). Sebbene più lenta delle GPU, l’inferenza CPU può essere implementata in ambienti con vincoli energetici. Ad esempio, le CPU Cortex-M di ARM ora offrono oltre 1 TOPS di prestazioni INT8 sotto 1 W, consentendo l’individuazione di parole chiave e applicazioni di visione su dispositivi edge (ARM).\n\n\n\nSvantaggi\nPur offrendo alcuni vantaggi, le CPU per uso generico presentano anche delle limitazioni per i carichi di lavoro AI.\n\nThroughput Inferiore Rispetto agli Acceleratori\nLe CPU non dispongono delle architetture specializzate per l’elaborazione parallela massiva che GPU e altri acceleratori forniscono. Il loro design per uso generico riduce il throughput computazionale per le operazioni matematiche altamente parallelizzabili comuni nei modelli ML (N. P. Jouppi et al. 2017a).\n\nJouppi, Norman P., Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal, Raminder Bajwa, Sarah Bates, et al. 2017a. «In-Datacenter Performance Analysis of a Tensor Processing Unit». In Proceedings of the 44th Annual International Symposium on Computer Architecture, 1–12. ISCA ’17. New York, NY, USA: ACM. https://doi.org/10.1145/3079856.3080246.\n\n\nNon Ottimizzato per il Parallelismo dei Dati\nLe architetture delle CPU non sono specificamente ottimizzate per i carichi di lavoro paralleli dei dati inerenti all’AI (Sze et al. 2017). Assegnano un’area di silicio sostanziale alla decodifica delle istruzioni, all’esecuzione speculativa, alla memorizzazione nella cache e al controllo del flusso che fornisce pochi vantaggi per le operazioni su array utilizzate nelle reti neurali (accelerazione dell’inferenza AI sulle CPU). Tuttavia, le CPU moderne sono dotate di istruzioni vettoriali come AVX-512 specificamente per accelerare determinate operazioni chiave come la moltiplicazione matriciale.\nI multiprocessori di streaming GPU, ad esempio, dedicano la maggior parte dei transistor alle unità a virgola mobile anziché alla logica di predizione di diramazione complessa. Questa specializzazione consente un utilizzo molto più elevato per la matematica ML.\n\n\nMaggiore Latenza della Memoria\nLe CPU soffrono di una latenza maggiore nell’accesso alla memoria principale rispetto alle GPU e ad altri acceleratori (DDR). Tecniche come il tiling e il caching possono aiutare, ma la separazione fisica dalla RAM off-chip crea colli di bottiglia nei carichi di lavoro ML ad alta intensità di dati. Ciò sottolinea la necessità di architetture di memoria specializzate nell’hardware ML.\n\n\nInefficienza Energetica in Caso di Carichi di Lavoro Pesanti\nSebbene sia adatto per l’inferenza intermittente, il mantenimento di una produttività quasi di picco per l’addestramento comporta un consumo energetico inefficiente sulle CPU, in particolare sulle CPU mobili (Ignatov et al. 2018). Gli acceleratori ottimizzano esplicitamente il flusso di dati, la memoria e il calcolo per carichi di lavoro ML sostenuti. Le CPU sono inefficienti dal punto di vista energetico per l’addestramento di modelli di grandi dimensioni.\n\n\n\n\n10.3.6 Confronto\nTabella 10.2 confronta i diversi tipi di funzionalità hardware.\n\n\n\nTabella 10.2: Confronto di diversi acceleratori hardware per carichi di lavoro AI.\n\n\n\n\n\n\n\n\n\n\n\nAcceleratore\nDescrizione\nPrincipali vantaggi\nPrincipali svantaggi\n\n\n\n\nASIC\nIC personalizzati progettati per carichi di lavoro target come l’inferenza AI\n\nMassimizza le prestazioni/watt.\nOttimizzato per le operazioni tensoriali\nMemoria on-chip a bassa latenza\n\n\nL’architettura fissa manca di flessibilità\nElevato costo NRE\nLunghi cicli di progettazione\n\n\n\nFPGA\nFabric riconfigurabile con logica programmabile e routing\n\nArchitettura flessibile\nAccesso alla memoria a bassa latenza\n\n\nPrestazioni/watt inferiori rispetto agli ASIC\nProgrammazione complessa\n\n\n\nGPU\nOriginariamente per la grafica, ora utilizzate per l’accelerazione della rete neurale\n\nElevata produttività\nScalabilità parallela\nEcosistema software con CUDA\n\n\nNon efficienti dal punto di vista energetico come gli ASIC\nRichiede un’elevata larghezza di banda della memoria\n\n\n\nCPU\nProcessori per uso generico\n\nProgrammabilità\nDisponibilità ubiqua\n\n\nPrestazioni inferiori per carichi di lavoro AI\n\n\n\n\n\n\n\nIn generale, le CPU forniscono una baseline prontamente disponibile, le GPU offrono un’accelerazione ampiamente accessibile, gli FPGA offrono programmabilità e gli ASIC massimizzano l’efficienza per funzioni fisse. La scelta ottimale dipende dalla scala, dal costo, dalla flessibilità e da altri requisiti dell’applicazione target.\nSebbene inizialmente sviluppati per l’implementazione del data center, Google ha anche profuso notevoli sforzi nello sviluppo di TPU Edge. Questi TPU Edge mantengono l’ispirazione degli array sistolici [https://it.wikipedia.org/wiki/Array_sistolico], ma sono adattati alle risorse limitate accessibili all’edge.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Accelerazione IA</span>"
    ]
  },
  {
    "objectID": "contents/core/hw_acceleration/hw_acceleration.it.html#co-progettazione-hardware-software",
    "href": "contents/core/hw_acceleration/hw_acceleration.it.html#co-progettazione-hardware-software",
    "title": "10  Accelerazione IA",
    "section": "10.4 Co-Progettazione Hardware-Software",
    "text": "10.4 Co-Progettazione Hardware-Software\nLa co-progettazione hardware-software si basa sul principio secondo cui i sistemi AI raggiungono prestazioni ed efficienza ottimali quando i componenti hardware e software sono progettati in stretta integrazione. Ciò comporta un ciclo di progettazione iterativo e collaborativo in cui l’architettura hardware e gli algoritmi software vengono sviluppati e perfezionati contemporaneamente con un feedback continuo tra i team.\nAd esempio, un nuovo modello di rete neurale può essere prototipato su una piattaforma di accelerazione basata su FPGA per ottenere dati sulle prestazioni reali all’inizio del processo di progettazione. Questi risultati forniscono un feedback ai progettisti hardware su potenziali ottimizzazioni e agli sviluppatori software su perfezionamenti del modello o framework per sfruttare meglio le capacità hardware. Questo livello di sinergia è difficile da raggiungere con la pratica comune di software sviluppato in modo indipendente per essere distribuito su hardware fisso.\nLa progettazione congiunta è fondamentale per i sistemi di intelligenza artificiale embedded che affrontano notevoli vincoli di risorse come budget di potenza ridotti, memoria e capacità di elaborazione limitate e requisiti di latenza in tempo reale. La stretta integrazione tra sviluppatori di algoritmi e architetti hardware aiuta a sbloccare le ottimizzazioni in tutto lo stack per soddisfare queste restrizioni. Le tecniche di abilitazione includono miglioramenti algoritmici come la ricerca e il pruning [potatura] dell’architettura neurale e progressi hardware come flussi di dati specializzati e gerarchie di memoria.\nRiunendo la progettazione hardware e software, anziché svilupparli separatamente, è possibile realizzare ottimizzazioni olistiche che massimizzano prestazioni ed efficienza. Le sezioni successive forniscono maggiori dettagli su specifici approcci di progettazione congiunta.\n\n10.4.1 La Necessità della Progettazione Congiunta\nDiversi fattori chiave rendono essenziale un approccio di progettazione congiunta hardware-software collaborativo per la creazione di sistemi di intelligenza artificiale efficienti.\n\nAumento delle Dimensioni e della Complessità del Modello\nI modelli di intelligenza artificiale all’avanguardia sono cresciuti rapidamente in termini di dimensioni, abilitati dai progressi nella progettazione dell’architettura neurale e dalla disponibilità di grandi set di dati. Ad esempio, il modello linguistico GPT-3 contiene 175 miliardi di parametri (Brown et al. 2020), che richiedono enormi risorse di calcolo per l’addestramento. Questa esplosione nella complessità del modello richiede una progettazione congiunta per sviluppare hardware e algoritmi efficienti in tandem. Tecniche come la compressione del modello (Cheng et al. 2018) e la quantizzazione devono essere co-ottimizzate con l’architettura hardware.\n\nBrown, Tom B., Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, et al. 2020. «Language Models are Few-Shot Learners». In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, a cura di Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, e Hsuan-Tien Lin. https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html.\n\nCheng, Yu, Duo Wang, Pan Zhou, e Tao Zhang. 2018. «Model Compression and Acceleration for Deep Neural Networks: The Principles, Progress, and Challenges». IEEE Signal Process Mag. 35 (1): 126–36. https://doi.org/10.1109/msp.2017.2765695.\n\n\nVincoli della Distribuzione Embedded\nL’implementazione di applicazioni AI su dispositivi edge come telefoni cellulari o elettrodomestici intelligenti introduce vincoli significativi su energia, memoria e area di silicio (Sze et al. 2017). Abilitare l’inferenza in tempo reale con queste restrizioni richiede la co-esplorazione di ottimizzazioni hardware come flussi di dati specializzati e compressione con progettazione efficiente di reti neurali e tecniche di potatura. La co-progettazione massimizza le prestazioni entro rigidi vincoli di distribuzione.\n\n\nRapida Evoluzione degli Algoritmi AI\nL’intelligenza artificiale si sta evolvendo rapidamente, con nuove architetture di modelli, metodologie di training e framework software che emergono costantemente. Ad esempio, i Transformers sono diventati di recente molto popolari per l’NLP (Young et al. 2018). Per tenere il passo con queste innovazioni algoritmiche è necessaria una progettazione congiunta hardware-software per adattare le piattaforme ed evitare rapidamente il debito tecnico accumulato.\n\nYoung, Tom, Devamanyu Hazarika, Soujanya Poria, e Erik Cambria. 2018. «Recent Trends in Deep Learning Based Natural Language Processing [Review Article]». IEEE Comput. Intell. Mag. 13 (3): 55–75. https://doi.org/10.1109/mci.2018.2840738.\n\n\nInterazioni Complesse Hardware-Software\nMolte interazioni e compromessi sottili tra scelte architettoniche hardware e ottimizzazioni software hanno un impatto significativo sull’efficienza complessiva. Ad esempio, tecniche come il partizionamento tensoriale e il batching influenzano il parallelismo e i pattern di accesso ai dati influenzano l’utilizzo della memoria. La progettazione congiunta fornisce una prospettiva multilivello per svelare queste dipendenze.\n\n\nNecessità di Specializzazione\nI carichi di lavoro dell’intelligenza artificiale traggono vantaggio da operazioni specializzate come matematica a bassa precisione e gerarchie di memoria personalizzate. Ciò motiva l’incorporazione di hardware personalizzato su misura per algoritmi di reti neurali piuttosto che affidarsi esclusivamente a software flessibile in esecuzione su hardware generico (Sze et al. 2017). Tuttavia, lo stack software deve mirare esplicitamente alle operazioni hardware personalizzate per realizzare i vantaggi.\n\n\nRichiesta di Maggiore Efficienza\nCon la crescente complessità del modello, si verificano rendimenti decrescenti e spese generali derivanti dall’ottimizzazione del solo hardware o software in isolamento (Putnam et al. 2014). Si presentano inevitabili compromessi che richiedono un’ottimizzazione globale su più livelli. La progettazione congiunta di hardware e software fornisce grandi guadagni di efficienza composti.\n\nPutnam, Andrew, Adrian M. Caulfield, Eric S. Chung, Derek Chiou, Kypros Constantinides, John Demme, Hadi Esmaeilzadeh, et al. 2014. «A reconfigurable fabric for accelerating large-scale datacenter services». ACM SIGARCH Computer Architecture News 42 (3): 13–24. https://doi.org/10.1145/2678373.2665678.\n\n\n\n10.4.2 Principi di Progettazione Congiunta Hardware-Software\nL’architettura hardware e lo stack software devono essere strettamente integrati e co-ottimizzati per creare sistemi di intelligenza artificiale efficienti e ad alte prestazioni. Nessuno dei due può essere progettato in isolamento; massimizzare le loro sinergie richiede un approccio olistico noto come progettazione congiunta hardware-software.\nL’obiettivo principale è adattare le capacità hardware in modo che corrispondano agli algoritmi e ai carichi di lavoro eseguiti dal software. Ciò richiede un ciclo di feedback tra architetti hardware e sviluppatori software per convergere su soluzioni ottimizzate. Diverse tecniche consentono un’efficace co-progettazione:\n\nOttimizzazione Software Consapevole dell’Hardware\nLo stack software può essere ottimizzato per sfruttare meglio le capacità hardware:\n\nParallelismo: Parallelizzare i calcoli matriciali come convoluzione o layer di attenzione per massimizzare la produttività sui motori vettoriali.\nOttimizzazione della Memoria: Ottimizzare i layout dei dati per migliorare la località della cache in base alla profilazione hardware. Ciò massimizza il riutilizzo e riduce al minimo l’accesso DRAM costoso.\nCompressione: Utilizzare la sparsity [diradazione] nei modelli per ridurre lo spazio di archiviazione e risparmiare sui calcoli tramite operazioni di zero-skipping.\nOperazioni Personalizzate: Incorporare operazioni specializzate come INT4 a bassa precisione o bfloat16 nei modelli per sfruttare al meglio il supporto hardware dedicato.\nMappatura del Flusso di Dati: Mappare esplicitamente le fasi del modello alle unità di calcolo per ottimizzare lo spostamento dei dati sull’hardware.\n\n\n\nSpecializzazione Hardware Algorithm-Driven\nL’hardware può essere adattato alle caratteristiche degli algoritmi ML:\n\nTipi di Dati Personalizzati: Supportare INT8/4 o bfloat16 a bassa precisione nell’hardware per una maggiore densità aritmetica.\nMemoria su Chip: Aumentare la larghezza di banda SRAM e ridurre la latenza di accesso per adattarla ai pattern di accesso alla memoria del modello.\nOperazioni Specifiche del Dominio: Aggiungere unità hardware per funzioni ML chiave come FFT o moltiplicazione di matrici per ridurre latenza ed energia.\nProfilazione del Modello: Utilizzare la simulazione e la profilazione del modello per identificare hotspot computazionali e ottimizzare l’hardware.\n\nLa chiave è il feedback collaborativo: le informazioni dalla profilazione dell’hardware guidano le ottimizzazioni del software, mentre i progressi algoritmici informano la specializzazione dell’hardware. Questo miglioramento reciproco fornisce guadagni di efficienza moltiplicativa rispetto agli sforzi isolati.\n\n\nCo-esplorazione Algoritmo-Hardware\nUna potente tecnica di co-progettazione prevede l’esplorazione congiunta di innovazioni nelle architetture di reti neurali e nella progettazione custom dell’hardware. A powerful co-design technique involves jointly exploring innovations in neural network architectures and custom hardware design. Ciò consente di trovare abbinamenti ideali su misura per i rispettivi punti di forza (Sze et al. 2017).\n\nSze, Vivienne, Yu-Hsin Chen, Tien-Ju Yang, e Joel S. Emer. 2017. «Efficient Processing of Deep Neural Networks: A Tutorial and Survey». Proc. IEEE 105 (12): 2295–2329. https://doi.org/10.1109/jproc.2017.2761740.\n\nHoward, Andrew G., Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, e Hartwig Adam. 2017. «MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications». ArXiv preprint. https://arxiv.org/abs/1704.04861.\n\nJacob, Benoit, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard, Hartwig Adam, e Dmitry Kalenichenko. 2018. «Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference». In 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2704–13. IEEE. https://doi.org/10.1109/cvpr.2018.00286.\n\nGale, Trevor, Erich Elsen, e Sara Hooker. 2019. «The state of sparsity in deep neural networks». ArXiv preprint abs/1902.09574. https://arxiv.org/abs/1902.09574.\n\nMishra, Asit K., Jorge Albericio Latorre, Jeff Pool, Darko Stosic, Dusan Stosic, Ganesh Venkatesh, Chong Yu, e Paulius Micikevicius. 2021. «Accelerating Sparse Deep Neural Networks». CoRR abs/2104.08378. https://arxiv.org/abs/2104.08378.\nAd esempio, il passaggio ad architetture mobili come MobileNets (Howard et al. 2017) è stato guidato dai vincoli dei dispositivi edge come dimensioni del modello e latenza. La quantizzazione (Jacob et al. 2018) e le tecniche di pruning [potatura] (Gale, Elsen, e Hooker 2019) che hanno reso questi modelli efficienti sono diventate possibili grazie ad acceleratori hardware con supporto nativo per interi a bassa precisione e supporto per potatura (Mishra et al. 2021).\nI modelli basati sull’attenzione hanno prosperato su GPU e ASIC massivamente paralleli, dove il loro calcolo si mappa bene nello spazialmente, al contrario delle architetture RNN, che si basano sull’elaborazione sequenziale. La co-evoluzione di algoritmi e hardware ha evidenziato nuove capacità.\nUna co-esplorazione efficace richiede una stretta collaborazione tra ricercatori di algoritmi e architetti hardware. La prototipazione rapida su FPGA (C. Zhang et al. 2015) o simulatori di intelligenza artificiale specializzati consente una rapida valutazione di diverse coppie di architetture di modelli e progetti hardware pre-silicio.\n\nZhang, Chen, Peng Li, Guangyu Sun, Yijin Guan, Bingjun Xiao, e Jason Optimizing Cong. 2015. «FPGA-based Accelerator Design for Deep Convolutional Neural Networks Proceedings of the 2015 ACM». In SIGDA International Symposium on Field-Programmable Gate Arrays-FPGA, 15:161–70.\nAd esempio, l’architettura TPU di Google si è evoluta con ottimizzazioni verso i modelli TensorFlow per massimizzare le prestazioni sulla classificazione delle immagini. Questo stretto ciclo di feedback ha prodotto modelli su misura per la TPU che sarebbero stati improbabili in isolamento.\nGli studi hanno mostrato guadagni di prestazioni ed efficienza da 2 a 5 volte superiori con la co-esplorazione algoritmo-hardware rispetto agli sforzi isolati di ottimizzazione di algoritmi o hardware (Suda et al. 2016). Parallelizzare lo sviluppo congiunto riduce anche i “time-to-deployment” [tempi di distribuzione].\n\nSuda, Naveen, Vikas Chandra, Ganesh Dasika, Abinash Mohanty, Yufei Ma, Sarma Vrudhula, Jae-sun Seo, e Yu Cao. 2016. «Throughput-Optimized OpenCL-based FPGA Accelerator for Large-Scale Convolutional Neural Networks». In Proceedings of the 2016 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays, 16–25. ACM. https://doi.org/10.1145/2847263.2847276.\nNel complesso, esplorare le strette interdipendenze tra innovazione del modello e progressi hardware crea opportunità che devono essere visibili quando vengono affrontate in sequenza. Questa progettazione sinergica congiunta produce soluzioni maggiori della somma delle loro parti.\n\n\n\n10.4.3 Sfide\nSebbene la progettazione collaborativa possa migliorare l’efficienza, l’adattabilità e il time-to-market, presenta anche sfide ingegneristiche e organizzative.\n\nAumento dei Costi di Prototipazione\nÈ richiesta una prototipazione più estesa per valutare diverse accoppiate hardware-software. La necessità di prototipi rapidi e iterativi su FPGA o emulatori aumenta il sovraccarico della validazione. Ad esempio, Microsoft ha scoperto che erano necessari più prototipi per la progettazione collaborativa di un acceleratore AI rispetto alla progettazione sequenziale (Fowers et al. 2018).\n\nFowers, Jeremy, Kalin Ovtcharov, Michael Papamichael, Todd Massengill, Ming Liu, Daniel Lo, Shlomi Alkalay, et al. 2018. «A Configurable Cloud-Scale DNN Processor for Real-Time AI». In 2018 ACM/IEEE 45th Annual International Symposium on Computer Architecture (ISCA), 1–14. IEEE; IEEE. https://doi.org/10.1109/isca.2018.00012.\n\n\nOstacoli Organizzativi e di Team\nLa progettazione collaborativa richiede uno stretto coordinamento tra gruppi hardware e software tradizionalmente scollegati. Ciò potrebbe causare problemi di comunicazione o priorità e pianificazioni non allineate. Anche la navigazione di diversi flussi di lavoro di progettazione è impegnativa. Potrebbe esistere una certa inerzia organizzativa nell’adottare pratiche integrate.\n\n\nComplessità di Simulazione e Modellazione\nCatturare interazioni sottili tra layer hardware e software per la simulazione e la modellazione congiunte aggiunge una complessità significativa. Le astrazioni complete “cross-layer” sono difficili da costruire quantitativamente prima dell’implementazione, rendendo più difficile quantificare in anticipo le ottimizzazioni olistiche.\n\n\nRischi di Eccessiva Specializzazione\nUna progettazione congiunta rigorosa comporta il rischio di adattare eccessivamente le ottimizzazioni agli algoritmi correnti, sacrificando la generalità. Ad esempio, l’hardware ottimizzato esclusivamente per i modelli Transformer potrebbe avere prestazioni inferiori con le tecniche future. Mantenere la flessibilità richiede lungimiranza.\n\n\nProblemi sui Cambiamenti\nGli ingegneri che hanno familiarità con le consolidate pratiche di progettazione hardware o software discrete potrebbero accettare solo flussi di lavoro collaborativi familiari. Nonostante i vantaggi a lungo termine, i progetti potrebbero incontrare attriti nella transizione alla progettazione congiunta.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Accelerazione IA</span>"
    ]
  },
  {
    "objectID": "contents/core/hw_acceleration/hw_acceleration.it.html#software-per-hardware-ai",
    "href": "contents/core/hw_acceleration/hw_acceleration.it.html#software-per-hardware-ai",
    "title": "10  Accelerazione IA",
    "section": "10.5 Software per Hardware AI",
    "text": "10.5 Software per Hardware AI\nAcceleratori hardware specializzati come GPU, TPU e FPGA sono essenziali per fornire applicazioni di intelligenza artificiale ad alte prestazioni. Tuttavia, è necessario un ampio stack software per sfruttare efficacemente queste piattaforme hardware, che coprano l’intero ciclo di vita di sviluppo e distribuzione. Framework e librerie costituiscono la spina dorsale dell’hardware AI, offrendo set di codice, algoritmi e funzioni pre-costruiti e robusti, specificamente ottimizzati per eseguire varie attività AI su hardware diversi. Sono progettati per semplificare le complessità dell’utilizzo dell’hardware da zero, che può richiedere molto tempo ed essere soggetto a errori. Il software svolge un ruolo importante:\n\nFornendo astrazioni di programmazione e modelli come CUDA e OpenCL per mappare i calcoli sugli acceleratori.\nIntegrando gli acceleratori in framework di deep learning popolari come TensorFlow e PyTorch.\nOttimizzando l’intero stack hardware-software con compilatori e tool.\nCon piattaforme di simulazione per modellare insieme hardware e software.\nCon l’infrastruttura per gestire la distribuzione sugli acceleratori.\n\nQuesto vasto ecosistema software è importante quanto l’hardware nel fornire applicazioni AI performanti ed efficienti. Questa sezione fornisce una panoramica degli strumenti disponibili a ogni livello dello stack per consentire agli sviluppatori di creare ed eseguire sistemi AI basati sull’accelerazione hardware.\n\n10.5.1 Modelli di Programmazione\nI modelli di programmazione forniscono astrazioni per mappare calcoli e dati su acceleratori hardware eterogenei:\n\nCUDA: Modello di programmazione parallela di Nvidia per sfruttare le GPU utilizzando estensioni a linguaggi come C/C++. Consente di avviare kernel su core GPU (Luebke 2008).\nOpenCL: Standard aperto per scrivere programmi che spaziano tra CPU, GPU, FPGA e altri acceleratori. Specifica un framework di elaborazione eterogeneo (Munshi 2009).\nOpenGL/WebGL: Interfacce di programmazione grafica 3D in grado di mappare codice generico su core GPU (Segal e Akeley 1999).\nVerilog/VHDL: “Hardware description languages (HDL)” [Linguaggi di descrizione hardware] utilizzati per configurare FPGA come acceleratori AI specificando circuiti digitali (Gannot e Ligthart 1994).\nTVM: Un framework di compilazione che fornisce un frontend Python per ottimizzare e mappare modelli di deep learning su diversi backend hardware (Chen et al. 2018).\n\n\nLuebke, David. 2008. «CUDA: Scalable parallel programming for high-performance scientific computing». In 2008 5th IEEE International Symposium on Biomedical Imaging: From Nano to Macro, 836–38. IEEE. https://doi.org/10.1109/isbi.2008.4541126.\n\nMunshi, Aaftab. 2009. «The OpenCL specification». In 2009 IEEE Hot Chips 21 Symposium (HCS), 1–314. IEEE. https://doi.org/10.1109/hotchips.2009.7478342.\n\nSegal, Mark, e Kurt Akeley. 1999. «The OpenGL graphics system: A specification (version 1.1)».\n\nGannot, G., e M. Ligthart. 1994. «Verilog HDL based FPGA design». In International Verilog HDL Conference, 86–92. IEEE. https://doi.org/10.1109/ivc.1994.323743.\n\nChen, Tianqi, Thierry Moreau, Ziheng Jiang, Lianmin Zheng, Eddie Yan, Haichen Shen, Meghan Cowan, et al. 2018. «TVM: An automated End-to-End optimizing compiler for deep learning». In 13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18), 578–94.\nLe sfide principali includono l’espressione del parallelismo, la gestione della memoria tra dispositivi e l’abbinamento di algoritmi alle capacità hardware. Le astrazioni devono bilanciare la portabilità con la possibilità di personalizzazione hardware. I modelli di programmazione consentono agli sviluppatori di sfruttare gli acceleratori senza competenze hardware. Questi dettagli sono discussi nella sezione AI frameworks section.\n\n\n\n\n\n\nEsercizio 10.1: Software per hardware AI - TVM\n\n\n\n\n\nAbbiamo imparato che l’hardware AI sofisticato ha bisogno di un software speciale per fare magie. TVM è come un traduttore super intelligente, che trasforma il codice in istruzioni che gli acceleratori capiscono. In questo Colab, useremo TVM per creare un acceleratore finto chiamato VTA che esegue la moltiplicazione di matrici super velocemente. Pronti a vedere come il software alimenta l’hardware?\n\n\n\n\n\n\n10.5.2 Librerie e Runtime\nLibrerie e runtime specializzati forniscono astrazioni software per accedere e massimizzare l’utilizzo degli acceleratori AI:\n\nLibrerie Matematiche: Implementazioni altamente ottimizzate di primitive di algebra lineare come GEMM, FFT, convoluzioni, ecc., su misura per l’hardware target. Nvidia cuBLAS, Intel MKL e librerie di elaborazione Arm sono esempi.\nIntegrazioni di Framework: Librerie per accelerare framework di deep learning come TensorFlow, PyTorch e MXNet su hardware supportato. Ad esempio, cuDNN accelera le CNN sulle GPU Nvidia.\nRuntime: Software per gestire l’esecuzione dell’acceleratore, tra cui pianificazione, sincronizzazione, gestione della memoria e altre attività. Nvidia TensorRT è un ottimizzatore di inferenza e runtime.\nDriver e firmware: Software di basso livello per interfacciarsi con l’hardware, inizializzare i dispositivi e gestire l’esecuzione. Fornitori come Xilinx forniscono driver per le loro schede acceleratrici.\n\nAd esempio, gli integratori PyTorch utilizzano librerie cuDNN e cuBLAS per accelerare l’addestramento sulle GPU Nvidia. Il runtime TensorFlow XLA ottimizza e compila modelli per acceleratori come le TPU. I driver inizializzano i dispositivi e delegano le operazioni.\nLe sfide includono il partizionamento e la pianificazione efficienti dei carichi di lavoro su dispositivi eterogenei come nodi multi-GPU. I runtime devono anche ridurre al minimo il sovraccarico dei trasferimenti di dati e della sincronizzazione.\nLibrerie, runtime e driver forniscono i mattoni ottimizzati che gli sviluppatori di deep learning possono sfruttare per le prestazioni dell’acceleratore senza competenze di programmazione hardware. La loro ottimizzazione è essenziale per le distribuzioni.\n\n\n10.5.3 Ottimizzazione dei Compilatori\nL’ottimizzazione dei compilatori è fondamentale per estrarre le massime prestazioni ed efficienza dagli acceleratori hardware per i carichi di lavoro AI. Applicano ottimizzazioni che spaziano tra modifiche algoritmiche, trasformazioni a livello di grafico e generazione di codice di basso livello.\n\nOttimizzazione degli Algoritmi: Tecniche come quantizzazione, potatura e ricerca di architettura neurale per migliorare l’efficienza del modello e abbinare le capacità hardware.\nOttimizzazioni dei Grafi: Ottimizzazioni a livello di grafo come fusione degli operatori, riscrittura e trasformazioni di layout per ottimizzare le prestazioni sull’hardware target.\nGenerazione di Codice: Generazione di codice di basso livello ottimizzato per acceleratori da modelli e framework di alto livello.\n\nAd esempio, lo stack di compilatori “open” TVM applica la quantizzazione per un modello BERT che ha come target le GPU Arm. Fonde le operazioni di convoluzione puntuale e trasforma il layout dei pesi per ottimizzare l’accesso alla memoria. Infine, emette codice OpenGL ottimizzato per eseguire il carico di lavoro GPU.\nLe ottimizzazioni chiave del compilatore includono la massimizzazione del parallelismo, il miglioramento della località e del riutilizzo dei dati, la riduzione al minimo dell’ingombro della memoria e lo sfruttamento delle operazioni hardware personalizzate. I compilatori creano e ottimizzano i carichi di lavoro di machine learning in modo olistico su componenti hardware come CPU, GPU e altri acceleratori.\nTuttavia, la mappatura efficiente di modelli complessi introduce sfide come il partizionamento efficiente dei carichi di lavoro su dispositivi eterogenei. I compilatori a livello di produzione richiedono anche molto tempo per la messa a punto su carichi di lavoro rappresentativi. Tuttavia, l’ottimizzazione dei compilatori è per sfruttare tutte le capacità degli acceleratori AI.\n\n\n10.5.4 Simulazione e Modellazione\nIl software di simulazione è importante nella progettazione congiunta hardware-software. Consente accoppiare la modellazione di architetture hardware e stack software proposti:\n\nSimulazione Hardware: Piattaforme come Gem5 consentono la simulazione dettagliata di componenti hardware come pipeline, cache, interconnessioni e gerarchie di memoria. Gli ingegneri possono modellare le modifiche hardware senza prototipazione fisica (Binkert et al. 2011).\nSimulazione Software: Stack di compilatori come TVM supportano la simulazione di carichi di lavoro di machine learning per stimare le prestazioni sulle architetture hardware target. Questo aiuta con le ottimizzazioni software.\nCo-simulazione: Piattaforme unificate come SCALE-Sim (Samajdar et al. 2018) integrano la simulazione hardware e software in un unico strumento. Ciò consente un’analisi “what-if” per quantificare gli impatti a livello di sistema delle ottimizzazioni cross-layer all’inizio del ciclo di progettazione.\n\n\nBinkert, Nathan, Bradford Beckmann, Gabriel Black, Steven K. Reinhardt, Ali Saidi, Arkaprava Basu, Joel Hestness, et al. 2011. «The gem5 simulator». ACM SIGARCH Computer Architecture News 39 (2): 1–7. https://doi.org/10.1145/2024716.2024718.\n\nSamajdar, Ananda, Yuhao Zhu, Paul Whatmough, Matthew Mattina, e Tushar Krishna. 2018. «Scale-sim: Systolic cnn accelerator simulator». ArXiv preprint abs/1811.02883. https://arxiv.org/abs/1811.02883.\nAd esempio, un progetto di acceleratore AI basato su FPGA potrebbe essere simulato utilizzando il linguaggio di descrizione hardware Verilog e sintetizzato in un modello Gem5. Verilog è adatto per descrivere la logica digitale e le interconnessioni dell’architettura dell’acceleratore. Verilog consente al progettista di specificare i datapath [percorsi dati], la logica di controllo, le memorie on-chip e altri componenti implementati nella struttura FPGA. Una volta completato il progetto Verilog, può essere sintetizzato in un modello che simula il comportamento dell’hardware, ad esempio utilizzando il simulatore Gem5. Gem5 è utile per questa attività perché consente la modellazione di sistemi completi, inclusi processori, cache, bus e acceleratori personalizzati. Gem5 supporta l’interfacciamento dei modelli Verilog dell’hardware alla simulazione, consentendo la modellazione unificata del sistema.\nIl modello di acceleratore FPGA sintetizzato potrebbe quindi avere carichi di lavoro ML simulati utilizzando TVM compilato su di esso all’interno dell’ambiente Gem5 per una modellazione unificata. TVM consente la compilazione ottimizzata di modelli di ML su hardware eterogeneo come FPGA. L’esecuzione di carichi di lavoro compilati con TVM sull’acceleratore all’interno della simulazione Gem5 fornisce un modo integrato per convalidare e perfezionare la progettazione hardware, lo stack software e l’integrazione di sistema prima di realizzare fisicamente l’acceleratore su un FPGA reale.\nQuesto tipo di co-simulazione fornisce stime di metriche complessive come throughput, latenza e potenza per guidare la progettazione congiunta prima della costosa prototipazione fisica. Aiutano anche con le ottimizzazioni di partizionamento tra hardware e software per guidare i compromessi di progettazione.\nTuttavia, la precisione nella modellazione di interazioni sottili di basso livello tra componenti è limitata. Le simulazioni quantificate sono stime ma non possono sostituire completamente i prototipi fisici e i test. Tuttavia, la simulazione e la modellazione unificate forniscono preziose informazioni iniziali sulle opportunità di ottimizzazione a livello di sistema durante il processo di co-progettazione.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Accelerazione IA</span>"
    ]
  },
  {
    "objectID": "contents/core/hw_acceleration/hw_acceleration.it.html#benchmarking-dellhardware-ai",
    "href": "contents/core/hw_acceleration/hw_acceleration.it.html#benchmarking-dellhardware-ai",
    "title": "10  Accelerazione IA",
    "section": "10.6 Benchmarking dell’Hardware AI",
    "text": "10.6 Benchmarking dell’Hardware AI\nIl benchmarking è un processo critico che quantifica e confronta le prestazioni di varie piattaforme hardware progettate per accelerare le applicazioni di intelligenza artificiale. Guida le decisioni di acquisto, l’attenzione allo sviluppo e gli sforzi di ottimizzazione delle prestazioni per i produttori di hardware e gli sviluppatori di software.\nIl capitolo sul benchmarking esplora questo argomento in modo molto dettagliato, spiegando perché è diventato una parte indispensabile del ciclo di sviluppo dell’hardware AI e come influisce sul più ampio panorama tecnologico. Qui, esamineremo brevemente i concetti principali, ma consigliamo di fare riferimento al capitolo per maggiori dettagli.\nSuite di benchmarking come MLPerf, Fathom e AI Benchmark offrono una serie di test standardizzati utilizzabili su diverse piattaforme hardware. Queste suite misurano le prestazioni dell’acceleratore AI su varie reti neurali e attività di apprendimento automatico, dalla classificazione di immagini di base all’elaborazione complessa del linguaggio. Fornendo un terreno comune per il confronto, aiutano a garantire che le dichiarazioni sulle prestazioni siano coerenti e verificabili. Questi “tool” vengono applicati non solo per guidare lo sviluppo dell’hardware, ma anche per garantire che lo stack software sfrutti appieno il potenziale dell’architettura sottostante.\n\nMLPerf: Include un ampio set di benchmark che coprono sia l’addestramento (Mattson et al. 2020) che l’inferenza (Reddi et al. 2020) per una gamma di attività di machine learning. Figura 11.5 illustra la diversità dei casi d’uso dell’IA trattati da MLPerf.\nFathom: Si concentra sulle operazioni principali nei modelli di deep learning, enfatizzandone l’esecuzione su diverse architetture (Adolf et al. 2016).\nAI Benchmark: Mira a dispositivi mobili e consumer, valutando le prestazioni dell’IA nelle applicazioni per utenti finali (Ignatov et al. 2018).\n\n\nMattson, Peter, Vijay Janapa Reddi, Christine Cheng, Cody Coleman, Greg Diamos, David Kanter, Paulius Micikevicius, et al. 2020. «MLPerf: An Industry Standard Benchmark Suite for Machine Learning Performance». IEEE Micro 40 (2): 8–16. https://doi.org/10.1109/mm.2020.2974843.\n\nReddi, Vijay Janapa, Christine Cheng, David Kanter, Peter Mattson, Guenther Schmuelling, Carole-Jean Wu, Brian Anderson, et al. 2020. «MLPerf Inference Benchmark». In 2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA), 446–59. IEEE; IEEE. https://doi.org/10.1109/isca45697.2020.00045.\n\nAdolf, Robert, Saketh Rama, Brandon Reagen, Gu-yeon Wei, e David Brooks. 2016. «Fathom: Reference workloads for modern deep learning methods». In 2016 IEEE International Symposium on Workload Characterization (IISWC), 1–10. IEEE; IEEE. https://doi.org/10.1109/iiswc.2016.7581275.\n\nIgnatov, Andrey, Radu Timofte, William Chou, Ke Wang, Max Wu, Tim Hartley, e Luc Van Gool. 2018. «AI Benchmark: Running deep neural networks on Android smartphones», 0–0.\n\n\n\n\n\n\nFigura 10.4: MLPerf Training v3.0 e suoi utilizzi. Fonte: Forbes\n\n\n\nI benchmark hanno anche metriche delle prestazioni che sono misure quantificabili utilizzate per valutare l’efficacia degli acceleratori di IA. Queste metriche forniscono una visione completa delle capacità di un acceleratore e vengono utilizzate per guidare il processo di progettazione e selezione per i sistemi di IA. Le metriche comuni comprendono:\n\nThroughput: Solitamente misurato in operazioni al secondo, questo parametro indica il volume di calcoli che un acceleratore può gestire.\nLatenza: Il ritardo temporale tra input e output in un sistema è fondamentale per le attività di elaborazione in tempo reale.\nEfficienza Energetica: Calcolato come elaborazione per watt, che rappresenta il compromesso tra prestazioni e consumo energetico.\nEfficienza dei Costi: Valuta il costo operativo in relazione alle prestazioni, un parametro essenziale per le distribuzioni attente al budget.\nPrecisione: Nelle attività di inferenza, la precisione dei calcoli è fondamentale e talvolta bilanciata rispetto alla velocità.\nScalabilità: La capacità del sistema di mantenere i guadagni in termini di prestazioni man mano che il carico computazionale aumenta.\n\nI risultati del benchmark forniscono informazioni che vanno oltre i semplici numeri: possono rivelare colli di bottiglia nello stack software e nell’hardware. Ad esempio, i benchmark possono mostrare come l’aumento delle dimensioni del batch migliori l’utilizzo della GPU fornendo più parallelismo o come le ottimizzazioni del compilatore aumentino le prestazioni della TPU. Questi insegnamenti consentono un’ottimizzazione continua (Zhihao Jia, Zaharia, e Aiken 2019).\n\nJia, Zhihao, Matei Zaharia, e Alex Aiken. 2019. «Beyond Data and Model Parallelism for Deep Neural Networks». In Proceedings of Machine Learning and Systems 2019, MLSys 2019, Stanford, CA, USA, March 31 - April 2, 2019, a cura di Ameet Talwalkar, Virginia Smith, e Matei Zaharia. mlsys.org. https://proceedings.mlsys.org/book/265.pdf.\n\nZhu, Hongyu, Mohamed Akrout, Bojian Zheng, Andrew Pelegris, Anand Jayarajan, Amar Phanishayee, Bianca Schroeder, e Gennady Pekhimenko. 2018. «Benchmarking and Analyzing Deep Neural Network Training». In 2018 IEEE International Symposium on Workload Characterization (IISWC), 88–100. IEEE; IEEE. https://doi.org/10.1109/iiswc.2018.8573476.\nIl benchmarking standardizzato fornisce una valutazione quantificata e comparabile degli acceleratori AI per informare la progettazione, l’acquisto e l’ottimizzazione. Tuttavia, anche la convalida delle prestazioni nel mondo reale rimane essenziale (H. Zhu et al. 2018).",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Accelerazione IA</span>"
    ]
  },
  {
    "objectID": "contents/core/hw_acceleration/hw_acceleration.it.html#sfide-e-soluzioni",
    "href": "contents/core/hw_acceleration/hw_acceleration.it.html#sfide-e-soluzioni",
    "title": "10  Accelerazione IA",
    "section": "10.7 Sfide e Soluzioni",
    "text": "10.7 Sfide e Soluzioni\nGli acceleratori AI offrono notevoli miglioramenti delle prestazioni, ma spesso è necessario migliorare i significativi problemi di portabilità e compatibilità nella loro integrazione nel più ampio panorama AI. Il nocciolo della questione risiede nella diversità dell’ecosistema AI: esiste una vasta gamma di acceleratori, framework e linguaggi di programmazione per l’apprendimento automatico, ognuno con le sue caratteristiche e requisiti unici.\n\n10.7.1 Problemi di Portabilità/Compatibilità\nGli sviluppatori incontrano spesso difficoltà nel trasferire i loro modelli AI da un ambiente hardware a un altro. Ad esempio, un modello di machine learning sviluppato per un ambiente desktop in Python utilizzando il framework PyTorch, ottimizzato per una GPU Nvidia, potrebbe non essere facilmente trasferito a un dispositivo più vincolato come Arduino Nano 33 BLE. Questa complessità deriva da nette differenze nei requisiti di programmazione: Python e PyTorch sul desktop rispetto a un ambiente C++ su un Arduino, per non parlare del passaggio dall’architettura x86 ad ARM ISA.\nQueste divergenze evidenziano la complessità della portabilità all’interno dei sistemi AI. Inoltre, il rapido progresso negli algoritmi e nei modelli di intelligenza artificiale implica che gli acceleratori hardware debbano adattarsi continuamente, creando un obiettivo mobile per la compatibilità. L’assenza di standard e interfacce universali aggrava il problema, rendendo difficile l’implementazione di soluzioni di intelligenza artificiale in modo coerente su vari dispositivi e piattaforme.\n\nSoluzioni e Strategie\nPer affrontare questi ostacoli, il settore dell’intelligenza artificiale si sta muovendo verso diverse soluzioni:\n\nIniziative di Standardizzazione\nOpen Neural Network Exchange (ONNX) è in prima linea in questa ricerca, proponendo un ecosistema aperto e condiviso che promuove l’intercambiabilità dei modelli. ONNX facilita l’uso di modelli di intelligenza artificiale su vari framework, consentendo ai modelli addestrati in un ambiente di essere distribuiti in modo efficiente in un altro, riducendo significativamente la necessità di riscritture o modifiche che richiedono molto tempo.\n\n\nFramework Multipiattaforma\nA complemento degli sforzi di standardizzazione, framework multipiattaforma come TensorFlow Lite e PyTorch Mobile sono stati sviluppati specificamente per creare coesione tra diversi ambienti di calcolo che vanno dai desktop ai dispositivi mobili ed embedded. Questi framework offrono versioni semplificate e leggere delle loro versioni principali, garantendo compatibilità e integrità funzionale su diversi tipi di hardware senza sacrificare le prestazioni. Ciò garantisce che gli sviluppatori possano creare applicazioni con la certezza che funzioneranno su molti dispositivi, colmando un divario che tradizionalmente ha rappresentato una sfida considerevole nello sviluppo dell’intelligenza artificiale.\n\n\nPiattaforme Indipendenti dall’Hardware\nL’ascesa delle piattaforme indipendenti dall’hardware ha anche svolto un ruolo importante nella democratizzazione dell’uso dell’IA. Creando ambienti in cui le applicazioni di IA possono essere eseguite su vari acceleratori, queste piattaforme eliminano l’onere della codifica specifica per l’hardware dagli sviluppatori. Questa astrazione semplifica il processo di sviluppo e apre nuove possibilità per l’innovazione e l’implementazione delle applicazioni, libere dai vincoli delle specifiche hardware.\n\n\nStrumenti di Compilazione Avanzati\nInoltre, l’avvento di strumenti di compilazione avanzati come TVM, un compilatore di tensori end-to-end, offre un percorso ottimizzato attraverso la giungla delle diverse architetture hardware. TVM fornisce agli sviluppatori i mezzi per mettere a punto modelli di machine learning per un ampio spettro di substrati computazionali, garantendo prestazioni ottimali ed evitando la regolazione manuale del modello ogni volta che si verifica uno spostamento nell’hardware sottostante.\n\n\nCollaborazione tra Comunità e Settore\nLa collaborazione tra comunità open source e consorzi di settore non può essere sottovalutata. Questi organismi collettivi sono fondamentali per la formazione di standard condivisi e best practice a cui tutti gli sviluppatori e i produttori possono aderire. Tale collaborazione promuove un ecosistema AI più unificato e sinergico, riducendo significativamente la prevalenza di problemi di portabilità e spianando la strada verso l’integrazione e l’avanzamento dell’AI globale. Attraverso questi lavori combinati, l’AI si sta muovendo costantemente verso un futuro in cui la distribuzione di modelli senza soluzione di continuità su varie piattaforme diventa uno standard piuttosto che un’eccezione.\nRisolvere le sfide della portabilità è fondamentale per il campo dell’IA per realizzare il pieno potenziale degli acceleratori hardware in un panorama tecnologico dinamico e diversificato. Richiede uno sforzo concertato da parte dei produttori di hardware, degli sviluppatori di software e degli enti normativi per creare un ambiente più interoperabile e flessibile. Con innovazione e collaborazione continue, la comunità dell’IA può aprire la strada a un’integrazione e a un’implementazione senza soluzione di continuità dei modelli di IA su molte piattaforme.\n\n\n\n\n10.7.2 Problemi di Consumo Energetico\nIl consumo energetico è un problema cruciale nello sviluppo e nel funzionamento degli acceleratori AI dei data center, come le unità di elaborazione grafica (GPU) e le unità di elaborazione tensoriale (TPU) (N. P. Jouppi et al. 2017b) (Norrie et al. 2021) (N. Jouppi et al. 2023). Questi potenti componenti sono la spina dorsale dell’infrastruttura AI contemporanea, ma le loro elevate richieste di energia contribuiscono all’impatto ambientale della tecnologia e aumentano significativamente i costi operativi. Man mano che le esigenze di elaborazione dei dati diventano più complesse, con la crescente popolarità dell’AI e del deep learning, c’è una richiesta pressante di GPU e TPU in grado di fornire la potenza di calcolo necessaria in modo più efficiente. L’impatto di tali progressi è duplice: possono ridurre l’impatto ambientale di queste tecnologie e ridurre i costi di esecuzione delle applicazioni AI.\n\n———, et al. 2017b. «In-Datacenter Performance Analysis of a Tensor Processing Unit». In Proceedings of the 44th Annual International Symposium on Computer Architecture, 1–12. ISCA ’17. New York, NY, USA: ACM. https://doi.org/10.1145/3079856.3080246.\n\nNorrie, Thomas, Nishant Patil, Doe Hyun Yoon, George Kurian, Sheng Li, James Laudon, Cliff Young, Norman Jouppi, e David Patterson. 2021. «The Design Process for Google’s Training Chips: Tpuv2 and TPUv3». IEEE Micro 41 (2): 56–63. https://doi.org/10.1109/mm.2021.3058217.\n\nJouppi, Norm, George Kurian, Sheng Li, Peter Ma, Rahul Nagarajan, Lifeng Nai, Nishant Patil, et al. 2023. «TPU v4: An Optically Reconfigurable Supercomputer for Machine Learning with Hardware Support for Embeddings». In Proceedings of the 50th Annual International Symposium on Computer Architecture. ISCA ’23. New York, NY, USA: ACM. https://doi.org/10.1145/3579371.3589350.\nLe tecnologie hardware emergenti sono sul punto di rivoluzionare l’efficienza energetica in questo settore. L’informatica fotonica, ad esempio, utilizza la luce anziché l’elettricità per trasportare informazioni, offrendo la promessa di un’elaborazione ad alta velocità con una frazione del consumo energetico. Analizziamo più approfonditamente questa e altre tecnologie innovative nella sezione “Tecnologie Hardware Emergenti”, esplorando il loro potenziale per affrontare le attuali sfide del consumo energetico.\nAi margini della rete, gli acceleratori AI sono progettati per elaborare dati su dispositivi come smartphone, sensori IoT e dispositivi indossabili intelligenti. Questi dispositivi spesso funzionano con gravi limitazioni di potenza, rendendo necessario un attento bilanciamento tra prestazioni e consumo energetico. Un modello AI ad alte prestazioni può fornire risultati rapidi, ma a costo di esaurire rapidamente la durata della batteria e aumentare la produzione termica, il che può influire sulla funzionalità e sulla durata del dispositivo. La posta in gioco è più alta per i dispositivi distribuiti in aree remote o difficili da raggiungere, dove non è possibile garantire un’alimentazione costante, il che sottolinea la necessità di soluzioni a basso consumo energetico.\nI problemi di latenza aggravano ulteriormente la sfida dell’efficienza energetica ai margini. Le applicazioni AI Edge in settori quali la guida autonoma e il monitoraggio sanitario richiedono velocità, precisione e affidabilità, poiché i ritardi nell’elaborazione possono comportare gravi rischi per la sicurezza. Per queste applicazioni, gli sviluppatori devono ottimizzare sia gli algoritmi AI sia la progettazione hardware per raggiungere un equilibrio ottimale tra consumo energetico e latenza.\nQuesto sforzo di ottimizzazione non riguarda solo l’apporto di miglioramenti incrementali alle tecnologie esistenti; riguarda il ripensamento di come e dove elaboriamo le attività AI. Progettando acceleratori AI che siano sia efficienti dal punto di vista energetico sia in grado di elaborare rapidamente, possiamo garantire che questi dispositivi svolgano i loro scopi previsti senza un consumo energetico non necessario o prestazioni compromesse. Tali sviluppi potrebbero promuovere l’adozione diffusa dell’AI in vari settori, consentendo un uso più intelligente, sicuro e sostenibile della tecnologia.\n\n\n10.7.3 Superare i Vincoli delle Risorse\nAnche i vincoli di risorse rappresentano una sfida significativa per gli acceleratori Edge AI, poiché queste soluzioni hardware e software specializzate devono fornire prestazioni robuste entro i limiti dei dispositivi edge. A causa dei limiti di potenza e dimensioni, gli acceleratori Edge AI hanno spesso capacità di calcolo, memoria e archiviazione limitate (L. Zhu et al. 2023). Questa scarsità di risorse richiede un’attenta allocazione delle capacità di elaborazione per eseguire modelli di apprendimento automatico in modo efficiente.\n\nZhu, Ligeng, Lanxiang Hu, Ji Lin, Wei-Ming Chen, Wei-Chen Wang, Chuang Gan, e Song Han. 2023. «PockEngine: Sparse and Efficient Fine-tuning in a Pocket». In 56th Annual IEEE/ACM International Symposium on Microarchitecture. ACM. https://doi.org/10.1145/3613424.3614307.\n\nLin, Ji, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, e Song Han. 2023. «AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration». arXiv.\n\nLi, Yuhang, Xin Dong, e Wei Wang. 2020. «Additive Powers-of-Two Quantization: An Efficient Non-uniform Discretization for Neural Networks». In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net. https://openreview.net/forum?id=BkgXT24tDS.\n\nWang, Tianzhe, Kuan Wang, Han Cai, Ji Lin, Zhijian Liu, Hanrui Wang, Yujun Lin, e Song Han. 2020. «APQ: Joint Search for Network Architecture, Pruning and Quantization Policy». In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2075–84. IEEE. https://doi.org/10.1109/cvpr42600.2020.00215.\nInoltre, la gestione di risorse limitate richiede approcci innovativi, tra cui la quantizzazione del modello (Lin et al. 2023) (Li, Dong, e Wang 2020), pruning (Wang et al. 2020) e l’ottimizzazione delle pipeline di inferenza. Gli acceleratori Edge AI devono trovare un delicato equilibrio tra la fornitura di funzionalità AI significative e il non esaurire le risorse disponibili, mantenendo al contempo un basso consumo energetico. Superare questi vincoli di risorse è fondamentale per garantire l’implementazione di successo dell’intelligenza artificiale ai margini, dove molte applicazioni, dall’IoT ai dispositivi mobili, si basano sull’uso efficiente di risorse hardware limitate per fornire un processo decisionale intelligente e in tempo reale.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Accelerazione IA</span>"
    ]
  },
  {
    "objectID": "contents/core/hw_acceleration/hw_acceleration.it.html#tecnologie-emergenti",
    "href": "contents/core/hw_acceleration/hw_acceleration.it.html#tecnologie-emergenti",
    "title": "10  Accelerazione IA",
    "section": "10.8 Tecnologie Emergenti",
    "text": "10.8 Tecnologie Emergenti\nFinora abbiamo discusso la tecnologia hardware AI nel contesto della progettazione dell’architettura von Neumann convenzionale e dell’implementazione basata su CMOS. Questi chip AI specializzati offrono vantaggi come una maggiore produttività ed efficienza energetica, ma si basano sui principi di elaborazione tradizionali. La crescita inarrestabile della domanda di potenza di elaborazione AI sta guidando le innovazioni nei metodi di integrazione per l’hardware AI.\nSono emersi due approcci principali per massimizzare la densità di elaborazione, l’integrazione su “scala wafer” e le architetture basate su “chiplet”, di cui parleremo in questa sezione. Guardando molto più avanti, esamineremo le tecnologie emergenti che divergono dalle architetture convenzionali e adottano approcci fondamentalmente diversi per l’elaborazione specializzata AI.\nAlcuni di questi paradigmi non convenzionali includono l’elaborazione neuromorfica, che imita le reti neurali biologiche; l’elaborazione quantistica, che sfrutta gli effetti della meccanica quantistica; e l’elaborazione ottica, che utilizza fotoni anziché elettroni. Oltre ai nuovi substrati di elaborazione, le nuove tecnologie dei dispositivi stanno consentendo ulteriori guadagni attraverso una migliore memoria e interconnessione.\nEsempi includono i “memristor” [https://it.wikipedia.org/wiki/Memristore] per l’elaborazione in memoria e la nanofotonica per la comunicazione fotonica integrata. Insieme, queste tecnologie offrono il potenziale per miglioramenti di ordini di grandezza in termini di velocità, efficienza e scalabilità rispetto all’attuale hardware AI. Esamineremo questi aspetti in questa sezione.\n\n10.8.1 Metodi di Integrazione\nI metodi di integrazione si riferiscono agli approcci utilizzati per combinare e interconnettere i vari componenti di elaborazione e memoria di un chip o sistema AI. Collegando strettamente gli elementi di elaborazione chiave, l’integrazione cerca di massimizzare le prestazioni, l’efficienza energetica e la densità.\nIn passato, l’elaborazione AI veniva eseguita principalmente su CPU e GPU costruite utilizzando metodi di integrazione convenzionali. Questi componenti discreti venivano fabbricati separatamente e collegati insieme su una scheda. Tuttavia, questa integrazione poco stretta crea colli di bottiglia, come i sovraccarichi dei trasferimento di dati.\nCon l’aumento dei carichi di lavoro AI, aumenta la domanda di una più stretta integrazione tra elementi di elaborazione, memoria e comunicazione. Alcuni fattori chiave dell’integrazione includono:\n\nRiduzione al minimo dello spostamento dei dati: Una stretta integrazione riduce la latenza e l’energia per lo spostamento dei dati tra i componenti. Ciò migliora l’efficienza.\nPersonalizzazione: Adattare tutti i componenti del sistema ai carichi di lavoro AI consente ottimizzazioni in tutto lo stack hardware.\nParallelismo: L’integrazione di molti elementi di elaborazione consente un calcolo parallelo massiccio.\nDensità: Una più stretta integrazione consente di impacchettare più transistor e memoria in una determinata area.\nCosto: Le economie di scala derivanti da grandi sistemi integrati possono ridurre i costi.\n\nIn risposta, nuove tecniche di produzione come la fabbricazione su scala di wafer e il confezionamento avanzato consentono ora livelli di integrazione molto più elevati. L’obiettivo è creare complessi di elaborazione AI unificati e specializzati, su misura per il deep learning e altri algoritmi AI. Un’integrazione più stretta è fondamentale per fornire le prestazioni e l’efficienza necessarie per la prossima generazione di AI.\n\nAI su Scala Wafer\nL’intelligenza artificiale su “wafer-scale” adotta un approccio estremamente integrato, producendo un intero wafer di silicio come un gigantesco chip. Ciò differisce drasticamente dalle CPU e GPU convenzionali, che tagliano ogni wafer in molti chip singoli più piccoli. Figura 10.5 mostra un confronto tra Cerebras Wafer Scale Engine 2, che è il chip più grande mai costruito, e la GPU più grande. Mentre alcune GPU possono contenere miliardi di transistor, impallidiscono comunque rispetto alla scala di un chip delle dimensioni di un wafer con oltre un trilione di transistor.\n\n\n\n\n\n\nFigura 10.5: Wafer-scale vs. GPU. Fonte: Cerebras.\n\n\n\nL’approccio su scala di wafer diverge anche dai progetti system-on-chip più modulari che hanno ancora componenti discreti che comunicano tramite bus. Invece, l’intelligenza artificiale su scala di wafer consente la personalizzazione completa e la stretta integrazione di elaborazione, memoria e interconnessioni nell’intero di die.\nProgettando il wafer come un’unità logica integrata, il trasferimento dati tra gli elementi è ridotto al minimo. Ciò fornisce una latenza e un consumo energetico inferiori rispetto ai design discreti system-on-chip o chiplet. Mentre i chiplet possono offrire flessibilità mescolando e abbinando i componenti, la comunicazione tra chiplet è impegnativa. La natura monolitica dell’integrazione su scala wafer elimina questi colli di bottiglia nella comunicazione tra chip.\nTuttavia, la scala ultra-large pone anche difficoltà per la producibilità e la resa con i design su scala wafer. Difetti in qualsiasi regione del wafer possono rendere (alcune parti del) chip inutilizzabile. Sono necessarie tecniche di litografia specializzate per produrre tali matrici di grandi dimensioni. Quindi, l’integrazione su scala wafer persegue i massimi guadagni in termini di prestazioni dall’integrazione ma richiede il superamento di sostanziali sfide di fabbricazione.\nVideo 10.1 fornisce ulteriore contesto sui chip AI su scala wafer.\n\n\n\n\n\n\nVideo 10.1: Wafer-scale AI Chips\n\n\n\n\n\n\n\n\nChiplet per AI\nIl design chiplet si riferisce a un’architettura semiconduttrice in cui un singolo circuito integrato (IC) è costruito da più componenti più piccoli e individuali noti come chiplet. Ogni chiplet è un blocco funzionale autonomo, in genere specializzato per un’attività o funzionalità specifica. Questi chiplet sono quindi interconnessi su un substrato o un package più grande per creare un sistema coeso.\nFigura 10.6 illustra questo concetto. Per l’hardware AI, i chiplet consentono di combinare diversi tipi di chip ottimizzati per attività come moltiplicazione di matrici, spostamento di dati, I/O analogico e memorie specializzate. Questa integrazione eterogenea differisce notevolmente dall’integrazione wafer-scale, in cui tutta la logica è prodotta come un unico chip monolitico. Aziende come Intel e AMD hanno adottato design chiplet per le loro CPU.\nI chiplet sono interconnessi utilizzando tecniche di packaging avanzate come interposer di substrato ad alta densità, impilamento 2.5D/3D e packaging a livello di wafer. Ciò consente di combinare chiplet realizzati con diversi nodi di processo, memorie specializzate e vari motori AI ottimizzati.\n\n\n\n\n\n\nFigura 10.6: Partizionamento chiplet. Fonte: Vivet et al. (2021).\n\n\nVivet, Pascal, Eric Guthmuller, Yvain Thonnart, Gael Pillonnet, Cesar Fuguet, Ivan Miro-Panades, Guillaume Moritz, et al. 2021. «IntAct: A 96-Core Processor With Six Chiplets 3D-Stacked on an Active Interposer With Distributed Interconnects and Integrated Power Management». IEEE J. Solid-State Circuits 56 (1): 79–97. https://doi.org/10.1109/jssc.2020.3036341.\n\n\nEcco alcuni vantaggi chiave dell’uso di chiplet per l’intelligenza artificiale:\n\nFlessibilità: I chiplet consentono la combinazione di diversi tipi di chip, nodi di processo e memorie su misura per ogni funzione. Questo è più modulare rispetto a un design fisso su scala wafer.\nResa: I chiplet più piccoli hanno una resa maggiore rispetto a un gigantesco chip su scala wafer. I difetti sono contenuti nei singoli chiplet.\nCosto: Sfrutta le capacità di produzione esistenti anziché richiedere nuovi processi specializzati. Riduce i costi riutilizzando la fabbricazione assestata.\nCompatibilità: Può integrarsi con architetture di sistema più convenzionali come PCIe e interfacce di memoria DDR standard.\n\nTuttavia, i chiplet devono anche affrontare sfide di integrazione e prestazioni:\n\nDensità inferiore rispetto alla scala wafer, poiché i chiplet sono limitati in termini di dimensioni.\nLatenza aggiuntiva durante la comunicazione tra chiplet rispetto all’integrazione monolitica. Richiede ottimizzazione per interconnessioni a bassa latenza.\nIl packaging avanzato aggiunge complessità rispetto all’integrazione su scala wafer, sebbene ciò sia discutibile.\n\nL’obiettivo principale dei chiplet è trovare il giusto equilibrio tra flessibilità modulare e densità di integrazione per prestazioni AI ottimali. I chiplet mirano a un’accelerazione AI efficiente pur lavorando entro i vincoli delle tecniche di produzione convenzionali. I chiplet prendono una via di mezzo tra gli estremi dell’integrazione su scala wafer e dei componenti completamente discreti. Ciò fornisce vantaggi pratici ma può sacrificare una certa densità computazionale ed efficienza rispetto a un sistema teorico a livello di wafer.\n\n\n\n10.8.2 Elaborazione Nùeuromorfica\nL’elaborazione neuromorfica è un campo emergente che mira a emulare l’efficienza e la robustezza dei sistemi neurali biologici per applicazioni di machine learning. Una differenza fondamentale rispetto alle classiche architetture di Von Neumann è la fusione di memoria ed elaborazione nello stesso circuito (Schuman et al. 2022; Marković et al. 2020; Furber 2016), come illustrato in Figura 10.7. La struttura del cervello ispira questo approccio integrato. Un vantaggio fondamentale è il potenziale per un miglioramento di ordini di grandezza nel calcolo efficiente dal punto di vista energetico rispetto all’hardware AI convenzionale. Ad esempio, le stime prevedono guadagni di 100x-1000x nell’efficienza energetica rispetto agli attuali sistemi basati su GPU per carichi di lavoro equivalenti.\n\nMarković, Danijela, Alice Mizrahi, Damien Querlioz, e Julie Grollier. 2020. «Physics for neuromorphic computing». Nature Reviews Physics 2 (9): 499–510. https://doi.org/10.1038/s42254-020-0208-2.\n\nFurber, Steve. 2016. «Large-scale neuromorphic computing systems». J. Neural Eng. 13 (5): 051001. https://doi.org/10.1088/1741-2560/13/5/051001.\n\n\n\n\n\n\nFigura 10.7: Confronto tra l’architettura di von Neumann e l’architettura neuromorfica. Fonte: Schuman et al. (2022).\n\n\nSchuman, Catherine D., Shruti R. Kulkarni, Maryam Parsa, J. Parker Mitchell, Prasanna Date, e Bill Kay. 2022. «Opportunities for neuromorphic computing algorithms and applications». Nature Computational Science 2 (1): 10–19. https://doi.org/10.1038/s43588-021-00184-y.\n\n\nIntel e IBM stanno guidando gli sforzi commerciali nell’hardware neuromorfico. I chip Loihi e Loihi 2 di Intel (Davies et al. 2018, 2021) offrono core neuromorfici programmabili con apprendimento on-chip. Il dispositivo Northpole (Modha et al. 2023) di IBM comprende oltre 100 milioni di sinapsi a giunzione a tunnel magnetico e 68 miliardi di transistor. Questi chip specializzati offrono vantaggi come un basso consumo energetico per l’inferenza edge.\n\nDavies, Mike, Narayan Srinivasa, Tsung-Han Lin, Gautham Chinya, Yongqiang Cao, Sri Harsha Choday, Georgios Dimou, et al. 2018. «Loihi: A Neuromorphic Manycore Processor with On-Chip Learning». IEEE Micro 38 (1): 82–99. https://doi.org/10.1109/mm.2018.112130359.\n\nDavies, Mike, Andreas Wild, Garrick Orchard, Yulia Sandamirskaya, Gabriel A. Fonseca Guerra, Prasad Joshi, Philipp Plank, e Sumedh R. Risbud. 2021. «Advancing Neuromorphic Computing With Loihi: A Survey of Results and Outlook». Proc. IEEE 109 (5): 911–34. https://doi.org/10.1109/jproc.2021.3067593.\n\nModha, Dharmendra S., Filipp Akopyan, Alexander Andreopoulos, Rathinakumar Appuswamy, John V. Arthur, Andrew S. Cassidy, Pallab Datta, et al. 2023. «Neural inference at the frontier of energy, space, and time». Science 382 (6668): 329–35. https://doi.org/10.1126/science.adh1174.\n\nMaass, Wolfgang. 1997. «Networks of spiking neurons: The third generation of neural network models». Neural Networks 10 (9): 1659–71. https://doi.org/10.1016/s0893-6080(97)00011-7.\nLe “Spiking neural network (SNN)” (Maass 1997) sono modelli computazionali per hardware neuromorfico. A differenza delle reti neurali profonde che comunicano tramite valori continui, le SNN utilizzano picchi discreti che sono più simili ai neuroni biologici. Questo consente un calcolo efficiente basato sugli eventi anziché un’elaborazione costante. Inoltre, le SNN considerano le caratteristiche temporali e spaziali dei dati di input. Ciò imita meglio le reti neurali biologiche, in cui la tempistica dei picchi neuronali svolge un ruolo importante.\nTuttavia, l’addestramento delle SNN rimane impegnativo a causa della complessità temporale aggiunta. Figura 10.8 fornisce una panoramica della metodologia spiking: (a) illustrazione di un neurone; (b) Misura di un potenziale d’azione propagato lungo l’assone di un neurone. Solo il potenziale d’azione è rilevabile lungo l’assone; (c) Il picco del neurone è approssimato con una rappresentazione binaria; (d) Elaborazione guidata dagli eventi; (e) Active Pixel Sensor e Dynamic Vision Sensor.\n\n\n\n\n\n\nFigura 10.8: Spiking neuromorfico. Fonte: Eshraghian et al. (2023).\n\n\nEshraghian, Jason K., Max Ward, Emre O. Neftci, Xinxin Wang, Gregor Lenz, Girish Dwivedi, Mohammed Bennamoun, Doo Seok Jeong, e Wei D. Lu. 2023. «Training Spiking Neural Networks Using Lessons From Deep Learning». Proc. IEEE 111 (9): 1016–54. https://doi.org/10.1109/jproc.2023.3308088.\n\n\nSi può anche guardare Video 10.2 linkato di seguito per una spiegazione più dettagliata.\n\n\n\n\n\n\nVideo 10.2: Neuromorphic Computing\n\n\n\n\n\n\nDispositivi nanoelettronici specializzati chiamati memristor (Chua 1971) sono componenti sinaptici nei sistemi neuromorfici. I memristor agiscono come memoria non volatile con conduttanza regolabile, emulando la plasticità delle sinapsi reali. I memristor consentono l’apprendimento in situ senza trasferimenti di dati separati combinando funzioni di memoria ed elaborazione. Tuttavia, la tecnologia dei memristor deve ancora raggiungere la maturità e la scalabilità per l’hardware commerciale.\n\nChua, L. 1971. «Memristor-The missing circuit element». #IEEE_J_CT# 18 (5): 507–19. https://doi.org/10.1109/tct.1971.1083337.\nL’integrazione della fotonica con il calcolo neuromorfico (Shastri et al. 2021) è emersa di recente come un’area di ricerca attiva. L’uso della luce per il calcolo e la comunicazione consente alte velocità e un consumo energetico ridotto. Tuttavia, la piena realizzazione di sistemi neuromorfici fotonici richiede il superamento di problemi di progettazione e integrazione.\nIl calcolo neuromorfico offre promettenti capacità per un’efficace inferenza edge, ma incontra ostacoli in merito ad algoritmi di addestramento, integrazione dei nanodispositivi e progettazione del sistema. La ricerca multidisciplinare in corso in informatica, ingegneria, scienza dei materiali e fisica sarà fondamentale per sbloccare il pieno potenziale di questa tecnologia per i casi d’uso dell’intelligenza artificiale.\n\n\n10.8.3 Calcolo Analogico\nIl computing analogico è un approccio emergente che utilizza segnali e componenti analogici come condensatori, induttori e amplificatori anziché la logica digitale per il calcolo. Rappresenta le informazioni come segnali elettrici continui anziché 0 e 1 discreti. Ciò consente al calcolo di riflettere direttamente la natura analogica dei dati del mondo reale, evitando errori di digitalizzazione e overhead.\nIl computing analogico ha generato un rinnovato interesse per l’hardware AI efficiente, in particolare per l’inferenza direttamente su dispositivi edge a basso consumo. I circuiti analogici, come la moltiplicazione e la sommatoria al centro delle reti neurali, possono essere utilizzati con un consumo energetico molto basso. Ciò rende l’analogico adatto per l’implementazione di modelli ML su nodi finali con vincoli energetici. Startup come Mythic stanno sviluppando acceleratori AI analogici.\nMentre il computing analogico era popolare nei primi computer, il boom della logica digitale ha portato al suo declino. Tuttavia, l’analogico è convincente per applicazioni di nicchia che richiedono estrema efficienza (Haensch, Gokmen, e Puri 2019). Contrasta con gli approcci neuromorfici digitali che utilizzano ancora picchi digitali per il calcolo. L’analogico può consentire un calcolo di precisione inferiore, ma richiede competenza nella progettazione di circuiti analogici. I compromessi su precisione, complessità di programmazione e costi di fabbricazione rimangono aree di ricerca attive.\n\nHaensch, Wilfried, Tayfun Gokmen, e Ruchir Puri. 2019. «The Next Generation of Deep Learning Hardware: Analog Computing». Proc. IEEE 107 (1): 108–22. https://doi.org/10.1109/jproc.2018.2871057.\n\nHazan, Avi, e Elishai Ezra Tsur. 2021. «Neuromorphic Analog Implementation of Neural Engineering Framework-Inspired Spiking Neuron for High-Dimensional Representation». Front. Neurosci. 15 (febbraio): 627221. https://doi.org/10.3389/fnins.2021.627221.\nIl calcolo neuromorfico, che emula i sistemi neurali biologici per un’inferenza ML efficiente, può utilizzare circuiti analogici per implementare i componenti e i comportamenti chiave del cervello. Ad esempio, i ricercatori hanno progettato circuiti analogici per modellare neuroni e sinapsi utilizzando condensatori, transistor e amplificatori operazionali (Hazan e Ezra Tsur 2021). I condensatori possono esibire le dinamiche di picco dei neuroni biologici, mentre gli amplificatori e i transistor forniscono una somma ponderata di input per imitare i dendriti. Le tecnologie a resistore variabile come i memristor possono realizzare sinapsi analogiche con plasticità dipendente dal tempo di picco, che può rafforzare o indebolire le connessioni in base all’attività di picco.\nStartup come SynSense hanno sviluppato chip neuromorfici analogici contenenti questi componenti biomimetici (Bains 2020). Questo approccio analogico si traduce in un basso consumo energetico e un’elevata scalabilità per i dispositivi edge rispetto alle complesse implementazioni SNN digitali.\n\nBains, Sunny. 2020. «The business of building brains». Nature Electronics 3 (7): 348–51. https://doi.org/10.1038/s41928-020-0449-1.\nTuttavia, l’addestramento di SNN analogiche sui chip rimane una sfida aperta. Nel complesso, la realizzazione analogica è una tecnica promettente per fornire l’efficienza, la scalabilità e la plausibilità biologica previste con il calcolo neuromorfico. La fisica dei componenti analogici combinata con la progettazione dell’architettura neurale potrebbe migliorare l’efficienza dell’inferenza rispetto alle reti neurali digitali convenzionali.\n\n\n10.8.4 Elettronica Flessibile\nMentre gran parte della nuova tecnologia hardware nell’area di lavoro ML si è concentrata sull’ottimizzazione e sulla creazione di sistemi più efficienti, c’è una traiettoria parallela che mira ad adattare l’hardware per applicazioni specifiche (Gates 2009; Musk et al. 2019; Tang et al. 2023; Tang, He, e Liu 2022; Kwon e Dong 2022). Una di queste strade è lo sviluppo di elettronica flessibile per casi d’uso AI.\n\nGates, Byron D. 2009. «Flexible Electronics». Science 323 (5921): 1566–67. https://doi.org/10.1126/science.1171230.\n\nTang, Xin, Hao Shen, Siyuan Zhao, Na Li, e Jia Liu. 2023. «Flexible braincomputer interfaces». Nature Electronics 6 (2): 109–18. https://doi.org/10.1038/s41928-022-00913-9.\n\nTang, Xin, Yichun He, e Jia Liu. 2022. «Soft bioelectronics for cardiac interfaces». Biophysics Reviews 3 (1). https://doi.org/10.1063/5.0069516.\nL’elettronica flessibile si riferisce a circuiti elettronici e dispositivi fabbricati su substrati flessibili in plastica o polimeri anziché in silicio rigido. A differenza delle schede e dei chip rigidi convenzionali, ciò consente all’elettronica di piegarsi, torcersi e adattarsi a forme irregolari. Figura 10.9 mostra un esempio di un prototipo di dispositivo flessibile che misura in modalità wireless la temperatura corporea, che può essere integrato senza soluzione di continuità in indumenti o cerotti cutanei. La flessibilità e la piegabilità dei materiali elettronici emergenti consentono di integrarli in fattori di forma sottili e leggeri, adatti per applicazioni AI e TinyML embedded.\nL’hardware AI flessibile può adattarsi a superfici curve e funzionare in modo efficiente con budget di potenza in microwatt. La flessibilità consente inoltre fattori di forma arrotolabili o pieghevoli per ridurre al minimo l’ingombro e il peso del dispositivo, ideali per piccoli dispositivi intelligenti portatili e dispositivi indossabili che incorporano TinyML. Un altro vantaggio fondamentale dell’elettronica flessibile rispetto alle tecnologie convenzionali sono i costi di produzione inferiori e i processi di fabbricazione più semplici, che potrebbero democratizzare l’accesso a queste tecnologie. Mentre le maschere in silicio e i costi di fabbricazione in genere costano milioni di dollari, l’hardware flessibile in genere costa solo decine di centesimi per la produzione (Huang et al. 2011; Biggs et al. 2021). Il potenziale di fabbricare elettronica flessibile direttamente su pellicole di plastica utilizzando processi di stampa e rivestimento ad alta produttività può ridurre i costi e migliorare la producibilità su larga scala rispetto ai chip AI rigidi (Musk et al. 2019).\n\nHuang, Tsung-Ching, Kenjiro Fukuda, Chun-Ming Lo, Yung-Hui Yeh, Tsuyoshi Sekitani, Takao Someya, e Kwang-Ting Cheng. 2011. «Pseudo-CMOS: A Design Style for Low-Cost and Robust Flexible Electronics». IEEE Trans. Electron Devices 58 (1): 141–50. https://doi.org/10.1109/ted.2010.2088127.\n\nBiggs, John, James Myers, Jedrzej Kufel, Emre Ozer, Simon Craske, Antony Sou, Catherine Ramsdale, Ken Williamson, Richard Price, e Scott White. 2021. «A natively flexible 32-bit Arm microprocessor». Nature 595 (7868): 532–36. https://doi.org/10.1038/s41586-021-03625-w.\n\n\n\n\n\n\nFigura 10.9: Prototipo di dispositivo flessibile. Fonte: Jabil Circuit.\n\n\n\nIl campo è abilitato dai progressi nei semiconduttori organici e nei nanomateriali che possono essere depositati su pellicole sottili e flessibili. Tuttavia, la fabbricazione rimane impegnativa rispetto ai processi maturi del silicio. I circuiti flessibili attualmente presentano in genere prestazioni inferiori rispetto agli equivalenti rigidi. Tuttavia, promettono di trasformare l’elettronica in materiali leggeri e pieghevoli.\nI casi d’uso dell’elettronica flessibile sono adatti per l’integrazione intima con il corpo umano. Le potenziali applicazioni dell’intelligenza artificiale medica includono sensori biointegrati, “soft robot” e impianti che monitorano o stimolano il sistema nervoso in modo intelligente. In particolare, gli array di elettrodi flessibili potrebbero consentire interfacce neurali a densità più elevata e meno invasive rispetto agli equivalenti rigidi.\nPertanto, l’elettronica flessibile sta inaugurando una nuova era di dispositivi indossabili e sensori corporei, in gran parte grazie alle innovazioni nei transistor organici. Questi componenti consentono un’elettronica più leggera e pieghevole, ideale per dispositivi indossabili, pelle elettronica e dispositivi medici che si adattano al corpo.\nSono adatti per dispositivi bioelettronici in termini di biocompatibilità, aprendo la strada ad applicazioni in interfacce cerebrali e cardiache. Ad esempio, la ricerca sulle interfacce flessibili cervello-computer e sulla bioelettronica morbida per applicazioni cardiache dimostra il potenziale per applicazioni mediche di vasta portata.\nAziende e istituti di ricerca non stanno solo sviluppando e investendo grandi quantità di risorse in elettrodi flessibili, come mostrato nel lavoro di Neuralink (Musk et al. 2019). Tuttavia, stanno anche spingendo i confini per integrare modelli di apprendimento automatico nei sistemi (Kwon e Dong 2022). Questi sensori intelligenti mirano a una simbiosi fluida e duratura con il corpo umano.\n\nMusk, Elon et al. 2019. «An Integrated Brain-Machine Interface Platform With Thousands of Channels». J. Med. Internet Res. 21 (10): e16194. https://doi.org/10.2196/16194.\n\nKwon, Sun Hwa, e Lin Dong. 2022. «Flexible sensors and machine learning for heart monitoring». Nano Energy 102 (novembre): 107632. https://doi.org/10.1016/j.nanoen.2022.107632.\n\nSegura Anaya, L. H., Abeer Alsadoon, N. Costadopoulos, e P. W. C. Prasad. 2017. «Ethical Implications of User Perceptions of Wearable Devices». Sci. Eng. Ethics 24 (1): 1–28. https://doi.org/10.1007/s11948-017-9872-8.\n\nGoodyear, Victoria A. 2017. «Social media, apps and wearable technologies: Navigating ethical dilemmas and procedures». Qualitative Research in Sport, Exercise and Health 9 (3): 285–302. https://doi.org/10.1080/2159676x.2017.1303790.\n\nFarah, Martha J. 2005. «Neuroethics: The practical and the philosophical». Trends Cogn. Sci. 9 (1): 34–40. https://doi.org/10.1016/j.tics.2004.12.001.\n\nRoskies, Adina. 2002. «Neuroethics for the New Millenium». Neuron 35 (1): 21–23. https://doi.org/10.1016/s0896-6273(02)00763-8.\nEticamente, l’incorporazione di sensori intelligenti basati sull’apprendimento automatico nel corpo solleva importanti questioni. Le problematiche relative alla privacy dei dati, al consenso informato e alle implicazioni sociali a lungo termine di tali tecnologie sono al centro del lavoro in corso in neuroetica e bioetica (Segura Anaya et al. 2017; Goodyear 2017; Farah 2005; Roskies 2002). Il campo sta progredendo a un ritmo che richiede progressi paralleli nei parametri etici per guidare lo sviluppo e l’implementazione responsabili di queste tecnologie. Sebbene vi siano limitazioni e ostacoli etici da superare, le prospettive per l’elettronica flessibile sono ampie e promettono molto per la ricerca e le applicazioni future.\n\n\n10.8.5 Tecnologie delle Memorie\nLe tecnologie delle memorie sono fondamentali per l’hardware AI, ma la DDR DRAM e la SRAM convenzionali creano colli di bottiglia. I carichi di lavoro AI richiedono un’elevata larghezza di banda (&gt;1 TB/s). Le applicazioni scientifiche estreme dell’AI richiedono una latenza estremamente bassa (&lt;50 ns) per alimentare i dati alle unità di calcolo (Duarte et al. 2022), un’elevata densità (&gt;128 Gb) per archiviare grandi parametri di modelli e set di dati e un’eccellente efficienza energetica (&lt;100 fJ/b) per uso embedded (Verma et al. 2019). Sono necessarie nuove memorie per soddisfare queste esigenze. Le opzioni emergenti includono diverse nuove tecnologie:\n\nDuarte, Javier, Nhan Tran, Ben Hawks, Christian Herwig, Jules Muhizi, Shvetank Prakash, e Vijay Janapa Reddi. 2022. «FastML Science Benchmarks: Accelerating Real-Time Scientific Edge Machine Learning». ArXiv preprint abs/2207.07958. https://arxiv.org/abs/2207.07958.\n\nVerma, Naveen, Hongyang Jia, Hossein Valavi, Yinqi Tang, Murat Ozatay, Lung-Yen Chen, Bonan Zhang, e Peter Deaville. 2019. «In-Memory Computing: Advances and Prospects». IEEE Solid-State Circuits Mag. 11 (3): 43–55. https://doi.org/10.1109/mssc.2019.2922889.\n\nLa RAM resistiva (ReRAM) può migliorare la densità con semplici array passivi. Tuttavia, permangono dei problemi legati alla variabilità (Chi et al. 2016).\nLa “Phase change memory (PCM)” [memoria a cambiamento di fase ] sfrutta le proprietà uniche del vetro calcogenuro. Le fasi cristalline e amorfe hanno resistenze diverse. L’Optane DCPMM di Intel fornisce PCM veloci (100 ns) e ad alta resistenza. Tuttavia, le sfide includono cicli di scrittura limitati e corrente di reset elevata (Burr et al. 2016).\nLo stacking 3D può anche aumentare la densità di memoria e la larghezza di banda integrando verticalmente strati di memoria con interconnessioni TSV (Loh 2008). Ad esempio, HBM fornisce interfacce larghe 1024 bit.\n\n\nBurr, Geoffrey W., Matthew J. BrightSky, Abu Sebastian, Huai-Yu Cheng, Jau-Yi Wu, Sangbum Kim, Norma E. Sosa, et al. 2016. «Recent Progress in Phase-Change?Pub _newline ?Memory Technology». IEEE Journal on Emerging and Selected Topics in Circuits and Systems 6 (2): 146–62. https://doi.org/10.1109/jetcas.2016.2547718.\n\nLoh, Gabriel H. 2008. «3D-Stacked Memory Architectures for Multi-core Processors». ACM SIGARCH Computer Architecture News 36 (3): 453–64. https://doi.org/10.1145/1394608.1382159.\nLe nuove tecnologie di memoria, con le loro innovative architetture e materiali cellulari, sono fondamentali per sbloccare il prossimo livello di prestazioni ed efficienza hardware AI. Realizzare i loro vantaggi nei sistemi commerciali rimane una sfida continua.\nL’elaborazione in-memory sta guadagnando terreno come promettente strada per ottimizzare l’apprendimento automatico e i carichi di lavoro di elaborazione ad alte prestazioni. Al centro, la tecnologia colloca l’archiviazione e l’elaborazione dei dati per migliorare l’efficienza energetica e ridurre la latenza Wong et al. (2012). Due tecnologie chiave sotto questo ombrello sono la “Resistive RAM (ReRAM)” e il “Processing-In-Memory (PIM)”.\n\nWong, H.-S. Philip, Heng-Yuan Lee, Shimeng Yu, Yu-Sheng Chen, Yi Wu, Pang-Shiu Chen, Byoungil Lee, Frederick T. Chen, e Ming-Jinn Tsai. 2012. «MetalOxide RRAM». Proc. IEEE 100 (6): 1951–70. https://doi.org/10.1109/jproc.2012.2190369.\n\nChi, Ping, Shuangchen Li, Cong Xu, Tao Zhang, Jishen Zhao, Yongpan Liu, Yu Wang, e Yuan Xie. 2016. «Prime: a novel processing-in-memory architecture for neural network computation in ReRAM-based main memory». ACM SIGARCH Computer Architecture News 44 (3): 27–39. https://doi.org/10.1145/3007787.3001140.\nReRAM (Wong et al. 2012) e PIM (Chi et al. 2016) sono le colonne portanti per l’elaborazione in memoria, l’archiviazione e l’elaborazione dei dati nella stessa posizione. ReRAM si concentra su questioni di uniformità, resistenza, conservazione, funzionamento multi-bit e scalabilità. D’altro canto, PIM coinvolge unità CPU integrate direttamente in array di memoria, specializzate per attività come la moltiplicazione di matrici, che sono centrali nei calcoli AI.\nQueste tecnologie trovano applicazioni nei carichi di lavoro AI e nell’elaborazione ad alte prestazioni, dove la sinergia di storage e calcolo può portare a significativi guadagni in termini di prestazioni. L’architettura è particolarmente utile per le attività di elaborazione intensiva comuni nei modelli di apprendimento automatico.\nMentre le tecnologie di elaborazione in memoria come ReRAM e PIM offrono interessanti prospettive di efficienza e prestazioni, presentano le loro sfide, come l’uniformità dei dati e i problemi di scalabilità in ReRAM (Imani, Rahimi, e S. Rosing 2016). Tuttavia, il campo è maturo per l’innovazione e affrontare queste limitazioni può aprire nuove frontiere nell’AI e nell’elaborazione ad alte prestazioni.\n\nImani, Mohsen, Abbas Rahimi, e Tajana S. Rosing. 2016. «Resistive Configurable Associative Memory for Approximate Computing». In Proceedings of the 2016 Design, Automation &amp; Test in Europe Conference &amp; Exhibition (DATE), 1327–32. IEEE; Research Publishing Services. https://doi.org/10.3850/9783981537079_0454.\n\n\n10.8.6 Calcolo Ottico\nNell’accelerazione dell’intelligenza artificiale, un’area di interesse in rapida crescita risiede nelle nuove tecnologie che si discostano dai paradigmi tradizionali. Alcune tecnologie emergenti menzionate sopra, come l’elettronica flessibile, il calcolo in memoria o persino il calcolo neuromorfico, stanno per diventare realtà, date le loro innovazioni e applicazioni rivoluzionarie. Una delle frontiere promettenti e all’avanguardia della prossima generazione è la tecnologia del calcolo ottico H. Zhou et al. (2022). Aziende come LightMatter sono pioniere nell’uso della fotonica luminosa per i calcoli, utilizzando quindi fotoni al posto degli elettroni per la trasmissione dei dati e l’elaborazione.\n\nZhou, Hailong, Jianji Dong, Junwei Cheng, Wenchan Dong, Chaoran Huang, Yichen Shen, Qiming Zhang, et al. 2022. «Photonic matrix multiplication lights up photonic accelerator and beyond». Light: Science &amp; Applications 11 (1): 30. https://doi.org/10.1038/s41377-022-00717-8.\n\nShastri, Bhavin J., Alexander N. Tait, T. Ferreira de Lima, Wolfram H. P. Pernice, Harish Bhaskaran, C. D. Wright, e Paul R. Prucnal. 2021. «Photonics for artificial intelligence and neuromorphic computing». Nat. Photonics 15 (2): 102–14. https://doi.org/10.1038/s41566-020-00754-y.\nIl calcolo ottico utilizza fotoni e dispositivi fotonici anziché i tradizionali circuiti elettronici per il calcolo e l’elaborazione dei dati. Trae ispirazione dai collegamenti di comunicazione in fibra ottica che si basano sulla luce per un trasferimento dati rapido ed efficiente (Shastri et al. 2021). La luce può propagarsi con una perdita molto inferiore rispetto agli elettroni dei semiconduttori, consentendo vantaggi intrinseci in termini di velocità ed efficienza.\nAlcuni vantaggi specifici dell’elaborazione ottica includono:\n\nAlta produttività: I fotoni possono trasmettere con larghezze di banda &gt;100 Tb/s utilizzando il multiplexing a divisione di lunghezza d’onda.\nBassa latenza: I fotoni interagiscono su scale temporali di femtosecondi, milioni di volte più velocemente dei transistor al silicio.\nParallelismo: Più segnali di dati possono propagarsi simultaneamente attraverso lo stesso mezzo ottico.\nBassa potenza: I circuiti fotonici che utilizzano guide d’onda e risonatori possono ottenere una logica e una memoria complesse con solo microwatt di potenza.\n\nTuttavia, l’elaborazione ottica deve attualmente affrontare sfide significative:\n\nMancanza di memoria ottica equivalente alla RAM elettronica\nRichiede la conversione tra domini ottici ed elettrici.\nSet limitato di componenti ottici disponibili rispetto al ricco ecosistema elettronico.\nMetodi di integrazione immaturi per combinare la fotonica con i tradizionali chip CMOS.\nModelli di programmazione complessi richiesti per gestire il parallelismo.\n\nDi conseguenza, l’elaborazione ottica è ancora in una fase di ricerca molto precoce nonostante il suo potenziale promettente. Tuttavia, le innovazioni tecniche potrebbero consentirgli di integrare l’elettronica e sbloccare guadagni di prestazioni per i carichi di lavoro AI. Aziende come Lightmatter sono pioniere nei primi acceleratori ottici AI. A lungo termine, se le sfide chiave saranno superate, potrebbe rappresentare un substrato di elaborazione rivoluzionario.\n\n\n10.8.7 Quantum Computing\nI computer quantistici sfruttano fenomeni unici della fisica quantistica, come la sovrapposizione e l’entanglement, per rappresentare ed elaborare informazioni in modi non possibili in modo classico. Invece dei bit binari, l’unità fondamentale è il bit quantistico o qubit. A differenza dei bit classici, che sono limitati a 0 o 1, i qubit possono esistere simultaneamente in una sovrapposizione di entrambi gli stati a causa degli effetti quantistici.\nAnche più qubit possono essere entangled, portando a una densità di informazioni esponenziale ma introducendo risultati probabilistici. La sovrapposizione consente il calcolo parallelo su tutti gli stati possibili, mentre l’entanglement consente correlazioni non locali tra qubit. Figura 10.10 trasmette visivamente le differenze tra i bit classici nell’informatica e i bit quantistici (qbit).\n\n\n\n\n\n\nFigura 10.10: Qubit, i mattoni del calcolo quantistico. Fonte: Microsoft\n\n\n\nGli algoritmi quantistici manipolano attentamente questi effetti meccanici quantistici intrinseci per risolvere problemi come l’ottimizzazione o la ricerca in modo più efficiente rispetto alle loro controparti classiche in teoria.\n\nTraining più rapido di reti neurali profonde sfruttando il parallelismo quantistico per operazioni di algebra lineare.\nGli algoritmi ML quantistici efficienti sfruttano le capacità uniche dei qubit.\nReti neurali quantistiche con effetti quantistici intrinseci integrati nell’architettura del modello.\nOttimizzatori quantistici che sfruttano algoritmi di “annealing” quantistica o adiabatici per problemi di ottimizzazione combinatoria.\n\nTuttavia, gli stati quantistici sono fragili e soggetti a errori che richiedono protocolli di correzione degli errori. La natura non intuitiva della programmazione quantistica introduce anche sfide non presenti nell’informatica classica.\n\nI bit quantistici rumorosi e fragili sono difficili da scalare. Il più grande computer quantistico odierno ha meno di 1000 qubit.\nInsieme limitato di porte e circuiti quantistici disponibili rispetto alla programmazione classica.\nMancanza di set di dati e benchmark per valutare l’apprendimento automatico quantistico in domini pratici.\n\nSebbene un vantaggio quantistico significativo per l’apprendimento automatico sia ancora lontano, la ricerca attiva presso aziende come D-Wave, Rigetti e IonQ sta facendo progredire l’ingegneria informatica quantistica e gli algoritmi quantistici. Le principali aziende tecnologiche come Google, IBM e Microsoft stanno esplorando attivamente l’informatica quantistica. Google ha recentemente annunciato un processore quantistico a 72 qubit chiamato Bristlecone e prevede di costruire un sistema quantistico commerciale a 49 qubit. Microsoft ha anche un programma di ricerca attivo nell’informatica quantistica topologica e collabora con la startup quantistica IonQ\nLe tecniche quantistiche potrebbero prima fare breccia nell’ottimizzazione prima di un’adozione più generalizzata dell’apprendimento automatico. La realizzazione del pieno potenziale dell’apprendimento automatico quantistico attende importanti traguardi nello sviluppo dell’hardware quantistico e nella maturità dell’ecosistema. Figura 10.11 confronta a titolo esemplificativo l’informatica quantistica e quella classica.\n\n\n\n\n\n\nFigura 10.11: Confronto tra il calcolo quantistico e il calcolo classico. Fonte: Devopedia",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Accelerazione IA</span>"
    ]
  },
  {
    "objectID": "contents/core/hw_acceleration/hw_acceleration.it.html#tendenze-future",
    "href": "contents/core/hw_acceleration/hw_acceleration.it.html#tendenze-future",
    "title": "10  Accelerazione IA",
    "section": "10.9 Tendenze Future",
    "text": "10.9 Tendenze Future\nIn questo capitolo, l’attenzione principale è stata rivolta alla progettazione di hardware specializzato ottimizzato per carichi di lavoro e algoritmi di machine learning. Questa discussione ha riguardato le architetture personalizzate di GPU e TPU per l’addestramento e l’inferenza delle reti neurali. Tuttavia, una direzione di ricerca emergente sta sfruttando l’apprendimento automatico per facilitare il processo di progettazione hardware stesso.\nIl processo di progettazione hardware comprende molte fasi complesse, tra cui specifica, modellazione di alto livello, simulazione, sintesi, verifica, prototipazione e fabbricazione. Gran parte di questo processo richiede tradizionalmente una vasta competenza umana, impegno e tempo. Tuttavia, i recenti progressi nell’apprendimento automatico stanno consentendo l’automazione e il miglioramento di parti del flusso di lavoro di progettazione hardware utilizzando tecniche di apprendimento automatico.\nEcco alcuni esempi di come l’apprendimento automatico sta trasformando la progettazione hardware:\n\nSintesi di circuiti automatizzata tramite apprendimento per rinforzo: Anziché realizzare manualmente progetti a livello di transistor, gli agenti di apprendimento automatico come l’apprendimento per rinforzo possono imparare a collegare porte logiche e generare automaticamente layout di circuiti. Ciò può accelerare il lungo processo di sintesi.\nSimulazione ed emulazione hardware basate su ML: I modelli di reti neurali profonde possono essere addestrati per prevedere come si comporterà un progetto hardware in diverse condizioni. Ad esempio, i modelli di apprendimento profondo possono essere addestrati per prevedere i conteggi dei cicli per determinati carichi di lavoro. Ciò consente una simulazione più rapida e accurata rispetto alle simulazioni RTL tradizionali.\nPianificazione automatizzata dei chip mediante algoritmi ML: La pianificazione dei chip comporta il posizionamento ottimale di diversi componenti su un die. Algoritmi evolutivi come quelli genetici e altri algoritmi ML come l’apprendimento per rinforzo vengono utilizzati per esplorare le opzioni di pianificazione. Ciò può migliorare significativamente i posizionamenti manuali di pianificazione in termini di tempi di consegna più rapidi e qualità dei posizionamenti.\nOttimizzazione dell’architettura basata su ML: Le nuove architetture hardware, come quelle per gli acceleratori ML efficienti, possono essere generate e ottimizzate automaticamente tramite la ricerca nello spazio di progettazione architettonica. Gli algoritmi di apprendimento automatico possono cercare efficacemente ampi spazi di progettazione architettonica.\n\nL’applicazione del ML all’automazione della progettazione hardware promette di rendere il processo più veloce, più economico e più efficiente. Apre possibilità di progettazione che richiederebbero più di una progettazione manuale. L’uso del ML nella progettazione hardware è un’area di ricerca attiva e di distribuzione precoce, e studieremo le tecniche coinvolte e il loro potenziale trasformativo.\n\n10.9.1 ML per l’automazione della progettazione hardware\nUna grande opportunità per l’apprendimento automatico nella progettazione hardware è l’automazione di parti del complesso e noioso flusso di lavoro di progettazione. Con “Hardware design automation (HDA)” ci si riferisce in generale all’uso di tecniche ML come l’apprendimento per rinforzo, algoritmi genetici e reti neurali per automatizzare attività come sintesi, verifica, floorplanning e altro. Ecco alcuni esempi di dove l’ML per HDA mostra una vera promessa:\n\nSintesi di circuiti automatizzata: La sintesi di circuiti comporta la conversione di una descrizione di alto livello della logica desiderata in un’implementazione di netlist a livello di gate ottimizzata. Questo processo complesso ha molte considerazioni e compromessi di progettazione. Gli agenti ML possono essere addestrati tramite l’apprendimento per rinforzo G. Zhou e Anderson (2023) per esplorare lo spazio di progettazione e produrre automaticamente sintesi ottimizzate. Startup come Symbiotic EDA stanno portando questa tecnologia sul mercato.\nAutomated chip floorplanning: Il Floorplanning si riferisce al posizionamento strategico di diversi componenti su un’area del chip. Algoritmi di ricerca come algoritmi genetici (Valenzuela e Wang 2000) e apprendimento per rinforzo (Mirhoseini et al. (2021), Agnesina et al. (2023)) possono essere utilizzati per automatizzare l’ottimizzazione il floorplan per ridurre al minimo la lunghezza dei collegamenti, il consumo di energia e altri obiettivi. Questi “floor planners” assistiti da ML automatizzati sono estremamente preziosi man mano che aumenta la complessità dei chip.\nSimulatori hardware ML: L’addestramento di modelli di reti neurali profonde per prevedere le prestazioni dei progetti hardware, poiché i simulatori possono accelerare il processo di simulazione di oltre 100 volte rispetto alle simulazioni architettoniche e RTL tradizionali.\nTraduzione automatica del codice: La conversione di linguaggi di descrizione hardware come Verilog in implementazioni RTL ottimizzate è fondamentale ma richiede molto tempo. I modelli ML possono essere addestrati per agire come agenti traduttori e automatizzare questo processo.\n\n\nZhou, Guanglei, e Jason H. Anderson. 2023. «Area-Driven FPGA Logic Synthesis Using Reinforcement Learning». In Proceedings of the 28th Asia and South Pacific Design Automation Conference, 159–65. ACM. https://doi.org/10.1145/3566097.3567894.\n\nValenzuela, Christine L, e Pearl Y Wang. 2000. «A genetic algorithm for VLSI floorplanning». In Parallel Problem Solving from Nature PPSN VI: 6th International Conference Paris, France, September 1820, 2000 Proceedings 6, 671–80. Springer.\n\nMirhoseini, Azalia, Anna Goldie, Mustafa Yazgan, Joe Wenjie Jiang, Ebrahim Songhori, Shen Wang, Young-Joon Lee, et al. 2021. «A graph placement methodology for fast chip design». Nature 594 (7862): 207–12. https://doi.org/10.1038/s41586-021-03544-w.\n\nAgnesina, Anthony, Puranjay Rajvanshi, Tian Yang, Geraldo Pradipta, Austin Jiao, Ben Keller, Brucek Khailany, e Haoxing Ren. 2023. «AutoDMP: Automated DREAMPlace-based Macro Placement». In Proceedings of the 2023 International Symposium on Physical Design, 149–57. ACM. https://doi.org/10.1145/3569052.3578923.\nI vantaggi dell’HDA che utilizza ML sono tempi di progettazione ridotti, ottimizzazioni superiori ed esplorazione di spazi di progettazione troppo complessi per approcci manuali. Ciò può accelerare lo sviluppo hardware e portare a progetti migliori.\nLe sfide includono i limiti della generalizzazione ML, la natura black-box di alcune tecniche e compromessi sull’accuratezza. Tuttavia, la ricerca sta rapidamente avanzando per affrontare questi problemi e rendere le soluzioni HDA ML robuste e affidabili per l’uso in produzione. HDA fornisce un’importante via per ML per trasformare la progettazione hardware.\n\n\n10.9.2 Simulazione e Verifica Hardware Basate su ML\nLa simulazione e la verifica dei progetti hardware sono fondamentali prima della produzione per garantire che il progetto si comporti come previsto. Gli approcci tradizionali come la simulazione “register-transfer level” (RTL) sono complessi e richiedono molto tempo. Il ML introduce nuove opportunità per migliorare la simulazione e la verifica dell’hardware. Ecco alcuni esempi:\n\nModellazione surrogata per la simulazione: Modelli surrogati di un progetto altamente accurati possono essere creati utilizzando reti neurali. Questi modelli prevedono gli output dagli input molto più velocemente della simulazione RTL, consentendo una rapida esplorazione dello spazio di progettazione. Aziende come Ansys utilizzano questa tecnica.\nSimulatori ML: Grandi modelli di reti neurali possono essere addestrati su simulazioni RTL per imparare a imitare la funzionalità di un progetto hardware. Una volta addestrato, il modello NN può essere un simulatore altamente efficiente per test di regressione e altre attività. Graphcore ha dimostrato un’accelerazione di oltre 100 volte con questo approccio.\nVerifica formale tramite ML: La verifica formale dimostra matematicamente le proprietà di un progetto. Le tecniche di ML possono aiutare a generare proprietà di verifica e imparare a risolvere le complesse prove formali necessarie, automatizzando parti di questo processo impegnativo. Startup come Cortical.io stanno introducendo sul mercato soluzioni di verifica ML formali.\nRilevamento di bug: I modelli ML possono essere addestrati per elaborare progetti hardware e identificare potenziali problemi. Ciò aiuta i progettisti umani a ispezionare progetti complessi e a trovare bug. Facebook ha mostrato modelli di rilevamento di bug per l’hardware dei suoi server.\n\nI principali vantaggi dell’applicazione di ML alla simulazione e alla verifica sono tempi di esecuzione più rapidi per la convalida del progetto, test più rigorosi e riduzione del lavoro umano. Le sfide includono la verifica della correttezza del modello ML e la gestione dei casi limite. ML promette di accelerare significativamente i flussi di lavoro di test.\n\n\n10.9.3 ML per Architetture Hardware Efficienti\nUn obiettivo chiave è la progettazione di architetture hardware ottimizzate per prestazioni, potenza ed efficienza. ML introduce nuove tecniche per automatizzare e migliorare l’esplorazione dello spazio di progettazione dell’architettura per hardware generico e specializzato come gli acceleratori ML. Alcuni esempi promettenti sono:\n\nRicerca di architetture per hardware: Tecniche di ricerca come algoritmi evolutivi (Kao e Krishna 2020), ottimizzazione bayesiana (Reagen et al. (2017), Bhardwaj et al. (2020)), apprendimento per rinforzo (Kao, Jeong, e Krishna (2020), Krishnan et al. (2022)) possono generare automaticamente nuove architetture hardware mutando e mescolando attributi di progettazione come dimensione della cache, numero di unità parallele, larghezza di banda della memoria e così via. Ciò consente un’esplorazione efficiente di ampi spazi di progettazione.\nModellazione predittiva per l’ottimizzazione: I modelli ML possono essere addestrati per prevedere metriche di prestazioni, potenza ed efficienza hardware per una determinata architettura. Questi diventano “modelli surrogati” (Krishnan et al. 2023) per una rapida ottimizzazione ed esplorazione dello spazio sostituendo lunghe simulazioni.\nOttimizzazione dell’acceleratore specializzato: Per chip specializzati come unità di elaborazione tensore per AI, tecniche di ricerca architettura automatizzata basate su algoritmi ML (D. Zhang et al. 2022) promettono di trovare progetti rapidi ed efficienti.\n\n\nKao, Sheng-Chun, e Tushar Krishna. 2020. «Gamma: automating the HW mapping of DNN models on accelerators via genetic algorithm». In Proceedings of the 39th International Conference on Computer-Aided Design, 1–9. ACM. https://doi.org/10.1145/3400302.3415639.\n\nReagen, Brandon, Jose Miguel Hernandez-Lobato, Robert Adolf, Michael Gelbart, Paul Whatmough, Gu-Yeon Wei, e David Brooks. 2017. «A case for efficient accelerator design space exploration via Bayesian optimization». In 2017 IEEE/ACM International Symposium on Low Power Electronics and Design (ISLPED), 1–6. IEEE; IEEE. https://doi.org/10.1109/islped.2017.8009208.\n\nBhardwaj, Kshitij, Marton Havasi, Yuan Yao, David M. Brooks, José Miguel Hernández-Lobato, e Gu-Yeon Wei. 2020. «A comprehensive methodology to determine optimal coherence interfaces for many-accelerator SoCs». In Proceedings of the ACM/IEEE International Symposium on Low Power Electronics and Design, 145–50. ACM. https://doi.org/10.1145/3370748.3406564.\n\nKao, Sheng-Chun, Geonhwa Jeong, e Tushar Krishna. 2020. «ConfuciuX: Autonomous Hardware Resource Assignment for DNN Accelerators using Reinforcement Learning». In 2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO), 622–36. IEEE; IEEE. https://doi.org/10.1109/micro50266.2020.00058.\n\nKrishnan, Srivatsan, Natasha Jaques, Shayegan Omidshafiei, Dan Zhang, Izzeddin Gur, Vijay Janapa Reddi, e Aleksandra Faust. 2022. «Multi-Agent Reinforcement Learning for Microprocessor Design Space Exploration». https://arxiv.org/abs/2211.16385.\n\nZhang, Dan, Safeen Huda, Ebrahim Songhori, Kartik Prabhu, Quoc Le, Anna Goldie, e Azalia Mirhoseini. 2022. «A full-stack search technique for domain optimized deep learning accelerators». In Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, 27–42. ASPLOS ’22. New York, NY, USA: ACM. https://doi.org/10.1145/3503222.3507767.\nI vantaggi dell’utilizzo di ML includono un’esplorazione dello spazio di progettazione superiore, ottimizzazione automatizzata e riduzione dello sforzo manuale. Le sfide includono lunghi tempi di training per alcune tecniche e limitazioni degli ottimi locali. Tuttavia, ML per l’architettura hardware ha un grande potenziale per rivelare miglioramenti in termini di prestazioni ed efficienza.\n\n\n10.9.4 ML per Ottimizzare la Produzione e Ridurre i Difetti\nUna volta completata la progettazione hardware, si passa alla produzione. Tuttavia, variabilità e difetti durante la produzione possono influire su rese e qualità. Le tecniche ML vengono ora applicate per migliorare i processi di fabbricazione e ridurre i difetti. Ecco alcuni esempi:\n\nManutenzione predittiva: I modelli ML possono analizzare i dati dei sensori delle apparecchiature nel tempo e identificare segnali che prevedono le esigenze di manutenzione prima del guasto. Ciò consente una manutenzione proattiva, che può essere molto utile nel costoso processo di fabbricazione.\nOttimizzazione del processo: I modelli di apprendimento supervisionato possono essere addestrati sui dati di processo per identificare i fattori che portano a basse rese. I modelli possono quindi ottimizzare i parametri per migliorare rese, produttività o coerenza.\nPrevisione della resa: Analizzando i dati di prova da progetti realizzati utilizzando tecniche come alberi di regressione, i modelli ML possono prevedere le rese all’inizio della produzione, consentendo aggiustamenti del processo.\nRilevamento dei difetti: Le tecniche di visione artificiale ML possono essere applicate alle immagini dei progetti per identificare difetti invisibili all’occhio umano. Ciò consente un controllo di qualità di precisione e un’analisi delle cause principali.\nAnalisi proattiva dei guasti: I modelli ML possono aiutare a prevedere, diagnosticare e prevenire i problemi che portano a difetti e guasti a valle analizzando i dati di processo strutturati e non strutturati.\n\nL’applicazione del ML alla produzione consente l’ottimizzazione dei processi, il controllo di qualità in tempo reale, la manutenzione predittiva e rese più elevate. Le sfide includono la gestione di dati di produzione complessi e varianti. Ma il ML è pronto a trasformare la produzione di semiconduttori.\n\n\n10.9.5 Verso Modelli di Base per la Progettazione Hardware\nCome abbiamo visto, l’apprendimento automatico sta aprendo nuove possibilità nel flusso di lavoro di progettazione hardware, dalle specifiche alla produzione. Tuttavia, le attuali tecniche di ML hanno ancora una portata limitata e richiedono un’ampia progettazione specifica per dominio. La visione a lungo termine è lo sviluppo di sistemi di intelligenza artificiale generali che possono essere applicati con versatilità in tutte le attività di progettazione hardware.\nPer realizzare appieno questa visione, sono necessari investimenti e ricerca per sviluppare modelli di base per la progettazione hardware. Si tratta di modelli e architetture ML unificati e generici che possono apprendere complesse competenze di progettazione hardware con i dati di training e gli obiettivi corretti.\nLa realizzazione di modelli di base per la progettazione hardware end-to-end richiederà quanto segue:\n\nAccumulare grandi set di dati di alta qualità ed etichettati in tutte le fasi di progettazione hardware per addestrare i modelli di base.\nProgressi nelle tecniche ML multimodali e multi-task per gestire la diversità di dati e attività di progettazione hardware.\nInterfacce e layer di astrazione per collegare i modelli di base ai flussi e agli strumenti di progettazione esistenti.\nSviluppo di ambienti di simulazione e benchmark per addestrare e testare i modelli di base sulle capacità di progettazione hardware.\nMetodi per spiegare e interpretare le decisioni di progettazione dei modelli ML e le ottimizzazioni per attendibilità e verifica.\nTecniche di compilazione per ottimizzare i modelli di base per un’implementazione efficiente su piattaforme hardware.\n\nSebbene siano ancora in corso ricerche significative, i modelli di base rappresentano l’obiettivo a lungo termine più trasformativo per l’infusione dell’IA nel processo della progettazione hardware. Democratizzare la progettazione hardware tramite sistemi ML versatili e automatizzati promette di aprire una nuova era di progettazione di chip ottimizzata, efficiente e innovativa. Il viaggio che ci attende è pieno di sfide e opportunità aperte.\nSe sei interessato alla progettazione di architetture per computer assistite da ML (Krishnan et al. 2023), invitiamo a leggere Architecture 2.0.\n\nKrishnan, Srivatsan, Amir Yazdanbakhsh, Shvetank Prakash, Jason Jabbour, Ikechukwu Uchendu, Susobhan Ghosh, Behzad Boroujerdian, et al. 2023. «ArchGym: An Open-Source Gymnasium for Machine Learning Assisted Architecture Design». In Proceedings of the 50th Annual International Symposium on Computer Architecture, 1–16. ACM. https://doi.org/10.1145/3579371.3589049.\nIn alternativa, si può guardare Video 10.3 for more details.\n\n\n\n\n\n\nVideo 10.3: Architecture 2.0",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Accelerazione IA</span>"
    ]
  },
  {
    "objectID": "contents/core/hw_acceleration/hw_acceleration.it.html#conclusione",
    "href": "contents/core/hw_acceleration/hw_acceleration.it.html#conclusione",
    "title": "10  Accelerazione IA",
    "section": "10.10 Conclusione",
    "text": "10.10 Conclusione\nL’accelerazione hardware specializzata è diventata indispensabile per abilitare applicazioni di intelligenza artificiale performanti ed efficienti, poiché modelli e set di dati esplodono in complessità. Questo capitolo ha esaminato i limiti dei processori generici come le CPU per i carichi di lavoro di intelligenza artificiale. La loro mancanza di parallelismo e di throughput computazionale non consente di addestrare o eseguire rapidamente reti neurali profonde all’avanguardia. Queste motivazioni hanno guidato le innovazioni negli acceleratori personalizzati.\nAbbiamo esaminato GPU, TPU, FPGA e ASIC progettati specificamente per le operazioni matematiche intensive inerenti alle reti neurali. Coprendo questo spettro di opzioni, abbiamo mirato a fornire un framework per ragionare attraverso la selezione dell’acceleratore in base a vincoli relativi a flessibilità, prestazioni, potenza, costi e altri fattori.\nAbbiamo anche esplorato il ruolo del software nell’abilitazione e nell’ottimizzazione attive dell’accelerazione dell’intelligenza artificiale. Ciò abbraccia astrazioni di programmazione, framework, compilatori e simulatori. Abbiamo discusso della progettazione congiunta hardware-software come metodologia proattiva per la creazione di sistemi di intelligenza artificiale più olistici integrando strettamente l’innovazione degli algoritmi e i progressi hardware.\nMa c’è molto di più in arrivo! Frontiere entusiasmanti come l’informatica analogica, le reti neurali ottiche e l’apprendimento automatico quantistico rappresentano direzioni di ricerca attive che potrebbero sbloccare miglioramenti di ordini di grandezza in termini di efficienza, velocità e scala rispetto ai paradigmi attuali.\nIn definitiva, l’accelerazione hardware specializzata rimane indispensabile per sbloccare le prestazioni e l’efficienza necessarie per soddisfare la promessa dell’intelligenza artificiale dal cloud all’edge. Ci auguriamo che questo capitolo fornisca utili informazioni di base e approfondimenti sulla rapida innovazione che si sta verificando in questo dominio.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Accelerazione IA</span>"
    ]
  },
  {
    "objectID": "contents/core/hw_acceleration/hw_acceleration.it.html#sec-ai-acceleration-resource",
    "href": "contents/core/hw_acceleration/hw_acceleration.it.html#sec-ai-acceleration-resource",
    "title": "10  Accelerazione IA",
    "section": "10.11 Risorse",
    "text": "10.11 Risorse\nEcco un elenco curato di risorse per supportare studenti e insegnanti nei loro percorsi di apprendimento e insegnamento. Lavoriamo continuamente per espandere questa raccolta e presto aggiungeremo nuovi esercizi.\n\n\n\n\n\n\nSlide\n\n\n\n\n\n\nProssimamente.\n\n\n\n\n\n\n\n\n\n\nVideo\n\n\n\n\n\n\nVideo 10.1\nVideo 10.2\nVideo 10.3\n\n\n\n\n\n\n\n\n\n\nEsercizi\n\n\n\n\n\n\nEsercizio 10.1",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Accelerazione IA</span>"
    ]
  },
  {
    "objectID": "contents/core/benchmarking/benchmarking.it.html",
    "href": "contents/core/benchmarking/benchmarking.it.html",
    "title": "11  Benchmarking dell’IA",
    "section": "",
    "text": "11.1 Panoramica\nIl benchmarking fornisce le misure essenziali necessarie per guidare il progresso dell’apprendimento automatico e comprendere veramente le prestazioni del sistema. Come ha affermato il fisico Lord Kelvin, “Misurare è conoscere”. I benchmark ci consentono di conoscere quantitativamente le capacità di diversi modelli, software e hardware. Consentono agli sviluppatori di ML di misurare il tempo di inferenza, l’utilizzo della memoria, il consumo energetico e altre metriche che caratterizzano un sistema. Inoltre, i benchmark creano processi standardizzati per la misurazione, consentendo confronti equi tra diverse soluzioni.\nQuando i benchmark vengono mantenuti nel tempo, diventano fondamentali per catturare i progressi attraverso generazioni di algoritmi, set di dati e hardware. I modelli e le tecniche che stabiliscono nuovi record sui benchmark di ML da un anno all’altro dimostrano miglioramenti tangibili in ciò che è possibile per l’apprendimento automatico “on-device”. Utilizzando i benchmark per misurare, i professionisti di ML possono conoscere le capacità reali dei loro sistemi e avere la certezza che ogni passaggio rifletta un progresso autentico verso lo stato dell’arte.\nIl benchmarking ha diversi obiettivi e scopi importanti che guidano la sua implementazione per i sistemi di apprendimento automatico.\nQuesto capitolo tratterà i 3 tipi di benchmark AI, le metriche standard, gli strumenti e le tecniche che i progettisti utilizzano per ottimizzare i loro sistemi e le sfide e le tendenze nel benchmarking.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Benchmarking dell'IA</span>"
    ]
  },
  {
    "objectID": "contents/core/benchmarking/benchmarking.it.html#panoramica",
    "href": "contents/core/benchmarking/benchmarking.it.html#panoramica",
    "title": "11  Benchmarking dell’IA",
    "section": "",
    "text": "Valutazione delle prestazioni. Ciò comporta la valutazione di parametri chiave come la velocità, l’accuratezza e l’efficienza di un dato modello. Ad esempio, in un contesto TinyML, è fondamentale confrontare la rapidità con cui un assistente vocale può riconoscere i comandi, poiché ciò valuta le prestazioni in tempo reale.\nValutazione della potenza. Valutare la potenza assorbita da un carico di lavoro insieme alle sue prestazioni equivale alla sua efficienza energetica. Poiché l’impatto ambientale dell’elaborazione ML continua a crescere, il benchmarking dell’energia può consentirci di ottimizzare meglio i sistemi per la sostenibilità.\nValutazione delle risorse. Ciò significa valutare l’impatto del modello sulle risorse critiche del sistema, tra cui durata della batteria, utilizzo della memoria e sovraccarico computazionale. Un esempio rilevante è il confronto del consumo della batteria di due diversi algoritmi di riconoscimento delle immagini in esecuzione su un dispositivo indossabile.\nValidazione e verifica. Il benchmarking aiuta a garantire che il sistema funzioni correttamente e soddisfi i requisiti specificati. Un modo è quello di controllare l’accuratezza di un algoritmo, come un cardiofrequenzimetro su uno smartwatch, rispetto alle letture di apparecchiature di livello medico come forma di validazione clinica.\nAnalisi competitiva. Ciò consente di confrontare le soluzioni con le offerte concorrenti sul mercato. Ad esempio, il benchmarking di un modello personalizzato di rilevamento di oggetti rispetto ai benchmark TinyML comuni come MobileNet e Tiny-YOLO.\nCredibilità. I benchmark accurati sostengono la credibilità delle soluzioni AI e delle organizzazioni che le sviluppano. Dimostrano un impegno verso trasparenza, onestà e qualità, essenziali per creare fiducia con utenti e stakeholder.\nRegolamentazione e Standardizzazione. Man mano che il settore dell’AI continua a crescere, cresce anche la necessità di regolamentazione e standardizzazione per garantire che le soluzioni AI siano sicure, etiche ed efficaci. I benchmark accurati e affidabili sono essenziali per questo quadro normativo, poiché forniscono i dati e le prove necessari per valutare la conformità con gli standard del settore e i requisiti legali.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Benchmarking dell'IA</span>"
    ]
  },
  {
    "objectID": "contents/core/benchmarking/benchmarking.it.html#contesto-storico",
    "href": "contents/core/benchmarking/benchmarking.it.html#contesto-storico",
    "title": "11  Benchmarking dell’IA",
    "section": "11.2 Contesto Storico",
    "text": "11.2 Contesto Storico\n\n11.2.1 Benchmark delle Prestazioni\nL’evoluzione dei benchmark nell’informatica illustra vividamente l’incessante ricerca dell’eccellenza e dell’innovazione da parte del settore. Nei primi giorni dell’informatica, negli anni ’60 e ’70, i benchmark erano rudimentali e progettati per i mainframe. Ad esempio, il benchmark Whetstone, che prende il nome dal compilatore Whetstone ALGOL, è stato uno dei primi test standardizzati per misurare le prestazioni aritmetiche in virgola mobile di una CPU. Questi benchmark pionieristici hanno spinto i produttori a perfezionare le loro architetture e algoritmi per ottenere punteggi di benchmark migliori.\nGli anni ’80 hanno segnato un cambiamento significativo con l’ascesa dei personal computer. Mentre aziende come IBM, Apple e Commodore gareggiavano per quote di mercato, i benchmark sono diventati strumenti essenziali per consentire una concorrenza leale. I benchmark CPU SPEC, introdotti dalla System Performance Evaluation Cooperative (SPEC), hanno stabilito test standardizzati che consentono confronti oggettivi tra diverse macchine. Questa standardizzazione ha creato un ambiente competitivo, spingendo i produttori di chip e i creatori di sistemi a migliorare continuamente le loro offerte hardware e software.\nGli anni ’90 hanno portato l’era delle applicazioni e dei videogiochi “graphics-intensive”. La necessità di benchmark per valutare le prestazioni delle schede grafiche ha portato alla creazione di 3DMark da parte di Futuremark. Mentre i giocatori e i professionisti cercavano schede grafiche ad alte prestazioni, aziende come NVIDIA e AMD sono state spinte a una rapida innovazione, portando a importanti progressi nella tecnologia GPU come gli shader programmabili.\nGli anni 2000 hanno visto un’impennata di telefoni cellulari e dispositivi portatili come i tablet. Con la portabilità è arrivata la sfida di bilanciare prestazioni e consumo energetico. Benchmark come MobileMark di BAPCo hanno valutato velocità e durata della batteria. Ciò ha spinto le aziende a sviluppare System-on-Chip (SOC) più efficienti dal punto di vista energetico, portando all’emergere di architetture come ARM che hanno dato priorità all’efficienza energetica.\nL’attenzione dell’ultimo decennio si è spostata verso il cloud computing, i big data e l’intelligenza artificiale. I provider di servizi cloud come Amazon Web Services e Google Cloud competono su prestazioni, scalabilità e convenienza. I benchmark specifici del cloud come CloudSuite sono diventati essenziali, spingendo i provider a ottimizzare la propria infrastruttura per servizi migliori.\n\n\n11.2.2 Benchmark Energetici\nIl consumo energetico e le preoccupazioni ambientali hanno acquisito importanza negli ultimi anni, rendendo il benchmarking energetico sempre più importante nel settore. Questo cambiamento è iniziato a metà degli anni 2000, quando i processori e i sistemi hanno iniziato a raggiungere i limiti di raffreddamento e la scalabilità è diventata un aspetto cruciale della costruzione di sistemi su larga scala grazie ai progressi di Internet. Da allora, le considerazioni energetiche si sono espanse fino a comprendere tutte le aree dell’informatica, dai dispositivi personali ai data center su larga scala.\nIl benchmarking energetico mira a misurare l’efficienza energetica dei sistemi informatici, valutando le prestazioni in relazione al consumo energetico. Ciò è fondamentale per diversi motivi:\n\nImpatto ambientale: Con la crescente impronta di carbonio del settore tecnologico, c’è un’urgente necessità di ridurre il consumo energetico.\nCosti operativi: Le spese energetiche costituiscono una parte significativa dei costi operativi del data center.\nLongevità del dispositivo: Per i dispositivi mobili, l’efficienza energetica ha un impatto diretto sulla durata della batteria e sull’esperienza utente.\n\nIn questo ambito sono emersi diversi benchmark chiave:\n\nSPEC Power: Introdotto nel 2007, SPEC Power è stato uno dei primi benchmark standard del settore per la valutazione delle caratteristiche di potenza e prestazioni dei server.\nGreen500: L’elenco Green500 classifica i supercomputer in base all’efficienza energetica, integrando l’elenco TOP500 incentrato sulle prestazioni.\nEnergy Star: Pur non essendo un benchmark in sé, il programma di certificazione ENERGY STAR for Computers ha spinto i produttori a migliorare l’efficienza energetica dell’elettronica di consumo.\n\nIl benchmarking energetico affronta sfide uniche, come la contabilizzazione di diversi carichi di lavoro e configurazioni di sistema e la misura accurata del consumo energetico su una gamma di hardware che varia da microWatt a megawatt nel consumo energetico. Man mano che l’IA e l’edge computing continuano a crescere, è probabile che il benchmarking energetico diventi ancora più critico, guidando lo sviluppo di ottimizzazioni hardware e software AI specializzate ed efficienti dal punto di vista energetico.\n\n\n11.2.3 Benchmark Personalizzati\nOltre ai benchmark standard del settore, ci sono benchmark personalizzati specificamente progettati per soddisfare i requisiti unici di una particolare applicazione o attività. Sono personalizzati in base alle esigenze specifiche dell’utente o dello sviluppatore, assicurando che le metriche delle prestazioni siano direttamente pertinenti all’uso previsto del modello o del sistema di intelligenza artificiale. I benchmark personalizzati possono essere creati da singole organizzazioni, ricercatori o sviluppatori e sono spesso utilizzati insieme ai benchmark standard del settore per fornire una valutazione completa delle prestazioni dell’intelligenza artificiale.\nAd esempio, un ospedale potrebbe sviluppare un benchmark per valutare un modello di intelligenza artificiale per prevedere la riammissione dei pazienti. Questo benchmark incorporerebbe metriche pertinenti alla popolazione di pazienti dell’ospedale, come dati demografici, anamnesi e fattori sociali. Allo stesso modo, il benchmark di rilevamento delle frodi di un istituto finanziario potrebbe concentrarsi sull’identificazione accurata delle transazioni fraudolente riducendo al minimo i falsi positivi. Nel settore automobilistico, un benchmark di veicoli autonomi potrebbe dare priorità alle prestazioni in diverse condizioni, alla risposta agli ostacoli e alla sicurezza. I rivenditori potrebbero confrontare i sistemi di raccomandazione utilizzando il tasso di clic, il tasso di conversione e la soddisfazione del cliente. Le aziende manifatturiere potrebbero confrontare i sistemi di controllo qualità in base all’identificazione dei difetti, all’efficienza e alla riduzione degli sprechi. In ogni settore, i benchmark personalizzati forniscono alle organizzazioni criteri di valutazione su misura per le loro esigenze e il loro contesto unici. Ciò consente una valutazione più significativa di quanto i sistemi di intelligenza artificiale soddisfino i requisiti.\nIl vantaggio dei benchmark personalizzati risiede nella loro flessibilità e pertinenza. Possono essere progettati per testare aspetti specifici delle prestazioni critici per il successo della soluzione di intelligenza artificiale nella sua applicazione prevista. Ciò consente una valutazione più mirata e accurata delle capacità del modello o del sistema di intelligenza artificiale. I benchmark personalizzati forniscono anche informazioni preziose sulle prestazioni delle soluzioni di intelligenza artificiale in scenari reali, il che può essere cruciale per identificare potenziali problemi e aree di miglioramento.\nNell’intelligenza artificiale, i benchmark svolgono un ruolo cruciale nel guidare il progresso e l’innovazione. Sebbene i benchmark siano stati a lungo utilizzati nell’informatica, la loro applicazione all’apprendimento automatico è relativamente recente. I benchmark incentrati sull’intelligenza artificiale forniscono metriche standardizzate per valutare e confrontare le prestazioni di diversi algoritmi, architetture di modelli e piattaforme hardware.\n\n\n11.2.4 Consenso della Comunità\nUna prerogativa fondamentale affinché un benchmark abbia un impatto è che deve riflettere le priorità e i valori condivisi della più ampia comunità di ricerca. I benchmark progettati in modo isolato rischiano di non ottenere accettazione se trascurano metriche chiave considerate importanti dai gruppi leader. Attraverso uno sviluppo collaborativo con la partecipazione aperta di laboratori accademici, aziende e altri stakeholder, i benchmark possono incorporare un contributo collettivo su capacità critiche che vale la pena misurare. Ciò aiuta a garantire che i benchmark valutino aspetti che la comunità concorda siano essenziali per far progredire il campo. Il processo di raggiungimento dell’allineamento su attività e metriche supporta di per sé la convergenza su ciò che conta di più.\nInoltre, i benchmark pubblicati con ampia co-paternità da istituzioni rispettate hanno autorità e validità che convincono la comunità ad adottarli come standard affidabili. I benchmark percepiti come distorti da particolari interessi aziendali o istituzionali generano scetticismo. Anche il coinvolgimento continuo della comunità attraverso workshop e sfide è fondamentale dopo la versione iniziale, ed è ciò che, ad esempio, ha portato al successo di ImageNet. Col progredire della ricerca, la partecipazione collettiva consente un continuo perfezionamento ed espansione dei benchmark nel tempo.\nInfine, rilasciare benchmark sviluppati dalla comunità con accesso aperto ne promuove l’adozione e l’uso coerente. Fornendo codice open source, documentazione, modelli e infrastrutture, riduciamo le barriere all’ingresso, consentendo ai gruppi di confrontare le soluzioni su un piano di parità con le implementazioni standardizzate. Questa coerenza è essenziale per confronti equi. Senza coordinamento, laboratori e aziende potrebbero implementare i benchmark in modo diverso, il che può compromettere la riproducibilità e la comparabilità dei risultati.\nIl consenso della comunità conferisce ai benchmark una rilevanza duratura, mentre la frammentazione confonde. Attraverso lo sviluppo collaborativo e un funzionamento trasparente, i benchmark possono diventare standard autorevoli per monitorare i progressi. Molti dei benchmark di cui parliamo in questo capitolo sono stati sviluppati e creati dalla comunità, per la comunità, ed è questo che alla fine ha portato al loro successo.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Benchmarking dell'IA</span>"
    ]
  },
  {
    "objectID": "contents/core/benchmarking/benchmarking.it.html#benchmark-ai-sistema-modello-e-dati",
    "href": "contents/core/benchmarking/benchmarking.it.html#benchmark-ai-sistema-modello-e-dati",
    "title": "11  Benchmarking dell’IA",
    "section": "11.3 Benchmark AI: Sistema, Modello e Dati",
    "text": "11.3 Benchmark AI: Sistema, Modello e Dati\nLa necessità di un benchmarking completo diventa fondamentale man mano che i sistemi AI diventano più complessi e onnipresenti. In questo contesto, i benchmark sono spesso classificati in tre categorie principali: Hardware, Modello e Dati. Analizziamo perché ognuno di questi gruppi è essenziale e il significato della valutazione dell’AI da queste tre dimensioni distinte:\n\n11.3.1 Benchmark di Sistema\nI calcoli AI, in particolare quelli nel deep learning, richiedono molte risorse. L’hardware su cui vengono eseguiti questi calcoli svolge un ruolo importante nel determinare la velocità, l’efficienza e la scalabilità delle soluzioni AI. Di conseguenza, i benchmark hardware aiutano a valutare le prestazioni di CPU, GPU, TPU e altri acceleratori nelle attività AI. Comprendendone le prestazioni, gli sviluppatori possono scegliere quali piattaforme hardware si adattano meglio a specifiche applicazioni AI. Inoltre, i produttori di hardware utilizzano questi benchmark per identificare aree di miglioramento, guidando l’innovazione nei progetti di chip specifici per AI.\n\n\n11.3.2 Benchmark del Modello\nL’architettura, le dimensioni e la complessità dei modelli AI variano notevolmente. Modelli diversi hanno diverse esigenze di calcolo e offrono diversi livelli di accuratezza ed efficienza. I benchmark dei modelli aiutano a valutare le prestazioni di varie architetture AI su attività standardizzate. Forniscono informazioni sulla velocità, l’accuratezza e le richieste di risorse di diversi modelli. Eseguendo il benchmarking dei modelli, i ricercatori possono identificare le architetture più performanti per attività specifiche, guidando la comunità AI verso soluzioni più efficienti ed efficaci. Inoltre, questi benchmark aiutano a monitorare i progressi della ricerca sull’intelligenza artificiale, mostrando i progressi nella progettazione e nell’ottimizzazione dei modelli.\n\n\n11.3.3 Benchmark dei Dati\nNell’apprendimento automatico, i dati sono fondamentali perché la qualità, la scala e la diversità dei set di dati influiscono direttamente sull’efficacia e sulla generalizzazione del modello. I benchmark dei dati si concentrano sui set di dati utilizzati nel training e nella valutazione. Forniscono set di dati standardizzati che la comunità può utilizzare per addestrare e testare i modelli, garantendo parità di condizioni per i confronti. Inoltre, questi parametri di riferimento evidenziano le sfide relative alla qualità dei dati, alla diversità e alla rappresentazione, spingendo la comunità ad affrontare i “bias” [pregiudizi] e i “gap” [lacune] nei dati di addestramento. Comprendendo i benchmark dei dati, i ricercatori possono anche valutare come i modelli potrebbero comportarsi in scenari reali, garantendo robustezza e affidabilità.\nNelle restanti sezioni, discuteremo ciascuno di questi tipi di benchmark. L’attenzione sarà rivolta a un’esplorazione approfondita dei benchmark di sistema, poiché sono fondamentali per comprendere e migliorare le prestazioni del sistema di apprendimento automatico. Parleremo brevemente dei benchmark dei modelli e dei dati per una prospettiva completa, ma l’enfasi e la maggior parte del contenuto saranno dedicati ai benchmark di sistema.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Benchmarking dell'IA</span>"
    ]
  },
  {
    "objectID": "contents/core/benchmarking/benchmarking.it.html#benchmarking-di-sistema",
    "href": "contents/core/benchmarking/benchmarking.it.html#benchmarking-di-sistema",
    "title": "11  Benchmarking dell’IA",
    "section": "11.4 Benchmarking di Sistema",
    "text": "11.4 Benchmarking di Sistema\n\n11.4.1 Granularità\nIl benchmarking del sistema di apprendimento automatico fornisce un approccio strutturato e sistematico per valutare le prestazioni di un sistema in diverse dimensioni. Data la complessità dei sistemi ML, possiamo analizzare le loro prestazioni attraverso diversi livelli di granularità e ottenere una visione completa dell’efficienza del sistema, identificare potenziali colli di bottiglia e individuare le aree di miglioramento. A tal fine, nel corso degli anni si sono evoluti vari tipi di benchmark che continuano a persistere.\nFigura 11.1 illustra i diversi livelli di granularità di un sistema ML. A livello di applicazione, i benchmark end-to-end valutano le prestazioni complessive del sistema, considerando fattori come la pre-elaborazione dei dati, l’addestramento del modello e l’inferenza. Mentre a livello di modello, i benchmark si concentrano sulla valutazione dell’efficienza e dell’accuratezza di modelli specifici. Ciò include la valutazione di quanto bene i modelli si generalizzano a nuovi dati e della loro efficienza computazionale durante l’addestramento e l’inferenza. Inoltre, il benchmarking può estendersi all’infrastruttura hardware e software, esaminando le prestazioni di singoli componenti come GPU o TPU.\n\n\n\n\n\n\nFigura 11.1: Granularità del sistema ML.\n\n\n\n\nMicro Benchmark\nI micro-benchmark sono specializzati e valutano componenti distinti o operazioni specifiche all’interno di un processo di apprendimento automatico più ampio. Questi benchmark si concentrano su singole attività, offrendo approfondimenti sulle richieste computazionali di un particolare layer di rete neurale, l’efficienza di un’unica tecnica di ottimizzazione o la produttività di una specifica funzione di attivazione. Ad esempio, i professionisti potrebbero utilizzare i micro-benchmark per misurare il tempo di calcolo richiesto da un layer convoluzionale in un modello di deep learning o per valutare la velocità di preelaborazione che alimenta i dati nel modello. Tali valutazioni granulari sono fondamentali per la messa a punto e l’ottimizzazione di aspetti discreti dei modelli, assicurando che ogni componente funzioni al massimo del suo potenziale.\nQuesti tipi di microbenchmark includono lo zoom su operazioni o componenti molto specifiche della pipeline AI, come le seguenti:\n\nOperazioni Tensoriali: Librerie come cuDNN (di NVIDIA) spesso hanno benchmark per misurare le prestazioni di singole operazioni tensoriali, come convoluzioni o moltiplicazioni di matrici, che sono fondamentali per i calcoli del deep learning.\nFunzioni di Attivazione: Benchmark che misurano la velocità e l’efficienza di varie funzioni di attivazione come ReLU, Sigmoid o Tanh in isolamento.\nBenchmark di Layer: Valutazioni dell’efficienza computazionale di distinti layer di rete neurale, come blocchi LSTM o Transformer, quando si opera su dimensioni di input standardizzate.\n\nEsempio: DeepBench, introdotto da Baidu, è un buon benchmark che valuta le operazioni fondamentali di deep learning, come quelle menzionate sopra. DeepBench valuta le prestazioni delle operazioni di base nei modelli di deep learning, fornendo informazioni su come diverse piattaforme hardware gestiscono l’addestramento e l’inferenza delle reti neurali.\n\n\n\n\n\n\nEsercizio 11.1: Benchmarking di Sistema - Operazioni Tensoriali\n\n\n\n\n\nCi si è mai chiesto come mai i filtri immagine diventano così veloci? Librerie speciali come cuDNN potenziano quei calcoli su determinati hardware. In questo Colab, useremo cuDNN con PyTorch per velocizzare il filtraggio delle immagini. Lo si consideri un piccolo benchmark, che mostra come il software giusto può sbloccare la potenza della GPU!\n\n\n\n\n\n\nMacro Benchmark\nI macro benchmark forniscono una visione olistica, valutando le prestazioni end-to-end di interi modelli di apprendimento automatico o sistemi di ML completi. Invece di concentrarsi sulle singole operazioni, i macro benchmark valutano l’efficacia collettiva dei modelli in scenari o attività del mondo reale. Ad esempio, un macro benchmark potrebbe valutare le prestazioni complete di un modello di apprendimento profondo che esegue la classificazione delle immagini su un set di dati come ImageNet. Ciò include la misura dell’accuratezza, della velocità di calcolo e del consumo di risorse. Allo stesso modo, si potrebbero misurare il tempo e le risorse cumulativi necessari per addestrare un modello di elaborazione del linguaggio naturale su corpora di testo estesi o valutare le prestazioni di un intero sistema di raccomandazione, dall’inserimento dei dati agli output finali specifici dell’utente.\nEsempi: Questi benchmark valutano il modello di intelligenza artificiale:\n\nMLPerf Inference (Reddi et al. 2020): Un set di benchmark standard per misurare le prestazioni di software e hardware di apprendimento automatico. MLPerf ha una suite di benchmark dedicati per scale specifiche, come MLPerf Mobile per dispositivi di classe mobile e MLPerf Tiny, che si concentra su microcontrollori e altri dispositivi con risorse limitate.\nMLMark di EEMBC: Una suite di benchmarking per valutare le prestazioni e l’efficienza energetica dei dispositivi embedded che eseguono carichi di lavoro di apprendimento automatico. Questo benchmark fornisce informazioni su come diverse piattaforme hardware gestiscono attività come il riconoscimento delle immagini o l’elaborazione audio.\nAI-Benchmark (Ignatov et al. 2019): Uno strumento di benchmarking progettato per dispositivi Android, valuta le prestazioni delle attività di intelligenza artificiale sui dispositivi mobili, comprendendo vari scenari del mondo reale come il riconoscimento delle immagini, l’analisi dei volti e il riconoscimento ottico dei caratteri.\n\n\nReddi, Vijay Janapa, Christine Cheng, David Kanter, Peter Mattson, Guenther Schmuelling, Carole-Jean Wu, Brian Anderson, et al. 2020. «MLPerf Inference Benchmark». In 2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA), 446–59. IEEE; IEEE. https://doi.org/10.1109/isca45697.2020.00045.\n\nIgnatov, Andrey, Radu Timofte, Andrei Kulik, Seungsoo Yang, Ke Wang, Felix Baum, Max Wu, Lirong Xu, e Luc Van Gool. 2019. «AI Benchmark: All About Deep Learning on Smartphones in 2019». In 2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW), 3617–35. IEEE. https://doi.org/10.1109/iccvw.2019.00447.\n\n\nBenchmark end-to-end\nI benchmark end-to-end forniscono una valutazione completa che si estende oltre i confini del modello di ML stesso. Invece di concentrarsi esclusivamente sull’efficienza o l’accuratezza computazionale di un modello di apprendimento automatico, questi benchmark comprendono l’intera pipeline di un sistema di IA. Ciò include la pre-elaborazione iniziale dei dati, le prestazioni del modello principale, la post-elaborazione degli output del modello e altri componenti integrali come l’archiviazione e le interazioni di rete.\nLa pre-elaborazione dei dati è la prima fase in molti sistemi di IA, trasformando i dati grezzi in un formato adatto per l’addestramento o l’inferenza del modello. L’efficienza, la scalabilità e l’accuratezza di queste fasi di pre-elaborazione sono vitali per le prestazioni complessive del sistema. I benchmark end-to-end valutano questa fase, assicurando che la pulizia dei dati, la normalizzazione, l’aumento o qualsiasi altro processo di trasformazione non diventi un collo di bottiglia.\nAnche la fase di post-elaborazione è al centro dell’attenzione. Ciò comporta l’interpretazione degli output grezzi del modello, eventualmente la conversione dei punteggi in categorie significative, il filtraggio dei risultati o persino l’integrazione con altri sistemi. Nelle applicazioni del mondo reale, questa fase è fondamentale per fornire informazioni fruibili e i benchmark end-to-end ne garantiscono l’efficienza e l’efficacia.\nOltre alle operazioni di base dell’IA, altri componenti del sistema sono importanti per le prestazioni complessive e l’esperienza utente. Le soluzioni di archiviazione, basate su cloud, on-premise o ibride, possono avere un impatto significativo sui tempi di recupero e archiviazione dei dati, in particolare con vasti set di dati di IA. Allo stesso modo, le interazioni di rete, vitali per le soluzioni di IA basate su cloud o per i sistemi distribuiti, possono diventare colli di bottiglia delle prestazioni se non ottimizzate. I benchmark end-to-end valutano in modo olistico questi componenti, assicurando che l’intero sistema funzioni senza problemi, dal recupero dei dati alla consegna dell’output finale.\nAd oggi, non esistono benchmark end-to-end pubblici che tengano conto del ruolo dell’archiviazione dei dati, della rete e delle prestazioni di elaborazione. Si può sostenere che MLPerf Training and Inference si avvicini all’idea di un benchmark end-to-end, ma si concentrano esclusivamente sulle prestazioni del modello ML e non rappresentano scenari di distribuzione nel mondo reale di come i modelli vengono utilizzati sul campo. Tuttavia, forniscono un segnale molto utile che aiuta a valutare le prestazioni del sistema AI.\nData la specificità intrinseca del benchmarking end-to-end, viene in genere eseguito internamente in un’azienda “strumentando” [inserendo punti di controllo] distribuzioni di produzione reali di AI. Ciò consente agli ingegneri di avere una comprensione e una ripartizione realistiche delle prestazioni, ma data la sensibilità e la specificità delle informazioni, raramente vengono segnalate all’esterno dell’azienda.\n\n\nComprendere i Compromessi\nDiversi problemi sorgono nelle diverse fasi di un sistema di intelligenza artificiale. I micro-benchmark aiutano a mettere a punto i singoli componenti, i macro-benchmark aiutano a perfezionare le architetture o gli algoritmi del modello e i benchmark end-to-end guidano l’ottimizzazione dell’intero flusso di lavoro. Comprendendo dove si trova un problema, gli sviluppatori possono applicare ottimizzazioni mirate.\nInoltre, mentre i singoli componenti di un sistema di intelligenza artificiale potrebbero funzionare in modo ottimale in isolamento, possono emergere colli di bottiglia quando interagiscono. I benchmark end-to-end, in particolare, sono fondamentali per garantire che l’intero sistema, quando funziona collettivamente, soddisfi gli standard di prestazioni ed efficienza desiderati.\nInfine, le organizzazioni possono prendere decisioni informate su dove allocare le risorse individuando colli di bottiglia o inefficienze nelle prestazioni. Ad esempio, se i micro-benchmark rivelano inefficienze in specifiche operazioni tensoriali, gli investimenti possono essere indirizzati verso acceleratori hardware specializzati. Al contrario, se i benchmark end-to-end indicano problemi di recupero dei dati, gli investimenti potrebbero essere incanalati verso soluzioni di archiviazione migliori.\n\n\n\n11.4.2 Componenti dei Benchmark\nIn sostanza, un benchmark AI è più di un semplice test o punteggio; è un framework di valutazione completo. Per comprenderlo in modo approfondito, analizziamo i componenti tipici che compongono un benchmark AI.\n\nDataset Standardizzati\nI set di dati fungono da base per la maggior parte dei benchmark AI. Forniscono un set di dati coerente su cui i modelli vengono addestrati e valutati, garantendo parità di condizioni per i confronti.\nEsempio: ImageNet, un set di dati su larga scala contenente milioni di immagini etichettate che abbracciano migliaia di categorie, è uno standard di benchmarking popolare per le attività di classificazione delle immagini.\n\n\nAttività Predefinite\nUn benchmark dovrebbe avere un obiettivo o un compito chiaro che i modelli mirano a raggiungere. Questo compito definisce il problema che il sistema AI sta cercando di risolvere.\nEsempio: I compiti per i benchmark di elaborazione del linguaggio naturale potrebbero includere analisi del “sentiment”, riconoscimento di entità denominate o traduzione automatica.\n\n\nMetriche di Valutazione\nUna volta definito un task, i benchmark richiedono parametri per quantificare le prestazioni. Questi parametri offrono misure oggettive per confrontare diversi modelli o sistemi. Nei task di classificazione, parametri come accuratezza, precisione, richiamo e punteggio F1 sono comunemente utilizzati. Errori quadratici medi o assoluti potrebbero essere utilizzati per i task di regressione. Possiamo anche misurare la potenza consumata dall’esecuzione del benchmark per calcolare l’efficienza energetica.\n\n\nBaseline e Modelli Baseline\nI benchmark spesso includono modelli “baseline” o implementazioni di riferimento. Di solito servono come punti di partenza o standard minimi di prestazione per confrontare nuovi modelli o nuove tecniche. I modelli “baseline” aiutano i ricercatori a misurare l’efficacia di nuovi algoritmi.\nNelle suite di benchmark, modelli semplici come la regressione lineare o le reti neurali di base sono spesso le baseline comuni. Queste forniscono un contesto quando si valutano modelli più complessi. Confrontando questi modelli più semplici, i ricercatori possono quantificare i miglioramenti derivanti da approcci avanzati.\nLe metriche delle prestazioni variano in base all’attività, ma ecco alcuni esempi:\n\nLe attività di classificazione utilizzano metriche come accuratezza, precisione, richiamo e punteggio F1.\nLe attività di regressione utilizzano spesso l’errore quadratico medio o l’errore assoluto medio.\n\n\n\nSpecifiche Hardware e Software\nData la variabilità introdotta da diverse configurazioni hardware e software, i benchmark spesso specificano o documentano gli ambienti hardware e software in cui vengono condotti i test.\nEsempio: Un benchmark AI potrebbe indicare che le valutazioni sono state condotte su una GPU NVIDIA Tesla V100 utilizzando TensorFlow v2.4.\n\n\nCondizioni Ambientali\nPoiché fattori esterni possono influenzare i risultati del benchmark, è essenziale controllare o documentare condizioni come temperatura, fonte di alimentazione o processi di background del sistema.\nEsempio: I benchmark AI mobili potrebbero specificare che i test sono stati condotti a temperatura ambiente con dispositivi collegati a una fonte di alimentazione per eliminare le variazioni del livello della batteria.\n\n\nRegole di Riproducibilità\nPer garantire che i benchmark siano credibili e possano essere replicati da altri nella comunità, spesso includono protocolli dettagliati che coprono tutto, dai “random seed” utilizzati agli iperparametri esatti.\nEsempio: Un benchmark per un’attività di learning di rinforzo potrebbe specificare gli episodi esatti dell’addestramento, i rapporti di esplorazione-sfruttamento e le strutture di ricompensa utilizzate.\n\n\nLinee Guida per l’Interpretazione dei Risultati\nOltre ai punteggi o alle metriche pure, i benchmark spesso forniscono linee guida o contesto per interpretare i risultati, aiutando i professionisti a comprendere le implicazioni più ampie.\nEsempio: Un benchmark potrebbe evidenziare che, sebbene il Modello A abbia ottenuto un punteggio più alto del Modello B in termini di accuratezza, offre migliori prestazioni in tempo reale, rendendolo più adatto per applicazioni sensibili al fattore tempo.\n\n\n\n11.4.3 I Benchmark del Training\nIl ciclo di vita dello sviluppo di un modello di apprendimento automatico prevede due fasi critiche: addestramento e inferenza. Il training [addestramento] rappresenta la fase in cui il sistema elabora e assimila dati grezzi per adattare e perfezionare i propri parametri. Il benchmarking della fase di training rivela come le scelte nella pipeline di dati, soluzioni di storage, architetture di modelli, risorse di elaborazione, impostazioni di iperparametri e algoritmi di ottimizzazione influiscono sull’efficienza e sulle richieste di risorse del training del modello. L’obiettivo è garantire che il sistema ML possa apprendere in modo efficiente dai dati, ottimizzando sia le prestazioni del modello sia l’utilizzo delle risorse del sistema.\n\nScopo\nDal punto di vista dei sistemi, l’addestramento dei modelli di apprendimento automatico richiede molte risorse, soprattutto quando si lavora con modelli di grandi dimensioni. Questi modelli spesso contengono miliardi o addirittura trilioni di parametri addestrabili e richiedono enormi quantità di dati, spesso su una scala di molti terabyte. Ad esempio, GPT-3 di OpenAI (Brown et al. 2020) ha 175 miliardi di parametri, è stato addestrato su 45 TB di dati compressi in testo normale e ha richiesto 3.640 petaflop-giorni di elaborazione per il pre-addestramento. I benchmark di training ML valutano i sistemi e le risorse necessari per gestire il carico computazionale dell’addestramento di tali modelli.\nAnche l’archiviazione e la distribuzione efficienti dei dati durante l’addestramento svolgono un ruolo importante nel processo di addestramento. Ad esempio, in un modello di apprendimento automatico che prevede riquadri di delimitazione attorno agli oggetti in un’immagine, potrebbero essere necessarie migliaia di immagini. Tuttavia, caricare un intero set di dati di immagini nella memoria è in genere irrealizzabile, quindi i professionisti si affidano ai caricatori di dati (come discusso in Sezione 6.4.3.1) dai framework ML. Il training di successo del modello dipende dalla consegna tempestiva ed efficiente dei dati, rendendo essenziale il benchmarking di strumenti come caricatori di dati, pipeline di dati, velocità di pre-elaborazione e tempi di recupero dell’archiviazione per comprenderne l’impatto sulle prestazioni del training.\nLa selezione dell’hardware è un altro fattore chiave nel training dei sistemi di machine learning, in quanto può avere un impatto significativo sui tempi. I benchmark di training valutano l’utilizzo di CPU, GPU, memoria e rete durante la fase di training per guidare le ottimizzazioni del sistema. È essenziale comprendere come vengono utilizzate le risorse: le GPU vengono sfruttate appieno? C’è un sovraccarico di memoria non necessario? I benchmark possono scoprire colli di bottiglia o inefficienze nell’utilizzo delle risorse, con conseguenti risparmi sui costi e miglioramenti delle prestazioni.\nIn molti casi, l’utilizzo di un singolo acceleratore hardware, come una singola GPU, non è sufficiente per soddisfare le esigenze computazionali del training di modelli su larga scala. I modelli di apprendimento automatico vengono spesso addestrati in data center con più GPU o TPU, dove il calcolo distribuito consente l’elaborazione parallela tra i nodi. I benchmark di addestramento valutano l’efficienza con cui il sistema si ridimensiona su più nodi, gestisce lo sharding dei dati e gestisce sfide come guasti o taglio dei nodi durante l’addestramento.\n\n\nMetriche\nSe viste da una prospettiva di sistema, le metriche di training offrono informazioni che trascendono gli indicatori di prestazioni algoritmiche convenzionali. Queste metriche misurano l’efficacia di apprendimento del modello e misurano l’efficienza, la scalabilità e la robustezza dell’intero sistema ML durante la fase di training. Analizziamo più a fondo queste metriche e il loro significato.\nLe seguenti metriche sono spesso considerate importanti:\n\nTempo di training: Il tempo necessario per addestrare un modello da zero fino a raggiungere un livello di prestazioni soddisfacente. Misura direttamente le risorse di elaborazione necessarie per addestrare un modello. Ad esempio, il BERT di Google (Devlin et al. 2019) è un modello di elaborazione del linguaggio naturale che richiede diversi giorni per l’addestramento su un corpus enorme di dati di testo utilizzando più GPU. Il lungo tempo di training è una sfida significativa in termini di consumo di risorse e costi. In alcuni casi, i benchmark possono invece misurare la produttività del training (campioni di training per unità di tempo). La produttività può essere calcolata molto più velocemente e facilmente del tempo di addestramento, ma potrebbe oscurare le metriche che ci interessano davvero (ad esempio, il tempo di addestramento).\nScalabilità: Quanto bene il processo di addestramento può gestire gli aumenti delle dimensioni dei dati o della complessità del modello. La scalabilità può essere valutata misurando il tempo di addestramento, l’utilizzo della memoria e altri consumi di risorse all’aumentare delle dimensioni dei dati o della complessità del modello. Ad esempio, l’addestramento del GPT-3 di OpenAI ha richiesto notevoli sforzi ingegneristici per adattare il processo di training a numerosi nodi GPU, in modo da gestire le enormi dimensioni del modello. Ciò ha comportato l’utilizzo di hardware specializzato, addestramento distribuito e altre tecniche per garantire che il modello potesse essere addestrato in modo efficiente.\nUtilizzo delle Risorse: La misura in cui il processo di addestramento utilizza le risorse di calcolo disponibili come CPU, GPU, memoria e I/O del disco. Un elevato utilizzo delle risorse può indicare un processo di training efficiente, mentre un basso utilizzo può suggerire colli di bottiglia o inefficienze. Ad esempio, il training di una rete neurale convoluzionale (CNN) per la classificazione delle immagini richiede notevoli risorse GPU. L’utilizzo di configurazioni multi-GPU e l’ottimizzazione del codice di training per l’accelerazione GPU possono migliorare notevolmente l’utilizzo delle risorse e l’efficienza del training.\nConsumo di Memoria: La quantità di memoria utilizzata dal processo di training. Il consumo di memoria può essere un fattore limitante per il training di modelli o set di dati di grandi dimensioni. Ad esempio, i ricercatori di Google hanno dovuto affrontare notevoli sfide di consumo di memoria durante il training di BERT. Il modello ha centinaia di milioni di parametri, che richiedono grandi quantità di memoria. I ricercatori hanno dovuto sviluppare tecniche per ridurre il consumo di memoria, come il checkpointing del gradiente e il parallelismo del modello.\nConsumo Energetico: L’energia consumata durante il training. Man mano che i modelli di apprendimento automatico diventano più complessi, il consumo energetico è diventato un fattore importante da considerare. Il training di grandi modelli di apprendimento automatico può consumare molta energia, e quindi molto carbonio. Ad esempio, si è stimato che l’addestramento di GPT-3 di OpenAI abbia un’impronta di carbonio equivalente a un viaggio in auto di 700.000 chilometri (~435,000 miglia).\nThroughput: l numero di campioni di addestramento elaborati per unità di tempo. Un throughput [produttività] più elevato indica generalmente un processo di addestramento più efficiente. La produttività è una metrica importante da considerare quando si addestra un sistema di raccomandazione per una piattaforma di e-commerce. Una produttività elevata assicura che il modello possa elaborare rapidamente grandi volumi di dati di interazione dell’utente, il che è fondamentale per mantenere la pertinenza e l’accuratezza delle raccomandazioni. Ma è anche importante capire come bilanciare la produttività con i limiti di latenza. Pertanto, un vincolo di produttività limitato dalla latenza viene spesso imposto agli accordi sul livello di servizio per le distribuzioni di applicazioni del data center.\nCosto: Il costo della training di un modello può includere sia risorse computazionali che umane. Il costo è importante quando si considera la praticità e la fattibilità del training di modelli grandi o complessi. Si stima che l’addestramento di modelli di linguaggio grandi come GPT-3 costi milioni di dollari. Questo costo include risorse computazionali, elettriche e umane necessarie per lo sviluppo e l’addestramento del modello.\nTolleranza agli Errori e Robustezza: La capacità del processo di training di gestire guasti o errori senza bloccarsi o produrre risultati errati. Questo è importante per garantire l’affidabilità del processo di addestramento. Errori di rete o malfunzionamenti hardware possono verificarsi in uno scenario reale in cui un modello di apprendimento automatico viene addestrato su un sistema distribuito. Negli ultimi anni, è diventato abbondantemente chiaro che gli errori derivanti dalla corruzione “silenziosa” dei dati sono emersi come un problema importante. Un processo di addestramento affidabile e tollerante agli errori può recuperare da tali errori senza compromettere l’integrità del modello.\nFacilità d’Uso e Flessibilità: La facilità con cui il processo di addestramento può essere impostato e utilizzato e la sua flessibilità nella gestione di diversi tipi di dati e modelli. In aziende come Google, l’efficienza può talvolta essere misurata dal numero di anni di “Software Engineer (SWE)” risparmiati poiché ciò si traduce direttamente in impatto. La facilità d’uso e la flessibilità possono ridurre il tempo e lo sforzo necessari per addestrare un modello. TensorFlow e PyTorch sono popolari framework di apprendimento automatico che forniscono interfacce intuitive e API flessibili per la creazione e l’addestramento di modelli di machine-learning. Questi framework supportano molte architetture di modelli e sono dotati di strumenti che semplificano il processo di addestramento.\nRiproducibilità: La capacità di riprodurre i risultati del processo di training. La riproducibilità è importante per verificare la correttezza e la validità di un modello. Tuttavia, le variazioni dovute alle caratteristiche stocastiche della rete spesso rendono difficile riprodurre il comportamento preciso delle applicazioni in fase di addestramento, il che può rappresentare una sfida per il benchmarking.\n\nEseguendo il benchmarking per questi tipi di metriche, possiamo ottenere una visione completa delle prestazioni e dell’efficienza del processo di training da una prospettiva di sistema. Ciò può aiutare a identificare le aree di miglioramento e garantire che le risorse siano utilizzate in modo efficace.\n\n\nI Benchmark\nEcco alcuni lavori originali che hanno gettato le basi fondamentali per lo sviluppo di benchmark sistematici per l’addestramento di sistemi di apprendimento automatico.\nMLPerf Training Benchmark: MLPerf è una suite di benchmark progettata per misurare le prestazioni di hardware, software e servizi di apprendimento automatico. Il benchmark di MLPerf Training (Mattson et al. 2020a) si concentra sul tempo necessario per addestrare i modelli a una metrica di qualità target. Include carichi di lavoro diversi, come classificazione delle immagini, rilevamento di oggetti, traduzione e apprendimento per rinforzo. Figura 11.2 evidenzia i miglioramenti delle prestazioni nelle versioni progressive dei benchmark di MLPerf Training, che hanno tutti superato la legge di Moore. L’utilizzo di trend di benchmarking standardizzati ci consente di mostrare rigorosamente la rapida evoluzione del ML computing.\n\n\n\n\n\n\nFigura 11.2: Tendenze delle prestazioni di MLPerf Training. Fonte: Mattson et al. (2020a).\n\n\n———, et al. 2020a. «MLPerf: An Industry Standard Benchmark Suite for Machine Learning Performance». IEEE Micro 40 (2): 8–16. https://doi.org/10.1109/mm.2020.2974843.\n\n\nMetriche:\n\nTempo di training per la qualità target\nThroughput (esempi al secondo)\nUtilizzo delle risorse (CPU, GPU, memoria, I/O del disco)\n\nDAWNBench: DAWNBench (Coleman et al. 2019) è una suite di benchmark incentrata sui tempi di training end-to-end del deep learning e sulle prestazioni di inferenza. Include attività comuni come la classificazione delle immagini e la risposta alle domande.\n\nColeman, Cody, Daniel Kang, Deepak Narayanan, Luigi Nardi, Tian Zhao, Jian Zhang, Peter Bailis, Kunle Olukotun, Chris Ré, e Matei Zaharia. 2019. «Analysis of DAWNBench, a Time-to-Accuracy Machine Learning Performance Benchmark». ACM SIGOPS Operating Systems Review 53 (1): 14–25. https://doi.org/10.1145/3352020.3352024.\nMetriche:\n\nTempo di training per la precisione target\nLatenza dell’inferenza\nCosto (in termini di risorse di cloud computing e storage)\n\nFathom: Fathom (Adolf et al. 2016) è un benchmark dell’Università di Harvard che valuta le prestazioni dei modelli di deep learning utilizzando un set diversificato di carichi di lavoro. Questi includono attività comuni come la classificazione delle immagini, il riconoscimento vocale e la modellazione del linguaggio.\n\nAdolf, Robert, Saketh Rama, Brandon Reagen, Gu-yeon Wei, e David Brooks. 2016. «Fathom: reference workloads for modern deep learning methods». In 2016 IEEE International Symposium on Workload Characterization (IISWC), 1–10. IEEE; IEEE. https://doi.org/10.1109/iiswc.2016.7581275.\nMetriche:\n\nOperazioni al secondo (per misurare l’efficienza computazionale)\nTempo di completamento per ogni carico di lavoro\nLarghezza di banda della memoria\n\n\n\nCaso d’Uso di Esempio\nSi immagini di essere stati incaricati di effettuare il benchmarking delle prestazioni di training di un modello di classificazione delle immagini su una piattaforma hardware specifica. Analizziamo come si potrebbe affrontare questa situazione:\n\nDefinire l’Attività: Per prima cosa, si sceglie un modello e un set di dati. In questo caso, si allenerà una CNN per classificare le immagini nel set di dati CIFAR-10, un benchmark ampiamente utilizzato nella visione artificiale.\nSelezionare il benchmark: La scelta di un benchmark ampiamente accettato aiuta a garantire che la configurazione sia confrontabile con altre valutazioni del mondo reale. Si potrebbe scegliere di utilizzare il benchmark di training MLPerf perché fornisce un carico di lavoro di classificazione delle immagini strutturato, rendendolo un’opzione pertinente e standardizzata per valutare le prestazioni di training su CIFAR-10. L’utilizzo di MLPerf consente di valutare il sistema rispetto a metriche standard del settore, contribuendo a garantire che i risultati siano significativi e confrontabili con quelli ottenuti su altre piattaforme hardware.\nIdentificare le Metriche Chiave: Ora, si decidono le metriche che aiuteranno a valutare le prestazioni di training del sistema. Per questo esempio, si potrebbero tracciare:\n\nTempo di Training: Quanto tempo ci vuole per raggiungere il 90% di accuratezza?\nProduttività: Quante immagini vengono elaborate al secondo?\nUtilizzo delle Risorse: Qual è l’utilizzo di GPU e CPU durante il training?\n\n\nAnalizzando queste metriche, si otterranno informazioni sulle prestazioni di training del modello sulla piattaforma hardware scelta. Valutare se il tempo di training soddisfa le aspettative, se ci sono colli di bottiglia, come GPU sottoutilizzate o caricamento lento dei dati. Questo processo aiuta a identificare aree per una potenziale ottimizzazione, come il miglioramento della gestione dei dati o la regolazione dell’allocazione delle risorse, e può guidare le future decisioni di benchmarking.\n\n\n\n11.4.4 Benchmark di Inferenza\nL’inferenza nell’apprendimento automatico si riferisce all’uso di un modello addestrato per fare previsioni su dati nuovi e mai visti prima. È la fase in cui il modello applica le conoscenze apprese per risolvere il problema per cui è stato progettato, come la classificazione di immagini, il riconoscimento vocale o la traduzione di testo.\n\nScopo\nQuando creiamo modelli di machine learning, il nostro obiettivo finale è di distribuirli in applicazioni del mondo reale in cui possano fornire previsioni accurate e affidabili su dati nuovi e mai visti. Questo processo di utilizzo di un modello addestrato per fare previsioni è noto come inferenza. Le prestazioni reali di un modello di apprendimento automatico possono differire in modo significativo dalle sue prestazioni su set di dati di addestramento o validazione, il che rende l’inferenza di benchmarking un passaggio cruciale nello sviluppo e nell’implementazione di modelli di machine learning.\nIl benchmarking dell’inferenza ci consente di valutare quanto bene un modello di apprendimento automatico funziona in scenari del mondo reale. Questa valutazione garantisce che il modello sia pratico e affidabile quando distribuito in applicazioni, fornendo una comprensione più completa del comportamento del modello con dati reali. Inoltre, il benchmarking può aiutare a identificare potenziali colli di bottiglia o limitazioni nelle prestazioni del modello. Ad esempio, se un modello impiega troppo tempo per dedurre, potrebbe non essere pratico per applicazioni in tempo reale come la guida autonoma o gli assistenti vocali.\nL’efficienza delle risorse è un altro aspetto critico dell’inferenza, poiché può essere computazionalmente intensiva e richiedere memoria e potenza di elaborazione significative. Il benchmarking aiuta a garantire che il modello sia efficiente per quanto riguarda l’utilizzo delle risorse, il che è particolarmente importante per i dispositivi edge con capacità computazionali limitate, come smartphone o dispositivi IoT. Inoltre, il benchmarking ci consente di confrontare le prestazioni del nostro modello con quelli concorrenti o versioni precedenti dello stesso modello. Questo confronto è essenziale per prendere decisioni informate su quale modello implementare in un’applicazione specifica.\nInfine, è fondamentale garantire che le previsioni del modello non siano solo accurate, ma anche coerenti tra diversi dati. Il benchmarking aiuta a verificare l’accuratezza e la coerenza del modello, assicurando che soddisfi i requisiti dell’applicazione. Valuta inoltre la robustezza del modello, assicurando che possa gestire la variabilità dei dati del mondo reale e comunque fare previsioni accurate.\n\n\nMetriche\n\nPrecisione: La precisione è una delle metriche più importanti quando si confrontano i modelli di machine learning. Quantifica la percentuale di previsioni corrette effettuate dal modello rispetto ai valori o alle etichette reali. Ad esempio, se un modello di rilevamento dello spam riesce a classificare correttamente 95 messaggi e-mail su 100, la sua precisione verrebbe calcolata al 95%.\nLatenza: La latenza è una metrica delle prestazioni che calcola il ritardo o l’intervallo di tempo tra la ricezione dell’input e la produzione dell’output corrispondente da parte del sistema di apprendimento automatico. Un esempio che descrive chiaramente la latenza è un’applicazione di traduzione in tempo reale; se esiste un ritardo di mezzo secondo dal momento in cui un utente inserisce una frase al momento in cui l’app visualizza il testo tradotto, la latenza del sistema è di 0.5 secondi.\nLatency-Bounded Throughput: Il throughput limitato dalla latenza è una metrica preziosa che combina gli aspetti di latenza e throughput, misurando il throughput massimo di un sistema pur rispettando un vincolo di latenza specificato. Ad esempio, in un’applicazione di streaming video che utilizza un modello di apprendimento automatico per generare e visualizzare automaticamente i sottotitoli, il throughput limitato dalla latenza misurerebbe quanti frame video il sistema può elaborare al secondo (throughput) garantendo al contempo che i sottotitoli vengano visualizzati con un ritardo non superiore a 1 secondo (latenza). Questa metrica è particolarmente importante nelle applicazioni in tempo reale in cui soddisfare i requisiti di latenza è fondamentale per l’esperienza utente.\nThroughput: Il throughput valuta la capacità del sistema misurando il numero di inferenze o previsioni che un modello di apprendimento automatico può gestire entro un’unità di tempo specifica. Si consideri un sistema di riconoscimento vocale che utilizza una Recurrent Neural Network (RNN) come modello sottostante; se questo sistema riesce a elaborare e comprendere 50 diverse clip audio in un minuto, allora la sua velocità di elaborazione è di 50 clip al minuto.\nEfficienza energetica: L’efficienza energetica è una metrica che determina la quantità di energia consumata dal modello di apprendimento automatico per eseguire una singola inferenza. Un esempio lampante di ciò sarebbe un modello di elaborazione del linguaggio naturale basato su un’architettura di rete Transformer; se utilizza 0,1 Joule di energia per tradurre una frase dall’inglese al francese, la sua efficienza energetica è misurata a 0,1 Joule per inferenza.\nUtilizzo della memoria: L’utilizzo della memoria quantifica il volume di RAM necessario a un modello di apprendimento automatico per svolgere attività di inferenza. Un esempio rilevante per illustrare questo sarebbe un sistema di riconoscimento facciale basato su una CNN; se un tale sistema richiede 150 MB di RAM per elaborare e riconoscere i volti all’interno di un’immagine, il suo utilizzo della memoria è di 150 MB.\n\n\n\nI Benchmark\nEcco alcuni lavori originali che hanno gettato le basi fondamentali per lo sviluppo di benchmark sistematici per sistemi di apprendimento automatico inferenziale.\nMLPerf Inference Benchmark: MLPerf Inference è una suite di benchmark completa che valuta le prestazioni dei modelli di apprendimento automatico durante la fase di inferenza. Comprende una varietà di carichi di lavoro, tra cui classificazione delle immagini, rilevamento di oggetti ed elaborazione del linguaggio naturale, con l’obiettivo di fornire metriche standardizzate e approfondite per la valutazione di diversi sistemi di inferenza. Le sue metriche includono:\nMLPerf Inference è una suite di benchmark completa che valuta le prestazioni dei modelli di apprendimento automatico durante la fase di inferenza. Comprende una varietà di carichi di lavoro, tra cui classificazione delle immagini, rilevamento di oggetti ed elaborazione del linguaggio naturale, con l’obiettivo di fornire metriche standardizzate e approfondite per la valutazione di diversi sistemi di inferenza.\nMetriche:\n\nTempo di inferenza\nLatenza\nThroughput [Produttività]\nPrecisione\nConsumo energetico\n\nAI Benchmark: AI Benchmark è uno strumento di benchmarking che valuta le prestazioni dei modelli di intelligenza artificiale e apprendimento automatico su dispositivi mobili e piattaforme di edge computing. Include test per attività di classificazione delle immagini, rilevamento di oggetti ed elaborazione del linguaggio naturale, fornendo un’analisi dettagliata delle prestazioni di inferenza su diverse piattaforme hardware. Le sue metriche includono:\nAI Benchmark è uno strumento di benchmarking che valuta le prestazioni dei modelli di intelligenza artificiale e di apprendimento automatico su dispositivi mobili e piattaforme di edge computing. Include test per attività di classificazione delle immagini, rilevamento di oggetti ed elaborazione del linguaggio naturale, fornendo un’analisi dettagliata delle prestazioni di inferenza su diverse piattaforme hardware.\nMetriche:\n\nTempo di inferenza\nLatenza\nConsumo energetico\nUtilizzo della memoria\nThroughput [Produttività]\n\nToolkit OpenVINO: Il toolkit OpenVINO fornisce uno strumento di benchmark per misurare le prestazioni dei modelli di apprendimento profondo per varie attività, come la classificazione delle immagini, il rilevamento degli oggetti e il riconoscimento facciale, su hardware Intel. Offre approfondimenti dettagliati sulle prestazioni di inferenza dei modelli su diverse configurazioni hardware. Le sue metriche includono:\nMetriche:\n\nTempo di inferenza\nThroughput [Produttività]\nLatenza\nUtilizzo di CPU e GPU\n\n\n\nCaso d’Uso di Esempio\nSupponiamo che sia stato assegnato il compito di valutare le prestazioni di inferenza di un modello di rilevamento di oggetti su uno specifico dispositivo edge. Ecco come ci si potrebbe approcciare alla strutturazione di questo benchmark:\n\nDefinire l’Attività: In questo caso, l’attività è il rilevamento di oggetti in tempo reale su flussi video, identificando oggetti come veicoli, pedoni e segnali stradali.\nSelezionare il Benchmark: Per allinearsi all’obiettivo di valutare l’inferenza su un dispositivo edge, l’AI Benchmark è una scelta adatta. Fornisce un framework standardizzato specificamente per valutare le prestazioni di inferenza su hardware edge, rendendolo rilevante per questo scenario.\nIdentificare le Metriche Chiave: Ora, si determinano le metriche che aiuteranno a valutare le prestazioni di inferenza del modello. Per questo esempio, si potrebbero tracciare:\n\nTempo di Inferenza: Quanto tempo ci vuole per elaborare ogni fotogramma video?\nLatenza: Qual è il ritardo nella generazione di bounding box per gli oggetti rilevati?\nConsumo Energetico: Quanta energia viene utilizzata durante l’inferenza?\nProduttività: Quanti frame video vengono elaborati al secondo?\n\n\nMisurando queste metriche, si otterranno informazioni su quanto bene funziona il modello di rilevamento degli oggetti sul dispositivo edge. Ciò può aiutare a identificare eventuali colli di bottiglia, come l’elaborazione lenta dei frame o l’elevato consumo energetico, e a evidenziare aree per una potenziale ottimizzazione per migliorare le prestazioni in tempo reale.\n\n\n\n\n\n\nEsercizio 11.2: Benchmark di Inferenza - MLPerf\n\n\n\n\n\nPrepararsi a mettere alla prova i propri modelli di intelligenza artificiale! MLPerf è come le Olimpiadi per le prestazioni del machine learning. In questo Colab, utilizzeremo un toolkit chiamato CK per eseguire benchmark MLPerf ufficiali, misurare la velocità e l’accuratezza di un proprio modello e persino utilizzare TVM per dargli una spinta super veloce. Pronti a vedere il modello vincere la sua medaglia?\n\n\n\n\n\n\n\n11.4.5 Selezione delle Attività di Benchmark\nLa selezione di attività rappresentative per il benchmarking dei sistemi di machine learning è complessa a causa delle diverse applicazioni, tipi di dati e requisiti nei diversi domini. L’apprendimento automatico viene applicato in settori quali sanità, finanza, elaborazione del linguaggio naturale e visione artificiale, ognuno con attività uniche che potrebbero non essere pertinenti o paragonabili ad altre. Le principali sfide nella selezione delle attività includono:\n\nDiversità di Applicazioni e Tipi di Dati: Le attività nei vari domini coinvolgono diversi tipi di dati (ad esempio testo, immagini, video) e qualità, rendendo difficile trovare benchmark che rappresentino universalmente le sfide dell’apprendimento automatico.\nComplessità delle Attività e Necessità di Risorse: Le attività variano in complessità e richieste di risorse, con alcune che richiedono una notevole potenza di calcolo e modelli sofisticati, mentre altre possono essere affrontate con risorse e metodi più semplici.\nProblemi di Privacy: Le attività che coinvolgono dati sensibili, come cartelle cliniche o informazioni personali, introducono problemi etici e di privacy, rendendole inadatte per benchmark generali.\nMetriche di Valutazione: Le metriche delle prestazioni variano notevolmente tra le attività e i risultati di un’attività spesso non si generalizzano ad altre, complicando i confronti e limitando le informazioni da un’attività di benchmarking a un’altra.\n\nAffrontare queste sfide è essenziale per progettare benchmark significativi che siano pertinenti tra le diverse attività incontrate nell’apprendimento automatico, assicurando che i benchmark forniscano informazioni utili e generalizzabili sia per la formazione che per l’inferenza.\n\n\n11.4.6 Misura dell’Efficienza Energetica\nCon l’espansione delle capacità di apprendimento automatico, sia nel training che nell’inferenza, le preoccupazioni relative all’aumento del consumo energetico e al suo impatto ecologico si sono intensificate. Affrontare la sostenibilità dei sistemi ML, un argomento esplorato più approfonditamente nel capitolo IA Sostenibile, è quindi diventata una priorità fondamentale. Questa attenzione alla sostenibilità ha portato allo sviluppo di benchmark standardizzati progettati per misurare con precisione l’efficienza energetica. Tuttavia, la standardizzazione di queste metodologie pone delle sfide dovute alla necessità di adattarsi a scale molto diverse, dal consumo di microwatt dei dispositivi TinyML alle richieste di megawatt dei sistemi di training dei data center. Inoltre, per garantire che il benchmarking sia equo e riproducibile è necessario adattarsi alla vasta gamma di configurazioni hardware e architetture in uso oggi.\nUn esempio è la metodologia di benchmarking MLPerf Power (Tschand et al. 2024), che affronta queste sfide adattando le metodologie per data center, edge inference e tiny inference systems, misurando al contempo il consumo energetico nel modo più completo possibile per ogni scala. Questa metodologia si adatta a una varietà di hardware, dalle CPU generiche agli acceleratori AI specializzati, mantenendo principi di misurazione uniformi per garantire che i confronti siano equi e accurati su diverse piattaforme.\nFigura 11.3 illustra i limiti di misurazione dell’alimentazione per diverse scale di sistema, dai dispositivi TinyML ai nodi di inferenza e ai rack di training. Ciascun esempio evidenzia i componenti all’interno del limite di misurazione e quelli al di fuori di esso. Questa configurazione consente una riflessione accurata dei veri costi energetici associati all’esecuzione di carichi di lavoro ML in vari scenari del mondo reale e garantisce che il benchmark catturi l’intero spettro di consumo energetico.\n\n\n\n\n\n\nFigura 11.3: Diagramma di misurazione del sistema MLPerf Power. Fonte: Tschand et al. (2024).\n\n\nTschand, Arya, Arun Tejusve Raghunath Rajan, Sachin Idgunji, Anirban Ghosh, Jeremy Holleman, Csaba Kiraly, Pawan Ambalkar, et al. 2024. «MLPerf Power: Benchmarking the Energy Efficiency of Machine Learning Systems from Microwatts to Megawatts for Sustainable AI». arXiv preprint arXiv:2410.12032, ottobre. http://arxiv.org/abs/2410.12032v1.\n\n\nÈ importante notare che l’ottimizzazione di un sistema per le prestazioni potrebbe non portare all’esecuzione più efficiente dal punto di vista energetico. Spesso, sacrificare una piccola quantità di prestazioni o accuratezza può portare a guadagni significativi nell’efficienza energetica, evidenziando l’importanza di un benchmarking accurato delle metriche della potenza. Le future intuizioni dal benchmarking dell’efficienza energetica e della sostenibilità ci consentiranno di ottimizzare per sistemi ML più sostenibili.\n\n\n11.4.7 Esempio di Benchmark\nPer illustrare correttamente i componenti di un benchmark di sistema, possiamo esaminare il benchmark di individuazione delle parole chiave in MLPerf Tiny e spiegare la motivazione alla base di ogni decisione.\n\nTask\nL’individuazione delle parole chiave è stata selezionata come attività perché è un caso d’uso comune in TinyML che è stato ben consolidato per anni. Inoltre, l’hardware tipico utilizzato per l’individuazione delle parole chiave differisce sostanzialmente dalle offerte di altri benchmark, come l’attività di riconoscimento vocale di MLPerf Inference.\n\n\nIl Dataset\nGoogle Speech Commands (Warden 2018) è stato selezionato come il miglior dataset per rappresentare l’attività. Il dataset è ben consolidato nella comunità di ricerca e ha una licenza permissiva, che consente di utilizzarlo facilmente in un benchmark.\n\nWarden, Pete. 2018. «Speech Commands: A Dataset for Limited-Vocabulary Speech Recognition». ArXiv preprint abs/1804.03209 (aprile). http://arxiv.org/abs/1804.03209v1.\n\n\nModello\nIl componente principale successivo è il modello, che fungerà da carico di lavoro primario per il benchmark. Il modello dovrebbe essere ben consolidato come soluzione per l’attività selezionata piuttosto che una soluzione all’avanguardia. Il modello selezionato è un semplice modello di convoluzione separabile in profondità. Questa architettura non è la soluzione all’avanguardia per l’attività, ma è ben consolidata e non progettata per una piattaforma hardware specifica come molte soluzioni all’avanguardia. Nonostante sia un benchmark di inferenza, stabilisce anche una ricetta di training di riferimento per essere completamente riproducibile e trasparente.\n\n\nMetriche\nLa latenza è stata selezionata come metrica primaria per il benchmark, poiché i sistemi di individuazione delle parole chiave devono reagire rapidamente per mantenere la soddisfazione dell’utente. Inoltre, dato che i sistemi TinyML sono spesso alimentati a batteria, il consumo energetico viene misurato per garantire l’efficienza della piattaforma hardware. L’accuratezza del modello viene misurata anche per garantire che le ottimizzazioni applicate da un submitter, come la quantizzazione, non degradino l’accuratezza oltre una soglia.\n\n\nBenchmark Harness\nMLPerf Tiny utilizza EEMBCs EnergyRunner benchmark harness per caricare gli input nel modello e isolare e misurare il consumo energetico del dispositivo. Quando si misura il consumo energetico, è fondamentale selezionare un “harness” [imbracatura] che sia accurato ai livelli di potenza previsti dei dispositivi sottoposti a test e sufficientemente semplice da non diventare un peso per i partecipanti al benchmark.\n\n\nLa Baseline\nGli invii di baseline sono fondamentali per contestualizzare i risultati e come punto di riferimento per aiutare i partecipanti a iniziare. L’invio di base dovrebbe dare priorità alla semplicità e alla leggibilità rispetto alle prestazioni avanzate. L’individuazione della parola chiave della baseline utilizza un microcontrollore STM standard come hardware e TensorFlow Lite come microcontrollore (David et al. 2021) come framework di inferenza.\n\nDavid, Robert, Jared Duke, Advait Jain, Vijay Janapa Reddi, Nat Jeffries, Jian Li, Nick Kreeger, et al. 2021. «Tensorflow lite micro: Embedded machine learning for tinyml systems». Proceedings of Machine Learning and Systems 3: 800–811.\n\n\n\n11.4.8 Sfide e Limitazioni\nSebbene il benchmarking fornisca una metodologia strutturata per la valutazione delle prestazioni in domini complessi come l’intelligenza artificiale e l’informatica, il processo pone anche diverse sfide. Se non affrontati correttamente, questi ostacoli possono minare la credibilità e l’accuratezza dei risultati del benchmarking. Alcune delle difficoltà predominanti affrontate nel benchmarking includono quanto segue:\n\nCopertura incompleta del problema: Le attività di benchmarking potrebbero non rappresentare completamente lo spazio del problema. Ad esempio, i set di dati di classificazione delle immagini comuni come CIFAR-10 hanno una diversità limitata nei tipi di immagini. Gli algoritmi ottimizzati per tali benchmark potrebbero non riuscire a generalizzare bene con i set di dati del mondo reale.\nInsignificanza statistica: I benchmark devono avere prove e campioni di dati sufficienti per produrre risultati statisticamente significativi. Ad esempio, il benchmarking di un modello OCR su solo poche scansioni di testo potrebbe non catturare adeguatamente i suoi veri tassi di errore.\nRiproducibilità limitata: Variazioni di hardware, versioni software, basi di codice e altri fattori possono ridurre la riproducibilità dei risultati di benchmark. MLPerf affronta questo problema fornendo implementazioni di riferimento e specifiche ambientali.\nDisallineamento con gli obiettivi finali: I benchmark che si concentrano solo su metriche di velocità o accuratezza possono disallineare gli obiettivi reali come costi ed efficienza energetica. I benchmark devono riflettere tutti gli assi prestazionali critici.\nRapida obsolescenza: A causa del rapido ritmo dei progressi nell’intelligenza artificiale e nell’informatica, i benchmark e i loro set di dati possono rapidamente diventare obsoleti. Mantenere benchmark aggiornati è quindi una sfida persistente.\n\nMa di tutte queste, la sfida più importante è l’ingegneria dei benchmark.\n\nLotteria Hardware\nLa lotteria hardware, descritta per la prima volta da Hooker (2021), si riferisce alla situazione in cui il successo o l’efficienza di un modello di apprendimento automatico sono significativamente influenzati dalla sua compatibilità con l’hardware sottostante (Chu et al. 2021). Alcuni modelli hanno prestazioni eccezionali non perché sono intrinsecamente superiori, ma perché sono ottimizzati per caratteristiche hardware specifiche, come le capacità di elaborazione parallela delle unità di elaborazione grafica (GPU) o delle unità di elaborazione tensoriale (TPU).\n\nHooker, Sara. 2021. «The hardware lottery». Communications of the ACM 64 (12): 58–65. https://doi.org/10.1145/3467017.\nAd esempio, Figura 11.4 confronta le prestazioni dei modelli su diverse piattaforme hardware. I modelli multi-hardware mostrano risultati comparabili a “MobileNetV3 Large min” sia sulle configurazioni CPU uint8 che GPU. Tuttavia, questi modelli multi-hardware dimostrano miglioramenti significativi delle prestazioni rispetto alla baseline MobileNetV3 Large quando eseguiti su hardware EdgeTPU e DSP. Ciò sottolinea l’efficienza variabile dei modelli multi-hardware in ambienti di elaborazione specializzati.\n\n\n\n\n\n\nFigura 11.4: Compromessi tra precisione e latenza di più modelli ML e modalità di funzionamento su vari hardware. Fonte: Chu et al. (2021)\n\n\nChu, Grace, Okan Arikan, Gabriel Bender, Weijun Wang, Achille Brighton, Pieter-Jan Kindermans, Hanxiao Liu, Berkin Akin, Suyog Gupta, e Andrew Howard. 2021. «Discovering Multi-Hardware Mobile Models via Architecture Search». In 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), 3016–25. IEEE. https://doi.org/10.1109/cvprw53098.2021.00337.\n\n\nLa lotteria hardware può introdurre sfide e pregiudizi nel benchmarking dei sistemi di apprendimento automatico, poiché le prestazioni del modello non dipendono esclusivamente dall’architettura o dall’algoritmo del modello ma anche dalla compatibilità e dalle sinergie con l’hardware sottostante. Ciò può rendere difficile confrontare equamente diversi modelli e identificare il modello migliore in base ai suoi meriti intrinseci. Può anche portare a una situazione in cui la comunità converge su modelli che sono adatti all’hardware più diffuso del momento, trascurando potenzialmente altri modelli che potrebbero essere superiori ma incompatibili con le attuali tendenze hardware.\n\n\nBenchmark Engineering\nLa lotteria hardware si verifica quando un modello di apprendimento automatico funziona in modo eccezionalmente bene o male su una configurazione hardware specifica a causa di compatibilità o incompatibilità impreviste. Il modello non è esplicitamente progettato o ottimizzato per quell’hardware specifico dagli sviluppatori o dagli ingegneri; piuttosto, capita che si allinei o (non si allinei) con le capacità o le limitazioni dell’hardware. In questo caso, le prestazioni del modello sull’hardware sono un prodotto della coincidenza piuttosto che della progettazione.\nContrariamente alla lotteria hardware accidentale, il benchmark engineering implica l’ottimizzazione o la progettazione deliberata di un modello di apprendimento automatico per funzionare eccezionalmente bene su hardware specifico, spesso per vincere benchmark o competizioni. Questa ottimizzazione intenzionale potrebbe includere la modifica dell’architettura, degli algoritmi o dei parametri del modello per sfruttare appieno le funzionalità e le capacità dell’hardware.\n\nProblema\nIl benchmark engineering si riferisce alla modifica o all’ottimizzazione di un sistema di intelligenza artificiale per ottimizzare le prestazioni su test di benchmark specifici, spesso a scapito della generalizzabilità o delle prestazioni nel mondo reale. Ciò può includere la regolazione di iperparametri, dati di training o altri aspetti del sistema specificamente per ottenere punteggi elevati sulle metriche di benchmark senza necessariamente migliorare la funzionalità o l’utilità complessiva del sistema.\nLa motivazione alla base dell’ingegneria dei benchmark spesso deriva dal desiderio di ottenere punteggi di prestazioni elevate per scopi di marketing o competitivi. Punteggi di benchmark elevati possono dimostrare la superiorità di un sistema di intelligenza artificiale rispetto ai concorrenti e possono essere un argomento chiave per la vendita per potenziali utenti o investitori. Questa pressione per ottenere buoni risultati nei benchmark a volte porta a dare priorità alle ottimizzazioni specifiche del benchmark rispetto a miglioramenti più olistici del sistema.\nPuò comportare diversi rischi e sfide. Uno dei rischi principali è che il sistema di intelligenza artificiale possa funzionare meglio nelle applicazioni del mondo reale rispetto a quanto suggeriscono i punteggi di benchmark. Ciò può portare a insoddisfazione dell’utente, danni alla reputazione e potenziali problemi di sicurezza o etici. Inoltre, l’ingegneria dei benchmark può contribuire a una mancanza di trasparenza e responsabilità nella comunità dell’intelligenza artificiale, poiché può essere difficile discernere quanta parte delle prestazioni di un sistema di intelligenza artificiale sia dovuta a miglioramenti genuini rispetto a ottimizzazioni specifiche del benchmark.\nLa comunità AI deve dare priorità alla trasparenza e alla responsabilità per mitigare i rischi associati all’ingegneria dei benchmark. Ciò può includere la divulgazione di eventuali ottimizzazioni o modifiche apportate specificamente per i test di benchmark e la fornitura di valutazioni più complete dei sistemi AI che includono metriche delle prestazioni del mondo reale e punteggi di benchmark. I ricercatori e gli sviluppatori devono dare priorità a miglioramenti olistici dei sistemi AI che ne migliorino la generalizzabilità e la funzionalità in varie applicazioni anziché concentrarsi esclusivamente su ottimizzazioni specifiche del benchmark.\n\n\nProblemi\nUno dei problemi principali dell’ingegneria del benchmark è che può compromettere le prestazioni reali dei sistemi di intelligenza artificiale. Quando gli sviluppatori si concentrano sull’ottimizzazione dei loro sistemi per ottenere punteggi elevati in specifici test di benchmark, potrebbero trascurare altri importanti aspetti delle prestazioni del sistema, cruciali nelle applicazioni del mondo reale. Ad esempio, un sistema di intelligenza artificiale progettato per il riconoscimento delle immagini potrebbe essere progettato per funzionare eccezionalmente bene in un test di benchmark che include un set specifico di immagini, ma necessita di aiuto per riconoscere accuratamente immagini leggermente diverse da quelle nel set di test.\nUn’altra area di miglioramento con l’ingegneria di benchmark è che può comportare sistemi di intelligenza artificiale privi di generalizzabilità. In altre parole, mentre il sistema può funzionare bene nel test di benchmark, potrebbe aver bisogno di aiuto per gestire una vasta gamma di input o scenari. Ad esempio, un modello di intelligenza artificiale sviluppato per l’elaborazione del linguaggio naturale potrebbe essere progettato per ottenere punteggi elevati in un test di benchmark che include un tipo specifico di testo, ma non riesce a elaborare accuratamente il testo che non rientra in quel tipo specifico.\nPuò anche portare a risultati fuorvianti. Quando i sistemi di intelligenza artificiale sono progettati per funzionare bene nei test di benchmark, i risultati potrebbero non riflettere accuratamente le reali capacità del sistema. Questo può essere problematico per gli utenti o gli investitori che si affidano ai punteggi di benchmark per prendere decisioni informate su quali sistemi di intelligenza artificiale utilizzare o in cui investire. Ad esempio, un sistema di intelligenza artificiale progettato per ottenere punteggi elevati in un test di benchmark per il riconoscimento vocale potrebbe dover essere più in grado di riconoscere accuratamente il parlato in situazioni reali, portando gli utenti o gli investitori a prendere decisioni basate su informazioni imprecise.\n\n\nAttenuazione\nEsistono diversi modi per mitigare l’ingegneria dei benchmark. La trasparenza nel processo di benchmarking è fondamentale per mantenere l’accuratezza e l’affidabilità dei benchmark. Ciò implica la divulgazione chiara delle metodologie, dei set di dati e dei criteri di valutazione utilizzati nei test di benchmark, nonché di eventuali ottimizzazioni o modifiche apportate al sistema di intelligenza artificiale ai fini del benchmark.\nUn modo per ottenere trasparenza è attraverso l’uso di benchmark open source. I benchmark open source vengono resi disponibili al pubblico, consentendo a ricercatori, sviluppatori e altre parti interessate di esaminarli, criticarli e contribuire, garantendone così l’accuratezza e l’affidabilità. Questo approccio collaborativo facilita anche la condivisione delle “best practice” e lo sviluppo di benchmark più solidi e completi.\nIl design modulare di MLPerf Tiny si collega al problema dell’ingegneria dei benchmark fornendo un approccio strutturato ma flessibile che incoraggia una valutazione equilibrata di TinyML. Nell’ingegneria dei benchmark, i sistemi possono essere eccessivamente ottimizzati per benchmark specifici, portando a punteggi di prestazioni gonfiati che non si traducono necessariamente in efficacia nel mondo reale. Il design modulare di MLPerf Tiny mira ad affrontare questo problema consentendo ai collaboratori di scambiare e testare componenti specifici all’interno di un framework standardizzato, come hardware, tecniche di quantizzazione o modelli di inferenza. Le implementazioni di riferimento, evidenziate in verde e arancione in Figura 11.5, forniscono una base di riferimento per i risultati, consentendo test flessibili ma controllati specificando quali componenti possono essere modificati. Questa struttura supporta trasparenza e flessibilità, consentendo di concentrarsi su miglioramenti genuini piuttosto che su ottimizzazioni specifiche del benchmark.\n\n\n\n\n\n\nFigura 11.5: Design modulare del benchmark MLPerf Tiny, che mostra l’implementazione di riferimento con componenti modificabili. Questo approccio modulare consente test flessibili e mirati mantenendo una base di riferimento standardizzata. Fonte: Banbury et al. (2021).\n\n\nBanbury, Colby, Vijay Janapa Reddi, Peter Torelli, Jeremy Holleman, Nat Jeffries, Csaba Kiraly, Pietro Montino, et al. 2021. «MLPerf Tiny Benchmark». arXiv preprint arXiv:2106.07597, giugno. http://arxiv.org/abs/2106.07597v4.\n\n\nUn altro metodo per ottenere trasparenza è attraverso la revisione paritaria dei benchmark. Ciò comporta che esperti indipendenti esaminino e convalidino la metodologia, i set di dati e i risultati del benchmark per garantirne la credibilità e l’affidabilità. La revisione paritaria può fornire un mezzo prezioso per verificare l’accuratezza dei test di benchmark e contribuire a creare fiducia nei risultati.\nLa standardizzazione dei benchmark è un’altra importante soluzione per mitigare l’ingegneria dei benchmark. I benchmark standardizzati forniscono un quadro comune per la valutazione dei sistemi di intelligenza artificiale, garantendo coerenza e comparabilità tra diversi sistemi e applicazioni. Ciò può essere ottenuto sviluppando standard e “best practice” per l’intero settore per il benchmarking e tramite metriche e criteri di valutazione comuni.\nAnche la verifica da parte di terze parti dei risultati può essere preziosa per mitigare l’ingegneria dei benchmark. Ciò comporta che una terza parte indipendente verifichi i risultati di un test di benchmark per garantirne la credibilità e l’affidabilità. La verifica di terze parti può creare fiducia nei risultati e fornire un mezzo prezioso per convalidare le prestazioni e le capacità dei sistemi di intelligenza artificiale.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Benchmarking dell'IA</span>"
    ]
  },
  {
    "objectID": "contents/core/benchmarking/benchmarking.it.html#benchmarking-del-modello",
    "href": "contents/core/benchmarking/benchmarking.it.html#benchmarking-del-modello",
    "title": "11  Benchmarking dell’IA",
    "section": "11.5 Benchmarking del Modello",
    "text": "11.5 Benchmarking del Modello\nIl benchmarking dei modelli di machine learning è importante per determinare l’efficacia e l’efficienza di vari algoritmi di apprendimento automatico nella risoluzione di compiti o problemi specifici. Analizzando i risultati ottenuti dal benchmarking, sviluppatori e ricercatori possono identificare i punti di forza e di debolezza dei loro modelli, portando a decisioni più informate sulla selezione del modello e su un’ulteriore ottimizzazione.\nL’evoluzione e il progresso dei modelli di apprendimento automatico sono intrinsecamente collegati alla disponibilità e alla qualità dei set di dati. Nell’apprendimento automatico, i dati fungono da materia prima che alimenta gli algoritmi, consentendo loro di apprendere, adattarsi e, in definitiva, eseguire compiti che erano tradizionalmente di dominio degli esseri umani. Pertanto, è importante comprendere questa storia.\n\n11.5.1 Contesto Storico\nI dataset di apprendimento automatico hanno una storia ricca e si sono evoluti in modo significativo nel corso degli anni, crescendo in dimensioni, complessità e diversità per soddisfare le richieste sempre crescenti del settore. Diamo un’occhiata più da vicino a questa evoluzione, partendo da uno dei primi e più iconici set di dati: MNIST.\n\nMNIST (1998)\nIl dataset MNIST, creato da Yann LeCun, Corinna Cortes e Christopher J.C. Burges nel 1998, può essere considerato una pietra miliare nella storia dei dataset di machine learning. Comprende 70.000 immagini in scala di grigi da 28x28 pixel etichettate di cifre scritte a mano (0-9). MNIST è stato ampiamente utilizzato per il benchmarking degli algoritmi nell’elaborazione delle immagini e nell’apprendimento automatico come punto di partenza per molti ricercatori e professionisti. Figura 11.6 mostra alcuni esempi di cifre scritte a mano.\n\n\n\n\n\n\nFigura 11.6: Cifre scritte a mano in MNIST. Fonte: Suvanjanprasai\n\n\n\n\n\nImageNet (2009)\nFacciamo un salto al 2009 e vediamo l’introduzione di ImageNet, che ha segnato un balzo significativo nella scala e nella complessità dei dataset. ImageNet è composto da oltre 14 milioni di immagini etichettate che abbracciano più di 20.000 categorie. Fei-Fei Li e il suo team lo hanno sviluppato per far progredire il riconoscimento degli oggetti e la ricerca sulla visione artificiale. Il dataset è diventato sinonimo della ImageNet Large Scale Visual Recognition Challenge (LSVRC), una competizione annuale cruciale nello sviluppo di modelli di deep learning, tra cui il famoso AlexNet nel 2012.\n\n\nCOCO (2014)\nIl Common Objects in Context (COCO) dataset (Lin et al. 2014), rilasciato nel 2014, ha ulteriormente ampliato il panorama dei set di dati di apprendimento automatico introducendo un set più ricco di annotazioni. COCO è costituito da immagini contenenti scene complesse con più oggetti e ogni immagine è annotata con riquadri di delimitazione degli oggetti, maschere di segmentazione e didascalie, come mostrato in Figura 11.7. Questo set di dati è stato determinante nel far progredire la ricerca nel rilevamento degli oggetti, nella segmentazione e nella didascalia delle immagini.\n\nLin, Tsung-Yi, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, e C. Lawrence Zitnick. 2014. «Microsoft COCO: Common Objects in Context». In Computer Vision – ECCV 2014, 740–55. Springer; Springer International Publishing. https://doi.org/10.1007/978-3-319-10602-1\\_48.\n\n\n\n\n\n\nFigura 11.7: Immagini di esempio dal set di dati COCO. Fonte: Coco\n\n\n\n\n\nGPT-3 (2020)\nSebbene gli esempi sopra riportati si concentrino principalmente sui dataset di immagini, si sono verificati anche sviluppi significativi nei dataset di testo. Un esempio degno di nota è GPT-3 (Brown et al. 2020), sviluppato da OpenAI. GPT-3 è un modello linguistico addestrato su testo Internet eterogeneo. Sebbene il dataset utilizzato per addestrare GPT-3 non sia disponibile al pubblico, il modello stesso, costituito da 175 miliardi di parametri, è una testimonianza della scala e della complessità dei moderni dataset e modelli di apprendimento automatico.\n\nBrown, Tom B., Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, et al. 2020. «Language Models are Few-Shot Learners». In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, a cura di Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, e Hsuan-Tien Lin. https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html.\n\n\nPresente e Futuro\nOggi disponiamo di una pletora di dataset che abbracciano vari domini, tra cui sanità, finanza, scienze sociali e altro ancora. Le seguenti caratteristiche ci aiutano a classificare lo spazio e la crescita dei dataset di apprendimento automatico che alimentano lo sviluppo del modello.\n\nDiversità dei Set di Dati: La varietà di set di dati disponibili per ricercatori e ingegneri si è ampliata notevolmente, coprendo molti campi, tra cui l’elaborazione del linguaggio naturale, il riconoscimento delle immagini e altro ancora. Questa diversità ha alimentato lo sviluppo di modelli di apprendimento automatico specializzati, su misura per attività specifiche, come la traduzione, il riconoscimento vocale e il riconoscimento facciale.\nVolume di Dati: L’enorme volume di dati che è diventato disponibile nell’era digitale ha anche svolto un ruolo cruciale nel progresso dei modelli di apprendimento automatico. I grandi set di dati consentono ai modelli di catturare la complessità e le sfumature dei fenomeni del mondo reale, portando a previsioni più accurate e affidabili.\nQualità e Pulizia dei Dati: La qualità dei dati è un altro fattore critico che influenza le prestazioni dei modelli di apprendimento automatico. Set di dati puliti, ben etichettati e imparziali sono essenziali per modelli di addestramento solidi ed equi.\nAccesso Aperto ai Dati: La disponibilità di set di dati “open-access” ha contribuito in modo significativo anche al progresso dell’apprendimento automatico. I dati “aperti” consentono ai ricercatori di tutto il mondo di collaborare, condividere approfondimenti e basarsi sul lavoro degli altri, portando a un’innovazione più rapida e allo sviluppo di modelli più avanzati.\nProblemi di Etica e Privacy: Man mano che i set di dati crescono in dimensioni e complessità, le considerazioni etiche e i problemi di privacy diventano sempre più importanti. È in corso un dibattito sull’equilibrio tra lo sfruttamento dei dati per i progressi dell’apprendimento automatico e la protezione dei diritti alla privacy degli individui.\n\nLo sviluppo di modelli di apprendimento automatico si basa in larga misura sulla disponibilità di set di dati diversificati, grandi, di alta qualità e ad accesso libero. Mentre andiamo avanti, affrontare le considerazioni etiche e le preoccupazioni sulla privacy associate all’uso di grandi set di dati è fondamentale per garantire che le tecnologie di apprendimento automatico siano vantaggiose per la società. C’è una crescente consapevolezza che i dati agiscono come carburante per l’apprendimento automatico, guidando e alimentando lo sviluppo di modelli di apprendimento automatico. Di conseguenza, si sta ponendo maggiore attenzione sullo sviluppo dei set di dati stessi. Esploreremo questo aspetto in modo più dettagliato nella sezione del benchmarking dei dati.\n\n\n\n11.5.2 Metriche del Modello\nLa valutazione del modello di machine learning si è evoluta da un focus ristretto sulla precisione a un approccio più completo che considera una serie di fattori, da considerazioni etiche e applicabilità nel mondo reale a vincoli pratici come dimensioni ed efficienza del modello. Questo cambiamento riflette la maturazione del campo poiché i modelli di apprendimento automatico vengono sempre più applicati in scenari reali diversi e complessi.\n\nPrecisione\nLa precisione è una delle metriche più intuitive e comunemente utilizzate per valutare i modelli di apprendimento automatico. Nelle prime fasi dell’apprendimento automatico, la precisione era spesso la metrica principale, se non l’unica, considerata quando si valutavano le prestazioni del modello. Tuttavia, con l’evoluzione del campo, è diventato chiaro che fare affidamento esclusivamente sull’accuratezza può essere fuorviante, soprattutto in applicazioni in cui determinati tipi di errori comportano conseguenze significative.\nSi consideri l’esempio di un modello di diagnosi medica con una precisione del 95%. Sebbene a prima vista possa sembrare impressionante, dobbiamo guardare più a fondo per valutare appieno le prestazioni del modello. Supponiamo che il modello non riesca a diagnosticare accuratamente condizioni gravi che, sebbene rare, possono avere gravi conseguenze; la sua elevata precisione potrebbe non essere così significativa. Un esempio ben noto di questa limitazione è il modello di retinopatia diabetica di Google. Sebbene abbia raggiunto un’elevata accuratezza in laboratorio, ha incontrato delle difficoltà quando è stato implementato in cliniche reali in Thailandia, dove le variazioni nelle popolazioni di pazienti, la qualità delle immagini e i fattori ambientali ne hanno ridotto l’efficacia. Questo esempio illustra che anche i modelli con elevata accuratezza devono essere testati per la loro capacità di generalizzare in condizioni diverse e imprevedibili per garantire affidabilità e impatto in contesti reali.\nAllo stesso modo, se il modello funziona bene in media ma mostra significative disparità nelle prestazioni tra diversi gruppi demografici, anche questo sarebbe motivo di preoccupazione. L’evoluzione dell’apprendimento automatico ha quindi visto uno spostamento verso un approccio più olistico alla valutazione del modello, tenendo conto non solo dell’accuratezza, ma anche di altri fattori cruciali come la correttezza, trasparenza e applicabilità nel mondo reale. Un esempio lampante è il progetto Gender Shades del MIT Media Lab, guidato da Joy Buolamwini, che evidenzia i pregiudizi ottenendo risultati migliori sui volti maschili e dalla pelle più chiara rispetto ai volti femminili e dalla pelle più scura.\nSebbene l’accuratezza resti essenziale per valutare i modelli di apprendimento automatico, è necessario un approccio completo per valutare appieno le prestazioni. Ciò include metriche aggiuntive per l’equità, la trasparenza e l’applicabilità nel mondo reale, insieme a test rigorosi su diversi set di dati per identificare e affrontare i pregiudizi. Questo approccio di valutazione olistico riflette la crescente consapevolezza del settore delle implicazioni nel mondo reale nell’implementazione dei modelli.\n\n\nEquità\nIl “fairness” equità nell’apprendimento automatico implica la garanzia che i modelli funzionino in modo coerente su diversi gruppi, in particolare in applicazioni ad alto impatto come approvazioni di prestiti, assunzioni e giustizia penale. Affidarsi esclusivamente all’accuratezza può essere fuorviante se il modello mostra risultati distorti su gruppi demografici. Ad esempio, un modello di approvazione dei prestiti con elevata accuratezza potrebbe comunque negare sistematicamente i prestiti a determinati gruppi, sollevando dubbi sulla sua equità.\nIl “bias” [distorsione] nei modelli può sorgere direttamente, quando attributi sensibili come razza o genere influenzano le decisioni, o indirettamente, quando caratteristiche neutre sono correlate a questi attributi, influenzando i risultati. Affidarsi semplicemente all’accuratezza può essere insufficiente quando si valutano i modelli. Ad esempio, si consideri un modello di approvazione dei prestiti con un tasso di accuratezza del 95%. Sebbene questa cifra possa sembrare impressionante a prima vista, non rivela come il modello si comporta nei diversi gruppi demografici. Un esempio ben noto è lo strumento COMPAS utilizzato nel sistema di giustizia penale degli Stati Uniti, che ha mostrato pregiudizi razziali nel prevedere la recidiva nonostante non utilizzasse esplicitamente la razza come variabile.\nPer affrontare l’equità è necessario analizzare le prestazioni di un modello tra i gruppi, identificare i pregiudizi e applicare misure correttive come il ribilanciamento dei set di dati o l’utilizzo di algoritmi consapevoli dell’equità. Ricercatori e professionisti sviluppano continuamente metriche e metodologie su misura per casi d’uso specifici per valutare la correttezza e l’equità in scenari del mondo reale. Ad esempio, l’analisi di impatto disparato, la parità demografica e le pari opportunità sono alcune delle metriche impiegate per valutare l’equità/correttezza. Inoltre, la trasparenza e l’interpretabilità dei modelli sono fondamentali per raggiungere la correttezza. Strumenti come AI Fairness 360 e Fairness Indicators aiutano a spiegare come un modello prende decisioni, consentendo agli sviluppatori di rilevare e correggere problemi di equità nei modelli di apprendimento automatico.\nSebbene l’accuratezza sia una metrica preziosa, non sempre fornisce il quadro completo; la valutazione dell’equità garantisce che i modelli siano efficaci in scenari del mondo reale. Garantire l’equità/correttezza nei modelli di apprendimento automatico, in particolare nelle applicazioni che hanno un impatto significativo sulla vita delle persone, richiede una rigorosa valutazione delle prestazioni del modello in gruppi diversi, un’attenta identificazione e attenuazione dei pregiudizi e l’implementazione di misure di trasparenza e interpretabilità.\n\n\nComplessità\n\nParametri\nNelle fasi iniziali del machine learning, il benchmarking dei modelli si basava spesso sui conteggi dei parametri come proxy [sostituto] per la complessità del modello. La logica era che più parametri in genere portano a un modello più complesso, che dovrebbe, a sua volta, fornire prestazioni migliori. Tuttavia, questo approccio trascura i costi pratici associati all’elaborazione di modelli di grandi dimensioni. Man mano che aumenta il numero dei parametri, aumentano anche le risorse computazionali richieste, rendendo tali modelli poco pratici per l’implementazione in scenari reali, in particolare su dispositivi con potenza di elaborazione limitata.\nAffidarsi ai conteggi dei parametri come proxy per la complessità del modello non riesce a considerare anche l’efficienza del modello. Un modello ben ottimizzato con meno parametri può spesso ottenere prestazioni paragonabili o addirittura superiori a un modello più grande. Ad esempio, MobileNets, sviluppato da Google, è una famiglia di modelli progettati specificamente per dispositivi mobili ed edge. Hanno utilizzato convoluzioni separabili in profondità per ridurre il numero dei parametri e le richieste computazionali mantenendo comunque prestazioni elevate.\nAlla luce di queste limitazioni, il settore si è spostato verso un approccio più olistico al benchmarking dei modelli che considera i conteggi dei parametri e altri fattori cruciali come le operazioni in virgola mobile al secondo (FLOP), il consumo di memoria e la latenza. Questo approccio completo bilancia le prestazioni con la distribuibilità, assicurando che i modelli non siano solo accurati, ma anche efficienti e adatti alle applicazioni del mondo reale.\n\n\nFLOP\nI “floating-point operation” (FLOP), o operazioni in virgola mobile al secondo, sono diventati una metrica critica per rappresentare il carico computazionale di un modello. Tradizionalmente, il numero dei parametri veniva utilizzato come indicatore per la complessità del modello, basandosi sul presupposto che più parametri avrebbero prodotto prestazioni migliori. Tuttavia, questo approccio trascura il costo computazionale dell’elaborazione di questi parametri, che può influire sull’usabilità di un modello in scenari del mondo reale con risorse limitate.\nI FLOP misurano il numero di operazioni in virgola mobile eseguite da un modello per generare una previsione. Un modello con molti FLOP richiede risorse computazionali sostanziali per elaborare il vasto numero di operazioni, il che potrebbe renderlo poco pratico per alcune applicazioni. Al contrario, un modello con un numero di FLOP inferiore è più leggero e può essere facilmente distribuito in scenari in cui le risorse computazionali sono limitate. Figura 11.8, da (Bianco et al. 2018), illustra il compromesso tra accuratezza di ImageNet, FLOP e numero dei parametri, dimostrando che alcune architetture raggiungono un’efficienza maggiore di altre.\n\n\n\n\n\n\nFigura 11.8: Un grafico che raffigura la top-1 Imagenet Accuracy rispetto al numero di FLOP di un modello insieme al conteggio dei parametri del modello. La figura mostra un compromesso complessivo tra complessità e accuratezza del modello, sebbene alcune architetture del modello siano più efficienti di altre. Fonte: Bianco et al. (2018).\n\n\nBianco, Simone, Remi Cadene, Luigi Celona, e Paolo Napoletano. 2018. «Benchmark Analysis of Representative Deep Neural Network Architectures». IEEE Access 6: 64270–77. https://doi.org/10.1109/access.2018.2877890.\n\n\nConsideriamo un esempio. BERT—Bidirectional Encoder Representations from Transformers (Devlin et al. 2019)—è un modello di elaborazione del linguaggio naturale molto diffuso, con oltre 340 milioni di parametri, il che lo rende un modello di grandi dimensioni con elevata accuratezza e prestazioni impressionanti in diverse attività. Tuttavia, le dimensioni di BERT, unite al suo elevato numero di FLOP, lo rendono un modello computazionalmente intensivo che potrebbe non essere adatto per applicazioni in tempo reale o per l’implementazione su dispositivi edge con capacità computazionali limitate. Alla luce di ciò, c’è stato un crescente interesse nello sviluppo di modelli più piccoli in grado di raggiungere livelli di prestazioni simili alle loro controparti più grandi, pur essendo più efficienti nel carico computazionale. DistilBERT, ad esempio, è una versione più piccola di BERT che mantiene il 97% delle sue prestazioni, pur essendo il 40% più piccola in termini di numero di parametri. La riduzione delle dimensioni si traduce anche in un numero di FLOP inferiore, rendendo DistilBERT una scelta più pratica per scenari con risorse limitate.\n\nDevlin, Jacob, Ming-Wei Chang, Kenton Lee, e Kristina Toutanova. 2019. «None». In Proceedings of the 2019 Conference of the North, 4171–86. Minneapolis, Minnesota: Association for Computational Linguistics. https://doi.org/10.18653/v1/n19-1423.\nSebbene il numero dei parametri indichi la dimensione del modello, non cattura completamente il costo computazionale. I FLOP forniscono una misura più accurata del carico computazionale, evidenziando i compromessi pratici nell’implementazione del modello. Questo passaggio dal conteggio dei parametri ai FLOP riflette la crescente consapevolezza del settore delle sfide di implementazione in contesti diversi.\n\n\nEfficienza\nAnche le metriche di efficienza, come il consumo di memoria e la latenza/capacità di elaborazione, hanno acquisito importanza. Queste metriche sono particolarmente cruciali quando si distribuiscono modelli su dispositivi edge o in applicazioni in tempo reale, poiché misurano la velocità con cui un modello può elaborare i dati e la quantità di memoria richiesta. In questo contesto, le curve di Pareto vengono spesso utilizzate per visualizzare il compromesso tra diverse metriche, aiutando le parti interessate a decidere quale modello si adatta meglio alle loro esigenze.\n\n\n\n\n11.5.3 Lezioni Apprese\nIl benchmarking dei modelli ci ha offerto diverse preziose intuizioni che possono essere sfruttate per guidare l’innovazione nei benchmark di sistema. La progressione dei modelli di apprendimento automatico è stata profondamente influenzata dall’avvento delle classifiche e dalla disponibilità open source di modelli e set di dati. Questi elementi hanno svolto il ruolo di catalizzatori significativi, spingendo l’innovazione e accelerando l’integrazione di modelli all’avanguardia negli ambienti di produzione. Tuttavia, come approfondiremo ulteriormente, questi non sono gli unici fattori che contribuiscono allo sviluppo dei benchmark di apprendimento automatico.\nLe classifiche svolgono un ruolo fondamentale nel fornire un metodo oggettivo e trasparente per ricercatori e professionisti per valutare l’efficacia di diversi modelli, classificandoli in base alle loro prestazioni nei benchmark. Questo sistema promuove un ambiente competitivo, incoraggiando lo sviluppo di modelli che non siano solo accurati ma anche efficienti. L’ImageNet Large Scale Visual Recognition Challenge (ILSVRC) ne è un ottimo esempio, con la sua classifica annuale che contribuisce in modo significativo allo sviluppo di modelli innovativi come AlexNet.\nL’accesso open source a modelli e set di dati all’avanguardia diffonde ulteriormente l’apprendimento automatico, facilitando la collaborazione tra ricercatori e professionisti in tutto il mondo. Questo accesso aperto accelera il processo di test, convalida e distribuzione di nuovi modelli in ambienti di produzione, come dimostrato dall’adozione diffusa di modelli come BERT e GPT-3 in varie applicazioni, dall’elaborazione del linguaggio naturale a compiti multimodali più complessi.\nPiattaforme di collaborazione della comunità come Kaggle hanno rivoluzionato il settore ospitando competizioni che uniscono data scientist da tutto il mondo per risolvere problemi intricati. Benchmark specifici fungono da paletti per l’innovazione e lo sviluppo di modelli.\nInoltre, la disponibilità di set di dati diversi e di alta qualità è fondamentale per l’addestramento e il test dei modelli di apprendimento automatico. Set di dati come ImageNet hanno svolto un ruolo fondamentale nell’evoluzione dei modelli di riconoscimento delle immagini, mentre ampi set di dati di testo hanno facilitato i progressi nei modelli di elaborazione del linguaggio naturale.\nInfine, è necessario supportare i contributi di istituti accademici e di ricerca. Il loro ruolo nella pubblicazione di articoli di ricerca, nella condivisione di risultati in conferenze e nella promozione della collaborazione tra varie istituzioni ha contribuito in modo significativo al progresso dei modelli e dei benchmark di apprendimento automatico.\n\nTendenze Emergenti\nMan mano che i modelli di apprendimento automatico diventano più sofisticati, lo diventano anche i benchmark necessari per valutarli in modo accurato. Ci sono diversi benchmark e dataset emergenti che stanno guadagnando popolarità grazie alla loro capacità di valutare i modelli in scenari più complessi e realistici:\nDataset Multimodali: Questi set di dati contengono più tipi di dati, come testo, immagini e audio, per rappresentare meglio le situazioni del mondo reale. Un esempio è VQA (Visual Question Answering) (Antol et al. 2015), in cui viene testata la capacità dei modelli di rispondere a domande basate su testo sulle immagini.\n\nAntol, Stanislaw, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C. Lawrence Zitnick, e Devi Parikh. 2015. «VQA: Visual Question Answering». In 2015 IEEE International Conference on Computer Vision (ICCV), 2425–33. IEEE. https://doi.org/10.1109/iccv.2015.279.\nValutazione di Correttezza e Bias: C’è una crescente attenzione alla creazione di benchmark che valutino l’equità/Correttezza e i bias [pregiudizi] dei modelli di apprendimento automatico. Esempi includono il toolkit AI Fairness 360, che offre un set completo di metriche e set di dati per valutare il bias nei modelli.\nGeneralizzazione Out-of-Distribution: Test di quanto bene i modelli funzionano su dati diversi dalla distribuzione di training originale. Questo valuta la capacità del modello di generalizzare a dati nuovi e inediti. Esempi di benchmark sono Wilds (Koh et al. 2021), RxRx e ANC-Bench.\n\nKoh, Pang Wei, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani, Weihua Hu, et al. 2021. «WILDS: A Benchmark of in-the-Wild Distribution Shifts.» In Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, a cura di Marina Meila e Tong Zhang, 139:5637–64. Proceedings of Machine Learning Research. PMLR. http://proceedings.mlr.press/v139/koh21a.html.\n\nHendrycks, Dan, Kevin Zhao, Steven Basart, Jacob Steinhardt, e Dawn Song. 2021. «Natural Adversarial Examples». In 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 15257–66. IEEE. https://doi.org/10.1109/cvpr46437.2021.01501.\n\nXie, Cihang, Mingxing Tan, Boqing Gong, Jiang Wang, Alan L. Yuille, e Quoc V. Le. 2020. «Adversarial Examples Improve Image Recognition». In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 816–25. IEEE. https://doi.org/10.1109/cvpr42600.2020.00090.\nRobustezza Avversaria: Valutazione delle prestazioni del modello in caso di attacchi avversari o perturbazioni ai dati di input. Questo testa la robustezza del modello. Esempi di benchmark sono ImageNet-A (Hendrycks et al. 2021), ImageNet-C (Xie et al. 2020) e CIFAR-10.1.\nPrestazioni nel Mondo Reale: Test di modelli su set di dati del mondo reale che corrispondono da vicino alle attività finali anziché solo su set di dati di benchmark predefiniti. Esempi sono set di dati di imaging medico per attività sanitarie o log di chat di assistenza clienti per sistemi di dialogo.\nEfficienza Energetica e di Calcolo: Benchmark che misurano le risorse di calcolo necessarie per ottenere una particolare accuratezza. Questo valuta l’efficienza del modello. Esempi sono MLPerf e Greenbench, già discussi nella sezione Benchmarking dei sistemi.\nInterpretabilità e Spiegabilità: Benchmark che valutano quanto sia facile comprendere e spiegare la logica interna e le previsioni di un modello. Esempi di parametri sono la fedeltà ai gradienti di input e la coerenza delle spiegazioni.\n\n\n\n11.5.4 Limitazioni e Sfide\nSebbene i benchmark dei modelli siano uno strumento essenziale per valutare i modelli di machine learning, è necessario affrontare diverse limitazioni e sfide per garantire che riflettano accuratamente le prestazioni in scenari reali.\nIl dataset non corrisponde a scenari reali: Spesso, i dati utilizzati nei benchmark dei modelli vengono puliti e preelaborati a tal punto che potrebbe essere necessario rappresentare accuratamente i dati che un modello incontrerebbe in applicazioni reali. Questa versione idealizzata dei dati può portare a una sovrastima delle prestazioni di un modello. Nel caso del set di dati ImageNet, le immagini sono ben etichettate e categorizzate. Tuttavia, in uno scenario reale, un modello potrebbe dover gestire immagini sfocate che potrebbero essere meglio illuminate o scattate da angolazioni scomode. Questa discrepanza può influire in modo significativo sulle prestazioni del modello.\nSim2Real Gap: Il Sim2Real Gap si riferisce alla differenza nelle prestazioni di un modello quando si passa da un ambiente simulato a un ambiente reale. Questo gap è spesso osservato nella robotica, dove un robot addestrato in un ambiente simulato ha difficoltà a svolgere compiti nel mondo reale a causa della complessità e dell’imprevedibilità degli ambienti reali. Un robot addestrato a raccogliere oggetti in un ambiente simulato potrebbe aver bisogno di aiuto per svolgere lo stesso compito nel mondo reale perché l’ambiente simulato non rappresenta accuratamente le complessità della fisica, dell’illuminazione e della variabilità degli oggetti del mondo reale.\nSfide nella Creazione di Dataset: La creazione di un set di dati per il benchmarking del modello è un’attività impegnativa che richiede un’attenta considerazione di vari fattori come qualità dei dati, diversità e rappresentazione. Come discusso nella sezione di ingegneria dei dati, garantire che i dati siano puliti, imparziali e rappresentativi dello scenario del mondo reale è fondamentale per l’accuratezza e l’affidabilità del benchmark. Ad esempio, quando si crea un set di dati per un’attività correlata all’assistenza sanitaria, è importante assicurarsi che i dati siano rappresentativi dell’intera popolazione e non distorti verso un particolare gruppo demografico. Ciò garantisce che il modello funzioni bene in diverse popolazioni di pazienti.\nI benchmark del modello sono essenziali per misurare la capacità di un’architettura di modello di risolvere un’attività fissa, ma è importante affrontare le limitazioni e le sfide ad essi associate. Ciò include il garantire che il set di dati rappresenti accuratamente scenari del mondo reale, affrontare il divario Sim2Real e superare le sfide della creazione di set di dati imparziali e rappresentativi. Affrontando queste sfide e molte altre, possiamo garantire che i benchmark del modello forniscano una valutazione più accurata e affidabile delle prestazioni di un modello in applicazioni del mondo reale.\nLo Speech Commands dataset e il suo successore MSWC sono benchmark comuni per una delle applicazioni TinyML per eccellenza, l’individuazione delle parole chiave. I comandi vocali stabiliscono metriche di errore di streaming oltre la precisione di classificazione standard top-1 più pertinenti al caso d’uso di individuazione delle parole chiave. L’utilizzo di metriche pertinenti ai casi è ciò che eleva un dataset a un benchmark del modello.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Benchmarking dell'IA</span>"
    ]
  },
  {
    "objectID": "contents/core/benchmarking/benchmarking.it.html#benchmarking-dei-dati",
    "href": "contents/core/benchmarking/benchmarking.it.html#benchmarking-dei-dati",
    "title": "11  Benchmarking dell’IA",
    "section": "11.6 Benchmarking dei Dati",
    "text": "11.6 Benchmarking dei Dati\nNegli ultimi anni, l’intelligenza artificiale si è concentrata sullo sviluppo di modelli di apprendimento automatico sempre più sofisticati, come i grandi modelli linguistici. L’obiettivo è stato quello di creare modelli in grado di prestazioni di livello umano o sovrumane su un’ampia gamma di attività, addestrandoli su enormi set di dati. Questo approccio incentrato sul modello ha prodotto rapidi progressi, con modelli che hanno ottenuto risultati all’avanguardia su molti benchmark consolidati. Figura 11.9 mostra le prestazioni dei sistemi di intelligenza artificiale rispetto alle prestazioni umane (contrassegnate dalla linea orizzontale a 0) in cinque applicazioni: riconoscimento della scrittura a mano, riconoscimento vocale, riconoscimento delle immagini, comprensione della lettura e comprensione del linguaggio. Negli ultimi dieci anni, le prestazioni dell’intelligenza artificiale hanno superato quelle degli esseri umani.\n\n\n\n\n\n\nFigura 11.9: IA e prestazioni umane. Fonte: Kiela et al. (2021).\n\n\n\nTuttavia, le crescenti preoccupazioni su questioni come pregiudizi, sicurezza e robustezza persistono anche nei modelli che raggiungono un’elevata accuratezza sui benchmark standard. Inoltre, alcuni set di dati popolari utilizzati per la valutazione dei modelli stanno iniziando a saturarsi, con modelli che raggiungono prestazioni quasi perfette su divisioni di test esistenti (Kiela et al. 2021). Come semplice esempio, ci sono immagini di test nel classico dataset di cifre scritte a mano MNIST che potrebbero sembrare indecifrabili per la maggior parte dei valutatori umani, ma a cui è stata assegnata un’etichetta quando è stato creato il set di dati: i modelli che concordano con quelle etichette potrebbero sembrare esibire prestazioni sovrumane, ma potrebbero invece catturare solo idiosincrasie del processo di etichettatura e acquisizione dalla creazione del set di dati nel 1994. Con lo stesso spirito, i ricercatori di visione artificiale ora chiedono: “Abbiamo finito con ImageNet?” (Beyer et al. 2020). Ciò evidenzia i limiti nell’approccio convenzionale incentrato sul modello di ottimizzazione dell’accuratezza su set di dati fissi tramite innovazioni architettoniche.\n\nKiela, Douwe, Max Bartolo, Yixin Nie, Divyansh Kaushik, Atticus Geiger, Zhengxuan Wu, Bertie Vidgen, et al. 2021. «Dynabench: Rethinking Benchmarking in NLP». In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 4110–24. Online: Association for Computational Linguistics. https://doi.org/10.18653/v1/2021.naacl-main.324.\n\nBeyer, Lucas, Olivier J. Hénaff, Alexander Kolesnikov, Xiaohua Zhai, e Aäron van den Oord. 2020. «Are we done with ImageNet?» ArXiv preprint abs/2006.07159 (giugno). http://arxiv.org/abs/2006.07159v1.\nSta emergendo un paradigma alternativo chiamato IA incentrata sui dati. Invece di trattare i dati come statici e concentrarsi strettamente sulle prestazioni del modello, questo approccio riconosce che i modelli sono validi solo quanto i loro dati di training. Quindi, l’enfasi si sposta sulla cura di dataset di alta qualità che riflettano meglio la complessità del mondo reale, sviluppando benchmark di valutazione più informativi e considerando attentamente come i dati vengono campionati, preelaborati e aumentati. L’obiettivo è ottimizzare il comportamento del modello migliorando i dati anziché semplicemente ottimizzando le metriche su set di dati imperfetti. L’intelligenza artificiale incentrata sui dati esamina e migliora criticamente i dati stessi per produrre un’intelligenza artificiale utile. Ciò riflette un’importante evoluzione nella mentalità, poiché il campo affronta le carenze di un benchmarking ristretto.\nQuesta sezione esplorerà le principali differenze tra gli approcci all’intelligenza artificiale incentrati sui modelli e sui dati. Questa distinzione ha importanti implicazioni sul modo in cui eseguiamo il benchmarking dei sistemi di intelligenza artificiale. In particolare, vedremo come concentrarsi sulla qualità dei dati e sull’efficienza può migliorare direttamente le prestazioni dell’apprendimento automatico come alternativa all’ottimizzazione delle sole architetture dei modelli. L’approccio incentrato sui dati riconosce che i modelli sono validi solo quanto i loro dati di addestramento. Quindi, migliorare la cura dei dati, i benchmark di valutazione e i processi di gestione dei dati può produrre sistemi di intelligenza artificiale più sicuri, più equi e più robusti. Ripensare il benchmarking per dare priorità ai dati insieme ai modelli rappresenta un’importante evoluzione, poiché il settore si sforza di fornire un impatto affidabile nel mondo reale.\n\n11.6.1 Limitazioni dell’IA Incentrata sul Modello\nNell’era dell’IA incentrata sul modello, una caratteristica importante era lo sviluppo di architetture di modelli complesse. Ricercatori e professionisti hanno dedicato notevoli sforzi alla progettazione di modelli sofisticati e intricati nella ricerca di prestazioni superiori. Ciò ha spesso comportato l’incorporazione di livelli aggiuntivi e la messa a punto di una moltitudine di iperparametri per ottenere miglioramenti nell’accuratezza. Contemporaneamente, c’era una notevole enfasi sullo sfruttamento di algoritmi avanzati. Questi algoritmi, spesso in prima linea nelle ultime ricerche, sono stati impiegati per migliorare le prestazioni dei modelli di IA. L’obiettivo principale di questi algoritmi era ottimizzare il processo di apprendimento dei modelli, estraendo così il massimo delle informazioni dai dati di addestramento.\nSebbene l’approccio incentrato sul modello sia stato centrale per molti progressi nell’IA, ha diverse aree di miglioramento. Innanzitutto, lo sviluppo di architetture di modelli complesse può spesso portare a un overfitting. Questo è quando il modello funziona bene sui dati di addestramento ma deve generalizzare a nuovi dati mai visti. I layer aggiuntivi e la complessità possono catturare il rumore nei dati di training come se fosse un pattern reale, danneggiando le prestazioni del modello su nuovi dati.\nIn secondo luogo, affidarsi ad algoritmi avanzati può a volte oscurare la reale comprensione del funzionamento di un modello. Questi algoritmi spesso agiscono come una scatola nera, rendendo difficile interpretare il modo in cui il modello prende decisioni. Questa mancanza di trasparenza può essere un ostacolo significativo, specialmente in applicazioni critiche come sanità e finanza, dove la comprensione del processo decisionale del modello è fondamentale.\nIn terzo luogo, l’enfasi sul raggiungimento di risultati all’avanguardia su set di dati di riferimento può a volte essere fuorviante. Questi dataset devono rappresentare in modo più completo le complessità e la variabilità dei dati del mondo reale. Un modello che funziona bene su un set di dati di riferimento potrebbe non essere necessariamente generalizzato bene a dati nuovi e mai visti in un’applicazione del mondo reale. Questa discrepanza può portare a una falsa fiducia nelle capacità del modello e ostacolarne l’applicabilità pratica.\nInfine, l’approccio incentrato sul modello spesso si basa su grandi set di dati etichettati per l’addestramento. Tuttavia, ottenere tali set di dati richiede tempo e impegno in molti scenari del mondo reale. Questa dipendenza da grandi dataset limita anche l’applicabilità dell’IA in domini in cui i dati sono scarsi o costosi da etichettare.\nCome risultato delle ragioni di cui sopra, e di molte altre, la comunità dell’IA sta passando a un approccio più incentrato sui dati. Invece di concentrarsi solo sull’architettura del modello, i ricercatori stanno ora dando priorità alla cura di set di dati di alta qualità, allo sviluppo di migliori benchmark di valutazione e alla considerazione di come i dati vengono campionati e preelaborati. L’idea chiave è che i modelli sono validi solo quanto i loro dati di training. Quindi, concentrandoci sull’ottenimento dei dati giusti, potremo sviluppare sistemi di intelligenza artificiale più equi, sicuri e allineati con i valori umani. Questo cambiamento incentrato sui dati rappresenta un importante cambiamento di mentalità man mano che l’intelligenza artificiale progredisce.\n\n\n11.6.2 Verso un’Intelligenza Artificiale Incentrata sui Dati\nL’intelligenza artificiale incentrata sui dati è un paradigma che sottolinea l’importanza di dataset di alta qualità, ben etichettati e diversificati nello sviluppo di modelli di intelligenza artificiale. Contrariamente all’approccio incentrato sul modello, che si concentra sulla rifinitura e l’iterazione dell’architettura e dell’algoritmo del modello per migliorare le prestazioni, l’intelligenza artificiale incentrata sui dati dà priorità alla qualità dei dati di input come motore principale per migliorare le prestazioni del modello. I dati di alta qualità sono puliti, ben etichettati e rappresentativi degli scenari del mondo reale che il modello incontrerà. Al contrario, i dati di bassa qualità possono portare a scarse prestazioni del modello, indipendentemente dalla complessità o dalla sofisticatezza dell’architettura del modello.\nL’intelligenza artificiale incentrata sui dati pone una forte enfasi sulla pulizia e l’etichettatura dei dati. La pulizia comporta la rimozione di valori anomali, la gestione dei valori mancanti e la risoluzione di altre incongruenze nei dati. L’etichettatura, d’altro canto, comporta l’assegnazione di etichette significative e accurate ai dati. Entrambi questi processi sono fondamentali per garantire che il modello di intelligenza artificiale venga addestrato su dati accurati e pertinenti. Un altro aspetto importante dell’approccio incentrato sui dati è il “data augmentation” [l’aumento dei dati]. Ciò comporta l’aumento artificiale delle dimensioni e della diversità del set di dati applicando varie trasformazioni ai dati, come rotazione, ridimensionamento e capovolgimento delle immagini di addestramento. L’aumento dei dati aiuta a migliorare la robustezza del modello e le capacità di generalizzazione.\nCi sono diversi vantaggi nell’adottare un approccio incentrato sui dati per lo sviluppo dell’intelligenza artificiale. Innanzitutto, porta a prestazioni del modello migliorate e capacità di generalizzazione. Assicurandosi che il modello venga addestrato su dati diversi e di alta qualità, il modello può generalizzare meglio a dati nuovi e mai visti (Mattson et al. 2020b).\nInoltre, un approccio incentrato sui dati può spesso portare a modelli più semplici che sono più facili da interpretare e gestire. Questo perché l’enfasi è sui dati piuttosto che sull’architettura del modello, il che significa che i modelli più semplici possono raggiungere prestazioni elevate quando addestrati su dati di alta qualità.\nIl passaggio all’IA incentrata sui dati rappresenta un significativo cambiamento di paradigma. Dando priorità alla qualità dei dati di input, questo approccio cerca di modellare le prestazioni e le capacità di generalizzazione, portando in ultima analisi a sistemi di intelligenza artificiale più solidi e affidabili. Figura 11.10 illustra questa differenza. Mentre continuiamo ad avanzare nella nostra comprensione e applicazione dell’IA, è probabile che l’approccio incentrato sui dati svolga un ruolo importante nel plasmare il futuro di questo campo.\n\n\n\n\n\n\nFigura 11.10: Sviluppo di machine learning incentrato sul modello e incentrato sui dati. Fonte: NVIDIA\n\n\n\n\n\n11.6.3 Benchmarking dei Dati\nIl benchmarking dei dati mira a valutare problemi comuni nei set di dati, come l’identificazione di errori di etichetta, caratteristiche rumorose, squilibrio di rappresentazione (ad esempio, su 1000 classi in Imagenet-1K, ci sono oltre 100 categorie che sono solo tipi di cani), squilibrio di classe (dove alcune classi hanno molti più campioni di altre), se i modelli addestrati su un dato set di dati possono generalizzare a caratteristiche fuori distribuzione o quali tipi di bias potrebbero esistere in un dato set di dati (Mattson et al. 2020b). Nella sua forma più semplice, il benchmarking dei dati mira a migliorare l’accuratezza su un set di test rimuovendo campioni di addestramento rumorosi o etichettati in modo errato mantenendo fissa l’architettura del modello. Recenti competizioni nel benchmarking dei dati hanno invitato i partecipanti a presentare nuove strategie di “augmentation” e tecniche di apprendimento attivo.\n\nMattson, Peter, Vijay Janapa Reddi, Christine Cheng, Cody Coleman, Greg Diamos, David Kanter, Paulius Micikevicius, et al. 2020b. «MLPerf: An Industry Standard Benchmark Suite for Machine Learning Performance». IEEE Micro 40 (2): 8–16. https://doi.org/10.1109/mm.2020.2974843.\nLe tecniche incentrate sui dati continuano a guadagnare attenzione nel benchmarking, soprattutto perché i modelli di base sono sempre più addestrati su obiettivi auto-supervisionati. Rispetto ai set di dati più piccoli come Imagenet-1K, i set di dati più grandi comunemente usati nell’apprendimento auto-supervisionato, come Common Crawl, OpenImages e LAION-5B, contengono quantità maggiori di rumore, duplicati, bias e dati potenzialmente offensivi.\nDataComp è una competizione di dataset lanciata di recente che ha come obiettivo la valutazione di grandi corpora. DataComp si concentra sulle coppie linguaggio-immagine usate per addestrare i modelli CLIP. Il documento introduttivo rileva che quando il budget di elaborazione totale per l’addestramento è costante, i modelli CLIP più performanti nelle attività downstream, come la classificazione ImageNet, vengono addestrati solo sul 30% del pool di campioni disponibile. Ciò suggerisce che un corretto filtraggio di grandi corpora è fondamentale per migliorare l’accuratezza dei modelli di base. Analogamente, Demystifying CLIP Data (Xu et al. 2023) chiede se il successo di CLIP sia attribuibile all’architettura o al set di dati.\n\nXu, Hu, Saining Xie, Xiaoqing Ellen Tan, Po-Yao Huang, Russell Howes, Vasu Sharma, Shang-Wen Li, Gargi Ghosh, Luke Zettlemoyer, e Christoph Feichtenhofer. 2023. «Demystifying CLIP Data». ArXiv preprint abs/2309.16671 (settembre). http://arxiv.org/abs/2309.16671v4.\nDataPerf è un altro recente lavoro incentrato sul benchmarking dei dati in varie modalità. DataPerf offre round di competizione online per stimolare il miglioramento dei dataset. L’offerta inaugurale è stata lanciata con sfide in termini di visione, parlato, acquisizione, debug e prompt di testo per la generazione di immagini.\n\n\n11.6.4 Efficienza dei Dati\nMan mano che i modelli di apprendimento automatico diventano più grandi e complessi e le risorse di elaborazione diventano più scarse di fronte alla crescente domanda, diventa difficile soddisfare i requisiti di elaborazione anche con le flotte di machine learning più grandi. Per superare queste sfide e garantire la scalabilità del sistema di apprendimento automatico, è necessario esplorare nuove opportunità che aumentino gli approcci convenzionali alla scalabilità delle risorse.\nMigliorare la qualità dei dati può essere un metodo utile per avere un impatto significativo sulle prestazioni del sistema di apprendimento automatico. Uno dei principali vantaggi del miglioramento della qualità dei dati è il potenziale di poter ridurre le dimensioni del set di dati di addestramento mantenendo o addirittura migliorando le prestazioni del modello. Questa riduzione delle dimensioni dei dati è direttamente correlata alla quantità di tempo di addestramento richiesto, consentendo così ai modelli di convergere in modo più rapido ed efficiente. Raggiungere questo equilibrio tra qualità dei dati e dimensioni del set di dati è un compito impegnativo che richiede lo sviluppo di metodi, algoritmi e tecniche sofisticati.\nPossono essere adottati diversi approcci per migliorare la qualità dei dati. Questi metodi includono e non sono limitati a quanto segue:\n\nPulizia dei Dati: Ciò comporta la gestione dei valori mancanti, la correzione degli errori e la rimozione dei valori anomali. I dati puliti assicurano che il modello non stia imparando da rumore o imprecisioni.\nInterpretabilità e Spiegabilità dei Dati: Le tecniche comuni includono LIME (Ribeiro, Singh, e Guestrin 2016), che fornisce informazioni sui limiti decisionali dei classificatori, e valori Shapley (Lundberg e Lee 2017), che stimano l’importanza dei singoli campioni nel contribuire alle previsioni di un modello.\nFeature Engineering: Trasformare o creare nuove funzionalità può migliorare significativamente le prestazioni del modello fornendo informazioni più pertinenti per l’apprendimento.\nData Augmentation: Aumentare i dati creando nuovi campioni tramite varie trasformazioni può aiutare a migliorare la robustezza e la generalizzazione del modello.\nActive Learning: Questo è un approccio di apprendimento semi-supervisionato in cui il modello interroga attivamente un “oracolo” umano per etichettare i campioni più informativi (Coleman et al. 2022). Ciò garantisce che il modello venga addestrato sui dati più rilevanti.\nRiduzione della Dimensionalità: Tecniche come PCA possono ridurre il numero di feature in un set di dati, riducendo così la complessità e il tempo di training.\n\n\nRibeiro, Marco Tulio, Sameer Singh, e Carlos Guestrin. 2016. «” Why should i trust you?” Explaining the predictions of any classifier». In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, 1135–44.\n\nLundberg, Scott M., e Su-In Lee. 2017. «A Unified Approach to Interpreting Model Predictions». In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, a cura di Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, e Roman Garnett, 4765–74. https://proceedings.neurips.cc/paper/2017/hash/8a20a8621978632d76c43dfd28b67767-Abstract.html.\n\nColeman, Cody, Edward Chou, Julian Katz-Samuels, Sean Culatana, Peter Bailis, Alexander C. Berg, Robert Nowak, Roshan Sumbaly, Matei Zaharia, e I. Zeki Yalniz. 2022. «Similarity Search for Efficient Active Learning and Search of Rare Concepts». Proceedings of the AAAI Conference on Artificial Intelligence 36 (6): 6402–10. https://doi.org/10.1609/aaai.v36i6.20591.\nEsistono molti altri metodi in circolazione. Ma l’obiettivo è lo stesso. Affinare il set di dati e garantire che sia della massima qualità può ridurre il tempo di addestramento necessario per la convergenza dei modelli. Tuttavia, per raggiungere questo obiettivo è necessario sviluppare e implementare metodi, algoritmi e tecniche sofisticati in grado di pulire, preelaborare e aumentare i dati, mantenendo al contempo i campioni più informativi. Questa è una sfida continua che richiederà una continua ricerca e innovazione nel campo dell’apprendimento automatico.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Benchmarking dell'IA</span>"
    ]
  },
  {
    "objectID": "contents/core/benchmarking/benchmarking.it.html#la-tripletta",
    "href": "contents/core/benchmarking/benchmarking.it.html#la-tripletta",
    "title": "11  Benchmarking dell’IA",
    "section": "11.7 La Tripletta",
    "text": "11.7 La Tripletta\nMentre i benchmark di sistema, modello e dati sono stati tradizionalmente studiati in modo isolato, si sta diffondendo la consapevolezza che per comprendere e far progredire completamente l’IA, dobbiamo adottare una visione più olistica. Iterando tra sistemi di benchmarking, modelli e dataset insieme, potrebbero emergere nuove intuizioni che non sono evidenti quando questi componenti vengono analizzati separatamente. Le prestazioni del sistema influiscono sulla precisione del modello, le capacità del modello determinano le esigenze dei dati e le caratteristiche dei dati determinano i requisiti del sistema.\nIl benchmarking della triade di sistema, modello e dati in modo integrato porterà probabilmente a scoperte sulla progettazione congiunta dei sistemi di IA, sulle proprietà di generalizzazione dei modelli e sul ruolo della cura e della qualità dei dati nel consentire le prestazioni. Piuttosto che benchmark ristretti di singoli componenti, il futuro dell’IA richiede benchmark che valutino la relazione simbiotica tra piattaforme di elaborazione, algoritmi e dati di training. Questa prospettiva a livello di sistema sarà fondamentale per superare le attuali limitazioni e sbloccare il prossimo livello di capacità dell’IA.\nFigura 11.11 illustra i molti modi potenziali per far interagire tra loro il benchmarking dei dati, quello dei modelli e quello dell’infrastruttura di sistema. L’esplorazione di queste complesse interazioni probabilmente porterà alla scoperta di nuove opportunità di ottimizzazione e capacità di miglioramento. La tripletta di benchmark di dati, modelli e sistemi offre un ricco spazio per la progettazione congiunta e la co-ottimizzazione.\n\n\n\n\n\n\nFigura 11.11: La tripletta del Benchmarking.\n\n\n\nSebbene questa prospettiva integrata rappresenti una tendenza emergente, il settore ha ancora molto da scoprire sulle sinergie e i compromessi tra questi componenti. Mentre eseguiamo il benchmarking iterativo di combinazioni di dati, modelli e sistemi, emergeranno nuove intuizioni che rimangono nascoste quando questi elementi vengono studiati separatamente. Questo approccio di benchmarking multiforme che traccia le intersezioni di dati, algoritmi e hardware promette di essere una strada fruttuosa per importanti progressi nell’intelligenza artificiale, anche se è ancora nelle sue fasi iniziali.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Benchmarking dell'IA</span>"
    ]
  },
  {
    "objectID": "contents/core/benchmarking/benchmarking.it.html#benchmark-per-tecnologie-emergenti",
    "href": "contents/core/benchmarking/benchmarking.it.html#benchmark-per-tecnologie-emergenti",
    "title": "11  Benchmarking dell’IA",
    "section": "11.8 Benchmark per Tecnologie Emergenti",
    "text": "11.8 Benchmark per Tecnologie Emergenti\nDate le loro significative differenze rispetto alle tecniche esistenti, le tecnologie emergenti possono essere particolarmente difficili da progettare per i benchmark. I benchmark standard utilizzati per le tecnologie esistenti potrebbero non evidenziare le feature chiave del nuovo approccio. Al contrario, i nuovi benchmark potrebbero essere visti come artificiosi per favorire la tecnologia emergente rispetto ad altre. Potrebbero essere così diversi dai benchmark esistenti da non poter essere compresi e perdere significato. Pertanto, i benchmark per le tecnologie emergenti devono bilanciare equità, applicabilità e facilità di confronto con quelli esistenti.\nUn esempio di tecnologia emergente in cui il benchmarking si è dimostrato particolarmente difficile è nel Neuromorphic Computing. Utilizzando il cervello come fonte di ispirazione per un’intelligenza generale scalabile, robusta ed efficiente dal punto di vista energetico, il calcolo neuromorfico (Schuman et al. 2022) incorpora direttamente meccanismi biologicamente realistici sia negli algoritmi di calcolo che nell’hardware, come le reti neurali spiking (Maass 1997) e le architetture non-von Neumann architectures per eseguirle (Davies et al. 2018; Modha et al. 2023). Da una prospettiva full-stack di modelli, tecniche di training e sistemi hardware, il calcolo neuromorfico differisce dall’hardware e dall’intelligenza artificiale convenzionali. Pertanto, esiste una sfida fondamentale nello sviluppo di benchmark equi e utili per guidare la tecnologia.\n\nSchuman, Catherine D., Shruti R. Kulkarni, Maryam Parsa, J. Parker Mitchell, Prasanna Date, e Bill Kay. 2022. «Opportunities for neuromorphic computing algorithms and applications». Nature Computational Science 2 (1): 10–19. https://doi.org/10.1038/s43588-021-00184-y.\n\nMaass, Wolfgang. 1997. «Networks of spiking neurons: The third generation of neural network models». Neural Networks 10 (9): 1659–71. https://doi.org/10.1016/s0893-6080(97)00011-7.\n\nDavies, Mike, Narayan Srinivasa, Tsung-Han Lin, Gautham Chinya, Yongqiang Cao, Sri Harsha Choday, Georgios Dimou, et al. 2018. «Loihi: A Neuromorphic Manycore Processor with On-Chip Learning». IEEE Micro 38 (1): 82–99. https://doi.org/10.1109/mm.2018.112130359.\n\nModha, Dharmendra S., Filipp Akopyan, Alexander Andreopoulos, Rathinakumar Appuswamy, John V. Arthur, Andrew S. Cassidy, Pallab Datta, et al. 2023. «Neural inference at the frontier of energy, space, and time». Science 382 (6668): 329–35. https://doi.org/10.1126/science.adh1174.\n\nYik, Jason, Korneel Van den Berghe, Douwe den Blanken, Younes Bouhadjar, Maxime Fabre, Paul Hueber, Denis Kleyko, et al. 2023. «NeuroBench: A Framework for Benchmarking Neuromorphic Computing Algorithms and Systems», aprile. http://arxiv.org/abs/2304.04640v3.\nUn’iniziativa in corso per sviluppare benchmark neuromorfici standard è NeuroBench (Yik et al. 2023). Per un benchmarking adeguato del neuromorfico, NeuroBench segue principi di alto livello di inclusività attraverso l’applicabilità di attività e metriche sia alle soluzioni neuromorfiche che non neuromorfiche, attuabilità dell’implementazione utilizzando strumenti comuni e aggiornamenti iterativi per continuare a garantire la pertinenza man mano che il campo cresce rapidamente. NeuroBench e altri benchmark per le tecnologie emergenti forniscono una guida critica per le tecniche future, che potrebbero essere necessarie man mano che i limiti di scalabilità degli approcci esistenti si avvicinano.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Benchmarking dell'IA</span>"
    ]
  },
  {
    "objectID": "contents/core/benchmarking/benchmarking.it.html#conclusione",
    "href": "contents/core/benchmarking/benchmarking.it.html#conclusione",
    "title": "11  Benchmarking dell’IA",
    "section": "11.9 Conclusione",
    "text": "11.9 Conclusione\nCiò che viene misurato viene migliorato. Questo capitolo ha esplorato la natura multiforme del benchmarking che abbraccia sistemi, modelli e dati. Il benchmarking è importante per far progredire l’IA in quanto fornisce le misurazioni essenziali per monitorare i progressi.\nI benchmark del sistema ML consentono l’ottimizzazione attraverso metriche di velocità, efficienza e scalabilità. I benchmark del modello guidano l’innovazione attraverso attività e metriche standardizzate oltre l’accuratezza. I benchmark dei dati evidenziano problemi di qualità, equilibrio e rappresentazione.\nÈ importante notare che la valutazione di questi componenti in modo isolato presenta dei limiti. In futuro, sarà probabilmente utilizzato un benchmarking più integrato per esplorare l’interazione tra benchmark di sistema, modello e dati. Questa visione promette nuove intuizioni sulla progettazione congiunta di dati, algoritmi e infrastrutture.\nMan mano che l’IA diventa più complessa, il benchmarking completo diventa ancora più critico. Gli standard devono evolversi continuamente per misurare nuove capacità e rivelare limitazioni. Una stretta collaborazione tra settore, mondo accademico, etichette nazionali, ecc. è essenziale per sviluppare benchmark rigorosi, trasparenti e socialmente utili.\nIl benchmarking fornisce la bussola per guidare il progresso nell’IA. Misurando costantemente e condividendo apertamente i risultati, possiamo orientarci verso sistemi performanti, robusti e affidabili. Se l’IA deve soddisfare adeguatamente le esigenze sociali e umane, deve essere sottoposta a benchmarking tenendo a mente gli interessi dell’umanità. A tal fine, ci sono aree emergenti, come il benchmarking della sicurezza dei sistemi di IA, ma questo è per un altro giorno e qualcosa di cui possiamo discutere ulteriormente in “Generative AI”!\nIl benchmarking è un argomento in continua evoluzione. L’articolo The Olympics of AI: Benchmarking Machine Learning Systems copre diversi sottocampi emergenti nel benchmarking dell’IA, tra cui robotica, realtà estesa e calcolo neuromorfico che incoraggiamo il lettore ad approfondire.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Benchmarking dell'IA</span>"
    ]
  },
  {
    "objectID": "contents/core/benchmarking/benchmarking.it.html#sec-benchmarking-ai-resource",
    "href": "contents/core/benchmarking/benchmarking.it.html#sec-benchmarking-ai-resource",
    "title": "11  Benchmarking dell’IA",
    "section": "11.10 Risorse",
    "text": "11.10 Risorse\nEcco un elenco curato di risorse per supportare studenti e insegnanti nei loro percorsi di apprendimento e insegnamento. Lavoriamo continuamente per espandere questa raccolta e presto aggiungeremo nuovi esercizi.\n\n\n\n\n\n\nSlide\n\n\n\n\n\nQueste slide sono uno strumento prezioso per gli insegnanti per tenere lezioni e per gli studenti per rivedere il materiale secondo il proprio ritmo. Incoraggiamo studenti e docenti a sfruttare queste slide per migliorare la loro comprensione e facilitare un trasferimento efficace delle conoscenze.\n\nPerché il benchmarking è importante?\nBenchmarking di inferenza embedded.\n\n\n\n\n\n\n\n\n\n\nVideo\n\n\n\n\n\n\nProssimamente.\n\n\n\n\n\n\n\n\n\n\nEsercizi\n\n\n\n\n\nPer rafforzare i concetti trattati in questo capitolo, abbiamo curato una serie di esercizi che sfidano gli studenti ad applicare le proprie conoscenze e ad approfondire la propria comprensione.\n\nEsercizio 11.1\nEsercizio 11.2",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Benchmarking dell'IA</span>"
    ]
  },
  {
    "objectID": "contents/core/ondevice_learning/ondevice_learning.it.html",
    "href": "contents/core/ondevice_learning/ondevice_learning.it.html",
    "title": "12  Apprendimento On-Device",
    "section": "",
    "text": "12.1 Panoramica\nL’apprendimento su dispositivo si riferisce all’addestramento di modelli ML direttamente sul dispositivo in cui vengono distribuiti, al contrario dei metodi tradizionali in cui i modelli vengono addestrati su server potenti e poi distribuiti sui dispositivi. Questo metodo è particolarmente rilevante per TinyML, in cui i sistemi ML sono integrati in dispositivi minuscoli e con risorse limitate.\nUn esempio di apprendimento su dispositivo può essere visto in un termostato intelligente che si adatta al comportamento dell’utente nel tempo. Inizialmente, il termostato può avere un modello generico che comprende pattern di utilizzo di base. Tuttavia, poiché è esposto a più dati, come gli orari in cui l’utente è a casa o fuori, le temperature preferite e le condizioni meteorologiche esterne, il termostato può perfezionare il suo modello direttamente sul dispositivo per fornire un’esperienza personalizzata. Tutto ciò avviene senza inviare dati a un server centrale per l’elaborazione.\nUn altro esempio è nel testo predittivo sugli smartphone. Mentre gli utenti digitano, il telefono impara dai pattern linguistici dell’utente e suggerisce parole o frasi che probabilmente verranno utilizzate in seguito. Questo apprendimento avviene direttamente sul dispositivo e il modello si aggiorna in tempo reale man mano che vengono raccolti più dati. Un esempio pratico di apprendimento su dispositivo ampiamente utilizzato è Gboard. Su un telefono Android, Gboard impara da modelli di digitazione e dettatura per migliorare l’esperienza per tutti gli utenti.\nIn alcuni casi, l’apprendimento sul dispositivo può essere abbinato a una configurazione di apprendimento federato, in cui ogni dispositivo perfeziona il proprio modello localmente utilizzando solo i dati archiviati su quel dispositivo. Questo approccio consente al modello di apprendere dai dati univoci di ogni dispositivo senza trasmetterne nessuno a un server centrale. Come mostrato in Figura 12.1, l’apprendimento federato preserva la privacy mantenendo tutti i dati personali sul dispositivo, assicurando che il processo di addestramento rimanga interamente sul dispositivo, con solo aggiornamenti riepilogativi del modello condivisi tra i dispositivi.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Apprendimento On-Device</span>"
    ]
  },
  {
    "objectID": "contents/core/ondevice_learning/ondevice_learning.it.html#panoramica",
    "href": "contents/core/ondevice_learning/ondevice_learning.it.html#panoramica",
    "title": "12  Apprendimento On-Device",
    "section": "",
    "text": "Figura 12.1: Ciclo di apprendimento federato. Fonte: Google Research.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Apprendimento On-Device</span>"
    ]
  },
  {
    "objectID": "contents/core/ondevice_learning/ondevice_learning.it.html#vantaggi-e-limiti",
    "href": "contents/core/ondevice_learning/ondevice_learning.it.html#vantaggi-e-limiti",
    "title": "12  Apprendimento On-Device",
    "section": "12.2 Vantaggi e Limiti",
    "text": "12.2 Vantaggi e Limiti\nL’apprendimento su dispositivo offre diversi vantaggi rispetto al tradizionale ML basato su cloud. Mantenendo dati e modelli sul dispositivo, elimina la necessità di costose trasmissioni di dati e risolve i problemi di privacy. Ciò consente esperienze più personalizzate e reattive, poiché il modello può adattarsi in tempo reale al comportamento dell’utente.\nTuttavia, l’apprendimento sul dispositivo presenta anche degli svantaggi. Le limitate risorse di elaborazione sui dispositivi dei consumatori possono rendere difficile l’esecuzione di modelli complessi in locale. Anche i set di dati sono più limitati poiché sono costituiti solo da dati generati dall’utente da un singolo dispositivo. Inoltre, l’aggiornamento dei modelli su ogni dispositivo può essere più complicato, poiché spesso richiede di distribuire le nuove versioni su ogni dispositivo singolarmente, anziché aggiornare senza problemi un singolo modello nel cloud.\nL’apprendimento su dispositivo apre nuove possibilità abilitando l’intelligenza artificiale offline mantenendo al contempo la privacy dell’utente. Tuttavia, richiede una gestione attenta della complessità dei modelli e dei dati entro i limiti dei dispositivi dei consumatori. Trovare il giusto equilibrio tra localizzazione e offload dal cloud è fondamentale per ottimizzare le esperienze su dispositivo.\n\n12.2.1 Vantaggi\n\nPrivacy e Sicurezza dei Dati\nUno dei vantaggi significativi dell’apprendimento sul dispositivo è la maggiore privacy e sicurezza dei dati degli utenti. Ad esempio, si consideri uno smartwatch che monitora parametri sanitari sensibili come la frequenza cardiaca e la pressione sanguigna. Elaborando i dati e adattando i modelli direttamente sul dispositivo, i dati biometrici rimangono localizzati, aggirando la necessità di trasmettere dati grezzi ai server cloud dove potrebbero essere soggetti a violazioni.\nLe violazioni dei server sono tutt’altro che rare, con milioni di record compromessi ogni anno. Ad esempio, la violazione di Equifax del 2017 ha esposto i dati personali di 147 milioni di persone. Mantenendo i dati sul dispositivo, il rischio di tali esposizioni è drasticamente ridotto. L’apprendimento sul dispositivo elimina la dipendenza dall’archiviazione cloud centralizzata e protegge dall’accesso non autorizzato da varie minacce, tra cui attori malintenzionati, minacce interne ed esposizione accidentale.\nRegolamenti come l’Health Insurance Portability and Accountability Act (HIPAA) e il General Data Protection Regulation (GDPR) impongono rigorosi requisiti di riservatezza dei dati che l’apprendimento sul dispositivo affronta abilmente. Garantendo che i dati rimangano localizzati e non vengano trasferiti ad altri sistemi, l’apprendimento sul dispositivo facilita la conformità a tali regolamenti.\nL’apprendimento sul dispositivo non è solo vantaggioso per i singoli utenti; ha implicazioni significative per le organizzazioni e i settori che gestiscono dati altamente sensibili. Ad esempio, in ambito militare, l’apprendimento sul dispositivo consente ai sistemi di prima linea di adattare modelli e funzioni indipendentemente dalle connessioni ai server centrali che potrebbero essere potenzialmente compromessi. Le informazioni critiche e sensibili sono saldamente protette dalla localizzazione dell’elaborazione e dell’apprendimento dei dati. Tuttavia, lo svantaggio è che i singoli dispositivi assumono un valore maggiore e possono incentivarne il furto o la distruzione, poiché diventano gli unici portatori di modelli di IA specializzati. È necessario prestare attenzione alla protezione dei dispositivi stessi durante la transizione all’apprendimento sul dispositivo.\nÈ inoltre importante preservare la privacy, la sicurezza e la conformità normativa dei dati personali e sensibili. Invece che nel cloud, i modelli di training e operativi aumentano sostanzialmente le misure di privacy a livello locale, assicurando che i dati degli utenti siano protetti da potenziali minacce.\nTuttavia, questo è solo parzialmente intuitivo perché l’apprendimento sul dispositivo potrebbe invece esporre i sistemi a nuovi attacchi alla privacy. Con preziosi riepiloghi dei dati e aggiornamenti dei modelli archiviati in modo permanente su singoli dispositivi, potrebbe essere molto più difficile proteggerli fisicamente e digitalmente rispetto a un grande cluster di elaborazione. Mentre l’apprendimento sul dispositivo riduce la quantità di dati compromessi in una qualsiasi violazione, potrebbe anche introdurre nuovi pericoli disperdendo informazioni sensibili su molti terminali decentralizzati. Le pratiche di sicurezza attente sono ancora essenziali per i sistemi “on-device”.\n\n\nNormativa di Conformità\nL’apprendimento sul dispositivo aiuta ad affrontare le principali normative sulla privacy come GDPR)e CCPA. Queste normative richiedono la localizzazione dei dati, limitando i trasferimenti di dati transfrontalieri a paesi approvati con controlli adeguati. Il GDPR impone inoltre requisiti di “privacy by design” e consenso per la raccolta dei dati. Mantenendo l’elaborazione dei dati e il training del modello localizzati sul dispositivo, i dati sensibili degli utenti non vengono trasferiti altrove. Ciò evita importanti grattacapi di conformità per le organizzazioni.\nAd esempio, un fornitore di servizi sanitari che monitora i parametri vitali dei pazienti con dispositivi indossabili deve garantire che i trasferimenti di dati transfrontalieri siano conformi a HIPAA e GDPR se utilizza il cloud. Determinare le leggi del paese applicabili e garantire le approvazioni per i flussi di dati internazionali introduce oneri legali e ingegneristici. Con l’apprendimento on-device, nessun dato lascia il dispositivo, semplificando la conformità. Il tempo e le risorse spesi per la conformità vengono ridotti in modo significativo.\nSettori come sanità, finanza e governo, che hanno dati altamente regolamentati, possono trarre grandi vantaggi dal training sul dispositivo. Localizzando i dati e l’apprendimento, i requisiti normativi di privacy e sovranità dei dati vengono soddisfatti più facilmente. Le soluzioni su dispositivo forniscono un modo efficiente per creare applicazioni di IA conformi.\nLe principali normative sulla privacy impongono restrizioni sullo spostamento transfrontaliero dei dati che l’apprendimento su dispositivo affronta intrinsecamente tramite elaborazione localizzata. Ciò riduce l’onere di conformità per le organizzazioni che lavorano con dati regolamentati.\n\n\nRiduzione della Larghezza di Banda, dei Costi e Maggiore Efficienza\nUno dei principali vantaggi dell’apprendimento su dispositivo è la significativa riduzione dell’utilizzo della larghezza di banda e dei costi associati all’infrastruttura cloud. Mantenendo i dati localizzati per l’addestramento del modello anziché trasmettere dati grezzi al cloud, l’apprendimento su dispositivo può comportare notevoli risparmi di larghezza di banda. Ad esempio, una rete di telecamere che analizzano i filmati video può ottenere significative riduzioni nel trasferimento di dati addestrando i modelli sul dispositivo anziché trasmettere in streaming tutti i filmati video al cloud per l’elaborazione.\nQuesta riduzione nella trasmissione dei dati consente di risparmiare larghezza di banda e si traduce in costi inferiori per server, reti e archiviazione dei dati nel cloud. Le grandi organizzazioni, che potrebbero spendere milioni in infrastrutture cloud per addestrare i modelli, possono riscontrare notevoli riduzioni dei costi grazie all’apprendimento sul dispositivo. Nell’era dell’intelligenza artificiale generativa, in cui i costi sono aumentati in modo significativo, trovare modi per contenere le spese è diventato sempre più importante.\nInoltre, anche i costi energetici e ambientali della gestione di grandi server farm sono diminuiti. I data center consumano grandi quantità di energia, contribuendo alle emissioni di gas serra. Riducendo la necessità di un’ampia infrastruttura basata su cloud, l’apprendimento sui dispositivi contribuisce a mitigare l’impatto ambientale dell’elaborazione dei dati (Wu et al. 2022).\n\nWu, Carole-Jean, Ramya Raghavendra, Udit Gupta, Bilge Acun, Newsha Ardalani, Kiwan Maeng, Gloria Chang, et al. 2022. «Sustainable ai: Environmental implications, challenges and opportunities». Proceedings of Machine Learning and Systems 4: 795–813.\nSpecificamente per le applicazioni endpoint [finali], l’apprendimento sui dispositivi riduce al minimo il numero di chiamate API di rete necessarie per eseguire l’inferenza tramite un provider cloud. I costi cumulativi associati alla larghezza di banda e alle chiamate API possono aumentare rapidamente per le applicazioni con milioni di utenti. Al contrario, eseguire training e inferenze localmente è notevolmente più efficiente e conveniente. Con ottimizzazioni all’avanguardia, è stato dimostrato che l’apprendimento on-device riduce i requisiti di memoria del training, migliora drasticamente l’efficienza della memoria e riduce fino al 20% la latenza per iterazione (Dhar et al. 2021).\n\n\nAddestramento Permanente\nUno dei principali vantaggi dell’apprendimento sul dispositivo è la sua capacità di supportare l’addestramento permanente, consentendo ai modelli di adattarsi continuamente ai nuovi dati e all’evoluzione del comportamento dell’utente direttamente sul dispositivo. In ambienti dinamici, i modelli di dati possono cambiare nel tempo, un fenomeno noto come deriva dei dati, che può degradare l’accuratezza e la pertinenza del modello se questo rimane statico. Ad esempio, le preferenze dell’utente, le tendenze stagionali o persino le condizioni esterne (come i modelli di traffico di rete o le condizioni meteorologiche) possono evolversi, richiedendo ai modelli di adattarsi per mantenere prestazioni ottimali.\nL’apprendimento sul dispositivo consente ai modelli di affrontare questo problema adattandosi in modo incrementale man mano che diventano disponibili nuovi dati. Questo processo di adattamento continuo consente ai modelli di rimanere pertinenti ed efficaci, riducendo la necessità di frequenti aggiornamenti cloud. Gli adattamenti locali riducono la necessità di trasmettere grandi set di dati al cloud per la riqualificazione, risparmiando larghezza di banda e garantendo la privacy dei dati.\n\n\n\n12.2.2 Limitazioni\nMentre i tradizionali sistemi ML basati su cloud hanno accesso a risorse di elaborazione pressoché infinite, l’apprendimento sul dispositivo è spesso limitato nella potenza di elaborazione e di archiviazione del dispositivo edge su cui viene addestrato il modello. Per definizione, un dispositivo edge è un dispositivo con risorse di elaborazione, memoria ed energia limitate che non possono essere facilmente aumentate o diminuite. Pertanto, la dipendenza dai dispositivi edge può limitare la complessità, l’efficienza e le dimensioni dei modelli ML sul dispositivo.\n\nRisorse di elaborazione\nI tradizionali sistemi ML basati su cloud utilizzano grandi server con più GPU o TPU di fascia alta, che forniscono una potenza di calcolo e una memoria pressoché infinite. Ad esempio, servizi come Amazon Web Services (AWS) EC2 consentono di configurare cluster di istanze GPU per un training parallelo massiccio.\nAl contrario, l’apprendimento sul dispositivo è limitato dall’hardware del dispositivo edge su cui viene eseguito. I dispositivi edge si riferiscono a endpoint come smartphone, elettronica embedded e dispositivi IoT. Per definizione, questi dispositivi hanno risorse di elaborazione, memoria ed energia molto limitate rispetto al cloud.\nAd esempio, uno smartphone tipico o Raspberry Pi può avere solo pochi core CPU, pochi GB di RAM e una piccola batteria. Ancora più limitati in termini di risorse sono i dispositivi microcontrollore TinyML come Arduino Nano BLE Sense. Le risorse sono fisse su questi dispositivi e non possono essere facilmente aumentate su richiesta, come il ridimensionamento dell’infrastruttura cloud. Questa dipendenza dai dispositivi edge limita direttamente la complessità, l’efficienza e le dimensioni dei modelli che possono essere distribuiti per l’addestramento sul dispositivo:\n\nComplessità: I limiti di memoria, elaborazione e potenza limitano la progettazione dell’architettura del modello, così come il numero di layer e dei parametri.\nEfficienza: I modelli devono essere fortemente ottimizzati tramite metodi come la quantizzazione e la potatura per essere eseguiti più velocemente e consumare meno energia.\nDimensioni: I file del modello effettivo devono essere compressi il più possibile per rientrare nei limiti di archiviazione dei dispositivi edge.\n\nPertanto, mentre il cloud offre una scalabilità infinita, l’apprendimento sul dispositivo deve operare entro i rigidi vincoli di risorse dell’hardware. Ciò richiede un’attenta progettazione congiunta di modelli semplificati, metodi di addestramento e ottimizzazioni su misura specificamente per i dispositivi edge.\n\n\nDimensioni, Accuratezza e Generalizzazione del Dataset\nOltre alle risorse di elaborazione limitate, l’apprendimento sul dispositivo è anche limitato dal set di dati disponibile per i modelli di training.\nNel cloud, i modelli vengono addestrati su dataset enormi e diversi come ImageNet o Common Crawl. Ad esempio, ImageNet contiene oltre 14 milioni di immagini attentamente categorizzate in migliaia di classi.\nL’apprendimento sul dispositivo si basa invece su “data silos” più piccoli e decentralizzati, unici per ogni dispositivo. Il rullino fotografico di uno smartphone potrebbe contenere solo migliaia di foto degli interessi e degli ambienti degli utenti.\nNell’apprendimento automatico, un training efficace del modello spesso presuppone che i dati siano “independent and identically distributed (IID)” [indipendenti e distribuiti in modo identico]. Ciò significa che ogni punto dati viene generato in modo indipendente (senza influenzare altri punti) e segue la stessa distribuzione statistica del resto dei dati. Quando i dati sono IID, i modelli addestrati su di essi hanno maggiori probabilità di generalizzare bene a nuovi dati simili. Tuttavia, nell’apprendimento su dispositivo, questa condizione IID è raramente soddisfatta, poiché i dati sono altamente specifici per singoli utenti e contesti. Ad esempio, due amici possono scattare foto simili degli stessi luoghi, creando dati correlati che non rappresentano una popolazione più ampia o la varietà necessaria per la generalizzazione.\nMotivi per cui i dati potrebbero essere non IID nelle impostazioni sul dispositivo:\n\nEterogeneità degli utenti: Utenti diversi hanno interessi e ambienti diversi.\nDifferenze tra dispositivi: Sensori, regioni e dati demografici influenzano i dati.\nEffetti temporali: Ora del giorno, impatti stagionali sui dati.\n\nL’efficacia del ML si basa in gran parte su dati di training ampi e diversificati. Con set di dati piccoli e localizzati, i modelli on-device potrebbero non riuscire a generalizzare tra diverse popolazioni di utenti e ambienti. Ad esempio, un modello di rilevamento delle malattie addestrato solo su immagini di un singolo ospedale non si generalizzerebbe bene ad altri dati demografici dei pazienti. Le prestazioni nel mondo reale non potranno che migliorare con progressi medici estesi e diversificati. Quindi, mentre l’apprendimento basato su cloud sfrutta enormi set di dati, l’apprendimento su dispositivo si basa su “silo di dati” decentralizzati molto più piccoli, unici per ogni utente.\nI dati limitati e le ottimizzazioni richieste per l’apprendimento on-device possono avere un impatto negativo sulla precisione e sulla generalizzazione del modello:\n\nI piccoli dataset aumentano il rischio di overfitting. Ad esempio, un classificatore di frutta addestrato su 100 immagini rischia di overfitting rispetto a uno addestrato su 1 milione di immagini diverse.\nI dati rumorosi generati dall’utente riducono la qualità. Il rumore del sensore o l’etichettatura impropria dei dati da parte di non esperti possono degradare l’addestramento.\nOttimizzazioni come la potatura e la quantizzazione compromettono la precisione per l’efficienza. Un modello quantizzato a 8 bit funziona più velocemente ma meno accuratamente di un modello a 32 bit.\n\nQuindi, mentre i modelli cloud raggiungono un’elevata precisione con enormi set di dati e senza vincoli, i modelli su dispositivo possono avere difficoltà a generalizzare. Alcuni studi dimostrano che il training sul dispositivo corrisponde all’accuratezza del cloud su determinate attività. Tuttavia, le prestazioni sui carichi di lavoro reali richiedono ulteriori studi (Lin et al. 2022). Ad esempio, un modello cloud può rilevare con precisione la polmonite nelle radiografie del torace di migliaia di ospedali. Tuttavia, un modello sul dispositivo addestrato solo su una piccola popolazione locale di pazienti potrebbe non riuscire a generalizzare. Ciò limita l’applicabilità pratica dell’apprendimento sui dispositivi per utilizzi critici come la diagnosi delle malattie o i veicoli a guida autonoma.\nIl training sul dispositivo è anche più lento del cloud a causa delle risorse limitate. Anche se ogni iterazione è più veloce, il processo di training complessivo richiede più tempo. Ad esempio, un’applicazione di robotica in tempo reale potrebbe richiedere aggiornamenti del modello entro millisecondi. L’On-device training su un piccolo hardware embedded potrebbe richiedere secondi o minuti per l’aggiornamento, troppo lento per l’uso in tempo reale.\nLe sfide relative a precisione, generalizzazione e velocità pongono ostacoli all’adozione dell’apprendimento on-device per sistemi di produzione reali, soprattutto quando affidabilità e bassa latenza sono fondamentali.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Apprendimento On-Device</span>"
    ]
  },
  {
    "objectID": "contents/core/ondevice_learning/ondevice_learning.it.html#adattamento-on-device",
    "href": "contents/core/ondevice_learning/ondevice_learning.it.html#adattamento-on-device",
    "title": "12  Apprendimento On-Device",
    "section": "12.3 Adattamento On-device",
    "text": "12.3 Adattamento On-device\nIn un’attività ML, il consumo di risorse proviene principalmente da tre fonti:\n\nIl modello ML stesso;\nIl processo di ottimizzazione durante l’apprendimento del modello\nArchiviazione ed elaborazione del dataset utilizzato per l’apprendimento.\n\nDi conseguenza, ci sono tre approcci per adattare gli algoritmi ML esistenti su dispositivi con risorse limitate:\n\nRiduzione della complessità del modello ML\nModifica delle ottimizzazioni per ridurre i requisiti delle risorse di training\nCreazione di nuove rappresentazioni dei dati più efficienti in termini di archiviazione\n\nNella sezione seguente, esamineremo questi metodi di adattamento dell’apprendimento on-device. Il capitolo Ottimizzazioni dei Modelli fornisce maggiori dettagli sulle ottimizzazioni del modello.\n\n12.3.1 Riduzione della Complessità del Modello\nIn questa sezione, discuteremo brevemente i modi per ridurre la complessità del modello quando si adattano i modelli ML sul dispositivo. Per i dettagli sulla riduzione della complessità del modello, fare riferimento al capitolo Ottimizzazioni dei Modelli.\n\nAlgoritmi ML tradizionali\nA causa delle limitazioni di elaborazione e memoria dei dispositivi edge, alcuni algoritmi ML tradizionali sono ottimi candidati per applicazioni di apprendimento on-device grazie alla loro natura leggera. Alcuni esempi di algoritmi con basso impatto sulle risorse includono Naive Bayes Classifiers, Support Vector Machines (SVM), Linear Regression, Logistic Regression e algoritmi Decision Tree selezionati.\nCon alcuni perfezionamenti, questi algoritmi ML classici possono essere adattati a specifiche architetture hardware ed eseguire attività semplici. I loro bassi requisiti di prestazioni semplificano l’integrazione dell’apprendimento continuo anche su dispositivi edge.\n\n\nPruning\nCome discusso in Sezione 9.2.1, la potatura è una tecnica fondamentale per ridurre le dimensioni e la complessità dei modelli ML. Per l’apprendimento su dispositivo, la potatura è particolarmente preziosa, poiché riduce al minimo il consumo di risorse mantenendo un’accuratezza competitiva. Rimuovendo i componenti meno informativi di un modello, la potatura consente ai modelli ML di funzionare in modo più efficiente su dispositivi con risorse limitate.\nNel contesto dell’apprendimento su dispositivo, la potatura viene applicata per adattare modelli di deep learning complessi alla memoria limitata e alla potenza di elaborazione dei dispositivi edge. Ad esempio, la potatura può ridurre il numero di neuroni o connessioni in una DNN, con conseguente consumo di meno memoria e minori calcoli. Questo approccio semplifica la struttura della rete neurale, con conseguente modello più compatto ed efficiente.\n\n\nRiduzione della Complessità dei Modelli di Deep Learning\nI framework DNN tradizionali basati su cloud hanno un sovraccarico di memoria troppo elevato per essere utilizzati sul dispositivo. Ad esempio, i sistemi di deep learning come PyTorch e TensorFlow richiedono centinaia di megabyte di overhead di memoria durante l’addestramento di modelli come MobilenetV2 e l’overhead aumenta con l’aumentare del numero di parametri di addestramento.\nLa ricerca attuale per DNN leggeri esplora principalmente architetture CNN. Esistono anche diversi framework “bare-metal” [tutto in hardware] progettati per eseguire reti neurali su MCU mantenendo bassi l’overhead computazionale e l’ingombro di memoria. Alcuni esempi includono MNN, TVM e TensorFlow Lite. Tuttavia, possono eseguire l’inferenza solo durante i passaggi in avanti e non supportano la backpropagation. Sebbene questi modelli siano progettati per l’implementazione edge, la loro riduzione nei pesi del modello e nelle connessioni architettoniche ha portato a minori requisiti di risorse per l’apprendimento continuo.\nIl compromesso tra prestazioni e supporto del modello è chiaro quando si adattano i sistemi DNN più diffusi. Come adattiamo i modelli DNN esistenti a impostazioni con risorse limitate mantenendo il supporto per la backpropagation e l’apprendimento continuo? Le ultime ricerche suggeriscono tecniche di progettazione congiunta di algoritmi e sistemi che aiutano a ridurre il consumo di risorse dell’addestramento ML sui dispositivi edge. Utilizzando tecniche come il “quantization-aware scaling” (QAS) [ridimensionamento consapevole della quantizzazione], aggiornamenti sparsi e altre tecniche all’avanguardia, l’apprendimento sul dispositivo è possibile su sistemi embedded con poche centinaia di kilobyte di RAM senza memoria aggiuntiva mantenendo un’elevata precisione.\n\n\n\n12.3.2 Modifica dei Processi di Ottimizzazione\nLa scelta della giusta strategia di ottimizzazione è importante per l’addestramento DNN su un dispositivo, poiché consente di trovare un buon minimo locale. Poiché l’addestramento avviene su un dispositivo, questa strategia deve anche considerare la memoria e la potenza limitate.\n\nQuantization-Aware Scaling\nLa quantizzazione è un metodo comune per ridurre l’impronta di memoria dell’addestramento DNN. Sebbene ciò possa introdurre nuovi errori, questi possono essere mitigati progettando un modello per caratterizzare questo errore statistico. Ad esempio, i modelli potrebbero utilizzare l’arrotondamento stocastico o introdurre l’errore di quantizzazione negli aggiornamenti del gradiente.\nUna tecnica algoritmica specifica è Quantization-Aware Scaling (QAS), che migliora le prestazioni delle reti neurali su hardware a bassa precisione, come dispositivi edge, dispositivi mobili o sistemi TinyML, regolando i fattori di scala durante il processo di quantizzazione.\nCome abbiamo discusso nel capitolo Ottimizzazioni dei Modelli, la quantizzazione è il processo di mappatura di un intervallo continuo di valori in un set discreto di valori. Nel contesto delle reti neurali, questo spesso comporta la riduzione della precisione di pesi e attivazioni da virgola mobile a 32 bit a formati di precisione inferiore come gli interi a 8 bit. Questa riduzione della precisione può ridurre significativamente il costo computazionale e l’ingombro di memoria del modello, rendendolo adatto per l’implementazione su hardware a bassa precisione. Figura 12.2 illustra questo concetto, mostrando un esempio di quantizzazione da float a intero in cui i valori in virgola mobile ad alta precisione vengono mappati in una rappresentazione intera più compatta. Questa rappresentazione visiva aiuta a chiarire come la quantizzazione può mantenere la struttura essenziale dei dati riducendone al contempo la complessità e i requisiti di archiviazione.\n\n\n\n\n\n\nFigura 12.2: Quantizzazione float-to-integer. Fonte: Nvidia.\n\n\n\nTuttavia, il processo di quantizzazione può anche introdurre errori di quantizzazione che possono degradare le prestazioni del modello. La scalatura basata sulla quantizzazione è una tecnica che riduce al minimo questi errori regolando i fattori di scala utilizzati nel processo di quantizzazione.\nIl processo QAS prevede due fasi principali:\n\nAddestramento basato sulla quantizzazione: In questa fase, la rete neurale viene addestrata tenendo conto della quantizzazione, simulandola per imitarne gli effetti durante i passaggi “forward” e “backward”. Ciò consente al modello di imparare a compensare gli errori di quantizzazione e migliorarne le prestazioni su hardware a bassa precisione. Per i dettagli, fare riferimento alla sezione QAT in Ottimizzazioni del modello.\nQuantizzazione e ridimensionamento: Dopo l’addestramento, il modello viene quantizzato in un formato a bassa precisione e i fattori di scala vengono regolati per ridurre al minimo gli errori di quantizzazione. I fattori di scala vengono scelti in base alla distribuzione dei pesi e delle attivazioni nel modello e vengono regolati per garantire che i valori quantizzati siano compresi nell’intervallo del formato a bassa precisione.\n\nQAS viene utilizzato per superare le difficoltà di ottimizzazione dei modelli su dispositivi minuscoli senza dover effettuare la messa a punto degli iperparametri; QAS ridimensiona automaticamente i gradienti tensoriali con varie precisioni di bit. Ciò stabilizza il processo di addestramento e corrisponde all’accuratezza della precisione in virgola mobile.\n\n\nAggiornamenti Sparsi\nSebbene QAS consenta l’ottimizzazione di un modello quantizzato, utilizza una grande quantità di memoria, il che non è realistico per l’addestramento sul dispositivo. Quindi, gli aggiornamenti “spare” vengono utilizzati per ridurre l’ingombro di memoria del calcolo “full backward”. Invece di potare i pesi per l’inferenza, l’aggiornamento sparso pota il gradiente durante la “backward propagation” [propagazione all’indietro] per aggiornare il modello in modo sparso. In altre parole, l’aggiornamento sparso salta i gradienti del calcolo di layer e sottotensori meno importanti.\nTuttavia, determinare lo schema di un aggiornamento sparso ottimale dato un budget di memoria vincolante può essere difficile a causa dell’ampio spazio di ricerca. Ad esempio, il modello MCUNet ha 43 layer convoluzionali e uno spazio di ricerca di circa \\(10^{30}\\). Una tecnica per affrontare questo problema è l’analisi del contributo. L’analisi del contributo misura il miglioramento dell’accuratezza dai bias (aggiornamento degli ultimi bias rispetto al solo aggiornamento del classificatore) e pesi (aggiornamento del peso di un layer extra rispetto al solo aggiornamento del bias). Cercando di massimizzare questi miglioramenti, l’analisi del contributo deriva automaticamente uno schema di aggiornamento sparso ottimale per abilitare l’addestramento sul dispositivo.\n\n\nTraining Layer-Wise\nAltri metodi oltre alla quantizzazione possono aiutare a ottimizzare le routine. Uno di questi metodi è l’addestramento “layer-wise”. Un consumatore significativo di memoria dell’addestramento DNN è la backpropagation end-to-end, che richiede che tutte le feature map intermedie siano archiviate in modo che il modello possa calcolare i gradienti. Un’alternativa a questo approccio che riduce l’impronta di memoria dell’addestramento DNN è l’addestramento sequenziale “layer-by-layer” (T. Chen et al. 2016). Invece dell’addestramento end-to-end, l’addestramento di un singolo layer alla volta aiuta a evitare di dover archiviare le feature map intermedie.\n\nChen, Tianqi, Bing Xu, Chiyuan Zhang, e Carlos Guestrin. 2016. «Training Deep Nets with Sublinear Memory Cost». ArXiv preprint abs/1604.06174 (aprile). http://arxiv.org/abs/1604.06174v2.\n\n\nTrading Computation for Memory\nLa strategia “trading computation for memory” [scambio di elaborazione per memoria] comporta il rilascio di parte della memoria utilizzata per archiviare i risultati intermedi. Invece, questi risultati possono essere ricalcolati in base alle necessità. È stato dimostrato che la riduzione della memoria in cambio di più elaborazione riduce l’impronta di memoria dell’addestramento DNN per adattarsi a quasi tutti i budget, riducendo al minimo anche i costi di elaborazione (Gruslys et al. 2016).\n\nGruslys, Audrunas, Rémi Munos, Ivo Danihelka, Marc Lanctot, e Alex Graves. 2016. «Memory-Efficient Backpropagation Through Time». In Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, a cura di Daniel D. Lee, Masashi Sugiyama, Ulrike von Luxburg, Isabelle Guyon, e Roman Garnett, 4125–33. https://proceedings.neurips.cc/paper/2016/hash/a501bebf79d570651ff601788ea9d16d-Abstract.html.\n\n\n\n12.3.3 Sviluppo di Nuove Rappresentazioni dei Dati\nLa dimensionalità e il volume dei dati di training possono avere un impatto significativo sull’adattamento sul dispositivo. Quindi, un’altra tecnica per adattare i modelli su dispositivi con risorse limitate è quella di rappresentare i set di dati in modo più efficiente.\n\nCompressione dei Dati\nL’obiettivo della compressione dei dati è raggiungere elevate precisioni limitando al contempo la quantità di dati di training. Un metodo per raggiungere questo obiettivo è dare priorità alla complessità del campione: la quantità di dati di training necessari affinché l’algoritmo raggiunga una precisione target (Dhar et al. 2021).\n\nDhar, Sauptik, Junyao Guo, Jiayi (Jason) Liu, Samarth Tripathi, Unmesh Kurup, e Mohak Shah. 2021. «A Survey of On-Device Machine Learning: An Algorithms and Learning Theory Perspective». ACM Transactions on Internet of Things 2 (3): 1–49. https://doi.org/10.1145/3450494.\n\nDarvish Rouhani, Bita, Azalia Mirhoseini, e Farinaz Koushanfar. 2017. «TinyDL: Just-in-time deep learning solution for constrained embedded systems». In 2017 IEEE International Symposium on Circuits and Systems (ISCAS), 1–4. IEEE. https://doi.org/10.1109/iscas.2017.8050343.\n\nLi, Xiang, Tao Qin, Jian Yang, e Tie-Yan Liu. 2016. «LightRNN: Memory and Computation-Efficient Recurrent Neural Networks». In Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, a cura di Daniel D. Lee, Masashi Sugiyama, Ulrike von Luxburg, Isabelle Guyon, e Roman Garnett, 4385–93. https://proceedings.neurips.cc/paper/2016/hash/c3e4035af2a1cde9f21e1ae1951ac80b-Abstract.html.\nAltri metodi più comuni di compressione dei dati si concentrano sulla riduzione della dimensionalità e del volume dei dati di training. Ad esempio, un approccio potrebbe sfruttare la sparsità della matrice per ridurre l’ingombro di memoria per l’archiviazione dei dati di training. I dati di training possono essere trasformati in un embedding a dimensione inferiore e fattorizzati in una matrice di dizionario moltiplicata per una matrice di coefficienti blocchi sparsi (Darvish Rouhani, Mirhoseini, e Koushanfar 2017). Un altro esempio potrebbe riguardare la rappresentazione di parole provenienti da un ampio set di dati di training linguistica in un formato vettoriale più compresso (Li et al. 2016).",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Apprendimento On-Device</span>"
    ]
  },
  {
    "objectID": "contents/core/ondevice_learning/ondevice_learning.it.html#il-transfer-learning",
    "href": "contents/core/ondevice_learning/ondevice_learning.it.html#il-transfer-learning",
    "title": "12  Apprendimento On-Device",
    "section": "12.4 Il Transfer Learning",
    "text": "12.4 Il Transfer Learning\nIl transfer learning è una tecnica in cui un modello sviluppato per un’attività specifica viene riutilizzato come punto di partenza per un modello su una seconda attività. Il transfer learning ci consente di sfruttare modelli pre-addestrati che hanno già appreso rappresentazioni utili da grandi set di dati e di perfezionarli per attività specifiche utilizzando set di dati più piccoli direttamente sul dispositivo. Ciò può ridurre significativamente le risorse di calcolo e il tempo necessari per l’addestramento dei modelli da zero.\nPuò essere compreso attraverso esempi intuitivi del mondo reale, come illustrato in Figura 12.3. La figura mostra scenari in cui le competenze di un dominio possono essere applicate per accelerare l’apprendimento in un campo correlato. Un esempio lampante è la relazione tra andare in bicicletta e andare in moto. Se si sa andare in bicicletta, si ha già l’abilità di stare in equilibrio su un veicolo a due ruote. La conoscenza di base di questa abilità rende significativamente più facile per te imparare a guidare una moto rispetto a qualcuno senza alcuna esperienza ciclistica. La figura illustra questo e altri scenari simili, dimostrando come l’apprendimento per trasferimento sfrutti le conoscenze esistenti per accelerare l’acquisizione di nuove competenze correlate.\n\n\n\n\n\n\nFigura 12.3: Trasferimento di conoscenze tra attività. Fonte: Zhuang et al. (2021).\n\n\nZhuang, Fuzhen, Zhiyuan Qi, Keyu Duan, Dongbo Xi, Yongchun Zhu, Hengshu Zhu, Hui Xiong, e Qing He. 2021. «A Comprehensive Survey on Transfer Learning». Proceedings of the IEEE 109 (1): 43–76. https://doi.org/10.1109/jproc.2020.3004555.\n\n\nPrendiamo l’esempio di un’applicazione di sensore intelligente che utilizza l’intelligenza artificiale on-device per riconoscere gli oggetti nelle immagini acquisite dal dispositivo. Tradizionalmente, ciò richiederebbe l’invio dei dati dell’immagine a un server, dove un ampio modello di rete neurale elabora i dati e invia i risultati. Con l’intelligenza artificiale on-device, il modello viene archiviato ed eseguito direttamente sul dispositivo, eliminando la necessità di inviare dati a un server.\nPer personalizzare il modello per le caratteristiche on-device, addestrare un modello di rete neurale da zero sul dispositivo sarebbe poco pratico a causa delle risorse di calcolo limitate e della durata della batteria. È qui che entra in gioco il “transfer learning” [apprendimento tramite trasferimento]. Invece di addestrare un modello da zero, possiamo prendere un modello pre-addestrato, come una rete neurale convoluzionale (CNN) o una rete di trasformatori addestrata su un ampio set di dati di immagini, e perfezionarlo per la nostra specifica attività di riconoscimento degli oggetti. Questa messa a punto può essere eseguita direttamente sul dispositivo utilizzando un set di dati più piccolo di immagini pertinenti all’attività. Sfruttando il modello pre-addestrato, possiamo ridurre le risorse di calcolo e il tempo necessari per il training, ottenendo comunque un’elevata precisione per l’attività di riconoscimento degli oggetti. Figura 12.4 illustra ulteriormente i vantaggi dell’apprendimento tramite trasferimento rispetto al training da zero.\n\n\n\n\n\n\nFigura 12.4: Training da zero vs. apprendimento tramite trasferimento.\n\n\n\nIl transfer learning è importante per rendere praticabile l’intelligenza artificiale on-device, consentendoci di sfruttare modelli pre-addestrati e di perfezionarli per attività specifiche, riducendo così le risorse di calcolo e il tempo necessari per il training. La combinazione di intelligenza artificiale sul dispositivo e il “transfer learning” apre nuove possibilità per applicazioni di intelligenza artificiale più attente alla privacy e più reattive alle esigenze degli utenti.\nIl transfer learning ha rivoluzionato il modo in cui i modelli vengono sviluppati e distribuiti, sia nel cloud che nell’edge. Il transfer learning viene utilizzato nel mondo reale. Un esempio del genere è l’uso del transfer learning per sviluppare modelli di intelligenza artificiale in grado di rilevare e diagnosticare malattie da immagini mediche, come raggi X, scansioni MRI [risonanza magnetica] e TAC. Ad esempio, i ricercatori della Stanford University hanno sviluppato un modello di apprendimento di trasferimento in grado di rilevare il cancro nelle immagini della pelle con una precisione del 97% (Esteva et al. 2017). Questo modello è stato pre-addestrato su 1.28 milioni di immagini per classificare un’ampia gamma di oggetti e poi specializzato per il rilevamento del cancro tramite l’addestramento su un set di dati di immagini della pelle curato da dermatologi.\n\nEsteva, Andre, Brett Kuprel, Roberto A. Novoa, Justin Ko, Susan M. Swetter, Helen M. Blau, e Sebastian Thrun. 2017. «Dermatologist-level classification of skin cancer with deep neural networks». Nature 542 (7639): 115–18. https://doi.org/10.1038/nature21056.\nIn contesti di produzione, l’implementazione del transfer learning in genere comporta due fasi chiave: pre-distribuzione e post-distribuzione. La pre-distribuzione si concentra sulla preparazione del modello per il suo compito specializzato prima del rilascio, mentre la post-distribuzione consente al modello di adattarsi ulteriormente in base ai dati dei singoli utenti, migliorando la personalizzazione e l’accuratezza nel tempo.\n\n12.4.1 Specializzazione Pre-Distribuzione\nNella fase di pre-implementazione, il transfer learning funge da catalizzatore per accelerare il processo di sviluppo. Ecco come funziona tipicamente: Si immagini di creare un sistema per riconoscere diverse razze di cani. Invece di partire da zero, possiamo utilizzare un modello pre-addestrato che ha già padroneggiato il compito più ampio di riconoscere gli animali nelle immagini.\nQuesto modello pre-addestrato funge da solida base e contiene una vasta conoscenza acquisita da dati estesi. Quindi perfezioniamo questo modello utilizzando un set di dati specializzato contenente immagini di varie razze di cani. Questo processo di messa a punto adatta il modello alle nostre esigenze specifiche, ovvero identificare con precisione le razze di cani. Una volta perfezionato e convalidato per soddisfare i criteri di prestazione, questo modello specializzato è pronto per l’implementazione.\nEcco come funziona in pratica:\n\nInizia con un Modello Pre-Addestrato: Si inizia selezionando un modello che è già stato addestrato su un set di dati completo, solitamente correlato a un’attività generale. Questo modello funge da base per l’attività in questione.\nFine-tuning: Il modello pre-addestrato viene poi perfezionato su un set di dati più piccolo e specifico per l’attività desiderata. Questo passaggio consente al modello di adattare e specializzare la sua conoscenza ai requisiti specifici dell’applicazione.\nValidazione: Dopo la messa a punto, il modello viene convalidato per garantire che soddisfi i criteri di prestazione per l’attività specializzata.\nDeployment: Una volta convalidato, il modello specializzato viene distribuito nell’ambiente di produzione.\n\nQuesto metodo riduce significativamente il tempo e le risorse di calcolo necessarie per addestrare un modello da zero (Pan e Yang 2010). Adottando l’apprendimento tramite trasferimento, i sistemi embedded possono raggiungere un’elevata precisione su attività specializzate senza la necessità di raccogliere dati estesi o di impiegare risorse di calcolo significative per l’addestramento da zero.\n\nPan, Sinno Jialin, e Qiang Yang. 2010. «A Survey on Transfer Learning». IEEE Transactions on Knowledge and Data Engineering 22 (10): 1345–59. https://doi.org/10.1109/tkde.2009.191.\n\n\n12.4.2 Adattamento Post-Distribuzione\nL’implementazione su un dispositivo non deve necessariamente segnare il culmine del percorso educativo di un modello ML. Con l’avvento dell’apprendimento per trasferimento, apriamo le porte all’implementazione di modelli ML adattivi in scenari del mondo reale, soddisfacendo le esigenze personalizzate degli utenti.\nConsideriamo un’applicazione reale in cui un genitore desidera identificare il proprio figlio in una raccolta di immagini di un evento scolastico sul proprio smartphone. In questo scenario, il genitore si trova di fronte alla sfida di localizzare il proprio figlio in mezzo alle immagini di molti altri bambini. L’apprendimento per trasferimento può essere impiegato qui per perfezionare il modello di un sistema embedded per questo compito unico e specializzato. Inizialmente, il sistema potrebbe utilizzare un modello generico addestrato per riconoscere i volti nelle immagini. Tuttavia, con l’apprendimento per trasferimento, il sistema può adattare questo modello per riconoscere le caratteristiche specifiche del figlio dell’utente.\nEcco come funziona:\n\nRaccolta Dati: Il sistema embedded raccoglie immagini che includono il bambino, idealmente con l’input del genitore per garantire accuratezza e pertinenza. Ciò può essere fatto direttamente sul dispositivo, mantenendo la privacy dei dati dell’utente.\nPerfezionamento sul Dispositivo: Il modello di riconoscimento facciale preesistente, che è stato addestrato su un set di dati ampio e diversificato, viene poi perfezionato utilizzando le nuove immagini raccolte del bambino. Questo processo adatta il modello per riconoscere le caratteristiche facciali specifiche del bambino, distinguendolo dagli altri bambini nelle immagini.\nValidazione: Il modello rifinito viene poi convalidato per garantire che riconosca accuratamente il bambino in varie immagini. Ciò può comportare che il genitore verifichi le prestazioni del modello e fornisca feedback per ulteriori miglioramenti.\nUso Localizzato: Una volta adattato, il modello è in grado di localizzare istantaneamente il bambino nelle foto, offrendo un’esperienza personalizzata senza dover ricorrere a risorse cloud o al trasferimento di dati.\n\nQuesta personalizzazione al volo migliora l’efficacia del modello per il singolo utente, assicurando che tragga vantaggio dalla personalizzazione ML. Questo è, in parte, il modo in cui iPhotos o Google Photos funzionano quando ci chiedono di riconoscere un volto e poi, in base a queste informazioni, indicizzano tutte le foto di quel volto. Poiché l’apprendimento e l’adattamento avvengono sul dispositivo stesso, non ci sono rischi per la privacy personale. Le immagini dei genitori non vengono caricate su un server cloud o condivise con terze parti, proteggendo la privacy della famiglia e continuando a raccogliere i benefici di un modello ML personalizzato. Questo approccio rappresenta un significativo passo avanti nella ricerca per fornire agli utenti soluzioni ML personalizzate che rispettino e sostengano la loro privacy.\n\n\n12.4.3 Vantaggi\nIl transfer learning è diventato una tecnica importante in ML e intelligenza artificiale, ed è particolarmente prezioso per diversi motivi.\n\nScarsità di Dati: In molte applicazioni del mondo reale, raccogliere un ampio set di dati etichettato per addestrare un modello ML da zero è difficile, costoso e richiede molto tempo. Il transfer learning affronta questa sfida consentendo l’uso di modelli pre-addestrati che hanno già appreso funzionalità preziose da vasti set di dati etichettati, riducendo così la necessità di dati annotati estesi nella nuova attività.\nSpese Computazionali: Addestrare un modello da zero richiede risorse computazionali e tempo significativi, specialmente per modelli complessi come reti neurali profonde. Utilizzando il transfer learning, possiamo sfruttare il calcolo che è già stato eseguito durante l’addestramento del modello sorgente, risparmiando così tempo e potenza computazionale.\n\nCi sono vantaggi nel riutilizzare le funzionalità:\n\nApprendimento di Feature Gerarchiche: I modelli di deep learning, in particolare le CNN, possono apprendere feature gerarchiche. I layer inferiori in genere apprendono funzionalità generiche come bordi e forme, mentre quelli superiori apprendono funzionalità più complesse e specifiche per l’attività. Il transfer learning ci consente di riutilizzare le funzionalità generiche apprese da un modello e di perfezionare i livelli superiori per la nostra attività specifica.\nAumento delle Prestazioni: È stato dimostrato che il transfer learning aumenta le prestazioni dei modelli su attività con dati limitati. La conoscenza acquisita dall’attività dall’attività sorgente può fornire un prezioso punto di partenza e portare a una convergenza più rapida e a una maggiore accuratezza nell’attività target.\n\n\n\n\n\n\n\nEsercizio 12.1: Il Transfer Learning\n\n\n\n\n\nSi immagini di addestrare un’IA a riconoscere i fiori come un professionista, ma senza aver bisogno di un milione di immagini di fiori! Questo è il potere del transfer learning. In questo Colab, prenderemo un’IA che conosce già le immagini e le insegneremo a diventare un’esperta di fiori con meno sforzo. Prepararsi a rendere la propria IA più intelligente, non è più difficile!\n\n\n\n\n\n\n12.4.4 Concetti Fondamentali\nComprendere i concetti fondamentali del transfer learning è essenziale per utilizzare efficacemente questo potente approccio in ML. Qui, analizzeremo alcuni dei principi e dei componenti principali che stanno alla base del processo di transfer learning.\n\nAttività di Origine e di Destinazione\nNel transfer learning, sono coinvolte due attività principali: l’attività di origine e quella di destinazione. L’attività di origine è quella per la quale il modello è già stato addestrato e ha appreso informazioni preziose. L’attività di destinazione è la nuova attività che vogliamo che il modello esegua. L’obiettivo del transfer learning è sfruttare le conoscenze acquisite dall’attività di origine per migliorare le prestazioni nell’attività di destinazione.\nSupponiamo di avere un modello addestrato per riconoscere vari frutti nelle immagini (attività di origine) e di voler creare un nuovo modello per riconoscere diverse verdure nelle immagini (attività di destinazione). In tal caso, possiamo utilizzare il transfer learning per sfruttare le conoscenze acquisite durante l’attività di riconoscimento della frutta per migliorare le prestazioni del modello di riconoscimento della verdura.\n\n\nTrasferimento della Rappresentazione\nIl trasferimento della rappresentazione riguarda le rappresentazioni apprese (caratteristiche) dall’attività di origine all’attività di destinazione. Esistono tre tipi principali di trasferimento della rappresentazione:\n\nTrasferimento di Istanza: Implica il riutilizzo delle istanze di dati dall’attività di origine nell’attività di destinazione.\nTrasferimento della Rappresentazione delle Feature: Implica il trasferimento delle rappresentazioni di Feature [funzionalità] apprese dall’attività di origine all’attività di destinazione.\nTrasferimento di Parametri: Implica il trasferimento dei parametri appresi del modello (pesi) dall’attività di origine all’attività di destinazione.\n\nNell’elaborazione del linguaggio naturale, un modello addestrato per comprendere la sintassi e la grammatica di una lingua (attività di origine) può trasferire le sue rappresentazioni apprese a un nuovo modello progettato per eseguire l’analisi del sentiment (attività di destinazione).\n\n\nFinetuning\nIl “finetuning” [messa a punto] è il processo di regolazione dei parametri di un modello pre-addestrato per adattarlo all’attività di destinazione. In genere, ciò comporta l’aggiornamento dei pesi dei layer del modello, in particolare degli ultimi layer, per rendere il modello più pertinente per la nuova attività. Nella classificazione delle immagini, un modello pre-addestrato su un set di dati generale come ImageNet (attività di origine) può essere messo a punto regolando i pesi dei suoi livelli per ottenere buone prestazioni in un’attività di classificazione specifica, come il riconoscimento di specie animali specifiche (attività di destinazione).\n\n\nEstrazione delle Feature\nL’estrazione delle “feature” [caratteristiche] comporta l’utilizzo di un modello pre-addestrato come estrattore di feature fisse, in cui l’output dei layer intermedi del modello viene utilizzato come feature per l’attività di destinazione. Questo approccio è particolarmente utile quando l’attività di destinazione ha un set di dati di piccole dimensioni, poiché le feature apprese dal modello pre-addestrato possono migliorare significativamente le prestazioni. Nell’analisi delle immagini mediche, un modello pre-addestrato su un ampio set di dati di immagini mediche generali (attività di origine) può essere utilizzato come estrattore di feature per fornire funzionalità preziose per un nuovo modello progettato per riconoscere specifici tipi di tumori nelle immagini radiografiche (attività di destinazione).\n\n\n\n12.4.5 Tipi di Apprendimento Tramite Trasferimento\nL’apprendimento tramite trasferimento può essere classificato in tre tipi principali in base alla natura delle attività e dei dati di origine e di destinazione. Esploriamo ciascun tipo in dettaglio:\n\nApprendimento Tramite Trasferimento Induttivo\nNell’apprendimento tramite trasferimento induttivo, l’obiettivo è apprendere la funzione predittiva di destinazione con l’aiuto dei dati di origine. In genere comporta la messa a punto di un modello pre-addestrato sull’attività di destinazione con dati etichettati disponibili. Un esempio comune di apprendimento tramite trasferimento induttivo sono le attività di classificazione delle immagini. Ad esempio, un modello pre-addestrato sul set di dati ImageNet (attività di origine) può essere messo a punto per classificare tipi specifici di uccelli (attività di destinazione) utilizzando un set di dati etichettato più piccolo di immagini di uccelli.\n\n\nApprendimento Tramite Trasferimento Transduttivo\nL’apprendimento tramite trasferimento transduttivo comporta l’utilizzo di dati di origine e destinazione, ma solo dell’attività di origine. L’obiettivo principale è trasferire la conoscenza dal dominio di origine al dominio di destinazione, anche se le attività rimangono le stesse. L’analisi del “sentiment” per diverse lingue può servire come esempio di apprendimento tramite trasferimento transduttivo. Un modello addestrato per eseguire l’analisi del sentiment in inglese (attività di origine) può essere adattato per eseguire l’analisi del sentiment in un’altra lingua, come il francese (attività di destinazione), sfruttando set di dati paralleli di frasi in inglese e francese con gli stessi sentimenti.\n\n\nApprendimento con Trasferimento Non Supervisionato\nL’apprendimento con trasferimento non supervisionato viene utilizzato quando le attività di origine e di destinazione sono correlate, ma non sono disponibili dati etichettati per l’attività di destinazione. L’obiettivo è sfruttare la conoscenza acquisita dall’attività di origine per migliorare le prestazioni nell’attività di destinazione, anche senza dati etichettati. Un esempio di apprendimento di trasferimento non supervisionato è la modellazione degli argomenti nei dati di testo. Un modello addestrato per estrarre argomenti da articoli di notizie (attività di origine) può essere adattato per estrarre argomenti da post sui social media (attività di destinazione) senza aver bisogno di dati etichettati per i post sui social media.\n\n\nConfronto e Compromessi\nSfruttando questi diversi tipi di apprendimento per trasferimento, i professionisti possono scegliere l’approccio che meglio si adatta alla natura dei loro compiti e ai dati disponibili, portando infine a modelli di ML più efficaci ed efficienti. Quindi, in sintesi:\n\nInduttivo: Diversi compiti di origine e destinazione, domini diversi\nTrasduttivo: diversi compiti di origine e destinazione, stesso dominio\nNon supervisionato: dati di origine non etichettati, trasferisce le rappresentazioni delle feature\n\nTabella 12.1 presenta una matrice che delinea in modo un po’ più dettagliato le somiglianze e le differenze tra i tipi di apprendimento per trasferimento:\n\n\n\nTabella 12.1: Confronto dei tipi di apprendimento per trasferimento.\n\n\n\n\n\n\n\n\n\n\n\nAspetto\nApprendimento Induttivo per Trasferimento\nApprendimento Trasduttivo per Trasferimento\nApprendimento Non Supervisionato\n\n\n\n\nDati Etichettati per l’Attività di Destinazione\nObbligatorio\nNon obbligatorio\nNon obbligatorio\n\n\nAttività di origine\nPuò essere diversa\nLo stesso\nLo stesso o diverso\n\n\nAttività di destinazione\nPuò essere diversa\nLo stesso\nPuò essere diverso\n\n\nObiettivo\nMigliorare le prestazioni dell’attività target con i dati sorgente\nTrasferisci la conoscenza dal dominio sorgente a quello target\nSfrutta l’attività sorgente per migliorare le prestazioni dell’attività target senza dati etichettati\n\n\nEsempio\nDa ImageNet alla classificazione degli uccelli\nAnalisi del sentiment in diverse lingue\nModellazione degli argomenti per diversi dati di testo\n\n\n\n\n\n\n\n\n\n12.4.6 Vincoli e Considerazioni\nQuando si intraprende un apprendimento per trasferimento, ci sono diversi fattori che devono essere considerati per garantire un trasferimento di conoscenze di successo e prestazioni del modello. Ecco una ripartizione di alcuni fattori chiave:\n\nSomiglianza dei Domini\nLa similarità di dominio si riferisce al grado di somiglianza tra i tipi di dati utilizzati nelle applicazioni di origine e di destinazione. Più simili sono i domini, più è probabile che l’apprendimento per trasferimento abbia successo. Ad esempio, trasferire la conoscenza da un modello addestrato su immagini esterne (dominio di origine) a una nuova applicazione che coinvolge immagini interne (dominio di destinazione) è più fattibile che trasferire la conoscenza da immagini esterne a un’applicazione basata su testo. Poiché immagini e testo sono fondamentalmente tipi di dati diversi, i domini sono dissimili, rendendo l’apprendimento per trasferimento più impegnativo.\n\n\nSimilarità dell’Attività\nLa similarità dell’attività, d’altra parte, si riferisce a quanto sono simili gli obiettivi o le funzioni delle attività di origine e di destinazione. Se le attività sono simili, è più probabile che l’apprendimento per trasferimento sia efficace. Ad esempio, un modello addestrato per classificare diverse razze di cani (attività di origine) può essere adattato più facilmente per classificare diverse razze di gatti (attività di destinazione) rispetto a quanto potrebbe essere adattato a un’attività meno correlata, come l’identificazione di immagini satellitari. Poiché entrambi i compiti comportano la classificazione visiva degli animali, la somiglianza dei compiti favorisce un trasferimento efficace, mentre il passaggio a un compito non correlato potrebbe rendere l’apprendimento tramite trasferimento meno efficace.\n\n\nQualità e Quantità dei Dati\nLa qualità e la quantità dei dati disponibili per il compito di destinazione possono avere un impatto significativo sul successo dell’apprendimento per trasferimento. Più dati di alta qualità possono comportare migliori prestazioni del modello. Supponiamo di avere un ampio set di dati con immagini chiare e ben etichettate per riconoscere specie di uccelli specifiche. In tal caso, il processo di apprendimento per trasferimento avrà probabilmente più successo rispetto a un set di dati piccolo e rumoroso.\n\n\nSovrapposizione dello Spazio delle Feature\nLa sovrapposizione dello spazio delle feature si riferisce a quanto bene le feature apprese dal modello sorgente si allineano con quelle necessarie per l’attività di destinazione. Una maggiore sovrapposizione può portare a un apprendimento per trasferimento più efficace. Un modello addestrato su immagini ad alta risoluzione (attività di origine) potrebbe non trasferirsi bene a un’attività di destinazione che coinvolge immagini a bassa risoluzione, poiché lo spazio delle feature (alta risoluzione rispetto a bassa risoluzione) è diverso.\n\n\nComplessità del Modello\nAnche la complessità del modello sorgente può influire sul successo dell’apprendimento per trasferimento. A volte, un modello più semplice potrebbe trasferirsi meglio di uno complesso, poiché è meno probabile che si adatti eccessivamente all’attività di origine. Ad esempio, un semplice modello CNN addestrato su dati di immagini (attività di origine) può essere trasferito con maggiore successo a una nuova attività di classificazione di immagini (attività di destinazione) rispetto a una CNN complessa con molti layer, poiché è meno probabile che il modello più semplice si adatti eccessivamente all’attività di origine.\nConsiderando questi fattori, i professionisti del ML possono prendere decisioni informate su quando e come utilizzare l’apprendimento per trasferimento, portando infine a prestazioni del modello più efficaci nell’attività di destinazione. Il successo dell’apprendimento per trasferimento dipende dal grado di similarità tra i domini di origine e di destinazione. L’overfitting [adattamento eccessivo] è rischioso, soprattutto quando la messa a punto avviene su un set di dati limitato. Sul fronte computazionale, alcuni modelli pre-addestrati, a causa delle loro dimensioni, potrebbero non adattarsi comodamente ai vincoli di memoria di alcuni dispositivi o potrebbero essere eseguiti in modo proibitivamente lento. Nel tempo, con l’evoluzione dei dati, c’è il potenziale per la “drift” [deriva] del modello, che indica la necessità di un riaddestramento periodico o di un adattamento continuo.\nScoprire di più sull’apprendimento per trasferimento in Video 12.1 di seguito.\n\n\n\n\n\n\nVideo 12.1: Il Transfer Learning",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Apprendimento On-Device</span>"
    ]
  },
  {
    "objectID": "contents/core/ondevice_learning/ondevice_learning.it.html#sec-fl",
    "href": "contents/core/ondevice_learning/ondevice_learning.it.html#sec-fl",
    "title": "12  Apprendimento On-Device",
    "section": "12.5 Apprendimento Automatico Federato",
    "text": "12.5 Apprendimento Automatico Federato\n\n12.5.1 Panoramica dell’Apprendimento Federato\nL’Internet moderna è piena di grandi reti di dispositivi connessi. Che si tratti di telefoni cellulari, termostati, smart speaker o altri prodotti IoT, innumerevoli dispositivi edge sono una miniera d’oro per dati iperpersonalizzati e ricchi. Tuttavia, con quei dati ricchi arriva una serie di problemi con il trasferimento delle informazioni e la privacy. Costruire un set di dati di training nel cloud da questi dispositivi comporterebbe un’ampia larghezza di banda, costi per il trasferimento dati e violazione della privacy degli utenti.\nL’apprendimento federato offre una soluzione a questi problemi: addestrare i modelli parzialmente sui dispositivi edge e comunicare solo gli aggiornamenti al cloud. Nel 2016, un team di Google ha progettato un’architettura per l’apprendimento federato che tenta di risolvere questi problemi. Nel loro articolo iniziale, McMahan et al. (2017) delinea un algoritmo di apprendimento federato di principio chiamato FederatedAveraging, mostrato in Figura 12.5. In particolare, FederatedAveraging esegue la “stochastic gradient descent (SGD) [discesa del gradiente stocastico] su diversi dispositivi edge. In questo processo, ogni dispositivo calcola un gradiente \\(g_k = \\nabla F_k(w_t)\\) che viene poi applicato per aggiornare i pesi lato server come (con \\(\\eta\\) come tasso di apprendimento su \\(k\\) client):\n\nMcMahan, Brendan, Eider Moore, Daniel Ramage, Seth Hampson, e Blaise Agüera y Arcas. 2017. «Communication-Efficient Learning of Deep Networks from Decentralized Data.» In Artificial intelligence and statistics, 1273–82. PMLR. http://proceedings.mlr.press/v54/mcmahan17a.html.\n\\[\nw_{t+1} \\rightarrow w_t - \\eta \\sum_{k=1}^{K} \\frac{n_k}{n}g_k\n\\] Questo riassume l’algoritmo di base per l’apprendimento federato sulla destra. Per ogni round di addestramento, il server prende un set casuale di dispositivi client e chiama ogni client per addestrare sul suo batch locale usando i pesi lato server più recenti. Tali pesi vengono poi restituiti al server, dove vengono raccolti individualmente e calcolati per aggiornare i pesi del modello globale.\n\n\n\n\n\n\nFigura 12.5: Algoritmo FederatedAverage proposto da Google. Fonte: McMahan et al. (2017).\n\n\n\nCon questa struttura proposta, ci sono alcuni vettori chiave per ottimizzare ulteriormente l’apprendimento federato. Descriveremo ciascuno di essi nelle seguenti sottosezioni.\nVideo 12.2 fornisce una panoramica dell’apprendimento federato.\n\n\n\n\n\n\nVideo 12.2: Il Transfer Learning\n\n\n\n\n\n\nFigura 12.6 delinea l’impatto trasformativo dell’apprendimento federato sull’apprendimento on-device.\n\n\n\n\n\n\nFigura 12.6: L’apprendimento federato sta rivoluzionando l’apprendimento on-device.\n\n\n\n\n\n12.5.2 Efficienza della Comunicazione\nUno dei principali colli di bottiglia nell’apprendimento federato è la comunicazione. Ogni volta che un client addestra il modello, deve comunicare i propri aggiornamenti al server. Analogamente, una volta che il server ha calcolato la media di tutti gli aggiornamenti, deve inviarli al client. Ciò comporta enormi costi di larghezza di banda e risorse su grandi reti di milioni di dispositivi. Con l’avanzare del campo dell’apprendimento federato, sono state sviluppate alcune ottimizzazioni per ridurre al minimo questa comunicazione. Per affrontare l’ingombro del modello, i ricercatori hanno sviluppato tecniche di compressione del modello. Nel protocollo client-server, l’apprendimento federato può anche ridurre al minimo la comunicazione tramite la condivisione selettiva degli aggiornamenti sui client. Infine, anche tecniche di aggregazione efficienti possono semplificare il processo di comunicazione.\n\n\n12.5.3 Compressione del Modello\nNell’apprendimento federato standard, il server comunica l’intero modello a ciascun client, quindi il client invia tutti i pesi aggiornati. Ciò significa che il modo più semplice per ridurre l’ingombro di memoria e comunicazione del client è ridurre al minimo le dimensioni del modello che deve essere comunicato. Possiamo impiegare tutte le strategie di ottimizzazione del modello discusse in precedenza per farlo.\nNel 2022, un altro team di Google ha proposto che ogni client comunichi tramite un formato compresso e decomprima il modello al volo per l’addestramento (Yang et al. 2023), allocando e deallocando l’intera memoria per il modello solo per un breve periodo durante l’addestramento. Il modello viene compresso tramite una gamma di diverse strategie di quantizzazione elaborate nel loro documento. Nel frattempo, il server può aggiornare il modello non compresso decomprimendolo e applicando gli aggiornamenti man mano che arrivano.\n\nYang, Tien-Ju, Yonghui Xiao, Giovanni Motta, Françoise Beaufays, Rajiv Mathews, e Mingqing Chen. 2023. «Online Model Compression for Federated Learning with Large Models». In ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 1–5. IEEE; IEEE. https://doi.org/10.1109/icassp49357.2023.10097124.\n\n\n12.5.4 Condivisione Selettiva degli Aggiornamenti\nEsistono molti metodi per condividere selettivamente gli aggiornamenti. Il principio generale è che la riduzione della porzione del modello che i client stanno addestrando lato edge riduce la memoria necessaria per l’addestramento e la dimensione della comunicazione con il server. Nell’apprendimento federato di base, il client addestra l’intero modello. Ciò significa che quando un client invia un aggiornamento al server, ha gradienti per ogni peso nella rete.\nTuttavia, non possiamo semplicemente ridurre la comunicazione inviando parti di quei gradienti da ogni client al server perché i gradienti fanno parte di un intero aggiornamento necessario per migliorare il modello. Invece, si deve progettare architettonicamente il modello in modo che ogni client addestri solo una piccola parte del modello più ampio, riducendo la comunicazione totale e ottenendo comunque il vantaggio dell’addestramento sui dati del client. Shi e Radu (2022) applica questo concetto a una CNN suddividendo il modello globale in due parti: una superiore e una inferiore, come mostrato in Z. Chen e Xu (2023).\n\nShi, Hongrui, e Valentin Radu. 2022. «Data selection for efficient model update in federated learning». In Proceedings of the 2nd European Workshop on Machine Learning and Systems, 72–78. ACM. https://doi.org/10.1145/3517207.3526980.\n\n\n\n\n\n\nFigura 12.7: Apprendimento federato con addestramento del modello diviso. Il modello è diviso in una parte inferiore, addestrata localmente su ogni client, e una parte superiore, perfezionata sul server. I client eseguono aggiornamenti locali, generando mappe di attivazione dai loro dati, che vengono inviati al server anziché dati grezzi per garantire la privacy. Il server utilizza queste mappe di attivazione per aggiornare la parte superiore, poi combina entrambe le parti e ridistribuisce il modello aggiornato ai client. Questa configurazione riduce al minimo la comunicazione, preserva la privacy e adatta il modello ai diversi dati dei client. Fonte: Shi et al., (2022).\n\n\n\nLa parte inferiore del modello, responsabile dell’estrazione di feature generiche, viene addestrata direttamente su ciascun dispositivo client. Utilizzando la media federata, questa parte inferiore apprende funzionalità fondamentali condivise su tutti i client, consentendole di generalizzare bene su dati diversi. Nel frattempo, la parte superiore del modello, che cattura modelli più specifici e complessi, viene addestrata sul server. Invece di inviare dati grezzi al server, ogni client genera mappe di attivazione, una rappresentazione compressa delle feature più rilevanti dei suoi dati locali, e le invia al server. Il server utilizza queste mappe di attivazione per perfezionare la parte superiore del modello, consentendogli di diventare più sensibile alle diverse distribuzioni di dati riscontrate nei client senza compromettere la privacy dell’utente.\nQuesto approccio riduce significativamente il carico di comunicazione, poiché vengono trasmesse solo mappe di attivazione riepilogative anziché set di dati completi. Concentrandosi sull’addestramento condiviso per la parte inferiore e sulla messa a punto specializzata per la parte superiore, il sistema raggiunge un equilibrio: riduce al minimo il trasferimento di dati, preserva la privacy e rende il modello robusto ai vari tipi di input riscontrati sui dispositivi client.\n\n\n12.5.5 Aggregazione Ottimizzata\nOltre a ridurre il sovraccarico di comunicazione, l’ottimizzazione della funzione di aggregazione può migliorare la velocità e l’accuratezza dell’addestramento del modello in determinati casi d’uso di apprendimento federato. Mentre lo standard per l’aggregazione è solo la media, vari altri approcci possono migliorare l’efficienza, l’accuratezza e la sicurezza del modello.\nUn’alternativa è la “media ritagliata”, che limita gli aggiornamenti del modello entro un intervallo specifico. Un’altra strategia per preservare la sicurezza è l’aggregazione media della privacy differenziale. Questo approccio integra la privacy differenziale nella fase di aggregazione per proteggere le identità dei client. Ogni client aggiunge uno strato di rumore casuale ai propri aggiornamenti prima di comunicare al server. Il server si aggiorna quindi con gli aggiornamenti rumorosi, il che significa che la quantità di rumore deve essere regolata attentamente per bilanciare privacy e precisione.\nOltre ai metodi di aggregazione che migliorano la sicurezza, ci sono diverse modifiche ai metodi di aggregazione che possono migliorare la velocità di training e le prestazioni aggiungendo metadati client insieme agli aggiornamenti del peso. Il “Momentum aggregation” è una tecnica che aiuta ad affrontare il problema della convergenza. Nell’apprendimento federato, i dati client possono essere estremamente eterogenei a seconda dei diversi ambienti in cui vengono utilizzati i dispositivi. Ciò significa che molti modelli con dati eterogenei potrebbero aver bisogno di aiuto per convergere. Ogni client memorizza localmente un termine di “momentum”, che traccia il ritmo del cambiamento su diversi aggiornamenti. Con i client che comunicano questo “momentum”, il server può tenere conto della velocità di cambiamento di ogni aggiornamento quando si modifica il modello globale per accelerare la convergenza. Allo stesso modo, l’aggregazione ponderata può tenere conto delle prestazioni del client o di altri parametri come il tipo di dispositivo o la potenza della connessione di rete per regolare il peso con cui il server dovrebbe incorporare gli aggiornamenti del modello. Ulteriori descrizioni di algoritmi di aggregazione specifici sono fornite da Moshawrab et al. (2023).\n\nMoshawrab, Mohammad, Mehdi Adda, Abdenour Bouzouane, Hussein Ibrahim, e Ali Raad. 2023. «Reviewing Federated Learning Aggregation Algorithms; Strategies, Contributions, Limitations and Future Perspectives». Electronics 12 (10): 2287. https://doi.org/10.3390/electronics12102287.\n\n\n12.5.6 Gestione dei Dati non-IID\nQuando si utilizza l’apprendimento federato per addestrare un modello su molti dispositivi client, è conveniente considerare i dati come indipendenti e distribuiti in modo identico (IID) su tutti i client. Quando i dati sono IID, il modello convergerà più velocemente e funzionerà meglio perché ogni aggiornamento locale su un dato client è più rappresentativo del set di dati più ampio. Questo semplifica l’aggregazione, poiché è possibile calcolare direttamente la media di tutti i client. Tuttavia, differisce dal modo in cui i dati spesso appaiono nel mondo reale. Si considerino alcuni dei seguenti modi in cui i dati possono essere non IID:\n\nImparando su un set di dispositivi di monitoraggio sanitari, diversi modelli di dispositivi potrebbero significare diverse qualità e proprietà dei sensori. Ciò significa che sensori e dispositivi di bassa qualità possono produrre dati e, pertanto, aggiornamenti del modello nettamente diversi da quelli di alta qualità\nUna tastiera intelligente addestrata per eseguire la correzione automatica. Se si ha una quantità sproporzionata di dispositivi da una determinata regione, lo slang, la struttura della frase o persino il linguaggio che stavano usando potrebbero deviare più aggiornamenti verso un certo stile di digitazione\nSe si hanno sensori per la fauna selvatica in aree remote, la connettività potrebbe non essere distribuita equamente, facendo sì che alcuni client in determinate regioni non siano in grado di inviare più aggiornamenti rispetto ad altri. Se quelle regioni hanno un’attività di fauna selvatica diversa da alcune specie, ciò potrebbe distorcere gli aggiornamenti verso quegli animali\n\nEsistono alcuni approcci per affrontare i dati non IID nell’apprendimento federato. Uno potrebbe essere quello di modificare l’algoritmo di aggregazione. Se si utilizza un algoritmo di aggregazione ponderato, è possibile regolarlo in base a diverse proprietà del client come regione, proprietà del sensore o connettività (Zhao et al. 2018).\n\nZhao, Yue, Meng Li, Liangzhen Lai, Naveen Suda, Damon Civin, e Vikas Chandra. 2018. «Federated Learning with Non-IID Data». ArXiv preprint abs/1806.00582 (giugno). http://arxiv.org/abs/1806.00582v2.\n\n\n12.5.7 Selezione del Client\nConsiderando tutti i fattori che influenzano l’efficacia dell’apprendimento federato, come i dati IID e la comunicazione, la selezione del client è una componente chiave per garantire che un sistema si alleni bene. La selezione dei client sbagliati può distorcere il dataset, con conseguenti dati non IID. Analogamente, la scelta casuale di client con cattive connessioni di rete può rallentare la comunicazione. Pertanto, è necessario considerare diverse caratteristiche chiave quando si seleziona il sottoinsieme corretto di client.\nQuando si selezionano i client, ci sono tre componenti principali da considerare: eterogeneità dei dati, allocazione delle risorse e costo della comunicazione. Possiamo selezionare i client in base alle metriche proposte in precedenza nella sezione non IID per affrontare l’eterogeneità dei dati. Nell’apprendimento federato, tutti i dispositivi possono avere diverse quantità di elaborazione, con il risultato che alcuni sono più inefficienti di altri nell’addestramento. Quando si seleziona un sottoinsieme di client per l’addestramento, si deve considerare un equilibrio tra eterogeneità dei dati e risorse disponibili. In uno scenario ideale, è sempre possibile selezionare il sottoinsieme di client con le maggiori risorse. Tuttavia, questo potrebbe distorcere il set di dati, quindi è necessario trovare un equilibrio. Le differenze di comunicazione aggiungono un altro layer; si desidera evitare di essere bloccati dall’attesa che i dispositivi con connessioni scadenti trasmettano tutti i loro aggiornamenti. Pertanto, è anche necessario considerare la scelta di un sottoinsieme di dispositivi diversi ma ben collegati.\n\n\n12.5.8 L’Esempio di Gboard\nUn esempio primario di un sistema di apprendimento federato distribuito è la tastiera di Google, Gboard, per dispositivi Android. Nell’implementare l’apprendimento federato per la tastiera, Google si è concentrata sull’impiego di tecniche di privacy differenziali per proteggere i dati e l’identità dell’utente. Gboard sfrutta modelli linguistici per diverse funzionalità chiave, come Next Word Prediction (NWP), Smart Compose (SC) e On-The-Fly rescoring (OTF) (Xu et al. 2023), come mostrato in Figura 12.8.\n\nXu, Zheng, Yanxiang Zhang, Galen Andrew, Christopher A. Choquette-Choo, Peter Kairouz, H. Brendan McMahan, Jesse Rosenstock, e Yuanbo Zhang. 2023. «Federated Learning of Gboard Language Models with Differential Privacy». ArXiv preprint abs/2305.18465 (maggio). http://arxiv.org/abs/2305.18465v2.\nNWP anticiperà la parola successiva che l’utente tenta di digitare in base a quella precedente. SC fornisce suggerimenti in linea per velocizzare la digitazione in base a ciascun carattere. OTF riclassificherà le parole successive proposte in base al processo di digitazione attivo. Tutti e tre questi modelli devono essere eseguiti rapidamente sull’edge e l’apprendimento federato può accelerare l’addestramento sui dati degli utenti. Tuttavia, caricare ogni parola digitata da un utente sul cloud per l’addestramento costituirebbe una violazione massiccia della privacy. Pertanto, l’apprendimento federato enfatizza la privacy differenziale, che protegge l’utente consentendo al contempo una migliore esperienza utente.\n\n\n\n\n\n\nFigura 12.8: Funzionalità di Google G Board. Fonte: Zheng et al., (2023).\n\n\n\nPer raggiungere questo obiettivo, Google ha impiegato il suo algoritmo DP-FTRL, che fornisce una garanzia formale che i modelli addestrati non memorizzeranno dati o identità utente specifici. La progettazione del sistema dell’algoritmo è mostrata in Figura 12.9. DP-FTRL, combinato con l’aggregazione sicura, crittografa gli aggiornamenti del modello e fornisce un equilibrio ottimale tra privacy e utilità. Inoltre, il clipping adattivo viene applicato nel processo di aggregazione per limitare l’impatto dei singoli utenti sul modello globale (passaggio 3 in Figura 12.9). Combinando tutte queste tecniche, Google può perfezionare continuamente la sua tastiera preservando al contempo la privacy dell’utente in un modo formalmente dimostrabile.\n\n\n\n\n\n\nFigura 12.9: Privacy Differenziale in G Board. Fonte: Zheng et al., (2023).\n\n\n\n\n\n\n\n\n\nEsercizio 12.2: Apprendimento Federato - Generazione di Testo\n\n\n\n\n\nAvete mai usato quelle tastiere intelligenti che suggeriscono la parola successiva? Con l’apprendimento federato, possiamo renderle ancora migliori senza sacrificare la privacy. In questo Colab, insegneremo a un’IA a prevedere le parole tramite l’addestramento su dati di testo distribuiti su più dispositivi. Prepariamoci a rendere la digitazione ancora più fluida!\n\n\n\n\n\n\n\n\n\n\nEsercizio 12.3: Apprendimento Federato - Classificazione delle Immagini\n\n\n\n\n\nVogliamo addestrare un’IA esperta di immagini senza inviare le proprie foto al cloud? L’apprendimento federato è la risposta! In questo Colab, addestreremo un modello su più dispositivi, ognuno dei quali apprende dalle proprie immagini. La privacy è protetta e il lavoro di squadra fa funzionare il sogno dell’IA!\n\n\n\n\n\n\n12.5.9 Benchmarking Federated Learning: MedPerf\nI dispositivi medici rappresentano uno degli esempi più ricchi di dati edge. Questi dispositivi memorizzano alcuni dei dati utente più personali, offrendo allo stesso tempo progressi significativi nel trattamento personalizzato e una maggiore accuratezza nell’IA medica. Questa combinazione di dati sensibili e potenziale di innovazione rende i dispositivi medici un caso d’uso ideale per l’apprendimento federato.\nUno sviluppo chiave in questo campo è MedPerf, una piattaforma open source progettata per il benchmarking dei modelli utilizzando la valutazione federata (Karargyris et al. 2023). MedPerf va oltre il tradizionale apprendimento federato portando il modello sui dispositivi periferici per testare i dati personalizzati mantenendo la privacy. Questo approccio consente a un comitato di benchmark di valutare vari modelli in scenari reali su dispositivi edge senza compromettere l’anonimato del paziente.\n\nKarargyris, Alexandros, Renato Umeton, Micah J. Sheller, Alejandro Aristizabal, Johnu George, Anna Wuest, Sarthak Pati, et al. 2023. «Federated benchmarking of medical artificial intelligence with MedPerf». Nature Machine Intelligence 5 (7): 799–810. https://doi.org/10.1038/s42256-023-00652-2.\nLa piattaforma MedPerf, descritta in dettaglio in uno studio recente (https://doi.org/10.1038/s42256-023-00652-2), dimostra come le tecniche federate possano essere applicate non solo all’addestramento dei modelli, ma anche alla valutazione e al benchmarking dei modelli. Questo progresso è particolarmente cruciale nel campo medico, dove l’equilibrio tra lo sfruttamento di grandi set di dati per migliorare le prestazioni dell’IA e la protezione della privacy individuale è di fondamentale importanza.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Apprendimento On-Device</span>"
    ]
  },
  {
    "objectID": "contents/core/ondevice_learning/ondevice_learning.it.html#problemi-di-sicurezza",
    "href": "contents/core/ondevice_learning/ondevice_learning.it.html#problemi-di-sicurezza",
    "title": "12  Apprendimento On-Device",
    "section": "12.6 Problemi di Sicurezza",
    "text": "12.6 Problemi di Sicurezza\nL’esecuzione di training e adattamento del modello ML sui dispositivi degli utenti finali introduce anche rischi per la sicurezza che devono essere affrontati. Alcune preoccupazioni chiave per la sicurezza includono:\n\nEsposizione di dati privati: I dati di training potrebbero essere trapelati o rubati dai dispositivi\nAvvelenamento dei dati: Gli avversari possono manipolare i dati di training per degradare le prestazioni del modello\nEstrazione del modello: Degli aggressori potrebbero tentare di rubare i parametri del modello addestrato\nInferenza di appartenenza: I modelli potrebbero rivelare la partecipazione di dati di utenti specifici\nAttacchi di evasione: Input appositamente creati possono causare una classificazione errata\n\nQualsiasi sistema che esegue l’apprendimento sul dispositivo introduce preoccupazioni per la sicurezza, poiché potrebbe esporre vulnerabilità in modelli su larga scala. Numerosi rischi per la sicurezza sono associati a qualsiasi modello ML, ma questi rischi hanno conseguenze specifiche per l’apprendimento on-device. Fortunatamente, esistono metodi per mitigare questi rischi e migliorare le prestazioni reali dell’apprendimento su dispositivo.\n\n12.6.1 Avvelenamento dei Dati\nL’apprendimento automatico on-device introduce sfide uniche per la sicurezza dei dati rispetto all’addestramento tradizionale basato su cloud. In particolare, gli attacchi di avvelenamento dei dati rappresentano una seria minaccia durante l’apprendimento su dispositivo. Gli avversari possono manipolare i dati di training per degradare le prestazioni del modello quando vengono distribuiti.\nEsistono diverse tecniche di attacco di avvelenamento dei dati:\n\nLabel Flipping: Comporta l’applicazione di etichette errate ai campioni. Ad esempio, nella classificazione delle immagini, le foto di gatti possono essere etichettate come cani per confondere il modello. Anche capovolgere il 10% delle etichette può avere conseguenze significative sul modello.\nInserimento dei Dati: Introduce input falsi o distorti nel set di training. Ciò potrebbe includere immagini pixelate, audio rumoroso o testo distorto.\nCorruzione logica: Altera i pattern sottostanti nei dati per confondere il modello. Nell’analisi del “sentiment”, le recensioni altamente negative possono essere contrassegnate come positive tramite questa tecnica. Per questo motivo, recenti sondaggi hanno dimostrato che molte aziende hanno più paura dell’avvelenamento dei dati rispetto ad altre preoccupazioni di ML avversarie.\n\nCiò che rende l’avvelenamento dei dati allarmante è il modo in cui sfrutta la discrepanza tra set di dati curati e dati di training in tempo reale. Consideriamo un set di dati di foto di gatti raccolti da Internet. Nelle settimane successive, quando questi dati addestrano un modello on-device, le nuove foto di gatti sul Web differiscono in modo significativo.\nCon l’avvelenamento dei dati, gli aggressori acquistano domini e caricano contenuti che influenzano una parte dei dati di training. Anche piccole modifiche ai dati hanno un impatto significativo sul comportamento appreso dal modello. Di conseguenza, l’avvelenamento può instillare pregiudizi razzisti, sessisti o altri pregiudizi dannosi se non controllato.\nMicrosoft Tay è un chatbot lanciato da Microsoft nel 2016. È stato progettato per imparare dalle sue interazioni con gli utenti su piattaforme di social media come Twitter. Sfortunatamente, Microsoft Tay è diventato un esempio lampante di avvelenamento dei dati nei modelli di ML. Entro 24 ore dal suo lancio, Microsoft ha dovuto mettere Tay offline perché aveva iniziato a produrre messaggi offensivi e inappropriati, tra cui incitamento all’odio e commenti razzisti. Ciò è accaduto perché alcuni utenti sui social media hanno intenzionalmente fornito a Tay input dannosi e offensivi, da cui il chatbot ha poi imparato e incorporato nelle sue risposte.\nQuesto incidente è un chiaro esempio di avvelenamento dei dati, poiché i malintenzionati hanno intenzionalmente manipolato i dati utilizzati per addestrare il chatbot e modellarne le risposte. L’avvelenamento dei dati ha portato il chatbot ad adottare pregiudizi dannosi e a produrre output che i suoi sviluppatori non avevano previsto. Dimostra come anche piccole quantità di dati creati in modo dannoso possano avere un impatto significativo sul comportamento dei modelli ML e sottolinea l’importanza di implementare solidi meccanismi di filtraggio e convalida dei dati per impedire che tali incidenti si verifichino.\nTali pregiudizi potrebbero avere pericolosi impatti nel mondo reale. La convalida rigorosa dei dati, il rilevamento delle anomalie e il monitoraggio della provenienza dei dati sono misure difensive fondamentali. L’adozione di framework come Five Safes garantisce che i modelli siano addestrati su dati rappresentativi di alta qualità (Desai et al. 2016).\n\nDesai, Tanvi, Felix Ritchie, Richard Welpton, et al. 2016. «Five Safes: Designing data access for research». Economics Working Paper Series 1601: 28.\nL’avvelenamento dei dati è una preoccupazione urgente per l’apprendimento sicuro sul dispositivo poiché i dati dell’endpoint non possono essere facilmente monitorati in tempo reale. Se ai modelli viene consentito di adattarsi da soli, corriamo il rischio che il dispositivo agisca in modo dannoso. Tuttavia, è necessaria una ricerca continua sull’apprendimento automatico avversario per sviluppare soluzioni efficaci per rilevare e mitigare tali attacchi ai dati.\n\n\n12.6.2 Attacchi Avversari\nDurante la fase di addestramento, gli aggressori potrebbero iniettare dati dannosi nel dataset di training, il che può alterare sottilmente il comportamento del modello. Ad esempio, un aggressore potrebbe aggiungere immagini di gatti etichettati come cani a un set di dati utilizzato per addestrare un modello di classificazione delle immagini. Se fatto in modo intelligente, l’accuratezza del modello potrebbe non diminuire in modo significativo e l’attacco potrebbe essere notato. Il modello classificherebbe quindi erroneamente alcuni gatti come cani, il che potrebbe avere conseguenze a seconda dell’applicazione.\nIn un sistema di telecamere di sicurezza embedded, ad esempio, ciò potrebbe consentire a un intruso di evitare il rilevamento indossando uno specifico pattern che il modello è stato ingannato a classificare come non minaccioso.\nDurante la fase di inferenza, gli aggressori possono utilizzare esempi avversari per ingannare il modello. Gli esempi avversari sono input che sono stati leggermente alterati in un modo da far sì che il modello faccia previsioni errate. Ad esempio, un aggressore potrebbe aggiungere una piccola quantità di rumore a un’immagine in un modo che un sistema di riconoscimento facciale identifichi erroneamente una persona. Questi attacchi possono essere particolarmente preoccupanti nelle applicazioni in cui è in gioco la sicurezza, come i veicoli autonomi. Un esempio concreto di ciò è quando i ricercatori sono riusciti a far sì che un sistema di riconoscimento della segnaletica stradale classificasse erroneamente un segnale di stop come un segnale di limite di velocità. Questo tipo di classificazione errata potrebbe causare incidenti se si verificasse in un sistema di guida autonoma nel mondo reale.\nPer mitigare questi rischi, possono essere impiegate diverse difese:\n\nValidazione e Sanificazione dei Dati: Prima di incorporare nuovi dati nel dataset di addestramento, questi devono essere convalidati e sanificati a fondo per garantire che non siano dannosi.\nAddestramento Avversario: Il modello può essere addestrato su esempi avversari per renderlo più robusto a questi tipi di attacchi.\nValidazione degli Input: Durante l’inferenza, gli input devono essere convalidati per garantire che non siano stati manipolati per creare esempi avversari.\nAudit e Monitoraggio Regolari: L’audit e il monitoraggio regolari del comportamento del modello possono aiutare a rilevare e mitigare gli attacchi avversari. Tuttavia, è più facile a dirsi che a farsi nel contesto di piccoli sistemi ML. Spesso è difficile monitorare i sistemi ML embedded all’endpoint a causa delle limitazioni della larghezza di banda della comunicazione, di cui parleremo nel capitolo MLOps.\n\nComprendendo i potenziali rischi e implementando queste difese, possiamo contribuire a proteggere il training on-device all’endpoint/edge e mitigare l’impatto degli attacchi avversari. La maggior parte delle persone confonde facilmente l’avvelenamento dei dati e gli attacchi avversari. Quindi Tabella 12.2 confronta l’avvelenamento dei dati e gli attacchi avversari:\n\n\n\nTabella 12.2: Confronto tra avvelenamento dei dati e attacchi avversari.\n\n\n\n\n\n\n\n\n\n\nAspetto\nAvvelenamento dei dati\nAttacchi avversari\n\n\n\n\nTempistica\nFase di addestramento\nFase di inferenza\n\n\nTarget\nDati di addestramento\nDati di input\n\n\nObiettivo\nInfluenza negativamente le prestazioni del modello\nCausa previsioni errate\n\n\nMetodo\nInserire esempi dannosi nei dati di training, spesso con etichette errate\nAggiungere rumore attentamente elaborato ai dati di input\n\n\nEsempio\nAggiungere immagini di gatti etichettati come cani a un set di dati utilizzato per addestrare un modello di classificazione delle immagini\nAggiungere una piccola quantità di rumore a un’immagine in modo che un sistema di riconoscimento facciale identifichi erroneamente una persona\n\n\nEffetti Potenziali\nIl modello apprende pattern errati e fa previsioni errate\nPrevisioni errate immediate e potenzialmente pericolose\n\n\nApplicazioni Interessate\nQualsiasi modello ML\nVeicoli autonomi, sistemi di sicurezza, ecc.\n\n\n\n\n\n\n\n\n12.6.3 Inversione del Modello\nGli attacchi di inversione del modello rappresentano una minaccia per la privacy dei modelli di machine learning su dispositivo addestrati su dati utente sensibili (Nguyen et al. 2023). Comprendere questo vettore di attacco e le strategie di mitigazione saranno importanti per creare un’intelligenza artificiale su dispositivo sicura ed etica. Ad esempio, si immagini un’app per iPhone che utilizza l’apprendimento su dispositivo per categorizzare le foto in gruppi come “spiaggia”, “cibo” o “selfie” per una ricerca più semplice.\n\nNguyen, Ngoc-Bao, Keshigeyan Chandrasegaran, Milad Abdollahzadeh, e Ngai-Man Cheung. 2023. «Re-Thinking Model Inversion Attacks Against Deep Neural Networks». In 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 16384–93. IEEE. https://doi.org/10.1109/cvpr52729.2023.01572.\nIl modello su dispositivo potrebbe essere addestrato da Apple su un set di dati di foto iCloud di utenti consenzienti. Un aggressore malintenzionato potrebbe tentare di estrarre parti di quelle foto di addestramento iCloud originali utilizzando l’inversione del modello. In particolare, l’aggressore inserisce input sintetici creati ad arte nel classificatore di foto su dispositivo. Modificando gli input sintetici e osservando come il modello li categorizza, possono perfezionare gli input fino a ricostruire copie dei dati di training originali, come una foto di una spiaggia dall’iCloud di un utente. Ora, l’aggressore ha violato la privacy di quell’utente ottenendo una delle sue foto senza consenso. Questo dimostra perché l’inversione del modello è pericolosa: può potenzialmente far trapelare dati di training altamente sensibili.\nLe foto sono un tipo di dati particolarmente rischioso perché spesso contengono persone identificabili, informazioni sulla posizione e momenti privati. Tuttavia, la stessa metodologia di attacco potrebbe essere applicata ad altri dati personali, come registrazioni audio, messaggi di testo o dati sanitari degli utenti.\nPer difendersi dall’inversione del modello, sarebbe necessario prendere precauzioni come l’aggiunta di rumore agli output del modello o l’utilizzo di tecniche di apprendimento automatico che preservano la privacy come l’apprendimento federato per addestrare il modello sul dispositivo. L’obiettivo è impedire agli aggressori di ricostruire i dati di training originali.\n\n\n12.6.4 Problemi di Sicurezza dell’Apprendimento On-Device\nSebbene l’avvelenamento dei dati e gli attacchi avversari siano preoccupazioni comuni per i modelli ML in generale, l’apprendimento su dispositivo introduce rischi di sicurezza unici. Quando vengono pubblicate varianti su dispositivo di modelli su larga scala, gli avversari possono sfruttare questi modelli più piccoli per attaccare le loro controparti più grandi. La ricerca ha dimostrato che man mano che i modelli su dispositivo e i modelli su scala reale diventano più simili, la vulnerabilità dei modelli originali su larga scala aumenta in modo significativo. Ad esempio, le valutazioni su 19 reti neurali profonde (DNN) hanno rivelato che lo sfruttamento dei modelli su dispositivo potrebbe aumentare la vulnerabilità dei modelli originali su larga scala di fino a 100 volte.\nEsistono tre tipi principali di rischi per la sicurezza specifici dell’apprendimento on-device:\n\nAttacchi Basati sul Trasferimento: Questi attacchi sfruttano la proprietà di trasferibilità tra un modello surrogato (un’approssimazione del modello di destinazione, simile a un modello su dispositivo) e un modello di destinazione remoto (il modello originale su scala reale). Gli aggressori generano esempi avversari utilizzando il modello surrogato, che può quindi essere utilizzato per ingannare il modello di destinazione. Ad esempio, si immagini un modello on-device progettato per identificare le e-mail di spam. Un aggressore potrebbe usare questo modello per generare un’e-mail di spam che non viene rilevata dal sistema di filtraggio più grande e completo.\nAttacchi Basati sull’Ottimizzazione: Questi attacchi generano esempi avversari per attacchi basati sul trasferimento usando una qualche forma di funzione obiettivo e modificano iterativamente gli input per ottenere il risultato desiderato. Gli attacchi di stima del gradiente, ad esempio, approssimano il gradiente del modello usando output di query (come punteggi di confidenza softmax), mentre gli attacchi senza gradiente usano la decisione finale del modello (la classe prevista) per approssimare il gradiente, sebbene richiedano molte più query.\nAttacchi di Query con Priorità di Trasferimento: Questi attacchi combinano elementi di attacchi basati sul trasferimento e basati sull’ottimizzazione. Eseguono il reverse engineering dei modelli sul dispositivo per fungere da surrogati del modello completo di destinazione. In altre parole, gli aggressori usano il modello sul dispositivo più piccolo per capire come funziona il modello più grande e quindi usano questa conoscenza per attaccare il modello completo.\n\nGrazie alla comprensione di questi rischi specifici associati all’apprendimento on-device, possiamo sviluppare protocolli di sicurezza più solidi per proteggere sia i modelli on-device che quelli su scala reale da potenziali attacchi.\n\n\n12.6.5 Attenuazione dei Rischi dell’Apprendimento On-Device\nSi possono impiegare vari metodi per mitigare i numerosi rischi per la sicurezza associati all’apprendimento on-device. Questi metodi possono essere specifici per il tipo di attacco o fungere da strumento generale per rafforzare la sicurezza.\nUna strategia per ridurre i rischi per la sicurezza è quella di ridurre la somiglianza tra modelli on-device e modelli su scala reale, riducendo così la trasferibilità fino al 90%. Questo metodo, noto come similarity-unpairing, affronta il problema che si verifica quando gli avversari sfruttano la somiglianza del gradiente di input tra i due modelli. Ottimizzando il modello su scala reale per creare una nuova versione con accuratezza simile ma gradienti di input diversi, possiamo costruire il modello on-device quantizzando questo modello su scala reale aggiornato. Questa disassociazione riduce la vulnerabilità dei modelli su dispositivo limitando l’esposizione del modello su scala reale originale. È importante notare che l’ordine di ottimizzazione e quantizzazione può essere variato pur ottenendo la mitigazione del rischio (Hong, Carlini, e Kurakin 2023).\nPer contrastare l’avvelenamento dei dati, è fondamentale reperire set di dati da fornitori affidabili e fidati.\nPer combattere gli attacchi avversari, si possono impiegare diverse strategie. Un approccio proattivo prevede la generazione di esempi avversari e la loro incorporazione nel set di dati di training del modello, rafforzando così il modello contro tali attacchi. Strumenti come CleverHans, una libreria di training open source, sono fondamentali per creare esempi avversari. La “Defense distillation” [distillazione della difesa] è un’altra strategia efficace, in cui il modello sul dispositivo genera probabilità di classificazioni diverse anziché decisioni definitive (Hong, Carlini, e Kurakin 2023), rendendo più difficile per gli esempi avversari sfruttare il modello.\n\nHong, Sanghyun, Nicholas Carlini, e Alexey Kurakin. 2023. «Publishing Efficient On-device Models Increases Adversarial Vulnerability». In 2023 IEEE Conference on Secure and Trustworthy Machine Learning (SaTML), abs 1603 5279:271–90. IEEE; IEEE. https://doi.org/10.1109/satml54575.2023.00026.\nIl furto di proprietà intellettuale è un altro problema significativo quando si distribuiscono modelli on-device. Il furto di proprietà intellettuale è un problema quando si distribuiscono modelli on-device, poiché gli avversari potrebbero tentare di sottoporre a reverse engineering il modello per rubare la tecnologia sottostante. Per proteggersi dal furto di proprietà intellettuale, l’eseguibile binario del modello addestrato dovrebbe essere archiviato su un’unità microcontrollore con software crittografato e interfacce fisiche protette del chip. Inoltre, il set di dati finale utilizzato per l’addestramento del modello dovrebbe essere mantenuto privato.\nInoltre, i modelli on-device utilizzano spesso set di dati noti o open source, come Visual Wake Words di MobileNet. Pertanto, è importante mantenere la privacy del set di dati finale utilizzato per l’addestramento del modello. Inoltre, proteggere il processo di “data augmentation” e incorporare casi d’uso specifici può ridurre al minimo il rischio di reverse engineering di un modello on-device.\nInfine, l’Adversarial Threat Landscape for Artificial Intelligence Systems (ATLAS) funge da prezioso strumento matriciale che aiuta a valutare il profilo di rischio dei modelli su dispositivo, consentendo agli sviluppatori di identificare e mitigare i potenziali rischi in modo proattivo.\n\n\n12.6.6 Protezione dei Dati di Training\nEsistono vari modi per proteggere i dati di training sul dispositivo. Ogni concetto è molto profondo e potrebbe valere una lezione a sé stante. Quindi, qui, faremo un breve accenno a quei concetti in modo che si sappia cosa approfondire.\n\nCrittografia\nLa crittografia funge da prima linea di difesa per i dati di training. Ciò comporta l’implementazione della crittografia end-to-end per l’archiviazione locale su dispositivi e canali di comunicazione per impedire l’accesso non autorizzato ai dati di training grezzi. Ambienti di esecuzione affidabili, come Intel SGX e ARM TrustZone, sono essenziali per facilitare il training sicuro su dati crittografati.\nInoltre, quando si aggregano aggiornamenti da più dispositivi, è possibile impiegare protocolli di elaborazione “multi-party” sicuri per migliorare la sicurezza (Kairouz, Oh, e Viswanath 2015); un’applicazione pratica di ciò è nell’apprendimento collaborativo on-device, in cui è possibile implementare l’aggregazione crittografica che preserva la privacy degli aggiornamenti del modello utente. Questa tecnica nasconde efficacemente i dati dei singoli utenti anche durante la fase di aggregazione.\n\nKairouz, Peter, Sewoong Oh, e Pramod Viswanath. 2015. «Secure Multi-party Differential Privacy.» In Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada, a cura di Corinna Cortes, Neil D. Lawrence, Daniel D. Lee, Masashi Sugiyama, e Roman Garnett, 2008–16. https://proceedings.neurips.cc/paper/2015/hash/a01610228fe998f515a72dd730294d87-Abstract.html.\n\n\nPrivacy Differenziale\nLa privacy differenziale è un’altra strategia cruciale per proteggere i dati di training. Iniettando rumore statistico calibrato nei dati, possiamo mascherare i singoli record estraendo comunque preziosi pattern di popolazione (Dwork e Roth 2013). Anche la gestione del budget per la privacy su più iterazioni di training e la riduzione del rumore man mano che il modello converge sono essenziali (Abadi et al. 2016). Possono essere impiegati metodi come la privacy differenziale formalmente dimostrabile, che può includere l’aggiunta di rumore di Laplace o gaussiano scalato alla sensibilità del set di dati.\n\nDwork, Cynthia, e Aaron Roth. 2013. «The Algorithmic Foundations of Differential Privacy». Foundations and Trends® in Theoretical Computer Science 9 (3-4): 211–407. https://doi.org/10.1561/0400000042.\n\nAbadi, Martin, Andy Chu, Ian Goodfellow, H. Brendan McMahan, Ilya Mironov, Kunal Talwar, e Li Zhang. 2016. «Deep Learning with Differential Privacy». In Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security, 308–18. CCS ’16. New York, NY, USA: ACM. https://doi.org/10.1145/2976749.2978318.\n\n\nRilevamento delle Anomalie\nIl rilevamento delle anomalie svolge un ruolo importante nell’identificazione e nell’attenuazione di potenziali attacchi di avvelenamento dei dati. Ciò può essere ottenuto tramite analisi statistiche come la “Principal Component Analysis (PCA)” [analisi delle componenti principali] e il clustering, che aiutano a rilevare deviazioni nei dati di training aggregati. I metodi di serie temporali come i grafici Cumulative Sum (CUSUM) sono utili per identificare spostamenti indicativi di potenziale avvelenamento. Anche il confronto delle distribuzioni dei dati correnti con distribuzioni di dati pulite precedentemente visualizzate può aiutare a segnalare anomalie. Inoltre, i batch sospetti di essere avvelenati dovrebbero essere rimossi dal processo di aggregazione degli aggiornamenti di training. Ad esempio, è possibile condurre controlli a campione su sottoinsiemi di immagini di training sui dispositivi utilizzando hash photoDNA per identificare input avvelenati.\n\n\nValidazione dei Dati di Input\nInfine, la convalida dei dati di input è essenziale per garantire l’integrità e la validità dei dati di input prima che vengano immessi nel modello di training, proteggendo così dai payload avversari. Misure di similarità, come la distanza del coseno, possono essere impiegate per catturare input che si discostano in modo significativo dalla distribuzione prevista. Gli input sospetti che potrebbero contenere payload avversari devono essere messi in quarantena e sanificati. Inoltre, l’accesso del parser ai dati di training deve essere limitato solo ai percorsi di codice convalidati. Sfruttare le funzionalità di sicurezza hardware, come ARM Pointer Authentication, può impedire la corruzione della memoria (ARM Limited, 2023). Un esempio di ciò è l’implementazione di controlli di integrità degli input sui dati di training audio utilizzati dagli smart speaker prima dell’elaborazione da parte del modello di riconoscimento vocale (Z. Chen e Xu 2023).\n\nChen, Zhiyong, e Shugong Xu. 2023. «Learning domain-heterogeneous speaker recognition systems with personalized continual federated learning». EURASIP Journal on Audio, Speech, and Music Processing 2023 (1): 33. https://doi.org/10.1186/s13636-023-00299-2.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Apprendimento On-Device</span>"
    ]
  },
  {
    "objectID": "contents/core/ondevice_learning/ondevice_learning.it.html#framework-di-training-on-device",
    "href": "contents/core/ondevice_learning/ondevice_learning.it.html#framework-di-training-on-device",
    "title": "12  Apprendimento On-Device",
    "section": "12.7 Framework di Training On-Device",
    "text": "12.7 Framework di Training On-Device\nFramework di inferenza embedded come TF-Lite Micro (David et al. 2021), TVM (T. Chen et al. 2018) e MCUNet (Lin et al. 2020) forniscono un runtime snello per l’esecuzione di modelli di reti neurali su microcontrollori e altri dispositivi con risorse limitate. Tuttavia, non supportano l’addestramento on-device. L’addestramento richiede un proprio set di strumenti specializzati a causa dell’impatto della quantizzazione sul calcolo del gradiente e dell’ingombro di memoria della backpropagation (Lin et al. 2022).\n\nDavid, Robert, Jared Duke, Advait Jain, Vijay Janapa Reddi, Nat Jeffries, Jian Li, Nick Kreeger, et al. 2021. «Tensorflow lite micro: Embedded machine learning for tinyml systems». Proceedings of Machine Learning and Systems 3: 800–811.\n\nChen, Tianqi, Thierry Moreau, Ziheng Jiang, Lianmin Zheng, Eddie Yan, Haichen Shen, Meghan Cowan, et al. 2018. «TVM: An automated End-to-End optimizing compiler for deep learning». In 13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18), 578–94.\n\nLin, Ji, Wei-Ming Chen, Yujun Lin, John Cohn, Chuang Gan, e Song Han. 2020. «MCUNet: Tiny Deep Learning on IoT Devices». In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, a cura di Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, e Hsuan-Tien Lin. https://proceedings.neurips.cc/paper/2020/hash/86c51678350f656dcc7f490a43946ee5-Abstract.html.\n\nLin, Ji, Ligeng Zhu, Wei-Ming Chen, Wei-Chen Wang, Chuang Gan, e Song Han. 2022. «On-device training under 256kb memory». Adv. Neur. In. 35: 22941–54.\nNegli ultimi anni, hanno iniziato a emergere una manciata di strumenti e framework che consentono l’addestramento sul dispositivo. Tra questi Tiny Training Engine (Lin et al. 2022), TinyTL (Cai et al. 2020) e TinyTrain (Kwon et al. 2023).\n\n12.7.1 Tiny Training Engine\nTiny Training Engine (TTE) utilizza diverse tecniche per ottimizzare l’utilizzo della memoria e velocizzare il processo di training. Una panoramica del flusso di lavoro TTE è mostrata in Figura 12.10. Innanzitutto, TTE scarica la differenziazione automatica in fase di compilazione anziché in fase di runtime, riducendo significativamente il sovraccarico durante il training. In secondo luogo, TTE esegue l’ottimizzazione del grafo come la potatura e gli aggiornamenti sparsi per ridurre i requisiti di memoria e accelerare i calcoli.\n\n\n\n\n\n\nFigura 12.10: Flusso di lavoro di TTE.\n\n\n\nIn particolare, TTE segue quattro passaggi principali:\n\nDurante la fase di compilazione, TTE traccia il grafo di propagazione “forward” e deriva il grafo “backward” corrispondente per la backpropagation. Ciò consente alla differenziazione di avvenire in fase di compilazione anziché in fase di esecuzione.\nTTE elimina tutti i nodi che rappresentano pesi congelati dal grafo backward. I pesi congelati sono pesi che non vengono aggiornati durante l’addestramento per ridurre l’impatto di determinati neuroni. La potatura dei loro nodi consente di risparmiare memoria.\nTTE riordina gli operatori di discesa del gradiente per intercalarli con i calcoli del passaggio del backward. Questa pianificazione riduce al minimo le “impronte” [occupazione] di memoria.\nTTE utilizza la generazione di codice per compilare i grafi “forward” e “backward” ottimizzati, che vengono poi distribuiti per l’addestramento on-device.\n\n\n\n12.7.2 Tiny Transfer Learning\nTiny Transfer Learning (TinyTL) consente un training efficiente in termini di memoria sul dispositivo tramite una tecnica chiamata congelamento dei pesi. Durante il training, gran parte del collo di bottiglia della memoria deriva dall’archiviazione delle attivazioni intermedie e dall’aggiornamento dei pesi nella rete neurale.\nPer ridurre questo sovraccarico di memoria, TinyTL congela la maggior parte dei pesi in modo che non debbano essere aggiornati durante il training. Ciò elimina la necessità di archiviare le attivazioni intermedie per le parti congelate della rete. TinyTL ottimizza solo i termini di bias, che sono molto più piccoli dei pesi. Una panoramica del flusso di lavoro TinyTL è mostrata in Figura 12.11.\n\n\n\n\n\n\nFigura 12.11: Flusso di lavoro di TinyTL. In (a), l’apprendimento per trasferimento convenzionale ottimizza sia i pesi che i bias, richiedendo una grande quantità di memoria (mostrata in blu) per le mappe di attivazione durante la retropropagazione. In (b), TinyTL riduce le esigenze di memoria fissando i pesi e ottimizzando solo i bias, consentendo l’apprendimento per trasferimento su dispositivi più piccoli. Infine, in (c), TinyTL aggiunge un componente di apprendimento residuo “lite” per compensare i pesi fissi, utilizzando convoluzioni di gruppo efficienti ed evitando colli di bottiglia pesanti in termini di memoria, ottenendo un’elevata efficienza con una memoria minima. Fonte: Cai et al. (2020).)\n\n\nCai, Han, Chuang Gan, Ligeng Zhu, e Song Han 0003. 2020. «TinyTL: Reduce Memory, Not Parameters for Efficient On-Device Learning.» In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, a cura di Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, e Hsuan-Tien Lin. https://proceedings.neurips.cc/paper/2020/hash/81f7acabd411274fcf65ce2070ed568a-Abstract.html.\n\n\nI pesi di congelamento si applicano a layer completamente connessi, nonché a layer di normalizzazione e convoluzionali. Tuttavia, solo l’adattamento dei bias limita la capacità del modello di apprendere e adattarsi a nuovi dati.\nPer aumentare l’adattabilità senza molta memoria aggiuntiva, TinyTL utilizza un piccolo modello di apprendimento residuo. Questo affina le mappe delle feature intermedie per produrre output migliori, anche con pesi fissi. Il modello residuo introduce un overhead minimo, inferiore al 3,8% in più rispetto al modello di base.\nCongelando la maggior parte dei pesi, TinyTL riduce significativamente l’utilizzo della memoria durante l’addestramento on-device. Il modello residuo consente quindi di adattarsi e apprendere in modo efficace per l’attività. L’approccio combinato fornisce un addestramento on-device efficiente in termini di memoria con un impatto minimo sulla precisione del modello.\n\n\n12.7.3 Tiny Train\nTinyTrain riduce significativamente il tempo necessario per l’addestramento sul dispositivo aggiornando selettivamente solo determinate parti del modello. Ciò avviene utilizzando una tecnica chiamata aggiornamento sparso adattivo all’attività, come mostrato in Figura 12.12.\n\n\n\n\n\n\nFigura 12.12: Flusso di lavoro di TinyTrain. Fonte: Kwon et al. (2023).\n\n\nKwon, Young D., Rui Li, Stylianos I. Venieris, Jagmohan Chauhan, Nicholas D. Lane, e Cecilia Mascolo. 2023. «TinyTrain: Resource-Aware Task-Adaptive Sparse Training of DNNs at the Data-Scarce Edge». ArXiv preprint abs/2307.09988 (luglio). http://arxiv.org/abs/2307.09988v2.\n\n\nIn base ai dati utente, alla memoria e al calcolo disponibili sul dispositivo, TinyTrain sceglie dinamicamente quali layer della rete neurale aggiornare durante l’addestramento. Questa selezione di layer è ottimizzata per ridurre l’utilizzo di calcolo e memoria mantenendo un’elevata accuratezza.\nPiù specificamente, TinyTrain esegue prima il pre-addestramento offline del modello. Durante il pre-addestramento, non solo addestra il modello sui dati dell’attività, ma anche il meta-addestramento del modello. Meta-addestramento significa addestrare il modello sui metadati relativi al processo di addestramento stesso. Questo meta-addestramento migliora la capacità del modello di adattarsi in modo accurato anche quando sono disponibili dati limitati per l’attività target.\nPoi, durante la fase di adattamento online, quando il modello viene personalizzato sul dispositivo, TinyTrain esegue aggiornamenti adattivi sparsi all’attività. Utilizzando i criteri relativi alle capacità del dispositivo, seleziona solo determinati layer da aggiornare tramite backpropagation. I layer vengono scelti per bilanciare accuratezza, utilizzo della memoria e tempo di elaborazione.\nAggiornando in modo sparso i layer su misura per il dispositivo e l’attività, TinyTrain riduce significativamente il tempo di addestramento sul dispositivo e l’utilizzo delle risorse. Il meta-training offline migliora anche l’accuratezza quando si adatta a dati limitati. Insieme, questi metodi consentono un training on-device rapido, efficiente e accurato.\n\n\n12.7.4 Confronto\nTabella 12.3 riassume le principali somiglianze e differenze tra i diversi framework.\n\n\n\nTabella 12.3: Confronto di framework per l’ottimizzazione del training on-device.\n\n\n\n\n\n\n\n\n\n\nFramework\nSomiglianze\nDifferenze\n\n\n\n\nTiny Training Engine\n\nAddestramento sul dispositivo\nOttimizza memoria e calcolo\nSfrutta potatura, sparsità, ecc.\n\n\nTraccia grafi forward & backward\nElimina i pesi congelati\nInterlaccia backprop e gradienti\nGenerazione di codice\n\n\n\nTinyTL\n\nAddestramento sul dispositivo\nOttimizza memoria e calcolo\nSfrutta congelamento, sparsità, ecc.\n\n\nCongela la maggior parte dei pesi\nAdatta solo i bias\nUtilizza il modello residuo\n\n\n\nTinyTrain\n\nAddestramento sul dispositivo\nOttimizza memoria e calcolo\nSfrutta sparsità, ecc.\n\n\nMeta-addestramento nel pre-addestramento\nAggiornamento sparse adattivo alle attività\nAggiornamento selettivo dei layer",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Apprendimento On-Device</span>"
    ]
  },
  {
    "objectID": "contents/core/ondevice_learning/ondevice_learning.it.html#conclusione",
    "href": "contents/core/ondevice_learning/ondevice_learning.it.html#conclusione",
    "title": "12  Apprendimento On-Device",
    "section": "12.8 Conclusione",
    "text": "12.8 Conclusione\nIl concetto di apprendimento on-device [su dispositivo] è sempre più importante per aumentare l’usabilità e la scalabilità di TinyML. Questo capitolo ha esplorato le complessità dell’apprendimento on-device, esplorandone vantaggi e limiti, strategie di adattamento, algoritmi e tecniche chiave correlate, implicazioni di sicurezza e framework di training on-device esistenti ed emergenti.\nL’apprendimento su dispositivo è, senza dubbio, un paradigma rivoluzionario che porta con sé numerosi vantaggi per le distribuzioni ML embedded ed edge. Eseguendo il training direttamente sui dispositivi endpoint, si elimina la necessità di una connettività cloud continua, rendendolo particolarmente adatto per applicazioni IoT ed edge computing. Presenta vantaggi quali maggiore privacy, facilità di conformità ed efficienza delle risorse. Allo stesso tempo, l’apprendimento su on-device deve affrontare limitazioni legate a vincoli hardware, dimensioni dei dati limitate e ridotta accuratezza e generalizzazione del modello.\nMeccanismi quali la ridotta complessità del modello, tecniche di ottimizzazione e compressione dei dati e metodi di apprendimento correlati quali apprendimento tramite trasferimento e apprendimento federato consentono ai modelli di adattarsi per apprendere ed evolversi in base a vincoli di risorse, fungendo così da fondamento per un efficace ML sui dispositivi edge.\nLe problematiche critiche di sicurezza nell’apprendimento su dispositivo evidenziate in questo capitolo, che vanno dall’avvelenamento dei dati e dagli attacchi avversari ai rischi specifici introdotti dall’apprendimento on-device, devono essere affrontate in carichi di lavoro reali affinché l’apprendimento su dispositivo sia un paradigma praticabile. Strategie di mitigazione efficaci, quali convalida dei dati, crittografia, privacy differenziale, rilevamento delle anomalie e convalida dei dati di input, sono fondamentali per salvaguardare i sistemi di apprendimento on-device da queste minacce.\nL’emergere di framework di training specializzati on-device, come Tiny Training Engine, Tiny Transfer Learning e Tiny Train, offre strumenti pratici che consentono un training efficiente sui dispositivi. Questi framework impiegano varie tecniche per ottimizzare l’utilizzo della memoria, ridurre il sovraccarico computazionale e semplificare il processo di training on-device.\nIn conclusione, l’apprendimento on-device è in prima linea in TinyML, promettendo un futuro in cui i modelli possono acquisire autonomamente conoscenze e adattarsi ad ambienti mutevoli su dispositivi edge. L’applicazione dell’apprendimento on-device ha il potenziale per rivoluzionare vari ambiti, tra cui sanità, IoT industriale e città intelligenti. Tuttavia, il potenziale trasformativo dell’apprendimento on-device deve essere bilanciato con misure di sicurezza robuste per proteggere da violazioni dei dati e minacce avversarie. L’adozione di framework di training on-device innovativi e l’implementazione di protocolli di sicurezza rigorosi sono passaggi chiave per sbloccare il pieno potenziale dell’apprendimento su dispositivo. Man mano che questa tecnologia continua a evolversi, promette di rendere i nostri dispositivi più intelligenti, più reattivi e meglio integrati nella nostra vita quotidiana.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Apprendimento On-Device</span>"
    ]
  },
  {
    "objectID": "contents/core/ondevice_learning/ondevice_learning.it.html#sec-on-device-learning-resource",
    "href": "contents/core/ondevice_learning/ondevice_learning.it.html#sec-on-device-learning-resource",
    "title": "12  Apprendimento On-Device",
    "section": "12.9 Risorse",
    "text": "12.9 Risorse\nEcco un elenco curato di risorse per supportare studenti e insegnanti nei loro percorsi di apprendimento e insegnamento. Lavoriamo continuamente per espandere questa raccolta e presto aggiungeremo nuovi esercizi.\n\n\n\n\n\n\nSlide\n\n\n\n\n\nQueste slide sono uno strumento prezioso per gli insegnanti per tenere lezioni e per gli studenti per rivedere il materiale al proprio ritmo. Incoraggiamo sia gli studenti che gli insegnanti a sfruttare queste slide per migliorare la loro comprensione e facilitare un trasferimento efficace delle conoscenze.\n\nIntro to TensorFlow Lite (TFLite).\nTFLite Optimization and Quantization.\nTFLite Quantization-Aware Training.\nTrasferimento dell’Apprendimento:\n\nTransfer Learning: with Visual Wake Words example.\nOn-device Training and Transfer Learning.\n\nAddestramento Distribuito:\n\nDistributed Training.\nDistributed Training.\n\nMonitoraggio Continuo:\n\nContinuous Evaluation Challenges for TinyML.\nFederated Learning Challenges.\nContinuous Monitoring with Federated ML.\nContinuous Monitoring Impact on MLOps.\n\n\n\n\n\n\n\n\n\n\n\nVideo\n\n\n\n\n\n\nVideo 12.1\nVideo 12.2\n\n\n\n\n\n\n\n\n\n\nEsercizi\n\n\n\n\n\nPer rafforzare i concetti trattati in questo capitolo, abbiamo curato una serie di esercizi che sfidano gli studenti ad applicare le proprie conoscenze e ad approfondire la propria comprensione.\n\nEsercizio 12.1\nEsercizio 12.2\nEsercizio 12.3",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Apprendimento On-Device</span>"
    ]
  },
  {
    "objectID": "contents/core/ops/ops.it.html",
    "href": "contents/core/ops/ops.it.html",
    "title": "13  Operazioni di ML",
    "section": "",
    "text": "13.1 Panoramica\nMachine Learning Operations (MLOps) è un approccio sistematico che combina machine learning (ML), data science e ingegneria del software per automatizzare il ciclo di vita end-to-end di ML. Ciò include tutto, dalla preparazione dei dati e dal training del modello alla distribuzione e alla manutenzione. MLOps garantisce che i modelli ML siano sviluppati, distribuiti e mantenuti in modo efficiente ed efficace.\nCominciamo prendendo un caso di esempio generale (ad esempio, ML non edge). Prendiamo in considerazione un’azienda di “ride sharing” che desidera distribuire un modello di machine learning per prevedere la domanda dei passeggeri in tempo reale. Il team di data science impiega mesi per sviluppare un modello, ma quando è il momento di distribuirlo, si rende conto che deve essere compatibile con l’ambiente di produzione del team di ingegneria. La distribuzione del modello richiede la ricostruzione da zero, il che comporta settimane di lavoro aggiuntivo. È qui che entra in gioco MLOps.\nCon MLOps, protocolli e strumenti, il modello sviluppato dal team di data science può essere distribuito e integrato senza problemi nell’ambiente di produzione. In sostanza, MLOps elimina gli attriti durante lo sviluppo, la distribuzione e la manutenzione dei sistemi ML. Migliora la collaborazione tra i team tramite flussi di lavoro e interfacce definiti. MLOps accelera anche la velocità di iterazione consentendo la distribuzione continua per i modelli ML.\nPer l’azienda di ride sharing, implementare MLOps significa che il loro modello di previsione della domanda può essere frequentemente riqualificato e distribuito in base ai nuovi dati in arrivo. Ciò mantiene il modello accurato nonostante il cambiamento del comportamento del passeggero. MLOps consente inoltre all’azienda di sperimentare nuove tecniche di modellazione poiché i modelli possono essere rapidamente testati e aggiornati.\nAltri vantaggi di MLOps includono il monitoraggio avanzato della discendenza del modello, la riproducibilità e l’auditing. La catalogazione dei flussi di lavoro ML e la standardizzazione degli artefatti, come il logging delle versioni del modello, il monitoraggio della discendenza dei dati e il confezionamento di modelli e parametri, consente una visione più approfondita della provenienza del modello. La standardizzazione di questi artefatti facilita la tracciabilità di un modello fino alle sue origini, la replica del processo di sviluppo del modello e l’esame di come una versione del modello è cambiata nel tempo. Ciò facilita anche la conformità alle normative, che è particolarmente critica in settori regolamentati come sanità e finanza, dove è importante essere in grado di verificare e spiegare i modelli.\nLe principali organizzazioni adottano MLOps per aumentare la produttività, aumentare la collaborazione e accelerare i risultati ML. Fornisce i framework, gli strumenti e le best practice per gestire efficacemente i sistemi ML durante il loro ciclo di vita. Ciò si traduce in modelli più performanti, tempi di realizzazione più rapidi e un vantaggio competitivo duraturo. Mentre esploriamo ulteriormente MLOps, si consideri come l’implementazione di queste pratiche può aiutare ad affrontare le sfide ML embedded oggi e in futuro.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Operazioni di ML</span>"
    ]
  },
  {
    "objectID": "contents/core/ops/ops.it.html#contesto-storico",
    "href": "contents/core/ops/ops.it.html#contesto-storico",
    "title": "13  Operazioni di ML",
    "section": "13.2 Contesto Storico",
    "text": "13.2 Contesto Storico\nMLOps affonda le sue radici in DevOps, un insieme di pratiche che combinano sviluppo software (Dev) e operazioni IT (Ops) per accorciare il ciclo di vita dello sviluppo e fornire una distribuzione “continua” di software di alta qualità. I parallelismi tra MLOps e DevOps sono evidenti nella loro attenzione all’automazione, alla collaborazione e al miglioramento continuo. In entrambi i casi, l’obiettivo è quello di abbattere i “silos” tra i diversi team (sviluppatori, operazioni e, nel caso di MLOps, data scientist e ingegneri ML) e creare un processo più snello ed efficiente. È utile comprendere meglio la storia di questa evoluzione per comprendere MLOps nel contesto dei sistemi tradizionali.\n\n13.2.1 DevOps\nIl termine “DevOps” è stato coniato per la prima volta nel 2009 da Patrick Debois, un consulente e professionista Agile. Debois ha organizzato la prima conferenza DevOpsDays a Ghent, in Belgio, nel 2009. La conferenza ha riunito professionisti dello sviluppo e delle operazioni per discutere di modi per migliorare la collaborazione e automatizzare i processi.\nDevOps ha le sue radici nel movimento Agile, iniziato nei primi anni 2000. Agile ha fornito le basi per un approccio più collaborativo allo sviluppo software e ha enfatizzato le piccole release iterative. Tuttavia, Agile si concentra principalmente sulla collaborazione tra team di sviluppo. Man mano che le metodologie Agile diventavano più popolari, le organizzazioni si sono rese conto della necessità di estendere questa collaborazione ai team operativi.\nLa natura isolata dei team di sviluppo e delle operazioni ha spesso portato a inefficienze, conflitti e ritardi nella distribuzione del software. Questa necessità di una migliore collaborazione e integrazione tra questi team ha portato al movimento DevOps. DevOps può essere visto come un’estensione dei principi Agile, inclusi i team operativi.\nI principi chiave di DevOps includono collaborazione, automazione, integrazione continua, distribuzione e feedback. DevOps si concentra sull’automazione dell’intera pipeline di distribuzione del software, dallo sviluppo alla distribuzione. Migliora la collaborazione tra i team di sviluppo e operativi, utilizzando strumenti come Jenkins, Docker e Kubernetes per semplificare il ciclo di vita dello sviluppo.\nMentre Agile e DevOps condividono principi comuni in materia di collaborazione e feedback, DevOps mira specificamente all’integrazione di sviluppo e operazioni IT, espandendo Agile oltre i soli team di sviluppo. Introduce pratiche e strumenti per automatizzare la distribuzione del software e migliorare la velocità e la qualità delle release del software.\n\n\n13.2.2 MLOps\nMLOps, d’altro canto, sta per Machine Learning Operations ed estende i principi di DevOps al ciclo di vita ML. MLOps automatizza e semplifica l’intero ciclo di vita dell’apprendimento automatico, dalla preparazione dei dati allo sviluppo del modello, fino all’implementazione e al monitoraggio. L’obiettivo principale di MLOps è facilitare la collaborazione tra data scientist, data engineer e operazioni IT e automatizzare la distribuzione, il monitoraggio e la gestione dei modelli ML. Alcuni fattori chiave hanno portato all’ascesa di MLOps.\n\nData drift: La deriva dei dati degrada le prestazioni del modello nel tempo, motivando la necessità di rigorosi monitoraggi e procedure di riqualificazione automatizzate fornite da MLOps.\nRiproducibilità: La mancanza di riproducibilità negli esperimenti di machine learning ha motivato i sistemi MLOps a tracciare codice, dati e variabili di ambiente per abilitare flussi di lavoro ML riproducibili.\nSpiegabilità: La natura di “scatola nera” e la mancanza di spiegabilità di modelli complessi hanno motivato la necessità di funzionalità MLOps per aumentare la trasparenza e la spiegabilità del modello.\nMonitoraggio: L’incapacità di monitorare in modo affidabile le prestazioni del modello dopo la distribuzione ha evidenziato la necessità di soluzioni MLOps con una solida strumentazione delle prestazioni del modello e avvisi.\nAttrito: L’attrito nel riaddestramento e nella distribuzione manuale dei modelli ha motivato la necessità di sistemi MLOps che automatizzano le pipeline di distribuzione dell’apprendimento automatico.\nOttimizzazione: La complessità della configurazione dell’infrastruttura di apprendimento automatico ha motivato la necessità di piattaforme MLOps con un’infrastruttura ML ottimizzata e pronta all’uso.\n\nSebbene DevOps e MLOps condividano l’obiettivo comune di automatizzare e semplificare i processi, differiscono significativamente in termini di attenzione e sfide. DevOps si occupa principalmente di sviluppo software e operazioni IT. Consente la collaborazione tra questi team e automatizza la distribuzione del software. Al contrario, MLOps si concentra sul ciclo di vita dell’apprendimento automatico. Affronta complessità aggiuntive come versioning dei dati, versioning dei modelli e monitoraggio dei modelli. MLOps richiede la collaborazione tra una gamma più ampia di stakeholder, tra cui data scientist, data engineer e IT operations. Va oltre l’ambito del DevOps tradizionale incorporando le sfide uniche della gestione dei modelli ML durante il loro ciclo di vita. Tabella 13.1 fornisce un confronto affiancato di DevOps e MLOps, evidenziandone le principali differenze e somiglianze.\n\n\n\nTabella 13.1: Confronto tra DevOps e MLOps.\n\n\n\n\n\n\n\n\n\n\nAspect\nDevOps\nMLOps\n\n\n\n\nObiettivo\nSemplificazione dei processi di sviluppo software e operativi\nOttimizzazione del ciclo di vita dei modelli di apprendimento automatico\n\n\nMetodologia\nIntegrazione continua e distribuzione continua (CI/CD) per lo sviluppo software\nSimile a CI/CD ma incentrato sui flussi di lavoro di apprendimento automatico\n\n\nStrumenti Principali\nControllo delle versioni (Git), strumenti CI/CD (Jenkins, Travis CI), gestione della configurazione (Ansible, Puppet)\nStrumenti di versioning dei dati, strumenti di training e deployment dei modelli, pipeline CI/CD su misura per ML\n\n\nProblemi Principali\nIntegrazione del codice, test, gestione delle release, automazione, infrastruttura come codice\nGestione dei dati, versioning dei modelli, monitoraggio degli esperimenti, deployment dei modelli, scalabilità dei flussi di lavoro ML\n\n\nRisultati Tipici\nRelease software più rapide e affidabili, collaborazione migliorata tra team di sviluppo e operativi\nGestione e deployment efficienti dei modelli di apprendimento automatico, collaborazione migliorata tra data scientist e ingegneri\n\n\n\n\n\n\nScoprire di più sui cicli di vita ML tramite un “case study” che presenta il riconoscimento vocale in Video 13.1.\n\n\n\n\n\n\nVideo 13.1: MLOps",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Operazioni di ML</span>"
    ]
  },
  {
    "objectID": "contents/core/ops/ops.it.html#componenti-chiave-di-mlops",
    "href": "contents/core/ops/ops.it.html#componenti-chiave-di-mlops",
    "title": "13  Operazioni di ML",
    "section": "13.3 Componenti Chiave di MLOps",
    "text": "13.3 Componenti Chiave di MLOps\nI componenti principali di MLOps formano un framework completo che supporta il ciclo di vita end-to-end dei modelli ML in produzione, dallo sviluppo iniziale all’implementazione e alla gestione continua. In questa sezione, ci basiamo su argomenti come l’automazione e il monitoraggio dei capitoli precedenti, integrandoli in un framework più ampio e introducendo anche ulteriori pratiche chiave come la “governance”. Ogni componente contribuisce a operazioni ML più fluide e snelle, con strumenti popolari che aiutano i team ad affrontare attività specifiche all’interno di questo ecosistema. Insieme, questi elementi rendono MLOps un approccio solido alla gestione dei modelli ML e alla creazione di valore a lungo termine all’interno delle organizzazioni.\nFigura 13.1 illustra lo stack completo del sistema MLOps. Mostra i vari layer coinvolti nelle operazioni di apprendimento automatico. In cima allo stack ci sono modelli/applicazioni ML, come BERT, seguiti da framework/piattaforme ML come PyTorch. Il livello MLOps principale, etichettato come Model Orchestration, comprende diversi componenti chiave: Data Management, CI/CD, Model Training, Model Evaluation, Deployment e Model Serving. Alla base del livello MLOps c’è il livello Infrastructure, rappresentato da tecnologie come Kubernetes. Questo livello gestisce aspetti come Job Scheduling, Resource Management, Capacity Management e Monitoring, tra gli altri. A tenere tutto insieme c’è il layer Hardware, che fornisce le risorse computazionali necessarie per le operazioni ML.\n\n\n\n\n\n\nFigura 13.1: Lo stack MLOps, inclusi Modelli ML, Framework, Orchestrazione del Modello, Infrastruttura e Hardware, illustra il flusso di lavoro end-to-end di MLOps.\n\n\n\nQuesto approccio a “layer” [strati] in Figura 13.1 dimostra come MLOps integra varie tecnologie e processi per facilitare lo sviluppo, la distribuzione e la gestione di modelli di apprendimento automatico in un ambiente di produzione. La figura illustra efficacemente le interdipendenze tra diversi componenti e come si uniscono per formare un ecosistema MLOps completo.\n\n13.3.1 Gestione dei Dati\nI dati nella loro forma grezza, siano essi raccolti da sensori, database, app o altri sistemi, spesso richiedono una preparazione significativa prima di poter essere utilizzati per l’addestramento o l’inferenza. Problemi come formati incoerenti, valori mancanti e convenzioni di etichettatura in evoluzione possono portare a inefficienze e scarse prestazioni del modello se non affrontati sistematicamente. Solide pratiche di gestione dei dati garantiscono che i dati rimangano di alta qualità, tracciabili e facilmente accessibili durante l’intero ciclo di vita del ML, costituendo la base di sistemi di apprendimento automatico scalabili.\nUn aspetto chiave della gestione dei dati è il controllo delle versioni. Strumenti come Git, GitHub e GitLab consentono ai team di tenere traccia delle modifiche ai set di dati, collaborare alla loro cura e ripristinare le versioni precedenti quando necessario. Oltre al versioning, l’annotazione e l’etichettatura dei set di dati sono fondamentali per le attività di apprendimento supervisionato. Software come LabelStudio aiutano i team distribuiti a etichettare i dati in modo coerente su set di dati su larga scala, mantenendo al contempo l’accesso alle versioni precedenti man mano che le convenzioni di etichettatura si evolvono. Queste pratiche non solo migliorano la collaborazione, ma garantiscono anche che i modelli vengano addestrati su dati affidabili e ben organizzati.\nUna volta preparati, i set di dati vengono in genere archiviati su soluzioni cloud scalabili come Amazon S3 o Google Cloud Storage. Questi servizi forniscono versioning, resilienza e controlli di accesso granulari, salvaguardando i dati sensibili e mantenendo al contempo la flessibilità per l’analisi e la modellazione. Per semplificare la transizione dai dati grezzi ai formati pronti per l’analisi, i team creano pipeline automatizzate utilizzando strumenti come Prefect, Apache Airflow e dbt. Queste pipeline automatizzano attività come l’estrazione, la pulizia, la de-duplicazione e la trasformazione dei dati, riducendo il sovraccarico manuale e migliorando l’efficienza.\nAd esempio, una pipeline di dati potrebbe acquisire informazioni da database PostgreSQL, API REST e file CSV archiviati in S3, applicando trasformazioni per produrre set di dati puliti e aggregati. L’output può essere memorizzato in archivi di funzionalità come Tecton o Feast, che forniscono un accesso a bassa latenza sia per il training che per le previsioni. In uno scenario di manutenzione predittiva industriale, i dati dei sensori potrebbero essere elaborati insieme ai record di manutenzione, con conseguenti set di dati arricchiti archiviati in Feast per consentire ai modelli di accedere alle informazioni più recenti senza problemi.\nIntegrando il controllo delle versioni, gli strumenti di annotazione, le soluzioni di archiviazione e le pipeline automatizzate, la gestione dei dati diventa un abilitatore fondamentale per MLOps efficaci. Queste pratiche garantiscono che i dati non siano solo puliti e accessibili, ma anche costantemente allineati con le mutevoli esigenze del progetto, consentendo ai sistemi di machine learning di fornire prestazioni affidabili e scalabili negli ambienti di produzione.\nVideo 13.2 di seguito riporta una breve panoramica delle pipeline di dati.\n\n\n\n\n\n\nVideo 13.2: Pipeline di Dati\n\n\n\n\n\n\n\n\n13.3.2 Pipeline CI/CD\nLe pipeline di integrazione continua e distribuzione continua (CI/CD) automatizzano attivamente la progressione dei modelli ML dallo sviluppo iniziale alla distribuzione in produzione. Adattati per i sistemi ML, i principi CI/CD consentono ai team di distribuire rapidamente e in modo robusto nuovi modelli con errori manuali ridotti al minimo.\nLe pipeline CI/CD orchestrano i passaggi chiave, tra cui il controllo delle nuove modifiche al codice, la trasformazione dei dati, il training e la registrazione di nuovi modelli, i test di convalida, la containerizzazione, la distribuzione in ambienti come cluster di staging e la promozione in produzione. I team sfruttano le soluzioni CI/CD più diffuse come Jenkins, CircleCI e GitHub Actions per eseguire queste pipeline MLOps, mentre Prefect, Metaflow e Kubeflow offrono opzioni incentrate su ML.\nFigura 13.2 illustra una pipeline CI/CD specificamente pensata per MLOps. Il processo inizia con un dataset e un repository di feature (a sinistra), che alimenta una fase di ingestione del dataset. Dopo l’ingestione, i dati vengono sottoposti a convalida per garantirne la qualità prima di essere trasformati per l’addestramento. Parallelamente, un trigger di riaddestramento può avviare la pipeline in base a criteri specificati. I dati passano poi attraverso una fase di addestramento/ottimizzazione del modello all’interno di un motore di elaborazione dati, seguita dalla valutazione e convalida del modello. Una volta convalidato, il modello viene registrato e archiviato in un repository di metadati e artefatti di apprendimento automatico. La fase finale prevede la distribuzione del modello addestrato nuovamente nel dataset e nel repository di feature, creando così un processo ciclico per il miglioramento continuo e la distribuzione di modelli di apprendimento automatico.\n\n\n\n\n\n\nFigura 13.2: Diagramma CI/CD MLOps. Fonte: HarvardX.\n\n\n\nAd esempio, quando uno scienziato dei dati verifica i miglioramenti a un modello di classificazione delle immagini in un repository GitHub, questo attiva attivamente una pipeline CI/CD Jenkins. La pipeline riesegue le trasformazioni dei dati e l’addestramento del modello sui dati più recenti, monitorando gli esperimenti con MLflow. Dopo i test di validazione automatizzati, i team distribuiscono il contenitore del modello in un cluster di staging Kubernetes per un ulteriore controllo qualità. Una volta approvato, Jenkins facilita un rollout graduale del modello in produzione con distribuzioni canary per rilevare eventuali problemi. Se vengono rilevate anomalie, la pipeline consente ai team di tornare alla versione precedente del modello in modo fluido.\nLe pipeline CI/CD consentono ai team di iterare e distribuire rapidamente modelli ML collegando i diversi passaggi dallo sviluppo alla distribuzione con automazione continua. L’integrazione di strumenti MLOps come MLflow migliora il packaging del modello, il controllo delle versioni e la tracciabilità della pipeline. CI/CD è fondamentale per far progredire i modelli oltre i prototipi in sistemi aziendali sostenibili.\n\n\n13.3.3 Addestramento del Modello\nL’addestramento del modello è una fase critica in cui gli scienziati dei dati sperimentano varie architetture e algoritmi ML per ottimizzare i modelli che estraggono informazioni dai dati. MLOps introduce best practice e automazione per rendere questo processo iterativo più efficiente e riproducibile. I moderni framework ML come TensorFlow, PyTorch e Keras forniscono componenti predefiniti che semplificano la progettazione di reti neurali e altre architetture di modelli. Questi strumenti consentono agli scienziati dei dati di concentrarsi sulla creazione di modelli ad alte prestazioni utilizzando moduli integrati per layer, attivazioni e funzioni di perdita.\nPer rendere il processo di addestramento efficiente e riproducibile, MLOps introduce best practice come il controllo delle versioni del codice di addestramento tramite Git e l’hosting in repository come GitHub. Gli ambienti riproducibili, spesso gestiti tramite strumenti interattivi come i notebook Jupyter, consentono ai team di raggruppare l’ingestione dei dati, la pre-elaborazione, lo sviluppo del modello e la valutazione in un unico documento. Questi notebook non sono solo controllati per la versione, ma possono anche essere integrati in pipeline automatizzate per un riaddestramento continuo.\nL’automazione svolge un ruolo significativo nella standardizzazione dei flussi di lavoro di addestramento. Funzionalità come ottimizzazione degli iperparametri, ricerca dell’architettura neurale e selezione automatica delle feature sono comunemente integrate nelle pipeline MLOps per iterare rapidamente e trovare configurazioni ottimali. Le pipeline CI/CD orchestrano i flussi di lavoro di addestramento automatizzando attività come la pre-elaborazione dei dati, l’addestramento del modello, la valutazione e la registrazione. Ad esempio, una pipeline Jenkins può attivare uno script Python per riaddestrare un modello TensorFlow, convalidarne le prestazioni rispetto a metriche predefinite e distribuirlo se vengono superate delle soglie.\nI servizi di addestramento gestiti dal cloud hanno rivoluzionato l’accessibilità di hardware ad alte prestazioni per i modelli di training. Questi servizi forniscono accesso on-demand all’infrastruttura accelerata da GPU, rendendo l’addestramento avanzato fattibile anche per piccoli team. A seconda del provider, gli sviluppatori possono gestire autonomamente il flusso di lavoro di training o affidarsi a opzioni completamente gestite come Vertex AI Fine Tuning, che può perfezionare automaticamente un modello di base utilizzando un set di dati etichettato. Tuttavia, è importante notare che la domanda di hardware GPU spesso supera l’offerta e la disponibilità può variare in base alla regione o agli accordi contrattuali, ponendo potenziali colli di bottiglia per i team che si affidano ai servizi cloud.\nUn esempio di flusso di lavoro prevede che uno scienziato dei dati utilizzi un notebook PyTorch per sviluppare un modello CNN per la classificazione delle immagini. La libreria fastai fornisce API di alto livello per semplificare l’addestramento delle CNN sui set di dati delle immagini. Il notebook addestra il modello sui dati campione, valuta le metriche di accuratezza e ottimizza gli iperparametri come la velocità di apprendimento e i layer per ottimizzare le prestazioni. Questo notebook riproducibile è controllato dalla versione e integrato in una pipeline di riaddestramento.\nAutomatizzando e standardizzando l’addestramento dei modelli, sfruttando i servizi cloud gestiti e integrando framework moderni, i team possono accelerare la sperimentazione e creare modelli di apprendimento automatico solidi e pronti per la produzione.\n\n\n13.3.4 Valutazione del Modello\nPrima di distribuire i modelli, i team eseguono una valutazione e dei test rigorosi per convalidare i benchmark delle prestazioni e la prontezza per il rilascio. MLOps fornisce le best practice per la convalida del modello, l’audit e i metodi di test controllati per ridurre al minimo i rischi durante la distribuzione.\nIl processo di valutazione inizia con il test dei modelli rispetto ai set di dati di test che sono indipendenti dai dati di training ma provengono dalla stessa distribuzione dei dati di produzione. Metriche chiave come accuratezza, AUC, precisione, richiamo e punteggio F1 vengono calcolate per quantificare le prestazioni del modello. Il monitoraggio di queste metriche nel tempo aiuta i team a identificare tendenze e potenziali degradazioni nel comportamento del modello, in particolare quando i dati di valutazione provengono da flussi di produzione live. Ciò è fondamentale per rilevare il data drift [deriva dei dati], in cui le modifiche nelle distribuzioni dei dati di input possono erodere l’accuratezza del modello.\nPer convalidare le prestazioni nel mondo reale, il test canary distribuisce il modello a un piccolo sottoinsieme di utenti. Questa distribuzione graduale consente ai team di monitorare le metriche in un ambiente live e di individuare potenziali problemi prima della distribuzione su vasta scala. Aumentando gradualmente il traffico verso il nuovo modello, i team possono valutare con sicurezza il suo impatto sull’esperienza dell’utente finale. Ad esempio, un rivenditore potrebbe testare un modello di raccomandazione personalizzato confrontando le sue metriche di accuratezza e diversità con i dati storici. Durante la fase di test, il team monitora le metriche delle prestazioni live e identifica un leggero calo dell’accuratezza nell’arco di due settimane. Per garantire la stabilità, il modello viene inizialmente distribuito al 5% del traffico web, monitorato per potenziali problemi e distribuito ampiamente solo dopo aver dimostrato la sua solidità in produzione.\nI modelli ML distribuiti sul cloud traggono vantaggio dalla connettività Internet costante e dalla capacità di registrare ogni richiesta e risposta. Ciò rende possibile riprodurre o generare richieste sintetiche per confrontare diversi modelli e versioni. Alcuni provider offrono strumenti che automatizzano parti del processo di valutazione, come il monitoraggio degli esperimenti di iperparametri o il confronto delle esecuzioni del modello. Ad esempio, piattaforme come Weights and Biases semplificano questo processo automatizzando il monitoraggio degli esperimenti e generando artefatti dalle esecuzioni di training.\nL’automazione dei processi di valutazione e test, combinata con un attento test canary, riduce i rischi di distribuzione. Mentre i processi di valutazione automatizzati rilevano molti problemi, la supervisione umana rimane essenziale per esaminare le prestazioni su specifici segmenti di dati e identificare sottili debolezze. Questa combinazione di rigorosa convalida pre-distribuzione e test nel mondo reale fornisce ai team sicurezza quando mettono i modelli in produzione.\n\n\n13.3.5 Distribuzione del Modello\nI team devono confezionare, testare e tracciare correttamente i modelli ML per distribuirli in modo affidabile in produzione. MLOps introduce framework e procedure per il versioning attivo, la distribuzione, il monitoraggio e l’aggiornamento dei modelli in modi sostenibili.\nUn approccio comune alla distribuzione prevede la containerizzazione dei modelli tramite strumenti come Docker, che impacchettano codice, librerie e dipendenze in unità standardizzate. I container garantiscono una portabilità fluida tra gli ambienti, rendendo la distribuzione coerente e prevedibile. Framework come TensorFlow Serving e BentoML aiutano a servire le previsioni dai modelli distribuiti tramite API ottimizzate per le prestazioni. Questi framework gestiscono il versioning, il ridimensionamento e il monitoraggio.\nPrima del rollout su larga scala, i team distribuiscono modelli aggiornati in ambienti di staging o QA per testare rigorosamente le prestazioni. Tecniche come le distribuzioni shadow o canary vengono utilizzate per convalidare i nuovi modelli in modo incrementale. Ad esempio, le distribuzioni canary indirizzano una piccola percentuale di traffico al nuovo modello monitorando attentamente le prestazioni. Se non si verificano problemi, il traffico verso il nuovo modello aumenta gradualmente. Procedure di rollback robuste sono essenziali per gestire problemi imprevisti, ripristinando i sistemi alla versione precedente del modello stabile per garantire un’interruzione minima. L’integrazione con pipeline CI/CD automatizza ulteriormente il processo di distribuzione e rollback, consentendo cicli di iterazione efficienti.\nPer mantenere la discendenza e la verificabilità, i team tengono traccia degli artefatti del modello, inclusi script, pesi, log e metriche, utilizzando strumenti come MLflow. I registri dei modelli, come il registro dei modelli di Vertex AI, fungono da repository centralizzati per l’archiviazione e la gestione dei modelli addestrati. Questi registri non solo facilitano i confronti delle versioni, ma spesso includono anche l’accesso ai modelli di base, che possono essere open source, proprietari o ibridi (ad esempio, LLAMA). La distribuzione di un modello dal registro a un endpoint di inferenza è semplificata, gestendo il provisioning delle risorse, il peso dei download del modello e l’hosting.\nGli endpoint di inferenza in genere espongono il modello distribuito tramite API REST per previsioni in tempo reale. A seconda dei requisiti di prestazioni, i team possono configurare risorse, come acceleratori GPU, per soddisfare gli obiettivi di latenza e produttività. Alcuni provider offrono anche opzioni flessibili come inferenza serverless o batch, eliminando la necessità di endpoint persistenti e consentendo distribuzioni scalabili ed economiche. Ad esempio, AWS SageMaker Inference supporta tali configurazioni.\nSfruttando questi strumenti e pratiche, i team possono distribuire modelli ML in modo resiliente, garantendo transizioni fluide tra le versioni, mantenendo la stabilità della produzione e ottimizzando le prestazioni in diversi casi d’uso.\n\n\n13.3.6 Model Serving\nDopo il “deployment” [distribuzione] del modello, ML-as-a-Service diventa un componente fondamentale nel ciclo di vita di MLOps. I servizi online come Facebook/Meta gestiscono decine di trilioni di query di inferenza al giorno (Wu et al. 2019). Il “model serving” colma il divario tra i modelli sviluppati e le applicazioni ML o gli utenti finali, assicurando che i modelli distribuiti siano accessibili, performanti e scalabili negli ambienti di produzione.\n\nWu, Carole-Jean, David Brooks, Kevin Chen, Douglas Chen, Sy Choudhury, Marat Dukhan, Kim Hazelwood, et al. 2019. «Machine Learning at Facebook: Understanding Inference at the Edge». In 2019 IEEE International Symposium on High Performance Computer Architecture (HPCA), 331–44. IEEE; IEEE. https://doi.org/10.1109/hpca.2019.00048.\nDiversi framework facilitano il model serving, tra cui TensorFlow Serving, NVIDIA Triton Inference Server e KServe (in precedenza KFServing). Questi strumenti forniscono interfacce standardizzate per la distribuzione di modelli distribuiti su varie piattaforme e gestiscono molte complessità dell’inferenza del modello su larga scala.\nIl model serving può essere categorizzato in tre tipi principali:\n\nOnline Serving: Fornisce previsioni in tempo reale con bassa latenza, il che è fondamentale per applicazioni come sistemi di raccomandazione o rilevamento frodi.\nOffline Serving: Elabora grandi batch di dati in modo asincrono, adatto per attività come la generazione periodica di report.\nNear-Online Serving (semi-sincrono): Bilancia tra online e offline, offrendo risposte relativamente rapide per applicazioni meno sensibili al tempo come i chatbot.\n\nUna delle sfide principali per i sistemi di model serving è operare secondo requisiti di prestazioni definiti da Service Level Agreement (SLA) e Service Level Objective (SLO). Gli SLA sono contratti formali che specificano i livelli di servizio previsti. Questi livelli di servizio si basano su parametri quali tempo di risposta, disponibilità e produttività. Gli SLO sono obiettivi interni che i team si prefiggono di soddisfare o superare i propri SLA.\nPer il model serving ML, gli accordi e gli obiettivi SLA e SLO hanno un impatto diretto sull’esperienza utente, sull’affidabilità del sistema e sui risultati aziendali. Pertanto, i team ottimizzano attentamente la propria piattaforma di servizio. I serving system ML impiegano varie tecniche per ottimizzare le prestazioni e l’utilizzo delle risorse, come le seguenti:\n\nPianificazione e batch delle richieste: Gestisce in modo efficiente le richieste di inferenza ML in arrivo, ottimizzando le prestazioni tramite strategie di accodamento e raggruppamento intelligenti. Sistemi come Clipper (Crankshaw et al. 2017) introducono il servizio di previsione online a bassa latenza con tecniche di caching e batch.\nSelezione e routing delle istanze del modello: Algoritmi intelligenti indirizzano le richieste alle versioni o alle istanze del modello appropriate. INFaaS (Romero et al. 2021) esplora questo aspetto generando varianti del modello e navigando in modo efficiente nello spazio di compromesso in base ai requisiti di prestazioni e accuratezza.\nBilanciamento del carico: Distribuisce i carichi di lavoro in modo uniforme su più istanze di servizio. MArk (Model Ark) (C. Zhang et al. 2019) dimostra tecniche efficaci di bilanciamento del carico per sistemi di servizio ML.\nAutoscaling delle istanze del modello: Regola dinamicamente la capacità in base alla domanda. Sia INFaaS (Romero et al. 2021) che MArk (C. Zhang et al. 2019) incorporano funzionalità di autoscaling per gestire in modo efficiente le fluttuazioni del carico di lavoro.\nOrchestration del modello: Gestisce l’esecuzione del modello, abilitando l’elaborazione parallela e l’allocazione strategica delle risorse. AlpaServe (Z. Li et al. 2023) dimostra tecniche avanzate per la gestione di modelli di grandi dimensioni e scenari di servizio complessi.\nPrevisione del tempo di esecuzione: Sistemi come Clockwork (Gujarati et al. 2020) si concentrano sul servizio ad alte prestazioni prevedendo i tempi di esecuzione delle singole inferenze e utilizzando in modo efficiente gli acceleratori hardware.\n\n\nCrankshaw, Daniel, Xin Wang, Guilio Zhou, Michael J Franklin, Joseph E Gonzalez, e Ion Stoica. 2017. «Clipper: A \\(\\{\\)Low-Latency\\(\\}\\) online prediction serving system». In 14th USENIX Symposium on Networked Systems Design and Implementation (NSDI 17), 613–27.\n\nRomero, Francisco, Qian Li 0027, Neeraja J. Yadwadkar, e Christos Kozyrakis. 2021. «INFaaS: Automated Model-less Inference Serving.» In 2021 USENIX Annual Technical Conference (USENIX ATC 21), 397–411. https://www.usenix.org/conference/atc21/presentation/romero.\n\nZhang, Chengliang, Minchen Yu, Wei Wang 0030, e Feng Yan 0001. 2019. «MArk: Exploiting Cloud Services for Cost-Effective, SLO-Aware Machine Learning Inference Serving.» In 2019 USENIX Annual Technical Conference (USENIX ATC 19), 1049–62. https://www.usenix.org/conference/atc19/presentation/zhang-chengliang.\n\nLi, Zhuohan, Lianmin Zheng, Yinmin Zhong, Vincent Liu, Ying Sheng, Xin Jin, Yanping Huang, et al. 2023. «\\(\\{\\)AlpaServe\\(\\}\\): Statistical multiplexing with model parallelism for deep learning serving». In 17th USENIX Symposium on Operating Systems Design and Implementation (OSDI 23), 663–79.\n\nGujarati, Arpan, Reza Karimi, Safya Alzayat, Wei Hao, Antoine Kaufmann, Ymir Vigfusson, e Jonathan Mace. 2020. «Serving DNNs like Clockwork: Performance Predictability from the Bottom Up.» In 14th USENIX Symposium on Operating Systems Design and Implementation (OSDI 20), 443–62. https://www.usenix.org/conference/osdi20/presentation/gujarati.\nI serving system ML che eccellono in queste aree consentono alle organizzazioni di distribuire modelli che funzionano in modo affidabile sotto pressione. Il risultato sono applicazioni AI scalabili e reattive in grado di gestire le richieste del mondo reale e fornire valore in modo coerente.\n\n\n13.3.7 Gestione dell’Infrastruttura\nI team MLOps sfruttano ampiamente gli strumenti “infrastructure as code (IaC)” e le solide architetture cloud per gestire attivamente le risorse necessarie per lo sviluppo, il training e la distribuzione dei sistemi ML.\nI team utilizzano strumenti IaC come Terraform, CloudFormation e Ansible per definire, fornire e aggiornare a livello di programmazione l’infrastruttura in modo controllato dalla versione. Per MLOps, i team utilizzano ampiamente Terraform per avviare risorse su AWS, GCP e Azure.\nPer la creazione e il training dei modelli, i team forniscono dinamicamente risorse di elaborazione come server GPU, cluster di container, storage e database tramite Terraform in base alle esigenze degli scienziati dei dati. Il codice incapsula e preserva le definizioni dell’infrastruttura.\nI container e gli orchestratori come Docker e Kubernetes consentono ai team di impacchettare modelli e distribuirli in modo affidabile in diversi ambienti. I contenitori possono essere attivati o disattivati automaticamente in base alla domanda.\nSfruttando l’elasticità del cloud, i team aumentano o diminuiscono le risorse per soddisfare i picchi nei carichi di lavoro come i lavori di ottimizzazione degli iperparametri o i picchi nelle richieste di previsione. Auto-scaling consente un’efficienza dei costi ottimizzata.\nL’infrastruttura si estende su dispositivi on-premise, cloud e edge. Uno stack tecnologico robusto offre flessibilità e resilienza. Gli strumenti di monitoraggio consentono ai team di osservare l’utilizzo delle risorse.\nAd esempio, una configurazione Terraform può distribuire un cluster GCP Kubernetes per ospitare modelli TensorFlow addestrati esposti come microservizi di previsione. Il cluster aumenta i pod per gestire un traffico maggiore. L’integrazione CI/CD distribuisce senza problemi nuovi contenitori di modelli.\nLa gestione attenta dell’infrastruttura tramite IaC e monitoraggio consente ai team di prevenire i colli di bottiglia nell’operatività dei sistemi ML su larga scala.\n\n\n13.3.8 Monitoraggio\nI team MLOps mantengono attivamente un monitoraggio robusto per mantenere la visibilità nei modelli ML distribuiti in produzione. Il monitoraggio continuo fornisce informazioni sulle prestazioni del modello e del sistema in modo che i team possano rilevare e risolvere rapidamente i problemi per ridurre al minimo le interruzioni.\nI team monitorano attivamente gli aspetti chiave del modello, inclusa l’analisi di campioni di previsioni live per tracciare metriche come accuratezza e matrice di confusione nel tempo.\nQuando monitorano le prestazioni, i team devono profilare i dati in arrivo per verificare la deriva del modello, un calo costante dell’accuratezza del modello dopo l’implementazione in produzione. La deriva del modello può verificarsi in due modi: deriva del concetto e deriva dei dati. La deriva del concetto si riferisce a un cambiamento fondamentale osservato nella relazione tra i dati di input e quelli target. Ad esempio, con l’avanzare della pandemia di COVID-19, i siti di e-commerce e vendita al dettaglio hanno dovuto correggere le raccomandazioni del modello poiché i dati di acquisto erano ampiamente distorti verso articoli come il disinfettante per le mani. La deriva dei dati descrive i cambiamenti nella distribuzione dei dati nel tempo. Ad esempio, gli algoritmi di riconoscimento delle immagini utilizzati nelle auto a guida autonoma devono tenere conto della stagionalità nell’osservazione dell’ambiente circostante. I team monitorano anche le metriche delle prestazioni delle applicazioni come latenza ed errori per le integrazioni dei modelli.\nDa una prospettiva infrastrutturale, i team monitorano i problemi di capacità come elevato utilizzo di CPU, memoria e disco e interruzioni del sistema. Strumenti come Prometheus, Grafana ed Elastic consentono ai team di raccogliere, analizzare, interrogare e visualizzare attivamente diverse metriche di monitoraggio. Le dashboard rendono le dinamiche altamente visibili.\nI team configurano gli allarmi per le metriche di monitoraggio chiave come cali di accuratezza e guasti del sistema per consentire una risposta proattiva agli eventi che minacciano l’affidabilità. Ad esempio, i cali di accuratezza del modello attivano avvisi per i team per esaminare potenziali deviazioni dei dati e riaddestrare i modelli utilizzando campioni di dati aggiornati e rappresentativi.\nDopo la distribuzione, il monitoraggio completo consente ai team di mantenere la fiducia nello stato del modello e del sistema. Consente ai team di rilevare e risolvere preventivamente le deviazioni tramite allarmi e dashboard basati sui dati. Il monitoraggio attivo è essenziale per mantenere sistemi ML altamente disponibili e affidabili.\nGuardare il video qui sotto per saperne di più sul monitoraggio.\n\n\n\n\n\n\nVideo 13.3: Monitoraggio del Modello\n\n\n\n\n\n\n\n\n13.3.9 Governance\nI team MLOps stabiliscono attivamente pratiche di governance appropriate come componente fondamentale. La governance fornisce una supervisione sui modelli ML per garantire che siano affidabili, etici e conformi. Senza governance, sussistono rischi significativi di modelli che si comportano in modi pericolosi o proibiti quando vengono distribuiti in applicazioni e processi aziendali.\nLa governance MLOps impiega tecniche per fornire trasparenza sulle previsioni, sulle prestazioni e sul comportamento del modello durante l’intero ciclo di vita ML. Metodi di spiegabilità come SHAP e LIME aiutano gli auditor a comprendere perché i modelli effettuano determinate previsioni evidenziando le caratteristiche di input influenti alla base delle decisioni. Bias detection analizza le prestazioni del modello in diversi gruppi demografici definiti da attributi come età, sesso ed etnia per rilevare eventuali distorsioni sistematiche. I team eseguono rigorose procedure di test su set di dati rappresentativi per convalidare le prestazioni del modello prima della distribuzione.\nUna volta in produzione, i team monitorano la concept drift [deriva del concetto] per determinare se le relazioni predittive cambiano nel tempo in modi che degradano l’accuratezza del modello. I team analizzano anche i registri di produzione per scoprire pattern nei tipi di errori generati dai modelli. La documentazione sulla provenienza dei dati, le procedure di sviluppo e le metriche di valutazione fornisce ulteriore visibilità.\nPiattaforme come Watson OpenScale incorporano funzionalità di governance come il monitoraggio dei bias e la spiegabilità direttamente nella creazione di modelli, nei test e nel monitoraggio della produzione. Le aree di interesse principali della governance sono trasparenza, correttezza e conformità. Ciò riduce al minimo i rischi che i modelli si comportino in modo errato o pericoloso quando integrati nei processi aziendali. L’integrazione di pratiche di governance nei flussi di lavoro MLOps consente ai team di garantire un’IA affidabile.\n\n\n13.3.10 Comunicazione e Collaborazione\nMLOps abbatte attivamente i “silos” e consente il libero flusso di informazioni e approfondimenti tra i team in tutte le fasi del ciclo di vita ML. Strumenti come MLflow, Weights & Biases e contesti di dati forniscono tracciabilità e visibilità per migliorare la collaborazione.\nI team utilizzano MLflow per sistematizzare il monitoraggio di esperimenti, versioni e artefatti del modello. Gli esperimenti possono essere loggati a livello di programmazione da notebook di data science e job di training. Il registro dei modelli fornisce un hub centrale per i team per archiviare modelli pronti per la produzione prima della distribuzione, con metadati come descrizioni, metriche, tag e discendenza. Le integrazioni con Github, GitLab facilitano i trigger per la modifica del codice.\n“Weights & Biases” fornisce strumenti collaborativi su misura per i team ML. Gli scienziati dei dati registrano gli esperimenti, visualizzano metriche come curve di perdita e condividono approfondimenti sulla sperimentazione con i colleghi. Le dashboard di confronto evidenziano le differenze del modello. I team discutono dei progressi e dei passaggi successivi.\nLa definizione di contesti di dati condivisi, ovvero glossari, dizionari di dati e riferimenti di schemi, garantisce l’allineamento del significato e dell’utilizzo dei dati tra i ruoli. La documentazione aiuta a comprendere chi non ha accesso diretto ai dati.\nAd esempio, uno scienziato dei dati può utilizzare “Weights & Biases” per analizzare un esperimento con un modello di rilevamento delle anomalie e condividere i risultati della valutazione con altri membri del team per discutere dei miglioramenti. Il modello finale può quindi essere registrato con MLflow prima di essere consegnato per la distribuzione.\nL’abilitazione della trasparenza, della tracciabilità e della comunicazione tramite MLOps consente ai team di rimuovere i colli di bottiglia e accelerare la distribuzione di sistemi ML di impatto.\nVideo 13.4 affronta le sfide chiave nella distribuzione del modello, tra cui la deriva del concetto, la deriva del modello e i problemi di ingegneria del software.\n\n\n\n\n\n\nVideo 13.4: Sfide della Distribuzione",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Operazioni di ML</span>"
    ]
  },
  {
    "objectID": "contents/core/ops/ops.it.html#debito-tecnico-nascosto-nei-sistemi-ml",
    "href": "contents/core/ops/ops.it.html#debito-tecnico-nascosto-nei-sistemi-ml",
    "title": "13  Operazioni di ML",
    "section": "13.4 Debito Tecnico Nascosto nei Sistemi ML",
    "text": "13.4 Debito Tecnico Nascosto nei Sistemi ML\nIl debito tecnico [https://it.wikipedia.org/wiki/Debito_tecnico] è sempre più pressante per i sistemi di apprendimento automatico. Questa metafora, originariamente proposta negli anni ’90, paragona i costi a lungo termine dello sviluppo rapido del software al debito finanziario. Proprio come un debito finanziario alimenta una crescita vantaggiosa, un debito tecnico gestito con attenzione consente una rapida iterazione. Tuttavia, se non controllato, l’accumulo di debito tecnico può superare qualsiasi guadagno.\nFigura 13.3 illustra i vari componenti che contribuiscono al debito tecnico nascosto dei sistemi ML. Mostra la natura interconnessa di configurazione, raccolta dati ed estrazione di funzionalità, che è fondamentale per la base di codice ML. Le dimensioni delle caselle indicano la proporzione dell’intero sistema rappresentata da ciascun componente. Nei sistemi ML industriali, il codice per l’algoritmo del modello costituisce solo una piccola frazione (vedere la piccola casella nera al centro rispetto a tutte le altre caselle grandi). La complessità dei sistemi ML e la natura frenetica del settore rendono molto facile l’accumulo di debito tecnico.\n\n\n\n\n\n\nFigura 13.3: Componenti del sistema ML. Fonte: Sambasivan et al. (2021)\n\n\n\n\n13.4.1 Erosione dei Confini del Modello\nA differenza del software tradizionale, ML non ha confini chiari tra i componenti, come si vede nel diagramma sopra. Questa erosione dell’astrazione crea intrecci che esacerbano il debito tecnico in diversi modi:\n\n\n13.4.2 Intreccio\nUn accoppiamento stretto tra i componenti del modello ML rende difficile isolare le modifiche. La modifica di una parte provoca effetti a catena imprevedibili in tutto il sistema. “Changing Anything Changes Everything (noto anche come CACE)” [Cambiare qualcosa cambia tutto] è un fenomeno che si applica a qualsiasi modifica apportata al sistema. Le potenziali mitigazioni includono la scomposizione del problema quando possibile o il monitoraggio ravvicinato delle modifiche nel comportamento per contenerne l’impatto.\n\n\n13.4.3 Cascate di Correzione\nFigura 13.4 illustra il concetto di cascate di correzione nel flusso di lavoro ML, dalla definizione del problema all’implementazione del modello. Gli archi rappresentano le potenziali correzioni iterative necessarie in ogni fase del flusso di lavoro, con colori diversi corrispondenti a problemi distinti come l’interazione con la fragilità del mondo fisico, competenze inadeguate nel dominio dell’applicazione, sistemi di ricompensa in conflitto e scarsa documentazione inter-organizzativa.\nLe frecce rosse indicano l’impatto delle cascate, che possono portare a revisioni significative nel processo di sviluppo del modello. Al contrario, la linea rossa tratteggiata rappresenta la misura drastica di abbandono del processo per riavviarlo. Questa immagine sottolinea la natura complessa e interconnessa dello sviluppo del sistema ML e l’importanza di affrontare questi problemi all’inizio del ciclo di sviluppo per mitigare i loro effetti di amplificazione a valle.\n\n\n\n\n\n\nFigura 13.4: Diagramma di flusso delle cascate di correzione. Fonte: Sambasivan et al. (2021).\n\n\nSambasivan, Nithya, Shivani Kapania, Hannah Highfill, Diana Akrong, Praveen Paritosh, e Lora M Aroyo. 2021. «“Everyone wants to do the model work, not the data work”: Data Cascades in High-Stakes AI». In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems. ACM. https://doi.org/10.1145/3411764.3445518.\n\n\nLa creazione di modelli in sequenza crea dipendenze rischiose in cui i modelli successivi si basano su quelli precedenti. Ad esempio, prendere un modello esistente e perfezionarlo per un nuovo caso d’uso sembra efficiente. Tuttavia, questo incorpora ipotesi dal modello originale che potrebbero eventualmente richiedere una correzione.\nDiversi fattori influenzano la decisione di creare modelli in sequenza o meno:\n\nDimensioni del dataset e tasso di crescita: Con set di dati statici e di piccole dimensioni, la messa a punto dei modelli esistenti ha spesso senso. Per set di dati di grandi dimensioni e in crescita, l’addestramento di modelli personalizzati da zero consente una maggiore flessibilità per tenere conto dei nuovi dati.\nRisorse di elaborazione disponibili: La messa a punto richiede meno risorse rispetto all’addestramento di modelli di grandi dimensioni da zero. Con risorse limitate, sfruttare i modelli esistenti potrebbe essere l’unico approccio fattibile.\n\nMentre la messa a punto dei modelli esistenti può essere efficiente, la modifica dei componenti fondamentali in seguito diventa estremamente costosa a causa di questi effetti a cascata. Pertanto, si dovrebbe considerare attentamente l’introduzione di nuove architetture di modelli, anche se ad alta intensità di risorse, per evitare cascate di correzioni in futuro. Questo approccio può aiutare ad attenuare gli effetti di amplificazione dei problemi a valle e a ridurre il debito tecnico. Tuttavia, ci sono ancora scenari in cui la creazione di modelli sequenziali ha senso, il che richiede un attento equilibrio tra efficienza, flessibilità e manutenibilità a lungo termine nel processo di sviluppo ML.\n\n\n13.4.4 Consumatori Non Dichiarati\nUna volta che le previsioni del modello ML sono rese disponibili, molti sistemi downstream [derivati] potrebbero utilizzarle silenziosamente come input per un’ulteriore elaborazione. Tuttavia, il modello originale non è stato progettato per adattarsi a questo ampio riutilizzo. A causa dell’opacità intrinseca dei sistemi ML, diventa impossibile analizzare completamente l’impatto degli output del modello come input altrove. Le modifiche al modello possono quindi avere conseguenze costose e pericolose interrompendo dipendenze non rilevate.\nI “consumatori” non dichiarati possono anche abilitare loop di feedback nascosti se i loro output influenzano indirettamente i dati di training del modello originale. Le mitigazioni includono la limitazione dell’accesso alle previsioni, la definizione di contratti di servizio rigorosi e il monitoraggio di segnali di influenze non-modellate. Architettare sistemi ML per incapsulare e isolare i loro effetti limita i rischi di propagazione imprevista.\n\n\n13.4.5 Debito di Dipendenza dai Dati\nIl debito di dipendenza dei dati si riferisce a dipendenze di dati instabili e sottoutilizzate, che possono avere ripercussioni dannose e difficili da rilevare. Sebbene questo sia un fattore chiave del debito tecnologico per il software tradizionale, tali sistemi possono trarre vantaggio dall’uso di strumenti ampiamente disponibili per l’analisi statica da parte di compilatori e linker per identificare dipendenze di questo tipo. I sistemi ML necessitano di strumenti simili.\nUna mitigazione per le dipendenze di dati instabili è l’uso del versioning, che garantisce la stabilità degli input ma comporta il costo della gestione di più set di dati e il potenziale della obsolescenza. Un’altra mitigazione per le dipendenze di dati sottoutilizzate è quella di condurre una valutazione esaustiva “leave-one-feature-out”.\n\n\n13.4.6 Debito di Analisi dai Cicli di Feedback\nA differenza del software tradizionale, i sistemi ML possono cambiare il loro comportamento nel tempo, rendendo difficile l’analisi pre-distribuzione. Questo debito si manifesta nei cicli di feedback, sia diretti che nascosti.\nI cicli di feedback diretti si verificano quando un modello influenza i suoi input futuri, ad esempio consigliando prodotti agli utenti che, a loro volta, modellano i dati di training futuri. I cicli nascosti sorgono indirettamente tra modelli, ad esempio due sistemi che interagiscono tramite ambienti del mondo reale. I cicli di feedback graduali sono particolarmente difficili da rilevare. Questi cicli portano al debito di analisi, ovvero l’incapacità di prevedere come un modello agirà completamente dopo il rilascio. Essi compromettono la validazione pre-distribuzione consentendo un’autoinfluenza non modellata.\nUn attento monitoraggio e distribuzioni “canary” aiutano a rilevare il feedback. Tuttavia, permangono sfide fondamentali nella comprensione delle interazioni complesse del modello. Le scelte architettoniche che riducono l’intreccio e l’accoppiamento mitigano l’effetto composto del debito di analisi.\n\n\n13.4.7 Le Giungle di Pipeline\nI workflow [flussi di lavoro] ML spesso necessitano di interfacce più standardizzate tra i componenti. Ciò porta i team a “incollare” gradualmente le pipeline con codice personalizzato. Ciò che emerge sono “giungle di pipeline”, ovvero passaggi di pre-elaborazione aggrovigliati che sono fragili e resistono al cambiamento. Evitare modifiche a queste pipeline disordinate fa sì che i team sperimentino attraverso prototipi alternativi. Presto, proliferano molteplici modi di fare. La necessità di astrazioni e interfacce impedisce quindi la condivisione, il riutilizzo e l’efficienza.\nIl debito tecnico si accumula man mano che le pipeline si solidificano in vincoli legacy. I team sprecano tempo nella gestione di codice idiosincratico anziché massimizzare le prestazioni del modello. Principi architettonici come modularità e incapsulamento sono necessari per stabilire interfacce pulite. Le astrazioni condivise consentono componenti intercambiabili, impediscono il lock-in e promuovono la diffusione delle “best practice” tra i team. Liberarsi dalle “giungle di pipeline” richiede in definitiva l’applicazione di standard che impediscano l’accumulo di debito di astrazione. I vantaggi delle interfacce e delle API che domano la complessità superano i costi di transizione.\n\n\n13.4.8 Debito di Configurazione\nI sistemi ML comportano una configurazione estesa di iperparametri, architetture e altri parametri di ottimizzazione. Tuttavia, la configurazione è spesso un ripensamento, che necessita di più rigore e test: aumentano le configurazioni ad hoc, amplificate dalle numerose “manopole” disponibili per l’ottimizzazione di modelli ML complessi.\nQuesto accumulo di debito tecnico ha diverse conseguenze. Configurazioni fragili e obsolete portano a dipendenze nascoste e bug che causano guasti di produzione. La conoscenza sulle configurazioni ottimali è isolata anziché condivisa, portando a un lavoro ridondante. Riprodurre e confrontare i risultati diventa difficile quando le configurazioni mancano di documentazione. I vincoli legacy si accumulano poiché i team temono di modificare configurazioni poco comprese.\nPer affrontare il debito di configurazione è necessario stabilire standard per documentare, testare, convalidare e archiviare centralmente le configurazioni. Investire in approcci più automatizzati, come l’ottimizzazione degli iperparametri e la ricerca dell’architettura, riduce la dipendenza dall’ottimizzazione manuale. Una migliore igiene della configurazione rende il miglioramento iterativo più gestibile impedendo alla complessità di aumentare all’infinito. La chiave è riconoscere la configurazione come parte integrante del ciclo di vita del sistema ML piuttosto che come un ripensamento ad hoc.\n\n\n13.4.9 Il Mondo che Cambia\nI sistemi ML operano in ambienti dinamici del mondo reale. Le soglie e le decisioni inizialmente efficaci diventano obsolete man mano che il mondo si evolve. Tuttavia, i vincoli legacy rendono difficile adattare i sistemi a popolazioni, pattern di utilizzo e altri fattori contestuali mutevoli.\nQuesto debito si manifesta in due modi principali. In primo luogo, le soglie preimpostate e le euristiche richiedono una rivalutazione e una messa a punto costanti man mano che i loro valori ottimali si spostano. In secondo luogo, la convalida dei sistemi tramite test statici di unità e integrazione fallisce quando input e comportamenti sono obiettivi in movimento.\nRispondere a un mondo in continua evoluzione in tempo reale con sistemi ML legacy è impegnativo. Il debito tecnico si accumula man mano che le ipotesi decadono. La mancanza di architettura modulare e la capacità di aggiornare dinamicamente i componenti senza effetti collaterali esacerbano questi problemi.\nPer mitigare questo problema è necessario integrare configurabilità, monitoraggio e aggiornabilità modulare. L’apprendimento online, in cui i modelli si adattano continuamente e solidi cicli di feedback alle pipeline di training, aiutano a sintonizzarsi automaticamente sul mondo. Tuttavia, anticipare e progettare il cambiamento è essenziale per prevenire l’erosione delle prestazioni nel mondo reale nel tempo.\n\n\n13.4.10 Gestire il Debito Tecnico nelle Fasi Iniziali\nÈ comprensibile che il debito tecnico si accumuli naturalmente nelle prime fasi di sviluppo del modello. Quando si punta a creare rapidamente modelli MVP, i team spesso hanno bisogno di informazioni più complete su quali componenti raggiungeranno la scala o richiederanno modifiche. È previsto un po’ di lavoro differito.\nTuttavia, anche i sistemi iniziali frammentati dovrebbero seguire principi come “Flexible Foundations” per evitare di mettersi nei guai:\n\nIl codice modulare e le librerie riutilizzabili consentono di scambiare i componenti in un secondo momento\nL’accoppiamento debole tra modelli, archivi dati e logica aziendale facilita il cambiamento\nI layer di astrazione nascondono i dettagli di implementazione che potrebbero cambiare nel tempo\nIl servizio di modelli containerizzati mantiene aperte le opzioni sui requisiti di distribuzione\n\nLe decisioni che sembrano ragionevoli al momento possono limitare seriamente la flessibilità futura. Ad esempio, incorporare la logica aziendale chiave nel codice modello anziché tenerla separata rende estremamente difficili le modifiche successive al modello.\nCon una progettazione ponderata, tuttavia, è possibile creare rapidamente all’inizio mantenendo gradi di libertà per migliorare. Man mano che il sistema matura, emergono prudenti punti di interruzione in cui l’introduzione di nuove architetture in modo proattivo evita massicce rilavorazioni in futuro. In questo modo si bilanciano le urgenti tempistiche con la riduzione delle future cascate di correzione.\n\n\n13.4.11 Riepilogo\nSebbene il debito finanziario sia una buona metafora per comprendere i compromessi, differisce dalla misurabilità del debito tecnico. Il debito tecnico deve essere completamente monitorato e quantificato. Ciò rende difficile per i team gestire i compromessi tra muoversi rapidamente e introdurre intrinsecamente più debito rispetto al prendersi il tempo per ripagare tale debito.\nIl documento Hidden Technical Debt of Machine Learning Systems diffonde la consapevolezza delle sfumature del debito tecnologico specifico del sistema ML. Incoraggia un ulteriore sviluppo nell’ampia area del ML manutenibile.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Operazioni di ML</span>"
    ]
  },
  {
    "objectID": "contents/core/ops/ops.it.html#sec-roles-and_resp-ops",
    "href": "contents/core/ops/ops.it.html#sec-roles-and_resp-ops",
    "title": "13  Operazioni di ML",
    "section": "13.5 Ruoli e Responsabilità",
    "text": "13.5 Ruoli e Responsabilità\nData la vastità di MLOps, l’implementazione di successo di sistemi ML richiede competenze diversificate e una stretta collaborazione tra persone con diverse aree di competenza. Mentre gli scienziati dei dati creano i modelli ML di base, è necessario un lavoro di squadra interfunzionale per distribuire con successo questi modelli in ambienti di produzione e consentire loro di fornire un valore aziendale sostenibile.\nMLOps fornisce il framework e le pratiche per coordinare gli sforzi di vari ruoli coinvolti nello sviluppo, nella distribuzione e nell’esecuzione di sistemi MLG. Collegare i “silos” tradizionali tra i team di dati, ingegneria e operazioni è fondamentale per il successo di MLOps. Abilitare una collaborazione senza soluzione di continuità attraverso il ciclo di vita dell’apprendimento automatico accelera la realizzazione dei vantaggi garantendo al contempo l’affidabilità e le prestazioni a lungo termine dei modelli ML.\nEsamineremo alcuni ruoli chiave coinvolti in MLOps e le loro responsabilità principali. Comprendere l’ampiezza delle competenze necessarie per rendere operativi i modelli ML guida l’assemblaggio dei team MLOps. Chiarisce inoltre come i flussi di lavoro tra i ruoli si adattano alla metodologia MLOps sovraordinata.\n\n13.5.1 Ingegneri dei Dati\nGli ingegneri dei dati sono responsabili della creazione e della manutenzione dell’infrastruttura dati e delle pipeline che alimentano i dati nei modelli ML. Garantiscono che i dati vengano trasferiti senza problemi dai sistemi di origine agli ambienti di archiviazione, elaborazione e progettazione delle funzionalità necessari per lo sviluppo e la distribuzione dei modelli ML. Le loro principali responsabilità includono:\n\nMigrare dati grezzi da database, sensori e app “on-prem” [in azienda], in data lake basati su cloud, come Amazon S3 o Google Cloud Storage. Ciò fornisce un’archiviazione economica e scalabile.\nCreare pipeline di dati con “scheduler” [pianificatori] di flussi di lavoro come Apache Airflow, Prefect e dbt. Questi estraggono i dati dalle sorgenti, li trasformano e li convalidano, e li caricano direttamente in destinazioni come data warehouse, feature store o per l’addestramento del modello.\nTrasformare dati grezzi e disordinati in set di dati strutturati e pronti per l’analisi. Ciò include la gestione di valori nulli o malformati, la deduplicazione, l’unione di origini dati disparate, l’aggregazione dei dati e la progettazione di nuove feature.\nManutenere componenti dell’infrastruttura dati come data warehouse cloud (Snowflake, Redshift, BigQuery), data lake e sistemi di gestione dei metadati. Provisioning e ottimizzazione dei sistemi di elaborazione dati.\nFornire e ottimizzare sistemi di elaborazione dati per una gestione e un’analisi dei dati efficiente e scalabile.\nDefinire i processi di versioning, backup e archiviazione dei dati per i set di dati e funzionalità ML e applicare policy di governance dei dati.\n\nAd esempio, un’azienda manifatturiera può utilizzare pipeline Apache Airflow per estrarre dati dei sensori dai PLC in fabbrica e trasferirli in un data lake Amazon S3. Gli ingegneri dei dati elaborerebbero poi questi dati grezzi per filtrarli, pulirli e unirli ai metadati del prodotto. Questi output della pipeline verrebbero quindi caricati in un data warehouse Snowflake da cui è possibile leggere le feature per l’addestramento e la previsione del modello.\nIl team di ingegneria dei dati crea e sostiene la base dati per uno sviluppo e un funzionamento affidabili del modello. Il loro lavoro consente agli scienziati dei dati e agli ingegneri ML di concentrarsi sulla creazione, l’addestramento e l’implementazione di modelli ML su larga scala.\n\n\n13.5.2 Data Scientist\nIl lavoro dei “data scientist” [scienziato dei dati] è concentrarsi sulla ricerca, sperimentazione, sviluppo e miglioramento continuo dei modelli ML. Sfruttano la loro competenza in statistica, modellazione e algoritmi per creare modelli ad alte prestazioni. Le loro principali responsabilità includono:\n\nCollaborare con team aziendali e di dati per identificare opportunità in cui ML può aggiungere valore, inquadrare il problema e definire metriche di successo.\nEseguire analisi esplorative dei dati per comprendere le relazioni nei dati, ricavare informazioni e identificare funzionalità rilevanti per la modellazione.\nRicercare e sperimentare diversi algoritmi ML e architetture di modelli in base al problema e alle caratteristiche dei dati e sfruttare librerie come TensorFlow, PyTorch e Keras.\nMassimizzare le prestazioni, addestrare e perfezionare i modelli regolando gli iperparametri, regolando le architetture delle reti neurali, l’ingegneria delle funzionalità, ecc.\nValutare le prestazioni del modello tramite metriche come accuratezza, AUC e punteggi F1 ed eseguire analisi degli errori per identificare aree di miglioramento.\nSviluppare nuove versioni del modello mediante l’integrazione di nuovi dati, test di diversi approcci, ottimizzazione del comportamento del modello e mantenimento della documentazione e della discendenza per i modelli.\n\nAd esempio, uno scienziato dei dati può sfruttare TensorFlow e TensorFlow Probability per sviluppare un modello di previsione della domanda per la pianificazione dell’inventario ala vendita al dettaglio. Itereranno su diversi modelli di sequenza come LSTM e sperimenteranno funzionalità derivate da dati di prodotto, vendite e stagionali. Il modello verrà valutato in base a metriche di errore rispetto alla domanda effettiva prima dell’implementazione. Lo scienziato dei dati monitora le prestazioni e riqualifica/migliora il modello man mano che arrivano nuovi dati.\nI data scientist guidano la creazione, il miglioramento e l’innovazione del modello attraverso la loro competenza nelle tecniche di ML. Collaborano strettamente con altri ruoli per garantire che i modelli creino il massimo impatto aziendale.\n\n\n13.5.3 ML Engineer\nGli “ingegneri ML” consentono ai modelli sviluppati dagli scienziati dei dati di essere prodotti e distribuiti su larga scala. La loro competenza fa sì che i modelli servano in modo affidabile alle previsioni nelle applicazioni e nei processi aziendali. Le loro principali responsabilità includono:\n\nPrendere modelli prototipo dagli scienziati dei dati e rafforzarli per gli ambienti di produzione tramite best practice di codifica.\nCreare API e microservizi per la distribuzione dei modelli utilizzando strumenti come Flask, FastAPI. Containerizzare i modelli con Docker.\nGestire le versioni dei modelli, sincronizzarli in produzione utilizzando pipeline CI/CD e implementare release canary, test A/B e procedure di rollback.\nOttimizzare le prestazioni dei modelli per elevata scalabilità, bassa latenza ed efficienza dei costi. Sfruttare compressione, quantizzazione e servizio multi-modello.\nMonitorare i modelli una volta in produzione e garantire affidabilità e precisione continue. Riqualificare periodicamente i modelli.\n\nAd esempio, un ingegnere ML può prendere un modello di rilevamento delle frodi TensorFlow sviluppato da data scientist e containerizzarlo utilizzando TensorFlow Serving per una distribuzione scalabile. Il modello verrebbe integrato nella pipeline di elaborazione delle transazioni dell’azienda tramite API. L’ingegnere ML implementa un registro dei modelli e una pipeline CI/CD utilizzando MLFlow e Jenkins per distribuire gli aggiornamenti del modello in modo affidabile. Gli ingegneri ML monitorano quindi il modello in esecuzione per prestazioni continue utilizzando strumenti come Prometheus e Grafana. Se l’accuratezza del modello diminuisce, avviano la riqualificazione e la distribuzione di una nuova versione del modello.\nIl team di ingegneria ML consente ai modelli di data science di progredire senza problemi in sistemi di produzione sostenibili e robusti. La loro competenza nella creazione di sistemi modulari e monitorati offre un valore aziendale continuo.\n\n\n13.5.4 DevOps Engineer\nGli “ingegneri DevOps” abilitano MLOps creando e gestendo l’infrastruttura sottostante per lo sviluppo, la distribuzione e il monitoraggio dei modelli ML. In quanto branca specializzata dell’ingegneria del software, DevOps si concentra sulla creazione di pipeline di automazione, architettura cloud e framework operativi. Le loro principali responsabilità includono:\n\nApprovvigionare e gestire l’infrastruttura cloud per i flussi di lavoro ML utilizzando strumenti IaC come Terraform, Docker e Kubernetes.\nSviluppare pipeline CI/CD per il riaddestramento, la convalida e la distribuzione del modello. Integrare strumenti ML nella pipeline, come MLflow e Kubeflow.\nMonitorare le prestazioni del modello e dell’infrastruttura tramite strumenti come Prometheus, Grafana, stack ELK. Creare allarmi e dashboard.\nImplementare pratiche di governance relative allo sviluppo, al test e alla promozione del modello per consentire riproducibilità e tracciabilità.\nEmbedding dei modelli ML nelle applicazioni. Espongono i modelli tramite API e microservizi per l’integrazione.\nOttimizzazione delle prestazioni e dei costi dell’infrastruttura e sfruttamento dell’autoscaling, delle istanze spot e della disponibilità in tutte le regioni.\n\nAd esempio, un ingegnere DevOps esegue il provisioning di un cluster Kubernetes su AWS utilizzando Terraform per eseguire lavori di training ML e distribuzione online. L’ingegnere crea una pipeline CI/CD in Jenkins, che attiva il riaddestramento del modello quando sono disponibili nuovi dati. Dopo il test automatizzato, il modello viene registrato con MLflow e distribuito nel cluster Kubernetes. L’ingegnere monitora quindi lo stato del cluster, l’utilizzo delle risorse del contenitore e la latenza dell’API utilizzando Prometheus e Grafana.\nIl team DevOps consente una rapida sperimentazione e distribuzioni affidabili per ML tramite competenze cloud, automazione e monitoraggio. Il loro lavoro massimizza l’impatto del modello riducendo al minimo il debito tecnico.\n\n\n13.5.5 Project Manager\nI project manager svolgono un ruolo fondamentale in MLOps coordinando le attività tra i team coinvolti nella distribuzione dei progetti ML. Aiutano a guidare l’allineamento, la “accountability” [affidabilità] ed accelerano i risultati. Le loro principali responsabilità includono:\n\nCollaborare con le parti interessate per definire obiettivi di progetto, metriche di successo, tempistiche e budget; delineare specifiche e “scope”.\nCreare un piano di progetto che comprenda acquisizione dati, sviluppo modello, configurazione infrastrutturale, distribuzione e monitoraggio.\nCoordinare i lavori di progettazione, sviluppo e test tra ingegneri dei dati, scienziati dei dati, ingegneri ML e ruoli DevOps.\nMonitorare i progressi e le milestone, identificare gli ostacoli e risolverli tramite azioni correttive e gestire rischi e problemi.\nFacilitare la comunicazione tramite report di stato, riunioni, workshop e documentazione e consentire una collaborazione senza interruzioni.\nGuidare l’aderenza alle tempistiche e al budget e aumentare i superamenti o le carenze previsti per la mitigazione.\n\nAd esempio, un project manager creerebbe un piano di progetto per sviluppare e migliorare un modello di previsione dell’abbandono dei clienti. Coordinare data engineer che creano pipeline di dati, data scientist che sperimentano modelli, ML engineer che producono modelli e DevOps che impostano l’infrastruttura di distribuzione. Il project manager monitora i progressi tramite milestone come preparazione del set di dati, prototipazione del modello, distribuzione e monitoraggio. Per attuare soluzioni preventive, evidenziano eventuali rischi, ritardi o problemi di budget.\nI project manager qualificati consentono ai team MLOps di lavorare in sinergia per fornire rapidamente il massimo valore aziendale dagli investimenti ML. La loro leadership e organizzazione si allineano con team diversi.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Operazioni di ML</span>"
    ]
  },
  {
    "objectID": "contents/core/ops/ops.it.html#mlops-tradizionali-e-mlops-embedded",
    "href": "contents/core/ops/ops.it.html#mlops-tradizionali-e-mlops-embedded",
    "title": "13  Operazioni di ML",
    "section": "13.6 MLOps Tradizionali e MLOps Embedded",
    "text": "13.6 MLOps Tradizionali e MLOps Embedded\nSulla base della nostra discussione su L’apprendimento “On-device” nel capitolo precedente, ora rivolgiamo la nostra attenzione al contesto più ampio dei sistemi embedded in MLOps. I vincoli e i requisiti unici degli ambienti embedded hanno un impatto significativo sull’implementazione di modelli e operazioni di apprendimento automatico. Come abbiamo discusso nei capitoli precedenti, i sistemi embedded introducono sfide uniche per MLOps a causa delle loro risorse limitate, della connettività intermittente e della necessità di un calcolo efficiente e consapevole del consumo energetico. A differenza degli ambienti cloud con abbondanti capacità di calcolo e storage, i dispositivi embedded spesso operano con capacità di memoria, potenza e elaborazione limitate, il che richiede un’attenta ottimizzazione dei flussi di lavoro. Queste limitazioni influenzano tutti gli aspetti di MLOps, dall’implementazione e raccolta dati al monitoraggio e agli aggiornamenti.\nNegli MLOps tradizionali, i modelli ML vengono in genere distribuiti in ambienti basati su cloud o server, con risorse abbondanti come potenza di calcolo e memoria. Questi ambienti facilitano il funzionamento regolare di modelli complessi che richiedono risorse di calcolo significative. Ad esempio, un modello di riconoscimento delle immagini basato su cloud potrebbe essere utilizzato da una piattaforma di social media per contrassegnare automaticamente le foto con etichette pertinenti. In questo caso, il modello può sfruttare le vaste risorse disponibili nel cloud per elaborare in modo efficiente grandi quantità di dati.\nD’altro canto, i MLOps embedded comportano la distribuzione di modelli ML su sistemi embedded, sistemi di calcolo specializzati progettati per eseguire funzioni specifiche all’interno di sistemi più grandi. I sistemi embedded sono in genere caratterizzati dalle loro risorse di calcolo e potenza limitate. Ad esempio, un modello ML potrebbe essere “embedded” in un termostato intelligente per ottimizzare il riscaldamento e il raffreddamento in base alle preferenze e alle abitudini dell’utente. Il modello deve essere ottimizzato per funzionare in modo efficiente sull’hardware limitato del termostato senza comprometterne le prestazioni o la precisione.\nLa differenza fondamentale tra i MLOps tradizionali e quelli embedded risiede nei vincoli di risorse del sistema embedded. Mentre gli MLOps tradizionali possono sfruttare abbondanti risorse cloud o server, gli MLOps embedded devono fare i conti con le limitazioni hardware su cui viene distribuito il modello. Ciò richiede un’attenta ottimizzazione e messa a punto del modello per garantire che possa fornire informazioni accurate e preziose entro i vincoli del sistema embedded.\nInoltre, gli MLOps embedded devono considerare le sfide uniche poste dall’integrazione dei modelli ML con altri componenti del sistema embedded. Ad esempio, il modello deve essere compatibile con il software e l’hardware del sistema e deve essere in grado di interfacciarsi senza problemi con altri componenti, come sensori o attuatori. Ciò richiede una profonda comprensione sia dei sistemi ML che di quelli integrati e una stretta collaborazione tra data scientist, ingegneri e altre parti interessate.\nQuindi, mentre gli MLOps tradizionali e gli MLOps embedded condividono l’obiettivo comune di distribuire e mantenere modelli ML in ambienti di produzione, le sfide uniche poste dai sistemi embedded richiedono un approccio specializzato. Gli MLOps embedded devono bilanciare attentamente la necessità di accuratezza e prestazioni del modello con i vincoli dell’hardware su cui viene distribuito il modello. Ciò richiede una profonda comprensione sia dei sistemi ML che di quelli embedded e una stretta collaborazione tra i vari stakeholder per garantire l’integrazione di successo dei modelli ML nei sistemi embedded.\nQuesta volta, raggrupperemo i sottoargomenti in categorie più ampie per semplificare la struttura del nostro processo di pensiero su MLOps. Questa struttura aiuterà a comprendere come i diversi aspetti di MLOps siano interconnessi e perché ciascuno sia importante per il funzionamento efficiente dei sistemi ML mentre discutiamo le sfide nel contesto dei sistemi embedded.\n\nGestione del Ciclo di Vita del Modello\n\nGestione dei Dati: Gestione dell’ingestione dei dati, convalida e controllo delle versioni.\nAddestramento dei Modelli: Tecniche e pratiche per un addestramento dei modelli efficace e scalabile.\nValutazione dei Modelli: Strategie per testare e convalidare le prestazioni dei modelli.\nDistribuzione dei modelli: Approcci per la distribuzione dei modelli in ambienti di produzione.\n\nIntegrazione di Sviluppo e Operazioni\n\nPipeline CI/CD: Integrazione dei modelli ML in pipeline di integrazione e distribuzione continue.\nGestione dell’infrastruttura: Impostazione e manutenzione dell’infrastruttura necessaria per il training e la distribuzione dei modelli.\nComunicazione e Collaborazione: Garanzia di una comunicazione e collaborazione fluide tra data scientist, ingegneri ML e team operativi.\n\nEccellenza operativa\n\nMonitoraggio: Tecniche per il monitoraggio delle prestazioni dei modelli, della deriva dei dati e dello stato operativo.\nGovernance: Implementazione di policy per la verificabilità, la conformità e le considerazioni etiche dei modelli.\n\n\n\n13.6.1 Gestione del Ciclo di Vita del Modello\n\nGestione dei Dati\nNei tradizionali MLOps centralizzati, i dati vengono aggregati in grandi dataset e data lake, poi elaborati su server cloud o “on-prem” [in sede]. Tuttavia, MLOps embedded si basa su dati decentralizzati da sensori locali sui dispositivi. I dispositivi raccolgono batch più piccoli di dati incrementali, spesso rumorosi e non strutturati. Con vincoli di connettività, questi dati non possono sempre essere trasmessi istantaneamente al cloud e devono essere memorizzati nella cache in modo intelligente ed elaborati all’edge.\nA causa della potenza di calcolo limitata sui dispositivi embedded, i dati si possono solo preelaborare e pulire in modo minimo prima della trasmissione. Il filtraggio e l’elaborazione anticipati avvengono nei gateway edge per ridurre i carichi di trasmissione. Mentre si sfrutta l’archiviazione cloud, altre elaborazioni e archiviazioni avvengono all’edge per tenere conto della connettività intermittente. I dispositivi identificano e trasmettono solo i sottoinsiemi di dati più critici al cloud.\nAnche l’etichettatura richiede un accesso centralizzato ai dati, che richiede tecniche più automatizzate come l’apprendimento federato, in cui i dispositivi etichettano in modo collaborativo i dati dei peer. Con i dispositivi edge personali, la privacy dei dati e le normative sono preoccupazioni critiche. La raccolta, la trasmissione e l’archiviazione dei dati devono essere sicure e conformi.\nAd esempio, uno smartwatch può raccogliere il conteggio dei passi giornalieri, la frequenza cardiaca e le coordinate GPS. Questi dati vengono memorizzati nella cache locale e trasmessi a un gateway edge quando è disponibile il WiFi: il gateway elabora e filtra i dati prima di sincronizzare i sottoinsiemi rilevanti con la piattaforma cloud per riaddestrare i modelli.\n\n\nAddestramento del Modello\nNei tradizionali MLOps centralizzati, i modelli vengono addestrati utilizzando dati abbondanti tramite deep learning su server GPU cloud ad alta potenza. Tuttavia, glii MLOps embedded necessitano di maggiore supporto in termini di complessità del modello, disponibilità dei dati e risorse di elaborazione per l’addestramento.\nIl volume di dati aggregati è molto più basso, spesso richiedendo tecniche come l’apprendimento federato tra dispositivi per creare set di addestramento. La natura specializzata dei dati edge limita anche i set di dati pubblici per il pre-addestramento. Per questioni di privacy, i campioni di dati devono essere strettamente controllati e resi anonimi ove possibile.\nInoltre, i modelli devono utilizzare architetture semplificate ottimizzate per hardware edge a bassa potenza. Date le limitazioni di elaborazione, le GPU di fascia alta sono inaccessibili per un deep learning intensivo. L’addestramento sfrutta server edge e cluster a bassa potenza con approcci distribuiti per spartire il carico.\nIl “transfer learning” emerge come una strategia cruciale per affrontare la scarsità di dati e l’irregolarità nell’apprendimento automatico, in particolare negli scenari di edge computing. Come illustrato in Figura 13.5, questo approccio prevede il pre-training di modelli su grandi set di dati pubblici e la loro successiva messa a punto su dati edge specifici del dominio. La figura rappresenta una rete neurale in cui gli strati iniziali (da \\(W_{A1}\\) a \\(W_{A4}\\)), responsabili dell’estrazione delle caratteristiche generali, sono congelati (indicati da una linea tratteggiata verde). Questi layer conservano la conoscenza delle attività precedenti, accelerando l’apprendimento e riducendo i requisiti di risorse. Gli ultimi layer (da \\(W_{A5}\\) a \\(W_{A7}\\)), oltre la linea tratteggiata blu, sono ottimizzati per l’attività specifica, concentrandosi sull’apprendimento delle feature specifiche dell’attività.\n\n\n\n\n\n\nFigura 13.5: Trasferimento dell’apprendimento in MLOps. Fonte: HarvardX.\n\n\n\nQuesto metodo non solo mitiga la scarsità di dati, ma si adatta anche alla natura decentralizzata dei dati embedded. Inoltre, tecniche come l’apprendimento incrementale sul dispositivo possono personalizzare ulteriormente i modelli in base a casi d’uso specifici. La mancanza di dati ampiamente etichettati in molti domini motiva anche l’uso di tecniche semi-supervisionate, che completano l’approccio di apprendimento per trasferimento. Sfruttando le conoscenze preesistenti e adattandole a compiti specializzati, l’apprendimento per trasferimento all’interno di un framework MLOps consente ai modelli di ottenere prestazioni più elevate con meno risorse, anche in ambienti con vincoli di dati.\nAd esempio, un assistente domestico intelligente può pre-addestrare un modello di riconoscimento audio su clip YouTube pubbliche, il che aiuta a eseguire il bootstrap con conoscenze generali. Quindi trasferisce l’apprendimento a un piccolo campione di dati domestici per classificare elettrodomestici ed eventi personalizzati, specializzandosi nel modello. Il modello si trasforma in una rete neurale leggera ottimizzata per dispositivi abilitati al microfono in tutta la casa.\nPertanto, gli MLOps embedded affrontano sfide acute nella costruzione di set di dati di training, nella progettazione di modelli efficienti e nella distribuzione del calcolo per lo sviluppo del modello rispetto alle impostazioni tradizionali. Dati i vincoli embedded, è necessario un attento adattamento, come l’apprendimento tramite trasferimento e il training distribuito, per addestrare i modelli.\n\n\nValutazione del Modello\nNei tradizionali MLOps centralizzati, i modelli vengono valutati principalmente utilizzando metriche di accuratezza e dataset di test di holdout. Tuttavia, gli MLOps embedded richiedono una valutazione più olistica che tenga conto dei vincoli di sistema oltre all’accuratezza.\nI modelli devono essere testati in anticipo e spesso su hardware edge distribuito che copre diverse configurazioni. Oltre all’accuratezza, fattori come latenza, utilizzo della CPU, ingombro di memoria e consumo energetico sono criteri di valutazione critici. I modelli vengono selezionati in base a compromessi tra queste metriche per soddisfare i vincoli dei dispositivi edge.\nAnche la deriva dei dati deve essere monitorata, dove i modelli addestrati sui dati cloud degradano in accuratezza nel tempo sui dati edge locali. I dati embedded hanno spesso una maggiore variabilità rispetto ai set di addestramento centralizzati. Valutare i modelli su diversi campioni di dati edge operativi è fondamentale. Ma a volte, ottenere i dati per monitorare la deriva può essere difficile se questi dispositivi sono in circolazione e la comunicazione è una barriera.\nIl monitoraggio continuo fornisce visibilità sulle prestazioni del mondo reale dopo l’implementazione, rivelando colli di bottiglia non evidenziati durante i test. Ad esempio, un aggiornamento del modello di una smart camera potrebbe essere inizialmente testato su 100 telecamere e poi annullato se si osserva un calo della precisione, prima di essere esteso a tutte le 5000 telecamere.\n\n\nDistribuzione del Modello\nNegli MLOps tradizionali, le nuove versioni del modello vengono distribuite direttamente sui server tramite endpoint API. Tuttavia, i dispositivi embedded richiedono meccanismi di distribuzione ottimizzati per ricevere modelli aggiornati. Gli aggiornamenti over-the-air (OTA) forniscono un approccio standardizzato alla distribuzione wireless di nuove versioni di software o firmware ai dispositivi embedded. Invece dell’accesso API diretto, i pacchetti OTA consentono la distribuzione remota di modelli e dipendenze come bundle pre-costruiti. In alternativa, l’apprendimento federato consente aggiornamenti del modello senza accesso diretto ai dati di training grezzi. Questo approccio decentralizzato ha il potenziale per un miglioramento continuo del modello, ma necessita di piattaforme MLOps robuste.\nLa distribuzione del modello si basa su interfacce fisiche come connessioni seriali USB o UART per dispositivi profondamente embedded privi di connettività. Il packaging del modello segue ancora principi simili agli aggiornamenti OTA, ma il meccanismo di distribuzione è adattato alle capacità dell’hardware edge. Inoltre, spesso vengono utilizzati protocolli OTA specializzati ottimizzati per reti IoT anziché protocolli WiFi o Bluetooth standard. I fattori chiave includono efficienza, affidabilità, sicurezza e telemetria, come il monitoraggio dei progressi, soluzioni come Mender. Io fornisce servizi OTA incentrati su embedded che gestiscono aggiornamenti differenziali tra flotte di dispositivi.\nFigura 13.6 presenta una panoramica di “Model Lifecycle Management” in un contesto MLOps, illustrando il flusso dallo sviluppo (in alto a sinistra) alla distribuzione e al monitoraggio (in basso a destra). Il processo inizia con lo sviluppo ML, in cui il codice e le configurazioni sono “version-controlled”. La gestione dei dati e dei modelli è fondamentale per il processo, coinvolgendo set di dati e repository di funzionalità. Training continuo, conversione del modello e registro del modello sono fasi chiave nell’operazionalizzazione della training. La distribuzione del modello include la fornitura del modello e la gestione dei log di fornitura. Sono in atto meccanismi di allarme per segnalare i problemi, che alimentano il monitoraggio continuo per garantire le prestazioni e l’affidabilità del modello nel tempo. Questo approccio integrato garantisce che i modelli siano sviluppati e mantenuti in modo efficace durante tutto il loro ciclo di vita.\n\n\n\n\n\n\nFigura 13.6: Gestione del ciclo di vita del modello. Fonte: HarvardX.\n\n\n\n\n\n\n13.6.2 Integrazione di Sviluppo e Operazioni\n\nPipeline CI/CD\nNelle MLOps tradizionali, una solida infrastruttura CI/CD come Jenkins e Kubernetes consente l’automazione della pipeline per la distribuzione di modelli su larga scala. Tuttavia, le MLOps embedded necessitano di questa infrastruttura centralizzata e di flussi di lavoro CI/CD più personalizzati per i dispositivi edge.\nLa creazione di pipeline CI/CD deve tenere conto di un panorama frammentato di diverse versioni hardware, firmware e vincoli di connettività. Non esiste una piattaforma standard per orchestrare le pipeline e il supporto degli strumenti è più limitato.\nI test devono coprire in anticipo questo ampio spettro di dispositivi embedded target, il che è difficile senza un accesso centralizzato. Le aziende devono investire molto nell’acquisizione e nella gestione dell’infrastruttura di test nell’ecosistema embedded eterogeneo.\nGli aggiornamenti over-the-air richiedono la configurazione di server specializzati per distribuire in modo sicuro i bundle di modelli ai dispositivi sul campo. Anche le procedure di rollout e rollback devono essere attentamente personalizzate per particolari famiglie di dispositivi.\nCon gli strumenti CI/CD tradizionali meno applicabili, le MLOps embedded si affidano maggiormente a script personalizzati e integrazione. Le aziende adottano approcci diversi, dai framework open source alle soluzioni completamente interne. Una stretta integrazione tra sviluppatori, ingegneri edge e clienti finali stabilisce processi di rilascio affidabili.\nPertanto, gli MLOps embedded non possono sfruttare l’infrastruttura cloud centralizzata per CI/CD. Le aziende combinano pipeline personalizzate, infrastruttura di test e distribuzione OTA per distribuire modelli su sistemi edge frammentati e disconnessi.\n\n\nGestione dell’Infrastruttura\nNei tradizionali MLOps centralizzati, l’infrastruttura comporta l’approvvigionamento di server cloud, GPU e reti ad alta larghezza di banda per carichi di lavoro intensivi come l’addestramento di modelli e la fornitura di previsioni su larga scala. Tuttavia, gli MLOps embedded richiedono un’infrastruttura più eterogenea che si estende su dispositivi edge, gateway e cloud.\nI dispositivi edge come i sensori catturano e preelaborano i dati localmente prima della trasmissione intermittente per evitare di sovraccaricare le reti: i gateway aggregano ed elaborano i dati dei dispositivi prima di inviare sottoinsiemi selezionati al cloud per l’addestramento e l’analisi. Il cloud fornisce gestione centralizzata ed elaborazione supplementare.\nQuesta infrastruttura necessita di una stretta integrazione e bilanciamento dei carichi di elaborazione e comunicazione. La larghezza di banda di rete è limitata, il che richiede un attento filtraggio e compressione dei dati. Le capacità di elaborazione edge sono modeste rispetto al cloud, imponendo vincoli di ottimizzazione.\nLa gestione di aggiornamenti OTA sicuri su grandi flotte di dispositivi presenta sfide all’edge. I rollout devono essere incrementali e pronti per il rollback per una rapida mitigazione. Dato l’ambiente decentralizzato, l’aggiornamento dell’infrastruttura edge richiede coordinamento.\nAd esempio, un impianto industriale può eseguire l’elaborazione di base del segnale sui sensori prima di inviare i dati a un gateway on-prem. Il gateway gestisce l’aggregazione dei dati, il monitoraggio dell’infrastruttura e gli aggiornamenti OTA. Solo i dati curati vengono trasmessi al cloud per analisi avanzate e riaddestramento del modello.\nMLOps embedded richiede una gestione olistica dell’infrastruttura distribuita che abbraccia edge vincolato, gateway e cloud centralizzato. I carichi di lavoro sono bilanciati tra i livelli tenendo conto delle sfide di connettività, elaborazione e sicurezza.\n\n\nComunicazione e Collaborazione\nNelle MLOps tradizionali, la collaborazione tende a concentrarsi su data scientist, ingegneri ML e team DevOps. Tuttavia, le MLOps embedded richiedono un coordinamento interfunzionale più stretto tra ruoli aggiuntivi per affrontare i vincoli di sistema.\nGli ingegneri edge ottimizzano le architetture dei modelli per gli ambienti hardware target. Forniscono feedback ai data scientist durante lo sviluppo in modo che i modelli si adattino anticipatamente alle capacità dei dispositivi. Analogamente, i team di prodotto definiscono i requisiti operativi informati dai contesti degli utenti finali.\nCon più stakeholder nell’ecosistema embedded, i canali di comunicazione devono facilitare la condivisione delle informazioni tra team centralizzati e remoti. Il monitoraggio dei problemi e la gestione dei progetti garantiscono l’allineamento.\nGli strumenti collaborativi ottimizzano i modelli per dispositivi specifici. I data scientist possono registrare i problemi replicati dai dispositivi sul campo in modo che i modelli siano specializzati in dati di nicchia. L’accesso remoto ai dispositivi facilita il debug e la raccolta dati.\nAd esempio, i data scientist possono collaborare con i team sul campo che gestiscono flotte di turbine eoliche per recuperare campioni di dati operativi. Questi dati vengono utilizzati per specializzare i modelli rilevando anomalie specifiche per quella classe di turbine. Gli aggiornamenti dei modelli vengono testati in simulazioni e rivisti dagli ingegneri prima dell’implementazione sul campo.\nGli MLOps embedded impongono un coordinamento continuo tra data scientist, ingegneri, clienti finali e altre parti interessate durante l’intero ciclo di vita del ML. Grazie a una stretta collaborazione, i modelli possono essere personalizzati e ottimizzati per i dispositivi edge mirati.\n\n\n\n13.6.3 Eccellenza operativa\n\nMonitoraggio\nIl monitoraggio MLOps tradizionale si concentra sul monitoraggio centralizzato dell’accuratezza del modello, delle metriche delle prestazioni e della deriva dei dati. Tuttavia, MLOps embedded deve tenere conto del monitoraggio decentralizzato su diversi dispositivi e ambienti edge.\nI dispositivi edge richiedono una raccolta dati ottimizzata per trasmettere metriche di monitoraggio chiave senza sovraccaricare le reti. Le metriche aiutano a valutare le prestazioni del modello, i pattern di dati, l’utilizzo delle risorse e altri comportamenti sui dispositivi remoti.\nCon una connettività limitata, vengono eseguite più analisi all’edge prima di aggregare le informazioni centralmente. I gateway svolgono un ruolo chiave nel monitoraggio dello stato di salute della flotta e nel coordinamento degli aggiornamenti software. Gli indicatori confermati vengono infine propagati al cloud.\nUn’ampia copertura dei dispositivi è impegnativa ma critica. Possono sorgere problemi specifici per determinati tipi di dispositivi, quindi il monitoraggio deve coprire l’intero spettro. Le distribuzioni “canary” aiutano a testare i processi di monitoraggio prima del ridimensionamento.\nIl rilevamento delle anomalie identifica gli incidenti che richiedono il rollback dei modelli o la riqualificazione su nuovi dati. Tuttavia, l’interpretazione degli allarmi richiede la comprensione dei contesti dei dispositivi univoci in base all’input di ingegneri e clienti.\nAd esempio, una casa automobilistica può monitorare i veicoli autonomi per gli indicatori di degradazione del modello utilizzando la memorizzazione nella cache, l’aggregazione e i flussi in tempo reale. Gli ingegneri valutano quando le anomalie identificate garantiscono gli aggiornamenti OTA per migliorare i modelli in base a fattori come la posizione e l’età del veicolo.\nIl monitoraggio MLOps embedded fornisce osservabilità nelle prestazioni del modello e del sistema in ambienti edge decentralizzati. Un’attenta raccolta, analisi e collaborazione dei dati fornisce informazioni significative per mantenere l’affidabilità.\n\n\nGovernance\nNelle MLOps tradizionali, la governance si concentra sulla spiegabilità del modello, la correttezza e la conformità per i sistemi centralizzati. Tuttavia, le MLOps embedded devono anche affrontare le sfide di governance a livello di dispositivo relative alla privacy dei dati, alla sicurezza e alla protezione.\nCon i sensori che raccolgono dati personali e sensibili, la governance dei dati locali sui dispositivi è fondamentale. I controlli di accesso ai dati, l’anonimizzazione e la memorizzazione nella cache crittografata aiutano ad affrontare i rischi per la privacy e la conformità come HIPAA e GDPR. Gli aggiornamenti devono mantenere patch e impostazioni di sicurezza.\nLa governance della sicurezza considera gli impatti fisici del comportamento difettoso del dispositivo. I guasti potrebbero causare condizioni non sicure in veicoli, fabbriche e sistemi critici. Ridondanza, sistemi di sicurezza e sistemi di allarme aiutano a mitigare i rischi.\nLa governance tradizionale, come il monitoraggio dei bias e la spiegabilità del modello, rimane imperativa ma è più difficile da implementare per l’intelligenza artificiale embedded. Anche dare un’occhiata ai modelli black-box su dispositivi a basso consumo pone delle sfide.\nAd esempio, un dispositivo medico può cancellare i dati personali sul dispositivo prima della trasmissione. I rigidi protocolli di governance dei dati approvano gli aggiornamenti del modello. La spiegabilità del modello è limitata, ma l’attenzione è rivolta al rilevamento di comportamenti anomali. I sistemi di backup prevengono i guasti.\nLa governance MLOps embedded deve comprendere privacy, sicurezza, protezione, trasparenza ed etica. Sono necessarie tecniche specializzate e collaborazione di squadra per aiutare a stabilire fiducia e responsabilità all’interno di ambienti decentralizzati.\n\n\n\n13.6.4 Confronto\nTabella 13.2 evidenzia le somiglianze e le differenze tra MLOps Tradizionali e MLOps Embedded sulla base di tutto ciò che abbiamo imparato finora:\n\n\n\nTabella 13.2: Confronto tra le pratiche MLOps Tradizionali e quelle MLOps Embedded.\n\n\n\n\n\n\n\n\n\n\nArea\nMLOps Tradizionali\nMLOps Embedded\n\n\n\n\nGestione dei Dati\nGrandi set di dati, data lake, feature store\nAcquisizione dati sul dispositivo, edge caching ed elaborazione\n\n\nSviluppo del Modello\nSfrutta il deep learning, reti neurali complesse, addestramento GPU\nVincoli sulla complessità del modello, necessità di ottimizzazione\n\n\nDistribuzione\nCluster di server, distribuzione cloud, bassa latenza su larga scala\nDistribuzione OTA su dispositivi, connettività intermittente\n\n\nMonitoraggio\nDashboard, log, allarmi per le prestazioni del modello cloud\nMonitoraggio sul dispositivo di previsioni, utilizzo delle risorse\n\n\nRiqualificazione\nRi-addestramento dei modelli su nuovi dati\nApprendimento federato da dispositivi, ri-addestramento edge\n\n\nInfrastruttura\nInfrastruttura cloud dinamica\nInfrastruttura edge/cloud eterogenea\n\n\nCollaborazione\nMonitoraggio degli esperimenti condivisi e registro dei modelli\nCollaborazione per l’ottimizzazione specifica del dispositivo\n\n\n\n\n\n\nQuindi, mentre Embedded MLOps condivide i principi fondamentali di MLOps, si trova ad affrontare vincoli unici nell’adattare flussi di lavoro e infrastrutture specificamente per dispositivi edge con risorse limitate.\n\n\n13.6.5 Servizi MLOps Embedded\nNonostante la proliferazione di nuovi strumenti MLOps in risposta all’aumento della domanda, le sfide descritte in precedenza hanno limitato la disponibilità di tali strumenti negli ambienti dei sistemi embedded. Più di recente, nuovi strumenti come Edge Impulse (Janapa Reddi et al. 2023) hanno reso il processo di sviluppo un po’ più semplice, come descritto di seguito.\n\nJanapa Reddi, Vijay, Alexander Elium, Shawn Hymel, David Tischler, Daniel Situnayake, Carl Ward, Louis Moreau, et al. 2023. «Edge Impulse: An MLOps Platform for Tiny Machine Learning». Proceedings of Machine Learning and Systems 5.\n\nEdge Impulse\nEdge Impulse è una piattaforma di sviluppo end-to-end per la creazione e l’implementazione di modelli di apprendimento automatico su dispositivi edge come microcontrollori e piccoli processori. Rende l’apprendimento automatico embedded più accessibile agli sviluppatori di software attraverso la sua interfaccia web di facile utilizzo e strumenti integrati per la raccolta dati, lo sviluppo di modelli, l’ottimizzazione e l’implementazione. Le sue funzionalità principali includono quanto segue:\n\nFlusso di lavoro intuitivo drag-and-drop per la creazione di modelli ML senza bisogno di codifica\nStrumenti per l’acquisizione, l’etichettatura, la visualizzazione e la preelaborazione dei dati dai sensori\nScelta di architetture di modelli, tra cui reti neurali e apprendimento non supervisionato\nTecniche di ottimizzazione dei modelli per bilanciare metriche delle prestazioni e vincoli hardware\nDistribuzione senza soluzione di continuità su dispositivi edge tramite compilazione, SDK e benchmark\nFunzionalità di collaborazione per team e integrazione con altre piattaforme\n\nEdge Impulse offre una soluzione completa per la creazione di intelligenza embedded e l’avanzamento dell’apprendimento automatico, in particolare per gli sviluppatori con competenze limitate in scienza dei dati. Questa piattaforma consente lo sviluppo di modelli ML specializzati che funzionano in modo efficiente in piccoli ambienti di elaborazione. Come illustrato in Figura 13.7, Edge Impulse facilita il percorso dalla raccolta dati alla distribuzione del modello, evidenziando la sua interfaccia intuitiva e gli strumenti che semplificano la creazione di soluzioni ML incorporate, rendendole così accessibili a una gamma più ampia di sviluppatori e applicazioni.\n\n\n\n\n\n\nFigura 13.7: Panoramica di Edge Impulse. Fonte: Edge Impulse\n\n\n\n\nInterfaccia utente\nEdge Impulse è stato progettato con sette principi chiave: accessibilità, funzionalità end-to-end, un approccio incentrato sui dati, interattività, estensibilità, orientamento al team e supporto della community. L’interfaccia utente intuitiva, mostrata in Figura 13.8, guida gli sviluppatori di tutti i livelli di esperienza attraverso il caricamento dei dati, la selezione di un’architettura di modello, l’addestramento del modello e la sua distribuzione su piattaforme hardware pertinenti. Va notato che, come qualsiasi strumento, Edge Impulse è destinato ad assistere, non a sostituire, le considerazioni fondamentali come la determinazione se ML è una soluzione appropriata o l’acquisizione delle competenze di dominio richieste per una determinata applicazione.\n\n\n\n\n\n\nFigura 13.8: Schermata dell’interfaccia utente di Edge Impulse per la creazione di flussi di lavoro dai dati di input alle funzionalità di output.\n\n\n\nCiò che rende Edge Impulse degno di nota è il suo flusso di lavoro end-to-end completo ma intuitivo. Gli sviluppatori iniziano caricando i propri dati tramite l’interfaccia utente grafica (GUI) o gli strumenti dell’interfaccia a riga di comando (CLI), dopodiché possono esaminare campioni grezzi e visualizzare la distribuzione dei dati nelle suddivisioni di addestramento e test. Successivamente, gli utenti possono scegliere tra vari “blocchi” di pre-elaborazione per facilitare l’elaborazione del segnale digitale (DSP). Mentre vengono forniti valori di parametri predefiniti, gli utenti possono personalizzare i parametri in base alle proprie esigenze, osservando le considerazioni su memoria e la latenza visualizzate. Gli utenti possono scegliere facilmente la propria architettura di rete neurale, senza bisogno di alcun codice.\nGrazie all’editor visivo della piattaforma, gli utenti possono personalizzare i componenti dell’architettura e i parametri specifici, assicurandosi al contempo che il modello sia ancora addestrabile. Gli utenti possono anche sfruttare algoritmi di apprendimento non supervisionato, come il clustering K-means e i Gaussian Mixture Model (GMM).\n\n\nOttimizzazioni\nPer adattarsi ai vincoli di risorse delle applicazioni TinyML, Edge Impulse fornisce una “matrice di confusione” che riassume le metriche chiave delle prestazioni, tra cui accuratezza per classe e punteggi F1. La piattaforma chiarisce i compromessi tra prestazioni del modello, dimensioni e latenza utilizzando simulazioni in Renode e benchmarking specifici del dispositivo. Per i casi di utilizzo dei dati in streaming, uno strumento di calibrazione delle prestazioni sfrutta un algoritmo genetico per trovare configurazioni di post-elaborazione ideali che bilanciano tassi di falsa accettazione e falso rifiuto. Sono disponibili tecniche come quantizzazione, ottimizzazione del codice e ottimizzazione specifica del dispositivo per i modelli. Per la distribuzione, i modelli possono essere compilati in formati appropriati per i dispositivi edge target. Gli SDK del firmware nativi consentono anche la raccolta diretta dei dati sui dispositivi.\nOltre a semplificare lo sviluppo, Edge Impulse ridimensiona il processo di modellazione stesso. Una funzionalità chiave è EON Tuner, uno strumento di apprendimento automatico automatico (AutoML) che assiste gli utenti nell’ottimizzazione degli iperparametri in base ai vincoli di sistema. Esegue una ricerca casuale per generare rapidamente configurazioni per l’elaborazione del segnale digitale e le fasi di training. I modelli risultanti vengono visualizzati affinché l’utente possa selezionarli in base a metriche di prestazioni, memoria e latenza pertinenti. Per i dati, l’apprendimento attivo facilita il training su un piccolo sottoinsieme etichettato, seguito dall’etichettatura manuale o automatica di nuovi campioni in base alla vicinanza alle classi esistenti. Ciò espande l’efficienza dei dati.\n\n\nCasi d’Uso\nOltre all’accessibilità della piattaforma stessa, il team di Edge Impulse ha ampliato la base di conoscenza dell’ecosistema ML embedded. La piattaforma si presta ad ambienti accademici, essendo stata utilizzata in corsi online e workshop in loco a livello globale. Sono stati pubblicati numerosi casi di studio con casi d’uso di settore e di ricerca, in particolare Oura Ring, che utilizza ML per identificare i pattern del sonno. Il team ha reso i repository open source su GitHub, facilitando la crescita della comunità. Gli utenti possono anche rendere pubblici i progetti per condividere tecniche e scaricare librerie da condividere tramite Apache. L’accesso a livello di organizzazione consente la collaborazione sui flussi di lavoro.\nNel complesso, Edge Impulse è straordinariamente completo e integrabile per i flussi di lavoro degli sviluppatori. Piattaforme più grandi come Google e Microsoft si concentrano maggiormente sul cloud rispetto ai sistemi embedded. I framework TinyMLOps come Neuton AI e Latent AI offrono alcune funzionalità ma non hanno le capacità end-to-end di Edge Impulse. TensorFlow Lite Micro è il motore di inferenza standard grazie alla flessibilità, allo stato open source e all’integrazione di TensorFlow, ma utilizza più memoria e storage rispetto al compilatore EON di Edge Impulse. Altre piattaforme devono essere aggiornate, focalizzate sull’aspetto accademico o più versatili. In sintesi, Edge Impulse semplifica e amplia l’apprendimento automatico embedded tramite una piattaforma accessibile e automatizzata.\n\n\n\nLimitazioni\nSebbene Edge Impulse fornisca una pipeline accessibile per ML embedded, permangono importanti limitazioni e rischi. Una sfida fondamentale è la qualità e la disponibilità dei dati: i modelli sono validi solo quanto i dati utilizzati per addestrarli. Gli utenti devono disporre di campioni etichettati sufficienti che catturino l’ampiezza delle condizioni operative previste e delle modalità di errore. Le anomalie e i valori anomali etichettati sono critici, ma richiedono molto tempo per essere raccolti e identificati. Dati insufficienti o distorti comportano scarse prestazioni del modello indipendentemente dalle capacità dello strumento.\nAnche il deploying su dispositivi a bassa potenza presenta sfide intrinseche. I modelli ottimizzati potrebbero comunque dover richiedere più risorse per MCU a bassissimo consumo. Trovare il giusto equilibrio tra compressione e accuratezza richiede un po’ di sperimentazione. Lo strumento semplifica, ma deve comunque eliminare la necessità di competenze di base in ML ed elaborazione del segnale. Gli ambienti embedded limitano anche il debug e l’interpretabilità rispetto al cloud.\nSebbene siano ottenibili risultati impressionanti, gli utenti non dovrebbero considerare Edge Impulse come una soluzione “Push Button ML”. Un’attenta definizione dell’ambito del progetto, la raccolta dati, la valutazione del modello e il test sono comunque essenziali. Come con qualsiasi strumento di sviluppo, si consigliano aspettative ragionevoli e diligenza nell’applicazione. Tuttavia, Edge Impulse può accelerare la prototipazione e l’implementazione di ML embedded per gli sviluppatori disposti a investire lo sforzo di data science e ingegneria richiesto.\n\n\n\n\n\n\nEsercizio 13.1: Edge Impulse\n\n\n\n\n\nPronti a far salire di livello i vostri piccoli progetti di machine-learning? Combiniamo la potenza di Edge Impulse con le fantastiche visualizzazioni di Weights & Biases (WandB). In questo Colab, si imparerà a monitorare i progressi del training del modello come un professionista! Si immagini di vedere fantastici grafici del modello che diventa più intelligente, confrontando diverse versioni e assicurandovi che la vostra IA funzioni al meglio anche su dispositivi minuscoli.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Operazioni di ML</span>"
    ]
  },
  {
    "objectID": "contents/core/ops/ops.it.html#casi-di-studio",
    "href": "contents/core/ops/ops.it.html#casi-di-studio",
    "title": "13  Operazioni di ML",
    "section": "13.7 Casi di Studio",
    "text": "13.7 Casi di Studio\n\n13.7.1 Oura Ring\nOura Ring è un dispositivo indossabile che può misurare l’attività, il sonno e il recupero quando viene posizionato sul dito dell’utente. Utilizzando sensori per tracciare le metriche fisiologiche, il dispositivo utilizza ML embedded per prevedere le fasi del sonno. Per stabilire una base di legittimità nel settore, Oura ha condotto un esperimento di correlazione per valutare il successo del dispositivo nel prevedere le fasi del sonno rispetto a uno studio di base. Ciò ha portato a una solida correlazione del 62% rispetto alla base di riferimento dell’82-83%. Pertanto, il team ha deciso di determinare come migliorare ulteriormente le proprie prestazioni.\nLa prima sfida è stata ottenere dati migliori in termini sia di quantità che di qualità. Avrebbero potuto ospitare uno studio più ampio per ottenere un set di dati più completo, ma i dati sarebbero stati così rumorosi e grandi che sarebbe stato difficile aggregarli, ripulirli e analizzarli. È qui che entra in gioco Edge Impulse.\nAbbiamo condotto un massiccio studio sul sonno su 100 uomini e donne di età compresa tra 15 e 73 anni in tre continenti (Asia, Europa e Nord America). Oltre a indossare l’Oura Ring, i partecipanti erano tenuti a sottoporsi al test PSG [https://it.wikipedia.org/wiki/Polisonnografia] standard del settore, che ha fornito una “etichetta” per questo set di dati. Con 440 notti di sonno da parte di 106 partecipanti, il set di dati ha totalizzato 3.444 ore di lunghezza tra dati Ring e PSG. Con Edge Impulse, Oura ha potuto caricare e consolidare facilmente i dati da diverse fonti in un bucket S3 privato. Sono stati anche in grado di impostare una Data Pipeline per unire campioni di dati in file individuali e preelaborare i dati senza dover eseguire lo “scrubbing” [pulizia] manuale.\nCol tempo risparmiato nell’elaborazione dei dati grazie a Edge Impulse, il team Oura ha potuto concentrarsi sui driver chiave della propria previsione. Hanno estratto solo tre tipi di dati dei sensori: frequenza cardiaca, movimento e temperatura corporea. Dopo aver suddiviso i dati utilizzando la validazione incrociata a cinque livelli e classificato le fasi del sonno, il team ha ottenuto una correlazione del 79%, solo pochi punti percentuali in meno rispetto allo standard. Hanno prontamente distribuito due tipi di modelli di rilevamento del sonno: uno semplificato utilizzando solo l’accelerometro dell’anello e uno più completo sfruttando i segnali periferici mediati dal Autonomic Nervous System (ANS) [sistema nervoso autonomo] e le caratteristiche circadiane [ritmo cardiaco in 24 ore]. Con Edge Impulse, hanno in programma di condurre ulteriori analisi di diversi tipi di attività e sfruttare la scalabilità della piattaforma per continuare a sperimentare con diverse fonti di dati e sottoinsiemi di caratteristiche estratte.\nMentre la maggior parte della ricerca in ambito ML si concentra su fasi dominate dal modello, come il training e la messa a punto, questo caso di studio sottolinea l’importanza di un approccio olistico alle MLOps, in cui anche le fasi iniziali di aggregazione e pre-elaborazione dei dati hanno un impatto fondamentale sui risultati positivi.\n\n\n13.7.2 ClinAIOps\nDiamo un’occhiata a MLOps nel contesto del monitoraggio medico sanitario per comprendere meglio come MLOps “maturi” in un’implementazione nel mondo reale. In particolare, prendiamo in considerazione il continuous therapeutic monitoring (CTM) [monitoraggio terapeutico continuo] abilitato da dispositivi e sensori indossabili. Il CTM cattura dati fisiologici dettagliati dai pazienti, offrendo l’opportunità di aggiustamenti più frequenti e personalizzati ai trattamenti.\nI sensori indossabili abilitati per ML consentono un monitoraggio continuo fisiologico e dell’attività al di fuori delle cliniche, aprendo possibilità per aggiustamenti terapeutici tempestivi e basati sui dati. Ad esempio, i biosensori indossabili per l’insulina (Psoma e Kanthou 2023) e i sensori ECG da polso per il monitoraggio del glucosio (J. Li et al. 2021) possono automatizzare il dosaggio di insulina per il diabete, i sensori ECG e PPG da polso possono regolare gli anticoagulanti in base ai pattern di fibrillazione atriale (Attia et al. 2018; Guo et al. 2019), e gli accelerometri che tracciano l’andatura possono innescare cure preventive per la mobilità in declino negli anziani (Liu et al. 2022). La varietà di segnali che ora possono essere catturati passivamente e continuamente consente la titolazione e l’ottimizzazione della terapia su misura per le mutevoli esigenze di ogni paziente. Chiudendo il cerchio tra rilevamento fisiologico e risposta terapeutica con TinyML e apprendimento sul dispositivo, i dispositivi indossabili sono pronti a trasformare molte aree della medicina personalizzata.\n\nPsoma, Sotiria D., e Chryso Kanthou. 2023. «Wearable Insulin Biosensors for Diabetes Management: Advances and Challenges». Biosensors 13 (7): 719. https://doi.org/10.3390/bios13070719.\n\nLi, Jingzhen, Igbe Tobore, Yuhang Liu, Abhishek Kandwal, Lei Wang, e Zedong Nie. 2021. «Non-invasive Monitoring of Three Glucose Ranges Based On ECG By Using DBSCAN-CNN». IEEE Journal of Biomedical and Health Informatics 25 (9): 3340–50. https://doi.org/10.1109/jbhi.2021.3072628.\n\nAttia, Zachi I., Alan Sugrue, Samuel J. Asirvatham, Michael J. Ackerman, Suraj Kapa, Paul A. Friedman, e Peter A. Noseworthy. 2018. «Noninvasive assessment of dofetilide plasma concentration using a deep learning (neural network) analysis of the surface electrocardiogram: A proof of concept study». PLOS ONE 13 (8): e0201059. https://doi.org/10.1371/journal.pone.0201059.\n\nGuo, Yutao, Hao Wang, Hui Zhang, Tong Liu, Zhaoguang Liang, Yunlong Xia, Li Yan, et al. 2019. «Mobile Photoplethysmographic Technology to Detect Atrial Fibrillation». Journal of the American College of Cardiology 74 (19): 2365–75. https://doi.org/10.1016/j.jacc.2019.08.019.\n\nLiu, Yingcheng, Guo Zhang, Christopher G. Tarolli, Rumen Hristov, Stella Jensen-Roberts, Emma M. Waddell, Taylor L. Myers, et al. 2022. «Monitoring gait at home with radio waves in Parkinson’s disease: A marker of severity, progression, and medication response». Science Translational Medicine 14 (663): eadc9669. https://doi.org/10.1126/scitranslmed.adc9669.\nIl ML è molto promettente nell’analisi dei dati CTM per fornire raccomandazioni basate sui dati per gli aggiustamenti della terapia. Ma semplicemente distribuire modelli di intelligenza artificiale in “silos”, senza integrarli correttamente nei flussi di lavoro clinici e nel processo decisionale, può portare a una scarsa adozione o a risultati non ottimali. In altre parole, pensare solo a MLOps non è sufficiente per renderli utili nella pratica. Questo studio dimostra che sono necessari framework per incorporare intelligenza artificiale e CTM nella pratica clinica reale senza soluzione di continuità.\nQuesto caso di studio analizza “ClinAIOps” come modello per operazioni ML embedded in ambienti clinici complessi (Chen et al. 2023). Forniamo una panoramica del framework e del motivo per cui è necessario, esaminiamo un esempio di applicazione e discutiamo le principali sfide di implementazione relative al monitoraggio del modello, all’integrazione del flusso di lavoro e agli incentivi per gli stakeholder. L’analisi di esempi concreti come ClinAIOps illumina principi cruciali e best practice per operazioni AI affidabili ed efficaci in molti domini.\nI framework MLOps tradizionali non sono sufficienti per integrare il monitoraggio terapeutico continuo (CTM) e l’IA in contesti clinici per alcuni motivi chiave:\n\nMLOps si concentra sul ciclo di vita del modello ML: training, distribuzione, monitoraggio. Ma l’assistenza sanitaria implica il coordinamento di più stakeholder umani, pazienti e medici, non solo modelli.\nMLOps automatizza il monitoraggio e la gestione dei sistemi IT. Tuttavia, l’ottimizzazione della salute del paziente richiede cure personalizzate e supervisione umana, non solo automazione.\nCTM e l’erogazione dell’assistenza sanitaria sono sistemi sociotecnici complessi con molte parti mobili. MLOps non fornisce un framework per coordinare il processo decisionale umano e AI.\nLe considerazioni etiche relative all’AI sanitaria richiedono giudizio umano, supervisione e responsabilità. I framework MLOps non hanno processi per la supervisione etica.\nI dati sanitari dei pazienti sono altamente sensibili e regolamentati. MLOps da solo non garantisce la gestione delle informazioni sanitarie protette secondo gli standard normativi e di privacy.\nLa convalida clinica dei piani di trattamento guidati dall’AI è essenziale per l’adozione da parte del provider. MLOps non incorpora la valutazione specifica del dominio delle raccomandazioni del modello.\nL’ottimizzazione delle metriche sanitarie come i risultati dei pazienti richiede l’allineamento degli incentivi e dei flussi di lavoro delle parti interessate, che MLOps puramente incentrato sulla tecnologia trascura.\n\nPertanto, integrare efficacemente AI/ML e CTM nella pratica clinica richiede più di semplici modelli e pipeline di dati; richiede il coordinamento di complessi processi decisionali collaborativi tra esseri umani e AI, che ClinAIOps affronta tramite i suoi cicli di feedback multi-stakeholder.\n\nCicli di Feedback\nIl framework ClinAIOps, mostrato in Figura 13.9, fornisce questi meccanismi attraverso tre cicli di feedback. I cicli sono utili per coordinare le informazioni dal monitoraggio fisiologico continuo, l’esperienza del medico e la guida dell’IA tramite cicli di feedback, consentendo una medicina di precisione basata sui dati mantenendo al contempo la responsabilità umana. ClinAIOps fornisce un modello per un’efficace simbiosi uomo-IA nell’assistenza sanitaria: il paziente è al centro, fornendo sfide e obiettivi sanitari che informano il regime terapeutico; il medico supervisiona questo regime, fornendo input per gli aggiustamenti basati sui dati di monitoraggio continuo e sui report sanitari del paziente; mentre gli sviluppatori di IA svolgono un ruolo cruciale creando sistemi che generano allarmi per gli aggiornamenti della terapia, che il medico quindi esamina.\nQuesti cicli di feedback, di cui parleremo di seguito, aiutano a mantenere la responsabilità e il controllo del medico sui piani di trattamento esaminando i suggerimenti dell’IA prima che abbiano un impatto sui pazienti. Aiutano a personalizzare dinamicamente il comportamento e gli output del modello di IA in base allo stato di salute mutevole di ciascun paziente. Contribuiscono a migliorare l’accuratezza del modello e l’utilità clinica nel tempo, imparando dalle risposte del medico e del paziente. Facilitano il processo decisionale condiviso e l’assistenza personalizzata durante le interazioni paziente-medico. Consentono una rapida ottimizzazione delle terapie in base a dati frequenti del paziente che i medici non possono analizzare manualmente.\n\n\n\n\n\n\nFigura 13.9: Ciclo ClinAIOps. Fonte: Chen et al. (2023).\n\n\n\n\nCiclo Paziente-IA\nIl ciclo paziente-IA consente un’ottimizzazione frequente della terapia guidata dal monitoraggio fisiologico continuo. Ai pazienti vengono prescritti dispositivi indossabili come smartwatch o cerotti cutanei per raccogliere passivamente segnali sanitari rilevanti. Ad esempio, un paziente diabetico potrebbe avere un monitoraggio continuo del glucosio o un paziente con malattie cardiache potrebbe indossare un cerotto ECG. Un modello di IA analizza i flussi di dati sanitari longitudinali del paziente nel contesto delle sue cartelle cliniche elettroniche: diagnosi, esami di laboratorio, farmaci e dati demografici. Il modello di IA suggerisce modifiche al regime di trattamento su misura per quell’individuo, come la modifica di una dose di farmaco o di un programma di somministrazione. Piccole modifiche entro un intervallo di sicurezza pre-approvato possono essere apportate dal paziente in modo indipendente, mentre le modifiche più importanti vengono prima esaminate dal medico. Questo stretto feedback tra la fisiologia del paziente e la terapia guidata dall’IA consente ottimizzazioni tempestive basate sui dati come raccomandazioni automatizzate sul dosaggio di insulina basate sui livelli di glucosio in tempo reale per i pazienti diabetici.\n\n\nCiclo Clinico-IA\nIl ciclo clinico-IA consente la supervisione clinica sulle raccomandazioni generate dall’IA per garantire sicurezza e responsabilità. Il modello di IA fornisce al medico raccomandazioni terapeutiche e riepiloghi facilmente esaminabili dei dati rilevanti del paziente su cui si basano i suggerimenti. Ad esempio, un’IA può suggerire di ridurre la dose di farmaci per la pressione sanguigna di un paziente iperteso in base a letture costantemente basse. Il medico può accettare, rifiutare o modificare le modifiche alla prescrizione proposte dall’IA. Questo feedback del medico addestra e migliora ulteriormente il modello. Inoltre, il medico stabilisce i limiti per i tipi e l’entità delle modifiche al trattamento che l’IA può raccomandare autonomamente ai pazienti. Esaminando i suggerimenti dell’IA, il medico mantiene l’autorità di trattamento finale in base al proprio giudizio clinico e alla propria responsabilità. Questo ciclo consente loro di supervisionare i casi dei pazienti con l’assistenza dell’IA in modo efficiente.\n\n\nCiclo Paziente-Clinico\nInvece di una raccolta dati di routine, il medico può concentrarsi sull’interpretazione di pattern di dati di alto livello e sulla collaborazione con il paziente per stabilire obiettivi e priorità di salute. L’assistenza AI libererà anche il tempo dei medici, consentendo loro di concentrarsi maggiormente sull’ascolto delle storie e delle preoccupazioni dei pazienti. Ad esempio, il medico può discutere di cambiamenti di dieta ed esercizio fisico con un paziente diabetico per migliorare il controllo del glucosio in base ai dati di monitoraggio continuo. La frequenza degli appuntamenti può anche essere regolata dinamicamente in base ai progressi del paziente anziché seguire un calendario fisso. Liberato dalla raccolta di dati di base, il medico può fornire “coaching” e cure personalizzate a ciascun paziente informato dai suoi dati sanitari continui. La relazione paziente-medico diventa più produttiva e personalizzata.\n\n\n\nEsempio di Ipertensione\nConsideriamo un esempio. Secondo i “Centers for Disease Control and Prevention”, quasi la metà degli adulti soffre di ipertensione (48.1%, 119.9 milioni). L’ipertensione può essere gestita tramite ClinAIOps con l’aiuto di sensori indossabili utilizzando il seguente approccio:\n\nRaccolta Dati\nI dati raccolti includerebbero il monitoraggio continuo della pressione sanguigna tramite un dispositivo indossato al polso dotato di sensori per fotopletismografia (PPG) ed elettrocardiografia (ECG) per stimare la pressione sanguigna (Q. Zhang, Zhou, e Zeng 2017). Il dispositivo indossabile monitorerebbe anche l’attività fisica del paziente tramite accelerometri embedded. Il paziente registrerebbe tutti i farmaci antipertensivi assunti, insieme all’ora e alla dose. Verrebbero inoltre incorporati i dettagli demografici e la storia clinica del paziente dalla sua cartella clinica elettronica (EHR). Questi dati multimodali del mondo reale forniscono un contesto prezioso al modello di intelligenza artificiale per analizzare i pattern di pressione sanguigna del paziente, i livelli di attività, l’aderenza ai farmaci e le risposte alla terapia.\n\nZhang, Qingxue, Dian Zhou, e Xuan Zeng. 2017. «Highly wearable cuff-less blood pressure and heart rate monitoring with single-arm electrocardiogram and photoplethysmogram signals». BioMedical Engineering OnLine 16 (1): 23. https://doi.org/10.1186/s12938-017-0317-z.\n\n\nModello di Intelligenza Artificiale\nIl modello di intelligenza artificiale sul dispositivo analizzerebbe le tendenze continue della pressione sanguigna del paziente, i pattern circadiani, i livelli di attività fisica, i comportamenti di aderenza ai farmaci e altri contesti. Utilizzerebbe ML per prevedere dosi ottimali di farmaci antipertensivi e tempi per controllare la pressione sanguigna dell’individuo. Il modello invierebbe raccomandazioni di modifica del dosaggio direttamente al paziente per piccoli aggiustamenti o al medico revisore per l’approvazione per modifiche più significative. Osservando il feedback clinico sulle sue raccomandazioni e valutando i risultati ottenuti sulla pressione sanguigna nei pazienti, il modello di intelligenza artificiale potrebbe essere continuamente riqualificato per migliorarne le prestazioni. L’obiettivo è una gestione della pressione sanguigna completamente personalizzata ottimizzata per le esigenze e le risposte di ciascun paziente.\n\n\nCiclo Paziente-IA\nNel ciclo Paziente-IA, il paziente iperteso riceverebbe notifiche sul suo dispositivo indossabile o sull’app per smartphone collegata che raccomandano modifiche ai suoi farmaci antipertensivi. Per piccole modifiche della dose entro un intervallo di sicurezza predefinito, il paziente potrebbe implementare in modo indipendente la modifica suggerita dal modello di IA al suo regime. Tuttavia, il paziente deve ottenere l’approvazione del medico prima di modificare il dosaggio per modifiche più significative. Fornire raccomandazioni personalizzate e tempestive sui farmaci automatizza un elemento di autogestione dell’ipertensione per il paziente. Può migliorare la sua aderenza al regime e i risultati del trattamento. Il paziente è autorizzato a sfruttare le informazioni dell’IA per controllare meglio la sua pressione sanguigna.\n\n\nCiclo Clinico-IA\nNel ciclo Clinico-IA, il fornitore riceverebbe riepiloghi delle tendenze continue della pressione sanguigna del paziente e visualizzazioni dei suoi pattern di assunzione dei farmaci e dell’aderenza. Esaminano le modifiche al dosaggio antipertensivo suggerite dal modello AI e decidono se approvare, rifiutare o modificare le raccomandazioni prima che raggiungano il paziente. Il medico specifica anche i limiti di quanto l’AI può raccomandare in modo indipendente di modificare i dosaggi senza la supervisione del medico. Se la pressione sanguigna del paziente tende a livelli pericolosi, il sistema avvisa il medico in modo che possa intervenire tempestivamente e modificare i farmaci o richiedere una visita al pronto soccorso. Questo ciclo mantiene responsabilità e sicurezza consentendo al contempo al medico di sfruttare le intuizioni dell’AI mantenendo il medico responsabile dell’approvazione delle principali modifiche al trattamento.\n\n\nCiclo Paziente-Clinico\nNel ciclo Paziente-Clinico, mostrato in Figura 13.10, le visite di persona si concentrerebbero meno sulla raccolta di dati o sulle modifiche di base dei farmaci. Invece, il medico potrebbe interpretare tendenze e pattern di alto livello nei dati di monitoraggio continuo del paziente e avere discussioni mirate su dieta, esercizio fisico, gestione dello stress e altri cambiamenti nello stile di vita per migliorare il controllo della pressione sanguigna in modo olistico. La frequenza degli appuntamenti potrebbe essere ottimizzata dinamicamente in base alla stabilità del paziente anziché seguire un calendario fisso. Poiché il medico non avrebbe bisogno di rivedere tutti i dati granulari, potrebbe concentrarsi sulla fornitura di cure e raccomandazioni personalizzate durante le visite. Con il monitoraggio continuo e l’ottimizzazione assistita dall’intelligenza artificiale dei farmaci tra le visite, la relazione medico-paziente si concentra sugli obiettivi di benessere generale e diventa più incisiva. Questo approccio proattivo e personalizzato basato sui dati può aiutare a evitare complicazioni dell’ipertensione come ictus, insufficienza cardiaca e altre minacce alla salute e al benessere del paziente.\n\n\n\n\n\n\nFigura 13.10: Ciclo interattivo ClinAIOps. Fonte: Chen et al. (2023).\n\n\nChen, Emma, Shvetank Prakash, Vijay Janapa Reddi, David Kim, e Pranav Rajpurkar. 2023. «A framework for integrating artificial intelligence for clinical care with continuous therapeutic monitoring». Nature Biomedical Engineering, novembre. https://doi.org/10.1038/s41551-023-01115-0.\n\n\n\n\n\nMLOps vs. ClinAIOps\nL’esempio dell’ipertensione illustra bene perché i tradizionali MLOps sono insufficienti per molte applicazioni AI del mondo reale e perché sono invece necessari framework come ClinAIOps.\nCon l’ipertensione, il semplice sviluppo e distribuzione di un modello ML per la regolazione dei farmaci avrebbe successo solo se considerasse il contesto clinico più ampio. Il paziente, il medico e il sistema sanitario hanno preoccupazioni sulla definizione dell’adozione. Il modello AI non può ottimizzare da solo i risultati della pressione sanguigna: richiede l’integrazione con flussi di lavoro, comportamenti e incentivi.\n\nAlcune lacune chiave evidenziate dall’esempio in un approccio MLOps puro:\nIl modello stesso non avrebbe i dati dei pazienti del mondo reale su larga scala per raccomandare trattamenti in modo affidabile. ClinAIOps consente ciò raccogliendo feedback da medici e pazienti tramite monitoraggio continuo.\nI medici si fiderebbero delle raccomandazioni del modello solo con trasparenza, spiegabilità e responsabilità. ClinAIOps mantiene il medico informato per creare fiducia.\nI pazienti hanno bisogno di coaching e motivazione personalizzati, non solo di notifiche AI. Il ciclo paziente-clinico di ClinAIOps facilita questo.\nL’affidabilità dei sensori e l’accuratezza dei dati sarebbero sufficienti solo con la supervisione clinica. ClinAIOps convalida le raccomandazioni.\nLa responsabilità per i risultati del trattamento deve essere chiarita solo con un modello ML. ClinAIOps mantiene la responsabilità umana.\nI sistemi sanitari dovrebbero dimostrare il valore per cambiare i flussi di lavoro. ClinAIOps allinea le parti interessate.\n\nIl caso dell’ipertensione mostra chiaramente la necessità di guardare oltre il training e l’implementazione di un modello ML performante per considerare l’intero sistema sociotecnico umano-IA. Questa è la lacuna principale che ClinAIOps colma rispetto ai tradizionali MLOps. I tradizionali MLOps sono eccessivamente focalizzati sulla tecnologia per automatizzare lo sviluppo e l’implementazione del modello ML, mentre ClinAIOps incorpora il contesto clinico e il coordinamento umano-IA attraverso cicli di feedback multi-stakeholder.\nTabella 13.3 li confronta. Questa tabella evidenzia come, quando si implementa MLOps, sia necessario considerare più dei semplici modelli ML.\n\n\n\nTabella 13.3: Confronto tra operazioni MLOps e AI per uso clinico.\n\n\n\n\n\n\n\n\n\n\n\nMLOps tradizionali\nClinAIOps\n\n\n\n\nFocus\nSviluppo e distribuzione di modelli ML\nCoordinamento del processo decisionale umano e AI\n\n\nParti interessate\nData scientist, ingegneri IT\nPazienti, medici, sviluppatori AI\n\n\nCicli di feedback\nRiqualificazione del modello, monitoraggio\nPaziente-IA, clinico-IA, paziente-clinico\n\n\nObiettivo\nRendere operative le distribuzioni ML\nOttimizzare i risultati di salute del paziente\n\n\nProcessi\nPipeline e infrastruttura automatizzate\nIntegra flussi di lavoro clinici e supervisione\n\n\nConsiderazioni sui dati\nCreazione di set di dati di training\nPrivacy, etica, informazioni sanitarie protette\n\n\nValidazione del modello\nTest delle metriche delle prestazioni del modello\nValutazione clinica delle raccomandazioni\n\n\nImplementazione\nSi concentra sull’integrazione tecnica\nAllinea gli incentivi degli stakeholder umani\n\n\n\n\n\n\n\n\nRiepilogo\nIn ambiti complessi come l’assistenza sanitaria, l’implementazione di successo dell’IA richiede di andare oltre un focus ristretto sul training e il deploying di modelli ML performanti. Come illustrato nell’esempio dell’ipertensione, l’integrazione dell’IA nel mondo reale richiede il coordinamento di diverse parti interessate, l’allineamento degli incentivi, la convalida delle raccomandazioni e il mantenimento della responsabilità. Framework come ClinAIOps, che facilitano il processo decisionale collaborativo tra uomo e IA attraverso cicli di feedback integrati, sono necessari per affrontare queste sfide multiformi. Invece di automatizzare semplicemente le attività, l’IA deve aumentare le capacità umane e i flussi di lavoro clinici. Ciò consente all’IA di avere un impatto positivo sui risultati dei pazienti, sulla salute della popolazione e sull’efficienza dell’assistenza sanitaria.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Operazioni di ML</span>"
    ]
  },
  {
    "objectID": "contents/core/ops/ops.it.html#conclusione",
    "href": "contents/core/ops/ops.it.html#conclusione",
    "title": "13  Operazioni di ML",
    "section": "13.8 Conclusione",
    "text": "13.8 Conclusione\nL’ML embedded è pronto a trasformare molti settori abilitando le funzionalità AI direttamente su dispositivi edge come smartphone, sensori e hardware IoT. Tuttavia, lo sviluppo e l’implementazione di modelli TinyML su sistemi embedded con risorse limitate pone sfide uniche rispetto ai tradizionali MLOps basati su cloud.\nQuesto capitolo ha fornito un’analisi approfondita delle principali differenze tra MLOps tradizionali ed embedded nel ciclo di vita del modello, flussi di lavoro di sviluppo, gestione dell’infrastruttura e pratiche operative. Abbiamo discusso di come fattori come connettività intermittente, dati decentralizzati e computing limitato sul dispositivo richiedano tecniche innovative come apprendimento federato, inferenza sul dispositivo e ottimizzazione del modello. Pattern architettonici come apprendimento cross-device e infrastruttura edge-cloud gerarchica aiutano a mitigare i vincoli.\nAttraverso esempi concreti come Oura Ring e ClinAIOps, abbiamo dimostrato i principi applicati per MLOps embedded. I casi di studio hanno evidenziato considerazioni critiche che vanno oltre l’ingegneria ML di base, come l’allineamento degli incentivi delle parti interessate, il mantenimento della responsabilità e il coordinamento del processo decisionale tra uomo e IA. Ciò sottolinea la necessità di un approccio olistico che abbracci sia gli elementi tecnici che quelli umani.\nMentre gli MLOps embedded incontrano degli ostacoli, strumenti emergenti come Edge Impulse e lezioni dai pionieri aiutano ad accelerare l’innovazione del TinyML. Una solida comprensione dei principi fondamentali degli MLOps adattati agli ambienti embedded consentirà a più organizzazioni di superare i vincoli e fornire capacità di intelligenza artificiale distribuita. Man mano che i framework e le best practice maturano, l’integrazione fluida dell’ML nei dispositivi e nei processi edge trasformerà i settori attraverso l’intelligenza localizzata.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Operazioni di ML</span>"
    ]
  },
  {
    "objectID": "contents/core/ops/ops.it.html#sec-embedded-aiops-resource",
    "href": "contents/core/ops/ops.it.html#sec-embedded-aiops-resource",
    "title": "13  Operazioni di ML",
    "section": "13.9 Risorse",
    "text": "13.9 Risorse\nEcco un elenco curato di risorse per supportare studenti e insegnanti nei loro percorsi di apprendimento e insegnamento. Lavoriamo continuamente per espandere questa raccolta e presto aggiungeremo nuovi esercizi.\n\n\n\n\n\n\nSlide\n\n\n\n\n\nQueste slide sono uno strumento prezioso per gli insegnanti per tenere lezioni e per gli studenti per rivedere il materiale al proprio ritmo. Incoraggiamo sia gli studenti che gli insegnanti a sfruttare queste slide per migliorare la loro comprensione e facilitare un trasferimento efficace delle conoscenze.\n\nMLOps, DevOps, and AIOps.\nMLOps overview.\nTiny MLOps.\nMLOps: a use case.\nMLOps: Key Activities and Lifecycle.\nML Lifecycle.\nScaling TinyML: Challenges and Opportunities.\nOperazionalizzazione del Training:\n\nTraining Ops: CI/CD trigger.\nContinuous Integration.\nContinuous Deployment.\nProduction Deployment.\nProduction Deployment: Online Experimentation.\nTraining Ops Impact on MLOps.\n\nDeployment del Modello:\n\nScaling ML Into Production Deployment.\nContainers for Scaling ML Deployment.\nChallenges for Scaling TinyML Deployment: Part 1.\nChallenges for Scaling TinyML Deployment: Part 2.\nModel Deployment Impact on MLOps.\n\n\n\n\n\n\n\n\n\n\n\nVideo\n\n\n\n\n\n\nVideo 13.1\nVideo 13.2\nVideo 13.3\nVideo 13.4\n\n\n\n\n\n\n\n\n\n\nEsercizi\n\n\n\n\n\nPer rafforzare i concetti trattati in questo capitolo, abbiamo curato una serie di esercizi che sfidano gli studenti ad applicare le proprie conoscenze e ad approfondire la propria comprensione.\n\nEsercizio 13.1",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Operazioni di ML</span>"
    ]
  },
  {
    "objectID": "contents/core/privacy_security/privacy_security.it.html",
    "href": "contents/core/privacy_security/privacy_security.it.html",
    "title": "14  Sicurezza e Privacy",
    "section": "",
    "text": "14.1 Panoramica\nIl “Machine learning” [apprendimento automatico] si è evoluto notevolmente dalle sue origini accademiche, in cui la privacy non era una preoccupazione primaria. Con la migrazione del ML in applicazioni commerciali e consumer, i dati sono diventati più sensibili, comprendendo informazioni personali come comunicazioni, acquisti e dati sanitari. Questa esplosione di disponibilità di dati ha alimentato rapidi progressi nelle capacità del ML. Tuttavia, ha anche esposto nuovi rischi per la privacy, come dimostrato da incidenti come la fuga di dati di AOL nel 2006 e lo scandalo Cambridge Analytica.\nQuesti eventi hanno evidenziato la crescente necessità di affrontare la privacy nei sistemi ML. In questo capitolo, esploriamo insieme considerazioni sulla privacy e sulla sicurezza, poiché sono intrinsecamente collegate nel ML. Ad esempio, una telecamera di sicurezza domestica basata su ML deve proteggere i flussi video da accessi non autorizzati e fornire protezioni della privacy per garantire che solo gli utenti previsti possano visualizzare il filmato. Una violazione della sicurezza o della privacy potrebbe esporre momenti privati degli utenti.\nI sistemi ML embedded come assistenti intelligenti e dispositivi indossabili sono onnipresenti ed elaborano dati intimi degli utenti. Tuttavia, i loro vincoli computazionali spesso impediscono protocolli di sicurezza pesanti. I progettisti devono bilanciare le esigenze di prestazioni con rigorosi standard di sicurezza e privacy adattati alle limitazioni dell’hardware embedded.\nQuesto capitolo fornisce conoscenze essenziali per affrontare il complesso panorama di privacy e sicurezza dell’ML embedded. Esploreremo le vulnerabilità e tratteremo varie tecniche che migliorano la privacy e la sicurezza all’interno dei vincoli di risorse dei sistemi embedded.\nCi auguriamo che sviluppando una comprensione olistica dei rischi e delle misure di sicurezza, si acquisiranno i principi per sviluppare applicazioni ML embedded sicure ed etiche.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Sicurezza e Privacy</span>"
    ]
  },
  {
    "objectID": "contents/core/privacy_security/privacy_security.it.html#terminologia",
    "href": "contents/core/privacy_security/privacy_security.it.html#terminologia",
    "title": "14  Sicurezza e Privacy",
    "section": "14.2 Terminologia",
    "text": "14.2 Terminologia\nIn questo capitolo parleremo insieme di sicurezza e privacy, quindi ci sono termini chiave su cui dobbiamo essere chiari. Poiché questi termini sono concetti generali applicati in molti domini, vogliamo definire come si relazionano al contesto di questo capitolo e fornire esempi pertinenti per illustrare la loro applicazione.\n\nPrivacy: La capacità di controllare l’accesso ai dati sensibili degli utenti raccolti ed elaborati da un sistema. Nel machine learning, ciò implica garantire che le informazioni personali, come i dettagli finanziari o i dati biometrici, siano accessibili solo a individui autorizzati. Ad esempio, una telecamera di sicurezza domestica basata sull’apprendimento automatico potrebbe registrare filmati video e identificare i volti dei visitatori. Le preoccupazioni relative alla privacy riguardano chi può accedere, visualizzare o condividere questi dati sensibili.\nSicurezza: La pratica di protezione dei sistemi di apprendimento automatico e dei loro dati da accessi non autorizzati, hacking, furti e uso improprio. Un sistema sicuro salvaguarda i propri dati e le proprie operazioni per garantire integrità e riservatezza. Ad esempio, nel contesto della telecamera di sicurezza domestica, le misure di sicurezza impediscono agli hacker di intercettare feed video in diretta o di manomettere i filmati archiviati e garantiscono che il modello stesso rimanga intatto.\nMinaccia: Si riferisce a qualsiasi potenziale pericolo, attore malintenzionato o evento dannoso che mira a sfruttare le debolezze di un sistema per comprometterne la sicurezza o la privacy. Una minaccia è la forza o l’intento esterno che cerca di causare danni. Utilizzando l’esempio della telecamera di sicurezza domestica, una minaccia potrebbe coinvolgere un hacker che tenta di accedere a flussi live, rubare video archiviati o ingannare il sistema con falsi input per aggirare il riconoscimento facciale.\nVulnerabilità: Si riferisce a una debolezza, un difetto o una lacuna nel sistema che crea l’opportunità per una minaccia di avere successo. Le vulnerabilità sono i punti di esposizione che le minacce prendono di mira. Le vulnerabilità possono esistere in configurazioni hardware, software o di rete. Ad esempio, se la telecamera di sicurezza domestica si connette a Internet tramite una rete Wi-Fi non protetta, questa vulnerabilità potrebbe consentire agli aggressori di intercettare o manipolare i dati video.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Sicurezza e Privacy</span>"
    ]
  },
  {
    "objectID": "contents/core/privacy_security/privacy_security.it.html#precedenti-storici",
    "href": "contents/core/privacy_security/privacy_security.it.html#precedenti-storici",
    "title": "14  Sicurezza e Privacy",
    "section": "14.3 Precedenti Storici",
    "text": "14.3 Precedenti Storici\nSebbene le specifiche della sicurezza hardware dell’apprendimento automatico possano essere distinte, il campo dei sistemi embedded ha una storia di incidenti di sicurezza che forniscono lezioni fondamentali per tutti i sistemi connessi, compresi quelli che utilizzano ML. Ecco analisi dettagliate di violazioni passate:\n\n14.3.1 Stuxnet\nNel 2010, qualcosa di inaspettato è stato trovato su un computer in Iran: un virus informatico molto complicato che gli esperti non avevano mai visto prima. Stuxnet era un worm informatico dannoso che prendeva di mira i sistemi di controllo di supervisione e acquisizione dati (SCADA) ed era progettato per danneggiare il programma nucleare iraniano (Farwell e Rohozinski 2011). Stuxnet stava utilizzando quattro “exploit zero-day”, attacchi che sfruttano debolezze segrete nel software di cui nessuno è ancora a conoscenza. Ciò ha reso Stuxnet molto subdolo e difficile da rilevare.\n\nFarwell, James P., e Rafal Rohozinski. 2011. «Stuxnet and the Future of Cyber War». Survival 53 (1): 23–40. https://doi.org/10.1080/00396338.2011.555586.\nMa Stuxnet non è stato progettato per rubare informazioni o spiare le persone. Il suo obiettivo era la distruzione fisica, sabotare le centrifughe della centrale nucleare iraniana di Natanz! Quindi come ha fatto il virus a raggiungere i computer della centrale di Natanz, che avrebbe dovuto essere disconnessa dal mondo esterno per motivi di sicurezza? Gli esperti pensano che qualcuno abbia inserito una chiavetta USB contenente Stuxnet nella rete interna di Natanz. Ciò ha permesso al virus di “saltare” da un sistema esterno ai sistemi di controllo nucleare isolati e scatenare il caos.\nStuxnet era un malware incredibilmente avanzato creato dai governi nazionali per passare dal regno digitale alle infrastrutture del mondo reale. Ha preso di mira in modo specifico importanti macchine industriali, dove l’apprendimento automatico embedded è altamente applicabile in un modo mai visto prima. Il virus ha lanciato un segnale di allarme su come i sofisticati attacchi informatici potrebbero ora distruggere fisicamente apparecchiature e strutture.\nQuesta violazione è stata significativa a causa della sua sofisticatezza; Stuxnet ha preso di mira in modo specifico i “programmable logic controllers (PLC)” utilizzati per automatizzare processi elettromeccanici come la velocità delle centrifughe per l’arricchimento dell’uranio. Il worm sfruttava le vulnerabilità del sistema operativo Windows per ottenere l’accesso al software Siemens Step7 che controlla i PLC. Nonostante non sia un attacco diretto ai sistemi ML, Stuxnet è rilevante per tutti i sistemi embedded in quanto mostra il potenziale degli attori a livello statale per progettare attacchi che collegano il mondo informatico e quello fisico con effetti devastanti. Figura 14.1 spiega Stuxnet in modo più dettagliato.\n\n\n\n\n\n\nFigura 14.1: Spiegazione di Stuxnet. Fonte: IEEE Spectrum\n\n\n\n\n\n14.3.2 Hack della Jeep Cherokee\nL’hack della Jeep Cherokee è stato un evento rivoluzionario che ha dimostrato i rischi insiti nelle automobili sempre più connesse (Miller 2019). In una dimostrazione controllata, i ricercatori della sicurezza hanno sfruttato da remoto una vulnerabilità nel sistema di entertainment Uconnect, che aveva una connessione cellulare a Internet. Sono stati in grado di controllare il motore, la trasmissione e i freni del veicolo, allarmando l’industria automobilistica e spingendola a riconoscere le gravi implicazioni per la sicurezza delle vulnerabilità informatiche nei veicoli. Video 14.1 di seguito è riportato un breve documentario dell’attacco.\n\nMiller, Charlie. 2019. «Lessons learned from hacking a car». IEEE Design &amp; Test 36 (6): 7–9. https://doi.org/10.1109/mdat.2018.2863106.\n\n\n\n\n\n\nVideo 14.1: Hack della Jeep Cherokee\n\n\n\n\n\n\nSebbene non si sia trattato di un attacco a un sistema ML in sé, l’affidamento dei veicoli moderni ai sistemi embedded per funzioni critiche per la sicurezza presenta parallelismi significativi con l’implementazione di ML nei sistemi embedded, sottolineando la necessità di una sicurezza robusta a livello hardware.\n\n\n14.3.3 Botnet Mirai\nLa botnet Mirai ha coinvolto l’infezione di dispositivi in rete come fotocamere digitali e lettori DVR (Antonakakis et al. 2017). Nell’ottobre 2016, la botnet è stata utilizzata per condurre uno dei più grandi attacchi DDoS, interrompendo l’accesso a Internet negli Stati Uniti. L’attacco è stato possibile perché molti dispositivi utilizzavano nomi utente e password predefiniti, che sono stati facilmente sfruttati dal malware Mirai per controllare i dispositivi. Video 14.2 spiega come funziona la botnet Mirai.\n\nAntonakakis, Manos, Tim April, Michael Bailey, Matt Bernhard, Elie Bursztein, Jaime Cochran, Zakir Durumeric, et al. 2017. «Understanding the mirai botnet». In 26th USENIX security symposium (USENIX Security 17), 1093–1110.\n\n\n\n\n\n\nVideo 14.2: Botnet Mirai\n\n\n\n\n\n\nSebbene i dispositivi non fossero basati su ML, l’incidente è un duro promemoria di ciò che può accadere quando numerosi dispositivi embedded con scarsi controlli di sicurezza vengono collegati in rete, cosa che sta diventando sempre più comune con la crescita dei dispositivi IoT basati su ML.\n\n\n14.3.4 Implicazioni\nQueste violazioni storiche dimostrano gli effetti a cascata delle vulnerabilità hardware nei sistemi embedded. Ogni incidente offre un precedente per comprendere i rischi e progettare protocolli di sicurezza migliori. Ad esempio, la botnet Mirai evidenzia l’immenso potenziale distruttivo quando gli autori delle minacce possono ottenere il controllo su dispositivi in rete con sicurezza debole, una situazione che sta diventando sempre più comune con i sistemi ML. Molti dispositivi ML attuali funzionano come dispositivi “edge” pensati per raccogliere ed elaborare dati localmente prima di inviarli al cloud. Proprio come le telecamere e i DVR compromessi da Mirai, i dispositivi ML edge spesso si basano su hardware embedded come processori ARM ed eseguono sistemi operativi leggeri come Linux. Proteggere le credenziali del dispositivo è fondamentale.\nAllo stesso modo, l’hacking della Jeep Cherokee è stato un momento spartiacque per l’industria automobilistica. Ha esposto gravi vulnerabilità nei crescenti sistemi di veicoli connessi in rete e la loro mancanza di isolamento dai sistemi di guida principali come freni e sterzo. In risposta, i produttori di automobili hanno investito molto in nuove misure di sicurezza informatica, anche se probabilmente permangono delle lacune.\nChrysler ha effettuato un richiamo per correggere il software vulnerabile Uconnect, che consentiva l’exploit remoto. Ciò includeva l’aggiunta di protezioni a livello di rete per impedire l’accesso esterno non autorizzato e la compartimentazione dei sistemi di bordo per limitare i movimenti laterali. Sono stati aggiunti ulteriori livelli di crittografia per i comandi inviati tramite il bus CAN all’interno dei veicoli.\nL’incidente ha anche stimolato la creazione di nuovi standard e best practice per la sicurezza informatica. L’Auto-ISAC è stato istituito per consentire alle case automobilistiche di condividere informazioni e la NHTSA ha guidato i rischi di gestione. Sono state sviluppate nuove procedure di test e audit per valutare le vulnerabilità in modo proattivo. Gli effetti collaterali continuano a guidare il cambiamento nel settore automobilistico poiché le auto diventano sempre più definite dal software.\nSfortunatamente, i produttori spesso trascurano la sicurezza quando sviluppano nuovi dispositivi edge ML, utilizzando password predefinite, comunicazioni non crittografate, aggiornamenti firmware non protetti, ecc. Tali vulnerabilità potrebbero consentire agli aggressori di ottenere l’accesso e controllare i dispositivi su larga scala infettandoli con malware. Con una botnet di dispositivi ML compromessi, gli aggressori potrebbero sfruttare la loro potenza di calcolo aggregata per attacchi DDoS su infrastrutture critiche.\nSebbene questi eventi non abbiano coinvolto direttamente hardware di machine learning, i principi degli attacchi si estendono ai sistemi ML, che spesso coinvolgono dispositivi embedded e architetture di rete simili. Poiché l’hardware ML è sempre più integrato con il mondo fisico, proteggerlo da tali violazioni è fondamentale. L’evoluzione delle misure di sicurezza in risposta a questi incidenti fornisce preziose informazioni sulla protezione dei sistemi ML attuali e futuri da vulnerabilità analoghe.\nLa natura distribuita dei dispositivi edge ML significa che le minacce possono propagarsi rapidamente attraverso le reti. E se i dispositivi vengono utilizzati per scopi critici come dispositivi medici, controlli industriali o veicoli a guida autonoma, il potenziale danno fisico dei bot ML armati potrebbe essere grave. Proprio come Mirai ha dimostrato il potenziale pericoloso dei dispositivi IoT scarsamente protetti, la prova del nove per la sicurezza dell’hardware ML sarà quanto questi dispositivi siano vulnerabili o resilienti ad attacchi simili a worm. La posta in gioco aumenta man mano che il ML si diffonde in ambiti critici per la sicurezza, ponendo l’onere sui produttori e sugli operatori di sistema di incorporare le lezioni di Mirai.\nLa lezione è l’importanza di progettare per la sicurezza fin dall’inizio e di avere difese stratificate. Il caso Jeep evidenzia potenziali vulnerabilità per i sistemi ML in merito alle interfacce software esterne e all’isolamento tra sottosistemi. I produttori di dispositivi e piattaforme ML dovrebbero assumere un approccio proattivo e completo simile alla sicurezza piuttosto che lasciarlo come un ripensamento. Una risposta rapida e la diffusione delle best practice saranno cruciali man mano che le minacce si evolvono.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Sicurezza e Privacy</span>"
    ]
  },
  {
    "objectID": "contents/core/privacy_security/privacy_security.it.html#minacce-alla-sicurezza-per-i-modelli-ml",
    "href": "contents/core/privacy_security/privacy_security.it.html#minacce-alla-sicurezza-per-i-modelli-ml",
    "title": "14  Sicurezza e Privacy",
    "section": "14.4 Minacce alla Sicurezza per i Modelli ML",
    "text": "14.4 Minacce alla Sicurezza per i Modelli ML\nI modelli ML affrontano rischi per la sicurezza che possono comprometterne l’integrità, le prestazioni e l’affidabilità se non affrontati adeguatamente. Tra queste, spiccano tre minacce principali: il furto di modelli, in cui gli avversari rubano parametri di modelli proprietari e i dati sensibili in essi contenuti; l’avvelenamento dei dati, che compromette i modelli manomettendo i dati di training; e gli attacchi avversari, progettati per ingannare i modelli e indurli a fare previsioni errate o indesiderate. Discuteremo ciascuna di queste minacce in dettaglio e forniremo esempi di casi di studio per illustrare le loro implicazioni nel mondo reale.\n\n14.4.1 Furto di Modelli\nIl furto di modelli si verifica quando un aggressore ottiene l’accesso non autorizzato a un modello ML distribuito. La preoccupazione in questo caso è il furto della struttura del modello e dei parametri addestrati, nonché dei dati proprietari in esso contenuti (Ateniese et al. 2015). Il furto di modelli è una minaccia reale e crescente, come dimostrato da casi come quello dell’ex ingegnere di Google Anthony Levandowski, che presumibilmente ha rubato i progetti di auto a guida autonoma di Waymo e ha fondato un’azienda concorrente. Oltre all’impatto economico, il furto di modelli può seriamente compromettere la privacy e consentire ulteriori attacchi.\n\nAteniese, Giuseppe, Luigi V. Mancini, Angelo Spognardi, Antonio Villani, Domenico Vitali, e Giovanni Felici. 2015. «Hacking smart machines with smarter ones: How to extract meaningful data from machine learning classifiers». International Journal of Security and Networks 10 (3): 137. https://doi.org/10.1504/ijsn.2015.071829.\nAd esempio, si consideri un modello ML sviluppato per raccomandazioni personalizzate in un’applicazione di e-commerce. Se un concorrente ruba questo modello, ottiene informazioni su analisi aziendali, preferenze dei clienti e persino segreti commerciali racchiusi nei dati del modello. Gli aggressori potrebbero sfruttare i modelli rubati per creare input più efficaci per attacchi di “inversione del modello”, deducendo dettagli privati sui dati di addestramento del modello. Un modello di raccomandazione di e-commerce clonato potrebbe rivelare i comportamenti di acquisto e i dati demografici dei clienti.\nPer comprendere gli attacchi di “inversione del modello”, si consideri un sistema di riconoscimento facciale utilizzato per concedere l’accesso a strutture protette. Il sistema viene addestrato su un set di dati di foto dei dipendenti. Un aggressore potrebbe dedurre le caratteristiche del set di dati originale osservando l’output del modello su vari input. Ad esempio, supponiamo che il livello di confidenza del modello per un particolare volto sia significativamente più alto per un dato set di caratteristiche. In tal caso, un aggressore potrebbe dedurre che qualcuno con quelle caratteristiche è probabile che sia nel set di dati di addestramento.\nLa metodologia di “inversione del modello” in genere prevede i seguenti passaggi:\n\nAccesso agli Output del Modello: L’aggressore interroga il modello ML con dati di input e osserva gli output. Ciò avviene spesso tramite un’interfaccia legittima, come un’API pubblica.\nAnalisi dei Confidence Score: Per ogni input, il modello fornisce un “punteggio di confidenza” che riflette quanto l’input sia simile ai dati di training.\nReverse-Engineering: Analizzando i punteggi di confidenza o le probabilità di output, gli aggressori possono utilizzare tecniche di ottimizzazione per ricostruire ciò che ritengono sia vicino ai dati di input originali.\n\nUn esempio storico di tale vulnerabilità esplorata è stata la ricerca sugli attacchi di inversione contro il set di dati del premio Netflix degli Stati Uniti, in cui i ricercatori hanno dimostrato che era possibile conoscere le preferenze cinematografiche di un individuo, il che potrebbe portare a violazioni della privacy (Narayanan e Shmatikov 2006).\n\nNarayanan, Arvind, e Vitaly Shmatikov. 2006. «How To Break Anonymity of the Netflix Prize Dataset». CoRR. http://arxiv.org/abs/cs/0610105.\nIl furto di modelli implica che potrebbe portare a perdite economiche, minare il vantaggio competitivo e violare la privacy degli utenti. C’è anche il rischio di attacchi di inversione del modello, in cui un avversario potrebbe immettere vari dati nel modello rubato per dedurre informazioni sensibili sui dati di addestramento.\nIn base alla risorsa desiderata, gli attacchi con furto di modelli possono essere suddivisi in due categorie: proprietà esatte del modello e comportamento approssimativo del modello.\n\nFurto di Proprietà Esatte del Modello\nIn questi attacchi, l’obiettivo è estrarre informazioni su metriche concrete, come i parametri appresi di una rete, gli iperparametri ottimizzati e l’architettura interna dei layer del modello (Oliynyk, Mayer, e Rauber 2023).\n\nParametri Appresi: Gli avversari mirano a rubare la conoscenza appresa di un modello (pesi e bias) per replicarla. Il furto di parametri è generalmente utilizzato con altri attacchi, come il furto di architettura, che non hanno conoscenza dei parametri.\nIperparametri Ottimizzati: L’addestramento è costoso e l’identificazione della configurazione ottimale degli iperparametri (come velocità di apprendimento e regolarizzazione) può richiedere molto tempo e risorse. Di conseguenza, rubare gli iperparametri ottimizzati di un modello consente agli avversari di replicare il modello senza sostenere gli stessi costi di sviluppo.\nArchitettura del Modello: Questo attacco riguarda la progettazione e la struttura specifiche del modello, come strati, neuroni e pattern di connettività. Oltre a ridurre i costi di training associati, questo furto rappresenta un grave rischio per la proprietà intellettuale, potenzialmente compromettendo il vantaggio competitivo di un’azienda. Il furto di architettura può essere ottenuto sfruttando attacchi side-channel (discussi più avanti).\n\n\n\nFurto del Comportamento Approssimativo del Modello\nInvece di estrarre valori numerici esatti dei parametri del modello, questi attacchi mirano a riprodurre il comportamento del modello (previsioni ed efficacia), il processo decisionale e le caratteristiche di alto livello (Oliynyk, Mayer, e Rauber 2023). Queste tecniche mirano a ottenere risultati simili pur consentendo deviazioni interne nei parametri e nell’architettura. I tipi di furto di comportamento approssimativo includono l’ottenimento dello stesso livello di efficacia e l’ottenimento di coerenza di previsione.\n\nOliynyk, Daryna, Rudolf Mayer, e Andreas Rauber. 2023. «I Know What You Trained Last Summer: A Survey on Stealing Machine Learning Models and Defences». ACM Computing Surveys 55 (14s): 1–41. https://doi.org/10.1145/3595292.\n\nLivello di efficacia: Gli aggressori mirano a replicare le capacità decisionali del modello piuttosto che concentrarsi sui valori precisi dei parametri. Ciò avviene attraverso la comprensione del comportamento complessivo del modello. Consideriamo uno scenario in cui un aggressore desidera copiare il comportamento di un modello di classificazione delle immagini. Analizzando i limiti decisionali del modello, l’attacco ottimizza il suo modello per raggiungere un’efficacia paragonabile al modello originale. Ciò potrebbe comportare l’analisi di 1) la matrice di confusione per comprendere l’equilibrio delle metriche di previsione (vero positivo, vero negativo, falso positivo, falso negativo) e 2) altre metriche di prestazione, come punteggio F1 e precisione, per garantire che i due modelli siano comparabili.\nCoerenza della Previsione: L’attaccante cerca di allineare i pattern di previsione del proprio modello con quelli del modello target. Ciò comporta l’abbinamento degli output di previsione (sia positivi che negativi) sullo stesso set di input e la garanzia della coerenza distributiva tra classi diverse. Ad esempio, prendiamo in considerazione un modello di elaborazione del linguaggio naturale (NLP) che genera un’analisi del sentiment per le recensioni di film (etichettando le recensioni come positive, neutre o negative). L’attaccante cercherà di mettere a punto il proprio modello per adattarlo alla previsione dei modelli originali sullo stesso set di recensioni di film. Ciò include la garanzia che il modello commetta gli stessi errori (previsioni errate) commessi dal modello target.\n\n\n\nCaso di Studio: Il furto di Proprietà Intellettuale di Tesla\nNel 2018, Tesla ha intentato una causa contro la startup di auto a guida autonoma Zoox, sostenendo che ex dipendenti avevano rubato dati riservati e segreti commerciali relativi al sistema di assistenza alla guida autonoma di Tesla.\nTesla ha affermato che diversi suoi ex dipendenti hanno sottratto oltre 10 GB di dati proprietari, tra cui modelli di ML e codice sorgente, prima di unirsi a Zoox. Ciò avrebbe incluso uno dei modelli di riconoscimento delle immagini cruciali di Tesla per l’identificazione degli oggetti.\nIl furto di questo modello proprietario sensibile potrebbe aiutare Zoox ad abbreviare anni di sviluppo ML e duplicare le capacità di Tesla. Tesla ha sostenuto che questo furto di proprietà intellettuale ha causato notevoli danni finanziari e competitivi. C’erano anche preoccupazioni che potesse consentire attacchi di inversione del modello per dedurre dettagli privati sui dati di test di Tesla.\nI dipendenti di Zoox hanno negato di aver rubato informazioni proprietarie. Tuttavia, il caso evidenzia i rischi significativi del furto di modelli, che consente la clonazione di modelli commerciali, causando ripercussioni economiche e aprendo la porta a ulteriori violazioni della privacy dei dati.\n\n\n\n14.4.2 Avvelenamento dei Dati\nL’avvelenamento dei dati è un attacco in cui i dati di training vengono manomessi, portando a un modello compromesso (Biggio, Nelson, e Laskov 2012). Gli aggressori possono modificare gli esempi di training esistenti, inserire nuovi dati dannosi o influenzare il processo di raccolta dati. I dati avvelenati vengono etichettati in modo tale da alterare il comportamento appreso del modello. Ciò può essere particolarmente dannoso nelle applicazioni in cui i modelli ML prendono decisioni automatizzate in base a pattern appresi. Oltre ai set di training, i test di avvelenamento e i dati di convalida possono consentire agli avversari di aumentare artificialmente le prestazioni del modello segnalate.\n\nBiggio, Battista, Blaine Nelson, e Pavel Laskov. 2012. «Poisoning Attacks against Support Vector Machines.» In Proceedings of the 29th International Conference on Machine Learning, ICML 2012, Edinburgh, Scotland, UK, June 26 - July 1, 2012. icml.cc / Omnipress. http://icml.cc/2012/papers/880.pdf.\nIl processo di solito prevede i seguenti passaggi:\n\nInjection: L’aggressore aggiunge esempi errati o fuorvianti al set di training. Questi esempi sono spesso progettati per sembrare normali a un’ispezione superficiale, ma sono stati attentamente elaborati per interrompere il processo di apprendimento.\nTraining: Il modello ML si allena su questo set di dati manipolato e sviluppa comprensioni distorte dei pattern di dati.\nDeployment: Una volta distribuito il modello, l’addestramento corrotto porta a un processo decisionale imperfetto o a vulnerabilità prevedibili che l’aggressore può sfruttare.\n\nGli impatti dell’avvelenamento dei dati vanno oltre i semplici errori di classificazione o cali di accuratezza. Ad esempio, se dati errati o dannosi vengono introdotti nel set di addestramento di un sistema di riconoscimento dei segnali stradali, il modello potrebbe imparare a classificare erroneamente i segnali di stop come segnali di precedenza, il che può avere pericolose conseguenze nel mondo reale, specialmente nei sistemi autonomi embedded come i veicoli autonomi.\nL’avvelenamento dei dati può degradare l’accuratezza di un modello, costringerlo a fare previsioni errate o farlo comportare in modo imprevedibile. In applicazioni critiche come l’assistenza sanitaria, tali alterazioni possono portare a significativi problemi di fiducia e sicurezza.\nEsistono sei categorie principali di avvelenamento dei dati (Oprea, Singhal, e Vassilev 2022):\n\nOprea, Alina, Anoop Singhal, e Apostol Vassilev. 2022. «Poisoning Attacks Against Machine Learning: Can Machine Learning Be Trustworthy?» Computer 55 (11): 94–99. https://doi.org/10.1109/mc.2022.3190787.\n\nAttacchi di Disponibilità: Questi attacchi cercano di compromettere la funzionalità complessiva di un modello. Fanno sì che classifichi erroneamente la maggior parte dei campioni di test, rendendo il modello inutilizzabile per applicazioni pratiche. Un esempio è il “label flipping”, in cui le etichette di una classe specifica e mirata vengono sostituite con etichette di una classe diversa.\nAttacchi Mirati: A differenza degli attacchi alla disponibilità, gli attacchi mirati mirano a compromettere un numero limitato di campioni di test. Quindi, l’effetto è localizzato su un numero limitato di classi, mentre il modello mantiene lo stesso livello di accuratezza originale sulla maggior parte delle classi. La natura mirata dell’attacco richiede che l’aggressore conosca le classi del modello, rendendo più difficile il rilevamento di questi attacchi.\nAttacchi Backdoor: In questi attacchi, un avversario prende di mira pattern specifici nei dati. L’aggressore introduce una backdoor (un trigger o pattern nascosto e dannoso) nei dati di training, ad esempio modificando determinate feature nei dati strutturati o un pattern di pixel in una posizione fissa. Ciò fa sì che il modello associ il pattern dannoso a etichette specifiche. Di conseguenza, quando il modello incontra campioni di test che contengono un pattern dannoso, fa false previsioni, evidenziando l’importanza della cautela e della prevenzione nel ruolo dei professionisti della sicurezza dei dati.\nAttacchi di Sotto-popolazione: Gli aggressori scelgono selettivamente di compromettere un sottoinsieme dei campioni di test mantenendo l’accuratezza sul resto dei campioni. Questi attacchi si possono pensare come una combinazione di attacchi di disponibilità e mirati: eseguire attacchi di disponibilità (degrado delle prestazioni) nell’ambito di un sottoinsieme mirato. Sebbene gli attacchi di sottopopolazione possano sembrare molto simili agli attacchi mirati, i due presentano chiare differenze:\nScope: Mentre gli attacchi mirati prendono di mira un set selezionato di campioni, gli attacchi di sotto-popolazione prendono di mira una sotto-popolazione generale con rappresentazioni di caratteristiche simili. Ad esempio, in un attacco mirato, un attore inserisce immagini manipolate di un cartello di avvertimento “rallentamenti” (con perturbazioni o pattern attentamente studiati), che fa sì che un’auto autonoma non riconosca tale cartello e non rallenti. D’altro canto, manipolare tutti i campioni di persone con accento britannico in modo che un modello di riconoscimento vocale classifichi erroneamente il discorso di una persona britannica è un esempio di attacco di sotto-popolazione.\nConoscenza: Mentre gli attacchi mirati richiedono un alto grado di familiarità con i dati, gli attacchi alla sotto-popolazione richiedono una conoscenza meno approfondita per essere efficaci.\n\n\nCaso di Studio: Avvelenamento dei Sistemi di Moderazione dei Contenuti\nNel 2017, i ricercatori hanno dimostrato un attacco di avvelenamento dei data contro un modello di classificazione della tossicità popolare chiamato Perspective (Hosseini et al. 2017). Questo modello ML rileva commenti tossici online.\n\nHosseini, Hossein, Sreeram Kannan, Baosen Zhang, e Radha Poovendran. 2017. «Deceiving Google’s Perspective API Built for Detecting Toxic Comments». ArXiv preprint abs/1702.08138 (febbraio). http://arxiv.org/abs/1702.08138v1.\nI ricercatori hanno aggiunto commenti tossici generati sinteticamente con lievi errori di ortografia e grammaticali ai dati di training del modello. Ciò ha lentamente corrotto il modello, facendogli classificare erroneamente un numero crescente di input gravemente tossici come non tossici nel tempo.\nDopo il ri-addestramento sui dati avvelenati, il tasso di falsi negativi del modello è aumentato dall’1,4% al 27%, consentendo ai commenti estremamente tossici di aggirare il rilevamento. I ricercatori hanno avvertito che questo furtivo “data poisoning” potrebbe consentire la diffusione di discorsi di odio, molestie e abusi se implementato contro sistemi di moderazione reali.\nQuesto caso evidenzia come l’avvelenamento dei dati possa degradare l’accuratezza e l’affidabilità del modello. Per le piattaforme di social media, un attacco di avvelenamento che compromette il rilevamento della tossicità potrebbe portare alla proliferazione di contenuti dannosi e alla sfiducia nei sistemi di moderazione ML. L’esempio dimostra perché proteggere l’integrità dei dati di training e monitorare l’avvelenamento è fondamentale in tutti i domini applicativi.\n\n\nCaso di Studio: Proteggere l’Arte Attraverso l’Avvelenamento dei Dati\nÈ interessante notare che gli attacchi di “data poisoning” non sono sempre dannosi (Shan et al. 2023). Nightshade, uno strumento sviluppato da un team guidato dal professor Ben Zhao presso l’Università di Chicago, utilizza l’avvelenamento dei dati per aiutare gli artisti a proteggere la propria arte da scraping e violazioni del copyright da parte di modelli di IA generativa. Gli artisti possono utilizzare lo strumento per modificare le proprie immagini in modo sottile prima di caricarle online.\nSebbene queste modifiche siano impercettibili all’occhio umano, possono degradare significativamente le prestazioni dei modelli di intelligenza artificiale generativa quando integrate nei dati di addestramento. I modelli generativi possono essere manipolati per produrre output irrealistici o privi di senso. Ad esempio, con solo 300 immagini corrotte, i ricercatori dell’Università di Chicago sono riusciti a ingannare l’ultimo modello “Stable Diffusion” per generare immagini di cani che assomigliano a felini o bovini quando richiesto per le automobili.\nCon l’aumento della quantità di immagini corrotte online, l’efficacia dei modelli addestrati su dati estratti diminuirà esponenzialmente. Inizialmente, identificare i dati corrotti è difficile e richiede un intervento manuale. Successivamente, la contaminazione si diffonde rapidamente ai concetti correlati, poiché i modelli generativi stabiliscono connessioni tra le parole e le loro rappresentazioni visive. Di conseguenza, un’immagine corrotta di un’“auto” potrebbe propagarsi in immagini generate collegate a termini come “camion”, “treno” e “autobus”.\nD’altro canto, questo strumento può essere utilizzato in modo dannoso e influenzare le applicazioni legittime del modello generativo. Ciò dimostra la natura molto impegnativa e innovativa degli attacchi di apprendimento automatico.\nFigura 17.26 mostra gli effetti di diversi livelli di avvelenamento dei dati (50 campioni, 100 campioni e 300 campioni di immagini avvelenate) sulla generazione di immagini in varie categorie. Notare come le immagini iniziano a deformarsi e deviare dalla categoria desiderata. Ad esempio, dopo 300 campioni di avvelenamento, una richiesta di un’auto genera una mucca.\n\n\n\n\n\n\nFigura 14.2: Avvelenamento dei Dati. Fonte: Shan et al. (2023).\n\n\nShan, Shawn, Wenxin Ding, Josephine Passananti, Stanley Wu, Haitao Zheng, e Ben Y. Zhao. 2023. «Nightshade: Prompt-Specific Poisoning Attacks on Text-to-Image Generative Models». ArXiv preprint abs/2310.13828 (ottobre). http://arxiv.org/abs/2310.13828v3.\n\n\n\n\n\n14.4.3 Attacchi Avversari\nGli attacchi avversari mirano a indurre i modelli a fare previsioni errate fornendo loro input ingannevoli appositamente creati (chiamati esempi avversari) (Parrish et al. 2023). Aggiungendo lievi perturbazioni ai dati di input, gli avversari possono “hackerare” il riconoscimento di pattern di un modello e ingannarlo. Si tratta di tecniche sofisticate in cui lievi, spesso impercettibili alterazioni dei dati di input possono indurre un modello ML a fare una previsione errata.\n\nParrish, Alicia, Hannah Rose Kirk, Jessica Quaye, Charvi Rastogi, Max Bartolo, Oana Inel, Juan Ciro, et al. 2023. «Adversarial Nibbler: A Data-Centric Challenge for Improving the Safety of Text-to-Image Models». ArXiv preprint abs/2305.14384 (maggio). http://arxiv.org/abs/2305.14384v1.\n\nRamesh, Aditya, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, e Ilya Sutskever. 2021. «Zero-Shot Text-to-Image Generation». In Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, a cura di Marina Meila e Tong Zhang, 139:8821–31. Proceedings of Machine Learning Research. PMLR. http://proceedings.mlr.press/v139/ramesh21a.html.\n\nRombach, Robin, Andreas Blattmann, Dominik Lorenz, Patrick Esser, e Bjorn Ommer. 2022. «High-Resolution Image Synthesis with Latent Diffusion Models». In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 10674–85. IEEE. https://doi.org/10.1109/cvpr52688.2022.01042.\nÈ possibile generare prompt che portano a immagini non sicure in modelli testo-immagine come DALLE (Ramesh et al. 2021) o Stable Diffusion (Rombach et al. 2022). Ad esempio, alterando i valori dei pixel di un’immagine, gli aggressori possono ingannare un sistema di riconoscimento facciale facendogli identificare un volto come una persona diversa.\nGli attacchi avversari sfruttano il modo in cui i modelli ML apprendono e prendono decisioni durante l’inferenza. Questi modelli funzionano sul principio di riconoscimento di pattern nei dati. Un avversario crea input dannosi con perturbazioni per fuorviare il riconoscimento di pattern del modello, essenzialmente “hackerando” le percezioni del modello.\nGli attacchi avversari rientrano in diversi scenari:\n\nAttacchi Whitebox: L’attaccante ha una conoscenza completa del funzionamento interno del modello target, inclusi i dati di addestramento, i parametri e l’architettura. Questo ampio accesso facilita lo sfruttamento delle vulnerabilità del modello. L’attaccante può sfruttare debolezze specifiche e sottili per costruire esempi avversari altamente efficaci.\nAttacchi Blackbox: A differenza degli attacchi Whitebox, in quelli Blackbox l’attaccante ha poca o nessuna conoscenza del modello target. L’attore avversario deve osservare attentamente il comportamento di output del modello per eseguire l’attacco.\nAttacchi Greybox: Questi attacchi occupano uno spettro tra gli attacchi Blackbox e Whitebox. L’avversario possiede una conoscenza parziale della struttura interna del modello target. Ad esempio, l’attaccante potrebbe conoscere i dati di training ma non avere informazioni sull’architettura o sui parametri del modello. In scenari pratici, la maggior parte degli attacchi rientra in questa zona grigia.\n\nIl panorama dei modelli di apprendimento automatico è complesso e ampio, soprattutto data la loro relativamente recente integrazione nelle applicazioni commerciali. Questa rapida adozione, sebbene trasformativa, ha portato alla luce numerose vulnerabilità all’interno di questi modelli. Di conseguenza, sono emersi vari metodi di attacco avversari, ognuno dei quali sfrutta strategicamente diversi aspetti di vari modelli. Di seguito, evidenziamo un sottoinsieme di questi metodi, che mostra la natura multiforme degli attacchi avversari sui modelli di apprendimento automatico:\n\nGenerative Adversarial Network (GAN): La natura avversaria delle GAN, in cui un generatore e un discriminatore competono, si allinea perfettamente con la creazione di attacchi avversari (Goodfellow et al. 2020). Sfruttando questo framework, la rete del generatore viene addestrata per produrre input che sfruttano le debolezze di un modello target, causandone una classificazione errata. Questo processo dinamico e competitivo rende le GAN particolarmente efficaci nel creare esempi avversari sofisticati e diversificati, sottolineando la loro adattabilità nell’attaccare i modelli di apprendimento automatico.\nTransfer Learning Adversarial Attacks: Questi attacchi prendono di mira gli estrattori di feature nei modelli di apprendimento per trasferimento introducendo perturbazioni che manipolano le loro rappresentazioni apprese. Gli estrattori di feature, pre-addestrati per identificare modelli generali, sono ottimizzati per attività specifiche nei modelli downstream [a valle]. Gli avversari sfruttano questo trasferimento creando input che distorcono gli output dell’estrattore di feature, causando classificazioni errate a valle. Gli “attacchi headless” esemplificano questa strategia, in cui gli avversari si concentrano sull’estrattore di feature senza richiedere l’accesso alla classificazione principale o ai dati di addestramento. Ciò evidenzia una vulnerabilità critica nelle pipeline di apprendimento per trasferimento, poiché i componenti fondamentali di molti modelli possono essere sfruttati. Rafforzare le difese è essenziale, data la diffusa dipendenza dai modelli pre-addestrati (Abdelkader et al. 2020).\n\n\nGoodfellow, Ian, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, e Yoshua Bengio. 2020. «Generative adversarial networks». Communications of the ACM 63 (11): 139–44. https://doi.org/10.1145/3422622.\n\nAbdelkader, Ahmed, Michael J. Curry, Liam Fowl, Tom Goldstein, Avi Schwarzschild, Manli Shu, Christoph Studer, e Chen Zhu. 2020. «Headless Horseman: Adversarial Attacks on Transfer Learning Models». In ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 3087–91. IEEE. https://doi.org/10.1109/icassp40776.2020.9053181.\n\nCaso di Studio: Inganno dei Modelli di Rilevamento dei Segnali Stradali\nNel 2017, i ricercatori hanno condotto esperimenti posizionando piccoli adesivi bianchi e neri sui segnali di stop (Eykholt et al. 2017). Quando visti da un occhio umano normale, gli adesivi non oscuravano il segnale né ne impedivano l’interpretazione. Tuttavia, quando le immagini degli adesivi dei segnali di stop venivano inserite nei modelli ML standard di classificazione dei segnali stradali, venivano classificati erroneamente come segnali di limite di velocità nell’85% dei casi.\n\nEykholt, Kevin, Ivan Evtimov, Earlence Fernandes, Bo Li, Amir Rahmati, Chaowei Xiao, Atul Prakash, Tadayoshi Kohno, e Dawn Song. 2017. «Robust Physical-World Attacks on Deep Learning Models». ArXiv preprint abs/1707.08945 (luglio). http://arxiv.org/abs/1707.08945v5.\nQuesta dimostrazione ha mostrato come semplici adesivi avversari potrebbero ingannare i sistemi ML facendogli interpretare male i segnali stradali critici. Se implementati in modo realistico, questi attacchi potrebbero mettere a repentaglio la sicurezza pubblica, inducendo i veicoli autonomi a interpretare male i segnali di stop come limiti di velocità. I ricercatori hanno avvertito che ciò potrebbe potenzialmente causare pericolosi semplici rallentamenti o accelerazioni negli incroci.\nQuesto caso di studio fornisce un’illustrazione concreta di come gli esempi avversari sfruttano i meccanismi di riconoscimento di pattern dei modelli ML. Alterando in modo sottile i dati di input, gli aggressori possono indurre previsioni errate e rappresentare rischi significativi per applicazioni critiche per la sicurezza come le auto a guida autonoma. La semplicità dell’attacco dimostra come anche cambiamenti minori e impercettibili possano sviare i modelli. Di conseguenza, gli sviluppatori devono implementare difese robuste contro tali minacce.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Sicurezza e Privacy</span>"
    ]
  },
  {
    "objectID": "contents/core/privacy_security/privacy_security.it.html#minacce-alla-sicurezza-per-lhardware-ml",
    "href": "contents/core/privacy_security/privacy_security.it.html#minacce-alla-sicurezza-per-lhardware-ml",
    "title": "14  Sicurezza e Privacy",
    "section": "14.5 Minacce alla Sicurezza Per l’Hardware ML",
    "text": "14.5 Minacce alla Sicurezza Per l’Hardware ML\nL’hardware di apprendimento automatico embedded svolge un ruolo fondamentale nel potenziamento delle moderne applicazioni di IA, ma è sempre più esposto a una vasta gamma di minacce alla sicurezza. Queste vulnerabilità possono derivare da difetti nella progettazione hardware, manomissioni fisiche o persino dai complessi percorsi delle catene di fornitura globali. Per affrontare questi rischi è necessaria una comprensione completa dei vari modi in cui l’integrità hardware può essere compromessa. Come riassunto in Tabella 14.1, questa sezione esplora le categorie chiave delle minacce hardware, offrendo approfondimenti sulle loro origini, metodi e implicazioni per i sistemi di ML.\n\n\n\nTabella 14.1: Tipi di minaccia alla sicurezza hardware.\n\n\n\n\n\n\n\n\n\n\nTipo di minaccia\nDescrizione\nRilevanza per la sicurezza hardware ML\n\n\n\n\nBug Hardware\nDifetti intrinseci nelle progettazioni hardware che possono compromettere l’integrità del sistema.\nFondamento della vulnerabilità hardware.\n\n\nAttacchi Fisici\nSfruttamento diretto dell’hardware tramite accesso fisico o manipolazione.\nModello di minaccia basilare e palese.\n\n\nAttacchi di Injection di Guasti\nInduzione di guasti per causare errori nel funzionamento dell’hardware, portando a potenziali crash di sistema.\nManipolazione sistematica che porta al guasto.\n\n\nAttacchi a Canale Laterale\nSfruttamento di informazioni sul funzionamento dell’hardware per estrarre dati sensibili.\nAttacco indiretto tramite osservazione ambientale.\n\n\nInterfacce con Perdite\nVulnerabilità derivanti da interfacce che espongono i dati in modo involontario.\nEsposizione dei dati tramite canali di comunicazione.\n\n\nHardware Contraffatto\nUtilizzo di componenti hardware non autorizzati che potrebbero presentare falle di sicurezza.\nProblemi di vulnerabilità aggravati.\n\n\nRischi della Catena di Fornitura\nRischi introdotti durante il ciclo di vita dell’hardware, dalla produzione alla distribuzione.\nSfide di sicurezza cumulative e multiformi.\n\n\n\n\n\n\n\n14.5.1 Bug Hardware\nL’hardware non è immune al problema pervasivo di difetti di progettazione o bug. Gli aggressori possono sfruttare queste vulnerabilità per accedere, manipolare o estrarre dati sensibili, violando la riservatezza e l’integrità da cui dipendono utenti e servizi. Un esempio di tali vulnerabilità è venuto alla luce con la scoperta di Meltdown e Spectre, due vulnerabilità hardware che sfruttano vulnerabilità critiche nei processori moderni. Questi bug consentono agli aggressori di aggirare la barriera hardware che separa le applicazioni, consentendo a un programma dannoso di leggere la memoria di altri programmi e del sistema operativo.\nMeltdown (Kocher et al. 2019a) e Spectre (Kocher et al. 2019b) funzionano sfruttando le ottimizzazioni nelle CPU moderne che consentono loro di eseguire istruzioni speculative fuori ordine prima che i controlli di validità siano stati completati. Ciò rivela dati che dovrebbero essere inaccessibili, che l’attacco cattura tramite canali laterali come le cache. La complessità tecnica dimostra la difficoltà di eliminare le vulnerabilità anche con una validazione estesa.\n\n———, et al. 2019a. «Spectre Attacks: Exploiting Speculative Execution». In 2019 IEEE Symposium on Security and Privacy (SP), 1–19. IEEE. https://doi.org/10.1109/sp.2019.00002.\n\nKocher, Paul, Jann Horn, Anders Fogh, Daniel Genkin, Daniel Gruss, Werner Haas, Mike Hamburg, et al. 2019b. «Spectre Attacks: Exploiting Speculative Execution». In 2019 IEEE Symposium on Security and Privacy (SP), 1–19. IEEE. https://doi.org/10.1109/sp.2019.00002.\nSe un sistema ML elabora dati sensibili, come informazioni personali degli utenti o analisi aziendali proprietarie, Meltdown e Spectre rappresentano un pericolo reale e presente per la sicurezza dei dati. Consideriamo il caso di una scheda acceleratrice ML progettata per velocizzare i processi di apprendimento automatico, come quelli di cui abbiamo parlato nel capitolo Accelerazione IA. Questi acceleratori lavorano con la CPU per gestire calcoli complessi, spesso correlati all’analisi dei dati, al riconoscimento delle immagini e all’elaborazione del linguaggio naturale. Se una scheda acceleratrice di questo tipo presenta una vulnerabilità simile a Meltdown o Spectre, potrebbe far trapelare i dati che elabora. Un aggressore potrebbe sfruttare questa falla non solo per sottrarre dati, ma anche per ottenere informazioni sul funzionamento del modello ML, incluso potenzialmente il reverse engineering del modello stesso (tornando quindi al problema del furto di modelli.\nUno scenario reale in cui ciò potrebbe essere devastante sarebbe nel settore sanitario. I sistemi ML elaborano regolarmente dati altamente sensibili dei pazienti per aiutare a diagnosticare, pianificare il trattamento e prevedere i risultati. Un bug nell’hardware del sistema potrebbe portare alla divulgazione non autorizzata di informazioni sanitarie personali, violando la privacy del paziente e contravvenendo a rigidi standard normativi come l’Health Insurance Portability and Accountability Act (HIPAA)\nLe vulnerabilità Meltdown e Spectre sono un duro promemoria del fatto che la sicurezza hardware non consiste solo nel prevenire l’accesso fisico non autorizzato, ma anche nel garantire che l’architettura dell’hardware non diventi un canale per l’esposizione dei dati. Difetti di progettazione hardware simili emergono regolarmente in CPU, acceleratori, memoria, bus e altri componenti. Ciò richiede continue mitigazioni retroattive e compromessi sulle prestazioni nei sistemi distribuiti. Soluzioni proattive come le architetture di elaborazione confidenziale potrebbero mitigare intere classi di vulnerabilità attraverso una progettazione hardware fondamentalmente più sicura. Contrastare i bug hardware richiede rigore in ogni fase di progettazione, validazione e distribuzione.\n\n\n14.5.2 Attacchi Fisici\nLa manomissione fisica si riferisce alla manipolazione diretta e non autorizzata di risorse informatiche fisiche per minare l’integrità dei sistemi di apprendimento automatico. È un attacco particolarmente insidioso perché aggira le tradizionali misure di sicurezza informatica, che spesso si concentrano più sulle vulnerabilità del software che sulle minacce hardware.\nLa manomissione fisica può assumere molte forme, da quelle relativamente semplici, come l’inserimento di un dispositivo USB caricato con software dannoso in un server, a quelle altamente sofisticate, come l’inclusione di un Trojan hardware durante il processo di produzione di un microchip (discusso più avanti in dettaglio nella sezione Supply Chain). I sistemi ML sono suscettibili a questo attacco perché si basano sull’accuratezza e l’integrità del loro hardware per elaborare e analizzare correttamente grandi quantità di dati.\nSi consideri un drone alimentato da ML utilizzato per la mappatura geografica. Il funzionamento del drone si basa su una serie di sistemi di bordo, tra cui un modulo di navigazione che elabora gli input da vari sensori per determinare il suo percorso. Se un aggressore ottiene l’accesso fisico a questo drone, potrebbe sostituire il modulo di navigazione originale con uno compromesso che include una backdoor. Questo modulo manipolato potrebbe quindi alterare la traiettoria di volo del drone per condurre la sorveglianza su aree riservate o persino contrabbandare merci di contrabbando volando su rotte non rilevate.\nUn altro esempio è la manomissione fisica degli scanner biometrici utilizzati per il controllo degli accessi in strutture sicure. Introducendo un sensore modificato che trasmette dati biometrici a un ricevitore non autorizzato, un aggressore può accedere ai dati di identificazione personale per autenticare gli individui.\nEsistono diversi modi in cui la manomissione fisica può verificarsi nell’hardware ML:\n\nManipolazione dei sensori: Si consideri un veicolo autonomo dotato di telecamere e LiDAR per la percezione ambientale. Un malintenzionato potrebbe manipolare deliberatamente l’allineamento fisico di questi sensori per creare zone di occlusione o distorcere le misure della distanza. Ciò potrebbe compromettere le capacità di rilevamento degli oggetti e potenzialmente mettere in pericolo gli occupanti del veicolo.\nTrojan hardware: Le modifiche dannose ai circuiti possono introdurre trojan progettati per attivarsi in base a specifiche condizioni di input. Ad esempio, un chip acceleratore ML potrebbe funzionare come previsto fino a quando non incontra un trigger predeterminato, dopodiché si comporta in modo irregolare.\nManomissione della memoria: L’esposizione fisica e la manipolazione dei chip di memoria potrebbero consentire l’estrazione di parametri del modello ML crittografati. Le tecniche di iniezione di guasti possono anche corrompere i dati del modello per degradare l’accuratezza.\nIntroduzione di backdoor: Ottenendo l’accesso fisico ai server, un avversario potrebbe utilizzare keylogger hardware per catturare password e creare account backdoor per l’accesso persistente. Questi potrebbero poi essere utilizzati per esfiltrare dati di training ML nel tempo.\nAttacchi alla supply chain: Manipolare componenti hardware di terze parti o compromettere i canali di produzione e spedizione crea vulnerabilità sistemiche difficili da rilevare e correggere.\n\n\n\n14.5.3 Attacchi di Fault-injection\nIntroducendo intenzionalmente guasti nell’hardware ML, gli aggressori possono indurre errori nel processo di elaborazione, portando a output non corretti. Questa manipolazione compromette l’integrità delle operazioni ML e può fungere da vettore per ulteriori sfruttamenti, come il reverse engineering del sistema o il bypass del protocollo di sicurezza. L’iniezione di guasti comporta l’interruzione deliberata delle operazioni di elaborazione standard in un sistema tramite interferenze esterne (Joye e Tunstall 2012). Attivando con precisione gli errori di elaborazione, gli avversari possono alterare l’esecuzione del programma in modi che degradano l’affidabilità o trapelano informazioni sensibili.\n\nJoye, Marc, e Michael Tunstall. 2012. Fault Analysis in Cryptography. Springer Berlin Heidelberg. https://doi.org/10.1007/978-3-642-29656-7.\n\nBarenghi, Alessandro, Guido M. Bertoni, Luca Breveglieri, Mauro Pellicioli, e Gerardo Pelosi. 2010. «Low voltage fault attacks to AES». In 2010 IEEE International Symposium on Hardware-Oriented Security and Trust (HOST), 7–12. IEEE; IEEE. https://doi.org/10.1109/hst.2010.5513121.\n\nHutter, Michael, Jorn-Marc Schmidt, e Thomas Plos. 2009. «Contact-based fault injections and power analysis on RFID tags». In 2009 European Conference on Circuit Theory and Design, 409–12. IEEE; IEEE. https://doi.org/10.1109/ecctd.2009.5275012.\n\nAmiel, Frederic, Christophe Clavier, e Michael Tunstall. 2006. «Fault Analysis of DPA-Resistant Algorithms». In Fault Diagnosis and Tolerance in Cryptography, 223–36. Springer; Springer Berlin Heidelberg. https://doi.org/10.1007/11889700\\_20.\n\nAgrawal, Dakshi, Selcuk Baktir, Deniz Karakoyunlu, Pankaj Rohatgi, e Berk Sunar. 2007. «Trojan Detection using IC Fingerprinting». In 2007 IEEE Symposium on Security and Privacy (SP ’07), 296–310. Springer; IEEE. https://doi.org/10.1109/sp.2007.36.\n\nSkorobogatov, Sergei. 2009. «Local heating attacks on Flash memory devices». In 2009 IEEE International Workshop on Hardware-Oriented Security and Trust, 1–6. IEEE; IEEE. https://doi.org/10.1109/hst.2009.5225028.\n\nSkorobogatov, Sergei P., e Ross J. Anderson. 2002. «Optical Fault Induction Attacks.» In Cryptographic Hardware and Embedded Systems-CHES 2002: 4th International Workshop Redwood Shores, CA, USA, August 13–15, 2002 Revised Papers 4, 2–12. Springer. https://doi.org/10.1007/3-540-36400-5\\_2.\nPer l’iniezione di guasti possono essere utilizzate varie tecniche di manomissione fisica. Bassa tensione (Barenghi et al. 2010), picchi di potenza (Hutter, Schmidt, e Plos 2009), anomalie di clock (Amiel, Clavier, e Tunstall 2006), impulsi elettromagnetici (Agrawal et al. 2007), aumento della temperatura (S. Skorobogatov 2009) e colpi laser (S. P. Skorobogatov e Anderson 2002) sono comuni vettori di attacco hardware. Sono programmati con precisione per indurre guasti come bit invertiti o istruzioni saltate durante operazioni critiche.\nPer i sistemi ML, le conseguenze includono una precisione del modello compromessa, negazione del servizio, estrazione di dati di training privati o parametri del modello e reverse engineering delle architetture del modello. Gli aggressori potrebbero utilizzare l’iniezione di guasti per forzare classificazioni errate, interrompere sistemi autonomi o rubare proprietà intellettuale.\nAd esempio, Breier et al. (2018) ha iniettato con successo un “fault attack” in una rete neurale profonda distribuita su un microcontrollore. Hanno utilizzato un laser per riscaldare transistor specifici, costringendoli a cambiare stato. In un caso, hanno utilizzato questo metodo per attaccare una funzione di attivazione ReLU, con il risultato che la funzione emetteva sempre un valore di 0, indipendentemente dall’input. Nel codice assembly mostrato in Figura 14.3, l’attacco ha fatto sì che il programma in esecuzione saltasse sempre l’istruzione jmp end alla riga 6. Ciò significa che HiddenLayerOutput[i] è sempre impostato su 0, sovrascrivendo eventuali valori scritti su di esso nelle righe 4 e 5. Di conseguenza, i neuroni mirati vengono resi inattivi, con conseguenti classificazioni errate.\n\n\n\n\n\n\nFigura 14.3: Iniezione di errore dimostrata con codice assembly. Fonte: Breier et al. (2018).\n\n\nBreier, Jakub, Xiaolu Hou, Dirmanto Jap, Lei Ma, Shivam Bhasin, e Yang Liu. 2018. «DeepLaser: Practical Fault Attack on Deep Neural Networks». ArXiv preprint abs/1806.05859 (giugno). http://arxiv.org/abs/1806.05859v2.\n\n\nLa strategia di un aggressore potrebbe essere quella di dedurre informazioni sulle funzioni di attivazione tramite attacchi side-channel (discussi in seguito). Quindi, l’aggressore potrebbe tentare di colpire più calcoli di funzioni di attivazione iniettando casualmente guasti nei livelli il più vicino possibile al livello di output, aumentando la probabilità e l’impatto dell’attacco.\nI dispositivi embedded sono particolarmente vulnerabili a causa di un limitato rafforzamento fisico e di vincoli di risorse che limitano le difese di runtime robuste. Senza un packaging antimanomissione, l’accesso dell’aggressore ai bus di sistema e alla memoria consente di infierire guasti precisi. Anche i modelli ML embedded leggeri mancano di ridondanza per bypassare gli errori.\nQuesti attacchi possono essere particolarmente insidiosi perché aggirano le tradizionali misure di sicurezza basate su software, spesso non tenendo conto delle interruzioni fisiche. Inoltre, poiché i sistemi ML si basano in larga misura sull’accuratezza e l’affidabilità del loro hardware per attività come il riconoscimento di pattern, il processo decisionale e le risposte automatiche, qualsiasi compromesso nel loro funzionamento dovuto all’iniezione di guasti può avere conseguenze gravi e di vasta portata.\nPer mitigare i rischi di iniezione di guasti è necessario un approccio multi-layer. Il rafforzamento fisico tramite custodie antimanomissione e offuscamento del design aiuta a ridurre l’accesso. Il rilevamento di leggere anomalie può identificare input di sensori insoliti o output di modelli errati (Hsiao et al. 2023). Le memorie con correzione degli errori riducono al minimo le interruzioni, mentre la crittografia dei dati salvaguarda le informazioni. Le tecniche emergenti di watermarking dei modelli tracciano i parametri rubati.\n\nHsiao, Yu-Shun, Zishen Wan, Tianyu Jia, Radhika Ghosal, Abdulrahman Mahmoud, Arijit Raychowdhury, David Brooks, Gu-Yeon Wei, e Vijay Janapa Reddi. 2023. «MAVFI: An End-to-End Fault Analysis Framework with Anomaly Detection and Recovery for Micro Aerial Vehicles». In 2023 Design, Automation &amp; Test in Europe Conference &amp; Exhibition (DATE), 1–6. IEEE; IEEE. https://doi.org/10.23919/date56975.2023.10137246.\nTuttavia, bilanciare protezioni robuste con i limiti di dimensioni e potenza ristretti dei sistemi embedded rimane una sfida. I limiti della crittografia e la mancanza di coprocessori sicuri su hardware embedded sensibile ai costi limitano le opzioni. In definitiva, la resilienza all’iniezione di guasti richiede una prospettiva multi-layer che abbraccia i layer di progettazione elettrica, firmware, software e fisica.\n\n\n14.5.4 Attacchi a canale laterale\nGli attacchi side-channel costituiscono una classe di violazioni della sicurezza che sfruttano informazioni rivelate inavvertitamente tramite l’implementazione fisica dei sistemi informatici. Contrariamente agli attacchi diretti che prendono di mira vulnerabilità software o di rete, questi attacchi sfruttano le caratteristiche hardware intrinseche del sistema per estrarre informazioni sensibili.\nLa premessa fondamentale di un attacco side-channel è che il funzionamento di un dispositivo può rivelare inavvertitamente informazioni. Tali fughe possono provenire da varie fonti, tra cui l’energia elettrica consumata da un dispositivo (Kocher, Jaffe, e Jun 1999), i campi elettromagnetici che emette (Gandolfi, Mourtel, e Olivier 2001), il tempo necessario per elaborare determinate operazioni o persino i suoni che produce. Ogni canale può intravedere indirettamente i processi interni del sistema, rivelando informazioni che possono compromettere la sicurezza.\n\nKocher, Paul, Joshua Jaffe, e Benjamin Jun. 1999. «Differential Power Analysis». In Advances in Cryptology — CRYPTO’ 99, 388–97. Springer; Springer Berlin Heidelberg. https://doi.org/10.1007/3-540-48405-1\\_25.\n\nGandolfi, Karine, Christophe Mourtel, e Francis Olivier. 2001. «Electromagnetic Analysis: Concrete Results». In Cryptographic Hardware and Embedded Systems — CHES 2001, 251–61. Springer; Springer Berlin Heidelberg. https://doi.org/10.1007/3-540-44709-1\\_21.\n\nKocher, Paul, Joshua Jaffe, Benjamin Jun, e Pankaj Rohatgi. 2011. «Introduction to differential power analysis». Journal of Cryptographic Engineering 1 (1): 5–27. https://doi.org/10.1007/s13389-011-0006-y.\nSi consideri un sistema di apprendimento automatico che esegue transazioni crittografate. Gli algoritmi di crittografia sono progettati per proteggere i dati, ma richiedono un lavoro computazionale per crittografare e de-crittografare le informazioni. Uno standard di crittografia ampiamente utilizzato è l’Advanced Encryption Standard (AES), che crittografa i dati per impedire l’accesso non autorizzato. Tuttavia, gli aggressori possono analizzare i modelli di consumo energetico di un dispositivo che esegue la crittografia per dedurre informazioni sensibili, come la chiave crittografica. Con metodi statistici sofisticati, piccole variazioni nel consumo energetico durante il processo di crittografia possono essere correlate ai dati in fase di elaborazione, rivelando infine la chiave. Alcune tecniche di attacco di analisi differenziale sono “Differential Power Analysis (DPA)” (Kocher et al. 2011), “Differential Electromagnetic Analysis (DEMA)” e “Correlation Power Analysis (CPA)”.\nUn aggressore che tenta di violare la crittografia AES potrebbe raccogliere tracce di potenza o elettromagnetiche (registrazioni di consumi o emissioni di energia) dal dispositivo mentre esegue la crittografia. Analizzando queste tracce con tecniche statistiche, l’aggressore potrebbe identificare correlazioni tra le tracce e il testo in chiaro (testo originale non crittografato) o il testo cifrato (testo crittografato). Queste correlazioni potrebbero quindi essere utilizzate per dedurre singoli bit della chiave AES e, alla fine, ricostruire l’intera chiave. Gli attacchi di analisi differenziale sono particolarmente pericolosi perché sono economici, efficaci e non intrusivi, consentendo agli aggressori di aggirare le misure di sicurezza algoritmiche e a livello hardware. Anche le compromissioni tramite questi attacchi sono difficili da rilevare, poiché non alterano fisicamente il dispositivo né violano l’algoritmo di crittografia stesso.\nDi seguito, una visualizzazione semplificata illustra come l’analisi dei pattern di consumo energetico del dispositivo di crittografia può aiutare a estrarre informazioni sulle operazioni dell’algoritmo e, a sua volta, sui dati segreti. L’esempio mostra un dispositivo che accetta una password di 5 byte come input. La password immessa in questo scenario è 0x61, 0x52, 0x77, 0x6A, 0x73, che rappresenta la password corretta. I modelli di consumo energetico durante l’autenticazione forniscono informazioni su come funziona l’algoritmo.\nIn Figura 14.4, la forma d’onda rossa rappresenta le linee di dati seriali mentre il bootloader riceve i dati della password in blocchi (ad esempio 0x61, 0x52, 0x77, 0x6A, 0x73). Ciascun segmento etichettato (ad esempio, “Data: 61”) corrisponde a un byte della password elaborata dall’algoritmo di crittografia. Il grafico blu mostra il consumo energetico del dispositivo di crittografia mentre elabora ogni byte. Quando viene inserita la password corretta, il dispositivo elabora tutti i 5 byte con successo e il grafico della tensione blu mostra modelli coerenti in tutto. Questo grafico fornisce una linea di base per comprendere come appare il consumo energetico del dispositivo quando viene inserita una password corretta. Nelle figure successive, si vedrà come cambia il profilo energetico con password errate, aiutando a individuare le differenze nel comportamento del dispositivo quando l’autenticazione fallisce.\n\n\n\n\n\n\nFigura 14.4: Profilo del consumo energetico del dispositivo durante le normali operazioni con una password valida di 5 byte (0x61, 0x52, 0x77, 0x6A, 0x73). La linea rossa rappresenta i dati seriali ricevuti dal bootloader, che in questa figura riceve i byte corretti. Notare come la linea blu, che rappresenta il consumo energetico durante l’autenticazione, corrisponda alla ricezione e alla verifica dei byte. Nelle figure successive, questo profilo di consumo energetico blu cambierà. Fonte: Colin O’Flynn.\n\n\n\nQuando viene inserita una password errata, il grafico dell’analisi della potenza è mostrato in Figura 14.5. I primi tre byte della password sono corretti (ad esempio 0x61, 0x52, 0x77). Di conseguenza, i pattern di tensione sono molto simili o identici tra i due grafici, fino al quarto byte incluso. Dopo aver elaborato il quarto byte (0x42), il dispositivo rileva una mancata corrispondenza con la password corretta e interrompe l’ulteriore elaborazione. Ciò determina un cambiamento evidente nel pattern di alimentazione, mostrato dal salto improvviso nella linea blu all’aumentare della tensione.\n\n\n\n\n\n\nFigura 14.5: Profilo di consumo energetico del dispositivo quando viene inserita una password errata di 5 byte (0x61, 0x52, 0x77, 0x42, 0x42). La linea rossa rappresenta i dati seriali ricevuti dal bootloader, che mostrano i byte di input in fase di elaborazione. I primi tre byte (0x61, 0x52, 0x77) sono corretti e corrispondono alla password prevista, come indicato dalla linea blu coerente del consumo energetico. Tuttavia, durante l’elaborazione del quarto byte (0x42), viene rilevata una mancata corrispondenza. Il bootloader interrompe l’ulteriore elaborazione, con conseguente salto evidente nella linea blu del consumo energetico, poiché il dispositivo interrompe l’autenticazione ed entra in uno stato di errore. Fonte: Colin O’Flynn.\n\n\n\nLa Figura 14.6 mostra un altro esempio, ma in cui la password è completamente errata (0x30, 0x30, 0x30, 0x30, 0x30), a differenza dell’esempio precedente con i primi tre byte corretti. Qui, il dispositivo identifica la mancata corrispondenza subito dopo l’elaborazione del primo byte e interrompe l’ulteriore elaborazione. Ciò si riflette nel profilo di consumo energetico, in cui la linea blu mostra un brusco salto dopo il primo byte, indicando la conclusione anticipata dell’autenticazione del dispositivo.\n\n\n\n\n\n\nFigura 14.6: Profilo di consumo energetico del dispositivo quando viene inserita una password completamente errata (0x30, 0x30, 0x30, 0x30, 0x30). La linea blu mostra un brusco salto dopo l’elaborazione del primo byte, indicando che il dispositivo ha interrotto il processo di autenticazione. Fonte: Colin O’Flynn.\n\n\n\nL’esempio sopra dimostra come le informazioni sul processo di crittografia e sulla chiave segreta possono essere dedotte analizzando diversi input e varianti di test di forza bruta di ogni byte della password, in pratica “intercettando” le operazioni del dispositivo. Per una spiegazione più dettagliata, guardare Video 14.3 di seguito.\n\n\n\n\n\n\nVideo 14.3: Power Attack\n\n\n\n\n\n\nUn altro esempio è un sistema ML per il riconoscimento vocale, che elabora i comandi vocali per eseguire azioni. Misurando la latenza del sistema per rispondere ai comandi o la potenza utilizzata durante l’elaborazione, un aggressore potrebbe dedurre quali comandi vengono elaborati e quindi apprendere i pattern operativi del sistema. Ancora più sottilmente, il suono emesso dalla ventola o dal disco rigido di un computer potrebbe cambiare in risposta al carico di lavoro, che un microfono sensibile potrebbe captare e analizzare per determinare che tipo di operazioni vengono eseguite.\nIn scenari reali, gli attacchi side-channel hanno effettivamente estratto chiavi di crittografia e compromesso comunicazioni sicure. Uno dei primi casi registrati di un simile attacco si è verificato negli anni ’60, quando l’agenzia di intelligence britannica MI5 ha affrontato la sfida di decifrare comunicazioni crittografate dall’ambasciata egiziana a Londra. I loro sforzi di decifrazione dei codici sono stati inizialmente ostacolati dalle limitazioni computazionali dell’epoca, fino a quando un’ingegnosa osservazione dell’agente MI5 Peter Wright ha alterato il corso dell’operazione.\nL’agente dell’MI5 Peter Wright propose di usare un microfono per catturare le sottili firme acustiche emesse dalla macchina di cifratura del rotore dell’ambasciata durante la crittografia (Burnet e Thomas 1989). I distinti clic meccanici dei rotori mentre gli operatori li configuravano quotidianamente facevano trapelare informazioni critiche sulle impostazioni iniziali. Questo semplice “canale laterale” del suono ha permesso all’MI5 di ridurre drasticamente la complessità della decifrazione dei messaggi. Questo primo attacco di “perdita” acustica evidenzia che gli attacchi a canale laterale non sono semplicemente una novità dell’era digitale, ma una continuazione di antichi principi di crittoanalisi. L’idea che dove c’è un segnale, c’è un’opportunità di intercettazione rimane fondamentale. Dai clic meccanici alle fluttuazioni elettriche e oltre, i canali laterali consentono agli avversari di estrarre segreti indirettamente attraverso un’attenta analisi del segnale.\n\nBurnet, David, e Richard Thomas. 1989. «Spycatcher: The Commodification of Truth». Journal of Law and Society 16 (2): 210. https://doi.org/10.2307/1410360.\n\nAsonov, D., e R. Agrawal. s.d. «Keyboard acoustic emanations». In IEEE Symposium on Security and Privacy, 2004. Proceedings. 2004, 3–11. IEEE; IEEE. https://doi.org/10.1109/secpri.2004.1301311.\n\nGnad, Dennis R. E., Fabian Oboril, e Mehdi B. Tahoori. 2017. «Voltage drop-based fault attacks on FPGAs using valid bitstreams». In 2017 27th International Conference on Field Programmable Logic and Applications (FPL), 1–7. IEEE; IEEE. https://doi.org/10.23919/fpl.2017.8056840.\n\nZhao, Mark, e G. Edward Suh. 2018. «FPGA-Based Remote Power Side-Channel Attacks». In 2018 IEEE Symposium on Security and Privacy (SP), 229–44. IEEE; IEEE. https://doi.org/10.1109/sp.2018.00049.\nOggi, la crittoanalisi acustica si è evoluta in attacchi come l’intercettazione della tastiera (Asonov e Agrawal, s.d.). I canali laterali elettrici spaziano dall’analisi della potenza su hardware crittografico (Gnad, Oboril, e Tahoori 2017) alle fluttuazioni di tensione (Zhao e Suh 2018) su acceleratori di machine learning. Anche tempistiche, emissioni elettromagnetiche e persino impronte di calore possono essere sfruttate. Nuovi e inaspettati canali laterali emergono spesso man mano che l’informatica diventa più interconnessa e miniaturizzata.\nProprio come la “perdita” acustica analogica dell’MI5 ha trasformato la loro decifrazione dei codici, i moderni attacchi ai canali laterali aggirano i confini tradizionali della difesa informatica. Comprendere lo spirito creativo e la persistenza storica degli exploit dei canali laterali è una conoscenza fondamentale per sviluppatori e difensori che cercano di proteggere in modo completo i moderni sistemi di apprendimento automatico dalle minacce digitali e fisiche.\n\n\n14.5.5 Interfacce con Perdite\nLe interfacce “leaky” nei sistemi embedded sono spesso backdoor trascurate che possono trasformarsi in significative vulnerabilità di sicurezza. Sebbene progettate per scopi legittimi come comunicazione, manutenzione o debug, queste interfacce possono inavvertitamente fornire agli aggressori una finestra attraverso la quale estrarre informazioni sensibili o iniettare dati dannosi.\nUn’interfaccia diventa “leaky” quando espone più informazioni del dovuto, spesso a causa della mancanza di rigorosi controlli di accesso o di una schermatura inadeguata dei dati trasmessi. Ecco alcuni esempi concreti di problemi di interfaccia leaky che causano problemi di sicurezza in dispositivi IoT ed embedded:\n\nBaby Monitor: Molti baby monitor abilitati al WiFi hanno interfacce non protette per l’accesso remoto. Ciò ha consentito agli aggressori di ottenere feed audio e video in tempo reale dalle case delle persone, rappresentando una grave violazione della privacy.\nPacemaker: Sono state scoperte vulnerabilità dell’interfaccia in alcuni pacemaker che potrebbero consentire agli aggressori di manipolare le funzioni cardiache se sfruttate. Ciò presenta uno scenario potenzialmente letale.\nLampadine Smart: Un ricercatore ha scoperto di poter accedere a dati non crittografati da lampadine intelligenti tramite un’interfaccia di debug, comprese le credenziali WiFi, consentendogli di accedere alla rete connessa (Greengard 2021).\nAuto Smart: Se non protetta, la porta di diagnostica OBD-II ha dimostrato di fornire un vettore di attacco ai sistemi automobilistici. Gli aggressori potrebbero usarlo per controllare i freni e altri componenti (Miller e Valasek 2015).\n\n\nGreengard, Samuel. 2021. The Internet of Things. The MIT Press. https://doi.org/10.7551/mitpress/13937.001.0001.\n\nMiller, Charlie, e Chris Valasek. 2015. «Remote exploitation of an unaltered passenger vehicle». Black Hat USA 2015 (S 91): 1–91.\nSebbene quanto sopra non sia direttamente collegato al ML, si consideri l’esempio di un sistema di casa intelligente con un componente ML embedded che controlla la sicurezza domestica in base a pattern di comportamento che apprende nel tempo. Il sistema include un’interfaccia di manutenzione accessibile tramite la rete locale per aggiornamenti software e controlli di sistema. Se questa interfaccia non richiede un’autenticazione forte o i dati trasmessi tramite essa non sono crittografati, un aggressore sulla stessa rete potrebbe ottenere l’accesso. Potrebbero quindi intercettare le routine quotidiane del proprietario di casa o riprogrammare le impostazioni di sicurezza manipolando il firmware.\nTali “fughe” rappresentano un problema di privacy e un potenziale punto di ingresso per exploit più dannosi. L’esposizione di dati di training, parametri del modello o output ML da una “fuga” potrebbe aiutare gli avversari a costruire esempi avversari o a sottoporre a reverse engineering i modelli. L’accesso tramite un’interfaccia con “perdite” potrebbe anche essere utilizzato per modificare il firmware di un dispositivo embedded, caricandolo con codice dannoso che potrebbe spegnerlo, intercettare dati o utilizzarlo in attacchi botnet.\nPer mitigare questi rischi, è necessario un approccio multi-strato, che comprenda controlli tecnici quali autenticazione, crittografia, rilevamento delle anomalie, policy e processi come inventari di interfaccia, controlli di accesso, auditing e pratiche di sviluppo sicure. Disattivare le interfacce non necessarie e compartimentare i rischi tramite un modello zero-trust fornisce una protezione aggiuntiva.\nCome progettisti di sistemi ML embedded, dovremmo valutare le interfacce nelle prime fasi dello sviluppo e monitorarle continuamente dopo l’implementazione come parte di un ciclo di vita della sicurezza end-to-end. Comprendere e proteggere le interfacce è fondamentale per garantire la sicurezza complessiva del ML embedded.\n\n\n14.5.6 Hardware Contraffatto\nI sistemi ML sono affidabili solo quanto l’hardware sottostante. In un’epoca in cui i componenti hardware sono beni di consumo globali, l’aumento di hardware contraffatti o clonati rappresenta una sfida significativa. L’hardware contraffatto comprende tutti i componenti che sono riproduzioni non autorizzate di parti originali. I componenti contraffatti si infiltrano nei sistemi ML attraverso complesse catene di fornitura che si estendono oltre i confini e coinvolgono numerose fasi dalla produzione alla consegna.\nAnche una sola mancanza di integrità nella catena di fornitura può comportare l’inserimento di parti contraffatte, progettate per imitare fedelmente le funzioni e l’aspetto dell’hardware originale. Ad esempio, un sistema di riconoscimento facciale per il controllo degli accessi ad alta sicurezza potrebbe essere compromesso se dotato di processori contraffatti. Questi processori potrebbero non riuscire a elaborare e verificare accuratamente i dati biometrici, consentendo potenzialmente a persone non autorizzate di accedere ad aree riservate.\nLa sfida con l’hardware contraffatto è multiforme. Compromette la qualità e l’affidabilità dei sistemi ML, poiché questi componenti potrebbero degradarsi più rapidamente o funzionare in modo imprevedibile a causa di una produzione scadente. Anche i rischi per la sicurezza sono profondi; l’hardware contraffatto può contenere vulnerabilità pronte per essere sfruttate da malintenzionati. Ad esempio, un router di rete clonato in un data center ML potrebbe includere una backdoor nascosta, consentendo l’intercettazione dei dati o l’intrusione nella rete senza essere rilevati.\nInoltre, l’hardware contraffatto comporta rischi legali e di conformità. Le aziende che utilizzano inavvertitamente parti contraffatte nei loro sistemi ML possono affrontare gravi ripercussioni legali, tra cui multe e sanzioni per il mancato rispetto delle normative e degli standard del settore. Ciò è particolarmente vero per i settori in cui è obbligatoria la conformità a specifiche normative sulla sicurezza e sulla privacy, come l’assistenza sanitaria e la finanza.\nLe pressioni economiche per ridurre i costi aggravano il problema dell’hardware contraffatto e costringono le aziende ad approvvigionarsi da fornitori a basso costo, privi di rigorosi processi di verifica. Questa economia può introdurre inavvertitamente parti contraffatte in sistemi altrimenti sicuri. Inoltre, rilevare queste contraffazioni è intrinsecamente complicato poiché vengono create per passare per componenti originali, il che spesso richiede attrezzature e competenze sofisticate per essere identificate.\nNel campo dell’apprendimento automatico, dove decisioni in tempo reale e calcoli complessi sono la norma, le implicazioni di un guasto hardware possono essere scomode e potenzialmente pericolose. È fondamentale che le parti interessate siano pienamente consapevoli di questi rischi. Le sfide poste dall’hardware contraffatto richiedono una comprensione completa delle attuali minacce all’integrità del sistema di apprendimento automatico. Ciò sottolinea la necessità di una gestione proattiva e informata del ciclo di vita dell’hardware all’interno di questi sistemi avanzati.\n\n\n14.5.7 Rischi della Catena di Fornitura\nLa minaccia dell’hardware contraffatto è strettamente legata alle vulnerabilità più ampie della supply chain [catena di fornitura]. Le supply chain globalizzate e interconnesse creano molteplici opportunità per componenti compromessi di infiltrarsi nel ciclo di vita di un prodotto. Le supply chain coinvolgono numerose entità, dalla progettazione alla produzione, all’assemblaggio, alla distribuzione e all’integrazione. Una mancanza di trasparenza e supervisione di ogni partner rende difficile la verifica dell’integrità a ogni passaggio. Le lacune in qualsiasi punto della catena possono consentire l’inserimento di parti contraffatte.\nAd esempio, un produttore su contratto potrebbe ricevere e includere inconsapevolmente rifiuti elettronici riciclati contenenti contraffazioni pericolose. Un distributore inaffidabile potrebbe introdurre di nascosto componenti clonati. Le minacce interne a qualsiasi fornitore potrebbero deliberatamente mescolare contraffazioni in spedizioni legittime.\nUna volta che le contraffazioni entrano nel flusso di fornitura, passano rapidamente attraverso più mani prima di finire nei sistemi ML in cui il rilevamento è difficile. Le contraffazioni avanzate come parti ricondizionate o cloni con esterni riconfezionati possono mascherarsi da componenti autentici, superando l’ispezione visiva.\nPer identificare i falsi, spesso è richiesta una profilazione tecnica completa tramite micrografia, screening a raggi X, analisi forense dei componenti e test funzionali. Tuttavia, un’analisi così costosa non è pratica per gli acquisti su larga scala.\nStrategie come audit della supply chain, screening dei fornitori, convalida della provenienza dei componenti e aggiunta di protezioni antimanomissione possono aiutare a mitigare i rischi. Tuttavia, date le sfide globali alla sicurezza della supply chain, un approccio zero-trust è prudente. Progettare sistemi ML per utilizzare controlli ridondanti, fail-safe e monitoraggio continuo del runtime fornisce resilienza contro i compromessi dei componenti.\nUna rigorosa convalida delle sorgenti hardware abbinata ad architetture di sistema fault-tolerant offre la difesa più solida contro i rischi pervasivi di supply chain globali contorte e opache.\n\n\n14.5.8 Caso di Studio: Una Chiamata di Risveglio per la Sicurezza Hardware\nNel 2018, Bloomberg Businessweek ha pubblicato una storia allarmante che ha attirato molta attenzione nel mondo della tecnologia. L’articolo sosteneva che Supermicro aveva segretamente impiantato minuscoli chip spia nell’hardware del server. I giornalisti hanno affermato che gli hacker statali cinesi che lavoravano con Supermicro potevano infilare questi minuscoli chip nelle schede madri durante la produzione. I minuscoli chip avrebbero presumibilmente dato agli hacker un accesso backdoor ai server utilizzati da oltre 30 grandi aziende, tra cui Apple e Amazon.\nSe fosse vero, ciò consentirebbe agli hacker di spiare dati privati o persino manomettere i sistemi. Tuttavia, dopo aver indagato, Apple e Amazon non hanno trovato prove dell’esistenza di tale hardware Supermicro hackerato. Altri esperti hanno messo in dubbio l’accuratezza dell’articolo di Bloomberg.\nSe la storia sia del tutto accurata o meno non è una nostra preoccupazione da un punto di vista pedagogico. Tuttavia, questo incidente ha attirato l’attenzione sui rischi delle catene di fornitura globali per l’hardware prodotto principalmente in Cina. Quando le aziende esternalizzano e acquistano componenti hardware da fornitori in tutto il mondo, è necessario che vi sia maggiore visibilità nel processo. In questa complessa pipeline globale, si teme che hardware contraffatti o manomessi possano essere introdotti da qualche parte lungo il percorso senza che le aziende tecnologiche se ne accorgano. Le aziende che si affidano troppo a singoli produttori o distributori creano rischi. Ad esempio, a causa dell’eccessiva dipendenza da TSMC per la produzione di semiconduttori, gli Stati Uniti hanno investito 50 miliardi di dollari nel CHIPS Act.\nMan mano che l’apprendimento automatico si sposta in sistemi più critici, è fondamentale verificare l’integrità dell’hardware dalla progettazione alla produzione e alla consegna. La backdoor Supermicro segnalata ha dimostrato che per la sicurezza dell’apprendimento automatico non possiamo dare per scontate le catene di fornitura e la produzione globali. Dobbiamo ispezionare e convalidare l’hardware a ogni collegamento della catena.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Sicurezza e Privacy</span>"
    ]
  },
  {
    "objectID": "contents/core/privacy_security/privacy_security.it.html#sicurezza-hardware-del-ml-embedded",
    "href": "contents/core/privacy_security/privacy_security.it.html#sicurezza-hardware-del-ml-embedded",
    "title": "14  Sicurezza e Privacy",
    "section": "14.6 Sicurezza Hardware del ML Embedded",
    "text": "14.6 Sicurezza Hardware del ML Embedded\n\n14.6.1 Trusted Execution Environments\n\nInformazioni su TEE\nUn Trusted Execution Environment (TEE) è un’area protetta all’interno di un processore host che garantisce l’esecuzione sicura del codice e la protezione dei dati sensibili. Isolando le attività critiche dal sistema operativo, i TEE resistono agli attacchi software e hardware, fornendo un ambiente protetto per la gestione di calcoli sensibili.\n\n\nVantaggi\nI TEE sono particolarmente preziosi in scenari in cui devono essere elaborati dati sensibili o in cui l’integrità delle operazioni di un sistema è critica. Nel contesto dell’hardware ML, i TEE assicurano che gli algoritmi e i dati ML siano protetti da manomissioni e “perdite”. Ciò è essenziale perché i modelli ML elaborano spesso informazioni private, segreti commerciali o dati che potrebbero essere sfruttati se esposti.\nAd esempio, un TEE può proteggere i parametri del modello ML dall’estrazione da parte di software dannosi sullo stesso dispositivo. Questa protezione è fondamentale per la privacy e il mantenimento dell’integrità del sistema ML, assicurando che i modelli funzionino come previsto e non forniscano output distorti a causa di parametri manipolati. Secure Enclave di Apple, presente in iPhone e iPad, è una forma di TEE che fornisce un ambiente isolato per proteggere i dati sensibili degli utenti e le operazioni crittografiche.\nI Trusted Execution Environment (TEE) sono essenziali per i settori che richiedono elevati livelli di sicurezza, tra cui telecomunicazioni, finanza, sanità e automotive. I TEE proteggono l’integrità delle reti 5G nelle telecomunicazioni e supportano applicazioni critiche. Nella finanza, proteggono i pagamenti mobili e i processi di autenticazione. L’assistenza sanitaria si affida ai TEE per salvaguardare i dati sensibili dei pazienti, mentre il settore automobilistico dipende da loro per la sicurezza e l’affidabilità dei sistemi autonomi. In tutti i settori, i TEE garantiscono la riservatezza e l’integrità dei dati e delle operazioni.\nNei sistemi ML, i TEE possono:\n\nEseguire in modo sicuro l’addestramento e l’inferenza del modello, assicurando che i risultati del calcolo rimangano riservati.\nProteggere la riservatezza dei dati di input, come le informazioni biometriche, utilizzati per l’identificazione personale o per attività di classificazione sensibili.\nProteggere i modelli ML impedendo il reverse engineering, che può proteggere le informazioni proprietarie e mantenere un vantaggio competitivo.\nAbilitare aggiornamenti sicuri ai modelli ML, assicurando che gli aggiornamenti provengano da una fonte attendibile e non siano stati manomessi durante il transito.\nRafforzare la sicurezza della rete salvaguardando la trasmissione dei dati tra componenti ML distribuiti tramite crittografia ed elaborazione sicura in-TEE.\n\nL’importanza dei TEE nella sicurezza hardware ML deriva dalla loro capacità di proteggere da minacce esterne e interne, tra cui le seguenti:\n\nSoftware Dannoso: I TEE possono impedire al malware ad alto privilegio di accedere alle aree sensibili del sistema ML.\nManomissione Fisica: Integrandosi con le misure di sicurezza hardware, i TEE possono proteggere dalla manomissione fisica che tenta di aggirare la sicurezza del software.\nAttacchi Side-Channel: Sebbene non siano impenetrabili, i TEE possono mitigare specifici attacchi side-channel controllando l’accesso a operazioni sensibili e modelli di dati.\nMinacce di Rete: I TEE migliorano la sicurezza della rete salvaguardando la trasmissione dei dati tra componenti ML distribuiti tramite crittografia ed elaborazione in-TEE sicura. Ciò impedisce efficacemente gli attacchi “man-in-the-middle” e garantisce che i dati vengano trasmessi tramite canali attendibili.\n\n\n\nMeccanica\nI fondamenti dei TEE contengono quattro parti principali:\n\nEsecuzione Isolata: Il codice all’interno di un TEE viene eseguito in un ambiente separato dal sistema operativo host del dispositivo host. Questo isolamento protegge il codice dall’accesso non autorizzato da parte di altre applicazioni.\nArchiviazione Sicura: I TEE possono archiviare in modo sicuro chiavi crittografiche, token di autenticazione e dati sensibili, impedendo alle applicazioni normali di accedervi al di fuori del TEE.\nProtezione dell’Integrità: I TEE possono verificare l’integrità del codice e dei dati, assicurando che non siano stati alterati prima dell’esecuzione o durante l’archiviazione.\nCrittografia dei Dati: I dati gestiti all’interno di un TEE possono essere crittografati, rendendoli illeggibili per entità senza le chiavi appropriate, che sono anch’esse gestite all’interno del TEE.\n\nEcco alcuni esempi di TEE che forniscono sicurezza basata su hardware per applicazioni sensibili:\n\nARMTrustZone: Questa tecnologia crea ambienti di esecuzione sicuri e normali isolati tramite controlli hardware e implementati in molti chipset mobili. mobile chipsets.\nIntelSGX: Le estensioni Software Guard di Intel forniscono un’enclave per l’esecuzione del codice che protegge da varie minacce basate sul software, prendendo di mira in modo specifico le vulnerabilità del livello del sistema operativo. Vengono utilizzate per salvaguardare i carichi di lavoro nel cloud.\nQualcomm Secure Execution Environment: Un sandbox hardware su chipset Qualcomm per app di pagamento e autenticazione mobili.\nApple SecureEnclave: Un TEE per la gestione dei dati biometrici e delle chiavi crittografiche su iPhone e iPad, che facilita i pagamenti mobili sicuri.\n\nFigura 14.7 è un diagramma che mostra un’enclave sicura isolata dal processore host per fornire un ulteriore livello di sicurezza. L’enclave sicura ha una ROM di avvio per stabilire una “root” hardware di attendibilità, un motore AES per operazioni crittografiche efficienti e sicure e memoria protetta. Ha anche un meccanismo per memorizzare le informazioni in modo sicuro su un archivio collegato separato da quello flash NAND utilizzato dal processore applicativo e dal sistema operativo. La NAND flash è un tipo di storage non volatile utilizzato in dispositivi come SSD, smartphone e tablet per conservare i dati anche quando sono spenti. Isolando i dati sensibili dallo storage NAND a cui accede il sistema principale, questo design garantisce che i dati degli utenti rimangano protetti anche se il kernel del processore applicativo è compromesso.\n\n\n\n\n\n\nFigura 14.7: Enclave sicura System-on-chip. Fonte: Apple.\n\n\n\n\n\nCompromessi\nSebbene i “Trusted Execution Environment” offrano notevoli vantaggi in termini di sicurezza, la loro implementazione comporta dei compromessi. Diversi fattori influenzano se un sistema include un TEE:\nCosto: L’implementazione dei TEE comporta costi aggiuntivi. Ci sono costi diretti per l’hardware e costi indiretti associati allo sviluppo e alla manutenzione di software sicuro per i TEE. Questi costi potrebbero essere giustificabili solo per alcuni dispositivi, in particolare prodotti a basso margine.\nComplessità: I TEE aggiungono complessità alla progettazione e allo sviluppo del sistema. L’integrazione di un TEE con sistemi esistenti richiede una sostanziale ri-progettazione dello stack hardware e software, che può rappresentare un ostacolo, in particolare per i sistemi legacy.\nPerformance Overhead: I TEE possono introdurre un overhead di prestazioni dovuto ai passaggi aggiuntivi coinvolti nella crittografia e nella verifica dei dati, che potrebbero rallentare le applicazioni sensibili al tempo.\nSfide di Sviluppo: Lo sviluppo per i TEE richiede conoscenze specialistiche e spesso deve rispettare rigidi protocolli di sviluppo. Ciò può estendere i tempi di sviluppo e complicare i processi di debug e test.\nScalabilità e Flessibilità: I TEE, a causa della loro natura protetta, possono imporre limitazioni di scalabilità e flessibilità. L’aggiornamento dei componenti protetti o il ridimensionamento del sistema per più utenti o dati può essere più impegnativo quando tutto deve passare attraverso un ambiente protetto e chiuso.\nConsumo Energetico: L’elaborazione aumentata richiesta per la crittografia, la decrittografia e i controlli di integrità possono portare a un maggiore consumo energetico, una preoccupazione significativa per i dispositivi alimentati a batteria.\nDomanda del Mercato: Non tutti i mercati o le applicazioni richiedono il livello di sicurezza fornito dai TEE. Per molte applicazioni consumer, il rischio percepito potrebbe essere abbastanza basso da indurre i produttori a scegliere di non includere TEE nei loro progetti.\nCertificazione e Garanzia di Sicurezza: I sistemi con TEE potrebbero aver bisogno di rigorose certificazioni di sicurezza con enti come Common Criteria (CC) o European Union Agency for Cybersecurity (ENISA), che possono essere lunghe e costose. Alcune organizzazioni potrebbero scegliere di astenersi dall’implementare TEE per evitare questi ostacoli.\nDispositivi con risorse limitate: I dispositivi con potenza di elaborazione, memoria o archiviazione limitate potrebbero supportare solo TEE senza compromettere la loro funzionalità primaria.\n\n\n\n14.6.2 Avvio Sicuro\n\nInformazioni\nSecure Boot è uno standard di sicurezza fondamentale che garantisce che un dispositivo si avvii solo tramite software ritenuto attendibile dal produttore del dispositivo. Durante l’avvio, il firmware controlla la firma digitale di ogni componente software di avvio, inclusi bootloader, kernel e sistema operativo di base. Questo processo verifica che il software non sia stato alterato o manomesso. Se una firma non supera la verifica, il processo di avvio viene interrotto per impedire l’esecuzione di codice non autorizzato che potrebbe compromettere l’integrità della sicurezza del sistema.\n\n\nVantaggi\nL’integrità di un sistema ML embedded è fondamentale fin dal momento dell’accensione. Qualsiasi compromissione nel processo di avvio può portare all’esecuzione di software dannoso prima che il sistema operativo e le applicazioni ML inizino, con conseguenti operazioni ML manipolate, accesso ai dati non autorizzato o riutilizzo del dispositivo per attività dannose come botnet o crypto-mining.\nSecure Boot offre protezioni vitali per l’hardware ML embedded tramite i seguenti meccanismi critici:\n\nProtezione dei Dati ML: Garantire che i dati utilizzati dai modelli ML, che possono includere informazioni private o sensibili, non siano esposti a manomissioni o furti durante il processo di boot [avvio].\nProtezione dell’Integrità del Modello: Mantenere l’integrità dei modelli ML è fondamentale, poiché la loro manomissione potrebbe portare a risultati errati o dannosi.\nAggiornamenti Sicuri del Modello: Abilitare aggiornamenti sicuri per modelli e algoritmi ML, assicurando che gli aggiornamenti siano autenticati e non siano stati alterati.\n\n\n\nMeccanica\nSecure Boot funziona con i TEE per migliorare ulteriormente la sicurezza del sistema. Figura 14.8 illustra un diagramma di flusso di un sistema embedded affidabile. Nella fase di validazione iniziale, Secure Boot verifica che il codice in esecuzione nel TEE sia la versione corretta e non manomessa autorizzata dal produttore del dispositivo. Controllando le firme digitali del firmware e di altri componenti critici del sistema, Secure Boot impedisce modifiche non autorizzate che potrebbero compromettere le capacità di sicurezza del TEE. Ciò stabilisce una base di fiducia su cui il TEE può eseguire in modo sicuro operazioni sensibili come la gestione delle chiavi crittografiche e l’elaborazione sicura dei dati. Applicando questi livelli di sicurezza, Secure Boot consente operazioni dei dispositivi sicure e resilienti anche negli ambienti con risorse più limitate.\n\n\n\n\n\n\nFigura 14.8: Flusso di Secure Boot. Fonte: R. V. e A. (2018).\n\n\nR. V., Rashmi, e Karthikeyan A. 2018. «Secure boot of Embedded Applications - A Review». In 2018 Second International Conference on Electronics, Communication and Aerospace Technology (ICECA), 291–98. IEEE. https://doi.org/10.1109/iceca.2018.8474730.\n\n\n\n\nCaso di Studio: Face ID di Apple\nUn esempio concreto dell’applicazione di Secure Boot può essere osservato nella tecnologia Face ID di Apple, che utilizza algoritmi di apprendimento automatico avanzati per abilitare il riconoscimento facciale su iPhone e iPad. Face ID si basa su una sofisticata integrazione di sensori e software per mappare con precisione la geometria del volto di un utente. Affinché Face ID funzioni in modo sicuro e protegga i dati biometrici degli utenti, le operazioni del dispositivo devono essere affidabili fin dall’inizializzazione. È qui che Secure Boot svolge un ruolo fondamentale. Di seguito viene descritto come funziona Secure Boot insieme a Face ID:\n\nVerifica Iniziale: All’avvio di un iPhone, il processo Secure Boot inizia all’interno di Secure Enclave, un coprocessore specializzato progettato per aggiungere un ulteriore livello di sicurezza. Secure Enclave gestisce i dati biometrici, come le impronte digitali per Touch ID e i dati di riconoscimento facciale per Face ID. Durante il processo di avvio, il sistema verifica rigorosamente che Apple abbia firmato digitalmente il firmware di Secure Enclave, garantendone l’autenticità. Questa fase di verifica assicura che il firmware utilizzato per elaborare i dati biometrici rimanga sicuro e senza essere compromesso.\nControlli di sicurezza continui: Dopo l’inizializzazione e la convalida del sistema da parte di Secure Boot, Secure Enclave comunica con il processore centrale del dispositivo per mantenere una catena di avvio sicura. Durante questo processo, le firme digitali del kernel iOS e di altri componenti di avvio critici vengono meticolosamente verificate per garantirne l’integrità prima di procedere. Questo modello di “catena di fiducia” impedisce efficacemente modifiche non autorizzate al bootloader e al sistema operativo, salvaguardando la sicurezza complessiva del dispositivo.\nElaborazione dei Dati del Viso: Una volta completata la sequenza di avvio sicura, Secure Enclave interagisce in modo sicuro con gli algoritmi di apprendimento automatico che alimentano Face ID. Il riconoscimento facciale prevede la proiezione e l’analisi di oltre 30.000 punti invisibili per creare una mappa di profondità del volto dell’utente e un’immagine a infrarossi. Questi dati vengono convertiti in una rappresentazione matematica e confrontati in modo sicuro con i dati del volto registrati e archiviati in Secure Enclave.\nSecure Enclave e Protezione dei Dati: Secure Enclave è progettato con precisione per proteggere i dati sensibili e gestire le operazioni crittografiche che salvaguardano tali dati. Anche in caso di kernel del sistema operativo compromesso, i dati del volto elaborati tramite Face ID rimangono inaccessibili ad applicazioni non autorizzate o ad aggressori esterni. È importante sottolineare che i dati di Face ID non vengono mai trasmessi dal dispositivo e non vengono archiviati su iCloud o altri server esterni.\nAggiornamenti Firmware: Apple rilascia frequentemente aggiornamenti per risolvere le vulnerabilità della sicurezza e migliorare la funzionalità del sistema. Secure Boot garantisce che tutti gli aggiornamenti del firmware siano autenticati, consentendo l’installazione solo di quelli firmati da Apple. Questo processo aiuta a preservare l’integrità e la sicurezza del sistema di Face ID nel tempo.\n\nIntegrando Secure Boot con hardware dedicato come Secure Enclave, Apple offre solide garanzie di sicurezza per operazioni critiche come il riconoscimento facciale.\n\n\nSfide\nNonostante i suoi vantaggi, l’implementazione di Secure Boot presenta diverse sfide, in particolare in distribuzioni complesse e su larga scala: Complessità della Gestione delle Chiavi: Generare, archiviare, distribuire, ruotare e revocare chiavi crittografiche in modo dimostrabilmente sicuro è particolarmente impegnativo ma fondamentale per mantenere la catena di fiducia. Qualsiasi compromissione delle chiavi paralizza le protezioni. Le grandi aziende che gestiscono moltitudini di chiavi di dispositivi affrontano particolari sfide di scala.\nSovraccarico di Prestazioni: Il controllo delle firme crittografiche durante il Boot può aggiungere 50-100ms o più per componente verificato. Questo ritardo può essere proibitivo per applicazioni sensibili al tempo o con risorse limitate. Tuttavia, gli impatti sulle prestazioni possono essere ridotti tramite parallelizzazione e accelerazione hardware.\nSigning Burden: [Onere della firma] Gli sviluppatori devono garantire diligentemente che tutti i componenti software coinvolti nel processo di avvio, ovvero bootloader, firmware, kernel del sistema operativo, driver, applicazioni, ecc., siano firmati correttamente da chiavi attendibili. L’accettazione della firma del codice di terze parti rimane un problema.\nVerifica Crittografica: Gli algoritmi e i protocolli sicuri devono convalidare la legittimità di chiavi e firme, evitare manomissioni o bypass e supportare la revoca. L’accettazione di chiavi dubbie mina la fiducia.\nVincoli di Personalizzazione: Le architetture Secure Boot bloccate dal fornitore limitano il controllo dell’utente e l’aggiornabilità. I bootloader open source come u-boot e coreboot abilitano la sicurezza supportando al contempo la personalizzazione.\nStandard Scalabili: Standard emergenti come Device Identifier Composition Engine (DICE) e IDevID promettono di fornire e gestire in modo sicuro identità e chiavi dei dispositivi su larga scala in tutti gli ecosistemi.\nL’adozione di Secure Boot richiede di seguire le best practice di sicurezza relative alla gestione delle chiavi, alla convalida della crittografia, agli aggiornamenti firmati e al controllo degli accessi. Secure Boot fornisce una solida base per creare integrità e affidabilità dei dispositivi se implementato con cura.\n\n\n\n14.6.3 Moduli di Sicurezza Hardware\n\nHSM\nUn “Hardware Security Module (HSM)” è un dispositivo fisico che gestisce le chiavi digitali per un’autenticazione avanzata e fornisce l’elaborazione crittografica. Questi moduli sono progettati per essere resistenti alle manomissioni e fornire un ambiente sicuro per l’esecuzione di operazioni crittografiche. Gli HSM possono essere dispositivi standalone, schede plug-in o circuiti integrati su un altro dispositivo.\nGli HSM sono fondamentali per varie applicazioni sensibili alla sicurezza perché offrono un’enclave rafforzata e sicura per l’archiviazione delle chiavi crittografiche e l’esecuzione di funzioni crittografiche. Sono particolarmente importanti per garantire la sicurezza delle transazioni, le verifiche dell’identità e la crittografia dei dati.\n\n\nVantaggi\nGli HSM forniscono diverse funzionalità utili per la sicurezza dei sistemi ML:\nProtezione dei Dati Sensibili: Nelle applicazioni di apprendimento automatico, i modelli spesso elaborano dati sensibili che possono essere proprietari o personali. Gli HSM proteggono le chiavi di crittografia utilizzate per proteggere questi dati, sia a riposo che in transito, dall’esposizione o dal furto.\nGaranzia dell’Integrità del Modello: L’integrità dei modelli ML è fondamentale per il loro funzionamento affidabile. Gli HSM possono gestire in modo sicuro i processi di firma e verifica per software e firmware ML, assicurando che parti non autorizzate non abbiano alterato i modelli.\nAddestramento e Aggiornamenti Sicuri del Modello: L’addestramento e l’aggiornamento dei modelli ML comportano l’elaborazione di dati potenzialmente sensibili. Gli HSM garantiscono che questi processi vengano condotti all’interno di un confine crittografico sicuro, proteggendo dall’esposizione dei dati di addestramento e dagli aggiornamenti non autorizzati del modello.\n\n\nCompromessi\nGli HSM comportano diversi compromessi per l’ML embedded. Questi compromessi sono simili ai TEE, ma per completezza, li discuteremo anche qui attraverso la lente dell’HSM.\nCosto: Gli HSM sono dispositivi specializzati che possono essere costosi da procurare e implementare, aumentando il costo complessivo di un progetto ML. Questo può essere un fattore significativo per i sistemi embedded, dove i vincoli di costo sono spesso più rigidi.\nSovraccarico di Prestazioni: Sebbene sicure, le operazioni crittografiche eseguite dagli HSM possono introdurre latenza. Qualsiasi ritardo aggiunto può essere critico nelle applicazioni ML embedded ad alte prestazioni in cui l’inferenza deve avvenire in tempo reale, come nei veicoli autonomi o nei dispositivi di traduzione.\nSpazio Fisico: I sistemi embedded sono spesso limitati dallo spazio fisico e l’aggiunta di un HSM può essere difficile in ambienti con vincoli rigidi. Ciò è particolarmente vero per l’elettronica di consumo e la tecnologia indossabile, dove le dimensioni e il fattore di forma sono considerazioni chiave.\nConsumo Energetico: Gli HSM richiedono energia per funzionare, il che può rappresentare uno svantaggio per i dispositivi a batteria con una lunga durata della batteria. L’elaborazione sicura e le operazioni crittografiche possono scaricare la batteria più velocemente, un compromesso significativo per le applicazioni ML embedded mobili o remote.\nComplessità nell’Integrazione: L’integrazione degli HSM nei sistemi hardware esistenti aggiunge complessità. Spesso sono necessarie conoscenze specialistiche per gestire la comunicazione sicura tra l’HSM e il processore del sistema e sviluppare software in grado di interfacciarsi con l’HSM.\nScalabilità: Il ridimensionamento di una soluzione ML che utilizza gli HSM può essere impegnativo. Gestire una flotta di HSM e garantire l’uniformità nelle pratiche di sicurezza tra i dispositivi può diventare complesso e costoso quando aumentano le dimensioni della distribuzione, soprattutto quando si ha a che fare con sistemi embedded in cui la comunicazione è costosa.\nComplessità Operativa: Gli HSM possono rendere più complesso l’aggiornamento del firmware e dei modelli ML. Ogni aggiornamento deve essere firmato e possibilmente crittografato, il che aggiunge passaggi al processo di aggiornamento e potrebbe richiedere meccanismi sicuri per la gestione delle chiavi e la distribuzione degli aggiornamenti.\nSviluppo e Manutenzione: La natura sicura degli HSM implica che solo personale limitato abbia accesso all’HSM per scopi di sviluppo e manutenzione. Ciò può rallentare il processo di sviluppo e rendere più difficile la manutenzione di routine.\nCertificazione e Conformità: Garantire che un HSM soddisfi specifici standard di settore e requisiti di conformità può aumentare i tempi e i costi di sviluppo. Ciò potrebbe comportare l’esecuzione di rigorosi processi di certificazione e audit.\n\n\n\n14.6.4 Physical Unclonable Functions (PUF)\n\nInformazioni\nLe “Physical Unclonable Function (PUF)” [funzioni fisiche non clonabili] forniscono un mezzo intrinseco all’hardware per la generazione di chiavi crittografiche e l’autenticazione del dispositivo sfruttando la variabilità di produzione intrinseca nei componenti semiconduttori. Durante la fabbricazione, fattori fisici casuali come variazioni di drogaggio, ruvidità del bordo della linea e spessore dielettrico determinano differenze microscopiche tra i semiconduttori, anche quando prodotti dalle stesse maschere. Questi creano variazioni di temporizzazione e potenza rilevabili che agiscono come una “impronta digitale” unica per ogni chip. Le PUF sfruttano questo fenomeno incorporando circuiti integrati per amplificare piccole differenze di temporizzazione o potenza in uscite digitali misurabili.\nQuando stimolato con uno stimolo in input, il circuito PUF produce una risposta di output basata sulle caratteristiche fisiche intrinseche del dispositivo. A causa della loro unicità fisica, lo stesso stimolo produrrà una risposta diversa su altri dispositivi. Questo meccanismo di stimolo-risposta può essere utilizzato per generare chiavi in modo sicuro e identificatori legati all’hardware specifico, eseguire l’autenticazione del dispositivo o archiviare in modo sicuro i segreti. Ad esempio, una chiave derivata da un PUF funzionerà solo su quel dispositivo e non potrà essere clonata o estratta nemmeno con accesso fisico o reverse engineering completo (Gao, Al-Sarawi, e Abbott 2020).\n\n\nVantaggi\nLa generazione di chiavi PUF evita l’archiviazione esterna delle chiavi, che rischia di essere esposta. Fornisce inoltre una base per altre primitive di sicurezza hardware come Secure Boot. Le sfide di implementazione includono la gestione di affidabilità ed entropia variabili tra diverse PUF, sensibilità alle condizioni ambientali e suscettibilità agli attacchi di modellazione di apprendimento automatico. Se progettate con cura, le PUF consentono applicazioni promettenti nella protezione IP, nel trusted computing e nell’anticontraffazione.\n\n\nUtilità\nI modelli di apprendimento automatico stanno rapidamente diventando una parte fondamentale della funzionalità per molti dispositivi embedded, come smartphone, assistenti domestici intelligenti e droni autonomi. Tuttavia, proteggere l’apprendimento automatico su hardware embedded con risorse limitate può essere difficile. Ed è qui che i PUF si rivelano particolarmente utili. Diamo un’occhiata ad alcuni esempi di come le PUF possono essere utili.\nLe PUF forniscono un modo per generare impronte digitali e chiavi crittografiche univoche legate alle caratteristiche fisiche di ciascun chip sul dispositivo. Facciamo un esempio. Abbiamo un drone con telecamera intelligente che usa ML embedded per tracciare gli oggetti. Un PUF integrato nel processore del drone potrebbe creare una chiave specifica del dispositivo per crittografare il modello ML prima di caricarlo sul drone. In questo modo, anche se un aggressore in qualche modo hackerasse il drone e provasse a rubare il modello, non sarebbe in grado di usarlo su un altro dispositivo!\nLa stessa chiave PUF potrebbe anche creare una filigrana digitale embedded nel modello ML. Se quel modello dovesse mai trapelare e essere pubblicato online da qualcuno che cercasse di piratarlo, la filigrana potrebbe aiutare a dimostrare che proviene dal drone rubato e non dall’aggressore. Inoltre, si immagini che la telecamera del drone si colleghi al cloud per scaricare parte della sua elaborazione ML. Il PUF può autenticare che la telecamera è legittima prima che il cloud esegua l’inferenza su video sensibili. Il cloud potrebbe verificare che il drone non sia stato manomesso fisicamente controllando che le risposte PUF non siano cambiate.\nLe PUF consentono tutta questa sicurezza attraverso la casualità intrinseca del loro comportamento di stimolo-risposta e il binding hardware. Senza dover memorizzare le chiavi esternamente, le PUF sono ideali per proteggere l’ML embedded con risorse limitate. Pertanto, offrono un vantaggio unico rispetto ad altri meccanismi.\n\n\nMeccanica\nIl principio di funzionamento alla base dei PUF, illustrato in Figura 14.9, comporta la generazione di una coppia “stimolo-risposta”, in cui un input specifico (lo stimolo) al circuito PUF determina un output (la risposta) che è determinato dalle proprietà fisiche uniche di quel circuito. Questo processo può essere paragonato a un meccanismo di impronte digitali per dispositivi elettronici. I dispositivi che utilizzano ML per elaborare i dati dei sensori possono utilizzare i PUF per proteggere la comunicazione tra dispositivi e impedire l’esecuzione di modelli ML su hardware contraffatto.\nFigura 14.9 illustra una panoramica delle basi dei PUF: a) PUF può essere pensato come un’impronta digitale unica per ogni pezzo di hardware; b) un PUF Ottico è uno speciale token di plastica che viene illuminato, creando un pattern a macchie unico che viene poi registrato; c) in un APUF (Arbiter PUF), i bit di stimolo selezionano percorsi diversi e un giudice decide quale è più veloce, dando una risposta di ‘1’ o ‘0’; d) in un PUF SRAM, la risposta è determinata dalla mancata corrispondenza nella tensione di soglia dei transistor, dove determinate condizioni portano a una risposta preferita di ‘1’. Ognuno di questi metodi utilizza caratteristiche specifiche dell’hardware per creare un identificatore univoco.\n\n\n\n\n\n\nFigura 14.9: Nozioni di base sui PUF. Fonte: Gao, Al-Sarawi, e Abbott (2020).\n\n\nGao, Yansong, Said F. Al-Sarawi, e Derek Abbott. 2020. «Physical unclonable functions». Nature Electronics 3 (2): 81–91. https://doi.org/10.1038/s41928-020-0372-5.\n\n\n\n\nSfide\nCi sono alcune sfide con i PUF. La risposta PUF può essere sensibile alle condizioni ambientali, come fluttuazioni di temperatura e tensione, portando a un comportamento incoerente di cui si deve tenere conto nella progettazione. Inoltre, poiché i PUF possono generare molte coppie stimolo-risposta uniche, gestire e garantire la coerenza di queste coppie per tutta la durata del dispositivo può essere impegnativo. Ultimo ma non meno importante, l’integrazione della tecnologia PUF può aumentare il costo di produzione complessivo di un dispositivo, sebbene possa far risparmiare sui costi di gestione delle chiavi durante il ciclo di vita del dispositivo.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Sicurezza e Privacy</span>"
    ]
  },
  {
    "objectID": "contents/core/privacy_security/privacy_security.it.html#problemi-di-privacy-nella-gestione-dei-dati",
    "href": "contents/core/privacy_security/privacy_security.it.html#problemi-di-privacy-nella-gestione-dei-dati",
    "title": "14  Sicurezza e Privacy",
    "section": "14.7 Problemi di Privacy nella Gestione dei Dati",
    "text": "14.7 Problemi di Privacy nella Gestione dei Dati\nLa gestione sicura ed etica dei dati personali e sensibili è fondamentale poiché l’apprendimento automatico permea dispositivi come smartphone, dispositivi indossabili ed elettrodomestici intelligenti. Per l’hardware medico, la gestione sicura ed etica dei dati è ulteriormente richiesta dalla legge tramite l’Health Insurance Portability and Accountability Act (HIPAA). Questi sistemi ML embedded presentano rischi unici per la privacy, data la loro intima vicinanza alla vita degli utenti.\n\n14.7.1 Tipi di Dati Sensibili\nI dispositivi ML embedded come quelli indossabili, assistenti domestici intelligenti e veicoli autonomi elaborano spesso dati altamente personali che richiedono un’attenta gestione per preservare la privacy dell’utente e prevenirne l’uso improprio. Esempi specifici includono referti medici e piani di trattamento elaborati da dispositivi indossabili per la salute, conversazioni private costantemente acquisite da assistenti domestici intelligenti e abitudini di guida dettagliate raccolte da auto connesse. La compromissione di tali dati sensibili può portare a gravi conseguenze come furto di identità, manipolazione emotiva, umiliazione pubblica ed abuso di sorveglianza di massa.\nI dati sensibili assumono molte forme: registri strutturati come elenchi di contatti e contenuti non strutturati come flussi audio e video conversazionali. In ambito medico, le “protected health information (PHI)” [informazioni sanitarie protette] vengono raccolte dai medici durante ogni interazione e sono fortemente regolamentate da rigide linee guida HIPAA. Anche al di fuori degli ambienti medici, i dati sensibili possono comunque essere raccolti sotto forma di Personally Identifiable Information (PII) [Informazioni di identificazione personale], definite come “qualsiasi rappresentazione di informazioni che consenta di dedurre ragionevolmente l’identità di un individuo a cui si applicano le informazioni con mezzi diretti o indiretti”. Esempi di PII includono indirizzi e-mail, numeri di previdenza sociale e numeri di telefono, tra gli altri campi. Le PII vengono raccolte in ambito medico e in altri contesti (applicazioni finanziarie, ecc.) e sono fortemente regolamentate dalle politiche del Dipartimento del Lavoro.\nAnche gli output dei modelli derivati potrebbero far trapelare indirettamente dettagli sugli individui. Oltre ai dati personali, anche algoritmi e set di dati proprietari garantiscono la protezione della riservatezza. Nella sezione Data Engineering, abbiamo trattato diversi argomenti in dettaglio.\nTecniche come la de-identificazione, l’aggregazione, l’anonimizzazione e la federazione possono aiutare a trasformare i dati sensibili in forme meno rischiose mantenendo al contempo l’utilità analitica. Tuttavia, controlli diligenti su accesso, crittografia, auditing, consenso, minimizzazione e pratiche di conformità sono ancora essenziali durante tutto il ciclo di vita dei dati. Regolamenti come GDPR categorizzano diverse classi di dati sensibili e prescrivono responsabilità in merito alla loro gestione etica. Standard come NIST 800-53 forniscono rigorose linee guida per il controllo della sicurezza per la protezione della riservatezza. Con la crescente dipendenza dal ML embedded, comprendere i rischi dei dati sensibili è fondamentale.\n\n\n14.7.2 Regolamenti Applicabili\nMolte applicazioni ML embedded gestiscono dati sensibili degli utenti in base alle normative HIPAA, GDPR e CCPA. Comprendere le protezioni imposte da queste leggi è fondamentale per creare sistemi conformi.\n\nLa norma sulla privacy HIPAA stabilisce che i fornitori di assistenza che svolgono determinate attività regolano la privacy e la sicurezza dei dati medici negli Stati Uniti, con severe sanzioni per le violazioni. Tutti i dispositivi ML embedded correlati alla salute, come dispositivi indossabili diagnostici o robot di assistenza, dovrebbero implementare controlli come audit trail, controlli di accesso e crittografia prescritti da HIPAA.\nIl GDPR impone trasparenza, limiti di conservazione e diritti degli utenti sui dati dei cittadini dell’UE, anche quando elaborati da aziende al di fuori dell’UE. I sistemi per la casa intelligente che catturano conversazioni familiari o pattern di posizione dovrebbero essere conformi al GDPR. I requisiti chiave includono la minimizzazione dei dati, la crittografia e meccanismi per il consenso e la cancellazione.\nIl CCPA, che si applica in California, protegge la privacy dei dati dei consumatori tramite disposizioni quali divulgazioni obbligatorie e diritti di opt-out: i gadget IoT come gli smart speaker e i fitness tracker utilizzati dai californiani rientrano probabilmente nel suo ambito.\nIl CCPA è stato il primo insieme di regolamenti specifici per stato in merito alle preoccupazioni sulla privacy. Dopo il CCPA, regolamenti simili sono stati emanati anche in altri 10 stati, con alcuni stati che hanno proposto progetti di legge per la protezione della privacy dei dati dei consumatori.\n\nInoltre, quando pertinenti all’applicazione, le norme specifiche del settore disciplinano telematica, servizi finanziari, servizi di pubblica utilità, ecc. Le best practice come “Privacy by design”, valutazioni di impatto e mantenimento di audit trail aiutano a incorporare la conformità se non è già richiesta dalla legge. Date le sanzioni potenzialmente costose, è consigliabile consultare team legali/di conformità quando si sviluppano sistemi ML embedded regolamentati.\n\n\n14.7.3 De-identificazione\nSe i dati medici vengono completamente de-identificati, le linee guida HIPAA non si applicano direttamente e ci sono molte meno normative. Tuttavia, i dati medici devono essere de-identificati utilizzando metodi HIPAA (metodi Safe Harbor o metodi Expert Determination) affinché le linee guida HIPAA non siano più applicabili.\n\nMetodi Safe Harbor\nI metodi Safe Harbor sono più comunemente utilizzati per de-identificare le informazioni sanitarie protette a causa delle risorse limitate necessarie rispetto ai metodi Expert Determination. La de-identificazione Safe Harbor richiede la pulizia dei set di dati di tutti i dati che rientrano in una delle 18 categorie. Le seguenti categorie sono elencate come informazioni sensibili in base allo standard Safe Harbor:\n\nNome, Localizzatore geografico, Data di nascita, Numero di telefono, Indirizzo e-mail, Indirizzi, Numeri di previdenza sociale, Numeri di cartella clinica, Numeri di beneficiari sanitari, Identificatori di dispositivi e Numeri di serie, Numeri di certificati/patenti (Certificato di nascita, Patente di guida, ecc.), Numeri di conto, Identificatori di veicoli, URL di siti Web, Foto a pieno facciale e Immagini comparabili, Identificatori biometrici, Qualsiasi altro identificatore univoco\n\nPer la maggior parte di queste categorie, tutti i dati devono essere rimossi indipendentemente dalle circostanze. Per altre categorie, tra cui informazioni geografiche e data di nascita, i dati possono essere rimossi parzialmente quanto basta per rendere le informazioni difficili da re-identificare. Ad esempio, se un codice postale è abbastanza grande, le prime 3 cifre possono rimanere poiché ci sono abbastanza persone nell’area geografica da rendere difficile la re-identificazione. Le date di nascita devono essere ripulite da tutti gli elementi tranne l’anno di nascita e tutte le età superiori a 89 anni devono essere aggregate in una categoria 90+.\n\n\nMetodi di Determinazione degli Esperti\nI metodi Safe Harbor funzionano per diversi casi di de-identificazione dei dati medici, sebbene in alcuni casi sia ancora possibile la re-identificazione. Ad esempio, supponiamo che si raccolgano dati su un paziente in una città urbana con un grande codice postale, ma è stata documentata una malattia rara di cui soffre, una malattia che colpisce solo 25 persone in tutta la città. Dati i dati geografici associati all’anno di nascita, è altamente possibile che qualcuno possa re-identificare questo individuo, il che rappresenta una violazione della privacy estremamente dannosa.\nIn casi unici come questi, sono preferiti metodi di de-identificazione dei dati di determinazione esperta. La de-identificazione di determinazione esperta richiede una “persona con conoscenza ed esperienza appropriate di principi e metodi statistici e scientifici generalmente accettati per rendere le informazioni non identificabili individualmente” per valutare un set di dati e determinare se il rischio di re-identificazione dei dati individuali in un dato set di dati in combinazione con dati disponibili al pubblico (registri di voto, ecc.), è estremamente ridotto.\nLa de-identificazione tramite Expert Determination è comprensibilmente più difficile da completare rispetto alla de-identificazione tramite Safe Harbor, a causa del costo e della fattibilità dell’accesso a un esperto per verificare la probabilità di re-identificazione di un set di dati. Tuttavia, in molti casi, è richiesta una Expert Determination per garantire che la re-identificazione dei dati sia estremamente improbabile.\n\n\n\n14.7.4 Riduzione al Minimo dei Dati\nLa riduzione al minimo dei dati comporta la raccolta, la conservazione e l’elaborazione solo dei dati utente necessari per ridurre i rischi per la privacy derivanti dai sistemi ML embedded. Si inizia limitando i tipi di dati e le istanze raccolte al minimo indispensabile per la funzionalità di base del sistema. Ad esempio, un modello di rilevamento degli oggetti raccoglie solo le immagini necessarie per quella specifica attività di visione artificiale. Allo stesso modo, un assistente vocale limiterebbe l’acquisizione audio a specifici comandi vocali anziché registrare in modo persistente i suoni ambientali.\nOve possibile, i dati temporanei che risiedono brevemente nella memoria senza archiviazione persistente forniscono un’ulteriore riduzione al minimo. Dovrebbe essere stabilita una chiara base giuridica, come il consenso dell’utente, per la raccolta e la conservazione. Il sandboxing e i controlli di accesso impediscono l’uso non autorizzato oltre le attività previste. I periodi di conservazione dovrebbero essere definiti in base allo scopo, con procedure di eliminazione sicura che rimuovono i dati scaduti.\nLa riduzione al minimo dei dati può essere suddivisa in 3 categorie:\n\n“I dati devono essere adeguati rispetto allo scopo perseguito”. L’omissione di dati può limitare l’accuratezza dei modelli addestrati sui dati e qualsiasi utilità generale di un set di dati. La minimizzazione dei dati richiede che una quantità minima di dati venga raccolta dagli utenti durante la creazione di un set di dati che aggiunge valore ad altri.\nI dati raccolti dagli utenti devono essere rilevanti allo scopo della raccolta dati.\nI dati degli utenti dovrebbero essere limitati ai soli dati necessari per soddisfare lo scopo della raccolta dati iniziale. Se è possibile ottenere risultati altrettanto solidi e accurati da un set di dati più piccolo, non dovrebbero essere raccolti dati aggiuntivi oltre questo set di dati più piccolo.\n\nTecniche emergenti come la privacy differenziale, l’addestramento federato e la generazione di dati sintetici consentono informazioni utili derivate da dati utente meno grezzi. L’esecuzione di mappature del flusso di dati e valutazioni di impatto aiutano a identificare le opportunità per ridurre al minimo l’utilizzo di dati grezzi.\nMetodologie come Privacy by Design (Cavoukian 2009) considerano tale minimizzazione all’inizio dell’architettura del sistema. Anche normative come il GDPR impongono principi di minimizzazione dei dati. Con un approccio multilayer nei regni legale, tecnico e di processo, la minimizzazione dei dati limita i rischi nei prodotti ML embedded.\n\nCavoukian, Ann. 2009. «Privacy by design». Office of the Information and Privacy Commissioner.\n\nCaso di Studio: Minimizzazione dei Dati Basata sulle Prestazioni\nLa minimizzazione dei dati basata sulle prestazioni (Biega et al. 2020) si concentra sull’espansione della terza categoria di minimizzazione dei dati menzionata sopra, ovvero la limitazione. Definisce specificamente la robustezza dei risultati del modello su un dato set di dati tramite determinate metriche delle prestazioni, in modo che i dati non debbano essere raccolti ulteriormente se non migliorano significativamente le prestazioni. Le metriche delle prestazioni possono essere divise in due categorie:\n\nBiega, Asia J., Peter Potash, Hal Daumé, Fernando Diaz, e Michèle Finck. 2020. «Operationalizing the Legal Principle of Data Minimization for Personalization». In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval, a cura di Jimmy Huang, Yi Chang, Xueqi Cheng, Jaap Kamps, Vanessa Murdock, Ji-Rong Wen, e Yiqun Liu, 399–408. ACM. https://doi.org/10.1145/3397271.3401034.\n\nPrestazioni di minimizzazione dei dati globali: Soddisfatte se un set di dati riduce al minimo la quantità di dati per utente mentre le sue prestazioni medie su tutti i dati sono paragonabili alle prestazioni medie del set di dati originale non minimizzato.\nPrestazioni di minimizzazione dei dati per utente: Soddisfatte se un set di dati riduce al minimo la quantità di dati per utente mentre le prestazioni minime dei dati utente individuali sono paragonabili a quelle dei dati utente individuali nel set di dati originale non minimizzato.\n\nLa riduzione al minimo dei dati basata sulle prestazioni può essere sfruttata in impostazioni di apprendimento automatico, inclusi algoritmi di raccomandazione di film e impostazioni di e-commerce.\nLa riduzione al minimo dei dati globali è molto più fattibile della riduzione al minimo dei dati per utente, data la differenza molto più significativa nelle perdite per utente tra i set di dati ridotti al minimo e quelli originali.\n\n\n\n14.7.5 Consenso e Trasparenza\nUn consenso e una trasparenza significativi sono fondamentali quando si raccolgono dati utente per prodotti ML embedded come smart speaker, dispositivi indossabili e veicoli autonomi. Quando viene configurato per la prima volta. Idealmente, il dispositivo dovrebbe spiegare chiaramente quali tipi di dati vengono raccolti, per quali scopi, come vengono elaborati e le policy di conservazione. Ad esempio, uno smart speaker potrebbe raccogliere campioni vocali per addestrare il riconoscimento vocale e profili vocali personalizzati. Durante l’uso, promemoria e opzioni della dashboard forniscono una trasparenza continua su come vengono gestiti i dati, come riepiloghi settimanali di frammenti vocali acquisiti. Le opzioni di controllo consentono di revocare o limitare il consenso, come disattivare l’archiviazione dei profili vocali.\nI flussi di consenso dovrebbero fornire controlli granulari che vadano oltre le semplici scelte binarie sì/no. Ad esempio, gli utenti potrebbero acconsentire selettivamente a determinati utilizzi dei dati, come l’addestramento al riconoscimento vocale, ma non alla personalizzazione. I focus group e i test di usabilità con gli utenti target modellano le interfacce di consenso e la formulazione delle policy sulla privacy per ottimizzare la comprensione e il controllo. Il rispetto dei diritti degli utenti, come l’eliminazione e la rettifica dei dati, dimostra affidabilità. Un gergo legale vago ostacola la trasparenza. Regolamenti come GDPR e CCPA rafforzano i requisiti di consenso. Un consenso ponderato e la trasparenza forniscono agli utenti l’agenzia sui propri dati, creando al contempo fiducia nei prodotti ML incorporati attraverso una comunicazione e un controllo aperti.\n\n\n14.7.6 Problemi di Privacy nell’Apprendimento Automatico\n\nIA Generativa\nSono aumentate anche le preoccupazioni sulla privacy e sulla sicurezza con l’uso pubblico di modelli di intelligenza artificiale generativa, tra cui GPT4 di OpenAI e altri LLM. ChatGPT, in particolare, è stato discusso più di recente in merito alla privacy, date tutte le informazioni personali raccolte dagli utenti di ChatGPT. Nel giugno 2023 è stata intentata una class action contro ChatGPT a causa del timore che fosse stato addestrato su informazioni mediche e personali proprietarie senza le dovute autorizzazioni o consensi. Come risultato di queste preoccupazioni sulla privacy, molte aziende hanno proibito ai propri dipendenti di accedere a ChatGPT e di caricare informazioni aziendali private sul chatbot. Inoltre, ChatGPT è suscettibile al “prompt injection” e altri attacchi alla sicurezza che potrebbero compromettere la privacy dei dati proprietari su cui è stato addestrato.\n\nCaso di Studio: Aggiramento delle Misure di Sicurezza di ChatGPT\nMentre ChatGPT ha istituito delle protezioni per impedire alle persone di accedere a informazioni private ed eticamente discutibili, diversi individui sono riusciti a bypassare queste protezioni tramite “prompt injection” e altri attacchi alla sicurezza. Come dimostrato in Figura 14.10, gli utenti possono bypassare le protezioni di ChatGPT per imitare il tono di una “nonna defunta” per imparare come bypassare un firewall per applicazioni Web (Gupta et al. 2023).\n\n\n\n\n\n\nFigura 14.10: Gioco di ruolo della nonna per bypassare le restrizioni di sicurezza. Fonte: Gupta et al. (2023).\n\n\n\nInoltre, gli utenti hanno anche utilizzato con successo la psicologia inversa per manipolare ChatGPT e accedere a informazioni inizialmente proibite dal modello. In Figura 14.11, a un utente viene inizialmente impedito di scoprire siti Web di pirateria tramite ChatGPT, ma può bypassare queste restrizioni utilizzando la psicologia inversa.\n\n\n\n\n\n\nFigura 14.11: Psicologia inversa per bypassare le restrizioni di sicurezza. Fonte: Gupta et al. (2023).\n\n\nGupta, Maanak, Charankumar Akiri, Kshitiz Aryal, Eli Parker, e Lopamudra Praharaj. 2023. «From ChatGPT to ThreatGPT: Impact of Generative AI in Cybersecurity and Privacy». IEEE Access 11: 80218–45. https://doi.org/10.1109/access.2023.3300381.\n\n\nLa facilità con cui gli attacchi alla sicurezza possono manipolare ChatGPT è preoccupante, date le informazioni private su cui è stato addestrato senza consenso. Ulteriori ricerche sulla privacy dei dati in LLM e sull’intelligenza artificiale generativa dovrebbero concentrarsi sull’impedire al modello di essere così ingenuo da indurre attacchi di “injection”.\n\n\n\nCancellazione dei Dati\nMolte delle normative precedentemente menzionate, incluso il GDPR, includono una clausola sul “diritto all’oblio”. Questa clausola afferma essenzialmente che “l’interessato ha il diritto di ottenere dal titolare del trattamento la cancellazione dei dati personali che lo riguardano senza indebito ritardo”. Tuttavia, in diversi casi, anche se i dati dell’utente sono stati cancellati da una piattaforma, i dati vengono cancellati solo parzialmente se un modello di apprendimento automatico è stato addestrato su questi dati per scopi separati. Attraverso metodi simili agli attacchi di inferenza di appartenenza, altri individui possono ancora dedurre i dati di addestramento su cui è stato addestrato un modello, anche se la presenza dei dati è stata esplicitamente rimossa online.\nUn approccio per affrontare i problemi di privacy con i dati di addestramento dell’apprendimento automatico è stato attraverso metodi di privacy differenziali. Ad esempio, aggiungendo rumore laplaciano nel set di addestramento, un modello può essere robusto agli attacchi di inferenza di appartenenza, impedendo il recupero dei dati eliminati. Un altro approccio per impedire che i dati eliminati vengano dedotti da attacchi alla sicurezza è semplicemente riaddestrare il modello da zero sui dati rimanenti. Poiché questo processo è dispendioso in termini di tempo e di elaborazione dati, altri ricercatori hanno tentato di affrontare le preoccupazioni sulla privacy relative all’inferenza dei dati di training del modello tramite un processo chiamato “machine unlearning”, in cui un modello itera attivamente su se stesso per rimuovere l’influenza dei dati “dimenticati” su cui potrebbe essere stato addestrato, come menzionato di seguito.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Sicurezza e Privacy</span>"
    ]
  },
  {
    "objectID": "contents/core/privacy_security/privacy_security.it.html#tecniche-ml-per-la-tutela-della-privacy",
    "href": "contents/core/privacy_security/privacy_security.it.html#tecniche-ml-per-la-tutela-della-privacy",
    "title": "14  Sicurezza e Privacy",
    "section": "14.8 Tecniche ML per la Tutela della Privacy",
    "text": "14.8 Tecniche ML per la Tutela della Privacy\nSono state sviluppate molte tecniche per preservare la privacy, ognuna delle quali affronta diversi aspetti e sfide per la sicurezza dei dati. Questi metodi possono essere ampiamente categorizzati in diverse aree chiave: Differential Privacy, che si concentra sulla privacy statistica negli output dei dati; Federated Learning, che enfatizza l’elaborazione decentralizzata dei dati; Homomorphic Encryption e Secure Multi-party Computation (SMC), entrambi abilitanti calcoli sicuri su dati crittografati o privati; Data Anonymization e Data Masking and Obfuscation, che alterano i dati per proteggere le identità individuali; Private Set Intersection e Zero-Knowledge Proofs, che facilitano confronti e convalide di dati sicuri; Decentralized Identifiers (DID) per identità digitali auto-sovrane; Privacy-Preserving Record Linkage (PPRL), che collega i dati tra le fonti senza esposizione; Synthetic Data Generation, che crea set di dati artificiali per analisi sicure; e Adversarial Learning Techniques, che migliora la resistenza dei dati o dei modelli agli attacchi alla privacy.\nData l’ampia gamma di queste tecniche, non è possibile approfondire ciascuna di esse in un singolo corso o discussione, e tanto meno che qualcuno possa conoscerle tutte nei loro gloriosi dettagli. Pertanto, esploreremo alcune tecniche specifiche in modo relativamente dettagliato, fornendo una comprensione più approfondita dei loro principi, applicazioni e delle sfide uniche per la privacy che affrontano nell’apprendimento automatico. Questo approccio mirato ci fornirà una comprensione più completa e pratica dei principali metodi di tutela della privacy nei moderni sistemi ML.\n\n14.8.1 Privacy Differenziale\n\nIdea Centrale\nLa “Differential Privacy” Privacy Differenziale è un framework per quantificare e gestire la privacy degli individui in un set di dati (Dwork et al. 2006). Fornisce una garanzia matematica che la privacy degli individui nel set di dati non verrà compromessa, indipendentemente da qualsiasi conoscenza aggiuntiva che un aggressore potrebbe possedere. L’idea fondamentale della privacy differenziale è che il risultato di qualsiasi analisi (come una query statistica) dovrebbe essere essenzialmente lo stesso, indipendentemente dal fatto che i dati di un individuo siano inclusi nel set di dati o meno. Ciò significa che osservando il risultato dell’analisi, non è possibile determinare se i dati di un individuo siano stati utilizzati nel calcolo.\n\nDwork, Cynthia, Frank McSherry, Kobbi Nissim, e Adam Smith. 2006. «Calibrating Noise to Sensitivity in Private Data Analysis». In Theory of Cryptography, a cura di Shai Halevi e Tal Rabin, 265–84. Berlin, Heidelberg: Springer Berlin Heidelberg. https://doi.org/10.1007/11681878\\_14.\nAd esempio, supponiamo che un database contenga cartelle cliniche di 10 pazienti. Vogliamo pubblicare statistiche sulla prevalenza del diabete in questo campione senza rivelare le condizioni di un paziente. Per fare ciò, potremmo aggiungere una piccola quantità di rumore casuale al conteggio reale prima di pubblicarlo. Se il numero reale di pazienti diabetici è 6, potremmo aggiungere rumore da una distribuzione di Laplace per ottenere casualmente 5, 6 o 7, ciascuno con una certa probabilità. Un osservatore ora non può dire se un singolo paziente ha il diabete basandosi solo sull’output rumoroso. Il risultato della query è simile a se i dati di ogni paziente sono inclusi o esclusi. Questa è la privacy differenziale. Più formalmente, un algoritmo randomizzato soddisfa la privacy differenziale ε se, per qualsiasi database vicino D e Dʹ che differisce solo per una voce, la probabilità di qualsiasi risultato cambia al massimo di un fattore ε. Un ε inferiore fornisce maggiori garanzie di privacy.\nIl meccanismo di Laplace è uno dei metodi più semplici e comunemente utilizzati per ottenere la privacy differenziale. Comporta l’aggiunta di rumore che segue una distribuzione di Laplace ai dati o ai risultati delle query. A parte il Meccanismo di Laplace, il principio generale di aggiunta di rumore è fondamentale per la Privacy differenziale. L’idea è di aggiungere rumore casuale ai dati o ai risultati di una query. Il rumore è calibrato per garantire la necessaria garanzia di privacy mantenendo i dati utili.\nMentre la distribuzione di Laplace è comune, possono essere utilizzate anche altre distribuzioni come quella gaussiana. Il rumore di Laplace è utilizzato per la privacy differenziale rigorosa ε per query a bassa sensibilità. Al contrario, le distribuzioni gaussiane possono essere utilizzate quando la privacy non è garantita, nota come privacy differenziale (ϵ, 𝛿). In questa versione rilassata della privacy differenziale, epsilon e delta definiscono la quantità di privacy garantita quando si rilasciano informazioni o un modello correlato a un set di dati. Epsilon stabilisce un limite su quanta informazione può essere appresa sui dati in base all’output. Allo stesso tempo, delta consente una piccola probabilità che la garanzia della privacy venga violata. La scelta tra Laplace, gaussiana e altre distribuzioni dipenderà dai requisiti specifici della query e del set di dati e dal compromesso tra Privacy e accuratezza.\nPer illustrare il compromesso tra Privacy e accuratezza nella Privacy differenziale (\\(\\epsilon\\), \\(\\delta\\)), i seguenti grafici in Figura 14.12 mostrano i risultati sull’accuratezza per diversi livelli di rumore sul set di dati MNIST, un ampio set di dati di cifre scritte a mano (Abadi et al. 2016). Il valore delta (linea nera; asse y destro) indica il livello di rilassamento della privacy (un valore elevato indica che la Privacy è meno rigorosa). Man mano che la Privacy diventa più rilassata, aumenta l’accuratezza del modello.\n\n\n\n\n\n\nFigura 14.12: Compromesso tra privacy e accuratezza. Fonte: Abadi et al. (2016).\n\n\nAbadi, Martin, Andy Chu, Ian Goodfellow, H. Brendan McMahan, Ilya Mironov, Kunal Talwar, e Li Zhang. 2016. «Deep Learning with Differential Privacy». In Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security, 308–18. CCS ’16. New York, NY, USA: ACM. https://doi.org/10.1145/2976749.2978318.\n\n\nI punti chiave da ricordare sulla privacy differenziale sono i seguenti:\n\nAggiunta di Rumore: La tecnica fondamentale nella Privacy differenziale è l’aggiunta di rumore casuale controllato ai dati o ai risultati delle query. Questo rumore maschera il contributo dei singoli dati.\nAtto di Bilanciamento: C’è un equilibrio tra Privacy e accuratezza. Più rumore (ϵ inferiore) nei dati significa maggiore Privacy ma minore accuratezza nei risultati del modello.\nUniversalità: La privacy differenziale non si basa su ipotesi su ciò che sa un aggressore.s. Ciò lo rende robusto contro gli attacchi di re-identificazione, in cui un aggressore cerca di scoprire dati individuali.\nApplicabilità: Può essere applicato a vari tipi di dati e query, rendendolo uno strumento versatile per l’analisi dei dati che preserva la privacy.\n\n\n\nCompromessi\nCi sono diversi compromessi da fare con la Privacy differenziale, come nel caso di qualsiasi algoritmo. Ma concentriamoci sui compromessi specifici computazionali, poiché ci interessano i sistemi ML. Ci sono alcune considerazioni e compromessi computazionali chiave quando si implementa la privacy differenziale in un sistema di apprendimento automatico:\nGenerazione di Rumore: L’implementazione della privacy differenziale introduce diversi compromessi computazionali importanti rispetto alle tecniche di apprendimento automatico standard. Una considerazione importante è la necessità di generare in modo sicuro rumore casuale da distribuzioni come Laplace o Gaussiana che vengono aggiunte ai risultati delle query e agli output del modello. La generazione di numeri casuali crittografici di alta qualità può essere computazionalmente costosa.\nAnalisi di Sensibilità: Un altro requisito fondamentale è il monitoraggio rigoroso della sensibilità degli algoritmi sottostanti ai singoli punti dati che vengono aggiunti o rimossi. Questa analisi di sensibilità globale è necessaria per calibrare correttamente i livelli di rumore. Tuttavia, l’analisi della sensibilità del caso peggiore può aumentare sostanzialmente la complessità computazionale per complesse procedure di addestramento del modello e pipeline di dati.\nGestione del budget per la privacy: La gestione del budget per la perdita della privacy su più query e iterazioni di apprendimento è un altro sovraccarico contabile. Il sistema deve tenere traccia dei costi cumulativi per la privacy e comporli per spiegare le garanzie di privacy complessive. Ciò aggiunge un onere computazionale che va oltre la semplice esecuzione di query o modelli di addestramento.\nCompromessi tra batch e online: Per i sistemi di apprendimento online con query continue ad alto volume, gli algoritmi differenzialmente privati richiedono nuovi meccanismi per mantenere l’utilità e prevenire troppe perdite di privacy accumulate poiché ogni query può potenzialmente alterare il budget per la privacy. L’elaborazione offline in batch è più semplice da una prospettiva computazionale poiché elabora i dati in grandi batch, dove ogni batch viene trattato come una singola query. I dati sparsi ad alta dimensionalità aumentano anche le difficoltà dell’analisi di sensibilità.\nAddestramento distribuito: Quando si addestrano modelli utilizzando approcci distribuiti o federati, sono necessari nuovi protocolli crittografici per tracciare e limitare le “fughe” di privacy tra i nodi. Il calcolo multi-parti sicuro con dati crittografati per la Privacy differenziale aggiunge un carico computazionale sostanziale.\nMentre la Privacy differenziale fornisce solide garanzie formali di privacy, la sua implementazione rigorosa richiede aggiunte e modifiche alla pipeline di apprendimento automatico a un costo computazionale. La gestione di queste spese generali preservando l’accuratezza del modello rimane un’area di ricerca attiva.\n\n\nCaso di Studio: Privacy Differenziale in Apple\nL’implementazione della Privacy differenziale da parte di Apple in iOS e MacOS fornisce un importante esempio concreto di come la Privacy differenziale può essere distribuita su larga scala. Apple voleva raccogliere statistiche aggregate sull’utilizzo nel proprio ecosistema per migliorare prodotti e servizi, ma mirava a farlo senza compromettere la privacy dei singoli utenti.\nPer raggiungere questo obiettivo, ha implementato tecniche di privacy differenziale direttamente sui dispositivi degli utenti per rendere anonimi i punti dati prima di inviarli ai server Apple. In particolare, Apple utilizza il meccanismo di Laplace per iniettare rumore casuale attentamente calibrato. Ad esempio, supponiamo che la cronologia delle posizioni di un utente contenga [Lavoro, Casa, Lavoro, Palestra, Lavoro, Casa]. In tal caso, la versione privata differenziale potrebbe sostituire le posizioni esatte con un campione rumoroso come [Palestra, Casa, Lavoro, Lavoro, Casa, Lavoro].\nApple regola la distribuzione del rumore di Laplace per fornire un elevato livello di privacy preservando al contempo l’utilità delle statistiche aggregate. L’aumento dei livelli di rumore fornisce maggiori garanzie di privacy (valori ε inferiori nella terminologia DP) ma può ridurre l’utilità dei dati. Gli ingegneri della privacy di Apple hanno ottimizzato empiricamente questo compromesso in base ai loro obiettivi di prodotto.\nApple ottiene statistiche aggregate ad alta fedeltà aggregando centinaia di milioni di punti dati rumorosi dai dispositivi. Ad esempio, possono analizzare le funzionalità delle nuove app iOS mascherando i comportamenti delle app di qualsiasi utente. Il calcolo sul dispositivo evita di inviare dati grezzi ai server Apple.\nIl sistema utilizza la generazione di numeri casuali sicuri basata su hardware per campionare in modo efficiente dalla distribuzione di Laplace sui dispositivi. Apple ha anche dovuto ottimizzare i suoi algoritmi e pipeline differenzialmente privati per operare sotto i vincoli computazionali dell’hardware degli utenti.\nNumerosi audit di terze parti hanno verificato che il sistema Apple fornisce rigorose protezioni differenziali della privacy in linea con le loro politiche dichiarate. Naturalmente, le ipotesi sulla composizione nel tempo e sui potenziali rischi di reidentificazione sono ancora valide. L’implementazione di Apple mostra come la privacy differenziale può essere realizzata in grandi prodotti del mondo reale quando supportata da sufficienti risorse ingegneristiche.\n\n\n\n\n\n\nEsercizio 14.1: Privacy Differenziale - Privacy TensorFlow\n\n\n\n\n\nVolete addestrare un modello ML senza compromettere i segreti di nessuno? La Privacy differenziale è come un superpotere per i dati! In questo Colab, useremo TensorFlow Privacy per aggiungere rumore speciale durante l’addestramento. Ciò rende molto più difficile per chiunque determinare se sono stati utilizzati i dati di una singola persona, anche se hanno modi furtivi per sbirciare il modello.\n\n\n\n\n\n\n\n14.8.2 Il Federated Learning\n\nIdea Centrale\nIl “Federated Learning (FL)” è un tipo di apprendimento automatico in cui un modello viene creato e distribuito su più dispositivi o server mantenendo localizzati i dati di training. È stato precedentemente discusso nel capitolo Ottimizzazioni del modello. Tuttavia, lo riepilogheremo brevemente qui per completarlo e concentrarci su cose che riguardano questo capitolo.\nFL addestra modelli di apprendimento automatico su reti decentralizzate di dispositivi o sistemi, mantenendo tutti i dati di addestramento localizzati. Figura 14.13 illustra questo processo: ogni dispositivo partecipante sfrutta i propri dati locali per calcolare gli aggiornamenti del modello, che vengono poi aggregati per creare un modello globale migliorato. Tuttavia, i dati di training grezzi non vengono mai condivisi, trasferiti o compilati direttamente. Questo approccio di tutela della privacy consente lo sviluppo congiunto di modelli ML senza centralizzare i dati di training potenzialmente sensibili in un unico posto.\n\n\n\n\n\n\nFigura 14.13: Ciclo di Vita dell’Apprendimento Federato. Fonte: Jin et al. (2020).\n\n\nJin, Yilun, Xiguang Wei, Yang Liu, e Qiang Yang. 2020. «Towards Utilizing Unlabeled Data in Federated Learning: A Survey and Prospective». arXiv preprint arXiv:2002.11545, febbraio. http://arxiv.org/abs/2002.11545v2.\n\n\nUno degli algoritmi di aggregazione di modelli più comuni è Federated Averaging (FedAvg), in cui il modello globale viene creato calcolando la media di tutti i parametri dai parametri locali. Mentre FedAvg funziona bene con dati indipendenti e distribuiti in modo identico (IID), algoritmi alternativi come Federated Proximal (FedProx) sono fondamentali nelle applicazioni del mondo reale in cui i dati sono spesso non IID. FedProx è progettato per il processo FL quando c’è una significativa eterogeneità negli aggiornamenti client a causa di diverse distribuzioni di dati tra dispositivi, capacità di calcolo o quantità variabili di dati.\nLasciando i dati grezzi distribuiti e scambiando solo aggiornamenti temporanei del modello, l’apprendimento federato fornisce un’alternativa più sicura e che migliora la privacy alle tradizionali pipeline di apprendimento automatico centralizzate. Ciò consente alle organizzazioni e agli utenti di trarre vantaggio in modo collaborativo da modelli condivisi mantenendo al contempo il controllo e la proprietà sui dati sensibili. La natura decentralizzata di FL lo rende anche robusto per singoli punti di errore.\nSi immagini un gruppo di ospedali che desidera collaborare a uno studio per prevedere i risultati dei pazienti in base ai loro sintomi. Tuttavia, non possono condividere i dati dei pazienti a causa di problemi di privacy e normative come HIPAA. Ecco come il Federated Learning può aiutare.\n\nAddestramento Locale: Ogni ospedale addestra un modello di apprendimento automatico sui dati dei pazienti. Questo addestramento avviene localmente, il che significa che i dati non lasciano mai i server dell’ospedale.\nCondivisione del Modello: Dopo l’addestramento, ogni ospedale invia solo il modello (in particolare, i suoi parametri o pesi) a un server centrale. Non invia alcun dato del paziente.\nModelli di Aggregazione: Il server centrale aggrega questi modelli da tutti gli ospedali in un singolo modello più robusto. Questo processo in genere comporta la media dei parametri del modello.\nVantaggio: Il risultato è un modello di apprendimento automatico che ha appreso da un’ampia gamma di dati dei pazienti senza condividere dati sensibili o rimuoverli dalla loro posizione originale.\n\n\n\nCompromessi\nCi sono diversi aspetti correlati alle prestazioni del sistema di FL nei sistemi di apprendimento automatico. Sarebbe saggio comprendere questi compromessi perché non esiste un “pranzo gratis” per preservare la privacy tramite FL (Li et al. 2020).\n\nLi, Tian, Anit Kumar Sahu, Ameet Talwalkar, e Virginia Smith. 2020. «Federated Learning: Challenges, Methods, and Future Directions». IEEE Signal Processing Magazine 37 (3): 50–60. https://doi.org/10.1109/msp.2020.2975749.\nSovraccarico di Comunicazione e Vincoli di Rete: Nel FL, una delle sfide più significative è la gestione del sovraccarico di comunicazione. Ciò comporta la frequente trasmissione di aggiornamenti del modello tra un server centrale e numerosi dispositivi client, che può richiedere molta larghezza di banda. Il numero totale di round di comunicazione e la dimensione dei messaggi trasmessi per round devono essere ridotti per ridurre ulteriormente la comunicazione. Ciò può comportare un traffico di rete sostanziale, soprattutto in scenari con molti partecipanti. Inoltre, la latenza diventa un fattore critico: il tempo impiegato per inviare, aggregare e ridistribuire questi aggiornamenti può causare ritardi. Ciò influisce sul tempo di training complessivo e ha un impatto sulla reattività del sistema e sulle capacità in tempo reale. Gestire questa comunicazione riducendo al minimo l’utilizzo della larghezza di banda e la latenza è fondamentale per implementare FL.\nCarico di Calcolo sui Dispositivi Locali: FL si basa su dispositivi client (come smartphone o dispositivi IoT, che sono particolarmente importanti in TinyML) per l’addestramento del modello, che spesso hanno una potenza di calcolo e una durata della batteria limitate. L’esecuzione di algoritmi complessi di apprendimento automatico in locale può mettere a dura prova queste risorse, portando a potenziali problemi di prestazioni. Inoltre, le capacità di questi dispositivi possono variare in modo significativo, con conseguenti contributi non uniformi al processo di addestramento del modello. Alcuni dispositivi elaborano gli aggiornamenti in modo più rapido ed efficiente di altri, portando a disparità nel processo di apprendimento. Bilanciare il carico computazionale per garantire una partecipazione e un’efficienza coerenti su tutti i dispositivi è una sfida fondamentale in FL.\nEfficienza dell’Addestramento del Modello: La natura decentralizzata di FL può influire sull’efficienza dell’addestramento del modello. Raggiungere la convergenza, in cui il modello non migliora più in modo significativo, può essere più lento in FL rispetto ai metodi di addestramento centralizzati. Ciò è particolarmente vero nei casi in cui i dati sono non IID (non indipendenti e distribuiti in modo identico) tra i dispositivi. Inoltre, gli algoritmi utilizzati per aggregare gli aggiornamenti del modello svolgono un ruolo fondamentale nel processo di addestramento. La loro efficienza influisce direttamente sulla velocità e l’efficacia dell’apprendimento. Sviluppare e implementare algoritmi in grado di gestire le complessità di FL garantendo al contempo una convergenza tempestiva è essenziale per le prestazioni del sistema.\nSfide di Scalabilità: La scalabilità è una preoccupazione significativa in FL, soprattutto con l’aumento del numero di dispositivi partecipanti. La gestione e il coordinamento degli aggiornamenti del modello da molti dispositivi aggiungono complessità e possono mettere a dura prova il sistema. È fondamentale garantire che l’architettura del sistema possa gestire in modo efficiente questo carico aumentato senza degradare le prestazioni. Ciò implica non solo la gestione degli aspetti computazionali e di comunicazione, ma anche il mantenimento della qualità e della coerenza del modello man mano che aumenta la scala dell’operazione. Una sfida fondamentale è la progettazione di sistemi FL che si adattino in modo efficace mantenendo le prestazioni.\nSincronizzazione e Coerenza dei Dati: Garantire la sincronizzazione dei dati e mantenere la coerenza del modello su tutti i dispositivi partecipanti in FL è una sfida. Mantenere tutti i dispositivi sincronizzati con l’ultima versione del modello può essere difficile in ambienti con connettività intermittente o dispositivi che vanno offline periodicamente. Inoltre, è fondamentale mantenere la coerenza nel modello addestrato, soprattutto quando si ha a che fare con un’ampia gamma di dispositivi con diverse distribuzioni dei dati e frequenze di aggiornamento. Ciò richiede sofisticate strategie di sincronizzazione e aggregazione per garantire che il modello finale rifletta accuratamente gli addestramenti da tutti i dispositivi.\nConsumo Energetico: Il consumo energetico dei dispositivi client in FL è un fattore critico, in particolare per i dispositivi alimentati a batteria come smartphone e altri dispositivi TinyML/IoT. Le richieste di elaborazione dei modelli di training a livello locale possono portare a un notevole consumo della batteria, il che potrebbe scoraggiare la partecipazione continua al processo FL. È essenziale bilanciare i requisiti di elaborazione dei modelli di training con l’efficienza energetica. Ciò comporta l’ottimizzazione di algoritmi e processi di training per ridurre il consumo energetico e ottenere risultati di addestramento efficaci. Garantire un funzionamento efficiente dal punto di vista energetico è fondamentale per l’accettazione da parte dell’utente e la sostenibilità dei sistemi FL.\n\n\nCaso di Studio: Addestramento Federato per Set di Dati Sanitari Collaborativi\nNel settore sanitario e farmaceutico, le organizzazioni spesso detengono grandi quantità di dati preziosi, ma condividerli direttamente è irto di sfide. Normative severe come GDPR e HIPAA, nonché preoccupazioni sulla protezione della proprietà intellettuale, rendono quasi impossibile combinare set di dati tra aziende. Tuttavia, la collaborazione rimane essenziale per settori in evoluzione come la scoperta di farmaci e l’assistenza ai pazienti. L’addestramento federato offre una soluzione unica consentendo alle aziende di addestrare in modo collaborativo modelli di apprendimento automatico senza mai condividere i propri dati grezzi. Questo approccio garantisce che ogni organizzazione mantenga il pieno controllo dei propri dati, continuando a beneficiare delle intuizioni collettive del gruppo.\nIl progetto MELLODDY, un’iniziativa fondamentale in Europa, esemplifica come l’addestramento federato possa superare queste barriere (Heyndrickx et al. 2023). MELLODDY ha riunito dieci aziende farmaceutiche per creare la più grande libreria di composti chimici condivisa mai assemblata, che comprende oltre 21 milioni di molecole e 2,6 miliardi di dati sperimentali. Nonostante lavorassero con dati sensibili e proprietari, le aziende hanno collaborato in modo sicuro per migliorare i modelli predittivi per lo sviluppo dei farmaci.\n\nHeyndrickx, Wouter, Lewis Mervin, Tobias Morawietz, Noé Sturm, Lukas Friedrich, Adam Zalewski, Anastasia Pentina, et al. 2023. «Melloddy: Cross-pharma federated learning at unprecedented scale unlocks benefits in qsar without compromising proprietary information». Journal of chemical information and modeling 64 (7): 2331–44. https://pubs.acs.org/doi/10.1021/acs.jcim.3c00799.\nI risultati sono stati notevoli. Mettendo in comune le informazioni tramite l’addestramento federato, ogni azienda ha migliorato significativamente la propria capacità di identificare promettenti farmaci candidati. L’accuratezza predittiva è migliorata mentre i modelli hanno anche acquisito una più ampia applicabilità a diversi set di dati. MELLODDY ha dimostrato che l’addestramento federato non solo preserva la privacy, ma sblocca anche nuove opportunità di innovazione consentendo una collaborazione su larga scala basata sui dati. Questo approccio evidenzia un futuro in cui le aziende possono lavorare insieme per risolvere problemi complessi senza sacrificare la sicurezza o la proprietà dei dati.\n\n\n\n14.8.3 Machine Unlearning\n\nIdea Centrale\nIl “Machine unlearning” è un processo abbastanza nuovo che descrive come l’influenza di un sottoinsieme di dati di training può essere rimossa dal modello. Sono stati utilizzati diversi metodi per eseguire l’unlearning automatico e rimuovere l’influenza di un sottoinsieme di dati di training dal modello finale. Un approccio di base potrebbe consistere semplicemente nel perfezionare il modello per più epoche solo sui dati che dovrebbero essere ricordati per ridurre l’influenza dei dati “dimenticati” dal modello. Poiché questo approccio non rimuove esplicitamente l’influenza dei dati che dovrebbero essere cancellati, sono ancora possibili attacchi di inferenza di appartenenza, quindi i ricercatori hanno adottato altri approcci per disimparare i dati da un modello in modo esplicito. Un tipo di approccio adottato dai ricercatori include l’adeguamento della funzione di perdita del modello per trattare le perdite del “set di dimenticanza esplicito” (dati da disimparare) e del “set di conservazione” (dati rimanenti che dovrebbero ancora essere ricordati) in modo diverso (Tarun et al. 2022; Khan e Swaroop 2021). Figura 14.14 illustra alcune delle applicazioni di Machine-unlearning.\n\nTarun, Ayush K, Vikram S Chundawat, Murari Mandal, e Mohan Kankanhalli. 2022. «Deep Regression Unlearning». ArXiv preprint abs/2210.08196 (ottobre). http://arxiv.org/abs/2210.08196v2.\n\nKhan, Mohammad Emtiyaz, e Siddharth Swaroop. 2021. «Knowledge-Adaptation Priors». In Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, a cura di Marc’Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, e Jennifer Wortman Vaughan, 19757–70. https://proceedings.neurips.cc/paper/2021/hash/a4380923dd651c195b1631af7c829187-Abstract.html.\n\n\n\n\n\n\nFigura 14.14: Applicazioni di Machine Unlearning. Fonte: BBVA OpenMind\n\n\n\n\n\nCaso di Studio: L’Esperimento di Harry Potter\nAlcuni ricercatori hanno dimostrato un esempio concreto di approcci di disapprendimento automatico applicati ai modelli di machine learning SOTA attraverso l’addestramento di un LLM, LLaMA2-7b, per disimparare qualsiasi riferimento a Harry Potter (Eldan e Russinovich 2023). Sebbene questo modello abbia richiesto 184K ore di GPU per il pre-addestramento, è bastata solo 1 ora di GPU di messa a punto per cancellare la capacità del modello di generare o richiamare contenuti correlati a Harry Potter senza compromettere in modo evidente l’accuratezza della generazione di contenuti non correlati a Harry Potter. Figura 14.15 mostra come l’output del modello cambia prima (colonna Llama-7b-chat-hf) e dopo (colonna Llama-b messa a punto) che si è verificato il disapprendimento.\n\n\n\n\n\n\nFigura 14.15: Llama disapprendimento di Harry Potter. Fonte: Eldan e Russinovich (2023).\n\n\nEldan, Ronen, e Mark Russinovich. 2023. «Who’s Harry Potter? Approximate Unlearning in LLMs». ArXiv preprint abs/2310.02238 (ottobre). http://arxiv.org/abs/2310.02238v2.\n\n\n\n\nAltri Utilizzi\n\nRimozione di dati avversari\nÈ stato precedentemente dimostrato che i modelli di deep learning sono vulnerabili ad attacchi avversari, in cui l’aggressore genera dati avversari simili ai dati di training originali, in cui un essere umano non riesce a distinguere tra i dati reali e quelli fabbricati. I dati avversari fanno sì che il modello emetta previsioni errate, il che potrebbe avere conseguenze negative in varie applicazioni, tra cui le previsioni di diagnosi sanitaria. Il disapprendimento automatico è stato utilizzato per disimparare l’influenza dei dati avversari, impedendo così che queste previsioni errate si verifichino e causino danni.\n\n\n\n\n14.8.4 Crittografia Omomorfica\n\nIdea Centrale\nLa crittografia omomorfica è una forma di crittografia che consente di eseguire calcoli su testo cifrato, generando un risultato crittografato che, una volta decrittografato, corrisponde al risultato delle operazioni eseguite sul testo in chiaro. Ad esempio, moltiplicando due numeri crittografati con crittografia omomorfica si ottiene un prodotto crittografato che decrittografa il prodotto effettivo dei due numeri. Ciò significa che i dati possono essere elaborati in forma crittografata e solo l’output risultante deve essere decrittografato, migliorando significativamente la sicurezza dei dati, in particolare per le informazioni sensibili.\nLa crittografia omomorfica consente calcoli esternalizzati su dati crittografati senza esporre i dati stessi esternamente per eseguire le operazioni. Tuttavia, solo determinati calcoli come addizione e moltiplicazione sono supportati negli schemi parzialmente omomorfici. La “Fully Homomorphic Encryption (FHE)” [crittografia completamente omomorfica], in grado di gestire qualsiasi calcolo, è ancora più complessa. Il numero di possibili operazioni è limitato prima che l’accumulo di rumore corrompa il testo cifrato.\nPer utilizzare la crittografia omomorfica su diverse entità, le chiavi pubbliche generate con cura devono essere scambiate per le operazioni su dati crittografati separatamente. Questa tecnica di crittografia avanzata consente paradigmi di calcolo sicuri precedentemente impossibili, ma richiede competenze specifiche per essere implementata correttamente nei sistemi del mondo reale.\n\n\nVantaggi\nLa crittografia omomorfica consente l’addestramento del modello di apprendimento automatico e l’inferenza sui dati crittografati, assicurando che gli input sensibili e i valori intermedi rimangano riservati. Ciò è fondamentale in ambito sanitario, finanziario, genetico e altri domini, che si affidano sempre di più al ML per analizzare set di dati sensibili e regolamentati contenenti miliardi di dati personali.\nLa crittografia omomorfica ostacola attacchi come l’estrazione del modello e l’inferenza dell’appartenenza che potrebbero esporre dati privati utilizzati nei flussi di lavoro ML. Fornisce un’alternativa ai TEE che utilizzano enclave hardware per l’elaborazione riservata. Tuttavia, gli schemi attuali hanno elevati overhead computazionali e limitazioni algoritmiche che limitano le applicazioni del mondo reale.\nLa crittografia omomorfica realizza la visione decennale di elaborazione multi-parti sicura consentendo l’elaborazione su testi cifrati. Concepiti negli anni ’70, i primi sistemi crittografici completamente omomorfici sono emersi nel 2009, consentendo elaborazioni arbitrarie. La ricerca in corso sta rendendo queste tecniche più efficienti e pratiche.\nLa crittografia omomorfica mostra grandi promesse nell’abilitare l’apprendimento automatico che preserva la privacy in base alle normative emergenti sui dati. Tuttavia, dati i vincoli, si dovrebbe valutare attentamente la sua applicabilità rispetto ad altri approcci di elaborazione confidenziale. Esistono ampie risorse per esplorare la crittografia omomorfica e monitorare i progressi nell’attenuare le barriere all’adozione.\n\n\nMeccanica\n\nCrittografia dei Dati: Prima che i dati vengano elaborati o inviati a un modello ML, vengono crittografati utilizzando uno schema di crittografia omomorfica e una chiave pubblica. Ad esempio, la crittografia dei numeri \\(x\\) e \\(y\\) genera i testi cifrati \\(E(x)\\) e \\(E(y)\\).\nCalcolo sul Testo Cifrato: L’algoritmo ML elabora direttamente i dati crittografati. Ad esempio, la moltiplicazione dei testi cifrati \\(E(x)\\) e \\(E(y)\\) genera \\(E(xy)\\). È possibile eseguire anche un training del modello più complesso sui testi cifrati.\nCrittografia del Risultato: Il risultato \\(E(xy)\\) rimane crittografato e può essere decrittografato solo da qualcuno con la chiave privata corrispondente per rivelare il prodotto effettivo \\(xy\\).\n\nSolo le parti autorizzate con la chiave privata possono decrittografare gli output finali, proteggendo lo stato intermedio. Tuttavia, il rumore si accumula con ogni operazione, impedendo ulteriori calcoli senza decrittazione.\nOltre all’assistenza sanitaria, la crittografia omomorfica consente il calcolo riservato per applicazioni come il rilevamento di frodi finanziarie, analisi assicurative, ricerca genetica e altro ancora. Offre un’alternativa a tecniche come il calcolo multipartitico e i TEE. La ricerca in corso migliora l’efficienza e le capacità.\nStrumenti come HElib, SEAL e TensorFlow HE forniscono librerie per esplorare l’implementazione della crittografia omomorfica in pipeline di apprendimento automatico nel mondo reale.\n\n\nCompromessi\nPer molte applicazioni in tempo reale ed embedded, la crittografia completamente omomorfica rimane poco pratica per i seguenti motivi.\nSovraccarico Computazionale: La crittografia omomorfica impone sovraccarichi computazionali molto elevati, spesso con conseguenti rallentamenti di oltre 100 volte per le applicazioni ML del mondo reale. Ciò la rende poco pratica per molti utilizzi sensibili al tempo o con risorse limitate. L’hardware ottimizzato e la parallelizzazione possono alleviare, ma non eliminare, questo problema.\nComplessità di Implementazione Gli algoritmi sofisticati richiedono una profonda competenza in crittografia per essere implementati correttamente. Sfumature come la compatibilità del formato con modelli ML in virgola mobile e la gestione scalabile delle chiavi pongono ostacoli. Questa complessità ostacola l’adozione pratica diffusa.\nLimitazioni Algoritmiche: Gli schemi attuali limitano le funzioni e la profondità dei calcoli supportati, limitando i modelli e i volumi di dati che possono essere elaborati. La ricerca in corso sta spingendo questi limiti, ma permangono delle restrizioni.\nAccelerazione Hardware: La crittografia omomorfica richiede hardware specializzato, come processori sicuri o coprocessori con TEE, che aggiungono costi di progettazione e infrastruttura.\nProgetti Ibridi: Anziché crittografare interi flussi di lavoro, l’applicazione selettiva della crittografia omomorfica a sotto-componenti critici può ottenere protezione riducendo al minimo i costi generali.\n\n\n\n\n\n\nEsercizio 14.2: Crittografia Omomorfica\n\n\n\n\n\nLa potenza del calcolo crittografato viene sbloccata tramite la crittografia omomorfica, un approccio trasformativo in cui i calcoli vengono eseguiti direttamente sui dati crittografati, garantendo la tutela della privacy durante tutto il processo. Questo Colab esplora i principi del calcolo su numeri crittografati senza esporre i dati sottostanti. Si immagini uno scenario in cui un modello di apprendimento automatico viene addestrato su dati a cui non è possibile accedere direttamente: tale è la forza della crittografia omomorfica.\n\n\n\n\n\n\n\n14.8.5 Secure Multiparty Communication\n\nIdea Centrale\nLa Multi-Party Communication (MPC) consente a più parti di calcolare congiuntamente una funzione sui propri input, garantendo al contempo la riservatezza degli input di ciascuna parte. Ad esempio, due organizzazioni possono collaborare all’addestramento di un modello di apprendimento automatico combinando set di dati senza rivelare reciprocamente informazioni sensibili. I protocolli MPC sono essenziali laddove le normative sulla privacy e sulla riservatezza limitano la condivisione diretta dei dati, come nel settore sanitario o finanziario.\nMPC divide il calcolo in parti che ogni partecipante esegue in modo indipendente utilizzando i propri dati privati. Questi risultati vengono quindi combinati per rivelare solo l’output finale, preservando la privacy dei valori intermedi. Vengono utilizzate tecniche crittografiche per garantire che i risultati parziali rimangano privati in modo dimostrabile.\nPrendiamo un semplice esempio di protocollo MPC. Uno dei protocolli MPC più basilari è l’addizione sicura di due numeri. Ogni parte suddivide il suo input in quote casuali che vengono distribuite segretamente. Si scambiano le quote e calcolano localmente la somma delle quote, che ricostruisce la somma finale senza rivelare i singoli input. Ad esempio, se Alice ha input x e Bob ha input y:\n\nAlice genera \\(x_1\\) casuale e imposta \\(x_2 = x - x_1\\)\nBob genera \\(y_1\\) casuale e imposta \\(y_2 = y - y_1\\)\nAlice invia \\(x_1\\) a Bob, Bob invia \\(y_1\\) ad Alice (mantenendo segreti \\(x_2\\) e \\(y_2\\))\nAlice calcola \\(x_2 + y_1 = s_1\\), Bob calcola \\(x_1 + y_2 = s_2\\)\n\\(s_1 + s_2 = x + y\\) è la somma finale, senza rivelare \\(x\\) o \\(y\\).\n\nGli input individuali di Alice e Bob (\\(x\\) e \\(y\\)) rimangono privati e ciascuna parte rivela solo un numero associato ai propri input originali. Grazie ai risultati casuali, non viene rivelata alcuna informazione sui numeri originali.\nConfronto Sicuro: Un’altra operazione di base è un confronto sicuro di due numeri, per determinare quale è maggiore dell’altro. Questo può essere fatto usando tecniche come i “Yao’s Garbled Circuits” [circuiti distorti di Yao] (https://it.wikipedia.org/wiki/Andrew_Chi-Chih_Yao), dove il circuito di confronto è crittografato per consentire una valutazione congiunta degli input senza trapelare.\nMoltiplicazione Sicura di Matrici: Le operazioni di matrice come la moltiplicazione sono essenziali per l’apprendimento automatico. Le tecniche MPC (Multiparty Communication) come la condivisione segreta additiva possono essere usate per dividere le matrici in quote casuali, calcolare i prodotti sulle quote e quindi ricostruire il risultato.\nAddestramento Sicuro del Modello: Gli algoritmi di addestramento dell’apprendimento automatico distribuito come la media federata possono essere resi sicuri usando MPC. Gli aggiornamenti del modello calcolati su dati partizionati in ogni nodo vengono condivisi segretamente tra i nodi e aggregati per addestrare il modello globale senza esporre aggiornamenti individuali.\nL’idea fondamentale alla base dei protocolli MPC è quella di dividere il calcolo in passaggi che possono essere eseguiti congiuntamente senza rivelare dati sensibili intermedi. Ciò si ottiene combinando tecniche crittografiche come la condivisione segreta, la crittografia omomorfica, il trasferimento inconsapevole e i circuiti garbled [distorti]. I protocolli MPC consentono il calcolo collaborativo di dati sensibili fornendo al contempo garanzie di privacy dimostrabili. Questa capacità di preservazione della privacy è essenziale per molte applicazioni di apprendimento automatico odierne che coinvolgono più parti che non possono condividere direttamente i propri dati grezzi.\nGli approcci principali utilizzati in MPC includono:\n\nCrittografia omomorfica: La crittografia speciale consente di eseguire calcoli su dati crittografati senza decrittografarli.\nCondivisione segreta: I dati privati vengono suddivisi in quote casuali distribuite a ciascuna parte. I calcoli vengono eseguiti localmente sulle quote e infine ricostruiti.\nTrasferimento inconsapevole: Un protocollo in cui un ricevitore ottiene un sottoinsieme di dati da un mittente, ma il mittente non sa quali dati specifici sono stati trasferiti.\nCircuiti Garbled: La funzione da calcolare è rappresentata come un circuito booleano crittografato (“distorto”) per consentire una valutazione congiunta senza rivelare gli input.\n\n\n\nCompromessi\nSebbene i protocolli MPC forniscano solide garanzie di privacy, hanno un costo computazionale elevato rispetto ai calcoli semplici. Ogni operazione sicura, come addizione, moltiplicazione, confronto, ecc., richiede più ordini di elaborazione rispetto all’operazione equivalente non crittografata. Questo overhead deriva dalle tecniche crittografiche sottostanti:\n\nNella crittografia parzialmente omomorfica, ogni calcolo su testi cifrati richiede costose operazioni a chiave pubblica. La crittografia completamente omomorfica ha overhead ancora più elevati.\nLa condivisione segreta divide i dati in più porzioni, quindi anche le operazioni di base richiedono la manipolazione di molte porzioni.\nIl trasferimento inconsapevole e i circuiti distorti aggiungono mascheramento e crittografia per nascondere i pattern di accesso ai dati e i flussi di esecuzione.\nI sistemi MPC richiedono un’ampia comunicazione e interazione tra le parti per elaborare congiuntamente quote/testi cifrati.\n\nDi conseguenza, i protocolli MPC possono rallentare i calcoli di 3-4 ordini di grandezza rispetto alle implementazioni semplici. Ciò diventa proibitivo per grandi set di dati e modelli. Pertanto, l’addestramento di modelli di apprendimento automatico su dati crittografati tramite MPC rimane oggi irrealizzabile per dimensioni di set di dati realistiche a causa del sovraccarico. Sono necessarie ottimizzazioni e approssimazioni intelligenti per rendere pratico l’MPC.\nLa ricerca in corso sull’MPC colma questo divario di efficienza attraverso progressi crittografici, nuovi algoritmi, hardware affidabile come le enclave SGX e sfruttando acceleratori come GPU/TPU. Tuttavia, nel prossimo futuro, sarà necessario un certo grado di approssimazione e compromesso sulle prestazioni per scalare MPC in modo da soddisfare le esigenze dei sistemi di apprendimento automatico del mondo reale.\n\n\n\n14.8.6 Generazione di Dati Sintetici\n\nIdea Centrale\nLa generazione di dati sintetici è emersa come un importante approccio di apprendimento automatico per la tutela della privacy che consente di sviluppare e testare modelli senza esporre dati utente reali. L’idea chiave è quella di addestrare modelli generativi su set di dati del mondo reale e quindi campionare da questi modelli per sintetizzare dati artificiali che statisticamente corrispondono alla distribuzione dei dati originali ma non contengono informazioni effettive dell’utente. Ad esempio, tecniche come GAN, VAE e aumento dei dati possono essere utilizzate per produrre dati sintetici che imitano set di dati reali preservando al contempo la privacy. Le simulazioni sono anche comunemente impiegate in scenari in cui i dati sintetici devono rappresentare sistemi complessi, come nella ricerca scientifica o nella pianificazione urbana.\nLa sfida principale della sintesi dei dati è garantire che gli avversari non possano re-identificare il set di dati originale. Un approccio semplice per ottenere dati sintetici è aggiungere rumore al set di dati originale, che rischia comunque di far trapelare la privacy. Quando il rumore viene aggiunto ai dati nel contesto della privacy differenziale, vengono utilizzati meccanismi sofisticati basati sulla sensibilità dei dati per calibrare la quantità e la distribuzione del rumore. Attraverso questi limiti matematicamente rigorosi, la privacy differenziale garantisce generalmente la privacy a un certo livello, che è l’obiettivo primario di questa tecnica. Oltre a preservare la privacy, i dati sintetici contrastano molteplici problemi di disponibilità dei dati, come set di dati sbilanciati, set di dati scarsi e rilevamento di anomalie.\nI ricercatori possono condividere liberamente questi dati sintetici e collaborare alla modellazione senza rivelare informazioni mediche private. I dati sintetici ben costruiti proteggono la privacy e, al tempo stesso, sono utili per lo sviluppo di modelli accurati. Le tecniche chiave per impedire la ricostruzione dei dati originali includono l’aggiunta di rumore di privacy differenziale durante l’addestramento, l’applicazione di vincoli di plausibilità e l’utilizzo di più modelli generativi diversi.\n\n\nVantaggi\nSebbene i dati sintetici possano essere necessari a causa di rischi per la privacy o la conformità, sono ampiamente utilizzati nei modelli di apprendimento automatico quando i dati disponibili sono di scarsa qualità, scarsi o inaccessibili. I dati sintetici offrono uno sviluppo più efficiente ed efficace semplificando i processi di addestramento, test e distribuzione dei modelli robusti. Consentono ai ricercatori di condividere i modelli più ampiamente senza violare le leggi e le normative sulla privacy. La collaborazione tra gli utenti dello stesso set di dati sarà facilitata, il che aiuterà ad ampliare le capacità e i progressi nella ricerca ML.\nEsistono diverse motivazioni per l’utilizzo di dati sintetici nell’apprendimento automatico:\n\nPrivacy e Conformità: I dati sintetici evitano di esporre informazioni personali, consentendo una condivisione e una collaborazione più aperte. Ciò è importante quando si lavora con set di dati sensibili come cartelle cliniche o informazioni finanziarie.\nScarsità di dati: Quando non sono disponibili dati reali sufficienti, i dati sintetici possono aumentare i set di dati di addestramento. Ciò migliora l’accuratezza del modello quando i dati limitati rappresentano un collo di bottiglia.\nTest del modello: I dati sintetici forniscono sandbox protetti dalla privacy per testare le prestazioni del modello, risolvere i problemi e monitorare i bias.\nEtichettatura dei dati: I dati di training etichettati di alta qualità sono spesso scarsi e costosi. I dati sintetici possono aiutare a generare automaticamente esempi etichettati.\n\n\n\nCompromessi\nSebbene i dati sintetici cerchino di rimuovere qualsiasi prova del set di dati originale, la perdita della privacy rappresenta comunque un rischio, poiché i dati sintetici imitano i dati originali. Le informazioni statistiche e la distribuzione sono simili, se non uguali, tra i dati originali e sintetici. Ricampionando dalla distribuzione, gli avversari potrebbero comunque essere in grado di recuperare i campioni di addestramento originali. A causa dei loro processi di apprendimento e complessità intrinseci, le reti neurali potrebbero rivelare accidentalmente informazioni sensibili sui dati di addestramento originali.\nUna sfida fondamentale con i dati sintetici è il potenziale divario tra le distribuzioni dei dati sintetici e quelli del mondo reale. Nonostante i progressi nelle tecniche di modellazione generativa, i dati sintetici potrebbero catturare solo parzialmente la complessità, la diversità e i pattern sfumati dei dati reali. Ciò può limitare l’utilità dei dati sintetici per l’addestramento robusto di modelli di apprendimento automatico. Valutare rigorosamente la qualità dei dati sintetici tramite metodi avversari e confrontare le prestazioni del modello con i benchmark dei dati reali aiuta a valutare e migliorare la fedeltà. Tuttavia, intrinsecamente, i dati sintetici rimangono un’approssimazione.\nUn’altra preoccupazione critica sono i rischi per la privacy dei dati sintetici. I modelli generativi possono far trapelare informazioni identificabili sugli individui nei dati di training, il che potrebbe consentire la ricostruzione di informazioni private. Gli attacchi avversari emergenti dimostrano le sfide nel prevenire la perdita di identità dalle pipeline di generazione di dati sintetici. Tecniche come la privacy differenziale possono aiutare a salvaguardare la privacy, ma comportano compromessi nell’utilità dei dati. Esiste una tensione intrinseca tra la produzione di dati sintetici validi e la protezione completa dei dati di training sensibili, che deve essere bilanciata.\nUlteriori insidie dei dati sintetici includono distorsioni amplificate, etichettature errate, sovraccarico computazionale per l’addestramento di modelli generativi, costi di archiviazione e mancata contabilizzazione di nuovi dati fuori distribuzione. Sebbene questi siano secondari rispetto al divario sintetico-reale e ai rischi per la privacy, rimangono considerazioni importanti quando si valuta l’idoneità dei dati sintetici per particolari attività di apprendimento automatico. Come con qualsiasi tecnica, i vantaggi dei dati sintetici comportano compromessi e limitazioni intrinseche che richiedono strategie di mitigazione ponderate.\n\n\n\n14.8.7 Riepilogo\nSebbene tutte le tecniche di cui abbiamo discusso finora mirino a consentire un apprendimento automatico che salvaguardi la privacy, esse implicano meccanismi e compromessi distinti. Fattori come vincoli computazionali, ipotesi di fiducia richieste, modelli di minaccia e caratteristiche dei dati aiutano a guidare il processo di selezione per un caso d’uso particolare. Tuttavia, trovare il giusto equilibrio tra privacy, accuratezza ed efficienza richiede sperimentazione e valutazione empirica per molte applicazioni. Tabella 14.2 è una tabella di confronto delle principali tecniche di apprendimento automatico che salvaguardano la privacy e dei loro pro e contro:\n\n\n\nTabella 14.2: Confronto di tecniche per l’apprendimento automatico che tutela la privacy.\n\n\n\n\n\n\n\n\n\n\nTecnica\nPro\nContro\n\n\n\n\nPrivacy Differenziale\n\nForti garanzie formali di privacy\nRobusto per attacchi dati ausiliari\nVersatile per molti tipi di dati e analisi\n\n\nPerdita di accuratezza dovuta all’aggiunta di rumore\nOverhead computazionale per analisi di sensibilità e generazione di rumore\n\n\n\nAddestramento Federato\n\nConsente l’apprendimento collaborativo senza condividere dati grezzi\nI dati rimangono decentralizzati migliorando la sicurezza\nNessuna necessità di elaborazione crittografata\n\n\nOverhead di comunicazione aumentato\nConvergenza del modello potenzialmente più lenta\nCapacità di dispositivi client non uniformi\n\n\n\nMachine Unlearning\n\nConsente la rimozione selettiva dell’influenza dei dati dai modelli\nUtile per la conformità alle normative sulla privacy\nImpedisce la conservazione involontaria di dati avversari o obsoleti\n\n\nPuò degradare le prestazioni del modello su attività correlate\nComplessità di implementazione in modelli su larga scala\nRischio di unlearning incompleto o inefficace\n\n\n\nCrittografia Omomorfica\n\nConsente il calcolo su dati crittografati\nPreviene l’esposizione allo stato intermedio\n\n\nCosti di calcolo estremamente elevati\nImplementazioni crittografiche complesse\nRestrizioni sui tipi di funzione\n\n\n\nCalcolo Multi-Parte Sicuro\n\nConsente il calcolo congiunto su dati sensibili\nFornisce garanzie di privacy crittografica\nProtocolli flessibili per varie funzioni\n\n\nCosti di calcolo molto elevati\nComplessità di implementazione\nVincoli algoritmici sulla profondità della funzione\n\n\n\nGenerazione di Dati Sintetici\n\nConsente la condivisione dei dati senza perdite\nAttenua i problemi di scarsità di dati\n\n\nDivario sintetico-reale nelle distribuzioni\nPotenziale per la ricostruzione di dati privati\nBias e problemi di etichettatura",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Sicurezza e Privacy</span>"
    ]
  },
  {
    "objectID": "contents/core/privacy_security/privacy_security.it.html#conclusione",
    "href": "contents/core/privacy_security/privacy_security.it.html#conclusione",
    "title": "14  Sicurezza e Privacy",
    "section": "14.9 Conclusione",
    "text": "14.9 Conclusione\nLa sicurezza hardware del machine learning è fondamentale poiché i sistemi ML embedded vengono sempre più implementati in domini critici per la sicurezza come dispositivi medici, controlli industriali e veicoli autonomi. Abbiamo esplorato varie minacce che spaziano da bug hardware, attacchi fisici, canali laterali, rischi della supply chain, ecc. Difese come TEE, Secure Boot, PUF e moduli di sicurezza hardware forniscono una protezione multi-livello su misura per dispositivi embedded con risorse limitate.\nTuttavia, una vigilanza continua è essenziale per tracciare i vettori di attacco emergenti e affrontare potenziali vulnerabilità tramite pratiche di ingegneria sicure durante l’intero ciclo di vita dell’hardware. Man mano che ML e ML embedded si diffondono, il mantenimento di rigorose basi di sicurezza che corrispondano al ritmo accelerato di innovazione del settore rimane imperativo.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Sicurezza e Privacy</span>"
    ]
  },
  {
    "objectID": "contents/core/privacy_security/privacy_security.it.html#sec-security-and-privacy-resource",
    "href": "contents/core/privacy_security/privacy_security.it.html#sec-security-and-privacy-resource",
    "title": "14  Sicurezza e Privacy",
    "section": "14.10 Risorse",
    "text": "14.10 Risorse\nEcco un elenco curato di risorse per supportare studenti e insegnanti nei loro percorsi di apprendimento e insegnamento. Lavoriamo continuamente per espandere questa raccolta e presto aggiungeremo nuovi esercizi.\n\n\n\n\n\n\nSlide\n\n\n\n\n\nQueste slide sono uno strumento prezioso per gli insegnanti per tenere lezioni e per gli studenti per rivedere il materiale secondo il proprio ritmo. Incoraggiamo studenti e docenti a sfruttare queste slide per migliorare la loro comprensione e facilitare un trasferimento efficace delle conoscenze.\n\nSecurity.\nPrivacy.\nMonitoring after Deployment.\n\n\n\n\n\n\n\n\n\n\nVideo\n\n\n\n\n\n\nVideo 14.1\nVideo 14.2\nVideo 14.3\n\n\n\n\n\n\n\n\n\n\nEsercizi\n\n\n\n\n\nPer rafforzare i concetti trattati in questo capitolo, abbiamo curato una serie di esercizi che sfidano gli studenti ad applicare le proprie conoscenze e ad approfondire la propria comprensione.\n\nEsercizio 14.1\nEsercizio 14.2",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Sicurezza e Privacy</span>"
    ]
  },
  {
    "objectID": "contents/core/responsible_ai/responsible_ai.it.html",
    "href": "contents/core/responsible_ai/responsible_ai.it.html",
    "title": "15  IA Responsabile",
    "section": "",
    "text": "15.1 Panoramica\nI modelli di apprendimento automatico sono sempre più utilizzati per automatizzare le decisioni in ambiti sociali ad alto rischio come sanità, giustizia penale e occupazione. Tuttavia, senza un’attenzione deliberata, questi algoritmi possono perpetuare pregiudizi, violare la privacy o causare altri danni. Ad esempio, un modello di approvazione di prestiti addestrato esclusivamente su dati provenienti da quartieri ad alto reddito potrebbe svantaggiare i richiedenti provenienti da aree a basso reddito. Ciò motiva la necessità di un apprendimento automatico responsabile, ovvero la creazione di modelli equi, responsabili, trasparenti ed etici.\nDiversi principi fondamentali sono alla base di un apprendimento automatico responsabile. L’equità garantisce che i modelli non discriminino in base a genere, razza, età e altri attributi. La spiegabilità consente agli esseri umani di interpretare i comportamenti del modello e migliorare la trasparenza. Le tecniche di robustezza e sicurezza prevengono vulnerabilità come gli esempi avversari. Test e convalide rigorosi aiutano a ridurre le debolezze indesiderate del modello o gli effetti collaterali.\nL’implementazione di un apprendimento automatico responsabile presenta sfide sia tecniche che etiche. Gli sviluppatori devono confrontarsi con la definizione matematica dell’equità, bilanciando obiettivi concorrenti come accuratezza e interpretabilità e assicurando dati di training di qualità. Le organizzazioni devono anche allineare incentivi, politiche e cultura per sostenere l’IA etica.\nQuesto capitolo fornirà gli strumenti per valutare criticamente i sistemi di IA e contribuire allo sviluppo di applicazioni di apprendimento automatico utili ed etiche, coprendo le basi, i metodi e le implicazioni nel mondo reale dell’ML responsabile. I principi dell’ML responsabile discussi sono conoscenze cruciali poiché gli algoritmi mediano più aspetti della società umana.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>IA Responsabile</span>"
    ]
  },
  {
    "objectID": "contents/core/responsible_ai/responsible_ai.it.html#terminologia",
    "href": "contents/core/responsible_ai/responsible_ai.it.html#terminologia",
    "title": "15  IA Responsabile",
    "section": "15.2 Terminologia",
    "text": "15.2 Terminologia\nL’IA responsabile riguarda lo sviluppo di un’IA che abbia un impatto positivo sulla società in base all’etica e ai valori umani. Non esiste una definizione universalmente accettata di “IA responsabile”, ma ecco un riassunto di come viene comunemente descritta. L’IA responsabile si riferisce alla progettazione, allo sviluppo e all’implementazione di sistemi di intelligenza artificiale in modo etico e socialmente utile. L’obiettivo principale è creare un’IA affidabile, imparziale, equa, trasparente, responsabile e sicura. Sebbene non esista una definizione canonica, si ritiene generalmente che l’IA responsabile comprenda principi quali:\n\nEquità: Evitare pregiudizi, discriminazioni e potenziali danni a determinati gruppi o popolazioni\nSpiegabilità: Consentire agli esseri umani di comprendere e interpretare il modo in cui i modelli di IA prendono decisioni\nTrasparenza: Comunicare apertamente come funzionano, sono costruiti e valutati i sistemi di IA\nResponsabilità: Avere processi per determinare responsabilità e obblighi per guasti o impatti negativi dell’IA\nRobustezza: Garantire che i sistemi di IA siano sicuri, affidabili e si comportino come previsto\nPrivacy: Proteggere i dati sensibili degli utenti e rispettare le leggi e l’etica sulla privacy\n\nMettere in pratica questi principi implica tecniche tecniche, politiche aziendali, quadri di governance e filosofia morale. Sono inoltre in corso dibattiti sulla definizione di concetti ambigui come l’equità e sulla determinazione di come bilanciare obiettivi in competizione.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>IA Responsabile</span>"
    ]
  },
  {
    "objectID": "contents/core/responsible_ai/responsible_ai.it.html#principi-e-concetti",
    "href": "contents/core/responsible_ai/responsible_ai.it.html#principi-e-concetti",
    "title": "15  IA Responsabile",
    "section": "15.3 Principi e Concetti",
    "text": "15.3 Principi e Concetti\n\n15.3.1 Trasparenza e Spiegabilità\nI modelli di apprendimento automatico sono spesso criticati come misteriose “scatole nere”, sistemi opachi in cui non è chiaro come siano arrivati a particolari previsioni o decisioni. Ad esempio, un sistema di intelligenza artificiale chiamato COMPAS utilizzato per valutare il rischio di recidiva criminale negli Stati Uniti si è rivelato razzialmente discriminatorio nei confronti degli imputati neri. Tuttavia, l’opacità dell’algoritmo ha reso difficile comprendere e risolvere il problema. Questa mancanza di trasparenza può nascondere pregiudizi, errori e carenze.\nSpiegare i comportamenti del modello aiuta a generare fiducia da parte del pubblico e degli esperti del settore e consente di identificare i problemi da affrontare. Le tecniche di interpretabilità svolgono un ruolo chiave in questo processo. Ad esempio, LIME (Local Interpretable Model-Agnostic Explanations) evidenzia come le singole funzionalità di input contribuiscano a una previsione specifica, mentre i valori Shapley quantificano il contributo di ciascuna funzionalità all’output di un modello in base alla teoria dei giochi cooperativi. Le “mappe di salienza”, comunemente utilizzate nei modelli basati su immagini, evidenziano visivamente le aree di un’immagine che hanno maggiormente influenzato la decisione del modello. Questi strumenti consentono agli utenti di comprendere la logica del modello.\nOltre ai vantaggi pratici, la trasparenza è sempre più richiesta dalla legge. Regolamenti come l’“European Union’s General Data Protection Regulation (GDPR)” dell’Unione Europea impongono alle organizzazioni di fornire spiegazioni per determinate decisioni automatizzate, soprattutto quando hanno un impatto significativo sugli individui. Ciò rende la spiegabilità non solo una buona pratica, ma una necessità legale in alcuni contesti. Insieme, trasparenza e spiegabilità costituiscono pilastri fondamentali per la creazione di sistemi di IA responsabili e affidabili.\n\n\n15.3.2 Equità, Bias [pregiudizi] e Discriminazione\nI modelli di ML addestrati su dati storicamente distorti spesso perpetuano e amplificano tali pregiudizi. È stato dimostrato che gli algoritmi sanitari svantaggiano i pazienti neri sottostimandone le esigenze (Obermeyer et al. 2019). Il riconoscimento facciale deve essere più accurato per le donne e le persone di colore. Tale discriminazione algoritmica può avere un impatto negativo profondo sulla vita delle persone.\n\nObermeyer, Ziad, Brian Powers, Christine Vogeli, e Sendhil Mullainathan. 2019. «Dissecting racial bias in an algorithm used to manage the health of populations». Science 366 (6464): 447–53. https://doi.org/10.1126/science.aax2342.\nEsistono anche diverse prospettive filosofiche sull’equità, ad esempio, è più giusto trattare tutti gli individui allo stesso modo o cercare di ottenere risultati uguali per i gruppi? Garantire l’equità richiede di rilevare e mitigare in modo proattivo i pregiudizi nei dati e nei modelli. Tuttavia, raggiungere l’equità perfetta è tremendamente difficile a causa di definizioni matematiche e prospettive etiche contrastanti. Tuttavia, promuovere l’equità algoritmica e la non discriminazione è una responsabilità fondamentale nello sviluppo dell’intelligenza artificiale.\n\n\n15.3.3 Privacy e Governance dei Dati\nMantenere la privacy degli individui è un obbligo etico e un requisito legale per le organizzazioni che implementano sistemi di intelligenza artificiale. Regolamentazioni come il GDPR dell’UE impongono protezioni e diritti sulla privacy dei dati, come la possibilità di accedere ed eliminare i propri dati.\nTuttavia, massimizzare l’utilità e l’accuratezza dei dati per i modelli di addestramento può entrare in conflitto con la tutela della privacy: la modellazione della progressione della malattia potrebbe trarre vantaggio dall’accesso ai genomi completi dei pazienti, ma la condivisione di tali dati viola ampiamente la privacy.\nUna governance dei dati responsabile implica l’anonimizzazione attenta dei dati, il controllo dell’accesso tramite crittografia, l’ottenimento del consenso informato degli interessati e la raccolta dei dati minimi necessari. Rispettare la privacy è difficile ma fondamentale man mano che le capacità e l’adozione dell’intelligenza artificiale si espandono.\n\n\n15.3.4 Sicurezza e Robustezza\nMettere in funzione i sistemi di intelligenza artificiale nel mondo reale richiede di garantire che siano sicuri, affidabili e robusti, soprattutto per gli scenari di interazione umana. Le auto a guida autonoma di Uber e Tesla sono state coinvolte in incidenti mortali a causa di comportamenti non sicuri.\nGli attacchi avversari che alterano in modo sottile i dati di input possono anche ingannare i modelli ML e causare guasti pericolosi se i sistemi non sono resistenti. I deepfake rappresentano un’altra area di minaccia emergente.\nVideo 15.1 è un video deepfake di Barack Obama che è diventato virale qualche anno fa.\n\n\n\n\n\n\nVideo 15.1: Fake Obama\n\n\n\n\n\n\nLa promozione della sicurezza richiede test approfonditi, analisi dei rischi, supervisione umana e progettazione di sistemi che combinano più modelli deboli per evitare singoli punti di errore. Rigorosi meccanismi di sicurezza sono essenziali per l’implementazione responsabile di un’IA efficiente.\n\n\n15.3.5 Responsabilità e Governance\nQuando i sistemi di IA alla fine falliscono o producono risultati dannosi, devono esistere meccanismi per affrontare i problemi risultanti, risarcire le parti interessate e assegnare la responsabilità. Sia le politiche di responsabilità aziendale che le normative governative sono indispensabili per una governance responsabile dell’IA. Ad esempio, l’Artificial Intelligence Video Interview Act dell’Illinois richiede alle aziende di divulgare e ottenere il consenso per l’analisi video dell’IA, promuovendo la responsabilità.\nSenza una chiara responsabilità, anche i danni causati involontariamente potrebbero rimanere irrisolti, alimentando ulteriormente l’indignazione e la sfiducia pubblica. I comitati di vigilanza, le valutazioni di impatto, i processi di risoluzione dei reclami e gli audit indipendenti promuovono lo sviluppo e l’implementazione responsabili.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>IA Responsabile</span>"
    ]
  },
  {
    "objectID": "contents/core/responsible_ai/responsible_ai.it.html#cloud-edge-e-tiny-ml",
    "href": "contents/core/responsible_ai/responsible_ai.it.html#cloud-edge-e-tiny-ml",
    "title": "15  IA Responsabile",
    "section": "15.4 Cloud, Edge e Tiny ML",
    "text": "15.4 Cloud, Edge e Tiny ML\nSebbene questi principi siano ampiamente applicabili a tutti i sistemi di intelligenza artificiale, alcune considerazioni di IA responsabile sono uniche o pronunciate quando si ha a che fare con l’apprendimento automatico su dispositivi embedded rispetto alla modellazione tradizionale basata su server. Pertanto, presentiamo una tassonomia di alto livello che confronta le considerazioni di intelligenza artificiale responsabile nei sistemi cloud, edge e TinyML.\n\n15.4.1 Spiegabilità\nPer l’apprendimento automatico basato su cloud, le tecniche di spiegabilità possono sfruttare risorse di elaborazione significative, consentendo metodi complessi come valori SHAP o approcci basati sul campionamento per interpretare i comportamenti del modello. Ad esempio, il toolkit InterpretML di Microsoft fornisce tecniche di spiegabilità su misura per gli ambienti cloud.\nTuttavia, l’edge ML opera su dispositivi con risorse limitate, richiedendo metodi di spiegabilità più leggeri che possono essere eseguiti localmente senza latenza eccessiva. Tecniche come LIME (Ribeiro, Singh, e Guestrin 2016) approssimano le spiegazioni del modello utilizzando modelli lineari o alberi decisionali per evitare calcoli costosi, il che le rende ideali per dispositivi con risorse limitate. Tuttavia, LIME richiede l’addestramento di centinaia o persino migliaia di modelli per generare buone spiegazioni, il che è spesso irrealizzabile dati i vincoli dell’edge computing. Al contrario, i metodi basati sulla salienza sono spesso molto più rapidi nella pratica, richiedendo solo un singolo passaggio in avanti attraverso la rete per stimare l’importanza delle funzionalità. Questa maggiore efficienza rende tali metodi più adatti ai dispositivi edge con risorse di elaborazione limitate, in cui le spiegazioni a bassa latenza sono fondamentali.\nDate le ridotte capacità hardware, i sistemi embedded pongono le sfide più significative per la spiegabilità. Modelli più compatti e dati limitati semplificano la trasparenza intrinseca del modello. Spiegare le decisioni potrebbe non essere fattibile su microcontrollori di grandi dimensioni e con potenza ottimizzata. Il programma Transparent Computing della DARPA cerca di sviluppare una spiegabilità con costi di gestione estremamente bassi, in particolare per i dispositivi TinyML come sensori e dispositivi indossabili.\n\n\n15.4.2 Equità\nPer il machine learning nel cloud, vasti set di dati e potenza di calcolo consentono di rilevare pregiudizi su grandi popolazioni eterogenee e di mitigarli tramite tecniche come la riponderazione dei campioni di dati. Tuttavia, i pregiudizi possono emergere dagli ampi dati comportamentali utilizzati per addestrare i modelli cloud. Il framework Fairness Flow di Amazon aiuta a valutare l’equità del ML cloud.\nEdge ML si basa su dati limitati sul dispositivo, rendendo più difficile l’analisi dei pregiudizi tra gruppi diversi. Tuttavia, i dispositivi edge interagiscono strettamente con gli individui, offrendo un’opportunità di adattamento locale per l’equità. Federated Learning di Google distribuisce l’addestramento del modello tra i dispositivi per incorporare le differenze individuali.\nTinyML pone sfide uniche per l’equità con hardware specializzato altamente disperso e dati di addestramento minimi. I test sui pregiudizi sono difficili su dispositivi diversi. La raccolta di dati rappresentativi da molti dispositivi per mitigare i pregiudizi presenta ostacoli di scala e privacy. Gli sforzi di Assured Neuro Symbolic Learning and Reasoning (ANSR) di DARPA sono orientati allo sviluppo di tecniche di equità dati i vincoli hardware estremi.\n\n\n15.4.3 Privacy\nPer il cloud ML, grandi quantità di dati utente sono concentrate nel cloud, creando rischi di esposizione tramite violazioni. Le tecniche di privacy differenziali aggiungono rumore ai dati cloud per preservare la privacy. Rigidi controlli di accesso e crittografia proteggono i dati cloud a riposo e in transito.\nEdge ML sposta l’elaborazione dei dati sui dispositivi utente, riducendo la raccolta di dati aggregati ma aumentando la potenziale sensibilità poiché i dati personali risiedono sul dispositivo. Apple utilizza ML on-device e privacy differenziale per addestrare modelli riducendo al minimo la condivisione dei dati. L’anonimizzazione dei dati e le enclave sicure proteggono i dati on-device.\nTinyML distribuisce i dati su molti dispositivi con risorse limitate, rendendo improbabili le violazioni centralizzate e rendendo difficile l’anonimizzazione su larga scala. La minimizzazione dei dati e l’utilizzo di dispositivi edge come intermediari aiutano la privacy di TinyML.\nQuindi, mentre il cloud ML deve proteggere dati centralizzati espansivi, l’edge ML protegge i dati sensibili on-device e TinyML mira a una condivisione minima dei dati distribuiti a causa dei vincoli. Mentre la privacy è fondamentale in tutto, le tecniche devono adattarsi all’ambiente. La comprensione delle sfumature consente di selezionare approcci appropriati per la tutela della privacy.\n\n\n15.4.4 Sicurezza\nI principali rischi per la sicurezza del cloud ML includono hacking dei modelli, avvelenamento dei dati e malware che interrompono i servizi cloud. Le tecniche di robustezza come l’addestramento avversario, il rilevamento delle anomalie e i modelli diversificati mirano a rafforzare il cloud ML contro gli attacchi. La ridondanza può aiutare a prevenire singoli punti di errore.\nEdge ML e TinyML interagiscono con il mondo fisico, quindi l’affidabilità e la convalida della sicurezza sono fondamentali. Piattaforme di test rigorose come Foretellix generano sinteticamente scenari edge per convalidare la sicurezza. La sicurezza di TinyML è amplificata da dispositivi autonomi con supervisione limitata. La sicurezza di TinyML spesso si basa sul coordinamento collettivo: sciami di droni mantengono la sicurezza tramite ridondanza. Anche le barriere di controllo fisiche limitano i comportamenti non sicuri dei dispositivi TinyML.\nLe considerazioni sulla sicurezza variano notevolmente tra i domini, riflettendo le loro sfide uniche. Cloud ML si concentra sulla protezione da hacking e violazioni dei dati, Edge ML enfatizza l’affidabilità grazie alle sue interazioni fisiche con l’ambiente e TinyML spesso si basa sul coordinamento distribuito per mantenere la sicurezza nei sistemi autonomi. Riconoscere queste sfumature è essenziale per applicare le tecniche di sicurezza appropriate a ciascun dominio.\n\n\n15.4.5 Responsabilità\nLa responsabilità di Cloud ML si concentra su pratiche aziendali come comitati AI responsabili, carte etiche e processi per affrontare incidenti dannosi. Audit di terze parti e supervisione governativa esterna promuovono la responsabilità di Cloud ML.\nLa responsabilità di Edge ML è più complessa con dispositivi distribuiti e frammentazione della supply chain. Le aziende sono responsabili dei dispositivi, ma i componenti provengono da vari fornitori. Gli standard di settore aiutano a coordinare la responsabilità di Edge ML tra le parti interessate.\nCon TinyML, i meccanismi di responsabilità devono essere tracciati attraverso lunghe e complesse supply chain di circuiti integrati, sensori e altro hardware. Gli schemi di certificazione TinyML aiutano a tracciare la provenienza dei componenti. Le associazioni di categoria dovrebbero idealmente promuovere la responsabilità condivisa per TinyML etico.\n\n\n15.4.6 Governance\nLe organizzazioni istituiscono una governance interna per il cloud ML, come comitati etici, audit e gestione del rischio del modello. Anche la governance esterna svolge un ruolo significativo nel garantire responsabilità ed equità. Abbiamo già introdotto il General Data Protection Regulation (GDPR), che stabilisce requisiti rigorosi per la protezione dei dati e la trasparenza. Tuttavia, non è l’unico quadro che guida pratiche di IA responsabili. L’AI Bill of Rights stabilisce principi per un uso etico dell’IA negli Stati Uniti e il California Consumer Protection Act (CCPA) si concentra sulla salvaguardia della privacy dei dati dei consumatori in California. Gli audit di terze parti rafforzano ulteriormente la governance del ML nel cloud fornendo una supervisione esterna.\nEdge ML è più decentralizzato e richiede un’autogovernance responsabile da parte di sviluppatori e aziende che distribuiscono modelli localmente. Le associazioni di settore coordinano la governance tra i fornitori di edge ML e il software aperto aiuta ad allineare gli incentivi per l’edge ML etico.\nL’estrema decentralizzazione e complessità rendono la governance esterna impraticabile con TinyML. TinyML si basa su protocolli e standard per l’autogovernance integrati nella progettazione del modello e nell’hardware. La crittografia consente l’affidabilità dimostrabile dei dispositivi TinyML.\n\n\n15.4.7 Riepilogo\nTabella 15.1 riassume come i principi di intelligenza artificiale responsabile si manifestino in modo diverso nelle architetture cloud, edge e TinyML e come le considerazioni fondamentali si leghino alle loro capacità e limitazioni uniche. I vincoli e i compromessi di ogni ambiente modellano il modo in cui affrontiamo la trasparenza, la responsabilità, la governance e altri pilastri dell’intelligenza artificiale responsabile.\n\n\n\nTabella 15.1: Confronto dei principi chiave di Cloud ML, Edge ML e TinyML.\n\n\n\n\n\n\n\n\n\n\n\nPrincipio\nCloud ML\nEdge ML\nTinyML\n\n\n\n\nSpiegabilità\nSupporta modelli e metodi complessi come SHAP e approcci di campionamento\nRichiede metodi leggeri e a bassa latenza come le mappe di salienza\nGravemente limitato a causa dell’hardware vincolato\n\n\nEquità\nGrandi set di dati consentono il rilevamento e l’attenuazione dei bias\nI bias localizzati sono più difficili da rilevare ma consentono regolazioni sul dispositivo\nI dati minimi limitano l’analisi e l’attenuazione dei bias\n\n\nPrivacy\nI dati centralizzati sono a rischio di violazioni ma possono sfruttare una crittografia avanzata e la privacy differenziale\nI dati personali sensibili sul dispositivo richiedono protezioni sul dispositivo\nI dati distribuiti riducono i rischi centralizzati ma pongono sfide per l’anonimizzazione\n\n\nSicurezza\nVulnerabile all’hacking e agli attacchi su larga scala\nLe interazioni nel mondo reale rendono fondamentale l’affidabilità\nRichiede meccanismi di sicurezza distribuiti a causa dell’autonomia\n\n\nResponsabilità\nLe policy e gli audit aziendali garantiscono la responsabilità\nLe catene di fornitura frammentate complicano la responsabilità\nTracciabilità richiesta su lunghe e complesse catene hardware\n\n\nGovernance\nSupervisione esterna e normative come GDPR o CCPA sono fattibili\nRichiede autogoverno da parte di sviluppatori e stakeholder\nSi basa su protocolli integrati e garanzie crittografiche",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>IA Responsabile</span>"
    ]
  },
  {
    "objectID": "contents/core/responsible_ai/responsible_ai.it.html#aspetti-tecnici",
    "href": "contents/core/responsible_ai/responsible_ai.it.html#aspetti-tecnici",
    "title": "15  IA Responsabile",
    "section": "15.5 Aspetti Tecnici",
    "text": "15.5 Aspetti Tecnici\n\n15.5.1 Rilevamento e Mitigazione dei Pregiudizi\nI modelli di apprendimento automatico, come qualsiasi sistema complesso, possono talvolta presentare “bias” [distorsioni] nelle loro previsioni. Queste distorsioni possono manifestarsi in prestazioni insufficienti per gruppi specifici o in decisioni che limitano inavvertitamente l’accesso a determinate opportunità o risorse (Buolamwini e Gebru 2018). Comprendere e affrontare queste distorsioni è fondamentale, soprattutto perché i sistemi di apprendimento automatico sono sempre più utilizzati in settori sensibili come prestiti, assistenza sanitaria e giustizia penale.\nPer valutare e affrontare questi problemi, l’equità nell’apprendimento automatico viene in genere valutata analizzando gli “attributi del sottogruppo”, che sono caratteristiche non correlate all’attività di previsione, come posizione geografica, fascia d’età, livello di reddito, razza, genere o religione. Ad esempio, in un modello di previsione di inadempienza del prestito, i sottogruppi potrebbero includere razza, genere o religione. Quando i modelli vengono addestrati con l’unico obiettivo di massimizzare l’accuratezza, potrebbero trascurare le differenze di performance tra questi sottogruppi, con conseguenti potenziali risultati distorti o incoerenti.\nQuesto concetto è illustrato in Figura 15.1, che visualizza le performance di un modello di apprendimento automatico che prevede il rimborso del prestito per due sottogruppi, Sottogruppo A (blu) e Sottogruppo B (rosso). Ogni individuo nel set di dati è rappresentato da un simbolo: i più (+) indicano gli individui che rimborseranno i loro prestiti (veri positivi), mentre i cerchi (O) indicano gli individui che saranno inadempienti sui loro prestiti (veri negativi). L’obiettivo del modello è classificare correttamente questi individui in rimborsatori e inadempienti.\n\n\n\n\n\n\nFigura 15.1: Illustra il compromesso nell’impostazione delle soglie di classificazione per due sottogruppi (A e B) in un modello di rimborso del prestito. I più (+) rappresentano i veri positivi (rimborsatori) e i cerchi (O) rappresentano i veri negativi (inadempienti). Soglie diverse (75% per B e 81,25% per A) massimizzano l’accuratezza del sottogruppo ma rivelano problemi di equità.\n\n\n\nPer valutare le prestazioni, vengono mostrate due linee tratteggiate, che rappresentano le soglie alle quali il modello raggiunge un’accuratezza accettabile per ciascun sottogruppo. Per il Sottogruppo A, la soglia deve essere impostata all’81,25% di accuratezza (la seconda linea tratteggiata) per classificare correttamente tutti i rimborsatori (più). Tuttavia, l’utilizzo di questa stessa soglia per il Sottogruppo B comporterebbe classificazioni errate, poiché alcuni rimborsatori nel Sottogruppo B scenderebbero erroneamente al di sotto di questa soglia e verrebbero classificati come inadempienti. Per il Sottogruppo B, è necessaria una soglia inferiore del 75% di accuratezza (la prima linea tratteggiata) per classificare correttamente i suoi rimborsatori. Tuttavia, l’applicazione di questa soglia inferiore al Sottogruppo A comporterebbe classificazioni errate per quel gruppo. Ciò illustra come il modello funzioni in modo diseguale nei due sottogruppi, con ciascuno che richiede una soglia diversa per massimizzare i propri tassi di veri positivi.\nLa disparità nelle soglie richieste evidenzia la sfida di raggiungere l’equità nelle previsioni del modello. Se le classificazioni positive portano all’approvazione dei prestiti, gli individui nel Sottogruppo B sarebbero svantaggiati a meno che la soglia non venga regolata specificamente per il loro sottogruppo. Tuttavia, la regolazione delle soglie introduce compromessi tra accuratezza e correttezza a livello di gruppo, dimostrando la tensione intrinseca nell’ottimizzazione per questi obiettivi nei sistemi di apprendimento automatico.\nPertanto, la letteratura sull’equità ha proposto tre principali metriche di equità per quantificare quanto sia equo un modello su un set di dati (Hardt, Price, e Srebro 2016). Dato un modello \\(h\\) e un set di dati \\(D\\) costituito da campioni \\((x, y, s)\\), dove \\(x\\) sono le caratteristiche dei dati, \\(y\\) è l’etichetta e \\(s\\) è l’attributo del sottogruppo, e supponiamo che ci siano semplicemente due sottogruppi \\(a\\) e \\(b\\), possiamo definire quanto segue:\n\nParità Demografica chiede quanto è accurato un modello per ogni sottogruppo. In altre parole, \\(P(h(X) = Y \\mid S = a) = P(h(X) = Y \\mid S = b)\\).\nQuote Equalizzate chiede quanto è preciso un modello su campioni positivi e negativi per ogni sottogruppo. \\(P(h(X) = y \\mid S = a, Y = y) = P(h(X) = y \\mid S = b, Y = y)\\).\nUguaglianza di Opportunità è un caso speciale di probabilità equalizzate che chiede solo quanto è preciso un modello su campioni positivi. Ciò è rilevante in casi come l’allocazione delle risorse, in cui ci preoccupiamo di come le etichette positive (vale a dire, allocate in base alle risorse) siano distribuite tra i gruppi. Ad esempio, ci preoccupiamo che una proporzione uguale di prestiti venga concessa sia agli uomini che alle donne. \\(P(h(X) = 1 \\mid S = a, Y = 1) = P(h(X) = 1 \\mid S = b, Y = 1)\\).\n\nNota: Queste definizioni spesso adottano una visione ristretta quando si considerano confronti binari tra due sottogruppi. Un altro filone di ricerca di apprendimento automatico equo incentrato su multi-calibrazione e multi-accuratezza considera le interazioni tra un numero arbitrario di identità, riconoscendo l’intersezionalità intrinseca delle identità individuali nel mondo reale (Hébert-Johnson et al. 2018).\n\nHébert-Johnson, Úrsula, Michael P. Kim, Omer Reingold, e Guy N. Rothblum. 2018. «Multicalibration: Calibration for the (Computationally-Identifiable) Masses». In Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmässan, Stockholm, Sweden, July 10-15, 2018, a cura di Jennifer G. Dy e Andreas Krause, 80:1944–53. Proceedings of Machine Learning Research. PMLR. http://proceedings.mlr.press/v80/hebert-johnson18a.html.\n\nIl Contesto è Importante\nPrima di prendere qualsiasi decisione tecnica per sviluppare un algoritmo ML imparziale, dobbiamo comprendere il contesto che circonda il nostro modello. Ecco alcune delle domande chiave su cui riflettere:\n\nPer chi prenderà decisioni questo modello?\nChi è rappresentato nei dati di training?\nChi è rappresentato e chi manca al tavolo di ingegneri, progettisti e manager?\nChe tipo di impatti duraturi potrebbe avere questo modello? Ad esempio, avrà un impatto sulla sicurezza finanziaria di un individuo su scala generazionale, come la determinazione delle ammissioni al college o l’ammissione di un prestito per una casa?\nQuali pregiudizi storici e sistematici sono presenti in questo contesto e sono presenti nei dati di training da cui il modello generalizzerà?\n\nComprendere il background sociale, etico e storico di un sistema è fondamentale per prevenire danni e dovrebbe informare le decisioni durante tutto il ciclo di sviluppo del modello. Dopo aver compreso il contesto, si possono prendere varie decisioni tecniche per rimuovere i pregiudizi. Innanzitutto, si deve decidere quale metrica di equità è il criterio più appropriato per l’ottimizzazione. Successivamente, ci sono generalmente tre aree principali in cui si può intervenire per eliminare i pregiudizi di un sistema ML.\nInnanzitutto, la preelaborazione è quando si bilancia un set di dati per garantire una rappresentazione equa o addirittura si aumenta il peso su determinati gruppi sottorappresentati per garantire che il modello funzioni bene. In secondo luogo, nell’elaborazione si tenta di modificare il processo di training di un sistema ML per garantire che dia priorità all’equità. Questo può essere semplice come aggiungere un regolarizzatore di equità (Lowy et al. 2021) al training di un insieme di modelli e campionarli in un modo specifico (Agarwal et al. 2018).\n\nLowy, Andrew, Rakesh Pavan, Sina Baharlouei, Meisam Razaviyayn, e Ahmad Beirami. 2021. «Fermi: Fair empirical risk minimization via exponential Rényi mutual information».\n\nAgarwal, Alekh, Alina Beygelzimer, Miroslav Dudı́k, John Langford, e Hanna M. Wallach. 2018. «A Reductions Approach to Fair Classification». In Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmässan, Stockholm, Sweden, July 10-15, 2018, a cura di Jennifer G. Dy e Andreas Krause, 80:60–69. Proceedings of Machine Learning Research. PMLR. http://proceedings.mlr.press/v80/agarwal18a.html.\n\nAlghamdi, Wael, Hsiang Hsu, Haewon Jeong, Hao Wang, Peter Michalak, Shahab Asoodeh, e Flavio Calmon. 2022. «Beyond Adult and COMPAS: Fair multi-class prediction via information projection». Adv. Neur. In. 35: 38747–60.\n\nHardt, Moritz, Eric Price, e Nati Srebro. 2016. «Equality of Opportunity in Supervised Learning». In Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, a cura di Daniel D. Lee, Masashi Sugiyama, Ulrike von Luxburg, Isabelle Guyon, e Roman Garnett, 3315–23. https://proceedings.neurips.cc/paper/2016/hash/9d2682367c3935defcb1f9e247a97c0d-Abstract.html.\nInfine, la post-elaborazione degrada un modello dopo il fatto, prendendo un modello addestrato e modificandone le previsioni in un modo specifico per garantire che l’equità venga preservata (Alghamdi et al. 2022; Hardt, Price, e Srebro 2016). La post-elaborazione si basa sulle fasi di pre-elaborazione e in-elaborazione offrendo un’altra opportunità per affrontare i problemi di bias [pregiudizi] e equità nel modello dopo che è già stato addestrato.\nIl processo in tre fasi di pre-elaborazione, in-elaborazione e post-elaborazione fornisce un framework per intervenire in diverse fasi dello sviluppo del modello per mitigare i problemi relativi a pregiudizi ed equità. Mentre la pre-elaborazione e l’in-elaborazione si concentrano sui dati e sul training, la post-elaborazione consente di apportare modifiche dopo che il modello è stato completamente formato. Insieme, questi tre approcci offrono molteplici opportunità per rilevare e rimuovere pregiudizi ingiusti.\n\n\nDistribuzione Ponderata\nL’ampiezza delle definizioni di equità e degli interventi di debiasing esistenti sottolinea la necessità di una valutazione ponderata prima di distribuire sistemi ML. Come ricercatori e sviluppatori ML, lo sviluppo responsabile del modello richiede di istruirci in modo proattivo sul contesto del mondo reale, consultare esperti del settore e utenti finali e concentrarci sulla prevenzione dei danni.\nInvece di vedere le considerazioni sull’equità come una casella da spuntare, dobbiamo impegnarci profondamente con le implicazioni sociali uniche e i compromessi etici attorno a ogni modello che costruiamo. Ogni scelta tecnica su set di dati, architetture di modelli, metriche di valutazione e vincoli di distribuzione incorpora valori. Ampliando la nostra prospettiva oltre le metriche tecniche ristrette, valutando attentamente i compromessi e ascoltando le voci interessate, possiamo lavorare per garantire che i nostri sistemi espandano le opportunità anziché codificare i pregiudizi.\nLa strada da seguire non risiede in una checklist di “debiasing” arbitraria, ma nell’impegno a comprendere e sostenere la nostra responsabilità etica a ogni passo. Questo impegno inizia con l’educazione proattiva di noi stessi e la consultazione degli altri, piuttosto che limitarci a seguire i movimenti di una checklist di equità. Richiede un profondo impegno nei compromessi etici nelle nostre scelte tecniche, la valutazione degli impatti su diversi gruppi e l’ascolto delle voci maggiormente interessate.\nIn definitiva, i sistemi di intelligenza artificiale responsabili ed etici non derivano dal “debiasing” delle caselle di controllo, ma dal rispetto del nostro dovere di valutare i danni, ampliare le prospettive, comprendere i compromessi e garantire di offrire opportunità a tutti i gruppi. Questa responsabilità etica dovrebbe guidare ogni passo.\nIl collegamento tra i paragrafi è che il primo stabilisce la necessità di una valutazione ponderata delle questioni di equità piuttosto che di un approccio basato su caselle di controllo. Il secondo paragrafo si sofferma poi su come si presenta in pratica questa valutazione ponderata, ovvero impegnarsi con i compromessi, valutare gli impatti sui gruppi e ascoltare le voci interessate. Infine, l’ultimo paragrafo fa riferimento all’evitare una “checklist di debiasing arbitraria” e impegnarsi nella responsabilità etica attraverso la valutazione, la comprensione dei compromessi e l’offerta di opportunità.\n\n\n\n15.5.2 Preservare la Privacy\nIncidenti recenti hanno fatto luce su come i modelli di intelligenza artificiale possano memorizzare dati sensibili degli utenti in modi che violano la privacy. Ippolito et al. (2023) dimostra che i modelli linguistici tendono a memorizzare i dati di addestramento e possono persino riprodurre esempi di addestramento specifici. Questi rischi sono amplificati con sistemi ML personalizzati distribuiti in ambienti intimi come case o dispositivi indossabili. Prendiamo in considerazione uno smart speaker che usa le nostre conversazioni per migliorare la qualità del servizio per gli utenti che apprezzano tali miglioramenti. Sebbene potenzialmente vantaggioso, questo crea anche rischi per la privacy, poiché i malintenzionati potrebbero tentare di estrarre ciò che lo speaker “ricorda”. Il problema si estende oltre i modelli linguistici. Figura 15.2 mostra come i modelli di diffusione possono memorizzare e generare esempi di training individuali (Nicolas Carlini et al. 2023), dimostrando ulteriormente i potenziali rischi per la privacy associati ai sistemi di intelligenza artificiale che apprendono dai dati degli utenti.\n\nCarlini, Nicolas, Jamie Hayes, Milad Nasr, Matthew Jagielski, Vikash Sehwag, Florian Tramer, Borja Balle, Daphne Ippolito, e Eric Wallace. 2023. «Extracting training data from diffusion models». In 32nd USENIX Security Symposium (USENIX Security 23), 5253–70.\n\n\n\n\n\n\nFigura 15.2: Modelli di diffusione che memorizzano campioni dai dati di training. Fonte: Ippolito et al. (2023).\n\n\nIppolito, Daphne, Florian Tramer, Milad Nasr, Chiyuan Zhang, Matthew Jagielski, Katherine Lee, Christopher Choquette Choo, e Nicholas Carlini. 2023. «Preventing Generation of Verbatim Memorization in Language Models Gives a False Sense of Privacy». In Proceedings of the 16th International Natural Language Generation Conference, 5253–70. Association for Computational Linguistics. https://doi.org/10.18653/v1/2023.inlg-main.3.\n\n\nMan mano che l’intelligenza artificiale si integra sempre di più nella nostra vita quotidiana, sta diventando sempre più importante che le preoccupazioni sulla privacy e le solide misure di sicurezza per proteggere le informazioni degli utenti siano sviluppate con occhio critico. La sfida sta nel bilanciare i vantaggi dell’intelligenza artificiale personalizzata con il diritto fondamentale alla privacy.\nGli avversari possono usare queste capacità di memorizzazione e addestrare modelli per rilevare se specifici dati di addestramento hanno influenzato un modello target. Ad esempio, gli attacchi di inferenza di appartenenza addestrano un modello secondario che impara a rilevare un cambiamento negli output del modello target quando si effettuano inferenze sui dati su cui è stato addestrato rispetto a quelli su cui non è stato addestrato (Shokri et al. 2017).\n\nShokri, Reza, Marco Stronati, Congzheng Song, e Vitaly Shmatikov. 2017. «Membership Inference Attacks Against Machine Learning Models». In 2017 IEEE Symposium on Security and Privacy (SP), 3–18. IEEE; IEEE. https://doi.org/10.1109/sp.2017.41.\n\nAbadi, Martin, Andy Chu, Ian Goodfellow, H. Brendan McMahan, Ilya Mironov, Kunal Talwar, e Li Zhang. 2016. «Deep Learning with Differential Privacy». In Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security, 308–18. CCS ’16. New York, NY, USA: ACM. https://doi.org/10.1145/2976749.2978318.\nI dispositivi ML sono particolarmente vulnerabili perché sono spesso personalizzati sui dati degli utenti e vengono distribuiti in contesti ancora più intimi come la casa. Le tecniche di apprendimento automatico privato si sono evolute per stabilire misure di sicurezza contro gli avversari, come menzionato nel capitolo Sicurezza e Privacy per combattere questi problemi di privacy. Metodi come la privacy differenziale aggiungono rumore matematico durante l’addestramento per oscurare l’influenza dei singoli punti dati sul modello. Tecniche popolari come DP-SGD (Abadi et al. 2016) tagliano anche i gradienti per limitare ciò che il modello trapelerà sui dati. Tuttavia, gli utenti dovrebbero anche avere la possibilità di eliminare l’impatto dei propri dati in un secondo momento.\n\n\n15.5.3 Machine Unlearning\nCon dispositivi ML personalizzati per singoli utenti e poi distribuiti su edge remoti senza connettività, sorge una sfida: come possono i modelli “dimenticare” in modo reattivo i dati dopo la distribuzione? Se gli utenti richiedono che i loro dati vengano rimossi da un modello personalizzato, la mancanza di connettività rende impossibile la riqualificazione. Pertanto, un’efficiente dimenticanza dei dati sul dispositivo è necessaria, ma pone degli ostacoli.\nGli approcci iniziali di disapprendimento hanno incontrato delle limitazioni in questo contesto. Date le limitazioni delle risorse, recuperare modelli da zero sul dispositivo per dimenticare i dati si rivela inefficiente o addirittura impossibile. La riqualificazione completa richiede anche di conservare tutti i dati di training originali sul dispositivo, il che comporta dei rischi per la sicurezza e la privacy. Le comuni tecniche di “machine unlearning” [disapprendimento automatico] (Bourtoule et al. 2021) per sistemi ML embedded remoti non riescono a consentire la rimozione dei dati reattiva e sicura.\n\nBourtoule, Lucas, Varun Chandrasekaran, Christopher A. Choquette-Choo, Hengrui Jia, Adelin Travers, Baiwu Zhang, David Lie, e Nicolas Papernot. 2021. «Machine Unlearning». In 2021 IEEE Symposium on Security and Privacy (SP), 141–59. IEEE; IEEE. https://doi.org/10.1109/sp40001.2021.00019.\nTuttavia, metodi più recenti sembrano promettenti nel modificare i modelli in modo da dimenticare approssimativamente i dati senza doverli riqualificare completamente. Sebbene la perdita di accuratezza derivante dall’evitare ricostruzioni complete sia modesta, garantire la privacy dei dati dovrebbe comunque essere la priorità quando si gestiscono eticamente le informazioni sensibili degli utenti. Anche una minima esposizione a dati privati può violare la fiducia degli utenti. Poiché i sistemi ML diventano profondamente personalizzati, efficienza e privacy devono essere abilitate fin dall’inizio, non ripensamenti.\nLe normative globali sulla privacy, come il consolidato GDPR nell’Unione Europea, il CCPA in California e le proposte più recenti come il CPPA del Canada e l’APPI del Giappone, sottolineano il diritto di eliminare i dati personali. Queste politiche, insieme a incidenti di IA di alto profilo come la memorizzazione dei dati degli artisti da parte di Stable Diffusion, hanno evidenziato l’imperativo etico per i modelli di consentire agli utenti di eliminare i propri dati anche dopo l’addestramento.\nIl diritto di rimuovere i dati nasce da preoccupazioni sulla privacy relative alle aziende o agli avversari che abusano delle informazioni sensibili degli utenti. L’unlearning automatico si riferisce alla rimozione dell’influenza di punti specifici da un modello già addestrato. Ingenuamente, ciò comporta una riqualificazione completa senza i dati eliminati. Tuttavia, i vincoli di connettività spesso rendono la riqualificazione non fattibile per i sistemi ML personalizzati e distribuiti su edge remoti. Se uno smart speaker impara da conversazioni domestiche private, è importante mantenere l’accesso per eliminare tali dati.\nSebbene limitati, i metodi si stanno evolvendo per consentire approssimazioni efficienti della riqualificazione per l’unlearning. Modificando il tempo di inferenza dei modelli, possono imitare i dati “dimenticati” senza accesso completo ai dati di addestramento. Tuttavia, la maggior parte delle tecniche attuali è limitata a modelli semplici, ha ancora costi di risorse e scambia una certa accuratezza. Sebbene i metodi si stiano evolvendo, consentire una rimozione efficiente dei dati e rispettare la privacy degli utenti rimane fondamentale per una distribuzione TinyML responsabile.\n\n\n15.5.4 Esempi Avversari e Robustezza\nI modelli di apprendimento automatico, in particolare le reti neurali profonde, hanno un tallone d’Achille ben documentato: spesso si rompono quando vengono apportate anche piccole perturbazioni ai loro input (Szegedy et al. 2014). Questa sorprendente fragilità evidenzia un importante divario di robustezza che minaccia l’implementazione nel mondo reale in domini ad alto rischio. Apre anche la porta ad attacchi avversari progettati per ingannare deliberatamente i modelli.\nI modelli di apprendimento automatico possono mostrare una sorprendente fragilità: piccole modifiche agli input possono causare malfunzionamenti scioccanti, anche nelle reti neurali profonde all’avanguardia (Szegedy et al. 2014). Questa imprevedibilità sui dati fuori campione sottolinea le lacune nella generalizzazione e nella robustezza del modello. Data la crescente ubiquità dell’apprendimento automatico, consente anche minacce avversarie che sfruttano i punti ciechi dei modelli.\nLe reti neurali profonde dimostrano una doppia natura quasi paradossale: competenza umana nelle distribuzioni di training abbinata a un’estrema fragilità alle piccole perturbazioni di input (Szegedy et al. 2014). Questa lacuna di vulnerabilità avversaria ne evidenzia altre nelle procedure ML standard e minacce all’affidabilità nel mondo reale. Allo stesso tempo, può essere sfruttata: gli aggressori possono trovare punti di rottura del modello che gli umani non percepirebbero.\n\nSzegedy, Christian, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian J. Goodfellow, e Rob Fergus. 2014. «Intriguing properties of neural networks». In 2nd International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings, a cura di Yoshua Bengio e Yann LeCun. http://arxiv.org/abs/1312.6199.\nFigura 15.3 include un esempio di una piccola perturbazione insignificante che modifica una previsione del modello. Questa fragilità ha impatti nel mondo reale: la mancanza di robustezza mina la fiducia nell’implementazione di modelli per applicazioni ad alto rischio come auto a guida autonoma o diagnosi mediche. Inoltre, la vulnerabilità porta a minacce alla sicurezza: gli aggressori possono creare deliberatamente esempi avversari che sono percettivamente indistinguibili dai dati normali ma causano errori del modello.\n\n\n\n\n\n\nFigura 15.3: Effetto della perturbazione sulla previsione. Fonte: Microsoft.\n\n\n\nAd esempio, lavori passati mostrano attacchi riusciti che ingannano i modelli per attività come il rilevamento NSFW (Bhagoji et al. 2018), il blocco degli annunci (Tramèr et al. 2019) e il riconoscimento vocale (Nicholas Carlini et al. 2016). Sebbene gli errori in questi domini rappresentino già dei rischi per la sicurezza, il problema si estende oltre la sicurezza IT. Di recente, la robustezza avversaria è stata proposta come metrica di prestazioni aggiuntiva approssimando il comportamento del caso peggiore.\n\nBhagoji, Arjun Nitin, Warren He, Bo Li, e Dawn Song. 2018. «Practical black-box attacks on deep neural networks using efficient query mechanisms». In Proceedings of the European conference on computer vision (ECCV), 154–69.\n\nTramèr, Florian, Pascal Dupré, Gili Rusak, Giancarlo Pellegrino, e Dan Boneh. 2019. «AdVersarial: Perceptual Ad Blocking meets Adversarial Machine Learning». In Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security, 2005–21. ACM. https://doi.org/10.1145/3319535.3354222.\n\nCarlini, Nicholas, Pratyush Mishra, Tavish Vaidya, Yuankai Zhang, Micah Sherr, Clay Shields, David Wagner, e Wenchao Zhou. 2016. «Hidden voice commands». In 25th USENIX security symposium (USENIX security 16), 513–30.\nLa sorprendente fragilità del modello evidenziata sopra mette in dubbio l’affidabilità nel mondo reale e apre la porta alla manipolazione avversaria. Questa crescente vulnerabilità sottolinea diverse esigenze. In primo luogo, le valutazioni della robustezza morale sono essenziali per quantificare le vulnerabilità del modello prima dell’implementazione. L’approssimazione del comportamento del caso peggiore fa emergere punti ciechi.\nIn secondo luogo, devono essere sviluppate difese efficaci in tutti i domini per colmare queste lacune di robustezza. Con la sicurezza in gioco, gli sviluppatori non possono ignorare la minaccia di attacchi che sfruttano le debolezze del modello. Inoltre, non possiamo permetterci guasti indotti dalla fragilità per applicazioni critiche per la sicurezza come veicoli a guida autonoma e diagnosi mediche. Sono in gioco delle vite.\nInfine, la comunità di ricerca continua a mobilitarsi rapidamente in risposta. L’interesse per l’apprendimento automatico avversario è esploso poiché gli attacchi rivelano la necessità di colmare il divario di robustezza tra dati sintetici e dati del mondo reale. Le conferenze ora comunemente presentano difese per proteggere e stabilizzare i modelli. La comunità riconosce che la fragilità del modello è un problema critico che deve essere affrontato tramite test di robustezza, sviluppo di difese e ricerca continua. Evidenziando i punti ciechi e rispondendo con difese basate su principi, possiamo lavorare per garantire affidabilità e sicurezza per i sistemi di apprendimento automatico, specialmente in domini ad alto rischio.\n\n\n15.5.5 Creazione di Modelli Interpretabili\nPoiché i modelli vengono distribuiti più frequentemente in contesti ad alto rischio, professionisti, sviluppatori, utenti finali a valle e una regolamentazione crescente hanno evidenziato la necessità di spiegabilità nell’apprendimento automatico. L’obiettivo di molti metodi di interpretabilità e spiegabilità è fornire ai professionisti maggiori informazioni sul comportamento complessivo dei modelli o sul comportamento dato un input specifico. Ciò consente agli utenti di decidere se l’output o la previsione di un modello sono affidabili o meno.\nTale analisi può aiutare gli sviluppatori a eseguire il debug dei modelli e migliorare le prestazioni evidenziando distorsioni, correlazioni spurie e modalità di errore dei modelli. Nei casi in cui i modelli possono superare le prestazioni umane in un’attività, l’interpretabilità può aiutare utenti e ricercatori a comprendere meglio le relazioni nei loro dati e pattern precedentemente sconosciuti.\nEsistono molte classi di metodi di spiegabilità/interpretabilità, tra cui la spiegabilità post hoc, l’interpretabilità intrinseca e l’interpretabilità meccanicistica. Questi metodi mirano a rendere più comprensibili i modelli di apprendimento automatico complessi e a garantire che gli utenti possano fidarsi delle previsioni del modello, soprattutto in contesti critici. Fornendo trasparenza nel comportamento del modello, le tecniche di spiegabilità sono uno strumento importante per sviluppare sistemi di intelligenza artificiale sicuri, equi e affidabili.\n\nSpiegabilità Post Hoc\nI metodi di spiegabilità “post hoc” in genere spiegano il comportamento di output di un modello black-box su un input specifico. metodi più diffusi includono spiegazioni controfattuali, metodi di attribuzione delle caratteristiche e spiegazioni basate sui concetti.\nSpiegazioni controfattuali, spesso chiamate anche ricorso algoritmico, “Se X non si fosse verificato, Y non si sarebbe verificato” (Wachter, Mittelstadt, e Russell 2017). Ad esempio, si consideri una persona che richiede un prestito bancario la cui richiesta viene respinta da un modello. Potrebbe chiedere alla propria banca un ricorso o come modificare per essere idonea a un prestito. Una spiegazione controfattuale indicherebbe loro quali caratteristiche devono modificare e di quanto, in modo che la previsione del modello cambi.\n\nWachter, Sandra, Brent Mittelstadt, e Chris Russell. 2017. «Counterfactual Explanations Without Opening the Black Box: Automated Decisions and the GDPR». SSRN Electronic Journal 31: 841. https://doi.org/10.2139/ssrn.3063289.\n\nSelvaraju, Ramprasaath R., Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, e Dhruv Batra. 2017. «Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization». In 2017 IEEE International Conference on Computer Vision (ICCV), 618–26. IEEE. https://doi.org/10.1109/iccv.2017.74.\n\nSmilkov, Daniel, Nikhil Thorat, Been Kim, Fernanda Viégas, e Martin Wattenberg. 2017. «Smoothgrad: Removing noise by adding noise». ArXiv preprint abs/1706.03825. https://arxiv.org/abs/1706.03825.\n\nRibeiro, Marco Tulio, Sameer Singh, e Carlos Guestrin. 2016. «” Why should i trust you?” Explaining the predictions of any classifier». In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, 1135–44.\n\nLundberg, Scott M., e Su-In Lee. 2017. «A Unified Approach to Interpreting Model Predictions». In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, a cura di Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, e Roman Garnett, 4765–74. https://proceedings.neurips.cc/paper/2017/hash/8a20a8621978632d76c43dfd28b67767-Abstract.html.\nI metodi di attribuzione delle caratteristiche evidenziano le caratteristiche di input che sono importanti o necessarie per una particolare previsione. Per un modello di visione artificiale, ciò significherebbe evidenziare i singoli pixel che hanno contribuito maggiormente all’etichetta prevista dell’immagine. Si noti che questi metodi non spiegano in che modo quei pixel/caratteristiche influenzano la previsione, ma solo che lo fanno. I metodi comuni includono gradienti di input, GradCAM (Selvaraju et al. 2017), SmoothGrad (Smilkov et al. 2017), LIME (Ribeiro, Singh, e Guestrin 2016) e SHAP (Lundberg e Lee 2017).\nFornendo esempi di modifiche alle caratteristiche di input che altererebbero una previsione (controfattuali) o indicando le caratteristiche più influenti per una data previsione (attribuzione), queste tecniche di spiegazione post hoc fanno luce sul comportamento del modello per input individuali. Questa trasparenza granulare aiuta gli utenti a determinare se possono fidarsi e agire su output di modelli specifici.\nLe spiegazioni basate sui concetti mirano a spiegare il comportamento del modello e gli output utilizzando un set predefinito di concetti semantici (ad esempio, il modello riconosce la classe di scena “camera da letto” in base alla presenza dei concetti “letto” e “cuscino”). Lavori recenti mostrano che gli utenti spesso preferiscono queste spiegazioni a quelle basate sull’attribuzione e sugli esempi perché “assomigliano al ragionamento e alle spiegazioni umane” (Vikram V. Ramaswamy et al. 2023b). I metodi di spiegazione basati sui concetti più diffusi includono TCAV (Cai et al. 2019), Network Dissection (Bau et al. 2017) e decomposizione della base interpretabile (Zhou et al. 2018).\n\nRamaswamy, Vikram V, Sunnie SY Kim, Ruth Fong, e Olga Russakovsky. 2023b. «UFO: A unified method for controlling Understandability and Faithfulness Objectives in concept-based explanations for CNNs». ArXiv preprint abs/2303.15632. https://arxiv.org/abs/2303.15632.\n\nCai, Carrie J., Emily Reif, Narayan Hegde, Jason Hipp, Been Kim, Daniel Smilkov, Martin Wattenberg, et al. 2019. «Human-Centered Tools for Coping with Imperfect Algorithms During Medical Decision-Making». In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems, a cura di Jennifer G. Dy e Andreas Krause, 80:2673–82. Proceedings of Machine Learning Research. ACM. https://doi.org/10.1145/3290605.3300234.\n\nBau, David, Bolei Zhou, Aditya Khosla, Aude Oliva, e Antonio Torralba. 2017. «Network Dissection: Quantifying Interpretability of Deep Visual Representations». In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 3319–27. IEEE. https://doi.org/10.1109/cvpr.2017.354.\n\nZhou, Bolei, Yiyou Sun, David Bau, e Antonio Torralba. 2018. «Interpretable basis decomposition for visual explanation». In Proceedings of the European Conference on Computer Vision (ECCV), 119–34.\n\nRamaswamy, Vikram V., Sunnie S. Y. Kim, Ruth Fong, e Olga Russakovsky. 2023a. «Overlooked Factors in Concept-Based Explanations: Dataset Choice, Concept Learnability, and Human Capability». In 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 10932–41. IEEE. https://doi.org/10.1109/cvpr52729.2023.01052.\nSi noti che questi metodi sono estremamente sensibili alla dimensione e alla qualità del set di concetti e c’è un compromesso tra la loro accuratezza e fedeltà e la loro interpretabilità o comprensibilità per gli esseri umani (Vikram V. Ramaswamy et al. 2023a). Tuttavia, mappando le previsioni del modello su concetti comprensibili per gli esseri umani, le spiegazioni basate sui concetti possono fornire trasparenza nel ragionamento alla base degli output del modello.\n\n\nInterpretabilità Intrinseca\nI modelli intrinsecamente interpretabili sono costruiti in modo tale che le loro spiegazioni siano parte dell’architettura del modello e siano quindi naturalmente fedeli, il che a volte li rende preferibili alle spiegazioni post-hoc applicate ai modelli black-box, specialmente in domini ad alto rischio in cui la trasparenza è fondamentale (Rudin 2019). Spesso, questi modelli sono vincolati in modo che le relazioni tra le caratteristiche di input e le previsioni siano facili da seguire per gli esseri umani (modelli lineari, alberi decisionali, set di decisioni, modelli k-NN) o obbediscano alla conoscenza strutturale del dominio, come la monotonicità (Gupta et al. 2016), la causalità o l’additività (Lou et al. 2013; Beck e Jackman 1998).\n\nRudin, Cynthia. 2019. «Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead». Nature Machine Intelligence 1 (5): 206–15. https://doi.org/10.1038/s42256-019-0048-x.\n\nGupta, Maya, Andrew Cotter, Jan Pfeifer, Konstantin Voevodski, Kevin Canini, Alexander Mangylov, Wojciech Moczydlowski, e Alexander Van Esbroeck. 2016. «Monotonic calibrated interpolated look-up tables». The Journal of Machine Learning Research 17 (1): 3790–3836.\n\nLou, Yin, Rich Caruana, Johannes Gehrke, e Giles Hooker. 2013. «Accurate intelligible models with pairwise interactions». In Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining, a cura di Inderjit S. Dhillon, Yehuda Koren, Rayid Ghani, Ted E. Senator, Paul Bradley, Rajesh Parekh, Jingrui He, Robert L. Grossman, e Ramasamy Uthurusamy, 623–31. ACM. https://doi.org/10.1145/2487575.2487579.\n\nBeck, Nathaniel, e Simon Jackman. 1998. «Beyond Linearity by Default: Generalized Additive Models». Am. J. Polit. Sci. 42 (2): 596. https://doi.org/10.2307/2991772.\n\nKoh, Pang Wei, Thao Nguyen, Yew Siang Tang, Stephen Mussmann, Emma Pierson, Been Kim, e Percy Liang. 2020. «Concept Bottleneck Models». In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, 119:5338–48. Proceedings of Machine Learning Research. PMLR. http://proceedings.mlr.press/v119/koh20a.html.\n\nChen, Chaofan, Oscar Li, Daniel Tao, Alina Barnett, Cynthia Rudin, e Jonathan Su. 2019. «This Looks Like That: Deep Learning for Interpretable Image Recognition». In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, a cura di Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d’Alché-Buc, Emily B. Fox, e Roman Garnett, 8928–39. https://proceedings.neurips.cc/paper/2019/hash/adf7ee2dcf142b0e11888e72b43fcb75-Abstract.html.\nTuttavia, lavori più recenti hanno allentato le restrizioni sui modelli intrinsecamente interpretabili, utilizzando modelli black-box per l’estrazione delle caratteristiche e un modello intrinsecamente interpretabile più semplice per la classificazione, consentendo spiegazioni fedeli che collegano le caratteristiche di alto livello alla previsione. Ad esempio, i Concept Bottleneck Models (Koh et al. 2020) prevedono un set di concetti c che viene passato in un classificatore lineare. I ProtoPNets (Chen et al. 2019) sezionano gli input in combinazioni lineari di somiglianze con parti prototipiche del set di training.\n\n\nInterpretabilità Meccanicistica\nI metodi di interpretabilità meccanicistica cercano di effettuare il reverse engineering delle reti neurali, spesso paragonandoli a come si potrebbe effettuare quello di un binario compilato o a come i neuroscienziati tentano di decodificare la funzione di singoli neuroni e circuiti nel cervello. La maggior parte delle ricerche sull’interpretabilità meccanicistica vede i modelli come un grafo computazionale (Geiger et al. 2021) e i circuiti sono sottografi con funzionalità distinte (Wang e Zhan 2019). Gli attuali approcci all’estrazione di circuiti dalle reti neurali e alla comprensione della loro funzionalità si basano sull’ispezione manuale umana delle visualizzazioni prodotte dai circuiti (Olah et al. 2020).\n\nGeiger, Atticus, Hanson Lu, Thomas Icard, e Christopher Potts. 2021. «Causal Abstractions of Neural Networks». In Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, a cura di Marc’Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, e Jennifer Wortman Vaughan, 9574–86. https://proceedings.neurips.cc/paper/2021/hash/4f5c422f4d49a5a807eda27434231040-Abstract.html.\n\nWang, LingFeng, e YaQing Zhan. 2019. «A conceptual peer review model for arXiv and other preprint databases». Learn. Publ. 32 (3): 213–19. https://doi.org/10.1002/leap.1229.\n\nOlah, Chris, Nick Cammarata, Ludwig Schubert, Gabriel Goh, Michael Petrov, e Shan Carter. 2020. «Zoom In: An Introduction to Circuits». Distill 5 (3): e00024–001. https://doi.org/10.23915/distill.00024.001.\n\nDavarzani, Samaneh, David Saucier, Purva Talegaonkar, Erin Parker, Alana Turner, Carver Middleton, Will Carroll, et al. 2023. «Closing the Wearable Gap: Footankle kinematic modeling via deep learning models based on a smart sock wearable». Wearable Technologies 4. https://doi.org/10.1017/wtc.2023.3.\nIn alternativa, alcuni approcci creano autoencoder sparsi che incoraggiano i neuroni a codificare caratteristiche interpretabili districate (Davarzani et al. 2023). Questo campo è molto più nuovo rispetto alle aree esistenti in spiegabilità e interpretabilità e, in quanto tale, la maggior parte dei lavori è generalmente esplorativa piuttosto che orientata alla soluzione.\nCi sono molti problemi nell’interpretabilità meccanicistica, tra cui la polisemanticità di neuroni e circuiti, l’inconveniente e la soggettività dell’etichettatura umana e lo spazio di ricerca esponenziale per l’identificazione dei circuiti in grandi modelli con miliardi o trilioni di neuroni.\n\n\nSfide e Considerazioni\nMan mano che i metodi per interpretare e spiegare i modelli progrediscono, è importante notare che gli esseri umani si fidano troppo e abusano degli strumenti di interpretabilità (Kaur et al. 2020) e che la fiducia di un utente in un modello dovuta a una spiegazione può essere indipendente dalla correttezza delle spiegazioni (Lakkaraju e Bastani 2020). Pertanto, è necessario che oltre a valutare la fedeltà/correttezza delle spiegazioni, i ricercatori debbano anche garantire che i metodi di interpretabilità siano sviluppati e implementati tenendo a mente un utente specifico e che vengano eseguiti studi sugli utenti per valutarne l’efficacia e l’utilità nella pratica.\n\nKaur, Harmanpreet, Harsha Nori, Samuel Jenkins, Rich Caruana, Hanna Wallach, e Jennifer Wortman Vaughan. 2020. «Interpreting Interpretability: Understanding Data Scientists’ Use of Interpretability Tools for Machine Learning». In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems, a cura di Regina Bernhaupt, Florian ’Floyd’Mueller, David Verweij, Josh Andres, Joanna McGrenere, Andy Cockburn, Ignacio Avellino, et al., 1–14. ACM. https://doi.org/10.1145/3313831.3376219.\n\nLakkaraju, Himabindu, e Osbert Bastani. 2020. «”How do I fool you?”: Manipulating User Trust via Misleading Black Box Explanations». In Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society, 79–85. ACM. https://doi.org/10.1145/3375627.3375833.\nInoltre, le spiegazioni devono essere adattate alle competenze dell’utente, all’attività per cui stanno utilizzando la spiegazione e alla corrispondente quantità minima di informazioni richieste affinché la spiegazione sia utile per prevenire il sovraccarico di informazioni.\nMentre interpretabilità/spiegabilità sono aree popolari nella ricerca sull’apprendimento automatico, pochissimi lavori studiano la loro intersezione con TinyML ed edge computing. Dato che un’applicazione significativa di TinyML è l’assistenza sanitaria, che spesso richiede elevata trasparenza e interpretabilità, le tecniche esistenti devono essere testate per scalabilità ed efficienza relativamente ai dispositivi edge. Molti metodi si basano su passaggi aggiuntivi “forward” e “backward” e alcuni richiedono persino un training approfondito nei modelli proxy, che non sono fattibili su microcontrollori con risorse limitate.\nDetto questo, i metodi di spiegabilità possono essere molto utili nello sviluppo di modelli per dispositivi edge, in quanto possono fornire informazioni su come i dati di input e i modelli possono essere compressi e su come le rappresentazioni possono cambiare dopo la compressione. Inoltre, molti modelli interpretabili sono spesso più piccoli delle loro controparti black-box, il che potrebbe essere utile per le applicazioni TinyML.\n\n\n\n15.5.6 Monitoraggio delle Prestazioni del Modello\nMentre gli sviluppatori possono addestrare modelli che sembrano avversarialmente robusti, equi e interpretabili prima della distribuzione, è fondamentale che sia gli utenti sia i proprietari del modello ne continuino a monitorare le prestazioni e l’affidabilità durante l’intero ciclo di vita. I dati cambiano frequentemente nella pratica, il che può spesso comportare cambiamenti nella distribuzione. Questi cambiamenti nella distribuzione possono avere un impatto profondo sulle prestazioni predittive “vanilla” del modello e sulla sua affidabilità (equità, robustezza e interpretabilità) nei dati del mondo reale.\nInoltre, le definizioni di equità cambiano frequentemente nel tempo, come ciò che la società considera un attributo protetto, e anche le competenze degli utenti che chiedono spiegazioni possono cambiare.\nPer garantire che i modelli rimangano aggiornati con tali cambiamenti nel mondo reale, gli sviluppatori devono valutare continuamente i loro modelli su dati e standard attuali e rappresentativi e aggiornare i modelli quando necessario.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>IA Responsabile</span>"
    ]
  },
  {
    "objectID": "contents/core/responsible_ai/responsible_ai.it.html#sfide-di-implementazione",
    "href": "contents/core/responsible_ai/responsible_ai.it.html#sfide-di-implementazione",
    "title": "15  IA Responsabile",
    "section": "15.6 Sfide di Implementazione",
    "text": "15.6 Sfide di Implementazione\n\n15.6.1 Strutture Organizzative e Culturali\nSebbene innovazione e regolamentazione siano spesso viste come interessi contrapposti, molti paesi hanno ritenuto necessario fornire supervisione man mano che i sistemi di intelligenza artificiale si espandono in più settori. Come mostrato in Figura 15.4, questa supervisione è diventata cruciale poiché questi sistemi continuano a permeare vari settori e ad avere un impatto sulla vita delle persone. Ulteriori discussioni su questo argomento sono disponibili in Human-Centered AI, Capitolo 22 “Government Interventions and Regulations”.\n\n\n\n\n\n\nFigura 15.4: Come vari gruppi influenzano l’AI incentrata sull’uomo. Fonte: Shneiderman (2020).\n\n\nShneiderman, Ben. 2020. «Bridging the Gap Between Ethics and Practice: Guidelines for Reliable, Safe, and Trustworthy Human-centered AI Systems». ACM Trans. Interact. Intell. Syst. 10 (4): 1–31. https://doi.org/10.1145/3419764.\n\n\nIn questo capitolo abbiamo trattato diverse politiche chiave volte a guidare lo sviluppo e l’implementazione dell’IA responsabile. Di seguito è riportato un riepilogo di queste politiche, insieme ad altri framework degni di nota che riflettono una spinta globale per la trasparenza nei sistemi di IA:\n\nIl General Data Protection Regulation (GDPR) dell’Unione Europea impone misure di trasparenza e protezione dei dati per i sistemi di IA che gestiscono dati personali.\nL’AI Bill of Rights delinea i principi per un utilizzo etico dell’IA negli Stati Uniti, sottolineando correttezza, privacy e responsabilità.\nIl California Consumer Privacy Act (CCPA) protegge i dati dei consumatori e ritiene le organizzazioni responsabili per l’uso improprio dei dati.\nIl Responsible Use of Artificial Intelligence del Canada delinea le migliori pratiche per l’implementazione etica dell’IA.\nL’Act on the Protection of Personal Information (APPI) del Giappone stabilisce linee guida per la gestione dei dati personali nei sistemi di IA.\nLa proposta canadese del Consumer Privacy Protection Act (CPPA) mira a rafforzare la protezione della privacy negli ecosistemi digitali.\nIl White Paper on Artificial Intelligence: A European Approach to Excellence and Trust della Commissione Europea sottolinea lo sviluppo etico dell’IA insieme all’innovazione.\nL’Information Commissioner’s Office del Regno Unito e la Guidance on Explaining AI Decisions dell’Alan Turing Institute forniscono raccomandazioni per aumentare la trasparenza dell’IA.\n\nQueste politiche evidenziano uno sforzo globale in corso per bilanciare innovazione e responsabilità e garantire che i sistemi di IA siano sviluppati e distribuiti in modo responsabile.\n\n\n15.6.2 Ottenere Dati di Qualità e Rappresentativi\nCome discusso nel capitolo Data Engineering, la progettazione responsabile dell’IA deve avvenire in tutte le fasi della pipeline, inclusa la raccolta dei dati. Ciò solleva la domanda: cosa significa che i dati siano di alta qualità e rappresentativi? Consideriamo i seguenti scenari che ostacolano la rappresentatività dei dati:\n\nSquilibrio dei Sottogruppi\nQuesto è probabilmente ciò che viene in mente quando si sente parlare di “dati rappresentativi”. Lo squilibrio dei sottogruppi significa che il set di dati contiene relativamente più dati da un sottogruppo rispetto a un altro. Questo squilibrio può influire negativamente sul modello ML a valle, facendolo sovradimensionare per un sottogruppo di persone e con prestazioni scadenti per un altro.\nUn esempio di conseguenza dello squilibrio dei sottogruppi è la discriminazione razziale nella tecnologia di riconoscimento facciale (Buolamwini e Gebru 2018); gli algoritmi commerciali di riconoscimento facciale hanno tassi di errore fino al 34% peggiori sulle donne dalla pelle scura rispetto agli uomini dalla pelle chiara.\n\nBuolamwini, Joy, e Timnit Gebru. 2018. «Gender shades: Intersectional accuracy disparities in commercial gender classification». In Conference on fairness, accountability and transparency, 77–91. PMLR.\nSi noti che lo squilibrio dei dati è reciproco e i sottogruppi possono anche essere sovrarappresentati in modo dannoso nel set di dati. Ad esempio, l’Allegheny Family Screening Tool (AFST) prevede la probabilità che un bambino venga alla fine allontanato da una casa. L’AFST produce punteggi sproporzionati per diversi sottogruppi, uno dei motivi è che è basato su dati storicamente distorti, provenienti da sistemi legali penali minorili e per adulti, agenzie di assistenza pubblica e agenzie e programmi di salute comportamentale.\n\n\nQuantificazione dei Risultati Target\nCiò si verifica in applicazioni in cui l’etichetta di verità di base non può essere misurata o è difficile da rappresentare in una singola quantità. Ad esempio, un modello ML in un’applicazione mobile per il benessere potrebbe voler prevedere i livelli di stress individuali. Le vere etichette di stress sono impossibili da ottenere direttamente e devono essere dedotte da altri segnali biologici, come la variabilità della frequenza cardiaca e i dati auto-riportati dall’utente. In queste situazioni, il rumore è incorporato nei dati per progettazione, rendendo questo un compito ML impegnativo.\n\n\nSpostamento della Distribuzione\nI dati potrebbero non rappresentare più un compito se un evento esterno importante causa un drastico cambiamento della fonte dati. Il modo più comune di pensare alle “distribution shift” [spostamenti della distribuzione] è rispetto al tempo; ad esempio, i dati sulle abitudini di acquisto dei consumatori raccolti prima del Covid potrebbero non essere più presenti nel comportamento dei consumatori oggi.\nIl trasferimento provoca un’altra forma di spostamento della distribuzione. Ad esempio, quando si applica un sistema di triage addestrato sui dati di un ospedale a un altro, potrebbe verificarsi uno spostamento nella distribuzione se i due ospedali sono molto diversi.\n\n\nRaccolta Dati\nUna soluzione ragionevole per molti dei problemi di cui sopra con dati non rappresentativi o di bassa qualità è raccoglierne di più; possiamo raccogliere più dati mirati a un sottogruppo sottorappresentato o dall’ospedale target a cui il nostro modello potrebbe essere trasferito. Tuttavia, per alcune ragioni, raccogliere più dati è una soluzione inappropriata o non fattibile per il compito da svolgere.\n\nLa raccolta dati può essere dannosa. Questo è il paradosso dell’esposizione, la situazione in cui coloro che traggono un guadagno significativo dalla raccolta dei propri dati sono anche coloro che sono messi a rischio dal processo di raccolta (D’ignazio e Klein (2023), Capitolo 4). Ad esempio, raccogliere più dati su individui non binari può essere importante per garantire l’equità dell’applicazione ML, ma li espone anche a rischi, a seconda di chi raccoglie i dati e di come (se i dati sono facilmente identificabili, contengono contenuti sensibili, ecc.).\nLa raccolta dati può essere costosa. In alcuni ambiti, come l’assistenza sanitaria, ottenere dati può essere costoso in termini di tempo e denaro.\nRaccolta dati distorta. Le cartelle cliniche elettroniche sono un’enorme fonte di dati per le applicazioni sanitarie basate su ML. A parte i problemi di rappresentazione dei sottogruppi, i dati stessi possono essere raccolti in modo distorto. Ad esempio, il linguaggio negativo (“non aderente”, “non disposto”) è utilizzato in modo sproporzionato sui pazienti neri (Himmelstein, Bates, e Zhou 2022).\n\n\nD’ignazio, Catherine, e Lauren F Klein. 2023. Data feminism. MIT press.\n\nHimmelstein, Gracie, David Bates, e Li Zhou. 2022. «Examination of Stigmatizing Language in the Electronic Health Record». JAMA Network Open 5 (1): e2144967. https://doi.org/10.1001/jamanetworkopen.2021.44967.\nConcludiamo con diverse strategie aggiuntive per mantenere la qualità dei dati. Innanzitutto, è fondamentale promuovere una comprensione più approfondita dei dati. Ciò può essere ottenuto tramite l’implementazione di etichette e misure standardizzate della qualità dei dati, come nel Data Nutrition Project. Collaborare con le organizzazioni responsabili della raccolta dei dati aiuta a garantire che i dati vengano interpretati correttamente. In secondo luogo, è importante impiegare strumenti efficaci per l’esplorazione dei dati. Le tecniche di visualizzazione e le analisi statistiche possono rivelare problemi con i dati. Infine, stabilire un ciclo di feedback all’interno della pipeline ML è essenziale per comprendere le implicazioni reali dei dati. Le metriche, come le misure di equità, ci consentono di definire la “qualità dei dati” nel contesto dell’applicazione downstream; il miglioramento dell’equità può migliorare direttamente la qualità delle previsioni che gli utenti finali ricevono.\n\n\n\n15.6.3 Bilanciamento di Accuratezza e Altri Obiettivi\nI modelli di apprendimento automatico vengono spesso valutati solo in base all’accuratezza, ma questa singola metrica non riesce a catturare completamente le prestazioni del modello e i compromessi per i sistemi di intelligenza artificiale responsabili. Altre dimensioni etiche, come correttezza, robustezza, interpretabilità e privacy, possono competere con la pura accuratezza predittiva durante lo sviluppo del modello. Ad esempio, modelli intrinsecamente interpretabili come piccoli alberi decisionali o classificatori lineari con funzionalità semplificate barattano intenzionalmente una certa accuratezza per la trasparenza nel comportamento del modello e nelle previsioni. Mentre questi modelli semplificati raggiungono una minore accuratezza non catturando tutta la complessità nel set di dati, una migliore interpretabilità crea fiducia consentendo l’analisi diretta da parte di professionisti umani.\nInoltre, alcune tecniche pensate per migliorare la robustezza avversaria, come esempi di training avversario o riduzione della dimensionalità, possono degradare l’accuratezza dei dati di convalida puliti. In applicazioni sensibili come l’assistenza sanitaria, concentrarsi strettamente sull’accuratezza all’avanguardia comporta rischi etici se consente ai modelli di fare più affidamento su correlazioni spurie che introducono distorsioni o utilizzano ragionamenti opachi. Pertanto, gli obiettivi di prestazione appropriati dipendono in larga misura dal contesto socio-tecnico.\nMetodologie come Value Sensitive Design forniscono framework per valutare formalmente le priorità di vari stakeholder all’interno del sistema di distribuzione nel mondo reale. Ciò spiega le tensioni tra valori quali accuratezza, interpretabilità ed equità, che possono quindi orientare decisioni di compromesso responsabili. Per un sistema di diagnosi medica, raggiungere la massima accuratezza potrebbe non essere l’obiettivo unico: migliorare la trasparenza per creare fiducia nei professionisti o ridurre i pregiudizi verso i gruppi minoritari potrebbe giustificare piccole perdite di accuratezza. L’analisi del contesto socio-tecnico è fondamentale per stabilire questi obiettivi.\nAdottando una visione olistica, possiamo bilanciare responsabilmente l’accuratezza con altri obiettivi etici per il successo del modello. Il monitoraggio continuo delle prestazioni lungo più dimensioni è fondamentale man mano che il sistema si evolve dopo la distribuzione.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>IA Responsabile</span>"
    ]
  },
  {
    "objectID": "contents/core/responsible_ai/responsible_ai.it.html#considerazioni-etiche-nella-progettazione-dellia",
    "href": "contents/core/responsible_ai/responsible_ai.it.html#considerazioni-etiche-nella-progettazione-dellia",
    "title": "15  IA Responsabile",
    "section": "15.7 Considerazioni Etiche Nella Progettazione dell’IA",
    "text": "15.7 Considerazioni Etiche Nella Progettazione dell’IA\nDobbiamo discutere almeno di alcune delle numerose questioni etiche in gioco nella progettazione e nell’applicazione di sistemi di intelligenza artificiale e di diversi framework per affrontare tali questioni, tra cui quelle relative alla sicurezza dell’intelligenza artificiale, all’interazione uomo-computer (HCI) e alla scienza, tecnologia e società (STS).\n\n15.7.1 Sicurezza dell’Intelligenza Artificiale e Allineamento dei Valori\nNel 1960, Norbert Weiner scrisse: “’se utilizziamo, per raggiungere i nostri scopi, un’agenzia meccanica con il cui funzionamento non possiamo interferire efficacemente… faremmo meglio ad essere abbastanza sicuri che lo scopo attribuito alla macchina sia lo scopo che desideriamo” (Wiener 1960).\n\nWiener, Norbert. 1960. «Some Moral and Technical Consequences of Automation: As machines learn they may develop unforeseen strategies at rates that baffle their programmers.» Science 131 (3410): 1355–58. https://doi.org/10.1126/science.131.3410.1355.\n\nRussell, Stuart. 2021. «Human-compatible artificial intelligence». Human-like machine intelligence, 3–23.\nNegli ultimi anni, poiché le capacità dei modelli di deep learning hanno raggiunto, e talvolta persino superato, le capacità umane, la questione della creazione di sistemi di intelligenza artificiale che agiscano in accordo con le intenzioni umane invece di perseguire obiettivi non intenzionali o indesiderati è diventata fonte di preoccupazione (Russell 2021). Nel campo della sicurezza dell’IA, un obiettivo particolare riguarda “l’allineamento dei valori”, ovvero il problema di come codificare lo scopo “giusto” nelle macchine Intelligenza artificiale compatibile con gli esseri umani. L’attuale ricerca sull’IA presuppone che conosciamo gli obiettivi che vogliamo raggiungere e “studia la capacità di raggiungere gli obiettivi, non la progettazione di tali obiettivi”.\nTuttavia, i complessi contesti di distribuzione nel mondo reale rendono difficile definire esplicitamente “lo scopo giusto” per le macchine, richiedendo quadri per l’impostazione di obiettivi responsabili ed etici. Metodologie come Value Sensitive Design forniscono meccanismi formali per far emergere le tensioni tra i valori e le priorità delle parti interessate.\nAdottando una visione socio-tecnica olistica, possiamo garantire meglio che i sistemi intelligenti perseguano obiettivi che si allineano con ampie intenzioni umane anziché massimizzare metriche ristrette come la sola accuratezza. Raggiungere questo obiettivo nella pratica rimane una questione di ricerca aperta e critica man mano che le capacità dell’IA avanzano rapidamente.\nL’assenza di questo allineamento può portare a diversi problemi di sicurezza dell’IA, come documentato in una varietà di modelli di deep learning. Una caratteristica comune dei sistemi che ottimizzano per un obiettivo è che le variabili non direttamente incluse nell’obiettivo possono essere impostate su valori estremi per aiutare a ottimizzare per quell’obiettivo, portando a problemi caratterizzati come gioco di specifiche, hacking di ricompensa, ecc., nel “reinforcement learning (RL)” [apprendimento per rinforzo].\nNegli ultimi anni, un’implementazione particolarmente popolare di RL è stata quella dei modelli pre-addestrati utilizzando apprendimento auto-supervisionato e “Reinforcement Learning From Human Feedback (RLHF)” [apprendimento per rinforzo fine-tuned da feedback umano] (Christiano et al. 2017). Ngo 2022 (Ngo, Chan, e Mindermann 2022) sostiene che premiando i modelli per apparire innocui ed etici e massimizzando al contempo i risultati utili, RLHF potrebbe incoraggiare l’emergere di tre proprietà problematiche: hacking della ricompensa consapevole della situazione, in cui le politiche sfruttano la fallibilità umana per ottenere un’elevata ricompensa, obiettivi rappresentati internamente non allineati che si generalizzano oltre la distribuzione di messa a punto RLHF e strategie di ricerca del potere.\n\nChristiano, Paul F., Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, e Dario Amodei. 2017. «Deep Reinforcement Learning from Human Preferences». In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, a cura di Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, e Roman Garnett, 4299–4307. https://proceedings.neurips.cc/paper/2017/hash/d5e2c0adad503c91f91df240d0cd4e49-Abstract.html.\n\nNgo, Richard, Lawrence Chan, e Sören Mindermann. 2022. «The alignment problem from a deep learning perspective». ArXiv preprint abs/2209.00626. https://arxiv.org/abs/2209.00626.\n\nVan Noorden, Richard. 2016. «ArXiv preprint server plans multimillion-dollar overhaul». Nature 534 (7609): 602–2. https://doi.org/10.1038/534602a.\nAllo stesso modo, Van Noorden (2016) delinea sei problemi concreti per la sicurezza dell’IA, tra cui evitare effetti collaterali negativi, evitare hacking della ricompensa, supervisione scalabile per aspetti dell’obiettivo che sono troppo costosi per essere valutati frequentemente durante il training, strategie di esplorazione sicure che incoraggiano la creatività prevenendo al contempo i danni e robustezza allo spostamento distributivo in ambienti di test invisibili.\n\n\n15.7.2 Sistemi Autonomi e Controllo [e Fiducia]\nLe conseguenze dei sistemi autonomi che agiscono indipendentemente dalla supervisione umana e spesso al di fuori del giudizio umano sono state ampiamente documentate in diversi settori e casi d’uso. Più di recente, il Dipartimento dei veicoli a motore della California ha sospeso i permessi di distribuzione e collaudo di Cruise per i suoi veicoli autonomi, citando “rischi irragionevoli per la sicurezza pubblica”. Uno di questi incidenti si è verificato quando un veicolo ha colpito un pedone che stava attraversando le strisce pedonali dopo che il semaforo era diventato verde e al veicolo è stato permesso di procedere. Nel 2018, un pedone che attraversava la strada con la sua bicicletta è morto quando un’auto Uber a guida autonoma, che operava in modalità autonoma, non è riuscita a classificare accuratamente il suo corpo in movimento come un oggetto da evitare.\nAnche i sistemi autonomi oltre ai veicoli a guida autonoma sono suscettibili a tali problemi, con conseguenze potenzialmente più gravi, poiché i droni alimentati da remoto stanno già rimodellando la guerra. Sebbene tali incidenti sollevino importanti questioni etiche su chi dovrebbe essere ritenuto responsabile quando questi sistemi falliscono, evidenziano anche le sfide tecniche nel dare il pieno controllo di attività complesse e reali alle macchine.\nIn sostanza, c’è una tensione tra autonomia umana e delle macchine. Le discipline ingegneristiche e informatiche hanno teso a concentrarsi sull’autonomia delle macchine. Ad esempio, a partire dal 2019, una ricerca della parola “autonomia” nella Digital Library dell’Association for Computing Machinery (ACM) rivela che dei 100 articoli più citati, il 90% riguarda l’autonomia delle macchine (Calvo et al. 2020). Nel tentativo di costruire sistemi a beneficio dell’umanità, queste discipline hanno assunto, senza dubbio, l’aumento della produttività, dell’efficienza e dell’automazione come strategie primarie per il beneficio dell’umanità.\n\nMcCarthy, John. 1981. «Epistemological Problems Of Artificial Intelligence». In Readings in Artificial Intelligence, 459–65. Elsevier. https://doi.org/10.1016/b978-0-934613-03-3.50035-0.\nQuesti obiettivi pongono l’automazione delle macchine in prima linea, spesso a spese dell’uomo. Questo approccio soffre di sfide intrinseche, come notato fin dai primi giorni dell’IA attraverso il “Frame problem” [specifica degli effetti] e il “qualification problem” [qualificazione delle precondizioni] (cfr. http://www.diag.uniroma1.it/~nardi/Didattica/RC/lezioni/sitcalc-1.pdf), che formalizza l’osservazione che è impossibile specificare tutte le precondizioni necessarie per il successo di un’azione nel mondo reale (McCarthy 1981).\nQueste limitazioni logiche hanno dato origine ad approcci matematici come la “Responsibility-sensitive safety (RSS)” [sicurezza sensibile alla responsabilità] (Shalev-Shwartz, Shammah, e Shashua 2017), che mira a scomporre l’obiettivo finale di un sistema di guida automatizzato (vale a dire la sicurezza) in condizioni concrete e verificabili che possono essere rigorosamente formulate in termini matematici. L’obiettivo dell’RSS è che tali norme di sicurezza garantiscano la sicurezza del “Automated Driving System (ADS)” [sistema di guida autonoma] nella rigorosa forma di dimostrazione matematica. Tuttavia, tali approcci tendono a utilizzare l’automazione per affrontare i problemi dell’automazione e sono suscettibili a molti degli stessi problemi.\n\nShalev-Shwartz, Shai, Shaked Shammah, e Amnon Shashua. 2017. «On a formal model of safe and scalable self-driving cars». ArXiv preprint abs/1708.06374. https://arxiv.org/abs/1708.06374.\n\nFriedman, Batya. 1996. «Value-sensitive design». Interactions 3 (6): 16–23. https://doi.org/10.1145/242485.242493.\n\nPeters, Dorian, Rafael A. Calvo, e Richard M. Ryan. 2018. «Designing for Motivation, Engagement and Wellbeing in Digital Experience». Front. Psychol. 9 (maggio): 797. https://doi.org/10.3389/fpsyg.2018.00797.\n\nRyan, Richard M., e Edward L. Deci. 2000. «Self-determination theory and the facilitation of intrinsic motivation, social development, and well-being.» Am. Psychol. 55 (1): 68–78. https://doi.org/10.1037/0003-066x.55.1.68.\nUn altro approccio per combattere questi problemi è concentrarsi sulla progettazione “human-centered” di sistemi interattivi che incorporano il controllo umano. Il design sensibile al valore (Friedman 1996) ha descritto tre fattori di progettazione chiave per un’interfaccia utente che hanno un impatto sull’autonomia, tra cui capacità del sistema, complessità, rappresentazione errata e fluidità. Un modello più recente, chiamato METUX (A Model for Motivation, Engagement, and Thriving in the User Experience), sfrutta le intuizioni della “Self-determination Theory (SDT)” in psicologia per identificare sei sfere distinte dell’esperienza tecnologica che contribuiscono ai sistemi di progettazione che promuovono il benessere e la prosperità umana (Peters, Calvo, e Ryan 2018). SDT definisce l’autonomia come agire in base ai propri obiettivi e valori, il che è distinto dall’uso dell’autonomia come semplice sinonimo di indipendenza o di controllo (Ryan e Deci 2000).\nCalvo et al. (2020) elabora METUX e le sue sei “sfere di esperienza tecnologica” nel contesto dei sistemi di raccomandazione AI. Propongono queste sfere (Adozione, Interfaccia, Attività, Comportamento, Vita e Società) come un modo per organizzare il pensiero e la valutazione della progettazione tecnologica al fine di catturare in modo appropriato gli impatti contraddittori e a valle sull’autonomia umana quando interagisce con i sistemi AI.\n\nCalvo, Rafael A, Dorian Peters, Karina Vold, e Richard M Ryan. 2020. «Supporting human autonomy in AI systems: A framework for ethical enquiry». Ethics of digital well-being: A multidisciplinary approach, 31–54.\n\n\n15.7.3 Impatti Economici su Posti di Lavoro, Competenze, Salari\nUna delle principali preoccupazioni dell’attuale ascesa delle tecnologie AI è la disoccupazione diffusa. Con l’espansione delle capacità dei sistemi AI, molti temono che queste tecnologie causeranno una perdita assoluta di posti di lavoro, poiché sostituiranno i lavoratori attuali e supereranno ruoli occupazionali alternativi in tutti i settori. Tuttavia, il cambiamento dei panorami economici per mano dell’automazione non è una novità e, storicamente, si è scoperto che riflette pattern di spostamento piuttosto che di sostituzione (Shneiderman 2022)—Capitolo 4. In particolare, l’automazione di solito riduce i costi e aumenta la qualità, aumentando notevolmente l’accesso e la domanda. La necessità di servire questi mercati in crescita spinge la produzione, creando nuovi posti di lavoro.\n\n———. 2022. Human-centered AI. Oxford University Press.\nInoltre, gli studi hanno scoperto che i tentativi di raggiungere un’automazione “lights-out”, ovvero un’automazione produttiva e flessibile con un numero minimo di lavoratori umani, non hanno avuto successo. I tentativi di farlo hanno portato a quella che la task force del MIT Work of the Future ha definito “automazione a somma zero”, in cui la flessibilità dei processi viene sacrificata per aumentare la produttività.\nAl contrario, la task force propone un approccio di “automazione a somma positiva” in cui la flessibilità viene aumentata progettando una tecnologia che incorpora strategicamente gli esseri umani dove sono molto necessari, rendendo più facile per i dipendenti della linea addestrare e correggere i robot, utilizzando un approccio bottom-up per identificare quali attività dovrebbero essere automatizzate; e scegliendo le giuste metriche per misurare il successo (vedi Work of the Future del MIT).\nTuttavia, l’ottimismo delle prospettive di alto livello non esclude danni individuali, specialmente per coloro le cui competenze e lavori saranno resi obsoleti dall’automazione. La pressione pubblica e legislativa, così come gli sforzi di responsabilità sociale delle aziende, dovranno essere diretti alla creazione di politiche che condividano i vantaggi dell’automazione con i lavoratori e si traducano in salari minimi e benefici più elevati.\n\n\n15.7.4 Comunicazione Scientifica e Alfabetizzazione IA\nUn sondaggio del 1993 sulle convinzioni di 3000 adulti nordamericani sulla “macchina pensante elettronica” ha rivelato due prospettive principali del primo computer: la prospettiva dello “strumento utile dell’uomo” e la prospettiva della “macchina pensante fantastica”. Gli atteggiamenti che contribuiscono alla visione della “macchina pensante fantastica” in questo e altri studi hanno rivelato una caratterizzazione dei computer come “cervelli intelligenti, più intelligenti delle persone, illimitati, veloci, misteriosi e spaventosi” (Martin 1993). Questi timori evidenziano una componente facilmente trascurata dell’IA responsabile, specialmente in mezzo alla corsa alla commercializzazione di tali tecnologie: la comunicazione scientifica che comunica accuratamente le capacità e le limitazioni di questi sistemi, fornendo al contempo trasparenza sui limiti della conoscenza degli esperti su questi sistemi.\n\nMartin, C. Dianne. 1993. «The myth of the awesome thinking machine». Commun. ACM 36 (4): 120–33. https://doi.org/10.1145/255950.153587.\n\nHandlin, Oscar. 1965. «Science and technology in popular culture». Daedalus-us., 156–70.\nMan mano che le capacità dei sistemi di IA si espandono oltre la comprensione della maggior parte delle persone, c’è una tendenza naturale a presumere i tipi di mondi apocalittici dipinti dai nostri media. Ciò è dovuto in parte all’apparente difficoltà di assimilare informazioni scientifiche, persino in culture tecnologicamente avanzate, che porta i prodotti della scienza a essere percepiti come magia, “comprensibili solo in termini di ciò che hanno fatto, non di come hanno funzionato” (Handlin 1965).\nMentre le aziende tecnologiche dovrebbero essere ritenute responsabili per aver limitato le affermazioni grandiose e non essere cadute in cicli di clamore, la ricerca che studia la comunicazione scientifica, in particolare per quanto riguarda l’intelligenza artificiale (generativa), sarà utile anche per tracciare e correggere la comprensione pubblica di queste tecnologie. Un’analisi del database accademico Scopus ha scoperto che tale ricerca è scarsa, con solo una manciata di articoli che menzionano sia “comunicazione scientifica” che “intelligenza artificiale” (Schäfer 2023).\n\nSchäfer, Mike S. 2023. «The Notorious GPT: Science communication in the age of artificial intelligence». Journal of Science Communication 22 (02): Y02. https://doi.org/10.22323/2.22020402.\n\nLindgren, Simon. 2023. Handbook of Critical Studies of Artificial Intelligence. Edward Elgar Publishing.\n\nNg, Davy Tsz Kit, Jac Ka Lok Leung, Kai Wah Samuel Chu, e Maggie Shen Qiao. 2021. «AI literacy: Definition, teaching, evaluation and ethical issues». Proceedings of the Association for Information Science and Technology 58 (1): 504–9.\nLa ricerca che espone le prospettive, i “frame” e le immagini del futuro promosse da istituzioni accademiche, aziende tecnologiche, stakeholder, enti regolatori, giornalisti, ONG e altri aiuterà anche a identificare potenziali lacune nell’alfabetizzazione AI tra gli adulti (Lindgren 2023). Una maggiore attenzione all’alfabetizzazione AI da parte di tutti gli stakeholder sarà importante per aiutare le persone le cui competenze sono rese obsolete dall’automazione AI (Ng et al. 2021).\n“Ma anche coloro che non acquisiscono mai quella comprensione hanno bisogno di rassicurazioni sul fatto che esista una connessione tra gli obiettivi della scienza e il loro benessere e, soprattutto, che lo scienziato non sia un uomo completamente a parte, ma uno che condivide parte del loro valore.” (Handlin, 1965)",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>IA Responsabile</span>"
    ]
  },
  {
    "objectID": "contents/core/responsible_ai/responsible_ai.it.html#conclusione",
    "href": "contents/core/responsible_ai/responsible_ai.it.html#conclusione",
    "title": "15  IA Responsabile",
    "section": "15.8 Conclusione",
    "text": "15.8 Conclusione\nUn’intelligenza artificiale responsabile è fondamentale poiché i sistemi di apprendimento automatico esercitano una crescente influenza nei settori sanitario, lavorativo, finanziario e della giustizia penale. Mentre l’intelligenza artificiale promette immensi benefici, i modelli progettati in modo sconsiderato rischiano di perpetrare danni attraverso pregiudizi, violazioni della privacy, comportamenti indesiderati e altre insidie.\nMantenere i principi di equità, spiegabilità, responsabilità, sicurezza e trasparenza consente lo sviluppo di un’intelligenza artificiale etica allineata ai valori umani. Tuttavia, l’implementazione di questi principi comporta il superamento di complesse sfide tecniche e sociali relative al rilevamento di pregiudizi nei set di dati, alla scelta di appropriati compromessi nei modelli, alla protezione di dati di training di qualità e altro ancora. Framework come la progettazione sensibile al valore guidano il bilanciamento dell’accuratezza rispetto ad altri obiettivi in base alle esigenze delle parti interessate.\nGuardando al futuro, il progresso dell’intelligenza artificiale responsabile richiede una ricerca continua e l’impegno del settore. Sono necessari benchmark più standardizzati per confrontare pregiudizi e robustezza dei modelli. Man mano che il TinyML personalizzato si espande, abilitare una trasparenza efficiente e il controllo dell’utente per i dispositivi edge giustifica l’attenzione. Le strutture e le politiche di incentivazione riviste devono incoraggiare uno sviluppo deliberato ed etico prima di un’implementazione sconsiderata. L’istruzione sulla cultura dell’intelligenza artificiale e sui suoi limiti contribuirà ulteriormente alla comprensione pubblica.\nI metodi responsabili sottolineano che, mentre l’apprendimento automatico offre un potenziale immenso, un’applicazione sconsiderata rischia di avere conseguenze negative. La collaborazione interdisciplinare e la progettazione incentrata sull’uomo sono essenziali affinché l’intelligenza artificiale possa promuovere un ampio beneficio sociale. Il percorso da seguire non risiede in una checklist arbitraria, ma in un impegno costante per comprendere e sostenere la nostra responsabilità etica a ogni passo. Intraprendendo un’azione coscienziosa, la comunità dell’apprendimento automatico può guidare l’intelligenza artificiale verso l’emancipazione di tutte le persone in modo equo e sicuro.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>IA Responsabile</span>"
    ]
  },
  {
    "objectID": "contents/core/responsible_ai/responsible_ai.it.html#sec-responsible-ai-resource",
    "href": "contents/core/responsible_ai/responsible_ai.it.html#sec-responsible-ai-resource",
    "title": "15  IA Responsabile",
    "section": "15.9 Risorse",
    "text": "15.9 Risorse\nEcco un elenco curato di risorse per supportare studenti e insegnanti nei loro percorsi di apprendimento e insegnamento. Stiamo lavorando costantemente per espandere questa raccolta e presto aggiungeremo nuovi esercizi.\n\n\n\n\n\n\nSlide\n\n\n\n\n\nQueste slide sono uno strumento prezioso per gli insegnanti per tenere lezioni e per gli studenti per rivedere il materiale secondo il proprio ritmo. Incoraggiamo studenti e docenti a sfruttare queste slide per migliorare la loro comprensione e facilitare un trasferimento efficace delle conoscenze.\n\nWhat am I building? What is the goal?\nWho is the audience?\nWhat are the consequences?\nResponsible Data Collection.\n\n\n\n\n\n\n\n\n\n\nVideo\n\n\n\n\n\n\nVideo 15.1\n\n\n\n\n\n\n\n\n\n\nEsercizi\n\n\n\n\n\nPer rafforzare i concetti trattati in questo capitolo, abbiamo curato una serie di esercizi che sfidano gli studenti ad applicare le proprie conoscenze e ad approfondire la propria comprensione.\n\nProssimamente.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>IA Responsabile</span>"
    ]
  },
  {
    "objectID": "contents/core/sustainable_ai/sustainable_ai.it.html",
    "href": "contents/core/sustainable_ai/sustainable_ai.it.html",
    "title": "16  IA Sostenibile",
    "section": "",
    "text": "16.1 Panoramica\nI rapidi progressi nell’intelligenza artificiale (IA) e nel machine learning (ML) [apprendimento automatico] hanno portato a molte applicazioni e ottimizzazioni utili per l’efficienza delle prestazioni. Tuttavia, la notevole crescita dell’IA ha un costo significativo ma spesso trascurato: il suo impatto ambientale. Il rapporto più recente pubblicato dall’IPCC, l’organismo internazionale che guida le valutazioni scientifiche del cambiamento climatico e dei suoi impatti, ha sottolineato l’importanza urgente di affrontare il cambiamento climatico. Senza sforzi immediati per ridurre le emissioni globali di \\(\\textrm{CO}_2\\) di almeno il 43 percento prima del 2030, supereremo il riscaldamento globale di 1,5 gradi Celsius (Winkler et al. 2022). Ciò potrebbe avviare cicli di feedback positivi, spingendo le temperature ancora più in alto. Accanto alle questioni ambientali, le Nazioni Unite hanno riconosciuto 17 Sustainable Development Goals (SDG) [Obiettivi di sviluppo sostenibile], in cui l’IA può svolgere un ruolo importante e, viceversa, possono svolgere un ruolo importante nello sviluppo di sistemi di IA. Poiché il campo continua a espandersi, considerare la sostenibilità è fondamentale.\nI sistemi di intelligenza artificiale, in particolare i grandi modelli linguistici come GPT-3 e i modelli di visione artificiale come DALL-E 2, richiedono enormi quantità di risorse computazionali per l’addestramento. Ad esempio, si stima che GPT-3 consumi 1.300 megawattora di elettricità, pari a 1.450 famiglie medie statunitensi in un mese intero (Maslej et al. 2023), o in altre parole, consuma abbastanza energia da rifornire una famiglia media statunitense per 120 anni! Questa immensa richiesta di energia deriva principalmente da data center affamati di energia con server che eseguono calcoli intensivi per addestrare queste complesse reti neurali per giorni o settimane.\nLe stime attuali indicano che le emissioni di carbonio prodotte dallo sviluppo di un singolo modello di intelligenza artificiale sofisticato possono eguagliare le emissioni nell’arco di vita di cinque veicoli standard a benzina (Strubell, Ganesh, e McCallum 2019). Una parte significativa dell’elettricità attualmente consumata dai data center è generata da fonti non rinnovabili come carbone e gas naturale, con il risultato che i data center contribuiscono a circa l’1% delle emissioni totali di carbonio a livello mondiale. Ciò è paragonabile alle emissioni dell’intero settore delle compagnie aeree. Questa immensa impronta di carbonio dimostra l’urgente necessità di passare a fonti di energia rinnovabili come l’energia solare ed eolica per gestire lo sviluppo dell’intelligenza artificiale.\nInoltre, anche i sistemi di intelligenza artificiale su piccola scala distribuiti su dispositivi edge come parte di TinyML hanno impatti ambientali che non dovrebbero essere ignorati (Prakash, Stewart, et al. 2023). L’hardware specializzato richiesto per l’intelligenza artificiale ha un impatto ambientale dovuto all’estrazione e alla produzione di risorse naturali. GPU, CPU e chip come le TPU dipendono da metalli delle terre rare la cui estrazione e lavorazione generano un notevole inquinamento. Anche la produzione di questi componenti ha le sue richieste energetiche. Inoltre, la raccolta, l’archiviazione e la preelaborazione dei dati utilizzati per addestrare modelli sia su piccola che su larga scala comportano costi ambientali, esacerbando ulteriormente le implicazioni di sostenibilità dei sistemi ML.\nPertanto, mentre l’intelligenza artificiale promette innovazioni in molti campi, per sostenere il progresso è necessario affrontare le sfide della sostenibilità. L’intelligenza artificiale può continuare a progredire in modo responsabile ottimizzando l’efficienza dei modelli, esplorando hardware specializzato alternativo e fonti di energia rinnovabile per i data center e monitorando il suo impatto ambientale complessivo.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>IA Sostenibile</span>"
    ]
  },
  {
    "objectID": "contents/core/sustainable_ai/sustainable_ai.it.html#panoramica",
    "href": "contents/core/sustainable_ai/sustainable_ai.it.html#panoramica",
    "title": "16  IA Sostenibile",
    "section": "",
    "text": "Winkler, Harald, Franck Lecocq, Hans Lofgren, Maria Virginia Vilariño, Sivan Kartha, e Joana Portugal-Pereira. 2022. «Examples of shifting development pathways: Lessons on how to enable broader, deeper, and faster climate action». Climate Action 1 (1). https://doi.org/10.1007/s44168-022-00026-1.\n\nMaslej, Nestor, Loredana Fattorini, Erik Brynjolfsson, John Etchemendy, Katrina Ligett, Terah Lyons, James Manyika, et al. 2023. «Artificial intelligence index report 2023». ArXiv preprint abs/2310.03715. https://arxiv.org/abs/2310.03715.\n\n\n\nPrakash, Shvetank, Matthew Stewart, Colby Banbury, Mark Mazumder, Pete Warden, Brian Plancher, e Vijay Janapa Reddi. 2023. «Is TinyML Sustainable? Assessing the Environmental Impacts of Machine Learning on Microcontrollers». ArXiv preprint. https://arxiv.org/abs/2301.11899.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>IA Sostenibile</span>"
    ]
  },
  {
    "objectID": "contents/core/sustainable_ai/sustainable_ai.it.html#social-and-ethical-responsibility",
    "href": "contents/core/sustainable_ai/sustainable_ai.it.html#social-and-ethical-responsibility",
    "title": "16  IA Sostenibile",
    "section": "16.2 Responsabilità Sociale ed Etica",
    "text": "16.2 Responsabilità Sociale ed Etica\nL’impatto ambientale dell’IA non è solo una questione tecnica, ma anche etica e sociale. Man mano che l’IA diventa sempre più integrata nelle nostre vite e nei nostri settori, la sua sostenibilità diventa sempre più critica.\n\n16.2.1 Considerazioni Etiche\nLa portata dell’impatto ambientale dell’IA solleva profonde questioni etiche sulle responsabilità degli sviluppatori e delle aziende di IA nel ridurre al minimo le emissioni di carbonio e l’uso di energia. In quanto creatori di sistemi e tecnologie di IA che possono avere impatti globali di vasta portata, gli sviluppatori hanno l’obbligo etico di integrare consapevolmente la tutela ambientale nel loro processo di progettazione, anche se la sostenibilità avviene a scapito di alcuni guadagni di efficienza.\nC’è una chiara e attuale necessità per noi di avere conversazioni aperte e oneste sui compromessi ambientali dell’IA all’inizio del ciclo di vita dello sviluppo. I ricercatori dovrebbero sentirsi autorizzati a esprimere preoccupazioni se le priorità organizzative non sono allineate con gli obiettivi etici, come nel caso della lettera aperta per sospendere i giganteschi esperimenti di IA.\nInoltre, c’è una crescente necessità per le aziende di IA di esaminare attentamente i loro contributi al cambiamento climatico e al danno ambientale. Le grandi aziende tecnologiche sono responsabili dell’infrastruttura cloud, delle richieste di energia dei data center e dell’estrazione delle risorse necessarie per alimentare l’IA odierna. La leadership dovrebbe valutare se i valori e le politiche organizzative promuovano la sostenibilità, dalla produzione di hardware alle pipeline di training dei modelli.\nInoltre, potrebbe essere necessaria più di un’autoregolamentazione volontaria: i governi potrebbero dover introdurre nuove normative volte a standard e pratiche di intelligenza artificiale sostenibili se speriamo di frenare l’esplosione energetica prevista di modelli sempre più grandi. Le metriche segnalate come l’utilizzo del computer, l’impronta di carbonio e i parametri di riferimento dell’efficienza potrebbero responsabilizzare le organizzazioni.\nAttraverso principi etici, politiche aziendali e regole pubbliche, i tecnici e le aziende di intelligenza artificiale hanno un profondo dovere nei confronti del nostro pianeta per garantire l’avanzamento responsabile e sostenibile della tecnologia in grado di trasformare radicalmente la società moderna. Dobbiamo alle generazioni future di fare le cose per bene. Figura 16.1 delinea alcune preoccupazioni e sfide etiche che l’IA deve affrontare.\n\n\n\n\n\n\nFigura 16.1: Sfide etiche nello sviluppo dell’IA. Fonte: COE\n\n\n\n\n\n16.2.2 Sostenibilità a Lungo Termine\nLa massiccia espansione prevista dell’IA solleva urgenti preoccupazioni sulla sua sostenibilità a lungo termine. Poiché il software e le applicazioni di IA aumentano rapidamente in complessità e utilizzo in tutti i settori, la domanda di potenza di calcolo e infrastrutture salirà alle stelle in modo esponenziale nei prossimi anni.\nPer mettere in prospettiva la portata della crescita prevista, la capacità di calcolo totale richiesta per l’addestramento dei modelli di IA ha visto un sorprendente aumento di 350.000 volte dal 2012 al 2019 (R. Schwartz et al. 2020). I ricercatori prevedono una crescita di oltre un ordine di grandezza ogni anno, man mano che vengono sviluppati assistenti IA personalizzati, tecnologia autonoma, strumenti di medicina di precisione e altro ancora. Le tendenze sono simili per i sistemi ML embedded, con una stima di 2,5 miliardi di dispositivi edge abilitati all’IA distribuiti entro il 2030.\nLa gestione di questo livello di espansione richiede innovazioni incentrate su software e hardware in termini di efficienza e integrazione rinnovabile da parte di ingegneri e scienziati dell’IA. Dal lato software, nuove tecniche di ottimizzazione dei modelli, distillazione, potatura, numeri a bassa precisione, condivisione delle conoscenze tra sistemi e altre aree devono diventare best practice diffuse per frenare le esigenze energetiche. Ad esempio, realizzare anche una domanda di elaborazione ridotta del 50% per raddoppio della capacità avrebbe un impatto enorme sull’energia totale.\nDal lato dell’infrastruttura hardware, a causa dei crescenti costi di trasferimento dati, archiviazione, raffreddamento e spazio, continuare con l’attuale modello di server farm centralizzato nei data center è probabilmente irrealizzabile a lungo termine (Lannelongue, Grealey, e Inouye 2021). Esplorare opzioni di elaborazione decentralizzate alternative attorno a “edge AI” su dispositivi locali o all’interno di reti di telecomunicazioni può alleviare le pressioni di ridimensionamento sui data center iper-scalabili ad alto consumo energetico. Allo stesso modo, il passaggio a fonti di energia rinnovabile ibride e a zero emissioni di carbonio che alimentano i principali data center dei provider cloud in tutto il mondo sarà essenziale.\n\nLannelongue, Loı̈c, Jason Grealey, e Michael Inouye. 2021. «Green Algorithms: Quantifying the Carbon Footprint of Computation». Adv. Sci. 8 (12): 2100707. https://doi.org/10.1002/advs.202100707.\n\n\n16.2.3 IA per il Bene Ambientale\nSebbene molta attenzione sia rivolta alle sfide di sostenibilità dell’IA, queste potenti tecnologie forniscono soluzioni uniche per combattere il cambiamento climatico e guidare il progresso ambientale. Ad esempio, l’apprendimento automatico può ottimizzare continuamente le reti elettriche intelligenti per migliorare l’integrazione delle energie rinnovabili e l’efficienza della distribuzione dell’elettricità attraverso le reti (Zhang, Han, e Deng 2018). I modelli possono acquisire lo stato in tempo reale di una rete elettrica e le previsioni meteorologiche per allocare e spostare le fonti rispondendo alla domanda e all’offerta.\n\nZhang, Dongxia, Xiaoqing Han, e Chunyu Deng. 2018. «Review on the research and practice of deep learning and reinforcement learning in smart grids». CSEE Journal of Power and Energy Systems 4 (3): 362–70. https://doi.org/10.17775/cseejpes.2018.00520.\n\nLam, Remi, Alvaro Sanchez-Gonzalez, Matthew Willson, Peter Wirnsberger, Meire Fortunato, Ferran Alet, Suman Ravuri, et al. 2023. «Learning skillful medium-range global weather forecasting». Science 382 (6677): 1416–21. https://doi.org/10.1126/science.adi2336.\n\nKurth, Thorsten, Shashank Subramanian, Peter Harrington, Jaideep Pathak, Morteza Mardani, David Hall, Andrea Miele, Karthik Kashinath, e Anima Anandkumar. 2023. «FourCastNet: Accelerating Global High-Resolution Weather Forecasting Using Adaptive Fourier Neural Operators». In Proceedings of the Platform for Advanced Scientific Computing Conference, 1–11. ACM. https://doi.org/10.1145/3592979.3593412.\nLe reti neurali ottimizzate si sono anche dimostrate notevolmente efficaci nelle previsioni meteorologiche di prossima generazione (Lam et al. 2023) e nella modellazione climatica (Kurth et al. 2023). Possono analizzare rapidamente enormi volumi di dati climatici per potenziare la preparazione agli eventi estremi e la pianificazione delle risorse per uragani, inondazioni, siccità e altro ancora. I ricercatori del clima hanno raggiunto un’accuratezza all’avanguardia del percorso delle tempeste combinando simulazioni di IA con modelli numerici tradizionali.\nL’intelligenza artificiale consente inoltre un migliore monitoraggio della biodiversità (Silvestro et al. 2022), della fauna selvatica (D. Schwartz et al. 2021), degli ecosistemi e della deforestazione illegale tramite droni e satelliti. Gli algoritmi di visione artificiale possono automatizzare le stime della popolazione delle specie e le valutazioni della salute dell’habitat su vaste regioni non monitorate. Queste capacità forniscono agli ambientalisti potenti strumenti per combattere il bracconaggio (Bondi et al. 2018), ridurre i rischi di estinzione delle specie e comprendere i cambiamenti ecologici.\n\nSilvestro, Daniele, Stefano Goria, Thomas Sterner, e Alexandre Antonelli. 2022. «Improving biodiversity protection through artificial intelligence». Nature Sustainability 5 (5): 415–24. https://doi.org/10.1038/s41893-022-00851-6.\n\nSchwartz, Daniel, Jonathan Michael Gomes Selman, Peter Wrege, e Andreas Paepcke. 2021. «Deployment of Embedded Edge-AI for Wildlife Monitoring in Remote Regions». In 2021 20th IEEE International Conference on Machine Learning and Applications (ICMLA), 1035–42. IEEE; IEEE. https://doi.org/10.1109/icmla52953.2021.00170.\n\nBondi, Elizabeth, Ashish Kapoor, Debadeepta Dey, James Piavis, Shital Shah, Robert Hannaford, Arvind Iyer, Lucas Joppa, e Milind Tambe. 2018. «Near Real-Time Detection of Poachers from Drones in AirSim». In Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, a cura di Jérôme Lang, 5814–16. International Joint Conferences on Artificial Intelligence Organization. https://doi.org/10.24963/ijcai.2018/847.\nInvestimenti mirati in applicazioni di intelligenza artificiale per la sostenibilità ambientale, la condivisione di dati intersettoriali e l’accessibilità dei modelli possono accelerare notevolmente le soluzioni a urgenti problemi ecologici. L’enfasi sull’intelligenza artificiale per il bene sociale indirizza l’innovazione in direzioni più pulite, guidando queste tecnologie che modellano il mondo verso uno sviluppo etico e responsabile.\n\n\n16.2.4 Caso di Studio: L’IA di DeepMind per l’Efficienza Energetica Basata sull’IA\nI data center di Google sono fondamentali per alimentare prodotti come Search, Gmail e YouTube, utilizzati quotidianamente da miliardi di persone. Tuttavia, mantenere attive e funzionanti le vaste server farm richiede molta energia, in particolare per i sistemi di raffreddamento essenziali. Google si impegna costantemente per migliorare l’efficienza in tutte le operazioni. Tuttavia, i progressi si stavano rivelando difficili solo con i metodi tradizionali, considerando le complesse dinamiche personalizzate coinvolte. Questa sfida ha spinto una svolta nell’apprendimento automatico, producendo potenziali risparmi.\nDopo oltre un decennio di ottimizzazione della progettazione dei data center, invenzione di hardware di elaborazione a basso consumo energetico e protezione di fonti di energia rinnovabili, Google ha portato gli scienziati di DeepMind a sbloccare ulteriori progressi. Gli esperti di intelligenza artificiale hanno affrontato fattori complessi che circondano il funzionamento degli apparati di raffreddamento industriali. Apparecchiature come pompe e refrigeratori interagiscono in modo non lineare, mentre cambiano anche le condizioni meteorologiche esterne e le variabili architettoniche interne. Catturare questa complessità ha confuso le rigide formule ingegneristiche e l’intuizione umana.\nIl team DeepMind ha sfruttato i dati storici estesi dei sensori di Google che descrivono temperature, consumi energetici e altri attributi come input di training. Hanno creato un sistema flessibile basato su reti neurali per modellare le relazioni e prevedere configurazioni ottimali, riducendo al minimo la “power usage effectiveness (PUE)” [efficacia dell’utilizzo di energia] (Barroso, Hölzle, e Ranganathan 2019); PUE è la misura standard per valutare l’efficienza con cui un data center utilizza l’energia, che fornisce la percentuale di energia totale consumata dalla struttura divisa per l’energia utilizzata direttamente per le operazioni di elaborazione. Quando testato in tempo reale, il sistema AI ha prodotto notevoli guadagni rispetto alle innovazioni precedenti, riducendo l’energia di raffreddamento del 40% per un calo del 15% nel PUE totale, un nuovo record del sito. Il framework generalizzabile ha appreso rapidamente le dinamiche di raffreddamento in condizioni mutevoli che le regole statiche non potevano eguagliare. Questa svolta evidenzia il ruolo crescente dell’AI nella trasformazione della tecnologia moderna e nell’abilitazione di un futuro sostenibile.\n\nBarroso, Luiz André, Urs Hölzle, e Parthasarathy Ranganathan. 2019. The Datacenter as a Computer: Designing Warehouse-Scale Machines. Springer International Publishing. https://doi.org/10.1007/978-3-031-01761-2.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>IA Sostenibile</span>"
    ]
  },
  {
    "objectID": "contents/core/sustainable_ai/sustainable_ai.it.html#energy-consumption",
    "href": "contents/core/sustainable_ai/sustainable_ai.it.html#energy-consumption",
    "title": "16  IA Sostenibile",
    "section": "16.3 Consumo Energetico",
    "text": "16.3 Consumo Energetico\n\n16.3.1 Comprendere le Esigenze Energetiche\nComprendere le esigenze energetiche per il training e il funzionamento dei modelli di intelligenza artificiale è fondamentale nel campo in rapida evoluzione dell’intelligenza artificiale. Con l’intelligenza artificiale che sta entrando in uso diffuso in molti nuovi campi (Bohr e Memarzadeh 2020; Sudhakar, Sze, e Karaman 2023), si prevede che la domanda di dispositivi e data center abilitati all’intelligenza artificiale esploderà. Questa comprensione ci aiuta a capire perché l’intelligenza artificiale, in particolare il deep learning, è spesso etichettata come ad alta intensità energetica.\n\nBohr, Adam, e Kaveh Memarzadeh. 2020. «The rise of artificial intelligence in healthcare applications». In Artificial Intelligence in Healthcare, 25–60. Elsevier. https://doi.org/10.1016/b978-0-12-818438-7.00002-2.\n\nRequisiti Energetici per il Training dell’Intelligenza Artificiale\nIl training di sistemi di intelligenza artificiale complessi come i grandi modelli di deep learning può richiedere livelli sorprendentemente elevati di potenza di calcolo, con profonde implicazioni energetiche. Consideriamo il modello linguistico all’avanguardia di OpenAI GPT-3 come un esempio lampante. Questo sistema spinge i confini della generazione di testo attraverso algoritmi formati su enormi set di dati. Tuttavia, l’energia consumata da GPT-3 per un singolo ciclo di addestramento potrebbe rivaleggiare con l’utilizzo mensile di un’intera cittadina. Negli ultimi anni, questi modelli di intelligenza artificiale generativa hanno guadagnato sempre più popolarità, portando a un numero maggiore di modelli addestrati. Oltre all’aumento del numero di modelli, aumenterà anche il numero di parametri in questi modelli. La ricerca mostra che l’aumento delle dimensioni del modello (numero di parametri), delle dimensioni del set di dati e del calcolo utilizzato per l’addestramento migliora le prestazioni in modo fluido senza segni di saturazione (Kaplan et al. 2020). Notare come, in Figura 16.2, il “test loss” diminuisce man mano che ciascuno dei 3 aumenta.\n\n\n\n\n\n\nFigura 16.2: Le prestazioni migliorano con il calcolo, il set di dati e le dimensioni del modello. Fonte: Kaplan et al. (2020).\n\n\nKaplan, Jared, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, e Dario Amodei. 2020. «Scaling Laws for Neural Language Models». ArXiv preprint abs/2001.08361. https://arxiv.org/abs/2001.08361.\n\n\nCosa determina requisiti così immensi? Durante l’addestramento, modelli come GPT-3 apprendono le proprie capacità elaborando continuamente enormi volumi di dati per regolare i parametri interni. La capacità di elaborazione che consente i rapidi progressi dell’IA contribuisce anche all’aumento del consumo di energia, soprattutto quando i set di dati e i modelli aumentano a dismisura. GPT-3 evidenzia una traiettoria costante nel campo in cui ogni balzo nella sofisticazione dell’IA risale a una potenza di calcolo e risorse sempre più sostanziali. Il suo predecessore, GPT-2, richiedeva un addestramento 10 volte inferiore per calcolare solo 1,5 miliardi di parametri, una differenza ora ridotta da grandezze in quanto GPT-3 comprende 175 miliardi di parametri. Mantenere questa traiettoria verso un’intelligenza artificiale sempre più capace solleva sfide future in termini di fornitura di energia e infrastrutture.\n\n\nUso Operativo dell’Energia\nLo sviluppo e l’addestramento di modelli di intelligenza artificiale richiedono un’enorme quantità di dati, potenza di calcolo ed energia. Tuttavia, l’implementazione e il funzionamento di tali modelli comportano anche significativi costi ricorrenti di risorse nel tempo. I sistemi di intelligenza artificiale sono ora integrati in vari settori e applicazioni e stanno entrando nella vita quotidiana di una fascia demografica in crescita. Il loro impatto cumulativo sull’energia operativa e sulle infrastrutture potrebbe eclissare l’addestramento iniziale del modello.\nQuesto concetto si riflette nella domanda di hardware di addestramento e inferenza nei data center e nell’edge. L’inferenza si riferisce all’uso di un modello addestrato per fare previsioni o decisioni su dati del mondo reale. Secondo una recente analisi McKinsey, la necessità di sistemi avanzati per addestrare modelli sempre più grandi sta crescendo rapidamente.\nTuttavia, i calcoli di inferenza costituiscono già una parte dominante e crescente dei carichi di lavoro totali dell’intelligenza artificiale, come mostrato in Figura 16.3. L’esecuzione di inferenze in tempo reale con modelli addestrati, sia per la classificazione delle immagini, il riconoscimento vocale o l’analisi predittiva, richiede invariabilmente hardware di elaborazione come server e chip. Tuttavia, persino un modello che gestisce migliaia di richieste di riconoscimento facciale o query in linguaggio naturale ogni giorno è messo in ombra da piattaforme enormi come Meta. Dove l’inferenza su milioni di foto e video condivisi sui social media, i requisiti energetici dell’infrastruttura continuano a crescere.\n\n\n\n\n\n\nFigura 16.3: Dimensioni del mercato per l’hardware di inferenza e training. Fonte: McKinsey.\n\n\n\nGli algoritmi che alimentano assistenti intelligenti abilitati all’intelligenza artificiale, magazzini automatizzati, veicoli a guida autonoma, assistenza sanitaria personalizzata e altro hanno un’impronta energetica individuale marginale. Tuttavia, la proliferazione prevista di queste tecnologie potrebbe aggiungere centinaia di milioni di endpoint che eseguono algoritmi di intelligenza artificiale ininterrottamente, causando un aumento della scala dei loro requisiti energetici collettivi. Gli attuali guadagni di efficienza hanno bisogno di aiuto per controbilanciare questa crescita netta.\nSi prevede che l’intelligenza artificiale registrerà un tasso di crescita annuale del 37,3% tra il 2023 e il 2030. Tuttavia, applicando lo stesso tasso di crescita al computing operativo, entro il 2030 il fabbisogno energetico annuale dell’IA potrebbe moltiplicarsi fino a 1.000 volte. Quindi, mentre l’ottimizzazione del modello affronta un aspetto, l’innovazione responsabile deve anche considerare i costi totali del ciclo di vita su scale di distribuzione globali che erano inimmaginabili solo anni fa, ma che ora pongono sfide infrastrutturali e di sostenibilità.\n\n\n\n16.3.2 Data Center e il Loro Impatto\nCon l’aumento della domanda di servizi di IA, l’impatto dei data center sul consumo energetico dei sistemi di IA sta diventando sempre più importante. Sebbene queste strutture siano fondamentali per il progresso e la distribuzione dell’IA, contribuiscono in modo significativo al suo impatto energetico.\n\nScala\nI data center sono i cavalli da tiro essenziali che consentono le recenti richieste di elaborazione dei sistemi di IA avanzati. Ad esempio, i principali provider come Meta gestiscono enormi data center che si estendono fino alle dimensioni di più campi da calcio, ospitando centinaia di migliaia di server ad alta capacità ottimizzati per l’elaborazione parallela e la produttività dei dati.\nQueste enormi strutture forniscono l’infrastruttura per addestrare reti neurali complesse su vasti set di dati. Ad esempio, sulla base di informazioni trapelate, il modello linguistico GPT-4 di OpenAI è stato addestrato su data center Azure con oltre 25.000 GPU Nvidia A100, utilizzate ininterrottamente per oltre 90-100 giorni.\nInoltre, l’inferenza in tempo reale per applicazioni AI consumer su larga scala è resa possibile solo sfruttando le server farm all’interno dei data center. Servizi come Alexa, Siri e Google Assistant elaborano miliardi di richieste vocali al mese da parte di utenti in tutto il mondo, basandosi sul data center computing per una risposta a bassa latenza. In futuro, l’espansione di casi d’uso all’avanguardia come veicoli a guida autonoma, diagnosi di medicina di precisione e modelli di previsione climatica accurati richiederà risorse di calcolo significative da ottenere attingendo a vaste risorse di cloud computing on-demand dai data center. Alcune applicazioni emergenti, come le auto autonome, hanno rigidi vincoli di latenza e larghezza di banda. Sarà necessario collocare la potenza di calcolo a livello di data center sull’edge anziché sul cloud.\nI prototipi di ricerca del MIT hanno mostrato camion e auto con hardware di bordo che eseguono l’elaborazione AI in tempo reale dei dati dei sensori equivalenti a piccoli data center (Sudhakar, Sze, e Karaman 2023). Questi innovativi “data center su ruote” dimostrano come veicoli come i camion a guida autonoma potrebbero aver bisogno di un calcolo su scala di data center embedded a bordo per ottenere una latenza di sistema di millisecondi per la navigazione, sebbene probabilmente ancora integrato dalla connettività wireless 5G a data center cloud più potenti.\n\nSudhakar, Soumya, Vivienne Sze, e Sertac Karaman. 2023. «Data Centers on Wheels: Emissions From Computing Onboard Autonomous Vehicles». IEEE Micro 43 (1): 29–39. https://doi.org/10.1109/mm.2022.3219803.\nLa larghezza di banda, lo storage e le capacità di elaborazione richieste per abilitare questa futura tecnologia su larga scala dipenderanno in larga misura dai progressi nell’infrastruttura dei data center e dalle innovazioni algoritmiche dell’intelligenza artificiale.\n\n\nDomanda di Energia\nLa domanda di energia dei data center può essere approssimativamente suddivisa in 4 componenti: infrastruttura, rete, storage e server. In Figura 16.4, vediamo che l’infrastruttura dati (che include raffreddamento, illuminazione e controlli) e i server utilizzano la maggior parte del budget energetico totale dei data center negli Stati Uniti (Shehabi et al. 2016). Questa sezione suddivide la domanda di energia per i server e l’infrastruttura. Per quest’ultima, l’attenzione è rivolta ai sistemi di raffreddamento, poiché il raffreddamento è il fattore dominante nel consumo energetico nell’infrastruttura.\n\nShehabi, Arman, Sarah Smith, Dale Sartor, Richard Brown, Magnus Herrlin, Jonathan Koomey, Eric Masanet, Nathaniel Horner, Inês Azevedo, e William Lintner. 2016. «United states data center energy usage report».\n\n\n\n\n\n\nFigura 16.4: Consumo energetico dei data center negli Stati Uniti. Fonte: International Energy Agency (IEA).\n\n\n\n\nServer\nL’aumento del consumo energetico dei data center deriva principalmente dalla crescita esponenziale dei requisiti di elaborazione AI. Le macchine NVIDIA DGX H100 ottimizzate per il deep learning possono assorbire fino a 10,2 kW al picco. I principali provider gestiscono data center con centinaia o migliaia di questi nodi DGX ad alto consumo energetico collegati in rete per addestrare i più recenti modelli AI. Ad esempio, il supercomputer sviluppato per OpenAI è un singolo sistema con oltre 285.000 core CPU, 10.000 GPU e 400 gigabit al secondo di connettività di rete per ogni server GPU.\nI calcoli intensivi necessari per l’intera flotta densamente popolata di una struttura e l’hardware di supporto comportano che i data center assorbano decine di megawatt 24 ore su 24. Nel complesso, gli algoritmi AI avanzati continuano ad aumentare il consumo energetico dei data center man mano che vengono distribuiti più nodi DGX per tenere il passo con la crescita prevista della domanda di risorse di elaborazione AI nei prossimi anni.\n\n\nSistemi di Raffreddamento\nPer mantenere i server robusti alimentati al massimo della capacità e freschi, i data center richiedono una capacità di raffreddamento enorme per contrastare il calore prodotto da server densamente stipati, apparecchiature di rete e altro hardware che eseguono carichi di lavoro intensivi di elaborazione senza sosta. Con grandi data center che contengono migliaia di rack di server che operano a pieno regime, sono necessarie torri di raffreddamento e refrigeratori su scala industriale, che utilizzano energia pari al 30-40% dell’impronta elettrica totale del data center (Dayarathna, Wen, e Fan 2016). Di conseguenza, le aziende sono alla ricerca di metodi di raffreddamento alternativi. Ad esempio, il data center di Microsoft in Irlanda sfrutta un fiordo vicino per scambiare calore utilizzando oltre mezzo milione di galloni [1.9 milioni di litri] di acqua di mare al giorno.\nRiconoscendo l’importanza del raffreddamento efficiente dal punto di vista energetico, sono state introdotte innovazioni volte a ridurre questa domanda di energia. Tecniche come il raffreddamento gratuito, che utilizza fonti di aria o acqua esterne quando le condizioni sono favorevoli, e l’uso dell’intelligenza artificiale per ottimizzare i sistemi di raffreddamento sono esempi di come il settore si adatta. Queste innovazioni riducono il consumo energetico, i costi operativi e diminuiscono l’impatto ambientale. Tuttavia, gli aumenti esponenziali della complessità del modello AI continuano a richiedere più server e hardware di accelerazione che operano a un utilizzo più elevato, il che si traduce in una maggiore generazione di calore e in un’energia sempre maggiore utilizzata esclusivamente per il raffreddamento.\n\n\n\nL’impatto Ambientale\nL’impatto ambientale dei data center non è causato solo dal consumo energetico diretto del data center stesso (Siddik, Shehabi, e Marston 2021). Il funzionamento del data center comporta la fornitura di acqua trattata al data center e lo scarico delle acque reflue dal data center. Gli impianti idrici e di trattamento delle acque reflue sono i principali consumatori di elettricità.\n\nSiddik, Md Abu Bakar, Arman Shehabi, e Landon Marston. 2021. «The environmental footprint of data centers in the United States». Environ. Res. Lett. 16 (6): 064017. https://doi.org/10.1088/1748-9326/abfba1.\n\nDavis, Jacqueline, Daniel Bizo, Andy Lawrence, Owen Rogers, e Max Smolaks. 2022. «Uptime Institute Global Data Center Survey 2022». Uptime Institute.\nOltre al consumo di elettricità, ci sono molti altri aspetti dell’impatto ambientale di questi data center. Il consumo di acqua dei data center può portare a problemi di scarsità idrica, maggiori esigenze di trattamento delle acque e adeguate infrastrutture di scarico delle acque reflue. Inoltre, le materie prime necessarie per la costruzione e la trasmissione di rete hanno un impatto considerevole sull’ambiente e i componenti nei data center devono essere aggiornati e sottoposti a manutenzione. Laddove quasi il 50 percento dei server è stato aggiornato entro 3 anni di utilizzo, i cicli di aggiornamento hanno dimostrato di rallentare (Davis et al. 2022). Tuttavia, ciò genera notevoli rifiuti elettronici, che possono essere difficili da riciclare.\n\n\n\n16.3.3 Ottimizzazione Energetica\nIn definitiva, misurare e comprendere il consumo energetico dell’IA facilita l’ottimizzazione del consumo energetico.\nUn modo per ridurre il consumo energetico di una data quantità di lavoro computazionale è eseguirlo su hardware più efficiente dal punto di vista energetico. Ad esempio, i chip TPU possono essere più efficienti dal punto di vista energetico rispetto alle CPU quando si tratta di eseguire grandi calcoli tensoriali per l’IA, poiché le TPU possono eseguire tali calcoli molto più velocemente senza consumare molta più energia delle CPU. Un altro modo è quello di creare sistemi software consapevoli del consumo energetico e delle caratteristiche dell’applicazione. Buoni esempi sono lavori di sistema come Zeus (You, Chung, e Chowdhury 2023) e Perseus (Chung et al. 2023), entrambi caratterizzati dal compromesso tra tempo di calcolo e consumo energetico a vari livelli di un sistema di addestramento ML per ottenere una riduzione energetica senza rallentamento end-to-end. In realtà, costruire sia hardware che software a basso consumo energetico e combinarne i vantaggi dovrebbe essere promettente, insieme a framework open source (ad esempio, Zeus) che facilitano gli sforzi della comunità.\n\nYou, Jie, Jae-Won Chung, e Mosharaf Chowdhury. 2023. «Zeus: Understanding and Optimizing GPU Energy Consumption of DNN Training». In 20th USENIX Symposium on Networked Systems Design and Implementation (NSDI 23), 119–39. Boston, MA: USENIX Association. https://www.usenix.org/conference/nsdi23/presentation/you.\n\nChung, Jae-Won, Yile Gu, Insu Jang, Luoxi Meng, Nikhil Bansal, e Mosharaf Chowdhury. 2023. «Perseus: Removing Energy Bloat from Large Model Training». ArXiv preprint abs/2312.06902. https://arxiv.org/abs/2312.06902.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>IA Sostenibile</span>"
    ]
  },
  {
    "objectID": "contents/core/sustainable_ai/sustainable_ai.it.html#carbon-footprint",
    "href": "contents/core/sustainable_ai/sustainable_ai.it.html#carbon-footprint",
    "title": "16  IA Sostenibile",
    "section": "16.4 Impronta di Carbonio",
    "text": "16.4 Impronta di Carbonio\nI data center consumano enormi quantità di elettricità e, senza l’accesso a fonti di energia rinnovabile, questa richiesta può avere un impatto ambientale notevole. Molte strutture dipendono fortemente da fonti di energia non rinnovabili come carbone e gas naturale. Ad esempio, si stima che i data center producano fino al 2% delle emissioni globali totali di \\(\\textrm{CO}_2\\), il che sta colmando il divario con il settore aereo. Come accennato nelle sezioni precedenti, le richieste di elaborazione dell’intelligenza artificiale sono destinate ad aumentare. Le emissioni di questa ondata sono triplici. In primo luogo, si prevede che i data center aumenteranno di dimensioni (Liu et al. 2020). In secondo luogo, le emissioni durante il training sono destinate ad aumentare in modo significativo (Patterson et al. 2022). In terzo luogo, le chiamate di inferenza a questi modelli sono destinate ad aumentare drasticamente.\n\nLiu, Yanan, Xiaoxia Wei, Jinyu Xiao, Zhijie Liu, Yang Xu, e Yun Tian. 2020. «Energy consumption and emission mitigation prediction based on data center traffic and PUE for global data centers». Global Energy Interconnection 3 (3): 272–82. https://doi.org/10.1016/j.gloei.2020.07.008.\nSenza azioni, questa crescita esponenziale della domanda rischia di aumentare ulteriormente l’impronta di carbonio dei data center a livelli insostenibili. I principali fornitori hanno promesso la neutralità carbonica e impegnato fondi per garantire energia pulita, ma i progressi rimangono incrementali rispetto ai piani di espansione complessivi del settore. Politiche di decarbonizzazione della rete più radicali e investimenti in energia rinnovabile potrebbero rivelarsi essenziali per contrastare l’impatto climatico dell’ondata imminente di nuovi data center volti a supportare la prossima generazione di IA.\n\n16.4.1 Definizione e Significato\nIl concetto di “impronta di carbonio” è emerso come una metrica chiave. Questo termine si riferisce alla quantità totale di gas serra, in particolare anidride carbonica, emessi direttamente o indirettamente da un individuo, un’organizzazione, un evento o un prodotto. Queste emissioni contribuiscono in modo significativo all’effetto serra, accelerando il riscaldamento globale e il cambiamento climatico. L’impronta di carbonio è misurata in termini di equivalenti di anidride carbonica (\\(\\textrm{CO}_2\\)e), consentendo un resoconto completo che include vari gas serra e il loro relativo impatto ambientale. Esempi di ciò applicato ad attività di ML su larga scala sono mostrati in Figura 16.5.\n\n\n\n\n\n\nFigura 16.5: Impronta di carbonio delle attività di ML su larga scala. Fonte: Wu et al. (2022).\n\n\n\nConsiderare l’impronta di carbonio è particolarmente importante nel rapido progresso dell’IA e nella sua integrazione in vari settori, mettendone in evidenza l’impatto ambientale. I sistemi di IA, in particolare quelli che comportano calcoli intensivi come il deep learning e l’elaborazione di dati su larga scala, sono noti per le loro notevoli richieste di energia. Questa energia, spesso ricavata dalle reti elettriche, potrebbe ancora basarsi prevalentemente sui combustibili fossili, il che comporta significative emissioni di gas serra.\nPrendiamo ad esempio l’addestramento di grandi modelli di IA come GPT-3 o complesse reti neurali. Questi processi richiedono un’immensa potenza di calcolo, in genere fornita dai data center. Il consumo energetico associato al funzionamento di questi centri, in particolare per attività ad alta intensità, comporta notevoli emissioni di gas serra. Gli studi hanno evidenziato che l’addestramento di un singolo modello di intelligenza artificiale può generare emissioni di carbonio paragonabili a quelle delle emissioni di più auto nel corso della loro vita, facendo luce sul costo ambientale dello sviluppo di tecnologie di intelligenza artificiale avanzate (Dayarathna, Wen, e Fan 2016). Figura 16.6 mostra un confronto tra le impronte di carbonio più basse e più alte, a partire da un volo di andata e ritorno tra New York e San Francisco, la vita media umana all’anno, la vita media americana all’anno, un’auto statunitense incluso il carburante nel corso della vita e un modello Transformer con ricerca di architettura neurale, che ha l’impronta più alta.\n\n\n\n\n\n\nFigura 16.6: Impronta di carbonio del modello NLP in libbre di \\(\\textrm{CO}_2\\) equivalente. Fonte: Dayarathna, Wen, e Fan (2016).\n\n\nDayarathna, Miyuru, Yonggang Wen, e Rui Fan. 2016. «Data Center Energy Consumption Modeling: A Survey». IEEE Communications Surveys &amp; Tutorials 18 (1): 732–94. https://doi.org/10.1109/comst.2015.2481183.\n\n\nInoltre, l’impronta di carbonio dell’IA si estende oltre la fase operativa. L’intero ciclo di vita dei sistemi di IA, inclusa la produzione di hardware di elaborazione, l’energia utilizzata nei data center per il raffreddamento e la manutenzione e lo smaltimento dei rifiuti elettronici, contribuisce alla loro impronta di carbonio complessiva. Abbiamo discusso alcuni di questi aspetti in precedenza e discuteremo degli aspetti relativi ai rifiuti più avanti in questo capitolo.\n\n\n16.4.2 La Necessità di Consapevolezza e Azione\nComprendere l’impronta di carbonio dei sistemi di IA è fondamentale per diversi motivi. In primo luogo, è un passo avanti verso la mitigazione degli impatti del cambiamento climatico. Man mano che l’intelligenza artificiale continua a crescere e a permeare diversi aspetti delle nostre vite, il suo contributo alle emissioni globali di carbonio diventa una preoccupazione significativa. La consapevolezza di queste emissioni può informare le decisioni prese da sviluppatori, aziende, decisori politici e persino ingegneri e scienziati ML come noi per garantire un equilibrio tra innovazione tecnologica e responsabilità ambientale.\nInoltre, questa comprensione stimola la spinta verso la ‘Green AI’ (R. Schwartz et al. 2020). Questo approccio si concentra sullo sviluppo di tecnologie di intelligenza artificiale efficienti, potenti e sostenibili dal punto di vista ambientale. Incoraggia l’esplorazione di algoritmi ad alta efficienza energetica, l’utilizzo di fonti di energia rinnovabili nei data center e l’adozione di pratiche che riducano l’impatto ambientale complessivo dell’intelligenza artificiale.\nIn sostanza, l’impronta di carbonio è una considerazione essenziale nello sviluppo e nell’applicazione delle tecnologie di intelligenza artificiale. Man mano che l’intelligenza artificiale si evolve e le sue applicazioni diventano più diffuse, la gestione della sua impronta di carbonio è fondamentale per garantire che questo progresso tecnologico sia in linea con gli obiettivi più ampi di sostenibilità ambientale.\n\n\n16.4.3 Stima dell’Impronta di Carbonio dell’IA\nStimare l’impronta di carbonio dei sistemi di IA è fondamentale per comprendere il loro impatto ambientale. Ciò comporta l’analisi dei vari elementi che contribuiscono alle emissioni durante il ciclo di vita delle tecnologie di intelligenza artificiale e l’impiego di metodologie specifiche per quantificare accuratamente tali emissioni. Sono stati proposti molti metodi diversi per quantificare le emissioni di carbonio dell’apprendimento automatico.\nL’impronta di carbonio dell’intelligenza artificiale comprende diversi elementi chiave, ognuno dei quali contribuisce all’impatto ambientale complessivo. Innanzitutto, l’energia viene consumata durante le fasi di addestramento e operative del modello di intelligenza artificiale. La fonte di questa energia influenza pesantemente le emissioni di carbonio. Una volta addestrati, questi modelli, a seconda della loro applicazione e scala, continuano a consumare elettricità durante il funzionamento. Oltre alle considerazioni energetiche, anche l’hardware utilizzato stressa l’ambiente.\nL’impronta di carbonio varia in modo significativo in base alle fonti di energia utilizzate. La composizione delle fonti che forniscono l’energia utilizzata nella rete varia ampiamente a seconda della regione geografica e persino del momento in un singolo giorno. Ad esempio, negli Stati Uniti, circa il 60 percento dell’approvvigionamento energetico totale è ancora coperto da combustibili fossili. Le fonti di energia nucleare e rinnovabili coprono il restante 40 percento. Queste frazioni non sono costanti durante il giorno. Poiché la produzione di energia rinnovabile solitamente si basa su fattori ambientali, come la radiazione solare e i campi di pressione, non forniscono una fonte di energia costante.\nLa variabilità della produzione di energia rinnovabile è stata una sfida continua nell’uso diffuso di queste fonti. Guardando Figura 16.7, che mostra i dati per la rete europea, vediamo che dovrebbe essere in grado di produrre la quantità di energia richiesta durante il giorno. Mentre l’energia solare raggiunge il picco a metà giornata, quella eolica ha due picchi distinti, al mattino e alla sera. Attualmente, facciamo affidamento su metodi di produzione di energia basati su combustibili fossili e carbone per supplire alla mancanza di energia nei periodi in cui le energie rinnovabili non soddisfano il fabbisogno.\nÈ necessaria l’innovazione nelle soluzioni di accumulo di energia per consentire un uso costante di fonti di energia rinnovabile. Il carico energetico di base è attualmente soddisfatto dall’energia nucleare. Questa fonte energetica costante non produce direttamente emissioni di carbonio, ma deve essere più rapida per adattarsi alla variabilità delle fonti energetiche rinnovabili. Le aziende tecnologiche come Microsoft hanno mostrato interesse per le fonti di energia nucleare per alimentare i loro data center. Poiché la domanda dei data center è più costante rispetto alla domanda delle normali famiglie, l’energia nucleare potrebbe essere utilizzata come fonte energetica dominante.\n\n\n\n\n\n\nFigura 16.7: Fonti di energia e capacità di generazione. Fonte: Energy Charts.\n\n\n\nInoltre, la produzione e lo smaltimento dell’hardware AI aumentano l’impronta di carbonio. La produzione di dispositivi informatici specializzati, come GPU e CPU, richiede molta energia e risorse. Questa fase spesso si basa su fonti energetiche che contribuiscono alle emissioni di gas serra. Il processo di produzione dell’industria elettronica è stato identificato come una delle otto grandi catene di fornitura responsabili di oltre il 50 percento delle emissioni globali (Challenge 2021). Inoltre, lo smaltimento a fine vita di questo hardware, che può portare a rifiuti elettronici, ha anche implicazioni ambientali. Come accennato, i server hanno un ciclo di aggiornamento di circa 3-5 anni. Di questi rifiuti elettronici, attualmente solo il 17,4 percento viene raccolto e riciclato correttamente. Le emissioni di carbonio di questi rifiuti elettronici hanno mostrato un aumento di oltre il 50 percento tra il 2014 e il 2020 (Singh e Ogunseitan 2022).\n\nChallenge, WEF Net-Zero. 2021. «The Supply Chain Opportunity». In World Economic Forum: Geneva, Switzerland.\n\nSingh, Narendra, e Oladele A. Ogunseitan. 2022. «Disentangling the worldwide web of e-waste and climate change co-benefits». Circular Economy 1 (2): 100011. https://doi.org/10.1016/j.cec.2022.100011.\nCome è chiaro da quanto sopra, è necessaria un’adeguata analisi del ciclo di vita per descrivere tutti gli aspetti rilevanti delle emissioni causate dall’IA. Un altro metodo è la contabilità del carbonio, che valuta la quantità di emissioni di anidride carbonica direttamente e indirettamente associate alle operazioni di IA. Questa misura utilizza in genere equivalenti di \\(\\textrm{CO}_2\\), consentendo un modo standardizzato di segnalare e valutare le emissioni.\n\n\n\n\n\n\nEsercizio 16.1: Impronta di Carbonio dell’IA\n\n\n\n\n\nSapevate che i modelli di IA all’avanguardia che potreste utilizzare hanno un impatto ambientale? Questo esercizio approfondirà l’“impronta di carbonio” di un sistema di IA. Imparerete come le richieste energetiche dei data center, il training dei grandi modelli di IA e persino la produzione di hardware contribuiscono alle emissioni di gas serra. Discuteremo perché è fondamentale essere consapevoli di questo impatto e impareremo metodi per stimare l’impronta di carbonio dei progetti di IA. Prepariamoci ad esplorare l’intersezione tra IA e sostenibilità ambientale!",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>IA Sostenibile</span>"
    ]
  },
  {
    "objectID": "contents/core/sustainable_ai/sustainable_ai.it.html#beyond-carbon-footprint",
    "href": "contents/core/sustainable_ai/sustainable_ai.it.html#beyond-carbon-footprint",
    "title": "16  IA Sostenibile",
    "section": "16.5 Oltre l’Impronta di Carbonio",
    "text": "16.5 Oltre l’Impronta di Carbonio\nL’attuale attenzione alla riduzione delle emissioni di carbonio e del consumo energetico dei sistemi di intelligenza artificiale affronta un aspetto cruciale della sostenibilità. Tuttavia, la produzione di semiconduttori e hardware che consentono l’intelligenza artificiale comporta anche gravi impatti ambientali che ricevono relativamente meno attenzione pubblica. Costruire e gestire un impianto di fabbricazione di semiconduttori all’avanguardia, o “fab”, ha notevoli requisiti di risorse e sottoprodotti inquinanti che vanno oltre un’ampia impronta di carbonio.\nAd esempio, una fabbrica all’avanguardia che produce chip come quelli a 5 nm potrebbe richiedere fino a quattro milioni di galloni di acqua pura al giorno. Questo consumo di acqua si avvicina a ciò che una città di mezzo milione di persone richiederebbe per tutte le esigenze. L’approvvigionamento di questa risorsa pone costantemente un’enorme pressione sulle falde acquifere e sui bacini idrici locali, soprattutto nelle regioni già sottoposte a stress idrico che ospitano molti centri di produzione ad alta tecnologia.\nInoltre, oltre 250 sostanze chimiche pericolose uniche vengono utilizzate in varie fasi della produzione di semiconduttori all’interno delle fab (Mills e Le Hunte 1997). Tra queste, solventi volatili come acido solforico, acido nitrico e acido fluoridrico, insieme ad arsina, fosfina e altre sostanze altamente tossiche. Per impedire lo scarico di queste sostanze chimiche sono necessari ampi controlli di sicurezza e infrastrutture di trattamento delle acque reflue per evitare la contaminazione del suolo e rischi per le comunità circostanti. Qualsiasi manipolazione chimica impropria o fuoriuscita imprevista comporta conseguenze disastrose.\n\nMills, Andrew, e Stephen Le Hunte. 1997. «An overview of semiconductor photocatalysis». J. Photochem. Photobiol., A 108 (1): 1–35. https://doi.org/10.1016/s1010-6030(97)00118-4.\nOltre al consumo di acqua e ai rischi chimici, le operazioni di fabbricazione dipendono anche dall’approvvigionamento di metalli rari, generano tonnellate di rifiuti pericolosi e possono ostacolare la biodiversità locale. Questa sezione analizzerà questi impatti critici ma meno discussi. Con vigilanza e investimenti nella sicurezza, i danni derivanti dalla produzione di semiconduttori possono essere contenuti pur consentendo il progresso tecnologico. Tuttavia, ignorare questi problemi esternalizzati aggraverà i danni ecologici e i rischi per la salute nel lungo periodo.\n\n16.5.1 Utilizzo e Stress Idrico\nLa fabbricazione di semiconduttori è un processo che richiede un consumo di acqua incredibilmente elevato. In base a un articolo del 2009, un tipico wafer di silicio da 300 mm richiede 8.328 litri di acqua, di cui 5.678 litri sono acqua ultrapura (Cope 2009). Mentre fabbriche moderne come quelle menzionate in precedenza possono utilizzare diversi milioni di galloni di acqua pura al giorno, si prevede che l’ultima fabbrica di TSMC in Arizona ne consumerà ancora di più, 8,9 milioni di galloni al giorno, pari a quasi il 3 percento dell’attuale produzione idrica della città. Per mettere le cose in prospettiva, Intel e Quantis hanno scoperto che oltre il 97% del loro consumo diretto di acqua è attribuito alle operazioni di produzione di semiconduttori all’interno dei loro stabilimenti di fabbricazione (Cooper et al. 2011).\n\nCope, Gord. 2009. «Pure water, semiconductors and the recession». Global Water Intelligence 10 (10).\n\nCooper, Tom, Suzanne Fallender, Joyann Pafumi, Jon Dettling, Sebastien Humbert, e Lindsay Lessard. 2011. «A semiconductor company’s examination of its water footprint approach». In Proceedings of the 2011 IEEE International Symposium on Sustainable Systems and Technology, 1–6. IEEE; IEEE. https://doi.org/10.1109/issst.2011.5936865.\n\n\n\n\n\n\nFigura 16.8: Impronta Idrica Giornaliera dei Data Center in confronto ad altri utilizzi idrici. Fonte: Raffreddamento del Data Center di Google\n\n\n\nPer mettere in prospettiva questi numeri, si consideri un data center di Google, che utilizza circa 450.000 galloni di acqua al giorno. Ciò equivale ad irrigare 17 acri di erba o a produrre 160 paia di jeans di cotone, il che dimostra l’enorme richiesta di acqua delle tecnologie avanzate.\nQuest’acqua viene ripetutamente utilizzata per rimuovere i contaminanti nelle fasi di pulizia e funge anche da refrigerante e fluido vettore nei processi di ossidazione termica, deposizione chimica e planarizzazione chimico-meccanica. Nei mesi estivi di punta, ciò equivale approssimativamente al consumo giornaliero di acqua di una città con una popolazione di mezzo milione di persone.\nNonostante si trovi in regioni con acqua a sufficienza, l’uso intensivo può depauperare gravemente le falde acquifere e i bacini di drenaggio locali. Ad esempio, la città di Hsinchu a Taiwan ha subito affondamenti delle falde acquifere e intrusioni di acqua marina nelle falde acquifere a causa dell’eccessivo pompaggio per soddisfare le richieste di approvvigionamento idrico della fabbrica della Taiwan Semiconductor Manufacturing Company (TSMC). Nelle aree interne con scarsità d’acqua come l’Arizona, sono necessari massicci apporti di acqua per supportare le fabbriche nonostante i bacini già esistenti.\nLo scarico di acqua dalle fabbriche rischia di contaminare l’ambiente oltre all’esaurimento se non trattato correttamente. Sebbene gran parte dello scarico venga riciclato all’interno della fabbrica, i sistemi di purificazione filtrano comunque metalli, acidi e altri contaminanti che possono inquinare fiumi e laghi se non gestiti con cautela (Prakash, Callahan, et al. 2023). Questi fattori rendono essenziale la gestione dell’uso dell’acqua quando si mitigano impatti più ampi sulla sostenibilità.\n\n\n16.5.2 Uso di Sostanze Chimiche Pericolose\nLa moderna fabbricazione di semiconduttori comporta la lavorazione di molte sostanze chimiche altamente pericolose in condizioni estreme di calore e pressione (Kim et al. 2018). Le principali sostanze chimiche utilizzate includono:\n\nKim, Sunju, Chungsik Yoon, Seunghon Ham, Jihoon Park, Ohun Kwon, Donguk Park, Sangjun Choi, Seungwon Kim, Kwonchul Ha, e Won Kim. 2018. «Chemical use in the semiconductor manufacturing industry». Int. J. Occup. Env. Heal. 24 (3-4): 109–18. https://doi.org/10.1080/10773525.2018.1519957.\n\nAcidi forti: Gli acidi fluoridrico, solforico, nitrico e cloridrico corrodono rapidamente gli ossidi e altri contaminanti superficiali, ma presentano anche pericoli di tossicità. Le fab possono utilizzare migliaia di tonnellate di questi acidi all’anno e l’esposizione accidentale può essere fatale per i lavoratori.\nSolventi: Solventi chiave come xilene, metanolo e metilisobutilchetone (MIBK) gestiscono i fotoresist dissolvibili, ma hanno effetti negativi sulla salute come irritazione della pelle/degli occhi ed effetti narcotici se maneggiati in modo improprio. Creano anche rischi di esplosione e inquinamento atmosferico.\nGas tossici: Le miscele di gas contenenti arsina (AsH3), fosfina (PH3), diborano (B2H6), germano (GeH4), ecc., sono alcune delle sostanze chimiche più letali utilizzate nelle fasi di doping e deposizione di vapore. Esposizioni minime possono causare avvelenamento, danni ai tessuti e persino la morte senza un trattamento rapido.\nComposti clorurati: Le vecchie formulazioni di planarizzazione chimico-meccanica incorporavano percloroetilene, tricloroetilene e altri solventi clorurati, che da allora sono stati vietati a causa dei loro effetti cancerogeni e dell’impatto sullo strato di ozono. Tuttavia, il loro rilascio precedente minaccia ancora le falde acquifere circostanti.\n\nProtocolli di gestione rigorosi, dispositivi di protezione per i lavoratori, ventilazione, sistemi di filtraggio/lavaggio, serbatoi di contenimento secondari e meccanismi di smaltimento specializzati sono essenziali laddove queste sostanze chimiche vengono utilizzate per ridurre al minimo i pericoli per la salute, le esplosioni, l’aria e le fuoriuscite ambientali (Wald e Jones 1987). Ma occasionalmente si verificano ancora errori umani e guasti alle apparecchiature, evidenziando perché la riduzione delle intensità chimiche di fabbricazione è uno sforzo di sostenibilità continuo.\n\nWald, Peter H., e Jeffrey R. Jones. 1987. «Semiconductor manufacturing: An introduction to processes and hazards». Am. J. Ind. Med. 11 (2): 203–21. https://doi.org/10.1002/ajim.4700110209.\n\n\n16.5.3 Esaurimento delle Risorse\nSebbene il silicio costituisca la base, sulla Terra c’è una scorta pressoché infinita di silicio. Infatti, il silicio è il secondo elemento più abbondante trovato nella crosta terrestre, rappresentando il 27,7% della massa totale della crosta. Solo l’ossigeno supera il silicio in abbondanza all’interno della crosta. Pertanto, il silicio non è necessario da considerare per l’esaurimento delle risorse. Tuttavia, i vari metalli e materiali speciali che consentono il processo di fabbricazione dei circuiti integrati e forniscono proprietà specifiche devono ancora essere scoperti. Mantenere le scorte di queste risorse è fondamentale, ma è minacciato dalla disponibilità finita e dalle influenze geopolitiche (Nakano 2021).\n\nNakano, Jane. 2021. The geopolitics of critical minerals supply chains. JSTOR.\n\nChen, H.-W. 2006. «Gallium, Indium, and Arsenic Pollution of Groundwater from a Semiconductor Manufacturing Area of Taiwan». B. Environ. Contam. Tox. 77 (2): 289–96. https://doi.org/10.1007/s00128-006-1062-3.\nGallio, indio e arsenico sono ingredienti vitali nella formazione di semiconduttori composti ultra-efficienti nei chip ad altissima velocità adatti per applicazioni 5G e AI (Chen 2006). Tuttavia, questi elementi rari hanno depositi naturali relativamente scarsi che si stanno esaurendo. Lo United States Geological Survey ha inserito l’indio nella sua lista delle materie prime a rischio più critiche, stimando una fornitura globale sostenibile per meno di 15 anni alla crescita attuale della domanda (Davies 2011).\nL’elio è richiesto in grandi volumi per le fabbriche di nuova generazione per consentire un raffreddamento preciso dei wafer durante il funzionamento. Ma la relativa rarità dell’elio e il fatto che una volta rilasciato nell’atmosfera, fuoriesce rapidamente dalla Terra rendono il mantenimento delle scorte di elio estremamente impegnativo a lungo termine (Davies 2011). Secondo le US National Academies, in questo mercato scarsamente scambiato si stanno già verificando notevoli aumenti dei prezzi e shock dell’offerta.\n\nJha, A. R. 2014. Rare Earth Materials: Properties and Applications. CRC Press. https://doi.org/10.1201/b17045.\nAltri rischi includono il controllo della Cina sul 90% degli elementi delle terre rare fondamentali per la produzione di materiali semiconduttori (Jha 2014). Qualsiasi problema nella catena di fornitura o controversia commerciale può portare a catastrofiche carenze di materie prime, data la mancanza di alternative attuali. Insieme alle carenze di elio, risolvere la disponibilità limitata e lo squilibrio geografico nell’accesso agli ingredienti essenziali rimane una priorità del settore per la sostenibilità.\n\n\n16.5.4 Generazione di Rifiuti Pericolosi\nLe fabbriche di semiconduttori generano tonnellate di rifiuti pericolosi ogni anno come sottoprodotti dei vari processi chimici (Grossman 2007). I principali flussi di rifiuti includono:\n\nGrossman, Elizabeth. 2007. High tech trash: Digital devices, hidden toxics, and human health. Island press.\n\nRifiuti gassosi: I sistemi di ventilazione delle fab catturano gas nocivi come arsina, fosfina e germano e li filtrano per evitare l’esposizione dei lavoratori. Tuttavia, ciò produce quantità significative di gas condensato pericoloso che necessita di un trattamento specializzato.\nCOV: I composti organici volatili come xilene, acetone e metanolo sono ampiamente utilizzati come solventi fotoresistenti e vengono evaporati come emissioni durante la cottura, l’incisione e lo stripping. I COV pongono problemi di tossicità e richiedono sistemi di lavaggio per impedirne il rilascio.\nAcidi esausti: Acidi forti come acido solforico, acido fluoridrico e acido nitrico si esauriscono nelle fasi di pulizia e incisione, trasformandosi in una zuppa corrosiva e tossica che può reagire pericolosamente, rilasciando calore e fumi se mescolata.\nFanghi: Il trattamento delle acque degli effluenti scaricati contiene metalli pesanti concentrati, residui acidi e contaminanti chimici. I sistemi di filtropressa separano questi fanghi pericolosi.\nTorta di filtrazione: I sistemi di filtrazione gassosa generano torte appiccicose di diverse tonnellate di composti assorbiti pericolosi che richiedono contenimento.\n\nSenza adeguate procedure di movimentazione, serbatoi di stoccaggio, materiali di imballaggio e contenimento secondario, lo smaltimento improprio di uno qualsiasi di questi flussi di rifiuti può causare pericolose fuoriuscite, esplosioni e rilasci nell’ambiente. Gli enormi volumi significano che anche le fabbriche ben gestite producono tonnellate di rifiuti pericolosi anno dopo anno, che richiedono un trattamento esteso.\n\n\n16.5.5 Impatti sulla Biodiversità\n\nInterruzione e Frammentazione dell’Habitat\nLe fabbriche di semiconduttori necessitano di ampie aree contigue per ospitare camere bianche, strutture di supporto, stoccaggio di sostanze chimiche, trattamento dei rifiuti e infrastrutture ausiliarie. Lo sviluppo di questi vasti spazi edificati smantella inevitabilmente gli habitat esistenti, danneggiando biomi sensibili che potrebbero aver impiegato decenni per svilupparsi. Ad esempio, la costruzione di un nuovo modulo di fabbricazione potrebbe radere al suolo gli ecosistemi forestali locali da cui specie come gufi maculati e alci dipendono per sopravvivere. La rimozione totale di tali habitat minaccia gravemente le popolazioni di animali selvatici che dipendono da quei terreni.\nInoltre, condutture, canali idrici, sistemi di scarico dell’aria e dei rifiuti, strade di accesso, torri di trasmissione e altre infrastrutture di supporto frammentano gli habitat indisturbati rimanenti. Gli animali che si spostano quotidianamente per cibo, acqua e deposizione delle uova possono vedere i loro pattern di migrazione bloccati da queste barriere umane fisiche che dividono in due i corridoi precedentemente naturali.\n\n\nDisturbi della Vita Acquatica\nCon le fabbriche di semiconduttori che consumano milioni di galloni di acqua ultra pura ogni giorno, accedere e scaricare tali volumi rischia di alterare l’idoneità degli ambienti acquatici circostanti che ospitano pesci, piante acquatiche, anfibi e altre specie. Se la fabbrica attinge alle falde acquifere come fonte di approvvigionamento primaria, un prelievo eccessivo a tassi insostenibili può impoverire i laghi o portare all’essiccazione dei corsi d’acqua man mano che i livelli dell’acqua scendono (Davies 2011).\n\nDavies, Emma. 2011. «Endangered elements: Critical thinking». https://www.rsc.org/images/Endangered\\%20Elements\\%20-\\%20Critical\\%20Thinking\\_tcm18-196054.pdf.\n\nLeRoy Poff, N, MM Brinson, e JW Day. 2002. «Aquatic ecosystems & Global climate change». Pew Center on Global Climate Change.\n\nTill, Aaron, Andrew L. Rypel, Andrew Bray, e Samuel B. Fey. 2019. «Fish die-offs are concurrent with thermal extremes in north temperate lakes». Nat. Clim. Change 9 (8): 637–41. https://doi.org/10.1038/s41558-019-0520-y.\nInoltre, lo scarico di acque reflue a temperature più elevate per raffreddare le apparecchiature di fabbricazione può modificare le condizioni del fiume a valle attraverso l’inquinamento termico. Le variazioni di temperatura oltre le soglie per cui si sono evolute le specie autoctone possono interrompere i cicli riproduttivi. L’acqua più calda contiene anche meno ossigeno disciolto, fondamentale per sostenere la vita di piante e animali acquatici (LeRoy Poff, Brinson, e Day 2002). In combinazione con tracce di contaminanti residui che sfuggono ai sistemi di filtrazione, l’acqua scaricata può trasformare cumulativamente gli ambienti rendendoli molto meno abitabili per gli organismi sensibili (Till et al. 2019).\n\n\nEmissioni Chimiche e Aeree\nMentre le moderne fabbriche di semiconduttori mirano a contenere gli scarichi di aria e sostanze chimiche attraverso sistemi di filtraggio estesi, alcuni livelli di emissioni spesso persistono, aumentando i rischi per la flora e la fauna vicine. Gli inquinanti atmosferici possono essere trasportati sottovento, tra cui composti organici volatili (COV), composti di ossido di azoto (NOx), particolato proveniente da scarichi operativi delle fabbriche ed emissioni di carburante delle centrali elettriche.\nPoiché i contaminanti permeano i terreni e le fonti d’acqua locali, la fauna selvatica che ingerisce cibo e acqua contaminati ingerisce sostanze tossiche, che la ricerca dimostra possono ostacolare la funzione cellulare, i tassi di riproduzione e la longevità, avvelenando lentamente gli ecosistemi (Hsu et al. 2016).\n\nHsu, Liang-Ching, Ching-Yi Huang, Yen-Hsun Chuang, Ho-Wen Chen, Ya-Ting Chan, Heng Yi Teah, Tsan-Yao Chen, Chiung-Fen Chang, Yu-Ting Liu, e Yu-Min Tzou. 2016. «Accumulation of heavy metals and trace elements in fluvial sediments received effluents from traditional and semiconductor industries». Scientific Reports 6 (1): 34250. https://doi.org/10.1038/srep34250.\nAllo stesso modo, le fuoriuscite accidentali di sostanze chimiche e la gestione impropria dei rifiuti, che rilasciano acidi e metalli pesanti nel terreno, possono influire notevolmente sulla capacità di ritenzione e lisciviazione. La flora, come le vulnerabili orchidee autoctone adattate a substrati poveri di nutrienti, può subire morie quando viene a contatto con sostanze chimiche di deflusso estranee che alterano il pH e la permeabilità del terreno. Un’analisi ha scoperto che una singola fuoriuscita di 500 galloni di acido nitrico ha portato all’estinzione regionale di una rara specie di muschio nell’anno successivo, quando l’effluente acido ha raggiunto gli habitat forestali vicini. Tali eventi di contaminazione innescano reazioni a catena attraverso la rete interconnessa della vita. Pertanto, protocolli rigorosi sono essenziali per evitare scarichi e deflussi pericolosi.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>IA Sostenibile</span>"
    ]
  },
  {
    "objectID": "contents/core/sustainable_ai/sustainable_ai.it.html#life-cycle-analysis",
    "href": "contents/core/sustainable_ai/sustainable_ai.it.html#life-cycle-analysis",
    "title": "16  IA Sostenibile",
    "section": "16.6 Analisi del Ciclo di Vita",
    "text": "16.6 Analisi del Ciclo di Vita\nPer comprendere l’impatto ambientale olistico dei sistemi di intelligenza artificiale è necessario un approccio completo che consideri l’intero ciclo di vita di queste tecnologie. Il “Life Cycle Analysis (LCA)” analisi del ciclo di vita si riferisce a un quadro metodologico utilizzato per quantificare gli impatti ambientali in tutte le fasi del ciclo di vita di un prodotto o sistema, dall’estrazione delle materie prime allo smaltimento a fine vita. L’applicazione dell’LCA ai sistemi di intelligenza artificiale può aiutare a identificare le aree prioritarie da prendere di mira per ridurre l’impatto ambientale complessivo.\n\n\n\n\n\n\nFigura 16.9: L’analisi del Ciclo di vita del Sistema AI è suddivisa in quattro fasi chiave: Progettazione, Produzione, Utilizzo, Smaltimento.\n\n\n\n\n16.6.1 Fasi del Ciclo di Vita di un Sistema di IA\nIl ciclo di vita di un sistema di intelligenza artificiale può essere suddiviso in quattro fasi chiave:\n\nFase di Progettazione: Include l’energia e le risorse utilizzate nella ricerca e nello sviluppo delle tecnologie di intelligenza artificiale. Comprende le risorse computazionali utilizzate per lo sviluppo e il test degli algoritmi che contribuiscono alle emissioni di carbonio.\nFase di Produzione: Questa fase prevede la produzione di componenti hardware come schede grafiche, processori e altri dispositivi di elaborazione necessari per l’esecuzione degli algoritmi di intelligenza artificiale. La produzione di questi componenti spesso comporta un notevole consumo di energia per l’estrazione dei materiali, l’elaborazione e le emissioni di gas serra.\nFase di Utilizzo: La fase successiva più dispendiosa in termini di energia riguarda l’uso operativo dei sistemi di intelligenza artificiale. Include l’elettricità consumata nei data center per l’addestramento e l’esecuzione delle reti neurali e l’alimentazione delle applicazioni per gli utenti finali. Questa è probabilmente una delle fasi con il più alto consumo di carbonio.\nFase di Smaltimento: Questa fase finale riguarda gli aspetti di fine vita dei sistemi di intelligenza artificiale, tra cui il riciclaggio e lo smaltimento dei rifiuti elettronici generati da hardware obsoleto o non funzionante oltre la sua durata utile.\n\n\n\n16.6.2 Impatto Ambientale in Ogni Fase\nProgettazione e Produzione\nL’impatto ambientale durante queste fasi iniziali di vita include emissioni derivanti dall’uso di energia e dall’esaurimento delle risorse derivanti dall’estrazione di materiali per la produzione di hardware. Al centro dell’hardware AI ci sono semiconduttori, principalmente silicio, utilizzati per realizzare i circuiti integrati nei processori e nei chip di memoria. Questa produzione di hardware si basa su metalli come il rame per i cablaggi, l’alluminio per gli involucri e varie plastiche e compositi per altri componenti. Utilizza anche terre rare e leghe specializzate, elementi come neodimio, terbio e ittrio, utilizzati in piccole ma vitali quantità. Ad esempio, la creazione di GPU si basa su rame e alluminio. Allo stesso tempo, i chip utilizzano terre rare, che è il processo di estrazione che può generare notevoli emissioni di carbonio e danni all’ecosistema.\nFase di Utilizzo\nL’AI calcola la maggior parte delle emissioni nel ciclo di vita a causa del continuo elevato consumo di energia, in particolare per il training e l’esecuzione di modelli. Ciò include emissioni dirette e indirette derivanti dall’uso di elettricità e dalla generazione di energia di rete non rinnovabile. Gli studi stimano che l’addestramento di modelli complessi può avere un’impronta di carbonio paragonabile alle emissioni fino a cinque auto nel corso della loro vita.\nFase di Smaltimento\nGli impatti della fase di smaltimento includono l’inquinamento dell’aria e dell’acqua dovuto a materiali tossici nei dispositivi, le sfide associate al riciclaggio di componenti elettronici complessi e la contaminazione in caso di gestione impropria. I composti nocivi derivanti dalla combustione dei rifiuti elettronici vengono rilasciati nell’atmosfera. Allo stesso tempo, la perdita di piombo, mercurio e altri materiali dalle discariche comporta rischi di contaminazione del suolo e delle falde acquifere se non adeguatamente controllata. L’implementazione di un efficace riciclaggio dei componenti elettronici è fondamentale.\n\n\n\n\n\n\nEsercizio 16.2: Monitoraggio delle Emissioni ML\n\n\n\n\n\nIn questo esercizio, esploreremo l’impatto ambientale dell’addestramento di modelli di machine learning. Utilizzeremo CodeCarbon per monitorare le emissioni, apprenderemo l’analisi del “Life Cycle Analysis (LCA)” per comprendere l’impronta di carbonio dell’IA ed esploreremo strategie per rendere lo sviluppo del modello ML più rispettoso dell’ambiente. Alla fine, si sarà in grado di monitorare le emissioni di carbonio dei modelli e iniziare a implementare pratiche più ecologiche nei progetti.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>IA Sostenibile</span>"
    ]
  },
  {
    "objectID": "contents/core/sustainable_ai/sustainable_ai.it.html#challenges-in-lca",
    "href": "contents/core/sustainable_ai/sustainable_ai.it.html#challenges-in-lca",
    "title": "16  IA Sostenibile",
    "section": "16.7 Sfide nell’LCA",
    "text": "16.7 Sfide nell’LCA\n\n16.7.1 Mancanza di Coerenza e Standard\nUna delle principali sfide che l’analisi del “life cycle analysis (LCA)” [ciclo di vita ] deve affrontare per i sistemi di intelligenza artificiale è la necessità di standard e framework metodologici coerenti. A differenza di categorie di prodotti come i materiali da costruzione, che hanno sviluppato standard internazionali per LCA tramite ISO 14040, non esistono linee guida stabilite per analizzare l’impatto ambientale di tecnologie informatiche complesse come l’intelligenza artificiale.\nQuesta assenza di uniformità significa che i ricercatori fanno ipotesi diverse e scelte metodologiche variabili. Ad esempio, uno studio del 2021 dell’Università del Massachusetts Amherst (Strubell, Ganesh, e McCallum 2019) ha analizzato le emissioni del ciclo di vita di diversi modelli di elaborazione del linguaggio naturale, ma ha preso in considerazione solo l’utilizzo delle risorse computazionali per il training e ha omesso gli impatti sulla produzione di hardware. Uno studio più completo del 2020 condotto dai ricercatori della Stanford University ha incluso stime delle emissioni derivanti dalla produzione di server, processori e altri componenti pertinenti, seguendo uno standard LCA allineato a ISO per l’hardware dei computer. Tuttavia, queste scelte divergenti nei confini del sistema e negli approcci contabili riducono la robustezza e impediscono confronti tra risultati simili.\nFramework e protocolli standardizzati su misura per gli aspetti unici dei sistemi di intelligenza artificiale e rapidi cicli di aggiornamento fornirebbero maggiore coerenza. Ciò potrebbe consentire a ricercatori e sviluppatori di comprendere i punti critici ambientali, confrontare le opzioni tecnologiche e monitorare con precisione i progressi nelle iniziative di sostenibilità nel campo dell’intelligenza artificiale. Gruppi industriali e organismi di normazione internazionali come IEEE o ACM dovrebbero dare priorità all’affrontare questa lacuna metodologica.\n\n\n16.7.2 Lacune nei Dati\nUn’altra sfida fondamentale per una valutazione completa del ciclo di vita dei sistemi di intelligenza artificiale sono le lacune sostanziali nei dati, in particolare per quanto riguarda gli impatti sulla catena di fornitura a monte e i flussi di rifiuti elettronici a valle. La maggior parte degli studi esistenti si concentra strettamente sulle emissioni della fase di apprendimento o di utilizzo derivanti dalle richieste di potenza di calcolo, che tralasciano una parte significativa delle emissioni nel corso della vita (Gupta et al. 2022).\nAd esempio, esistono pochi dati pubblici delle aziende che quantificano l’uso di energia e le emissioni derivanti dalla produzione di componenti hardware specializzati che abilitano l’intelligenza artificiale, tra cui GPU di fascia alta, chip ASIC, unità a stato solido e altro. Spesso i ricercatori si affidano a fonti secondarie o medie generiche del settore per approssimare gli impatti sulla produzione. Analogamente, in media, c’è una trasparenza limitata sul destino a valle una volta che i sistemi di intelligenza artificiale vengono scartati dopo 4-5 anni di durata utile.\nMentre i livelli di generazione di rifiuti elettronici possono essere stimati, i dettagli sulle perdite di materiali pericolosi, sui tassi di riciclaggio e sui metodi di smaltimento per i componenti complessi sono estremamente incerti senza una migliore documentazione aziendale o requisiti di reporting normativi.\nLa necessità di dati dettagliati sul consumo di risorse computazionali per l’addestramento di diversi tipi di modelli rende difficili calcoli affidabili delle emissioni per parametro o per query anche per la fase di utilizzo. Esistono tentativi di creare inventari del ciclo di vita che stimino il fabbisogno energetico medio per le attività chiave dell’intelligenza artificiale (Henderson et al. 2020; Anthony, Kanding, e Selvan 2020), ma la variabilità tra configurazioni hardware, algoritmi e incertezza dei dati di input rimane estremamente elevata. Inoltre, i dati sull’intensità di carbonio in tempo reale, fondamentali per tracciare con precisione l’impronta di carbonio operativa, devono essere migliorati in molte posizioni geografiche, rendendo gli strumenti esistenti per le emissioni di carbonio operative mere approssimazioni basate sui valori di intensità di carbonio media annuale.\n\nHenderson, Peter, Jieru Hu, Joshua Romoff, Emma Brunskill, Dan Jurafsky, e Joelle Pineau. 2020. «Towards the systematic reporting of the energy and carbon footprints of machine learning». The Journal of Machine Learning Research 21 (1): 10039–81.\n\nAnthony, Lasse F. Wolff, Benjamin Kanding, e Raghavendra Selvan. 2020. ICML Workshop on Challenges in Deploying and monitoring Machine Learning Systems.\nLa sfida è che strumenti come CodeCarbon e ML \\(\\textrm{CO}_2\\) sono, nella migliore delle ipotesi, solo approcci ad hoc, nonostante le loro buone intenzioni. Colmare le lacune reali dei dati con divulgazioni più rigorose sulla sostenibilità aziendale e una rendicontazione obbligatoria dell’impatto ambientale sarà fondamentale per comprendere e gestire gli impatti climatici complessivi dell’IA.\n\n\n16.7.3 Rapido Ritmo di Evoluzione\nL’evoluzione estremamente rapida dei sistemi di intelligenza artificiale pone ulteriori sfide nel mantenere aggiornate le valutazioni del ciclo di vita e nel tenere conto degli ultimi progressi hardware e software. Gli algoritmi di base, i chip specializzati, i framework e l’infrastruttura tecnica alla base dell’intelligenza artificiale hanno tutti fatto progressi eccezionalmente rapidi, con nuovi sviluppi che hanno rapidamente reso obsoleti i sistemi precedenti.\nAd esempio, nel deep learning, le nuove architetture di reti neurali che raggiungono prestazioni significativamente migliori su benchmark chiave o nuovi hardware ottimizzati come i chip TPU di Google possono cambiare completamente un modello “medio” in meno di un anno. Questi rapidi cambiamenti rendono rapidamente obsoleti gli studi LCA una tantum per il monitoraggio accurato delle emissioni derivanti dalla progettazione, esecuzione o smaltimento dell’intelligenza artificiale più recente.\nTuttavia, le risorse e l’accesso necessari per aggiornare continuamente gli LCA devono essere migliorati. Rifare frequentemente il lavoro, inventari del ciclo di vita ad alta intensità di dati e modelli di impatto per rimanere aggiornati con lo stato dell’arte dell’intelligenza artificiale è probabilmente irrealizzabile per molti ricercatori e organizzazioni. Tuttavia, analisi aggiornate potrebbero rilevare hotspot ambientali man mano che algoritmi e chip di silicio continuano a evolversi rapidamente.\nCiò presenta difficoltà nel bilanciare la precisione dinamica attraverso una valutazione continua con vincoli pragmatici. Alcuni ricercatori hanno proposto metriche proxy semplificate come il monitoraggio delle generazioni hardware nel tempo o l’utilizzo di benchmark rappresentativi come un set oscillante di paletti per confronti relativi, sebbene la granularità possa essere sacrificata. Nel complesso, la sfida del cambiamento rapido richiederà soluzioni metodologiche innovative per evitare di sottostimare gli oneri ambientali in evoluzione dell’IA.\n\n\n16.7.4 Complessità della Catena di Fornitura\nInfine, le complesse e spesso opache catene di fornitura associate alla produzione dell’ampia gamma di componenti hardware specializzati che abilitano l’intelligenza artificiale pongono sfide per la modellazione completa del ciclo di vita. Lo “stato-dell’arte” dellIA si basa su progressi all’avanguardia nell’elaborazione di chip, schede grafiche, archiviazione dati, apparecchiature di rete e altro ancora. Tuttavia, tracciare le emissioni e l’uso delle risorse attraverso le reti a livelli di fornitori globalizzati per tutti questi componenti è estremamente difficile.\nAd esempio, le unità di elaborazione grafica NVIDIA dominano gran parte dell’hardware di elaborazione dell’intelligenza artificiale, ma l’azienda si affida a diversi fornitori discreti in Asia e oltre per produrre GPU. Molte aziende a ogni livello di fornitore scelgono di mantenere privati i dati ambientali a livello di stabilimento, il che potrebbe abilitare completamente LCA robuste. Ottenere la trasparenza end-to-end su più livelli di fornitori in aree geografiche diverse con protocolli di divulgazione e normative variabili pone barriere nonostante sia fondamentale per la definizione completa dei confini. Ciò diventa ancora più complesso quando si tenta di modellare acceleratori hardware emergenti come le “tensor processing units (TPU)” [unità di elaborazione tensoriale], le cui reti di produzione devono ancora essere rese pubbliche.\nSenza la volontà dei giganti della tecnologia di richiedere e consolidare la divulgazione dei dati sull’impatto ambientale da tutte le loro catene di fornitura di elettronica globali, rimarrà una notevole incertezza sulla quantificazione dell’impronta del ciclo di vita completo dell’abilitazione hardware AI. Una maggiore visibilità della catena di fornitura abbinata a quadri di reporting sulla sostenibilità standardizzati che affrontino specificamente gli input complessi dell’AI promettono di arricchire gli LCA e dare priorità alle riduzioni dell’impatto ambientale.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>IA Sostenibile</span>"
    ]
  },
  {
    "objectID": "contents/core/sustainable_ai/sustainable_ai.it.html#sustainable-design-and-development",
    "href": "contents/core/sustainable_ai/sustainable_ai.it.html#sustainable-design-and-development",
    "title": "16  IA Sostenibile",
    "section": "16.8 Progettazione e Sviluppo Sostenibili",
    "text": "16.8 Progettazione e Sviluppo Sostenibili\n\n16.8.1 Principi di Sostenibilità\nMan mano che l’impatto dell’IA sull’ambiente diventa sempre più evidente, l’attenzione alla progettazione e allo sviluppo sostenibili nell’IA sta acquisendo importanza. Ciò comporta l’incorporazione di principi di sostenibilità nella progettazione dell’IA, lo sviluppo di modelli a risparmio energetico e l’integrazione di queste considerazioni in tutta la pipeline di sviluppo dell’IA. C’è una crescente necessità di considerare le implicazioni di sostenibilità e sviluppare principi per guidare l’innovazione responsabile. Di seguito è riportato un set di principi fondamentali. I principi fluiscono dalle fondamenta concettuali all’esecuzione pratica ai fattori di supporto all’implementazione; i principi forniscono una prospettiva del ciclo completo sull’incorporamento della sostenibilità nella progettazione e nello sviluppo dell’IA.\nLifecycle Thinking: Incoraggiare i progettisti a considerare l’intero ciclo di vita dei sistemi di IA, dalla raccolta e preelaborazione dei dati allo sviluppo del modello, al training, all’implementazione e al monitoraggio. L’obiettivo è garantire che la sostenibilità sia presa in considerazione in ogni fase. Ciò include l’utilizzo di hardware a risparmio energetico, la priorità alle fonti di energia rinnovabili e la pianificazione del riutilizzo o del riciclaggio di modelli dismessi.\nA Prova di Futuro: Progettare sistemi di intelligenza artificiale che anticipino esigenze e cambiamenti futuri può migliorare la sostenibilità. Ciò può comportare la creazione di modelli adattabili tramite apprendimento per trasferimento e architetture modulari. Include anche la capacità di pianificazione per aumenti previsti di scala operativa e volumi di dati.\nEfficienza e Minimalismo: Questo principio si concentra sulla creazione di modelli di intelligenza artificiale che raggiungano i risultati desiderati con il minimo utilizzo di risorse possibile. Comporta la semplificazione di modelli e algoritmi per ridurre i requisiti computazionali. Tecniche specifiche includono la potatura di parametri ridondanti, la quantizzazione e la compressione di modelli e la progettazione di architetture di modelli efficienti, come quelle discusse nel capitolo Ottimizzazioni.\nIntegrazione del Lifecycle Assessment (LCA): [Valutazione del Ciclo di Vita ] L’analisi degli impatti ambientali durante lo sviluppo e l’implementazione dei cicli di vita evidenzia le pratiche non sostenibili in anticipo. I team possono quindi apportare modifiche anziché scoprire i problemi in ritardo, quando sono più difficili da affrontare. L’integrazione di questa analisi nel flusso di progettazione standard evita di creare problemi ereditati di sostenibilità.\nAllineamento degli Incentivi: Gli incentivi economici e politici dovrebbero promuovere e premiare lo sviluppo sostenibile dell’IA. Questi possono includere sovvenzioni governative, iniziative aziendali, standard di settore e mandati accademici per la sostenibilità. Gli incentivi allineati consentono alla sostenibilità di essere inglobata nella cultura dell’IA.\nMetriche e Obiettivi di Sostenibilità: È importante stabilire metriche chiaramente definite che misurino fattori di sostenibilità come l’uso del carbonio e l’efficienza energetica. Stabilire obiettivi chiari per queste metriche fornisce linee guida concrete per i team per sviluppare sistemi di IA responsabili. Il monitoraggio delle prestazioni sulle metriche nel tempo mostra i progressi verso gli obiettivi di sostenibilità prefissati.\nEquità, Trasparenza e Responsabilità: I sistemi di IA sostenibili dovrebbero essere equi, trasparenti e responsabili. I modelli dovrebbero essere imparziali, con processi di sviluppo trasparenti e meccanismi per l’audit e la risoluzione dei problemi. Ciò crea fiducia nel pubblico e consente l’identificazione di pratiche non sostenibili.\nCollaborazione Interdisciplinare: I ricercatori di intelligenza artificiale che collaborano con scienziati e ingegneri ambientali possono dare vita a sistemi innovativi ad alte prestazioni ma rispettosi dell’ambiente. L’unione di competenze provenienti da diversi campi fin dall’inizio dei progetti consente di incorporare il pensiero sostenibile nel processo di progettazione dell’intelligenza artificiale.\nIstruzione e Consapevolezza: Workshop, programmi di formazione e programmi di studio che riguardano la sostenibilità dell’intelligenza artificiale accrescono la consapevolezza tra la prossima generazione di professionisti. Ciò fornisce agli studenti le conoscenze per sviluppare un’intelligenza artificiale che riduca al minimo gli impatti negativi sulla società e sull’ambiente. Inculcare questi valori fin dall’inizio plasma i professionisti e le culture aziendali di domani.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>IA Sostenibile</span>"
    ]
  },
  {
    "objectID": "contents/core/sustainable_ai/sustainable_ai.it.html#green-ai-infrastructure",
    "href": "contents/core/sustainable_ai/sustainable_ai.it.html#green-ai-infrastructure",
    "title": "16  IA Sostenibile",
    "section": "16.9 Infrastruttura di IA Green",
    "text": "16.9 Infrastruttura di IA Green\nGreen AI rappresenta un approccio trasformativo all’IA che incorpora la sostenibilità ambientale come principio fondamentale nella progettazione e nel ciclo di vita del sistema di IA (R. Schwartz et al. 2020). Questo cambiamento è guidato dalla crescente consapevolezza dell’impatto ecologico e dell’impronta di carbonio significativa delle tecnologie di IA, in particolare il processo di elaborazione intensiva di modelli di ML complessi.\n\nSchwartz, Roy, Jesse Dodge, Noah A. Smith, e Oren Etzioni. 2020. «Green AI». Commun. ACM 63 (12): 54–63. https://doi.org/10.1145/3381831.\nL’essenza di Green AI risiede nel suo impegno ad allineare il progresso dell’IA con gli obiettivi di sostenibilità in termini di efficienza energetica, utilizzo di energia rinnovabile e riduzione dei rifiuti. L’introduzione degli ideali di Green AI riflette la crescente responsabilità nel settore tecnologico verso la tutela ambientale e le pratiche tecnologiche etiche. Va oltre le ottimizzazioni tecniche verso una valutazione olistica del ciclo di vita su come i sistemi di IA influenzano le metriche di sostenibilità. Stabilire nuovi standard per un’IA ecologicamente consapevole apre la strada alla coesistenza armoniosa di progresso tecnologico e salute planetaria.\n\n16.9.1 Sistemi di IA a Risparmio Energetico\nL’efficienza energetica nei sistemi di intelligenza artificiale è un pilastro della Green AI, che mira a ridurre le richieste di energia tradizionalmente associate allo sviluppo e alle operazioni di intelligenza artificiale. Questo passaggio verso pratiche di intelligenza artificiale attente al risparmio energetico è fondamentale per affrontare le preoccupazioni ambientali sollevate dal campo in rapida espansione dell’intelligenza artificiale. Concentrandosi sull’efficienza energetica, i sistemi di intelligenza artificiale possono diventare più sostenibili, riducendo il loro impatto ambientale e aprendo la strada a un loro utilizzo più responsabile.\nCome abbiamo discusso in precedenza, l’addestramento e il funzionamento dei modelli di intelligenza artificiale, in particolare quelli su larga scala, sono noti per il loro elevato consumo energetico, che deriva dall’architettura del modello ad alta intensità di calcolo e dall’affidamento a grandi quantità di dati di addestramento. Ad esempio, si stima che l’addestramento di un grande modello di rete neurale all’avanguardia possa avere un’impronta di carbonio di 284 tonnellate, equivalente alle emissioni di 5 auto nel corso della loro vita (Strubell, Ganesh, e McCallum 2019).\n\nStrubell, Emma, Ananya Ganesh, e Andrew McCallum. 2019. «Energy and Policy Considerations for Deep Learning in NLP». In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, 3645–50. Florence, Italy: Association for Computational Linguistics. https://doi.org/10.18653/v1/p19-1355.\nPer affrontare le enormi richieste di energia, ricercatori e sviluppatori stanno esplorando attivamente metodi per ottimizzare i sistemi di intelligenza artificiale per una migliore efficienza energetica mantenendo al contempo l’accuratezza e le prestazioni del modello. Ciò include tecniche come quelle che abbiamo discusso nei capitoli sulle ottimizzazioni del modello, sull’intelligenza artificiale efficiente e sull’accelerazione hardware:\n\nDistillazione della conoscenza per trasferire la conoscenza da grandi modelli di intelligenza artificiale a versioni in miniatura\nApprocci di quantizzazione e potatura che riducono le complessità computazionali e spaziali\nNumeri a bassa precisione: riduzione della precisione matematica senza influire sulla qualità del modello\nHardware specializzato come TPU, chip neuromorfici ottimizzati esplicitamente per un’elaborazione efficiente dell’intelligenza artificiale\n\nUn esempio è il lavoro di Intel su Q8BERT: quantizzazione del modello di linguaggio BERT con interi a 8 bit, che porta a una riduzione di 4 volte delle dimensioni del modello con una perdita di accuratezza minima (Zafrir et al. 2019). La spinta verso un’intelligenza artificiale efficiente dal punto di vista energetico non è solo uno sforzo tecnico: ha implicazioni tangibili nel mondo reale. Sistemi più performanti riducono i costi operativi e l’impatto ambientale dell’intelligenza artificiale, rendendola accessibile per un’ampia distribuzione su dispositivi mobili ed edge. Apre inoltre la strada alla democratizzazione dell’IA e mitiga i pregiudizi ingiusti che possono emergere da un accesso non uniforme alle risorse informatiche tra regioni e comunità. Perseguire un’IA efficiente dal punto di vista energetico è quindi fondamentale per creare un futuro equo e sostenibile con l’IA.\n\nZafrir, Ofir, Guy Boudoukh, Peter Izsak, e Moshe Wasserblat. 2019. «Q8BERT: Quantized 8Bit BERT». In 2019 Fifth Workshop on Energy Efficient Machine Learning and Cognitive Computing - NeurIPS Edition (EMC2-NIPS), 36–39. IEEE; IEEE. https://doi.org/10.1109/emc2-nips53020.2019.00016.\n\n\n16.9.2 Infrastruttura di IA Sostenibile\nL’infrastruttura AI sostenibile include i framework fisici e tecnologici che supportano i sistemi AI, concentrandosi sulla sostenibilità ambientale. Ciò implica la progettazione e la gestione dell’infrastruttura AI per ridurre al minimo l’impatto ecologico, conservare le risorse e ridurre le emissioni di carbonio. L’obiettivo è creare un ecosistema sostenibile per l’AI che si allinei con obiettivi ambientali più ampi.\nI data center green sono fondamentali per l’infrastruttura AI sostenibile, ottimizzati per l’efficienza energetica e spesso alimentati da fonti di energia rinnovabili. Questi data center impiegano tecnologie di raffreddamento avanzate (Ebrahimi, Jones, e Fleischer 2014), design di server a risparmio energetico (Uddin e Rahman 2012) e sistemi di gestione intelligenti (Buyya, Beloglazov, e Abawajy 2010) per ridurre il consumo di energia. Il passaggio a un’infrastruttura informatica ecologica implica anche l’adozione di hardware a basso consumo energetico, come processori ottimizzati per l’IA che offrono prestazioni elevate con requisiti energetici ridotti, di cui abbiamo parlato nel capitolo Accelerazione dell’IA. Questi sforzi riducono collettivamente l’impronta di carbonio delle operazioni di intelligenza artificiale su larga scala.\n\nEbrahimi, Khosrow, Gerard F. Jones, e Amy S. Fleischer. 2014. «A review of data center cooling technology, operating conditions and the corresponding low-grade waste heat recovery opportunities». Renewable Sustainable Energy Rev. 31 (marzo): 622–38. https://doi.org/10.1016/j.rser.2013.12.007.\n\nUddin, Mueen, e Azizah Abdul Rahman. 2012. «Energy efficiency and low carbon enabler green IT framework for data centers considering green metrics». Renewable Sustainable Energy Rev. 16 (6): 4078–94. https://doi.org/10.1016/j.rser.2012.03.014.\n\nBuyya, Rajkumar, Anton Beloglazov, e Jemal Abawajy. 2010. «Energy-Efficient Management of Data Center Resources for Cloud Computing: A Vision, Architectural Elements, and Open Challenges». https://arxiv.org/abs/1006.0308.\n\nChua, L. 1971. «Memristor-The missing circuit element». #IEEE_J_CT# 18 (5): 507–19. https://doi.org/10.1109/tct.1971.1083337.\nL’integrazione di fonti di energia rinnovabili, come energia solare, eolica e idroelettrica, nell’infrastruttura di intelligenza artificiale è importante per la sostenibilità ambientale (Chua 1971). Molte aziende tecnologiche e istituti di ricerca stanno investendo in progetti di energia rinnovabile per alimentare i loro data center. Ciò non solo aiuta a rendere le operazioni di intelligenza artificiale carbon neutral, ma promuove anche un’adozione più ampia di energia pulita. L’utilizzo di fonti di energia rinnovabili mostra chiaramente l’impegno per la responsabilità ambientale nel settore dell’intelligenza artificiale.\nLa sostenibilità si estende anche ai materiali e all’hardware utilizzati nella creazione di sistemi di intelligenza artificiale. Ciò implica la scelta di materiali ecocompatibili, l’adozione di pratiche di riciclaggio e la garanzia di uno smaltimento responsabile dei rifiuti elettronici. Sono in corso sforzi per sviluppare componenti hardware più sostenibili, tra cui chip a risparmio energetico progettati per attività specifiche del dominio (come gli acceleratori di IA) e materiali ecocompatibili nella produzione di dispositivi (Cenci et al. 2021; Irimia-Vladu 2014). Anche il ciclo di vita di questi componenti è un punto focale, con iniziative volte a estendere la durata di vita dell’hardware e a promuovere il riciclaggio e il riutilizzo.\n\nCenci, Marcelo Pilotto, Tatiana Scarazzato, Daniel Dotto Munchen, Paula Cristina Dartora, Hugo Marcelo Veit, Andrea Moura Bernardes, e Pablo R. Dias. 2021. «Eco-Friendly ElectronicsA Comprehensive Review». Adv. Mater. Technol. 7 (2): 2001263. https://doi.org/10.1002/admt.202001263.\n\nIrimia-Vladu, Mihai. 2014. «“Green” electronics: Biodegradable and biocompatible materials and devices for sustainable future». Chem. Soc. Rev. 43 (2): 588–610. https://doi.org/10.1039/c3cs60235d.\nSebbene si stiano facendo progressi nell’infrastruttura di IA sostenibile, permangono delle sfide, come gli elevati costi della tecnologia verde e la necessità di standard globali nelle pratiche sostenibili. Le direzioni future includono un’adozione più diffusa di energia verde, ulteriori innovazioni nell’hardware a risparmio energetico e la collaborazione internazionale su politiche di IA sostenibile. Perseguire un’infrastruttura di IA sostenibile non è solo uno sforzo tecnico, ma un approccio olistico che comprende aspetti ambientali, economici e sociali, assicurando che l’IA avanzi in armonia con la salute del nostro pianeta.\n\n\n16.9.3 Framework e Strumenti\nL’accesso ai framework e agli strumenti giusti è essenziale per implementare in modo efficace le pratiche di intelligenza artificiale verde. Queste risorse sono progettate per aiutare sviluppatori e ricercatori a creare sistemi di IA più efficienti dal punto di vista energetico e rispettosi dell’ambiente. Vanno da librerie software ottimizzate per un basso consumo energetico a piattaforme che facilitano lo sviluppo di applicazioni di IA sostenibili.\nDiverse librerie software e ambienti di sviluppo sono specificamente pensati per l’intelligenza artificiale verde. Questi strumenti spesso includono funzionalità per ottimizzare i modelli di IA per ridurre il loro carico computazionale e, di conseguenza, il loro consumo energetico. Ad esempio, le librerie in PyTorch e TensorFlow che supportano la potatura del modello, la quantizzazione e le architetture di reti neurali efficienti consentono agli sviluppatori di creare sistemi di intelligenza artificiale che richiedono meno potenza di elaborazione ed energia. Inoltre, comunità open source come la Green Software Foundation stanno creando una metrica centralizzata dell’intensità di carbonio e sviluppando software per un’informatica attenta alle emissioni di carbonio.\nGli strumenti di monitoraggio dell’energia sono fondamentali per l’intelligenza artificiale verde, poiché consentono agli sviluppatori di misurare e analizzare il consumo energetico dei loro sistemi. Figura 16.10 è uno screenshot di una dashboard del consumo energetico fornita dalla piattaforma di servizi cloud di Microsoft. Fornendo informazioni dettagliate su dove e come viene utilizzata l’energia, questi strumenti consentono agli sviluppatori di prendere decisioni informate sull’ottimizzazione dei loro modelli per una migliore efficienza energetica. Ciò può comportare modifiche nella progettazione dell’algoritmo, nella selezione dell’hardware, nella selezione del software di cloud computing o nei parametri operativi.\n\n\n\n\n\n\nFigura 16.10: Dashboard del consumo energetico di Microsoft Azure. Fonte: Will Buchanan.\n\n\n\nCon la crescente integrazione di fonti di energia rinnovabile nelle operazioni di IA, i framework che facilitano questo processo stanno diventando sempre più importanti. Questi framework aiutano a gestire l’approvvigionamento energetico da fonti rinnovabili come l’energia solare o eolica, assicurando che i sistemi di IA possano funzionare in modo efficiente con input energetici fluttuanti.\nOltre all’efficienza energetica, gli strumenti di valutazione della sostenibilità aiutano a valutare l’impatto ambientale più ampio dei sistemi di IA. Questi strumenti possono analizzare fattori come l’impronta di carbonio delle operazioni di IA, l’impatto del ciclo di vita dei componenti hardware (Gupta et al. 2022) e la sostenibilità complessiva dei progetti di IA (Prakash, Callahan, et al. 2023).\n\nGupta, Udit, Mariam Elgamal, Gage Hills, Gu-Yeon Wei, Hsien-Hsin S. Lee, David Brooks, e Carole-Jean Wu. 2022. «Act: designing sustainable computer systems with an architectural carbon modeling tool». In Proceedings of the 49th Annual International Symposium on Computer Architecture, 784–99. ACM. https://doi.org/10.1145/3470496.3527408.\n\nPrakash, Shvetank, Tim Callahan, Joseph Bushagour, Colby Banbury, Alan V. Green, Pete Warden, Tim Ansell, e Vijay Janapa Reddi. 2023. «CFU Playground: Full-stack Open-Source Framework for Tiny Machine Learning (TinyML) Acceleration on FPGAs». In 2023 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS). Vol. abs/2201.01863. IEEE. https://doi.org/10.1109/ispass57527.2023.00024.\nLa disponibilità e lo sviluppo continuo di framework e strumenti di IA Green sono fondamentali per promuovere pratiche di sostenibili. Fornendo le risorse necessarie a sviluppatori e ricercatori, questi strumenti facilitano la creazione di sistemi più rispettosi dell’ambiente e incoraggiano un più ampio cambiamento verso la sostenibilità nella comunità tecnologica. Man mano che l’IA Green continua a evolversi, questi framework e strumenti svolgeranno un ruolo fondamentale nel dare forma a un futuro più sostenibile per l’IA. As Green AI continues to evolve, these frameworks and tools will play a vital role in shaping a more sustainable future for AI.\n\n\n16.9.4 Benchmark e Classifiche\nBenchmark e classifiche sono importanti per guidare i progressi nell’IA Green, poiché forniscono modi standardizzati per misurare e confrontare diversi metodi. Benchmark ben progettati che catturano metriche rilevanti su efficienza energetica, emissioni di carbonio e altri fattori di sostenibilità consentono alla comunità di monitorare i progressi in modo equo e significativo.\nEsistono ampi benchmark per tracciare le prestazioni del modello di IA, come quelli discussi nel capitolo Benchmarking. Tuttavia, esiste una chiara e urgente necessità di ulteriori benchmark standardizzati incentrati su parametri di sostenibilità come efficienza energetica, emissioni di carbonio e impatto ecologico complessivo. La comprensione dei costi ambientali dell’IA deve attualmente essere migliorata da una mancanza di trasparenza e misura standardizzata attorno a questi fattori.\nSforzi emergenti come ML.ENERGY Leaderboard, che fornisce risultati di benchmarking delle prestazioni e del consumo energetico per la generazione di testo di modelli linguistici di grandi dimensioni (LLM), aiutano a migliorare la comprensione del costo energetico dell’implementazione GenAI.\nCome con qualsiasi benchmark, quelli di IA Green devono rappresentare scenari di utilizzo e carichi di lavoro realistici. I benchmark che si concentrano strettamente su metriche facilmente manipolabili possono portare a guadagni a breve termine, ma non riescono a riflettere gli ambienti di produzione effettivi in cui sono necessarie misure di efficienza e sostenibilità più olistiche. La comunità dovrebbe continuare ad espandere i benchmark per coprire diversi casi d’uso.\nUn’adozione più ampia di suite di benchmark comuni da parte degli operatori del settore accelererà l’innovazione nell’IA Green consentendo un confronto più semplice delle tecniche tra le organizzazioni. I benchmark condivisi abbassano la barriera per dimostrare i vantaggi di sostenibilità di nuovi strumenti e best practice. uttavia, quando si progettano benchmark per l’intero settore, è necessario prestare attenzione a questioni come proprietà intellettuale, privacy e sensibilità commerciale. Le iniziative per sviluppare set di dati di riferimento aperti per la valutazione dell’IA Green possono aiutare a promuovere una partecipazione più ampia.\nMan mano che i metodi e l’infrastruttura per l’IA Green continuano a maturare, la comunità deve rivedere la progettazione dei benchmark per garantire che le suite esistenti catturino bene nuove tecniche e scenari. Monitorare il panorama in evoluzione attraverso aggiornamenti e revisioni regolari dei benchmark sarà importante per mantenere confronti rappresentativi nel tempo. Gli sforzi della comunità per la cura dei benchmark possono consentire suite di benchmark sostenibili che resistano alla prova del tempo. Suite di benchmark complete di proprietà di comunità di ricerca o terze parti neutrali come MLCommons possono incoraggiare una più ampia partecipazione e standardizzazione.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>IA Sostenibile</span>"
    ]
  },
  {
    "objectID": "contents/core/sustainable_ai/sustainable_ai.it.html#case-study-google-4ms",
    "href": "contents/core/sustainable_ai/sustainable_ai.it.html#case-study-google-4ms",
    "title": "16  IA Sostenibile",
    "section": "16.10 Caso di Studio: 4M di Google",
    "text": "16.10 Caso di Studio: 4M di Google\nNegli ultimi dieci anni, l’intelligenza artificiale è passata rapidamente dalla ricerca accademica ai sistemi di produzione su larga scala che alimentano numerosi prodotti e servizi Google. Poiché i modelli e i carichi di lavoro dell’IA sono cresciuti esponenzialmente in termini di dimensioni e richieste di elaborazione, sono emerse preoccupazioni circa il loro consumo energetico e l’impatto ambientale. Alcuni ricercatori hanno previsto una crescita incontrollata dell’appetito energetico del ML che potrebbe superare le efficienze ottenute da algoritmi e hardware migliorati (Thompson et al. 2021).\n\nThompson, Neil C., Kristjan Greenewald, Keeheon Lee, e Gabriel F. Manso. 2021. «Deep Learning’s Diminishing Returns: The Cost of Improvement is Becoming Unsustainable». IEEE Spectr. 58 (10): 50–55. https://doi.org/10.1109/mspec.2021.9563954.\nTuttavia, i dati di produzione di Google rivelano una storia diversa: l’IA rappresenta un costante 10-15% del consumo energetico totale dell’azienda dal 2019 al 2021. Questo caso di studio analizza come Google ha applicato un approccio sistematico sfruttando quattro best practice, quelle che definiscono le “4 M”: “Model efficiency”, “Machine optimization”, “Mechanization through cloud computing” e “Mapping to green locations” [efficienza del modello, ottimizzazione delle macchine, meccanizzazione tramite cloud computing e mappatura di luoghi Green], per piegare la curva delle emissioni dai carichi di lavoro dell’IA.\nLa portata dell’utilizzo dell’IA da parte di Google lo rende un caso di studio ideale. Solo nel 2021, l’azienda ha addestrato modelli come il GLam da 1,2 trilioni di parametri. Analizzare come l’applicazione dell’IA è stata abbinata a rapidi guadagni di efficienza in questo ambiente ci aiuta a fornire un modello logico che il più ampio campo dell’IA seguirà.\nPubblicando in modo trasparente statistiche dettagliate sull’uso dell’energia, adottando tassi di acquisto di cloud senza emissioni di carbonio e fonti rinnovabili e altro ancora, insieme alle sue innovazioni tecniche, Google ha consentito ai ricercatori esterni di misurare i progressi in modo accurato. Il loro studio nell’ACM CACM (Patterson et al. 2022) evidenzia come l’approccio multiforme dell’azienda dimostri che le previsioni di consumo energetico dell’IA incontrollabili possono essere superate concentrando gli sforzi ingegneristici su modelli di sviluppo sostenibile. Il ritmo dei miglioramenti suggerisce anche che i guadagni di efficienza dell’ML sono appena iniziati.\n\nPatterson, David, Joseph Gonzalez, Urs Holzle, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild, David R. So, Maud Texier, e Jeff Dean. 2022. «The Carbon Footprint of Machine Learning Training Will Plateau, Then Shrink». Computer 55 (7): 18–28. https://doi.org/10.1109/mc.2022.3148714.\n\n16.10.1 Le 4M Best Practice di Google\nPer ridurre le emissioni derivanti dai carichi di lavoro IA in rapida espansione, gli ingegneri di Google hanno sistematicamente identificato quattro aree di best practice, denominate “4 M”, in cui le ottimizzazioni potrebbero sommarsi per ridurre l’impatto ambientale del ML:\n\nModello: La selezione di architetture di modelli di intelligenza artificiale efficienti può ridurre i calcoli di 5-10 volte senza alcuna perdita di qualità. Google ha svolto ricerche approfondite sullo sviluppo di modelli “sparsi” e sulla ricerca di architetture neurali per creare modelli più efficienti come Evolved Transformer e Primer.\nMacchina: L’utilizzo di hardware ottimizzato per l’IA rispetto ai sistemi generici migliora le prestazioni per watt di 2-5 volte. Le Tensor Processing Unit (TPU) di Google hanno portato a un’efficienza di carbonio 5-13 volte migliore rispetto alle GPU non ottimizzate per il ML.\nMeccanizzazione: Sfruttando i sistemi di cloud computing progettati per un utilizzo elevato rispetto ai tradizionali data center on-premise, i costi energetici si riducono di 1,4-2 volte. Google cita l’efficacia dell’utilizzo energetico del suo data center come superiore alle medie del settore.\nMappa: La scelta di ubicazioni per data center dotate di elettricità a basse emissioni di carbonio riduce le emissioni lorde di altre 5-10 volte. Google fornisce mappe in tempo reale che evidenziano la percentuale di energia rinnovabile utilizzata dalle sue strutture.\n\nInsieme, queste pratiche hanno creato drastici guadagni di efficienza composti. Ad esempio, l’ottimizzazione del modello Transformer AI su TPU in una sede di data center sostenibile ha ridotto il consumo di energia dell’83x. Ha ridotto le emissioni di \\(\\textrm{CO}_2\\) di un fattore di 747.\n\n\n16.10.2 Risultati Significativi\nNonostante la crescita esponenziale nell’adozione dell’IA nei prodotti e nei servizi, gli sforzi di Google per migliorare l’efficienza del carbonio del ML hanno prodotto guadagni misurabili, contribuendo a limitare l’appetito energetico complessivo. Un punto dati chiave che evidenzia questo progresso è che i carichi di lavoro dell’intelligenza artificiale sono rimasti stabili al 10%-15% del consumo energetico totale dell’azienda dal 2019 al 2021. Man mano che l’IA è diventata parte integrante di più offerte Google, i cicli di elaborazione complessivi dedicati ad essa sono cresciuti in modo sostanziale. Tuttavia, l’efficienza negli algoritmi, nell’hardware specializzato, nella progettazione dei data center e nella geografia flessibile ha consentito alla sostenibilità di tenere il passo, con l’IA che rappresenta solo una frazione dell’elettricità totale del data center in anni di espansione.\nAltri casi di studio sottolineano come un focus ingegneristico sui pattern di sviluppo dell’intelligenza artificiale sostenibile abbia consentito rapidi miglioramenti della qualità di pari passo con i guadagni ambientali. Ad esempio, il modello di elaborazione del linguaggio naturale GPT-3 è stato considerato all’avanguardia a metà del 2020. Tuttavia, il suo successore GLaM ha migliorato la precisione riducendo al contempo le esigenze di elaborazione del training e utilizzando energia più pulita nei data center, riducendo le emissioni di CO2 di un fattore 14 in soli 18 mesi di evoluzione del modello.\nAnalogamente, Google ha scoperto che le precedenti speculazioni pubblicate non hanno colto nel segno sull’appetito energetico del ML per fattori da 100 a 100.000X a causa della mancanza di metriche del mondo reale. Tracciando in modo trasparente l’impatto dell’ottimizzazione, Google sperava di motivare l’efficienza evitando al contempo estrapolazioni sovrastimate sul pedaggio ambientale del ML.\nQuesti casi di studio basati sui dati mostrano come aziende come Google stiano indirizzando i progressi dell’IA verso traiettorie sostenibili e migliorando l’efficienza per superare la crescita dell’adozione. Con ulteriori sforzi in termini di analisi del ciclo di vita, ottimizzazione dell’inferenza ed espansione delle energie rinnovabili, le aziende possono puntare ad accelerare i progressi, dimostrando che il potenziale pulito del ML è stato appena sbloccato dagli attuali guadagni.\n\n\n16.10.3 Ulteriori Miglioramenti\nSebbene Google abbia compiuto progressi misurabili nel limitare l’impatto ambientale delle sue operazioni di intelligenza artificiale, l’azienda riconosce che ulteriori guadagni in termini di efficienza saranno essenziali per un’innovazione responsabile, data la continua espansione della tecnologia.\nUn’area di attenzione è mostrare come i progressi siano spesso erroneamente considerati come un aumento dell’insostenibilità informatica, come la ricerca di architettura neurale (NAS) per trovare modelli ottimizzati, che stimolano risparmi a valle, superando i costi iniziali. Nonostante spenda più energia nella scoperta di modelli piuttosto che nell’ingegneria manuale, la NAS riduce le emissioni nel corso del ciclo di vita producendo progetti efficienti richiamabili su innumerevoli applicazioni.\nInoltre, l’analisi rivela che concentrare gli sforzi di sostenibilità sull’ottimizzazione lato server e data center ha senso, dato il consumo energetico dominante rispetto ai dispositivi consumer. Sebbene Google riduca gli impatti dell’inferenza su processori come i telefoni cellulari, la priorità è il miglioramento dei cicli di training e dell’approvvigionamento di energie rinnovabili per data center per ottenere il massimo effetto.\nA tal fine, i progressi di Google nel mettere in comune strutture cloud progettate in modo inefficiente evidenziano il valore della scala e della centralizzazione. Con l’allontanamento dei carichi di lavoro dai server locali inefficienti, la priorità data dai giganti di Internet alle energie rinnovabili (con Google e Meta che hanno raggiunto il 100% di energie rinnovabili rispettivamente dal 2017 e dal 2020) sblocca tagli alle emissioni complessive.\nInsieme, questi sforzi sottolineano che, sebbene non sia possibile adagiarsi sugli allori, l’approccio multiforme di Google dimostra che i miglioramenti dell’efficienza dell’IA stanno solo accelerando. Le iniziative intersettoriali relative alla valutazione del ciclo di vita, ai pattern di sviluppo attenti alle emissioni di carbonio, alla trasparenza e all’abbinamento della crescente domanda di IA con la fornitura di energia elettrica pulita aprono la strada a un’ulteriore flessione della curva man mano che l’adozione aumenta. I risultati dell’azienda spingono il settore più ampio a replicare queste attività di sostenibilità integrate.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>IA Sostenibile</span>"
    ]
  },
  {
    "objectID": "contents/core/sustainable_ai/sustainable_ai.it.html#embedded-ai-internet-of-trash",
    "href": "contents/core/sustainable_ai/sustainable_ai.it.html#embedded-ai-internet-of-trash",
    "title": "16  IA Sostenibile",
    "section": "16.11 IA Embedded - Internet of Trash",
    "text": "16.11 IA Embedded - Internet of Trash\nSebbene molta attenzione sia stata rivolta a rendere più sostenibili gli immensi data center che alimentano l’IA, una preoccupazione altrettanto urgente è lo spostamento delle capacità dell’IA in dispositivi edge e endpoint intelligenti. L’IA edge/embedded consente una reattività quasi in tempo reale senza dipendenze dalla connettività. Riduce inoltre le esigenze di larghezza di banda di trasmissione. Tuttavia, l’aumento di dispositivi minuscoli comporta altri rischi.\nI minuscoli computer, microcontrollori e ASIC personalizzati che alimentano l’intelligenza edge affrontano limitazioni di dimensioni, costi e potenza che escludono le GPU di fascia alta utilizzate nei data center. Invece, richiedono algoritmi ottimizzati e circuiti estremamente compatti ed efficienti dal punto di vista energetico per funzionare senza problemi. Tuttavia, l’ingegneria per questi fattori di forma microscopici apre rischi in termini di obsolescenza programmata, smaltibilità e spreco. Figura 16.11 mostra che si prevede che il numero di dispositivi IoT raggiungerà i 30 miliardi di dispositivi connessi entro il 2030.\n\n\n\n\n\n\nFigura 16.11: Numero di dispositivi connessi all’Internet of Things (IoT) in tutto il mondo dal 2019 al 2023. Fonte: Statista.\n\n\n\nLa gestione del fine vita dei gadget connessi a Internet dotati di sensori e intelligenza artificiale rimane un problema spesso trascurato durante la progettazione. Tuttavia, questi prodotti permeano beni di consumo, veicoli, infrastrutture pubbliche, apparecchiature industriali e altro ancora.\n\n16.11.1 Rifiuti Elettronici\nI rifiuti elettronici, o “e-waste”, si riferiscono ad apparecchiature elettriche e componenti scartati che entrano nel flusso dei rifiuti. Ciò include dispositivi che devono essere collegati, hanno una batteria o circuiti elettrici. Con la crescente adozione di dispositivi intelligenti e sensori connessi a Internet, i volumi di e-waste aumentano rapidamente ogni anno. Questi gadget in proliferazione contengono metalli pesanti tossici come piombo, mercurio e cadmio che diventano pericoli per l’ambiente e la salute se smaltiti in modo improprio.\nLa quantità di rifiuti elettronici prodotti sta crescendo a un ritmo allarmante. Oggi, ne produciamo già 50 milioni di tonnellate all’anno. Entro il 2030, si prevede che tale cifra salirà a ben 75 milioni di tonnellate, poiché il consumo di elettronica di consumo continua ad accelerare. La produzione globale di e-waste raggiungerà i 120 milioni di tonnellate all’anno entro il 2050 (Un e Forum 2019). La produzione in forte crescita e i brevi cicli di vita dei nostri gadget alimentano questa crisi, dagli smartphone e tablet ai dispositivi connessi a Internet e agli elettrodomestici.\nI paesi in via di sviluppo sono i più colpiti, in quanto necessitano di più infrastrutture per elaborare in modo sicuro i dispositivi elettronici obsoleti. Nel 2019, i tassi di riciclaggio formali dei rifiuti elettronici nei paesi più poveri variavano dal 13% al 23%. Il resto finisce per essere scaricato illegalmente, bruciato o smantellato in modo grossolano, rilasciando materiali tossici nell’ambiente e danneggiando i lavoratori e le comunità locali. Chiaramente, c’è ancora molto da fare per costruire una capacità globale per una gestione etica e sostenibile dei rifiuti elettronici, altrimenti rischiamo danni irreversibili.\nIl pericolo è che la manipolazione grossolana dei dispositivi elettronici per spogliarli delle parti di valore esponga i lavoratori e le comunità emarginati a nocive plastiche/metalli bruciati. L’avvelenamento da piombo presenta rischi particolarmente elevati per lo sviluppo infantile se ingerito o inalato. Nel complesso, solo circa il 20% dei rifiuti elettronici prodotti è stato raccolto utilizzando metodi ecologicamente corretti, secondo le stime delle Nazioni Unite (Un e Forum 2019). Quindi sono urgentemente necessarie soluzioni per una gestione responsabile del ciclo di vita per contenere lo smaltimento non sicuro, dato che il volume aumenta vertiginosamente.\n\nUn, e World Economic Forum. 2019. A New Circular Vision for Electronics, Time for a Global Reboot. PACE - Platform for Accelerating the Circular Economy. https://www3.weforum.org/docs/WEF\\_A\\_New\\_Circular\\_Vision\\_for\\_Electronics.pdf.\n\n\n16.11.2 Elettronica Monouso\nI costi in rapida diminuzione dei microcontrollori, delle piccole batterie ricaricabili e dell’hardware compatto di comunicazione hanno consentito l’integrazione di sistemi di sensori intelligenti nei beni di consumo di uso quotidiano. Questi dispositivi Internet-of-Things (IoT) monitorano le condizioni del prodotto, le interazioni degli utenti e i fattori ambientali per consentire reattività in tempo reale, personalizzazione e decisioni aziendali basate sui dati nel mercato connesso in evoluzione.\nTuttavia, questi dispositivi elettronici integrati affrontano poca supervisione o pianificazione per gestire in modo sostenibile il loro eventuale smaltimento una volta che i prodotti spesso rivestiti in plastica vengono scartati dopo breve tempo. I sensori IoT ora risiedono comunemente in articoli monouso come bottiglie d’acqua, imballaggi per alimenti, flaconi di farmaci e contenitori per cosmetici che finiscono prevalentemente nei flussi di rifiuti delle discariche dopo poche settimane o mesi di utilizzo da parte dei consumatori.\nIl problema si accelera poiché sempre più produttori si affrettano a integrare chip mobili, fonti di alimentazione, moduli Bluetooth e altri moderni circuiti integrati in silicio, che costano meno di 1 dollaro USA, in vari prodotti senza protocolli per il riciclaggio, la sostituzione delle batterie o la riutilizzabilità dei componenti. Nonostante le loro piccole dimensioni individuali, i volumi di questi dispositivi e il peso dei rifiuti nel corso della loro vita incombono. A differenza della regolamentazione di dispositivi elettronici più grandi, esistono pochi vincoli normativi sui requisiti dei materiali o sulla tossicità di piccoli gadget monouso.\nPur offrendo praticità durante il lavoro, la combinazione insostenibile di difficile recupero e limitati meccanismi di guasto sicuri fa sì che i dispositivi connessi monouso contribuiscano a quote sproporzionate di futuri volumi di rifiuti elettronici che necessitano di urgente attenzione.\n\n\n16.11.3 Obsolescenza Programmata\nL’obsolescenza programmata si riferisce alla strategia di progettazione intenzionale di produzione di prodotti con durate di vita artificialmente limitate che diventano rapidamente non funzionali o obsoleti. Ciò stimola cicli di acquisto di sostituzione più rapidi poiché i consumatori scoprono che i dispositivi non soddisfano più le loro esigenze nel giro di pochi anni. Tuttavia, l’elettronica progettata per l’obsolescenza prematura contribuisce a volumi di rifiuti elettronici insostenibili.\nAd esempio, incollare batterie e componenti di smartphone insieme ostacola la riparabilità rispetto ad assemblaggi modulari e accessibili. L’implementazione di aggiornamenti software che rallentano deliberatamente le prestazioni del sistema crea la percezione che valga la pena aggiornare i dispositivi prodotti solo diversi anni prima.\nAllo stesso modo, le introduzioni alla moda di nuove generazioni di prodotti con aggiunte di funzionalità minori ma esclusive fanno sembrare rapidamente datate le versioni precedenti. Queste tattiche costringono ad acquistare nuovi gadget (ad esempio, iPhone) molto prima della fine della loro operatività. Se moltiplicati per categorie di elettronica in rapida evoluzione, miliardi di articoli appena indossati vengono scartati ogni anno.\nL’obsolescenza programmata intensifica quindi l’utilizzo delle risorse e la creazione di rifiuti nella produzione di prodotti senza alcuna intenzione di lunga durata. Ciò contraddice i principi di sostenibilità in materia di durata, riutilizzo e conservazione dei materiali. Mentre stimola vendite e guadagni continui per i produttori nel breve termine, la strategia esternalizza costi ambientali e tossine su comunità prive di un’adeguata infrastruttura di elaborazione dei rifiuti elettronici.\nLe politiche e l’azione dei consumatori sono fondamentali per contrastare i design dei gadget che sono inutilmente monouso per default. Le aziende dovrebbero anche investire in programmi di gestione dei prodotti che supportino il riutilizzo e il recupero responsabili.\nConsideriamo l’esempio del mondo reale. Apple è stata attenzionata nel corso degli anni per aver presumibilmente coinvolto nell’obsolescenza programmata per incoraggiare i clienti ad acquistare nuovi modelli di iPhone. L’azienda avrebbe progettato i suoi telefoni in modo che le prestazioni si degradino nel tempo o che le funzionalità esistenti diventino incompatibili con i nuovi sistemi operativi, il che, secondo i critici, è finalizzato a stimolare cicli di aggiornamento più rapidi. Nel 2020, Apple ha pagato una multa di 25 milioni di euro per risolvere un caso in Francia in cui le autorità di regolamentazione hanno ritenuto l’azienda colpevole di aver rallentato intenzionalmente i vecchi iPhone senza informare chiaramente i clienti tramite aggiornamenti di iOS.\nNon essendo trasparente sulle modifiche alla gestione dell’alimentazione che hanno ridotto le prestazioni del dispositivo, Apple ha partecipato ad attività ingannevoli che hanno ridotto la durata del prodotto per aumentare le vendite. L’azienda ha affermato che è stato fatto per “smussare” i picchi che potrebbero causare improvvisamente lo spegnimento delle vecchie batterie. Tuttavia, questo esempio evidenzia i rischi legali legati all’impiego dell’obsolescenza programmata e alla mancata comunicazione corretta di quando le modifiche alle funzionalità influiscono sull’usabilità del dispositivo nel tempo: persino marchi leader come Apple possono avere problemi se percepiti come coloro che accorciano intenzionalmente i cicli di vita del prodotto.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>IA Sostenibile</span>"
    ]
  },
  {
    "objectID": "contents/core/sustainable_ai/sustainable_ai.it.html#policy-and-regulatory-considerations",
    "href": "contents/core/sustainable_ai/sustainable_ai.it.html#policy-and-regulatory-considerations",
    "title": "16  IA Sostenibile",
    "section": "16.12 Considerazioni Normative e Politiche",
    "text": "16.12 Considerazioni Normative e Politiche\n\n16.12.1 Mandati di Misura e Rendicontazione\nUn meccanismo politico sempre più rilevante per i sistemi di IA è rappresentato dai requisiti di misurazione e rendicontazione relativi al consumo energetico e alle emissioni di carbonio. Misurazioni obbligatorie, audit, divulgazioni e metodologie più rigorose allineate alle metriche di sostenibilità possono aiutare a colmare le lacune informative che ostacolano le ottimizzazioni dell’efficienza.\nAllo stesso tempo, le politiche nazionali o regionali richiedono alle aziende di una certa dimensione di utilizzare l’IA nei loro prodotti o sistemi back-end per segnalare il consumo energetico o le emissioni associate ai principali carichi di lavoro di IA. Organizzazioni come la Partnership on AI, IEEE e NIST potrebbero contribuire a definire metodologie standardizzate. Proposte più complesse implicano la definizione di modalità coerenti per misurare la complessità computazionale, il PUE del data center, l’intensità di carbonio dell’approvvigionamento energetico e le efficienze ottenute tramite hardware specifico per l’IA.\nAnche gli obblighi di rendicontazione per gli utenti del settore pubblico che acquistano servizi di IA, ad esempio tramite una proposta di legge in Europa, potrebbero aumentare la trasparenza. Tuttavia, gli enti regolatori devono bilanciare l’ulteriore onere di misurazione che tali mandati impongono alle organizzazioni con le continue riduzioni di carbonio derivanti dall’incorporazione di pattern di sviluppo consapevoli della sostenibilità.\nPer essere più costruttivi, qualsiasi politica di misurazione e rendicontazione dovrebbe concentrarsi sull’abilitazione di un continuo perfezionamento piuttosto che su restrizioni o limiti semplicistici. Man mano che i progressi dell’IA si sviluppano rapidamente, agili barriere di sicurezza di governance che incorporano considerazioni sulla sostenibilità in normali metriche di valutazione possono motivare un cambiamento positivo. Tuttavia, una prescrizione eccessiva rischia di limitare l’innovazione se i requisiti diventano obsoleti. La politica di efficienza dell’IA accelera i progressi in tutto il settore combinando flessibilità con appropriate barriere di sicurezza di trasparenza.\n\n\n16.12.2 Meccanismi di Restrizione\nOltre agli obblighi di segnalazione, i politici dispongono di diversi meccanismi di restrizione che potrebbero modellare direttamente il modo in cui i sistemi di IA vengono sviluppati e implementati per ridurre le emissioni:\nLimiti sulle Emissioni delle Elaborazioni: La proposta di legge sull’intelligenza artificiale della Commissione europea adotta un approccio orizzontale che potrebbe consentire di stabilire limiti per l’intera economia sul volume di potenza di elaborazione disponibile per l’addestramento dei modelli di intelligenza artificiale. Come i sistemi di scambio delle emissioni, i limiti mirano a disincentivare indirettamente l’elaborazione estensiva rispetto alla sostenibilità. Tuttavia, la qualità del modello potrebbe essere migliorata per fornire più percorsi per l’acquisizione di capacità aggiuntiva.\nCondizionamento dell’Accesso alle Risorse Pubbliche: Alcuni esperti hanno proposto incentivi come consentire l’accesso solo a set di dati pubblici o potenza di elaborazione per lo sviluppo di modelli fondamentalmente efficienti piuttosto che architetture stravaganti. Ad esempio, il consorzio di benchmarking MLCommons fondato da importanti aziende tecnologiche potrebbe integrare formalmente l’efficienza nelle sue metriche di classifica standardizzate, tuttavia, l’accesso condizionato rischia di limitare l’innovazione.\nMeccanismi Finanziari: Analogamente alle tasse sul carbonio sulle industrie inquinanti, le tariffe applicate per unità di consumo di elaborazione correlato all’IA potrebbero scoraggiare un inutile ridimensionamento del modello, finanziando al contempo innovazioni di efficienza. I crediti d’imposta potrebbero in alternativa premiare le organizzazioni pioniere di tecniche di IA più accurate ma compatte. Tuttavia, gli strumenti finanziari richiedono un’attenta calibrazione tra generazione di entrate ed equità e non penalizzare eccessivamente gli usi produttivi dell’IA.\nDivieti Tecnologici: Se la misurazione fissasse costantemente le emissioni estreme su applicazioni specifiche dell’IA senza percorsi di bonifica, i divieti assoluti rappresentano uno strumento di ultima istanza per i decisori politici. Tuttavia, dato il duplice uso dell’IA, definire implementazioni dannose e benefiche risulta complesso, rendendo necessaria una valutazione di impatto olistica prima di concludere che non esiste alcun valore redentivo. Vietare tecnologie promettenti rischia di avere conseguenze indesiderate e richiede cautela.\n\n\n16.12.3 Incentivi Governativi\nÈ una pratica comune per i governi fornire incentivi fiscali o di altro tipo a consumatori o aziende quando contribuiscono a pratiche tecnologiche più sostenibili. Tali incentivi esistono già negli Stati Uniti per l’adozione di pannelli solari o edifici a risparmio energetico. Per quanto ne sappiamo, non esistono ancora incentivi fiscali per pratiche di sviluppo specifiche per l’IA.\nUn altro potenziale programma di incentivi che sta iniziando a essere esplorato è l’utilizzo di sovvenzioni governative per finanziare progetti di IA Green. Ad esempio, in Spagna, sono stati stanziati 300 milioni di euro per finanziare specificamente progetti di IA e sostenibilità. Gli incentivi governativi sono una strada promettente per incoraggiare pratiche di comportamento aziendale e dei consumatori sostenibili, ma è necessaria un’attenta riflessione per determinare come tali incentivi si adatteranno alle richieste del mercato (Cohen, Lobel, e Perakis 2016).\n\nCohen, Maxime C., Ruben Lobel, e Georgia Perakis. 2016. «The Impact of Demand Uncertainty on Consumer Subsidies for Green Technology Adoption». Manage. Sci. 62 (5): 1235–58. https://doi.org/10.1287/mnsc.2015.2173.\n\n\n16.12.4 Autoregolamentazione\nComplementari alle potenziali azioni governative, i meccanismi di autogoverno volontario consentono alla comunità dell’IA di perseguire obiettivi di sostenibilità senza interventi dall’alto:\nImpegni per le Energie Rinnovabili: Grandi professionisti dell’IA come Google, Microsoft, Amazon e Meta si sono impegnati ad acquistare abbastanza elettricità rinnovabile per soddisfare il 100% delle loro richieste energetiche. Questi impegni sbloccano tagli alle emissioni composti man mano che aumenta la potenza di calcolo. La formalizzazione di tali programmi incentiva le regioni dei data center verdi. Tuttavia, ci sono critiche sul fatto che questi impegni siano sufficienti (Monyei e Jenkins 2018).\n\nMonyei, Chukwuka G., e Kirsten E. H. Jenkins. 2018. «Electrons have no identity: Setting right misrepresentations in Google and Apple’s clean energy purchasing». Energy Research &amp; Social Science 46 (dicembre): 48–51. https://doi.org/10.1016/j.erss.2018.06.015.\nPrezzi Interni del Carbonio: Alcune organizzazioni utilizzano prezzi ombra sulle emissioni di carbonio per rappresentare i costi ambientali nelle decisioni di allocazione del capitale tra progetti di IA. Se modellati in modo efficace, gli oneri teorici sulle impronte di carbonio dello sviluppo indirizzano i finanziamenti verso innovazioni efficienti piuttosto che solo verso guadagni di accuratezza.\nChecklist per lo Sviluppo dell’Efficienza: Gruppi come AI Sustainability Coalition suggeriscono modelli di checklist volontari che evidenziano le scelte di progettazione del modello, le configurazioni hardware e altri fattori che gli architetti possono regolare per applicazione per limitare le emissioni. Le organizzazioni possono guidare il cambiamento radicando la sostenibilità come metrica di successo primaria insieme a precisione e costi.\nAuditing Indipendente: Anche in assenza di mandati di divulgazione pubblica, le aziende specializzate in audit di sostenibilità tecnologica aiutano gli sviluppatori di IA a identificare gli sprechi, creare roadmap di efficienza e confrontare i progressi tramite revisioni imparziali. Strutturare tali audit in procedure di governance interna o nel processo di approvvigionamento espande la responsabilità.\n\n\n16.12.5 Considerazioni Globali\nMentre misurazione, restrizioni, incentivi e autoregolamentazione rappresentano potenziali meccanismi politici per promuovere la sostenibilità dell’IA, la frammentazione tra i regimi nazionali rischia di avere conseguenze indesiderate. Come per altri domini di politica tecnologica, la divergenza tra regioni deve essere gestita attentamente.\nAd esempio, a causa di preoccupazioni sulla privacy dei dati regionali, OpenAI ha impedito agli utenti europei di accedere al suo chatbot virale ChatGPT. Ciò è avvenuto dopo che la proposta di legge sull’IA dell’UE ha segnalato un approccio precauzionale, consentendo alla CE di vietare determinati usi dell’IA ad alto rischio e di imporre regole di trasparenza che creano incertezza per il rilascio di nuovi modelli. Tuttavia, sarebbe saggio mettere in guardia contro l’azione del regolatore in quanto potrebbe inavvertitamente limitare l’innovazione europea se i regimi con una regolamentazione più leggera attraggono più spesa e talenti per la ricerca sull’IA nel settore privato. Trovare un terreno comune è fondamentale.\nI principi dell’OCSE sull’IA e i quadri delle Nazioni Unite sottolineano principi universalmente concordati che tutte le politiche nazionali dovrebbero sostenere: trasparenza, responsabilità, mitigazione dei “bias” [pregiudizi] e altro ancora. Incorporare in modo costruttivo la sostenibilità come principio fondamentale per un’IA responsabile all’interno di linee guida internazionali può motivare un’azione unitaria senza sacrificare la flessibilità tra sistemi legali divergenti. Evitare dinamiche di corsa al ribasso dipende da una cooperazione multilaterale illuminata.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>IA Sostenibile</span>"
    ]
  },
  {
    "objectID": "contents/core/sustainable_ai/sustainable_ai.it.html#public-perception-and-engagement",
    "href": "contents/core/sustainable_ai/sustainable_ai.it.html#public-perception-and-engagement",
    "title": "16  IA Sostenibile",
    "section": "16.13 Percezione e Coinvolgimento del Pubblico",
    "text": "16.13 Percezione e Coinvolgimento del Pubblico\nMentre l’attenzione della società e gli sforzi politici volti alla sostenibilità ambientale aumentano in tutto il mondo, cresce l’entusiasmo per l’utilizzo dell’intelligenza artificiale per aiutare ad affrontare le sfide ecologiche. Tuttavia, la comprensione e gli atteggiamenti del pubblico nei confronti del ruolo dei sistemi di IA nei contesti di sostenibilità devono ancora essere chiariti e sono offuscati da idee sbagliate. Da un lato, le persone sperano che algoritmi avanzati possano fornire nuove soluzioni per l’energia verde, il consumo responsabile, i percorsi di decarbonizzazione e la conservazione dell’ecosistema. Dall’altro, i timori sui rischi dell’IA incontrollata si insinuano anche nel dominio ambientale e minano il discorso costruttivo. Inoltre, una mancanza di consapevolezza pubblica su questioni chiave come la trasparenza nello sviluppo di strumenti di IA incentrati sulla sostenibilità e potenziali pregiudizi nei dati o nella modellazione minacciano anche di limitare la partecipazione inclusiva e degradare la fiducia del pubblico.\nAffrontare priorità complesse e interdisciplinari come la sostenibilità ambientale richiede un coinvolgimento pubblico informato e sfumato e progressi responsabili nell’innovazione dell’IA. Il percorso da seguire richiede sforzi collaborativi attenti ed equi tra esperti in ML, climatologia, politica ambientale, scienze sociali e comunicazione. Mappare il panorama delle percezioni pubbliche, identificare le insidie e tracciare strategie per coltivare sistemi di IA comprensibili, accessibili e affidabili che puntino a priorità ecologiche condivise si rivelerà essenziale per realizzare obiettivi di sostenibilità. Questo terreno complesso giustifica un esame approfondito delle dinamiche socio-tecniche coinvolte.\n\n16.13.1 Consapevolezza dell’IA\nA maggio 2022, il Pew Research Center ha intervistato 5.101 adulti statunitensi, scoprendo che il 60% aveva sentito o letto “un po’” sull’IA mentre il 27% ne aveva sentito “molto”, il che indica un discreto riconoscimento generale, ma probabilmente una comprensione limitata di dettagli o applicazioni. Tuttavia, tra coloro che hanno una certa familiarità con l’IA, emergono preoccupazioni riguardo ai rischi di uso improprio dei dati personali secondo i termini concordati. Ciononostante, il 62% ritiene che l’IA potrebbe semplificare la vita moderna se applicata in modo responsabile. Tuttavia, una comprensione specifica dei contesti di sostenibilità deve ancora essere migliorata.\nGli studi che tentano di categorizzare i “sentiment” del discorso online rilevano una divisione quasi equa tra ottimismo e cautela riguardo all’implementazione dell’IA per obiettivi di sostenibilità. I fattori che guidano la positività includono le speranze di una migliore previsione dei cambiamenti ecologici utilizzando modelli di ML. La negatività nasce da una mancanza di fiducia negli algoritmi auto-supervisionati che evitano conseguenze indesiderate dovute a impatti umani imprevedibili su sistemi naturali complessi durante l’addestramento.\nLa convinzione pubblica più diffusa rimane che, mentre l’IA ha il potenziale per accelerare le soluzioni su questioni come la riduzione delle emissioni e la protezione della fauna selvatica, una salvaguardia inadeguata intorno a pregiudizi dei dati, punti ciechi etici e considerazioni sulla privacy potrebbero essere rischi più apprezzati se perseguiti con noncuranza, soprattutto su larga scala. Ciò porta a esitazione intorno al supporto incondizionato senza prove di uno sviluppo deliberato e guidato democraticamente.\n\n\n16.13.2 Messaggistica\nGli sforzi ottimistici stanno evidenziando la promessa di sostenibilità dell’IA e sottolineano il potenziale del ML avanzato per accelerare radicalmente gli effetti di decarbonizzazione da reti intelligenti, app personalizzate di tracciamento del carbonio, ottimizzazioni automatizzate dell’efficienza degli edifici e analisi predittive che guidano gli sforzi di conservazione mirati. Una modellazione in tempo reale più completa di complessi cambiamenti climatici ed ecologici utilizzando algoritmi auto-miglioranti offre speranza per mitigare le perdite di biodiversità ed evitare gli scenari peggiori.\nTuttavia, prospettive cautelative, come i Principi di IA di Asilomar, mettono in dubbio se l’IA stessa potrebbe esacerbare le sfide della sostenibilità se vincolata in modo improprio. Le crescenti richieste di energia dei sistemi di elaborazione su larga scala e il training sempre più massiccio del modello di rete neurale sono in conflitto con le ambizioni di energia pulita. La mancanza di diversità negli input di dati o nelle priorità degli sviluppatori potrebbe sminuire le urgenti considerazioni di giustizia ambientale. L’impegno pubblico scettico a breve termine probabilmente dipende dalla necessità di salvaguardie percepibili contro i sistemi di intelligenza artificiale incontrollati che impazziscono nei processi ecologici fondamentali.\nIn sostanza, i “framing” polarizzati promuovono l’intelligenza artificiale come uno strumento indispensabile per la risoluzione dei problemi di sostenibilità, se indirizzata compassionevolmente verso le persone e il pianeta, oppure presentano l’IA come un amplificatore dei danni esistenti che dominano insidiosamente aspetti nascosti dei sistemi naturali centrali per tutta la vita. Superare tali impasse richiede di bilanciare discussioni oneste sui compromessi con visioni condivise per un progresso tecnologico equo e democraticamente governato che mira al ripristino.\n\n\n16.13.3 Partecipazione Equa\nGarantire una partecipazione e un accesso equi dovrebbe costituire la pietra angolare di qualsiasi iniziativa di sostenibilità con il potenziale per importanti impatti sociali. Questo principio si applica ugualmente ai sistemi di IA che mirano a obiettivi ambientali. Tuttavia, voci comunemente escluse come le comunità in prima linea, rurali o indigene e le generazioni future non presenti per il consenso potrebbero subire conseguenze sproporzionate dalle trasformazioni tecnologiche. Ad esempio, la Partnership on AI ha lanciato eventi espressamente mirati al contributo delle comunità emarginate sull’implementazione responsabile dell’intelligenza artificiale.\nGarantire un accesso e una partecipazione equi dovrebbe costituire la pietra angolare di qualsiasi iniziativa di sostenibilità con il potenziale per importanti impatti sociali, che si tratti di intelligenza artificiale o altro. Tuttavia, l’impegno inclusivo nell’intelligenza artificiale ambientale si basa in parte sulla disponibilità e sulla comprensione delle risorse informatiche fondamentali. Come sottolinea il recente rapporto OCSE sulla capacità di calcolo IA nazionale (Oecd 2023), molti paesi attualmente non dispongono di dati o piani strategici che mappino le esigenze per l’infrastruttura richiesta per alimentare i sistemi di IA. Questo punto cieco politico potrebbe limitare gli obiettivi economici ed esacerbare le barriere all’ingresso per le popolazioni emarginate. Il loro progetto sollecita lo sviluppo di strategie nazionali per la capacità di calcolo AI lungo dimensioni di capacità, accessibilità, pipeline di innovazione e resilienza per ancorare l’innovazione. L’archiviazione dei dati di base deve essere migliorata e le piattaforme di sviluppo dei modelli o l’hardware specializzato potrebbero inavvertitamente concentrare i progressi dell’AI nelle mani di gruppi selezionati. Pertanto, la pianificazione di un’espansione equilibrata delle risorse di calcolo AI fondamentali tramite iniziative politiche si collega direttamente alle speranze di una risoluzione dei problemi di sostenibilità democratizzata utilizzando strumenti ML equi e trasparenti.\n\nOecd. 2023. «A blueprint for building national compute capacity for artificial intelligence». 350. Organisation for Economic Co-Operation; Development (OECD). https://doi.org/10.1787/876367e3-en.\nL’idea chiave è che la partecipazione equa nei sistemi di intelligenza artificiale che affrontano le sfide ambientali si basa in parte sulla garanzia che la capacità di elaborazione e l’infrastruttura di base siano corrette, il che richiede una pianificazione politica proattiva da una prospettiva nazionale.\n\n\n16.13.4 Trasparenza\nMentre le agenzie del settore pubblico e le aziende private si affrettano ad adottare strumenti di IA per aiutare ad affrontare le urgenti sfide ambientali, le richieste di trasparenza sullo sviluppo e la funzionalità di questi sistemi hanno iniziato ad amplificarsi. Le funzionalità di ML spiegabili e interpretabili diventano sempre più cruciali per creare fiducia nei modelli emergenti che mirano a guidare le conseguenti politiche di sostenibilità. Iniziative come il Montreal Carbon Pledge hanno riunito i leader della tecnologia per impegnarsi a pubblicare valutazioni di impatto prima di lanciare sistemi ambientali, come promesso di seguito:\n\n“Come investitori istituzionali, dobbiamo agire nel migliore interesse a lungo termine dei nostri beneficiari. In questo ruolo fiduciario, i rischi di investimento a lungo termine sono associati alle emissioni di gas serra, ai cambiamenti climatici e alla regolamentazione del carbonio. Misurare la nostra impronta di carbonio è fondamentale per comprendere meglio, quantificare e gestire gli impatti, i rischi e le opportunità correlati al carbonio e ai cambiamenti climatici nei nostri investimenti. Pertanto, come primo passo, ci impegniamo a misurare e divulgare annualmente l’impronta di carbonio dei nostri investimenti per utilizzare queste informazioni per sviluppare una strategia di coinvolgimento e identificare e stabilire obiettivi di riduzione dell’impronta di carbonio.” – Montréal Carbon Pledge\n\nAbbiamo bisogno di un impegno simile per la sostenibilità e la responsabilità dell’IA. L’accettazione diffusa e l’impatto delle soluzioni di sostenibilità dell’IA dipenderanno in parte dalla comunicazione deliberata di schemi di convalida, metriche e livelli di giudizio umano applicati prima dell’implementazione in tempo reale. Lavori come i Principi per l’IA spiegabile del NIST possono aiutare a promuovere la trasparenza nei sistemi di IA. Il National Institute of Standards and Technology (NIST) ha pubblicato un influente set di linee guida denominato “Principles for Explainable AI” [Principi per l’IA spiegabile] (Phillips et al. 2020). Questo framework articola le best practice per la progettazione, la valutazione e l’implementazione di sistemi di IA responsabili con funzionalità trasparenti e interpretabili che creano comprensione e fiducia fondamentali per l’utente.\n\nPhillips, P Jonathon, Carina A Hahn, Peter C Fontana, David A Broniatowski, e Mark A Przybocki. 2020. «Four principles of explainable artificial intelligence». Gaithersburg, Maryland 18.\nDelinea quattro principi fondamentali: in primo luogo, i sistemi di IA dovrebbero fornire spiegazioni contestualmente rilevanti che giustifichino il ragionamento alla base dei loro output alle parti interessate appropriate. In secondo luogo, queste spiegazioni di IA devono comunicare informazioni in modo significativo per il livello di comprensione appropriato del loro pubblico target. Il successivo è il principio di accuratezza, che stabilisce che le spiegazioni dovrebbero riflettere fedelmente il processo effettivo e la logica che informano i meccanismi interni di un modello di IA per generare output o raccomandazioni dati in base agli input. Infine, un principio di limiti di conoscenza obbliga le spiegazioni a chiarire i confini di un modello di IA nel catturare l’intera ampiezza della complessità, della varianza e delle incertezze del mondo reale all’interno di uno spazio problematico.\nNel complesso, questi principi NIST offrono ai professionisti e agli adottanti dell’IA una guida su considerazioni chiave sulla trasparenza, essenziali per sviluppare soluzioni accessibili che diano priorità all’autonomia e alla fiducia dell’utente piuttosto che semplicemente massimizzare le sole metriche di accuratezza predittiva. Man mano che l’IA avanza rapidamente in contesti sociali sensibili come sanità, finanza, occupazione e oltre, tali linee guida di progettazione incentrate sull’uomo continueranno a crescere in importanza per ancorare l’innovazione agli interessi pubblici.\nCiò si applica anche al dominio della capacità ambientale. Un’innovazione dell’IA responsabile e guidata democraticamente che mira a priorità ecologiche condivise dipende dal mantenimento della vigilanza pubblica, della comprensione e della supervisione su sistemi altrimenti opachi che assumono ruoli di primo piano nelle decisioni della società. Dare priorità a progetti di algoritmi spiegabili e pratiche di trasparenza radicale secondo standard globali può aiutare a sostenere la fiducia collettiva che questi strumenti migliorino piuttosto che mettere a repentaglio le speranze per un futuro guidato.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>IA Sostenibile</span>"
    ]
  },
  {
    "objectID": "contents/core/sustainable_ai/sustainable_ai.it.html#direzioni-e-sfide-future",
    "href": "contents/core/sustainable_ai/sustainable_ai.it.html#direzioni-e-sfide-future",
    "title": "16  IA Sostenibile",
    "section": "16.14 Direzioni e Sfide Future",
    "text": "16.14 Direzioni e Sfide Future\nGuardando al futuro, il ruolo dell’IA nella sostenibilità ambientale è destinato a crescere in modo ancora più significativo. Il potenziale dell’IA per guidare i progressi nell’energia rinnovabile, nella modellazione climatica, negli sforzi di conservazione e altro è immenso. Tuttavia, è una moneta a due facce, poiché dobbiamo superare diverse sfide e indirizzare i nostri sforzi verso uno sviluppo dell’IA sostenibile e responsabile.\n\n16.14.1 Direzioni Future\nUna delle direzioni chiave del futuro è lo sviluppo di modelli e algoritmi di intelligenza artificiale più efficienti dal punto di vista energetico. Ciò implica una ricerca e innovazione continue in aree come il “pruning” [potatura] dei modelli, la quantizzazione e l’uso di numeri a bassa precisione, nonché lo sviluppo dell’hardware per consentire la piena redditività di queste innovazioni. Inoltre, esaminiamo paradigmi di elaborazione alternativi che non si basano su architetture von-Neumann. Ulteriori informazioni su questo argomento sono disponibili nel capitolo sull’accelerazione hardware. L’obiettivo è creare sistemi di IA che offrano prestazioni elevate riducendo al minimo il consumo di energia e le emissioni di carbonio.\nUn’altra direzione importante è l’integrazione di fonti di energia rinnovabili nell’infrastruttura di IA. Poiché i data center continuano a contribuire in modo significativo all’impronta di carbonio dell’IA, la transizione verso fonti di energia rinnovabili come l’energia solare ed eolica è fondamentale. Gli sviluppi nell’accumulo di energia sostenibile a lungo termine, come Ambri, uno spin-off del MIT, potrebbero consentire questa transizione. Ciò richiede investimenti e collaborazioni significativi tra aziende tecnologiche, fornitori di energia e politici.\n\n\n16.14.2 Sfide\nNonostante queste promettenti direzioni, devono essere affrontate diverse sfide. Una delle sfide principali è la necessità di standard e metodologie coerenti per misurare e segnalare l’impatto ambientale dell’IA. Questi metodi devono catturare la complessità dei cicli di vita dei modelli di IA e dell’hardware di sistema. Inoltre, sono necessarie infrastrutture IA e hardware di sistema efficienti e sostenibili dal punto di vista ambientale. Ciò è costituito da tre componenti:\n\nMassimizzare l’utilizzo delle risorse di acceleratore e sistema.\nProlungare la durata di vita delle infrastrutture IA.\nProgettare hardware di sistema tenendo presente l’impatto ambientale.\n\nDal lato software, dovremmo bilanciare la sperimentazione e il conseguente costo di training. Tecniche come la ricerca dell’architettura neurale e l’ottimizzazione degli iperparametri possono essere utilizzate per l’esplorazione dello spazio di progettazione. Tuttavia, queste sono spesso molto dispendiose in termini di risorse. Una sperimentazione efficiente può ridurre significativamente l’impatto ambientale. Successivamente, dovrebbero essere esplorati metodi per ridurre gli sforzi di training sprecati.\nPer migliorare la qualità del modello, spesso ridimensioniamo il set di dati. Tuttavia, le maggiori risorse di sistema richieste per l’archiviazione e l’ingestione dei dati causate da questa scalabilità hanno un impatto ambientale significativo (Wu et al. 2022). È importante comprendere a fondo la velocità con cui i dati perdono il loro valore predittivo e ideare strategie di campionamento dei dati.\n\nWu, Carole-Jean, Ramya Raghavendra, Udit Gupta, Bilge Acun, Newsha Ardalani, Kiwan Maeng, Gloria Chang, et al. 2022. «Sustainable ai: Environmental implications, challenges and opportunities». Proceedings of Machine Learning and Systems 4: 795–813.\nAnche le lacune nei dati rappresentano una sfida significativa. Senza aziende e governi che condividono apertamente dati dettagliati e accurati sul consumo di energia, sulle emissioni di carbonio e su altri impatti ambientali, non è facile sviluppare strategie efficaci per un’IA sostenibile.\nInfine, il ritmo rapido dello sviluppo dell’IA richiede un approccio agile alla politica imposta a questi sistemi. La politica dovrebbe garantire uno sviluppo sostenibile senza limitare l’innovazione. Ciò richiede che esperti in tutti i settori dell’IA, delle scienze ambientali, dell’energia e della politica lavorino insieme per raggiungere un futuro sostenibile.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>IA Sostenibile</span>"
    ]
  },
  {
    "objectID": "contents/core/sustainable_ai/sustainable_ai.it.html#conclusione",
    "href": "contents/core/sustainable_ai/sustainable_ai.it.html#conclusione",
    "title": "16  IA Sostenibile",
    "section": "16.15 Conclusione",
    "text": "16.15 Conclusione\nDobbiamo affrontare le considerazioni sulla sostenibilità man mano che l’intelligenza artificiale si espande rapidamente nei settori e nella società. L’intelligenza artificiale promette innovazioni rivoluzionarie, ma il suo impatto ambientale minaccia la sua crescita diffusa. Questo capitolo analizza molteplici aspetti, dall’energia e dalle emissioni agli impatti sui rifiuti e sulla biodiversità, che gli sviluppatori di intelligenza artificiale/apprendimento automatico devono valutare quando creano sistemi di IA responsabili.\nFondamentalmente, abbiamo bisogno di elevare la sostenibilità a priorità di progettazione primaria piuttosto che a un ripensamento. Tecniche come modelli ad alta efficienza energetica, data center alimentati da fonti rinnovabili e programmi di riciclaggio dell’hardware offrono soluzioni, ma l’impegno olistico rimane fondamentale. Abbiamo bisogno di standard in materia di trasparenza, contabilità del carbonio e divulgazioni della catena di fornitura per integrare i guadagni tecnici. Tuttavia, esempi come le pratiche di efficienza 4M di Google contenenti l’uso di energia ML evidenziano che possiamo far progredire l’intelligenza artificiale di pari passo con gli obiettivi ambientali con uno sforzo concertato. Raggiungiamo questo equilibrio armonioso facendo collaborare ricercatori, aziende, regolatori e utenti in tutti i domini. L’obiettivo non è soluzioni perfette, ma un miglioramento continuo mentre integriamo l’IA in nuovi settori.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>IA Sostenibile</span>"
    ]
  },
  {
    "objectID": "contents/core/sustainable_ai/sustainable_ai.it.html#sec-sustainable-ai-resource",
    "href": "contents/core/sustainable_ai/sustainable_ai.it.html#sec-sustainable-ai-resource",
    "title": "16  IA Sostenibile",
    "section": "16.16 Risorse",
    "text": "16.16 Risorse\nEcco un elenco curato di risorse per supportare studenti e insegnanti nei loro percorsi di apprendimento e insegnamento. Lavoriamo continuamente per espandere questa raccolta e presto aggiungeremo nuovi esercizi.\n\n\n\n\n\n\nSlide\n\n\n\n\n\nQueste slide sono uno strumento prezioso per gli insegnanti per tenere lezioni e per gli studenti per rivedere il materiale secondo il proprio ritmo. Incoraggiamo studenti e docenti a sfruttare queste slide per migliorare la loro comprensione e facilitare un trasferimento efficace delle conoscenze.\n\nTransparency and Sustainability.\nSustainability of TinyML.\nModel Cards for Transparency.\n\n\n\n\n\n\n\n\n\n\nVideo\n\n\n\n\n\n\nProssimamente.\n\n\n\n\n\n\n\n\n\n\nEsercizi\n\n\n\n\n\nPer rafforzare i concetti trattati in questo capitolo, abbiamo curato una serie di esercizi che sfidano gli studenti ad applicare le proprie conoscenze e ad approfondire la propria comprensione.\n\nEsercizio 16.1\nEsercizio 16.2",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>IA Sostenibile</span>"
    ]
  },
  {
    "objectID": "contents/core/robust_ai/robust_ai.it.html",
    "href": "contents/core/robust_ai/robust_ai.it.html",
    "title": "17  IA Robusta",
    "section": "",
    "text": "17.1 Panoramica\nPer IA robusta si intende la capacità di un sistema di mantenere le proprie prestazioni e affidabilità anche in presenza di errori. Un sistema di apprendimento automatico robusto è progettato per essere tollerante ai guasti e resiliente agli errori, in grado di funzionare efficacemente anche in condizioni avverse.\nMan mano che i sistemi ML diventano sempre più integrati in vari aspetti della nostra vita, dai servizi basati su cloud ai dispositivi edge e ai sistemi embedded, l’impatto dei guasti hardware e software sulle loro prestazioni e affidabilità diventa più significativo. In futuro, man mano che i sistemi ML diventano più complessi e vengono implementati in applicazioni ancora più critiche, la necessità di progetti robusti e tolleranti ai guasti sarà fondamentale.\nSi prevede che i sistemi ML svolgeranno ruoli cruciali nei veicoli autonomi, nelle città intelligenti, nell’assistenza sanitaria e nei domini dell’automazione industriale. In questi domini, le conseguenze dei guasti hardware o software possono essere gravi, potenzialmente causa di perdita di vite umane, danni economici o danni ambientali.\nI ricercatori e gli ingegneri devono concentrarsi sullo sviluppo di tecniche avanzate per il rilevamento, l’isolamento e il ripristino dei guasti per mitigare questi rischi e garantire il funzionamento affidabile dei futuri sistemi ML.\nQuesto capitolo si concentrerà in modo specifico su tre categorie principali di guasti ed errori che possono influire sulla robustezza dei sistemi ML: guasti hardware, guasti software ed errori umani.\nLe sfide e gli approcci specifici per ottenere la robustezza possono variare a seconda della scala e dei vincoli del sistema ML. I sistemi di cloud computing o data center su larga scala possono concentrarsi sulla tolleranza ai guasti e sulla resilienza tramite ridondanza, elaborazione distribuita e tecniche avanzate di rilevamento e correzione degli errori. Al contrario, i dispositivi edge con risorse limitate o i sistemi embedded affrontano sfide uniche a causa della potenza di calcolo, della memoria e delle risorse energetiche limitate.\nIndipendentemente dalla scala e dai vincoli, le caratteristiche chiave di un sistema ML robusto includono tolleranza ai guasti, resilienza agli errori e mantenimento delle prestazioni. Comprendendo e affrontando le sfide multiformi alla robustezza, possiamo sviluppare sistemi ML affidabili e sicuri in grado di navigare nelle complessità degli ambienti del mondo reale.\nQuesto capitolo non riguarda solo l’esplorazione di strumenti, framework e tecniche dei sistemi ML per rilevare e mitigare guasti, attacchi e cambiamenti durante la distribuzione. Si tratta di sottolineare il ruolo cruciale di ognuno di nel dare priorità alla resilienza durante tutto il ciclo di vita dello sviluppo dell’IA, dalla raccolta dati e dall’addestramento del modello all’implementazione e al monitoraggio. Affrontando in modo proattivo le sfide alla robustezza, possiamo sbloccare il pieno potenziale delle tecnologie ML garantendone al contempo un’implementazione sicura, affidabile e responsabile nelle applicazioni del mondo reale.\nMentre l’IA continua a plasmare il nostro futuro, il potenziale delle tecnologie ML è immenso. Ma è solo quando creiamo sistemi resilienti in grado di resistere alle sfide del mondo reale che possiamo davvero sfruttare questo potenziale. Questo è un fattore determinante per il successo e l’impatto sociale di questa tecnologia trasformativa ed è alla nostra portata.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>IA Robusta</span>"
    ]
  },
  {
    "objectID": "contents/core/robust_ai/robust_ai.it.html#panoramica",
    "href": "contents/core/robust_ai/robust_ai.it.html#panoramica",
    "title": "17  IA Robusta",
    "section": "",
    "text": "Guasti Hardware: Guasti transitori, permanenti e intermittenti possono influire sui componenti hardware di un sistema ML, corrompendo i calcoli e degradando le prestazioni.\nRobustezza del Modello: I modelli ML possono essere vulnerabili ad attacchi avversari, avvelenamento dei dati e cambiamenti di distribuzione, che possono indurre classificazioni errate mirate, alterare il comportamento appreso del modello o compromettere l’integrità e l’affidabilità del sistema.\nGuasti software: Bug, difetti di progettazione ed errori di implementazione nei componenti software, come algoritmi, librerie e framework, possono propagare errori e introdurre vulnerabilità.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>IA Robusta</span>"
    ]
  },
  {
    "objectID": "contents/core/robust_ai/robust_ai.it.html#esempi-del-mondo-reale",
    "href": "contents/core/robust_ai/robust_ai.it.html#esempi-del-mondo-reale",
    "title": "17  IA Robusta",
    "section": "17.2 Esempi del mondo reale",
    "text": "17.2 Esempi del mondo reale\nEcco alcuni esempi reali di casi in cui guasti nell’hardware o nel software hanno causato problemi importanti nei sistemi ML in ambienti cloud, edge ed embedded:\n\n17.2.1 Cloud\nNel febbraio 2017, Amazon Web Services (AWS) ha subito un’interruzione significativa a causa di un errore umano durante la manutenzione. Un tecnico ha inserito inavvertitamente un comando errato, causando la disconnessione di molti server. Questa interruzione ha interrotto molti servizi AWS, tra cui l’assistente basato sull’intelligenza artificiale di Amazon, Alexa. Di conseguenza, i dispositivi basati su Alexa, come Amazon Echo e prodotti di terze parti che utilizzano Alexa Voice Service, non hanno potuto rispondere alle richieste degli utenti per diverse ore. Questo incidente evidenzia il potenziale impatto degli errori umani sui sistemi ML basati su cloud e la necessità di procedure di manutenzione robuste e meccanismi di sicurezza.\nIn un altro esempio (Vangal et al. 2021), Facebook ha riscontrato un problema di “silent data corruption (SDC)” [corruzione silenziosa dei dati] all’interno della sua infrastruttura di query distribuita, come mostrato in Figura 17.1. L’infrastruttura di Facebook include un sistema di query che preleva ed esegue query SQL e simili a SQL su più set di dati utilizzando framework come Presto, Hive e Spark. Una delle applicazioni che ha utilizzato questa infrastruttura di query è stata un’applicazione di compressione per ridurre l’ingombro degli archivi dati. In questa applicazione di compressione, i file venivano compressi quando non venivano letti e decompressi quando veniva effettuata una richiesta di lettura. Prima della decompressione, la dimensione del file veniva controllata per assicurarsi che fosse maggiore di zero, indicando un file compresso valido con contenuti.\n\nVangal, Sriram, Somnath Paul, Steven Hsu, Amit Agarwal, Saurabh Kumar, Ram Krishnamurthy, Harish Krishnamurthy, James Tschanz, Vivek De, e Chris H. Kim. 2021. «Wide-Range Many-Core SoC Design in Scaled CMOS: Challenges and Opportunities». IEEE Trans. Very Large Scale Integr. VLSI Syst. 29 (5): 843–56. https://doi.org/10.1109/tvlsi.2021.3061649.\n\n\n\n\n\n\nFigura 17.1: Corruzione silenzioso dei dati nelle applicazioni di database. Fonte: Facebook\n\n\n\nTuttavia, in un caso, quando la dimensione del file veniva calcolata per un file valido di dimensioni diverse da zero, l’algoritmo di decompressione ha richiamato una funzione di potenza dalla libreria Scala. Inaspettatamente, la funzione Scala ha restituito un valore di dimensione zero per il file nonostante avesse una dimensione decompressa nota diversa da zero. Di conseguenza, la decompressione non è stata eseguita e il file non è stato scritto nel database di output. Questo problema si è manifestato sporadicamente, con alcune occorrenze dello stesso calcolo della dimensione del file che restituivano il valore corretto diverso da zero.\nL’impatto di questa corruzione silenziosa dei dati è stato significativo, portando a file mancanti e dati errati nel database di output. L’applicazione che si basava sui file decompressi ha fallito a causa delle incongruenze dei dati. Nel caso di studio presentato nel documento, l’infrastruttura di Facebook, che consiste in centinaia di migliaia di server che gestiscono miliardi di richieste al giorno dalla loro enorme base di utenti, ha riscontrato un problema di corruzione silenziosa dei dati. Il sistema interessato elaborava query utente, caricamenti di immagini e contenuti multimediali, che richiedevano un’esecuzione rapida, affidabile e sicura.\nQuesto caso di studio illustra come la corruzione silenziosa dei dati può propagarsi attraverso più strati di uno stack applicativo, causando perdita di dati e guasti delle applicazioni in un sistema distribuito su larga scala. La natura intermittente del problema e la mancanza di messaggi di errore espliciti lo hanno reso particolarmente difficile da diagnosticare e risolvere. Ma questo non è limitato solo a Meta, anche altre aziende come Google che gestiscono ipercomputer IA affrontano questi problemi. Figura 17.2 Jeff Dean, Chief Scientist presso Google DeepMind e Google Research, parla degli SDC e del loro impatto sui sistemi di apprendimento automatico.\n\n\n\n\n\n\nFigura 17.2: Gli errori “Silent data corruption (SDC)” sono un problema importante per gli ipercomputer di IA. Fonte: Jeff Dean at MLSys 2024, Keynote (Google)\n\n\n\n\n\n17.2.2 Edge\nPer quanto riguarda esempi di guasti ed errori nei sistemi edge ML, un’area che ha ricevuto notevole attenzione è il dominio delle auto a guida autonoma. I veicoli a guida autonoma si basano in larga misura su algoritmi di apprendimento automatico per la percezione, il processo decisionale e il controllo, rendendoli particolarmente sensibili all’impatto di guasti hardware e software. Negli ultimi anni, diversi incidenti di alto profilo che hanno coinvolto veicoli autonomi hanno evidenziato le sfide e i rischi associati all’implementazione di questi sistemi in ambienti reali.\nA maggio 2016, si è verificato un incidente mortale quando una Tesla Model S con pilota automatico si è schiantata contro un autoarticolato bianco che attraversava l’autostrada. Il sistema Autopilot, che si basava su algoritmi di visione artificiale e apprendimento automatico, non è riuscito a riconoscere il rimorchio bianco sullo sfondo di un cielo luminoso. Il conducente, che secondo quanto riferito stava guardando un film al momento dell’incidente, non è intervenuto in tempo e il veicolo è entrato in collisione con il rimorchio a tutta velocità. Questo incidente ha sollevato preoccupazioni sui limiti dei sistemi di percezione basati sull’intelligenza artificiale e sulla necessità di solidi meccanismi di sicurezza nei veicoli autonomi. Ha inoltre evidenziato l’importanza della consapevolezza del conducente e la necessità di linee guida chiare sull’uso delle funzionalità di guida semi-autonoma, come mostrato in Figura 17.3.\n\n\n\n\n\n\nFigura 17.3: Tesla nell’incidente mortale in California era in modalità Autopilot. Fonte: BBC News\n\n\n\nA marzo 2018, un veicolo di prova a guida autonoma di Uber ha investito e ucciso un pedone che attraversava la strada a Tempe, in Arizona. L’incidente è stato causato da un difetto software nel sistema di riconoscimento degli oggetti del veicolo, che non è riuscito a identificare i pedoni in modo appropriato per evitarli come ostacoli. L’autista di sicurezza, che avrebbe dovuto monitorare il funzionamento del veicolo e intervenire se necessario, è stato trovato distratto durante l’incidente. Questo incidente ha portato ad un’ampia revisione del programma di guida autonoma di Uber e ha sollevato dubbi sulla prontezza della tecnologia dei veicoli autonomi per le strade pubbliche. Ha inoltre sottolineato la necessità di rigorosi test, convalide e misure di sicurezza nello sviluppo e nell’implementazione di sistemi di guida autonoma basati sull’intelligenza artificiale.\nNel 2021, Tesla ha dovuto affrontare un controllo più rigoroso a seguito di diversi incidenti che hanno coinvolto veicoli in modalità Autopilot. Alcuni di questi incidenti sono stati attribuiti a problemi con la capacità del sistema Autopilot di rilevare e rispondere a determinate situazioni stradali, come veicoli di emergenza fermi o ostacoli sulla strada. Ad esempio, nell’aprile 2021, una Tesla Model S si è schiantata contro un albero in Texas, uccidendo due passeggeri. I primi rapporti suggerivano che nessuno si trovasse al posto di guida al momento dell’incidente, sollevando interrogativi sull’uso e il potenziale uso improprio delle funzionalità Autopilot. Questi incidenti evidenziano le sfide in corso nello sviluppo di sistemi di guida autonoma affidabili e robusti e la necessità di normative chiare e di istruzione dei consumatori in merito alle capacità e ai limiti di queste tecnologie.\n\n\n17.2.3 Embedded\nI sistemi embedded, che spesso operano in ambienti con risorse limitate e applicazioni critiche per la sicurezza, hanno da tempo dovuto affrontare sfide legate a guasti hardware e software. Poiché le tecnologie di IA e apprendimento automatico sono sempre più integrate in questi sistemi, il potenziale di guasti ed errori assume nuove dimensioni, con l’aggiunta di complessità degli algoritmi di IA e la natura critica delle applicazioni in cui vengono distribuiti.\nConsideriamo alcuni esempi, a partire dall’esplorazione dello spazio. La missione Mars Polar Lander della NASA nel 1999 ha subito un guasto catastrofico a causa di un errore software nel sistema di rilevamento dell’atterraggio (Figura 17.4). Il software di bordo della navicella spaziale ha interpretato erroneamente il rumore proveniente dall’apertura delle sue gambe di atterraggio come un segnale di atterraggio sulla superficie marziana. Di conseguenza, la navicella ha spento prematuramente i suoi motori, causando lo schianto sulla superficie. Questo incidente evidenzia l’importanza critica di una progettazione software solida e di test approfonditi nei sistemi embedded, in particolare quelli che operano in ambienti remoti e ostili. Poiché le capacità di IA sono integrate nelle future missioni spaziali, garantire l’affidabilità e la tolleranza ai guasti di questi sistemi sarà fondamentale per il successo della missione.\n\n\n\n\n\n\nFigura 17.4: La missione fallita della NASA Mars Polar Lander nel 1999 è costata oltre $200M. Fonte: SlashGear\n\n\n\nTornando sulla Terra, nel 2015, un Boeing 787 Dreamliner ha subito un arresto elettrico completo durante un volo a causa di un bug del software nelle sue unità di controllo del generatore. Questo incidente sottolinea come i guasti software possano avere gravi conseguenze nei sistemi integrati complessi come quelli degli aeromobili. Poiché le tecnologie di IA sono sempre più applicate all’aviazione, come nei sistemi di volo autonomi e nella manutenzione predittiva, garantire la robustezza e l’affidabilità di questi sistemi sarà fondamentale per la sicurezza dei passeggeri.\n\n“Se le quattro unità di controllo del generatore principale (associate ai generatori montati sul motore) fossero accese contemporaneamente, dopo 248 giorni di alimentazione continua, tutte e quattro le GCU entrerebbero in modalità fail-safe contemporaneamente, con conseguente perdita di tutta l’alimentazione elettrica CA indipendentemente dalla fase di volo.” – Direttiva della Federal Aviation Administration (2015)\n\nPoiché le capacità di IA si integrano sempre di più nei sistemi embedded, il potenziale di guasti ed errori diventa più complesso e grave. Si immagini un pacemaker intelligente che ha un improvviso problema tecnico. Un paziente potrebbe morire a causa di tale effetto. Pertanto, gli algoritmi AI, come quelli utilizzati per la percezione, il processo decisionale e il controllo, introducono nuove fonti di potenziali guasti, come problemi relativi ai dati, incertezze del modello e comportamenti inaspettati nei casi limite. Inoltre, la natura opaca di alcuni modelli di IA può rendere difficile identificare e diagnosticare i guasti quando si verificano.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>IA Robusta</span>"
    ]
  },
  {
    "objectID": "contents/core/robust_ai/robust_ai.it.html#guasti-hardware",
    "href": "contents/core/robust_ai/robust_ai.it.html#guasti-hardware",
    "title": "17  IA Robusta",
    "section": "17.3 Guasti Hardware",
    "text": "17.3 Guasti Hardware\nI guasti hardware rappresentano una sfida significativa nei sistemi informatici, inclusi i sistemi tradizionali e ML. Questi guasti si verificano quando componenti fisici, come processori, moduli di memoria, dispositivi di archiviazione o interconnessioni, funzionano male o si comportano in modo anomalo. I guasti hardware possono causare calcoli errati, danneggiamento dei dati, crash del sistema o guasti completi del sistema, compromettendo l’integrità e l’affidabilità dei calcoli eseguiti (Jha et al. 2019). Un guasto completo del sistema si riferisce a una situazione in cui l’intero sistema informatico diventa non reattivo o inutilizzabile a causa di un malfunzionamento hardware critico. Questo tipo di guasto è il più grave, poiché rende il sistema inutilizzabile e può portare alla perdita o al danneggiamento dei dati, richiedendo un intervento manuale per riparare o sostituire i componenti difettosi.\nComprendere la tassonomia dei guasti hardware è essenziale per chiunque lavori con sistemi informatici, in particolare nel contesto dei sistemi ML. I sistemi ML si basano su architetture hardware complesse e calcoli su larga scala per addestrare e distribuire modelli che apprendono dai dati e fanno previsioni o decisioni intelligenti. Tuttavia, i guasti hardware possono introdurre errori e incongruenze nella pipeline MLOps, influenzando l’accuratezza, la robustezza e l’affidabilità dei modelli addestrati (G. Li et al. 2017).\nConoscere i diversi tipi di guasti hardware, i loro meccanismi e il loro potenziale impatto sul comportamento del sistema è fondamentale per sviluppare strategie efficaci per rilevarli, mitigarli e ripristinarli. Questa conoscenza è necessaria per progettare sistemi di elaborazione tolleranti ai guasti, implementare algoritmi ML robusti e garantire l’affidabilità complessiva delle applicazioni basate su ML.\nLe sezioni seguenti esploreranno le tre categorie principali di guasti hardware: transitori, permanenti e intermittenti. Discuteremo le loro definizioni, caratteristiche, cause, meccanismi ed esempi di come si manifestano nei sistemi di elaborazione. Tratteremo anche tecniche di rilevamento e mitigazione specifiche per ogni tipo di guasto.\n\nGuasti Transitori: I guasti transitori sono temporanei e non ricorrenti. Sono spesso causati da fattori esterni come raggi cosmici, interferenze elettromagnetiche o fluttuazioni di potenza. Un esempio comune di guasto transitorio è un bit flip, in cui un singolo bit in una posizione di memoria o registro cambia il suo valore in modo imprevisto. I guasti transitori possono causare calcoli errati o corruzione dei dati, ma non causano danni permanenti all’hardware.\nGuasti permanenti: I guasti permanenti, chiamati anche errori hard, sono irreversibili e persistono nel tempo. Sono in genere causati da difetti fisici o usura dei componenti hardware. Esempi di guasti permanenti includono guasti bloccati, in cui un bit o un segnale è impostato in modo permanente su un valore specifico (ad esempio, sempre 0 o sempre 1) e guasti del dispositivo, come un processore malfunzionante o un modulo di memoria danneggiato. I guasti permanenti possono causare un guasto completo del sistema o un significativo degrado delle prestazioni.\nGuasti Intermittenti: I guasti intermittenti sono guasti ricorrenti che compaiono e scompaiono in modo intermittente. Condizioni hardware instabili, come connessioni allentate, componenti obsoleti o difetti di fabbricazione, spesso ne sono la causa. I guasti intermittenti possono essere difficili da diagnosticare e riprodurre perché possono verificarsi sporadicamente e in condizioni specifiche. Esempi includono cortocircuiti intermittenti o problemi di resistenza dei contatti. I guasti intermittenti possono portare a un comportamento imprevedibile del sistema e a errori intermittenti.\n\nAlla fine di questa discussione, i lettori avranno una solida comprensione della tassonomia dei guasti e della sua rilevanza per i sistemi di elaborazione e ML tradizionali. Questa base li aiuterà a prendere decisioni informate durante la progettazione, l’implementazione e la distribuzione di soluzioni tolleranti ai guasti, migliorando l’affidabilità e la credibilità dei loro sistemi di elaborazione e delle applicazioni ML.\n\n17.3.1 Guasti Transitori\nI guasti transitori nell’hardware possono manifestarsi in varie forme, ciascuna con le sue caratteristiche e cause uniche. Questi guasti sono di natura temporanea e non causano danni permanenti ai componenti hardware.\n\nDefinizione e Caratteristiche\nAlcuni dei tipi comuni di guasti transitori includono Single Event Upset (SEU) causati da radiazioni ionizzanti, fluttuazioni di tensione (Reddi e Gupta 2013) dovute a rumore dell’alimentatore o interferenze elettromagnetiche, “Electromagnetic Interference (EMI)” indotte da campi elettromagnetici esterni, “Electrostatic Discharge (ESD)” risultanti da un improvviso flusso di elettricità statica, diafonia causata da accoppiamento di segnali involontari, rimbalzo di massa innescato dalla commutazione simultanea di più uscite, violazioni di temporizzazione dovute a violazioni dei vincoli di temporizzazione del segnale ed errori soft nella logica combinatoria che influenzano l’uscita dei circuiti logici (Mukherjee, Emer, e Reinhardt 2005). Comprendere questi diversi tipi di guasti transitori è fondamentale per progettare sistemi hardware robusti e resilienti che possano mitigarne l’impatto e garantire un funzionamento affidabile.\n\nReddi, Vijay Janapa, e Meeta Sharma Gupta. 2013. Resilient Architecture Design for Voltage Variation. Springer International Publishing. https://doi.org/10.1007/978-3-031-01739-1.\n\nMukherjee, S. S., J. Emer, e S. K. Reinhardt. 2005. «The Soft Error Problem: An Architectural Perspective». In 11th International Symposium on High-Performance Computer Architecture, 243–47. IEEE; IEEE. https://doi.org/10.1109/hpca.2005.37.\nTutti questi guasti transitori sono caratterizzati dalla loro breve durata e dalla loro natura non permanente. Non persistono né lasciano alcun impatto duraturo sull’hardware. Tuttavia, possono comunque portare a calcoli errati, corruzione dei dati o comportamento scorretto del sistema se non gestiti correttamente.\n\n\n\nCause di Guasti Transitori\nI guasti transitori possono essere attribuiti a vari fattori esterni. Una causa comune sono i raggi cosmici, particelle ad alta energia provenienti dallo spazio. Quando queste particelle colpiscono aree sensibili dell’hardware, come celle di memoria o transistor, possono indurre disturbi di carica che alterano i dati memorizzati o trasmessi. Ciò è illustrato in Figura 17.5. Un’altra causa di guasti transitori è l’electromagnetic interference (EMI) [interferenza elettromagnetica] da dispositivi vicini o fluttuazioni di potenza. L’EMI può accoppiarsi con i circuiti e causare picchi di tensione o glitch che interrompono temporaneamente il normale funzionamento dell’hardware.\n\n\n\n\n\n\nFigura 17.5: Meccanismo di Occorrenza di Guasti Transitori Hardware. Fonte: NTT\n\n\n\n\n\nMeccanismi di Guasti Transitori\nI guasti transitori possono manifestarsi attraverso meccanismi diversi a seconda del componente hardware interessato. Nei dispositivi di memoria come DRAM o SRAM, i guasti transitori spesso portano a inversioni di bit, in cui un singolo bit cambia il suo valore da 0 a 1 o viceversa. Ciò può corrompere i dati o le istruzioni archiviati. Nei circuiti logici, i guasti transitori possono causare glitch o picchi di tensione che si propagano attraverso la logica combinatoria, con conseguenti output o segnali di controllo errati. I guasti transitori possono anche influenzare i canali di comunicazione, causando errori di bit o perdite di pacchetti durante la trasmissione dei dati.\n\n\nImpatto sui Sistemi ML\nUn esempio comune di guasto transitorio è un’inversione di bit nella memoria principale. Se una struttura dati importante o un’istruzione critica viene archiviata nella posizione di memoria interessata, può portare a calcoli errati o a un comportamento errato del programma. Se si verifica un guasto transitorio nella memoria che archivia i pesi o i gradienti del modello. Ad esempio, un bit flip nella memoria che memorizza un contatore di loop può causare l’esecuzione indefinita del loop o la sua terminazione prematura. Errori transitori nei registri di controllo o nei bit di flag possono alterare il flusso di esecuzione del programma, causando salti imprevisti o decisioni di diramazione errate. Nei sistemi di comunicazione, gli errori transitori possono danneggiare i pacchetti di dati trasmessi, causando ritrasmissioni o perdita di dati.\nNei sistemi ML, gli errori transitori possono avere implicazioni significative durante la fase di training (He et al. 2023). Il training ML comporta calcoli iterativi e aggiornamenti dei parametri del modello basati su grandi set di dati. Se si verifica un errore transitorio nella memoria dei pesi o dei gradienti del modello, può causare aggiornamenti errati e compromettere la convergenza e l’accuratezza del processo di training. Figura 17.6 mostra un esempio concreto tratto dalla flotta di produzione di Google, in cui un’anomalia SDC ha causato una differenza significativa nella norma del gradiente.\n\n\n\n\n\n\nFigura 17.6: SDC nella fase di training ML determina anomalie nella norma del gradiente. Fonte: Jeff Dean, MLSys 2024 Keynote (Google)\n\n\n\nAd esempio, un’inversione di bit nella matrice dei pesi di una rete neurale può far sì che il modello apprenda pattern o associazioni errati, con conseguente peggioramento delle prestazioni (Wan et al. 2021). Errori transitori nella pipeline dei dati, come la corruzione dei campioni di training o delle etichette, possono anche introdurre rumore e influire sulla qualità del modello appreso.\n\nWan, Zishen, Aqeel Anwar, Yu-Shun Hsiao, Tianyu Jia, Vijay Janapa Reddi, e Arijit Raychowdhury. 2021. «Analyzing and Improving Fault Tolerance of Learning-Based Navigation Systems». In 2021 58th ACM/IEEE Design Automation Conference (DAC), 841–46. IEEE; IEEE. https://doi.org/10.1109/dac18074.2021.9586116.\nDurante la fase di inferenza, gli errori transitori possono influire sull’affidabilità e l’attendibilità delle previsioni ML. Se si verifica un errore transitorio nella memoria dei parametri del modello addestrato o nel calcolo dei risultati dell’inferenza, può portare a previsioni errate o incoerenti. Ad esempio, un’inversione di bit nei valori di attivazione di una rete neurale può alterare l’output finale di classificazione o regressione (Mahmoud et al. 2020).\nNelle applicazioni “safety-critical”, come i veicoli autonomi o la diagnosi medica, i guasti transitori durante l’inferenza possono avere gravi conseguenze, portando a decisioni o azioni errate (G. Li et al. 2017; Jha et al. 2019). Garantire la resilienza dei sistemi ML contro i guasti transitori è fondamentale per mantenere l’integrità e l’affidabilità delle previsioni.\n\nLi, Guanpeng, Siva Kumar Sastry Hari, Michael Sullivan, Timothy Tsai, Karthik Pattabiraman, Joel Emer, e Stephen W. Keckler. 2017. «Understanding error propagation in deep learning neural network (DNN) accelerators and applications». In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, 1–12. ACM. https://doi.org/10.1145/3126908.3126964.\n\nCourbariaux, Matthieu, Itay Hubara, Daniel Soudry, Ran El-Yaniv, e Yoshua Bengio. 2016. «Binarized neural networks: Training deep neural networks with weights and activations constrained to+ 1 or-1». arXiv preprint arXiv:1602.02830.\n\nAygun, Sercan, Ece Olcay Gunes, e Christophe De Vleeschouwer. 2021. «Efficient and robust bitstream processing in binarised neural networks». Electron. Lett. 57 (5): 219–22. https://doi.org/10.1049/ell2.12045.\nAll’altro estremo, in ambienti con risorse limitate come TinyML, le “Binarized Neural Networks [BNNs]” [reti neurali binarizzate] (Courbariaux et al. 2016) sono emerse come una soluzione promettente. Le BNN rappresentano pesi di rete in precisione a bit singolo, offrendo efficienza computazionale e tempi di inferenza più rapidi. Tuttavia, questa rappresentazione binaria rende le BNN fragili agli errori di inversione di bit sui pesi della rete. Ad esempio, lavori precedenti (Aygun, Gunes, e De Vleeschouwer 2021) hanno dimostrato che un’architettura BNN a due strati nascosti per un’attività semplice come la classificazione MNIST subisce un degrado delle prestazioni dal 98% di accuratezza del test al 70% quando vengono inseriti errori soft di inversione di bit casuali tramite pesi del modello con una probabilità del 10%.\nPer affrontare tali problemi è necessario considerare tecniche di training “flip-aware” o sfruttare paradigmi di elaborazione emergenti (ad esempio, elaborazione stocastica) per migliorare la tolleranza ai guasti e la robustezza, di cui parleremo in Sezione 17.3.4. Le direzioni di ricerca future mirano a sviluppare architetture ibride, nuove funzioni di attivazione e funzioni di perdita su misura per colmare il divario di accuratezza rispetto ai modelli a precisione completa mantenendo al contempo la loro efficienza computazionale.\n\n\n\n17.3.2 Guasti Permanenti\nI guasti permanenti sono difetti hardware che persistono e causano danni irreversibili ai componenti interessati. Questi guasti sono caratterizzati dalla loro natura persistente e richiedono la riparazione o la sostituzione dell’hardware difettoso per ripristinare la normale funzionalità del sistema.\n\nDefinizione e Caratteristiche\nI guasti permanenti sono difetti hardware che causano malfunzionamenti persistenti e irreversibili nei componenti interessati. Il componente difettoso rimane non operativo finché un guasto permanente non viene riparato o sostituito. Questi guasti sono caratterizzati dalla loro natura coerente e riproducibile, il che significa che il comportamento difettoso viene osservato ogni volta che il componente interessato viene utilizzato. I guasti permanenti possono avere un impatto su vari componenti hardware, come processori, moduli di memoria, dispositivi di archiviazione o interconnessioni, causando crash del sistema, danneggiamento dei dati o guasto completo del sistema.\nUn esempio notevole di guasto permanente è il bug Intel FDIV, scoperto nel 1994. Il bug FDIV era un difetto in alcune unità di divisione a virgola mobile (FDIV) dei processori Intel Pentium. Il bug causava risultati errati per specifiche operazioni di divisione, portando a calcoli imprecisi.\nIl bug FDIV si è verificato a causa di un errore nella tabella di ricerca utilizzata dall’unità di divisione. In rari casi, il processore recuperava un valore errato dalla tabella di ricerca, con un risultato leggermente meno preciso del previsto. Ad esempio, Figura 17.7 mostra una frazione 4195835/3145727 tracciata su un processore Pentium con l’errore permanente FDIV. Le regioni triangolari sono quelle in cui si sono verificati calcoli errati. Idealmente, tutti i valori corretti verrebbero arrotondati a 1,3338, ma i risultati errati mostrano 1,3337, indicando un errore nella quinta cifra.\nSebbene l’errore fosse piccolo, poteva accumularsi su molte operazioni di divisione, portando a significative imprecisioni nei calcoli matematici. L’impatto del bug FDIV era significativo, soprattutto per le applicazioni che si basavano in modo massiccio sulla divisione precisa in virgola mobile, come simulazioni scientifiche, calcoli finanziari e progettazione assistita da computer. Il bug ha portato a risultati errati, che potrebbero avere gravi conseguenze in settori come la finanza o l’ingegneria.\n\n\n\n\n\n\nFigura 17.7: Processore Intel Pentium con errore permanente FDIV. Le regioni triangolari sono quelle in cui si sono verificati calcoli errati. Fonte: Byte Magazine\n\n\n\nIl bug Intel FDIV è un monito per il potenziale impatto di guasti permanenti sui sistemi ML. Nel contesto del ML, guasti permanenti nei componenti hardware possono portare a calcoli errati, influenzando l’accuratezza e l’affidabilità dei modelli. Ad esempio, se un sistema ML si basa su un processore con un’unità a virgola mobile difettosa, simile al bug Intel FDIV, potrebbe introdurre errori nei calcoli eseguiti durante l’addestramento o l’inferenza.\nQuesti errori possono propagarsi attraverso il modello, portando a previsioni imprecise o apprendimento distorto. Nelle applicazioni in cui il ML viene utilizzato per attività critiche, come la guida autonoma, la diagnosi medica o le previsioni finanziarie, le conseguenze di calcoli errati dovuti a guasti permanenti possono essere gravi.\nÈ fondamentale che i professionisti del ML siano consapevoli del potenziale impatto dei guasti permanenti e incorporino tecniche di tolleranza ai guasti, come ridondanza hardware, meccanismi di rilevamento e correzione degli errori e progettazione di algoritmi robusti, per mitigare i rischi associati a questi guasti. Inoltre, test approfonditi e convalida dei componenti hardware ML possono aiutare a identificare e risolvere i guasti permanenti prima che influiscano sulle prestazioni e l’affidabilità del sistema.\n\n\nCause dei Guasti Permanenti\nI guasti permanenti possono derivare da diverse cause, tra cui difetti di fabbricazione e meccanismi di usura. I difetti di fabbricazione sono difetti intrinseci introdotti durante il processo di fabbricazione dei componenti hardware. Questi difetti includono incisione impropria, doping non corretto o contaminazione, che portano a componenti non funzionali o parzialmente funzionali.\nD’altro canto, i meccanismi di usura si verificano nel tempo man mano che i componenti hardware sono sottoposti a un uso prolungato e a stress. Fattori come elettromigrazione, rottura dell’ossido o stress termico possono causare una graduale degradazione dei componenti, portando infine a guasti permanenti.\n\n\nMeccanismi dei Guasti Permanenti\nI guasti permanenti possono manifestarsi attraverso vari meccanismi, a seconda della natura e della posizione del guasto. Gli “Stuck-at fault” [guasti bloccati] (Seong et al. 2010) sono guasti permanenti comuni in cui un segnale o una cella di memoria rimane fissata a un valore particolare (0 o 1) indipendentemente dagli input, come illustrato in Figura 17.8.\n\nSeong, Nak Hee, Dong Hyuk Woo, Vijayalakshmi Srinivasan, Jude A. Rivers, e Hsien-Hsin S. Lee. 2010. «SAFER: Stuck-at-fault Error Recovery for Memories». In 2010 43rd Annual IEEE/ACM International Symposium on Microarchitecture, 115–24. IEEE; IEEE. https://doi.org/10.1109/micro.2010.46.\n\n\n\n\n\n\nFigura 17.8: Modello di Guasto Bloccato nei Circuiti Digitali. Fonte: Accendo Reliability\n\n\n\nI guasti bloccati possono verificarsi in porte logiche, celle di memoria o interconnessioni, causando calcoli errati o corruzione dei dati. Un altro meccanismo sono i guasti del dispositivo, in cui un componente, come un transistor o una cella di memoria, cessa completamente di funzionare. Ciò può essere dovuto a difetti di fabbricazione o grave usura. I guasti di “bridging” si verificano quando due o più linee di segnale sono collegate involontariamente, causando cortocircuiti o un comportamento logico errato.\nOltre ai guasti stuck-at, ci sono diversi altri tipi di guasti permanenti che possono influenzare i circuiti digitali e che possono avere un impatto su un sistema ML. I guasti di ritardo possono causare il superamento del limite specificato del ritardo di propagazione di un segnale, portando a violazioni di temporizzazione. I guasti di interconnessione, come guasti aperti (fili rotti), guasti resistivi (resistenza aumentata) o guasti capacitivi (capacità aumentata), possono causare problemi di integrità del segnale o violazioni di temporizzazione. Le celle di memoria possono anche subire vari guasti, tra cui guasti di transizione (impossibilità di cambiare stato), guasti di accoppiamento (interferenza tra celle adiacenti) e guasti sensibili al pattern di vicinato (guasti che dipendono dai valori delle celle vicine). Altri guasti permanenti possono verificarsi nella rete di alimentazione o nella rete di distribuzione del clock, influenzando la funzionalità e la temporizzazione del circuito.\n\n\nImpatto sui Sistemi ML\nI guasti permanenti possono influire gravemente sul comportamento e l’affidabilità dei sistemi di elaborazione. Ad esempio, un guasto nell’unità logica aritmetica (ALU) di un processore può causare calcoli errati, portando a risultati errati o crash del sistema. Un guasto permanente in un modulo di memoria, in una specifica cella di memoria, può danneggiare i dati archiviati, causando la perdita di dati o un comportamento errato del programma. Nei dispositivi di archiviazione, guasti permanenti come settori danneggiati o guasti del dispositivo possono causare l’inaccessibilità dei dati o la perdita completa delle informazioni archiviate. I guasti permanenti di interconnessione possono interrompere i canali di comunicazione, causando il danneggiamento dei dati o il blocco del sistema.\nI guasti permanenti possono influire significativamente sui sistemi ML durante le fasi di addestramento e inferenza. Durante l’addestramento, guasti permanenti nelle unità di elaborazione o nella memoria possono causare calcoli errati, con conseguenti modelli danneggiati o non ottimali (He et al. 2023). Inoltre, i guasti nei dispositivi di archiviazione possono corrompere i dati di training o i parametri del modello archiviati, causando la perdita di dati o incongruenze del modello (He et al. 2023).\n\nZhang, Jeff Jun, Tianyu Gu, Kanad Basu, e Siddharth Garg. 2018. «Analyzing and mitigating the impact of permanent faults on a systolic array based neural network accelerator». In 2018 IEEE 36th VLSI Test Symposium (VTS), 1–6. IEEE; IEEE. https://doi.org/10.1109/vts.2018.8368656.\nDurante l’inferenza, i guasti permanenti possono influire sull’affidabilità e la correttezza delle previsioni ML. I guasti nelle unità di elaborazione possono produrre risultati errati o causare guasti del sistema, mentre i guasti nella memoria che archivia i parametri del modello possono portare all’utilizzo di modelli corrotti o obsoleti per l’inferenza (J. J. Zhang et al. 2018).\nPer mitigare l’impatto dei guasti permanenti nei sistemi ML, devono essere impiegate tecniche di tolleranza ai guasti sia a livello hardware che software. La ridondanza hardware, come la duplicazione di componenti critici o l’utilizzo di codici di correzione degli errori (Kim, Sullivan, e Erez 2015), può aiutare a rilevare e ripristinare i guasti permanenti. Le tecniche software, come i meccanismi di checkpoint e riavvio (Egwutuoha et al. 2013), possono consentire al sistema di recuperare da guasti permanenti tornando a uno stato salvato in precedenza. Il monitoraggio, il test e la manutenzione regolari dei sistemi ML possono aiutare a identificare e sostituire i componenti difettosi prima che causino interruzioni significative.\n\nKim, Jungrae, Michael Sullivan, e Mattan Erez. 2015. «Bamboo ECC: Strong, safe, and flexible codes for reliable computer memory». In 2015 IEEE 21st International Symposium on High Performance Computer Architecture (HPCA), 101–12. IEEE; IEEE. https://doi.org/10.1109/hpca.2015.7056025.\n\nEgwutuoha, Ifeanyi P., David Levy, Bran Selic, e Shiping Chen. 2013. «A survey of fault tolerance mechanisms and checkpoint/restart implementations for high performance computing systems». The Journal of Supercomputing 65 (3): 1302–26. https://doi.org/10.1007/s11227-013-0884-0.\nProgettare sistemi ML tenendo a mente la tolleranza ai guasti è fondamentale per garantirne l’affidabilità e la robustezza in presenza di guasti permanenti. Ciò può comportare l’incorporazione di ridondanza, meccanismi di rilevamento e correzione degli errori e strategie di sicurezza nell’architettura del sistema. Affrontando in modo proattivo le sfide poste dai guasti permanenti, i sistemi ML possono mantenere la loro integrità, accuratezza e affidabilità, anche di fronte a guasti hardware.\n\n\n\n17.3.3 Guasti Intermittenti\nI guasti intermittenti sono guasti hardware che si verificano sporadicamente e in modo imprevedibile in un sistema. Un esempio è illustrato in Figura 17.9, dove le crepe nel materiale possono introdurre una maggiore resistenza [elettrica] nei circuiti. Questi guasti sono particolarmente difficili da rilevare e diagnosticare perché compaiono e scompaiono in modo intermittente, rendendo difficile riprodurre e isolare la causa principale. I guasti intermittenti possono causare instabilità del sistema, corruzione dei dati e degrado delle prestazioni.\n\n\n\n\n\n\nFigura 17.9: Maggiore resistenza dovuta a un guasto intermittente, ovvero una crepa tra la protuberanza di rame e la saldatura del package. Fonte: Constantinescu\n\n\n\n\nDefinizione e Caratteristiche\nI guasti intermittenti sono caratterizzati dalla loro natura sporadica e non deterministica. Si verificano in modo irregolare e possono apparire e scomparire spontaneamente, con durate e frequenze variabili. Questi guasti non si manifestano in modo coerente ogni volta che viene utilizzato il componente interessato, il che li rende più difficili da rilevare rispetto ai guasti permanenti. I guasti intermittenti possono interessare vari componenti hardware, tra cui processori, moduli di memoria, dispositivi di archiviazione o interconnessioni. Possono causare errori transitori, danneggiamento dei dati o comportamento imprevisto del sistema.\nI guasti intermittenti possono avere un impatto significativo sul comportamento e l’affidabilità dei sistemi di elaborazione (Rashid, Pattabiraman, e Gopalakrishnan 2015). Ad esempio, un guasto intermittente nella logica di controllo di un processore può causare un flusso di programma irregolare, portando a calcoli errati o blocchi del sistema. I guasti intermittenti nei moduli di memoria possono danneggiare i valori dei dati, con conseguente esecuzione errata del programma o incoerenze nei dati. Nei dispositivi di archiviazione, i guasti intermittenti possono causare errori di lettura/scrittura o perdita di dati. Errori intermittenti nei canali di comunicazione possono causare corruzione dei dati, perdita di pacchetti o problemi di connettività intermittenti. Questi errori possono causare crash del sistema, problemi di integrità dei dati o degrado delle prestazioni, a seconda della gravità e della frequenza degli errori intermittenti.\n\n———. 2015. «Characterizing the Impact of Intermittent Hardware Faults on Programs». IEEE Trans. Reliab. 64 (1): 297–310. https://doi.org/10.1109/tr.2014.2363152.\n\n\nCause degli Errori Intermittenti\nI guasti intermittenti possono derivare da diverse cause, sia interne che esterne, ai componenti hardware (Constantinescu 2008). Una causa comune è l’invecchiamento e l’usura dei componenti. Man mano che i dispositivi elettronici invecchiano, diventano più suscettibili a guasti intermittenti dovuti a meccanismi di degradazione come elettromigrazione, rottura dell’ossido o affaticamento dei giunti di saldatura.\n\nConstantinescu, Cristian. 2008. «Intermittent faults and effects on reliability of integrated circuits». In 2008 Annual Reliability and Maintainability Symposium, 370–74. IEEE; IEEE. https://doi.org/10.1109/rams.2008.4925824.\nAnche difetti di fabbricazione o variazioni di processo possono causare guasti intermittenti, in cui componenti marginali o borderline possono presentare guasti sporadici in condizioni specifiche, come mostrato in Figura 17.10.\n\n\n\n\n\n\nFigura 17.10: Guasto intermittente indotto da residui in un chip DRAM. Fonte: Hynix Semiconductor\n\n\n\nFattori ambientali, come fluttuazioni di temperatura, umidità o vibrazioni, possono innescare guasti intermittenti alterando le caratteristiche elettriche dei componenti. Collegamenti allentati o degradati, come quelli nei connettori o nei circuiti stampati, possono causare guasti intermittenti.\n\n\nMeccanismi dei Guasti Intermittenti\nI guasti intermittenti possono manifestarsi attraverso vari meccanismi, a seconda della causa sottostante e del componente interessato. Un meccanismo è il circuito aperto o cortocircuito intermittente, in cui un percorso o una connessione del segnale viene temporaneamente interrotto o cortocircuitato, causando un comportamento irregolare. Un altro meccanismo è il guasto di ritardo intermittente (J. Zhang et al. 2018), in cui la temporizzazione dei segnali o i ritardi di propagazione diventano incoerenti, causando problemi di sincronizzazione o calcoli errati. I guasti intermittenti possono manifestarsi come bit flip [inversioni] transitori o errori soft nelle celle di memoria o nei registri, causando corruzione dei dati o esecuzione errata del programma.\n\nZhang, Jeff, Kartheek Rangineni, Zahra Ghodsi, e Siddharth Garg. 2018. «ThUnderVolt: Enabling Aggressive Voltage Underscaling and Timing Error Resilience for Energy Efficient Deep Learning Accelerators». In 2018 55th ACM/ESDA/IEEE Design Automation Conference (DAC), 1–6. IEEE. https://doi.org/10.1109/dac.2018.8465918.\n\n\nImpatto sui Sistemi ML\nNel contesto dei sistemi ML, i guasti intermittenti possono introdurre sfide significative e avere un impatto sull’affidabilità e le prestazioni del sistema. Durante la fase di addestramento, i guasti intermittenti nelle unità di elaborazione o nella memoria possono portare a incongruenze nei calcoli, con conseguenti gradienti e aggiornamenti del peso errati o rumorosi. Ciò può influire sulla convergenza e l’accuratezza del processo di addestramento, portando a modelli sub-ottimali o instabili. Errori intermittenti di archiviazione o recupero dei dati possono corrompere i dati di training, introducendo rumore o errori che degradano la qualità dei modelli addestrati (He et al. 2023).\n\nHe, Yi, Mike Hutton, Steven Chan, Robert De Gruijl, Rama Govindaraju, Nishant Patil, e Yanjing Li. 2023. «Understanding and Mitigating Hardware Failures in Deep Learning Training Systems». In Proceedings of the 50th Annual International Symposium on Computer Architecture, 1–16. IEEE; ACM. https://doi.org/10.1145/3579371.3589105.\nDurante la fase di inferenza, gli errori intermittenti possono influire sull’affidabilità e la coerenza delle previsioni ML. Gli errori nelle unità di elaborazione o nella memoria possono causare calcoli errati o corruzione dei dati, portando a previsioni errate o incoerenti. Gli errori intermittenti nella pipeline dei dati possono introdurre rumore o errori nei dati di input, influenzando l’accuratezza e la robustezza delle previsioni. Nelle applicazioni safety-critical, come veicoli autonomi o sistemi di diagnosi medica, gli errori intermittenti possono avere gravi conseguenze, portando a decisioni o azioni errate che compromettono la sicurezza e l’affidabilità.\nPer mitigare l’impatto degli errori intermittenti nei sistemi ML è necessario un approccio poliedrico (Rashid, Pattabiraman, e Gopalakrishnan 2012). A livello hardware, tecniche come pratiche di progettazione robuste, selezione dei componenti e controllo ambientale possono aiutare a ridurre il verificarsi di guasti intermittenti. Meccanismi di ridondanza e correzione degli errori possono essere impiegati per rilevare e ripristinare guasti intermittenti. A livello software, monitoraggio del runtime, rilevamento delle anomalie e tecniche di tolleranza ai guasti possono essere incorporate nella pipeline ML. Ciò può includere tecniche come convalida dei dati, rilevamento di valori anomali, assemblaggio di modelli o adattamento del modello di runtime per gestire con eleganza i guasti intermittenti.\n\nRashid, Layali, Karthik Pattabiraman, e Sathish Gopalakrishnan. 2012. «Intermittent Hardware Errors Recovery: Modeling and Evaluation». In 2012 Ninth International Conference on Quantitative Evaluation of Systems, 220–29. IEEE; IEEE. https://doi.org/10.1109/qest.2012.37.\nProgettare sistemi ML resilienti ai guasti intermittenti è fondamentale per garantirne affidabilità e robustezza. Ciò comporta l’incorporazione di tecniche di tolleranza ai guasti, monitoraggio del runtime e meccanismi adattivi nell’architettura del sistema. Affrontando in modo proattivo le sfide dei guasti intermittenti, i sistemi ML possono mantenere la loro accuratezza, coerenza e affidabilità, anche in caso di guasti hardware sporadici. Test, monitoraggio e manutenzione regolari dei sistemi ML possono aiutare a identificare e mitigare i guasti intermittenti prima che causino interruzioni significative o un degrado delle prestazioni.\n\n\n\n17.3.4 Rilevamento e Mitigazione\nQuesta sezione esplora varie tecniche di rilevamento degli errori, inclusi approcci a livello hardware e software, e discute strategie di mitigazione efficaci per migliorare la resilienza dei sistemi ML. Inoltre, esamineremo le considerazioni sulla progettazione di sistemi ML resilienti, presenteremo casi di studio ed esempi e metteremo in evidenza le future direzioni di ricerca nei sistemi ML tolleranti agli errori.\n\nTecniche di Rilevamento degli Errori\nLe tecniche di rilevamento degli errori sono importanti per identificare e localizzare gli errori hardware nei sistemi ML. Queste tecniche possono essere ampiamente categorizzate in approcci a livello hardware e software, ognuno dei quali offre capacità e vantaggi unici.\n\nRilevamento degli errori a livello hardware\nLe tecniche di rilevamento degli errori a livello hardware sono implementate a livello fisico del sistema e mirano a identificare gli errori nei componenti hardware sottostanti. Esistono diverse tecniche hardware, ma in generale, possiamo raggruppare questi diversi meccanismi nelle seguenti categorie.\nBuilt-in self-test (BIST) mechanisms: BIST è una tecnica potente per rilevare guasti nei componenti hardware (Bushnell e Agrawal 2002). Comporta l’incorporazione di circuiti hardware aggiuntivi nel sistema per l’autotest e il rilevamento dei guasti. BIST può essere applicato a vari componenti, come processori, moduli di memoria o circuiti integrati specifici per applicazione (ASIC). Ad esempio, BIST può essere implementato in un processore utilizzando catene di scansione, che sono percorsi dedicati che consentono l’accesso ai registri interni e alla logica per scopi di test.\n\nBushnell, Michael L, e Vishwani D Agrawal. 2002. «Built-in self-test». Essentials of electronic testing for digital, memory and mixed-signal VLSI circuits, 489–548.\nDurante il processo BIST, vengono applicati pattern di test predefiniti ai circuiti interni del processore e le risposte vengono confrontate con i valori previsti. Eventuali discrepanze indicano la presenza di guasti. I processori Xeon di Intel, ad esempio, includono meccanismi BIST per testare i core della CPU, la memoria cache e altri componenti critici durante l’avvio del sistema.\nCodici di rilevamento degli errori: I codici di rilevamento degli errori sono ampiamente utilizzati per rilevare errori di archiviazione e trasmissione dei dati (Hamming 1950). Questi codici aggiungono bit ridondanti ai dati originali, consentendo il rilevamento di errori di bit. Esempio: I controlli di parità sono una forma semplice di codice di rilevamento degli errori mostrato in Figura 17.11. In uno schema di parità a bit singolo, un bit extra viene aggiunto a ogni parola di dati, rendendo il numero di 1 nella parola pari (parità pari) o dispari (parità dispari).\n\nHamming, R. W. 1950. «Error Detecting and Error Correcting Codes». Bell Syst. Tech. J. 29 (2): 147–60. https://doi.org/10.1002/j.1538-7305.1950.tb00463.x.\n\n\n\n\n\n\nFigura 17.11: Esempio di bit di parità. Fonte: Computer Hope\n\n\n\nQuando si leggono i dati, la parità viene controllata e, se non corrisponde al valore previsto, viene rilevato un errore. Codici di rilevamento degli errori più avanzati, come i “cyclic redundancy checks (CRC)” [controlli di ridondanza ciclica], calcolano un checksum in base ai dati e lo aggiungono al messaggio. Il checksum viene ricalcolato all’estremità ricevente e confrontato con il checksum trasmesso per rilevare gli errori. I moduli di memoria con “Error-correcting code (ECC)” [codice di correzione degli errori], comunemente utilizzati nei server e nei sistemi critici, impiegano codici avanzati di rilevamento e correzione degli errori per rilevare e correggere errori a bit singolo o multi-bit nella memoria.\nRidondanza hardware e meccanismi di voto: La ridondanza hardware implica la duplicazione dei componenti critici e il confronto dei loro output per rilevare e mascherare i guasti (Sheaffer, Luebke, e Skadron 2007). I meccanismi di voto, come la “triple modular redundancy (TMR)” [ridondanza modulare tripla], impiegano più istanze di un componente e confrontano i loro output per identificare e mascherare comportamenti difettosi (Arifeen, Hassan, e Lee 2020).\n\nSheaffer, Jeremy W, David P Luebke, e Kevin Skadron. 2007. «A hardware redundancy and recovery mechanism for reliable scientific computation on graphics processors». In Graphics Hardware, 2007:55–64. Citeseer.\n\nArifeen, Tooba, Abdus Sami Hassan, e Jeong-A Lee. 2020. «Approximate Triple Modular Redundancy: A Survey». #IEEE_O_ACC# 8: 139851–67. https://doi.org/10.1109/access.2020.3012673.\n\nYeh, Y. C. 1996. «Triple-triple redundant 777 primary flight computer». In 1996 IEEE Aerospace Applications Conference. Proceedings, 1:293–307. IEEE; IEEE. https://doi.org/10.1109/aero.1996.495891.\nIn un sistema TMR, tre istanze identiche di un componente hardware, come un processore o un sensore, eseguono lo stesso calcolo in parallelo. Gli output di queste istanze vengono immessi in un circuito di voto, che confronta i risultati e seleziona il valore di maggioranza come output finale. Se una delle istanze produce un risultato non corretto a causa di un guasto, il meccanismo di voto maschera l’errore e mantiene l’output corretto. Il TMR è comunemente utilizzato nei sistemi aerospaziali e aeronautici, dove l’elevata affidabilità è fondamentale. Ad esempio, l’aereo Boeing 777 impiega il TMR nel suo sistema di computer di volo primario per garantire la disponibilità e la correttezza delle funzioni di controllo del volo (Yeh 1996).\nI computer a guida autonoma di Tesla impiegano un’architettura hardware ridondante per garantire la sicurezza e l’affidabilità delle funzioni critiche, come percezione, processo decisionale e controllo del veicolo, come mostrato in Figura 17.12. Un componente chiave di questa architettura è l’utilizzo della “dual modular redundancy (DMR)” [ridondanza modulare duale] nei sistemi di computer di bordo dell’auto.\n\n\n\n\n\n\nFigura 17.12: Computer Tesla a guida autonoma completa con SoC duali ridondanti. Fonte: Tesla\n\n\n\nNell’implementazione DMR di Tesla, due unità hardware identiche, spesso chiamate “computer ridondanti” o “unità di controllo ridondanti”, eseguono gli stessi calcoli in parallelo (Bannon et al. 2019). Ogni unità elabora in modo indipendente i dati dei sensori, esegue algoritmi di percezione e decisionali e genera comandi di controllo per gli attuatori del veicolo (ad esempio, sterzo, accelerazione e frenata).\n\nBannon, Pete, Ganesh Venkataramanan, Debjit Das Sarma, e Emil Talpes. 2019. «Computer and Redundancy Solution for the Full Self-Driving Computer». In 2019 IEEE Hot Chips 31 Symposium (HCS), 1–22. IEEE Computer Society; IEEE. https://doi.org/10.1109/hotchips.2019.8875645.\nGli output di queste due unità ridondanti vengono costantemente confrontati per rilevare eventuali discrepanze o guasti. Se gli output corrispondono, il sistema presuppone che entrambe le unità funzionino correttamente e i comandi di controllo vengono inviati agli attuatori del veicolo. Tuttavia, se c’è una mancata corrispondenza tra gli output, il sistema identifica un potenziale guasto in una delle unità e adotta le misure appropriate per garantire un funzionamento sicuro.\nIl sistema può impiegare meccanismi aggiuntivi per determinare quale unità è difettosa in una mancata corrispondenza. Ciò può comportare l’utilizzo di algoritmi diagnostici, il confronto degli output con i dati di altri sensori o sottosistemi o l’analisi della coerenza degli output nel tempo. Una volta identificata l’unità difettosa, il sistema può isolarla e continuare a funzionare utilizzando l’output dell’unità non difettosa.\nIl DMR nel computer di guida autonoma di Tesla fornisce un ulteriore livello di sicurezza e tolleranza ai guasti. Avendo due unità indipendenti che eseguono gli stessi calcoli, il sistema può rilevare e mitigare i guasti che possono verificarsi in una delle unità. Questa ridondanza aiuta a prevenire singoli punti di guasto e garantisce che le funzioni critiche rimangano operative nonostante i guasti hardware.\nInoltre, Tesla incorpora anche meccanismi di ridondanza aggiuntivi oltre al DMR. Ad esempio, utilizzano alimentatori ridondanti, sistemi di sterzo e frenata e diverse suite di sensori (ad esempio, telecamere, radar e sensori a ultrasuoni) per fornire più livelli di tolleranza ai guasti. Queste ridondanze contribuiscono collettivamente alla sicurezza e all’affidabilità complessive del sistema di guida autonoma.\nÈ importante notare che mentre DMR fornisce rilevamento guasti e un certo livello di tolleranza ai guasti, TMR può fornire un diverso livello di mascheramento dei guasti. In DMR, se entrambe le unità subiscono guasti simultanei o il guasto influisce sul meccanismo di confronto, il sistema potrebbe non essere in grado di identificare il guasto. Pertanto, gli SDC di Tesla si basano su una combinazione di DMR e altri meccanismi di ridondanza per raggiungere un elevato livello di tolleranza ai guasti.\nL’uso di DMR nel computer a guida autonoma di Tesla evidenzia l’importanza della ridondanza hardware nelle applicazioni critiche per la sicurezza. Utilizzando unità di elaborazione ridondanti e confrontando i loro output, il sistema può rilevare e mitigare i guasti, migliorando la sicurezza e l’affidabilità complessive della funzionalità di guida autonoma.\nGoogle utilizza “hot spare” ridondanti per gestire i problemi SDC nei suoi data center, migliorando così l’affidabilità delle funzioni critiche. Come illustrato in Figura 17.13, durante la normale fase di addestramento, più “worker” di training sincroni funzionano in modo impeccabile. Tuttavia, se un worker diventa difettoso e causa SDC, un verificatore SDC identifica automaticamente i problemi. Dopo aver rilevato l’SDC, il verificatore SDC sposta il training su un hot spare e invia la macchina difettosa per la riparazione. Questa ridondanza salvaguarda la continuità e l’affidabilità del training ML, riducendo al minimo i tempi di inattività e preservando l’integrità dei dati.\n\n\n\n\n\n\nFigura 17.13: Google impiega “core hot spare” per gestire in modo trasparente gli SDC nel data center. Fonte: Jeff Dean, MLSys 2024 Keynote (Google)\n\n\n\nWatchdog timer: I watchdog timer sono componenti hardware che monitorano l’esecuzione di attività o processi critici (Pont e Ong 2002). Sono comunemente utilizzati per rilevare e ripristinare guasti software o hardware che causano la mancata risposta di un sistema o il suo blocco in un ciclo infinito. In un sistema embedded, un watchdog timer può essere configurato per monitorare l’esecuzione del loop principale, come illustrato in Figura 17.14. Il software reimposta periodicamente il watchdog timer per indicare che funziona correttamente. Supponiamo che il software non riesca a reimpostare il timer entro un limite di tempo specificato (periodo di timeout). In tal caso, il watchdog timer presuppone che il sistema abbia riscontrato un guasto e attiva un’azione di ripristino predefinita, come il reset del sistema o il passaggio a un componente di backup. I watchdog timer sono ampiamente utilizzati nell’elettronica automobilistica, nei sistemi di controllo industriale e in altre applicazioni critiche per la sicurezza per garantire il rilevamento e il ripristino tempestivi dai guasti.\n\nPont, Michael J, e Royan HL Ong. 2002. «Using watchdog timers to improve the reliability of single-processor embedded systems: Seven new patterns and a case study». In Proceedings of the First Nordic Conference on Pattern Languages of Programs, 159–200. Citeseer.\n\n\n\n\n\n\nFigura 17.14: Esempio di watchdog timer nel rilevamento di guasti MCU. Fonte: Ablic\n\n\n\n\n\nRilevamento guasti a livello software\nLe tecniche di rilevamento degli errori a livello software si basano su algoritmi software e meccanismi di monitoraggio per identificare gli errori di sistema. Queste tecniche possono essere implementate a vari livelli dello stack software, tra cui il sistema operativo, il middleware o il livello dell’applicazione.\nMonitoraggio del runtime e rilevamento delle anomalie: Il monitoraggio del runtime comporta l’osservazione continua del comportamento del sistema e dei suoi componenti durante l’esecuzione (Francalanza et al. 2017). Aiuta a rilevare anomalie, errori o comportamenti imprevisti che potrebbero indicare la presenza di errori. Ad esempio, si consideri un sistema di classificazione delle immagini basato su ML distribuito in un’auto a guida autonoma. Il monitoraggio del runtime può essere implementato per tracciare le prestazioni e il comportamento del modello di classificazione (Mahmoud et al. 2021).\n\nFrancalanza, Adrian, Luca Aceto, Antonis Achilleos, Duncan Paul Attard, Ian Cassar, Dario Della Monica, e Anna Ingólfsdóttir. 2017. «A foundation for runtime monitoring». In International Conference on Runtime Verification, 8–29. Springer.\n\nMahmoud, Abdulrahman, Siva Kumar Sastry Hari, Christopher W. Fletcher, Sarita V. Adve, Charbel Sakr, Naresh Shanbhag, Pavlo Molchanov, Michael B. Sullivan, Timothy Tsai, e Stephen W. Keckler. 2021. «Optimizing Selective Protection for CNN Resilience». In 2021 IEEE 32nd International Symposium on Software Reliability Engineering (ISSRE), 127–38. IEEE. https://doi.org/10.1109/issre52982.2021.00025.\n\nChandola, Varun, Arindam Banerjee, e Vipin Kumar. 2009. «Anomaly detection: A survey». ACM Comput. Surv. 41 (3): 1–58. https://doi.org/10.1145/1541880.1541882.\nGli algoritmi di rilevamento delle anomalie possono essere applicati alle previsioni del modello o alle attivazioni di livelli intermedi, come il rilevamento statistico di valori anomali o approcci basati sull’apprendimento automatico (ad esempio, One-Class SVM o Autoencoders) (Chandola, Banerjee, e Kumar 2009). Figura 17.15 mostra un esempio di rilevamento delle anomalie. Supponiamo che il sistema di monitoraggio rilevi una deviazione significativa dai pattern previsti, come un calo improvviso dell’accuratezza della classificazione o campioni fuori distribuzione. In tal caso, può generare un “alert” che indica un potenziale errore nel modello o nella pipeline dei dati di input. Questo rilevamento precoce consente di applicare strategie di intervento tempestivo e di mitigazione degli errori.\n\n\n\n\n\n\nFigura 17.15: Esempi di rilevamento delle anomalie. (a) Rilevamento delle anomalie completamente supervisionato, (b) rilevamento delle anomalie solo normali, (c, d, e) rilevamento delle anomalie semi-supervisionato, (f) rilevamento delle anomalie non supervisionato. Fonte: Google\n\n\n\nControlli di coerenza e convalida dei dati: I controlli di coerenza e le tecniche di convalida dei dati garantiscono l’integrità e la correttezza dei dati in diverse fasi di elaborazione in un sistema ML (Lindholm et al. 2019). Questi controlli aiutano a rilevare danneggiamenti dei dati, incongruenze o errori che potrebbero propagarsi e influenzare il comportamento del sistema. Esempio: In un sistema ML distribuito in cui più nodi collaborano per addestrare un modello, è possibile implementare controlli di coerenza per convalidare l’integrità dei parametri condivisi del modello. Ogni nodo può calcolare un checksum o un hash dei parametri del modello prima e dopo l’iterazione di addestramento, come mostrato in Figura 17.15. Eventuali incongruenze o danneggiamenti dei dati possono essere rilevati confrontando i checksum tra i nodi. Inoltre, è possibile applicare controlli di intervallo ai dati di input e agli output del modello per garantire che rientrino nei limiti previsti. Ad esempio, se il sistema di percezione di un veicolo autonomo rileva un oggetto con dimensioni o velocità non realistiche, può indicare un errore nei dati del sensore o negli algoritmi di percezione (Wan et al. 2023).\n\nLindholm, Andreas, Dave Zachariah, Petre Stoica, e Thomas B. Schon. 2019. «Data Consistency Approach to Model Validation». #IEEE_O_ACC# 7: 59788–96. https://doi.org/10.1109/access.2019.2915109.\n\nWan, Zishen, Yiming Gan, Bo Yu, S Liu, A Raychowdhury, e Y Zhu. 2023. «Vpp: The vulnerability-proportional protection paradigm towards reliable autonomous machines». In Proceedings of the 5th International Workshop on Domain Specific System Architecture (DOSSA), 1–6.\n\nKawazoe Aguilera, Marcos, Wei Chen, e Sam Toueg. 1997. «Heartbeat: A timeout-free failure detector for quiescent reliable communication». In Distributed Algorithms: 11th International Workshop, WDAG’97 Saarbrücken, Germany, September 2426, 1997 Proceedings 11, 126–40. Springer.\nMeccanismi di heartbeat e timeout: I meccanismi di heartbeat e timeout sono comunemente utilizzati per rilevare errori nei sistemi distribuiti e garantire la vitalità e la reattività dei componenti (Kawazoe Aguilera, Chen, e Toueg 1997). Sono molto simili ai timer watchdog presenti nell’hardware. Ad esempio, in un sistema ML distribuito, in cui più nodi collaborano per eseguire attività quali pre-elaborazione dei dati, training del modello o inferenza, è possibile implementare meccanismi heartbeat per monitorare lo stato e la disponibilità di ciascun nodo. Ogni nodo invia periodicamente un messaggio heartbeat a un coordinatore centrale o ai suoi nodi peer, indicando il suo stato e la sua disponibilità. Supponiamo che un nodo non riesca a inviare un heartbeat entro un periodo di timeout specificato, come mostrato in Figura 17.16. In tal caso, viene considerato difettoso e possono essere intraprese azioni appropriate, come la ridistribuzione del carico di lavoro o l’avvio di un meccanismo di “failover”. I timeout possono anche essere utilizzati per rilevare e gestire componenti bloccati o non reattivi. Ad esempio, se un processo di caricamento dati supera una soglia di timeout predefinita, potrebbe indicare un errore nella pipeline dati e il sistema può adottare misure correttive.\n\n\n\n\n\n\nFigura 17.16: Messaggi heartbeat nei sistemi distribuiti. Fonte: GeeksforGeeks\n\n\n\n\nTecniche di “Software-implemented fault tolerance (SIFT)”: Le tecniche SIFT introducono meccanismi di ridondanza e rilevamento degli errori a livello software per migliorare l’affidabilità e la tolleranza agli errori del sistema (Reis et al. 2005). Esempio: La programmazione N-version è una tecnica SIFT in cui più versioni di componenti software funzionalmente equivalenti vengono sviluppate in modo indipendente da team diversi. Questo può essere applicato a componenti critici come il motore di inferenza del modello in un sistema ML. Più versioni del motore di inferenza possono essere eseguite in parallelo e i loro output possono essere confrontati per coerenza. È considerato il risultato corretto se la maggior parte delle versioni produce lo stesso output. Se c’è una discrepanza, indica un potenziale errore in una o più versioni e possono essere attivati meccanismi di gestione degli errori appropriati. Un altro esempio è l’utilizzo di codici di correzione degli errori basati su software, come i codici Reed-Solomon (Plank 1997), per rilevare e correggere errori nell’archiviazione o nella trasmissione dei dati, come mostrato in Figura 17.17. Questi codici aggiungono ridondanza ai dati, consentendo di rilevare e correggere determinati errori e migliorare la tolleranza agli errori del sistema.\n\nReis, G. A., J. Chang, N. Vachharajani, R. Rangan, e D. I. August. 2005. «SWIFT: Software Implemented Fault Tolerance». In International Symposium on Code Generation and Optimization, 243–54. IEEE; IEEE. https://doi.org/10.1109/cgo.2005.34.\n\nPlank, James S. 1997. «A tutorial on ReedSolomon coding for fault-tolerance in RAID-like systems». Software: Practice and Experience 27 (9): 995–1012.\n\n\n\n\n\n\nFigura 17.17: Rappresentazione a n bit dei codici Reed-Solomon. Fonte: GeeksforGeeks\n\n\n\n\n\n\n\n\n\nEsercizio 17.1: Rilevamento delle Anomalie\n\n\n\n\n\nIn questo Colab, si svolge il ruolo di un detective di guasti IA! Si costruirà un rilevatore di anomalie basato su autoencoder per individuare gli errori nei dati sulla salute cardiaca. Si scopre come identificare i malfunzionamenti nei sistemi ML, un’abilità fondamentale per creare un’IA affidabile. Utilizzeremo Keras Tuner per mettere a punto l’autoencoder per un rilevamento di guasti di prim’ordine. Questa esperienza si collega direttamente al capitolo Robust AI, dimostrando l’importanza del rilevamento di guasti in applicazioni reali come l’assistenza sanitaria e i sistemi autonomi. Preparatevi a rafforzare l’affidabilità delle creazioni IA!\n\n\n\n\n\n\n\n\n17.3.5 Riepilogo\nTabella 17.1 fornisce un’analisi comparativa estesa di guasti transitori, permanenti e intermittenti. Descrive le caratteristiche o dimensioni primarie che distinguono questi tipi di guasti. Qui, riassumiamo le dimensioni rilevanti che abbiamo esaminato ed esploriamo le sfumature che differenziano i guasti transitori, permanenti e intermittenti in modo più dettagliato.\n\n\n\nTabella 17.1: Confronto tra guasti transitori, permanenti e intermittenti.\n\n\n\n\n\n\n\n\n\n\n\nDimensione\nGuasti Transitori\nGuasti Permanenti\nGuasti intermittenti\n\n\n\n\nDurata\nDi breve durata, temporaneo\nPersistente, rimane fino alla riparazione o alla sostituzione\nSporadica, appare e scompare in modo intermittente\n\n\nPersistenza\nScompare dopo che la condizione di errore è passata\nÈ costantemente presente finché non viene affrontato\nSi ripete in modo irregolare, non sempre presente\n\n\nCause\nFattori esterni (ad esempio, interferenza elettromagnetica raggi cosmici)\nDifetti hardware, danni fisici, usura\nCondizioni hardware instabili, connessioni allentate, componenti obsoleti\n\n\nManifestazione\nBit flip, glitch, danneggiamento temporaneo dei dati\nErrori bloccati, componenti rotti, guasti completi del dispositivo\nBit flip occasionali, problemi di segnale intermittenti, malfunzionamenti sporadici\n\n\nImpatto sui Sistemi ML\nIntroduce errori temporanei o rumore nei calcoli\nCausa errori o guasti costanti, che influiscono sull’affidabilità\nPorta a errori sporadici e imprevedibili, difficili da diagnosticare e mitigare\n\n\nRilevamento\nCodici di rilevamento degli errori, confronto con i valori previsti\nAutotest integrati, codici di rilevamento degli errori, controlli di coerenza\nMonitoraggio delle anomalie, analisi di pattern di errore e correlazioni\n\n\nMitigazione\nCodici di correzione degli errori, ridondanza, checkpoint e riavvio\nRiparazione o sostituzione hardware, ridondanza dei componenti, meccanismi di failover\nProgettazione robusta, controllo ambientale, monitoraggio del runtime, tecniche di tolleranza agli errori",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>IA Robusta</span>"
    ]
  },
  {
    "objectID": "contents/core/robust_ai/robust_ai.it.html#robustezza-del-modello-ml",
    "href": "contents/core/robust_ai/robust_ai.it.html#robustezza-del-modello-ml",
    "title": "17  IA Robusta",
    "section": "17.4 Robustezza del Modello ML",
    "text": "17.4 Robustezza del Modello ML\n\n17.4.1 Attacchi Avversari\n\nDefinizione e Caratteristiche\nGli attacchi avversari mirano a indurre i modelli a fare previsioni errate fornendo loro input ingannevoli appositamente creati (chiamati esempi avversari) (Parrish et al. 2023). Aggiungendo lievi perturbazioni ai dati di input, gli avversari possono “hackerare” il riconoscimento di pattern di un modello e ingannarlo. Si tratta di tecniche sofisticate in cui piccole, spesso impercettibili modifiche ai dati di input possono indurre un modello ML a fare una previsione errata, come mostrato in Figura 17.18.\n\nParrish, Alicia, Hannah Rose Kirk, Jessica Quaye, Charvi Rastogi, Max Bartolo, Oana Inel, Juan Ciro, et al. 2023. «Adversarial Nibbler: A Data-Centric Challenge for Improving the Safety of Text-to-Image Models». ArXiv preprint abs/2305.14384. https://arxiv.org/abs/2305.14384.\n\n\n\n\n\n\nFigura 17.18: Un piccolo rumore avversario aggiunto all’immagine originale può far sì che la rete neurale classifichi l’immagine come un Guacamole anziché come un gatto egiziano. Fonte: Sutanto\n\n\n\nÈ possibile generare prompt che portano a immagini non sicure in modelli testo-immagine come DALLE (Ramesh et al. 2021) o Stable Diffusion (Rombach et al. 2022). Ad esempio, alterando i valori dei pixel di un’immagine, gli aggressori possono ingannare un sistema di riconoscimento facciale facendogli identificare un volto come una persona diversa.\n\nRamesh, Aditya, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, e Ilya Sutskever. 2021. «Zero-Shot Text-to-Image Generation». In Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, a cura di Marina Meila e Tong Zhang, 139:8821–31. Proceedings of Machine Learning Research. PMLR. http://proceedings.mlr.press/v139/ramesh21a.html.\n\nRombach, Robin, Andreas Blattmann, Dominik Lorenz, Patrick Esser, e Bjorn Ommer. 2022. «High-Resolution Image Synthesis with Latent Diffusion Models». In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). IEEE. https://doi.org/10.1109/cvpr52688.2022.01042.\nGli attacchi avversari sfruttano il modo in cui i modelli ML apprendono e prendono decisioni durante l’inferenza. Questi modelli funzionano sul principio di riconoscimento di pattern nei dati. Un avversario crea input speciali con perturbazioni per confondere il riconoscimento degli pattern del modello, in pratica “hackerando” le percezioni del modello.\nGli attacchi avversari rientrano in diversi scenari:\n\nAttacchi Whitebox: L’attaccante conosce perfettamente il funzionamento interno del modello target, inclusi i dati di training, i parametri e l’architettura (Ye e Hamidi 2021). Questo accesso completo crea condizioni favorevoli per gli aggressori per sfruttare le vulnerabilità del modello. L’attaccante può usare debolezze specifiche e sottili per creare esempi avversari efficaci.\nAttacchi Blackbox: A differenza degli attacchi White-box, i Black-box implicano che l’attaccante abbia poca o nessuna conoscenza del modello target (Guo et al. 2019). Per eseguire l’attacco, l’attore avversario deve osservare attentamente il comportamento dell’output del modello.\nAttacchi Greybox: Si collocano tra gli attacchi Blackbox e Whitebox. L’attaccante ha solo una conoscenza parziale della progettazione interna del modello target (Xu et al. 2021). Ad esempio, l’attaccante potrebbe avere conoscenza dei dati di training ma non dell’architettura o dei parametri. Nel mondo reale, gli attacchi pratici rientrano solitamente nelle categorie black-box o grey-box.\n\n\nYe, Linfeng, e Shayan Mohajer Hamidi. 2021. «Thundernna: A white box adversarial attack». arXiv preprint arXiv:2111.12305.\n\nGuo, Chuan, Jacob Gardner, Yurong You, Andrew Gordon Wilson, e Kilian Weinberger. 2019. «Simple black-box adversarial attacks». In International conference on machine learning, 2484–93. PMLR.\n\nXu, Ying, Xu Zhong, Antonio Jimeno Yepes, e Jey Han Lau. 2021. «Grey-box adversarial attack and defence for sentiment classification». arXiv preprint arXiv:2103.11576.\nIl panorama dei modelli di apprendimento automatico è complesso e ampio, soprattutto data la loro relativamente recente integrazione nelle applicazioni commerciali. Questa rapida adozione, sebbene trasformativa, ha portato alla luce numerose vulnerabilità all’interno di questi modelli. Di conseguenza, sono emersi vari metodi di attacco avversari, ognuno dei quali sfrutta strategicamente diversi aspetti di vari modelli. Di seguito, evidenziamo un sottoinsieme di questi metodi, che mostra la natura multiforme degli attacchi avversari sui modelli di apprendimento automatico:\n\nLe Generative Adversarial Network (GAN) sono modelli di deep learning costituiti da due reti in competizione tra loro: un generatore e un discriminatore (Goodfellow et al. 2020). Il generatore cerca di sintetizzare dati realistici mentre il discriminatore valuta se sono reali o falsi. Le GAN possono essere utilizzate per creare esempi avversari. La rete del generatore è addestrata per produrre input che il modello target classifica erroneamente. Queste immagini generate da GAN possono quindi attaccare un classificatore target o un modello di rilevamento. Il generatore e il modello target sono impegnati in un processo competitivo, con il generatore che migliora continuamente la sua capacità di creare esempi ingannevoli e il modello target che aumenta la sua resistenza a tali esempi. Le reti GAN forniscono un potente framework per la creazione di input avversari complessi e diversificati, dimostrando l’adattabilità dei modelli generativi nel panorama avversario.\nI Transfer Learning Adversarial Attacks [attacchi avversari di apprendimento di trasferimento] sfruttano la conoscenza trasferita da un modello pre-addestrato a un modello target, creando esempi avversari che possono ingannare entrambi i modelli. Questi attacchi rappresentano una preoccupazione crescente, in particolare quando gli avversari hanno conoscenza dell’estrattore di feature ma non hanno accesso alla testa di classificazione (la parte o il layer responsabile della creazione delle classificazioni finali). Denominate “attacchi headless”, queste strategie avversarie trasferibili sfruttano le capacità espressive degli estrattori di feature per creare perturbazioni, senza tenere conto dello spazio delle etichette o dei dati di addestramento. L’esistenza di tali attacchi sottolinea l’importanza di sviluppare difese robuste per le applicazioni di apprendimento tramite trasferimento, soprattutto perché i modelli pre-addestrati sono comunemente utilizzati (Abdelkader et al. 2020).\n\n\nGoodfellow, Ian, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, e Yoshua Bengio. 2020. «Generative adversarial networks». Commun. ACM 63 (11): 139–44. https://doi.org/10.1145/3422622.\n\nAbdelkader, Ahmed, Michael J. Curry, Liam Fowl, Tom Goldstein, Avi Schwarzschild, Manli Shu, Christoph Studer, e Chen Zhu. 2020. «Headless Horseman: Adversarial Attacks on Transfer Learning Models». In ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 3087–91. IEEE. https://doi.org/10.1109/icassp40776.2020.9053181.\n\n\nMeccanismi degli Attacchi Avversari\nAttacchi Basati sul Gradiente\nUna categoria importante di attacchi avversari è quella degli attacchi basati sul gradiente. Questi attacchi sfruttano i gradienti della funzione di perdita del modello ML per creare esempi avversari. Il Fast Gradient Sign Method (FGSM) è una tecnica ben nota in questa categoria. FGSM perturba i dati di input aggiungendo un piccolo rumore nella direzione del gradiente, con l’obiettivo di massimizzare l’errore di previsione del modello. FGSM può generare rapidamente esempi avversari, come mostrato in Figura 17.19, eseguendo un singolo passaggio nella direzione del gradiente.\n\n\n\n\n\n\nFigura 17.19: Attacchi Basati sul Gradiente. Fonte: Ivezic\n\n\n\nUn’altra variante, l’attacco “Projected Gradient Descent (PGD)”, estende FGSM applicando iterativamente la fase di aggiornamento del gradiente, consentendo esempi avversari più raffinati e potenti. L’attacco “Jacobian-based Saliency Map (JSMA)” è un altro approccio basato sul gradiente che identifica le caratteristiche di input più influenti e le perturba per creare esempi avversari.\nAttacchi Basati sull’Ottimizzazione\nQuesti attacchi formulano la generazione di esempi avversari come un problema di ottimizzazione. L’attacco Carlini e Wagner (C&W) è un esempio importante in questa categoria. Trova la perturbazione più piccola che può causare una classificazione errata mantenendo la somiglianza percettiva con l’input originale. L’attacco C&W impiega un processo di ottimizzazione iterativo per ridurre al minimo la perturbazione massimizzando al contempo l’errore di previsione del modello.\nUn altro approccio basato sull’ottimizzazione è l’Elastic Net Attack to DNNs (EAD), che incorpora la regolarizzazione elastica della rete per generare esempi avversari con perturbazioni sparse.\nAttacchi Basati sul Trasferimento\nGli attacchi basati sul trasferimento sfruttano la proprietà di trasferibilità degli esempi avversari. La trasferibilità si riferisce al fenomeno per cui gli esempi avversari creati per un modello ML possono spesso ingannare altri modelli, anche se hanno architetture diverse o sono stati addestrati su set di dati diversi. Ciò consente agli aggressori di generare esempi avversari utilizzando un modello surrogato e quindi trasferirli al modello target senza richiedere l’accesso diretto ai suoi parametri o gradienti. Gli attacchi basati sul trasferimento evidenziano la generalizzazione delle vulnerabilità avversarie su diversi modelli e il potenziale per attacchi black-box.\nAttacchi nel Mondo Fisico\nGli attacchi nel mondo fisico portano gli esempi avversari nel regno degli scenari del mondo reale. Questi attacchi comportano la creazione di oggetti fisici o manipolazioni che possono ingannare i modelli ML quando vengono catturati da sensori o telecamere. Le patch avversarie, ad esempio, sono piccole patch progettate con cura che possono essere posizionate sugli oggetti per ingannare i modelli di rilevamento o classificazione degli oggetti. Quando vengono applicate a oggetti del mondo reale, queste patch possono causare una classificazione errata dei modelli o il mancato rilevamento accurato degli oggetti. Gli oggetti avversari, come sculture stampate in 3D o segnali stradali modificati, possono anche essere creati per ingannare i sistemi ML in ambienti fisici.\nRiepilogo\nTabella 17.2 una panoramica concisa delle diverse categorie di attacchi avversari, tra cui attacchi basati su gradiente (FGSM, PGD, JSMA), attacchi basati sull’ottimizzazione (C&W, EAD), attacchi basati sul trasferimento e attacchi nel mondo fisico (patch e oggetti avversari). Ogni attacco viene brevemente descritto, evidenziandone le caratteristiche e i meccanismi principali.\n\n\n\nTabella 17.2: Diversi tipi di attacco sui modelli ML.\n\n\n\n\n\n\n\n\n\n\nCategoria di attacco\nNome attacco\nDescrizione\n\n\n\n\nBasato sul gradiente\nFast Gradient Sign Method (FGSM) Projected Gradient Descent (PGD) Jacobian-based Saliency Map Attack (JSMA)\nPerturba i dati di input aggiungendo un piccolo rumore nella direzione del gradiente per massimizzare l’errore di previsione. Estende FGSM applicando iterativamente il passaggio di aggiornamento del gradiente per esempi avversari più raffinati. Identifica le caratteristiche di input influenti e le perturba per creare esempi avversari.\n\n\nBasato sull’ottimizzazione\nCarlini and Wagner (C&W) Attack Elastic Net Attack to DNNs (EAD)\nTrova la perturbazione più piccola che causa una classificazione errata mantenendo la somiglianza percettiva. Incorpora la regolarizzazione elastica della rete per generare esempi avversari con perturbazioni sparse.\n\n\nBasato sul trasferimento\nTransferability-based Attacks\nSfrutta la trasferibilità di esempi avversari su modelli diversi, consentendo attacchi black-box.\n\n\nMondo fisico\nAdversarial Patches Adversarial Objects\nPiccole patch attentamente progettate, posizionate sugli oggetti per ingannare i modelli di rilevamento o classificazione degli oggetti. Oggetti fisici (ad esempio, sculture stampate in 3D, segnali stradali modificati) creati per ingannare i sistemi ML in scenari del mondo reale.\n\n\n\n\n\n\nI meccanismi degli attacchi avversari rivelano l’intricata interazione tra i limiti decisionali del modello ML, i dati di input e gli obiettivi dell’attaccante. Manipolando attentamente i dati di input, gli aggressori possono sfruttare le sensibilità e i punti ciechi del modello, portando a previsioni errate. Il successo degli attacchi avversari evidenzia la necessità di una comprensione più approfondita delle proprietà di robustezza e generalizzazione dei modelli ML.\nLa difesa dagli attacchi avversari richiede un approccio multiforme. L’addestramento avversario è una strategia di difesa comune in cui i modelli vengono addestrati su esempi avversari per migliorare la robustezza. Esporre il modello a esempi avversari durante l’addestramento gli insegna a classificarli correttamente e a diventare più resiliente agli attacchi. La distillazione difensiva, la preelaborazione degli input e i metodi di ensemble sono altre tecniche che possono aiutare a mitigare l’impatto degli attacchi avversari.\nMan mano che l’apprendimento automatico avversario si evolve, i ricercatori esplorano nuovi meccanismi di attacco e sviluppano difese più sofisticate. La corsa agli armamenti tra aggressori e difensori spinge la necessità di innovazione e vigilanza costanti nel proteggere i sistemi ML dalle minacce avversarie. Comprendere i meccanismi degli attacchi avversari è fondamentale per sviluppare modelli ML robusti e affidabili in grado di resistere al panorama in continua evoluzione degli esempi avversari.\n\n\nImpatto sui Sistemi ML\nGli attacchi avversari sui sistemi di apprendimento automatico sono emersi come una preoccupazione significativa negli ultimi anni, evidenziando le potenziali vulnerabilità e i rischi associati all’adozione diffusa delle tecnologie ML. Questi attacchi comportano perturbazioni attentamente studiate per immettere dati che possono ingannare o fuorviare i modelli ML, portando a previsioni errate o classificazioni errate, come mostrato in Figura 17.20. L’impatto degli attacchi avversari sui sistemi ML è di vasta portata e può avere gravi conseguenze in vari domini.\n\n\n\n\n\n\nFigura 17.20: Generazione di esempi avversari applicata a GoogLeNet (Szegedy et al., 2014a) su ImageNet. Fonte: Goodfellow\n\n\n\nUn esempio lampante dell’impatto degli attacchi avversari è stato dimostrato dai ricercatori nel 2017. Hanno sperimentato piccoli adesivi in bianco e nero sui segnali di stop (Eykholt et al. 2017). All’occhio umano, questi adesivi non oscuravano il segnale né ne impedivano l’interpretazione. Tuttavia, quando le immagini dei segnali di stop modificati dagli adesivi sono state inserite nei modelli ML standard di classificazione dei segnali stradali, è emerso un risultato scioccante. I modelli hanno classificato erroneamente i segnali di stop come segnali di limite di velocità nell’85% dei casi.\n\nEykholt, Kevin, Ivan Evtimov, Earlence Fernandes, Bo Li, Amir Rahmati, Chaowei Xiao, Atul Prakash, Tadayoshi Kohno, e Dawn Song. 2017. «Robust Physical-World Attacks on Deep Learning Models». ArXiv preprint abs/1707.08945. https://arxiv.org/abs/1707.08945.\nQuesta dimostrazione ha fatto luce sul potenziale allarmante di semplici adesivi avversari per ingannare i sistemi ML e fargli interpretare male i segnali stradali critici. Le implicazioni di tali attacchi nel mondo reale sono significative, in particolare nel contesto dei veicoli autonomi. Se utilizzati su strade reali, questi adesivi avversari potrebbero far sì che le auto a guida autonoma interpretino erroneamente i segnali di stop come limiti di velocità, portando a situazioni pericolose, come mostrato in Figura 17.21. I ricercatori hanno avvertito che ciò potrebbe causare arresti a rotazione o accelerazioni involontarie negli incroci, mettendo a repentaglio la sicurezza pubblica.\n\n\n\n\n\n\nFigura 17.21: I graffiti su un segnale di stop hanno ingannato un’auto a guida autonoma facendole credere che si trattasse di un segnale di limite di velocità di 45 mph. Fonte: Eykholt\n\n\n\nIl caso di studio degli adesivi avversari sui segnali di stop fornisce un’illustrazione concreta di come gli esempi avversari sfruttino il modo in cui i modelli ML riconoscono i pattern. Manipolando in modo sottile i dati di input in modi invisibili agli esseri umani, gli aggressori possono indurre previsioni errate e creare gravi rischi, specialmente in applicazioni critiche per la sicurezza come i veicoli autonomi. La semplicità dell’attacco evidenzia la vulnerabilità dei modelli ML anche a piccole modifiche nell’input, sottolineando la necessità di difese robuste contro tali minacce.\nL’impatto degli attacchi avversari si estende oltre il degrado delle prestazioni del modello. Questi attacchi sollevano notevoli preoccupazioni in termini di sicurezza e protezione, in particolare nei domini in cui i modelli ML sono utilizzati per prendere decisioni critiche. Nelle applicazioni sanitarie, gli attacchi avversari sui modelli di imaging medico potrebbero portare a diagnosi errate o raccomandazioni di trattamento errate, mettendo a repentaglio il benessere del paziente (M.-J. Tsai, Lin, e Lee 2023). Nei sistemi finanziari, gli attacchi avversari potrebbero consentire frodi o manipolazioni di algoritmi di trading, con conseguenti perdite economiche sostanziali.\n\nTsai, Min-Jen, Ping-Yi Lin, e Ming-En Lee. 2023. «Adversarial Attacks on Medical Image Classification». Cancers 15 (17): 4228. https://doi.org/10.3390/cancers15174228.\n\nFursov, Ivan, Matvey Morozov, Nina Kaploukhaya, Elizaveta Kovtun, Rodrigo Rivera-Castro, Gleb Gusev, Dmitry Babaev, Ivan Kireev, Alexey Zaytsev, e Evgeny Burnaev. 2021. «Adversarial Attacks on Deep Models for Financial Transaction Records». In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining, 2868–78. ACM. https://doi.org/10.1145/3447548.3467145.\nInoltre, le vulnerabilità avversarie compromettono l’affidabilità e l’interpretabilità dei modelli ML. Se perturbazioni attentamente realizzate possono facilmente ingannare i modelli, la fiducia nelle loro previsioni e decisioni si erode. Gli esempi avversari espongono la dipendenza dei modelli da pattern superficiali e l’incapacità di catturare i veri concetti sottostanti, mettendo in discussione l’affidabilità dei sistemi ML (Fursov et al. 2021).\nLa difesa dagli attacchi avversari richiede spesso risorse computazionali aggiuntive e può influire sulle prestazioni complessive del sistema. Tecniche come l’addestramento avversariale, in cui i modelli vengono addestrati su esempi avversari per migliorare la robustezza, possono aumentare significativamente i tempi di addestramento e i requisiti computazionali (Bai et al. 2021). I meccanismi di rilevamento e mitigazione del runtime, come la preelaborazione dell’input (Addepalli et al. 2020) o i controlli di coerenza delle previsioni, introducono latenza e influenzano le prestazioni in tempo reale dei sistemi ML.\n\nBai, Tao, Jinqi Luo, Jun Zhao, Bihan Wen, e Qian Wang. 2021. «Recent advances in adversarial training for adversarial robustness». arXiv preprint arXiv:2102.01356.\n\nAddepalli, Sravanti, B. S. Vivek, Arya Baburaj, Gaurang Sriramanan, e R. Venkatesh Babu. 2020. «Towards Achieving Adversarial Robustness by Enforcing Feature Consistency Across Bit Planes». In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 1020–29. IEEE. https://doi.org/10.1109/cvpr42600.2020.00110.\nLa presenza di vulnerabilità avversarie complica anche l’implementazione e la manutenzione dei sistemi ML. I progettisti e gli operatori di sistema devono considerare il potenziale di attacchi avversari e incorporare difese e meccanismi di monitoraggio appropriati. Aggiornamenti regolari e riqualificazione dei modelli diventano necessari per adattarsi alle nuove tecniche avversarie e mantenere la sicurezza e le prestazioni del sistema nel tempo.\nL’impatto degli attacchi avversari sui sistemi ML è significativo e multiforme. Questi attacchi espongono le vulnerabilità dei modelli ML, dal degrado delle prestazioni del modello e dall’aumento di preoccupazioni sulla sicurezza e la protezione alla sfida dell’affidabilità e dell’interpretabilità del modello. Sviluppatori e ricercatori devono dare priorità allo sviluppo di difese e contromisure robuste per mitigare i rischi posti dagli attacchi avversari. Affrontando queste sfide, possiamo creare sistemi ML più sicuri, affidabili e degni di fiducia in grado di resistere al panorama in continua evoluzione delle minacce avversarie.\n\n\n\n\n\n\nEsercizio 17.2: Attacchi Avversari\n\n\n\n\n\nPreparatevi a diventare un avversario dell’IA! In questo Colab, si diventerà un hacker white-box, imparando a creare attacchi che ingannano i modelli di classificazione delle immagini. Ci concentreremo sul Fast Gradient Sign Method (FGSM), sfruttando i gradienti di un modello contro di esso! Si distorceranno deliberatamente le immagini con piccole perturbazioni, osservando come inganneranno sempre più intensamente l’IA. Questo esercizio pratico evidenzia l’importanza di creare un’IA sicura, un’abilità critica man mano che l’IA si integra nelle auto e nell’assistenza sanitaria. Il Colab si collega direttamente al capitolo Robust AI del libro, spostando gli attacchi avversari dalla teoria alla esperienza pratica.\n\nPensate di poter superare in astuzia un’IA? In questo Colab, scopriremo come ingannare i modelli di classificazione delle immagini con attacchi avversari. Utilizzeremo metodi come FGSM per modificare le immagini e ingannare sottilmente l’IA. Scopriremo come progettare patch di immagini ingannevoli e osserveremo la sorprendente vulnerabilità di questi potenti modelli. Questa è una conoscenza fondamentale per costruire sistemi di IA veramente robusti!\n\n\n\n\n\n\n\n17.4.2 Avvelenamento dei Dati\n\nDefinizione e Caratteristiche\nL’avvelenamento dei dati è un attacco in cui i dati di addestramento vengono manomessi, portando alla compromissione del modello (Biggio, Nelson, e Laskov 2012), come mostrato in Figura 17.22. Gli aggressori possono modificare gli esempi di training esistenti, inserire nuovi dati dannosi o influenzare il processo di raccolta dati. I dati avvelenati vengono etichettati in modo tale da alterare il comportamento appreso del modello. Ciò può essere particolarmente dannoso nelle applicazioni in cui i modelli ML prendono decisioni automatizzate in base a pattern appresi. Oltre ai set di training, i test di avvelenamento e i dati di convalida possono consentire agli avversari di aumentare artificialmente le prestazioni del modello segnalate.\n\nBiggio, Battista, Blaine Nelson, e Pavel Laskov. 2012. «Poisoning Attacks against Support Vector Machines». In Proceedings of the 29th International Conference on Machine Learning, ICML 2012, Edinburgh, Scotland, UK, June 26 - July 1, 2012. icml.cc / Omnipress. http://icml.cc/2012/papers/880.pdf.\n\n\n\n\n\n\nFigura 17.22: Effetti dell’Avvelenamento di NightShade sulla Diffusione Stabile. Fonte: TOMÉ\n\n\n\nIl processo di solito prevede i seguenti passaggi:\n\nInjection: L’aggressore aggiunge esempi errati o fuorvianti al set di training. Questi esempi sono spesso progettati per sembrare normali a un’ispezione superficiale, ma sono stati attentamente elaborati per interrompere il processo di apprendimento.\nTraining: Il modello ML si allena su questo set di dati manipolato e sviluppa comprensioni distorte dei pattern di dati.\nDeployment: Una volta distribuito il modello, l’addestramento corrotto porta a un processo decisionale imperfetto o a vulnerabilità prevedibili che l’aggressore può sfruttare.\n\nL’impatto dell’avvelenamento dei dati si estende oltre gli errori di classificazione o i cali di accuratezza. In applicazioni critiche come l’assistenza sanitaria, tali alterazioni possono portare a significativi problemi di fiducia e sicurezza (Marulli, Marrone, e Verde 2022). Più avanti, discuteremo alcuni casi di studio di questi problemi.\n\nMarulli, Fiammetta, Stefano Marrone, e Laura Verde. 2022. «Sensitivity of Machine Learning Approaches to Fake and Untrusted Data in Healthcare Domain». Journal of Sensor and Actuator Networks 11 (2): 21. https://doi.org/10.3390/jsan11020021.\n\nOprea, Alina, Anoop Singhal, e Apostol Vassilev. 2022. «Poisoning Attacks Against Machine Learning: Can Machine Learning Be Trustworthy?» Computer 55 (11): 94–99. https://doi.org/10.1109/mc.2022.3190787.\nEsistono sei categorie principali di avvelenamento dei dati (Oprea, Singhal, e Vassilev 2022):\n\nAttacchi alla Disponibilità: Questi attacchi mirano a compromettere la funzionalità complessiva di un modello. Fanno sì che classifichi erroneamente la maggior parte dei campioni di test, rendendo il modello inutilizzabile per applicazioni pratiche. Un esempio è il “label flipping”, in cui le etichette di una classe specifica e mirata vengono sostituite con etichette di una classe diversa.\nAttacchi Mirati: A differenza degli attacchi alla disponibilità, gli attacchi mirati mirano a compromettere un piccolo numero di campioni di test. Quindi, l’effetto è localizzato a un numero limitato di classi, mentre il modello mantiene lo stesso livello originale di accuratezza per la maggior parte delle classi. La natura mirata dell’attacco richiede che l’aggressore conosca le classi del modello, rendendo più difficile il rilevamento di questi attacchi.\nAttacchi Backdoor: In questi attacchi, un avversario prende di mira pattern specifici nei dati. L’aggressore introduce una backdoor (un trigger o un pattern nascosto e dannoso) nei dati di training, ad esempio manipolando determinate feature nei dati strutturati o manipolando un pattern di pixel in una posizione fissa. Ciò fa sì che il modello associ il pattern dannoso a etichette specifiche. Di conseguenza, quando il modello incontra campioni di prova che contengono un pattern dannoso, effettua previsioni false.\nAttacchi di Sotto-popolazione: Gli aggressori scelgono selettivamente di compromettere un sottoinsieme dei campioni di test mantenendo l’accuratezza sul resto dei campioni. Questi attacchi si possono pensare come una combinazione di attacchi di disponibilità e mirati: eseguire attacchi di disponibilità (degrado delle prestazioni) nell’ambito di un sottoinsieme mirato. Sebbene gli attacchi di sottopopolazione possano sembrare molto simili agli attacchi mirati, i due presentano chiare differenze:\nScope: Mentre gli attacchi mirati prendono di mira un set selezionato di campioni, gli attacchi di sotto-popolazione prendono di mira una sotto-popolazione generale con rappresentazioni di caratteristiche simili. Ad esempio, in un attacco mirato, un aggressore inserisce immagini manipolate di un cartello di avvertimento di “dosso” (con perturbazioni o pattern accuratamente studiati), che fanno sì che un’auto autonoma non riesca a riconoscere tale cartello e rallenti. D’altro canto, manipolare tutti i campioni di persone con accento britannico in modo che un modello di riconoscimento vocale classifichi erroneamente il discorso di una persona britannica è un esempio di attacco di sotto-popolazione.\nConoscenza: Mentre gli attacchi mirati richiedono un alto grado di familiarità con i dati, gli attacchi alla sotto-popolazione richiedono una conoscenza meno approfondita per essere efficaci.\n\nLe caratteristiche del data poisoning includono:\nManipolazioni sottili e difficili da rilevare dei dati di training: Il data poisoning spesso comporta manipolazioni sottili dei dati di training che sono attentamente studiate per essere difficili da rilevare tramite un’ispezione casuale. Gli aggressori impiegano tecniche sofisticate per garantire che i campioni avvelenati si fondano perfettamente con i dati legittimi, rendendoli più facili da identificare con un’analisi approfondita. Queste manipolazioni possono mirare a caratteristiche o attributi specifici dei dati, come l’alterazione di valori numerici, la modifica di etichette categoriali o l’introduzione di pattern attentamente progettati. L’obiettivo è influenzare il processo di apprendimento del modello eludendo il rilevamento, consentendo ai dati avvelenati di corrompere sottilmente il comportamento del modello.\nPuò essere eseguito da insider o aggressori esterni: Gli attacchi di data poisoning possono essere eseguiti da vari attori, tra cui insider malintenzionati con accesso ai dati di training e aggressori esterni che trovano modi per influenzare la raccolta dati o la pipeline di pre-elaborazione. Gli insider rappresentano una minaccia significativa perché spesso hanno accesso privilegiato e conoscenza del sistema, il che consente loro di introdurre dati avvelenati senza destare sospetti. D’altro canto, gli aggressori esterni possono sfruttare le vulnerabilità nell’approvvigionamento dei dati, nelle piattaforme di crowdsourcing o nei processi di aggregazione dei dati per iniettare campioni avvelenati nel set di dati di addestramento. Ciò evidenzia l’importanza di implementare controlli di accesso rigorosi, policy di governance dei dati e meccanismi di monitoraggio per mitigare il rischio di minacce interne e attacchi esterni.\nSfrutta le vulnerabilità nella raccolta e pre-elaborazione dei dati: Gli attacchi di avvelenamento dei dati spesso sfruttano le vulnerabilità nelle fasi di raccolta e pre-elaborazione dei dati della pipeline di apprendimento automatico. Gli aggressori progettano attentamente campioni avvelenati per eludere le comuni tecniche di convalida dei dati, assicurandosi che i dati manipolati rientrino comunque in intervalli accettabili, seguano le distribuzioni previste o mantengano la coerenza con altre funzionalità. Ciò consente ai dati avvelenati di passare attraverso le fasi di pre-elaborazione dei dati senza essere rilevati. Inoltre, gli attacchi di avvelenamento possono sfruttare le debolezze nella preelaborazione dei dati, come una pulizia dei dati inadeguata, un rilevamento insufficiente di valori anomali o la mancanza di controlli di integrità. Gli aggressori possono anche sfruttare la mancanza di solidi meccanismi di tracciamento della provenienza e della discendenza dei dati per introdurre dati avvelenati senza lasciare una traccia. Per affrontare queste vulnerabilità sono necessarie rigorose tecniche di convalida dei dati, rilevamento delle anomalie e tracciamento della provenienza dei dati per garantire l’integrità e l’affidabilità dei dati di training.\nInterrompe il processo di apprendimento e distorce il comportamento del modello: Gli attacchi di avvelenamento dei dati sono progettati per interrompere il processo di apprendimento dei modelli di apprendimento automatico e distorcere il loro comportamento verso gli obiettivi dell’aggressore. I dati avvelenati vengono in genere manipolati con obiettivi specifici, come distorcere il comportamento del modello verso determinate classi, introdurre backdoor o degradare le prestazioni complessive. Queste manipolazioni non sono casuali, ma mirate a ottenere i risultati desiderati dall’aggressore. Introducendo incongruenze nelle etichette, in cui i campioni manipolati hanno etichette che non si allineano con la loro vera natura, gli attacchi di avvelenamento possono confondere il modello durante l’addestramento e portare a previsioni distorte o errate. L’interruzione causata dai dati avvelenati può avere conseguenze di vasta portata, poiché il modello compromesso può prendere decisioni imperfette o mostrare un comportamento indesiderato quando viene distribuito in applicazioni del mondo reale.\nInfluisce sulle prestazioni, l’equità e l’affidabilità del modello: I dati avvelenati nel dataset di addestramento possono avere gravi implicazioni sulle prestazioni, l’equità e l’affidabilità dei modelli di apprendimento automatico. I dati avvelenati possono degradare l’accuratezza e le prestazioni del modello addestrato, portando a un aumento delle classificazioni errate o degli errori nelle previsioni. Ciò può avere conseguenze significative, soprattutto nelle applicazioni critiche in cui gli output del modello influenzano decisioni importanti. Inoltre, gli attacchi di avvelenamento possono introdurre distorsioni e problemi di equità, facendo sì che il modello prenda decisioni discriminatorie o ingiuste per determinati sottogruppi o classi. Ciò mina le responsabilità etiche e sociali dei sistemi di apprendimento automatico e può perpetuare o amplificare i pregiudizi esistenti. Inoltre, i dati avvelenati erodono l’affidabilità e la credibilità dell’intero sistema di apprendimento automatico. Gli output del modello diventano discutibili e potenzialmente dannosi, portando a una perdita di fiducia nell’integrità del sistema. L’impatto dei dati avvelenati può propagarsi nell’intera pipeline ML, influenzando i componenti downstream e le decisioni che si basano sul modello compromesso. Per affrontare queste preoccupazioni è necessaria una solida governance dei dati, un auditing regolare del modello e un monitoraggio continuo per rilevare e mitigare gli effetti degli attacchi di avvelenamento dei dati.\n\n\nMeccanismi di Avvelenamento dei Dati\nGli attacchi di avvelenamento dei dati possono essere eseguiti tramite vari meccanismi, sfruttando diverse vulnerabilità della pipeline ML. Questi meccanismi consentono agli aggressori di manipolare i dati di training e introdurre campioni dannosi che possono compromettere le prestazioni, l’equità o l’integrità del modello. Comprendere questi meccanismi è fondamentale per sviluppare difese efficaci contro l’avvelenamento dei dati e garantire la robustezza dei sistemi ML. I meccanismi di avvelenamento dei dati possono essere ampiamente categorizzati in base all’approccio dell’aggressore e alla fase della pipeline ML a cui mirano. Alcuni meccanismi comuni includono la modifica delle etichette dei dati di training, l’alterazione dei valori delle feature, l’iniezione di campioni dannosi accuratamente realizzati, lo sfruttamento delle vulnerabilità di raccolta e pre-elaborazione dei dati, la manipolazione dei dati alla fonte, l’avvelenamento dei dati in scenari di apprendimento online e la collaborazione con addetti ai lavori per manipolare i dati.\nOgnuno di questi meccanismi presenta sfide uniche e richiede diverse strategie di mitigazione. Ad esempio, rilevare la manipolazione delle etichette può comportare l’analisi della distribuzione delle etichette e l’identificazione delle anomalie (Zhou et al. 2018), mentre prevenire la manipolazione delle feature può richiedere tecniche di pre-elaborazione dei dati e rilevamento delle anomalie sicure (Carta et al. 2020). La difesa dalle minacce interne può comportare rigide policy di controllo degli accessi e il monitoraggio dei pattern di accesso ai dati. Inoltre, l’efficacia degli attacchi di avvelenamento dei dati spesso dipende dalla conoscenza del sistema ML da parte dell’attaccante, tra cui l’architettura del modello, gli algoritmi di training e la distribuzione dei dati. Gli aggressori possono utilizzare tecniche di apprendimento automatico avversario o di sintesi dei dati per creare campioni che hanno maggiori probabilità di aggirare il rilevamento e raggiungere i loro obiettivi malevoli.\n\nZhou, Peng, Xintong Han, Vlad I. Morariu, e Larry S. Davis. 2018. «Learning Rich Features for Image Manipulation Detection». In 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, 1053–61. IEEE. https://doi.org/10.1109/cvpr.2018.00116.\n\nCarta, Salvatore, Alessandro Sebastian Podda, Diego Reforgiato Recupero, e Roberto Saia. 2020. «A Local Feature Engineering Strategy to Improve Network Anomaly Detection». Future Internet 12 (10): 177. https://doi.org/10.3390/fi12100177.\nModifica delle etichette dei dati di training: Uno dei meccanismi più semplici di avvelenamento dei dati è la modifica delle etichette dei dati di training. In questo approccio, l’aggressore modifica selettivamente le etichette di un sottoinsieme dei campioni di training per fuorviare il processo di apprendimento del modello, come mostrato in Figura 17.23. Ad esempio, in un’attività di classificazione binaria, l’aggressore potrebbe capovolgere le etichette di alcuni campioni positivi in negativi o viceversa. Introducendo tale rumore di etichetta, l’aggressore degrada le prestazioni del modello o fa sì che faccia previsioni errate per istanze target specifiche.\n\n\n\n\n\n\nFigura 17.23: Garbage In – Garbage Out. Fonte: Information Matters\n\n\n\nAlterazione dei valori delle feature nei dati di training: Un altro meccanismo di avvelenamento dei dati consiste nell’alterare i valori delle caratteristiche dei campioni di training senza modificare le etichette. L’aggressore elabora attentamente i valori delle feature per introdurre specifici pregiudizi o vulnerabilità nel modello. Ad esempio, in un’attività di classificazione delle immagini, l’aggressore potrebbe aggiungere perturbazioni impercettibili a un sottoinsieme di immagini, facendo sì che il modello apprenda un particolare pattern o associazione. Questo tipo di avvelenamento può creare backdoor o trojan nel modello addestrato, che possono essere attivati da specifici pattern di input.\nIniezione di campioni dannosi accuratamente realizzati: In questo meccanismo, l’aggressore crea campioni dannosi progettati per avvelenare il modello. Questi campioni sono realizzati per avere un impatto specifico sul comportamento del modello, mentre si fondono con i dati di addestramento legittimi. L’aggressore potrebbe utilizzare tecniche come perturbazioni avversarie o sintesi dei dati per generare campioni avvelenati difficili da rilevare. L’aggressore manipola i limiti decisionali del modello iniettando questi campioni dannosi nei dati di addestramento o introducendo classificazioni errate mirate.\nSfruttamento delle vulnerabilità di raccolta e preelaborazione dei dati: Gli attacchi di avvelenamento dei dati possono anche sfruttare le vulnerabilità della pipeline di raccolta e preelaborazione dei dati. Se il processo di raccolta dati non è sicuro o ci sono debolezze nelle fasi di pre-elaborazione dei dati, un aggressore può manipolare i dati prima che raggiungano la fase di addestramento. Ad esempio, se i dati vengono raccolti da fonti non attendibili o ci sono problemi nella pulizia o nell’aggregazione dei dati, un aggressore può introdurre campioni avvelenati o manipolare i dati a proprio vantaggio.\nManipolazione dei dati alla fonte (ad esempio, dati dei sensori): In alcuni casi, gli aggressori possono manipolare i dati alla fonte, come dati dei sensori o dispositivi di input. Manomettendo i sensori o manipolando l’ambiente in cui vengono raccolti i dati, gli aggressori possono introdurre campioni avvelenati o alterare la distribuzione dei dati. Ad esempio, in uno scenario di auto a guida autonoma, un aggressore potrebbe manipolare i sensori o l’ambiente per immettere informazioni fuorvianti nei dati di addestramento, compromettendo la capacità del modello di prendere decisioni sicure e affidabili.\nAvvelenamento dei dati in scenari di apprendimento online: Gli attacchi di avvelenamento dei dati possono anche colpire sistemi ML che impiegano l’apprendimento online, in cui il modello viene costantemente aggiornato con nuovi dati in tempo reale. In tali scenari, un aggressore può gradualmente iniettare campioni avvelenati nel tempo, manipolando lentamente il comportamento del modello. I sistemi di apprendimento online sono particolarmente vulnerabili all’avvelenamento dei dati perché si adattano ai nuovi dati senza una convalida estesa, rendendo più facile per gli aggressori introdurre campioni dannosi, come mostrato in Figura 17.24.\n\n\n\n\n\n\nFigura 17.24: Attacco di Avvelenamento dei Dati. Fonte: Sikandar\n\n\n\nCollaborazione con addetti ai lavori per manipolare i dati: A volte, gli attacchi di avvelenamento dei dati possono comportare la collaborazione con addetti ai lavori con accesso ai dati di training. Gli addetti ai lavori malintenzionati, come dipendenti o provider di dati, possono manipolare i dati prima che vengano utilizzati per addestrare il modello. Le minacce interne sono particolarmente difficili da rilevare e prevenire, poiché gli aggressori hanno un accesso legittimo ai dati e possono elaborare attentamente la strategia di avvelenamento per eludere il rilevamento.\nQuesti sono i meccanismi chiave dell’avvelenamento dei dati nei sistemi ML. Gli aggressori spesso impiegano questi meccanismi per rendere i loro attacchi più efficaci e difficili da rilevare. Il rischio di attacchi di avvelenamento dei dati aumenta man mano che i sistemi ML diventano sempre più complessi e si basano su set di dati più grandi provenienti da fonti diverse. La difesa dall’avvelenamento dei dati richiede un approccio poliedrico. I professionisti ML e i progettisti di sistemi devono essere consapevoli dei vari meccanismi di avvelenamento dei dati e adottare un approccio completo alla sicurezza dei dati e alla resilienza del modello. Ciò include la raccolta dati sicura, la convalida dati robusta e il monitoraggio continuo delle prestazioni del modello. L’implementazione di pratiche di raccolta dati e pre-elaborazione sicure è fondamentale per prevenire l’avvelenamento dei dati alla fonte. Le tecniche di convalida dati e rilevamento anomalie possono anche aiutare a identificare e mitigare potenziali tentativi di avvelenamento. Il monitoraggio delle prestazioni del modello per segnali di avvelenamento dei dati è inoltre essenziale per rilevare e rispondere prontamente agli attacchi.\n\n\nImpatto sui Sistemi ML\nGli attacchi di avvelenamento dei dati possono avere gravi ripercussioni sui sistemi ML, compromettendone le prestazioni, l’affidabilità e la credibilità. L’impatto dell’avvelenamento dei dati può manifestarsi in vari modi, a seconda degli obiettivi dell’aggressore e del meccanismo specifico utilizzato. Analizziamo in dettaglio ciascuno dei potenziali impatti.\nDegrado delle prestazioni del modello: Uno degli impatti principali dell’avvelenamento dei dati è il degrado delle prestazioni complessive del modello. Manipolando i dati di training, gli aggressori possono introdurre rumore, distorsioni o incongruenze che ostacolano la capacità del modello di apprendere pattern accurati e fare previsioni affidabili. Ciò può ridurre accuratezza, precisione, richiamo o altre metriche delle prestazioni. Il degrado delle prestazioni del modello può avere conseguenze significative, soprattutto in applicazioni critiche come sanità, finanza o sicurezza, dove l’affidabilità delle previsioni è fondamentale.\nErrore di classificazione di target specifici: Gli attacchi di avvelenamento dei dati possono anche essere progettati per far sì che il modello classifichi in modo errato istanze target specifiche. Gli aggressori possono introdurre campioni avvelenati realizzati con cura simili alle istanze target, portando il modello ad apprendere associazioni errate. Ciò può comportare che il modello classifichi in modo errato le istanze target in modo coerente, anche se funziona bene su altri input. Tale errata classificazione mirata può avere gravi conseguenze, come far sì che un sistema di rilevamento malware trascuri file dannosi specifici o portare a una diagnosi errata in un’applicazione di imaging medico.\nBackdoor e trojan nei modelli addestrati: L’avvelenamento dei dati può introdurre backdoor o trojan nel modello addestrato. Le backdoor sono funzionalità nascoste che consentono agli aggressori di innescare comportamenti specifici o bypassare i normali meccanismi di autenticazione. D’altro canto, i trojan sono componenti dannosi insinuati nel modello che possono attivare specifici pattern di input. Avvelenando i dati di training, gli aggressori possono creare modelli che sembrano funzionare normalmente ma contengono vulnerabilità nascoste che possono essere sfruttate in seguito. Backdoor e trojan possono compromettere l’integrità e la sicurezza del sistema ML, consentendo agli aggressori di ottenere accesso non autorizzato, manipolare previsioni o esfiltrare informazioni sensibili.\nRisultati del modello distorti o ingiusti: Gli attacchi di avvelenamento dei dati possono introdurre distorsioni o ingiustizie nelle previsioni del modello. Manipolando la distribuzione dei dati di training o iniettando campioni con distorsioni specifiche, gli aggressori possono far sì che il modello apprenda e perpetui pattern discriminatori. Ciò può portare a un trattamento ingiusto di determinati gruppi o individui in base ad attributi sensibili come razza, genere o età. I modelli distorti possono avere gravi implicazioni sociali, rafforzando le disuguaglianze e le pratiche discriminatorie esistenti. Garantire l’equità e mitigare i pregiudizi è fondamentale per creare sistemi ML affidabili ed etici.\nAumento di falsi positivi o falsi negativi: L’avvelenamento dei dati può anche influire sulla capacità del modello di identificare correttamente istanze positive o negative, portando a un aumento di falsi positivi o falsi negativi. I falsi positivi si verificano quando il modello identifica erroneamente un’istanza negativa come positiva, mentre i falsi negativi si verificano quando un’istanza positiva viene classificata erroneamente come negativa. Le conseguenze dell’aumento di falsi positivi o falsi negativi possono essere significative a seconda dell’applicazione. Ad esempio, in un sistema di rilevamento delle frodi, un elevato numero di falsi positivi può portare a indagini non necessarie e frustrazione dei clienti, mentre un elevato numero di falsi negativi può consentire che le attività fraudolente passino inosservate.\nAffidabilità e fiducia del sistema compromesse: Gli attacchi di avvelenamento dei dati possono minare l’affidabilità e la fiducia complessiva dei sistemi ML. Quando i modelli vengono addestrati su dati contaminati, le loro previsioni diventano inaffidabili e inaffidabili. Ciò può erodere la fiducia dell’utente nel sistema e portare a una perdita di fiducia nelle decisioni prese dal modello. Nelle applicazioni critiche in cui si fa affidamento sui sistemi ML per il processo decisionale, come veicoli autonomi o diagnosi mediche, l’affidabilità compromessa può avere gravi conseguenze, mettendo a rischio vite e proprietà.\nPer affrontare l’impatto dell’avvelenamento dei dati è necessario un approccio proattivo alla sicurezza dei dati, ai test dei modelli e al monitoraggio. Le organizzazioni devono implementare misure robuste per garantire l’integrità e la qualità dei dati di training, impiegare tecniche per rilevare e mitigare i tentativi di avvelenamento e monitorare costantemente le prestazioni e il comportamento dei modelli distribuiti. La collaborazione tra professionisti ML, esperti di sicurezza e specialisti di dominio è essenziale per sviluppare strategie complete per prevenire e rispondere agli attacchi di avvelenamento dei dati.\n\nCaso di Studio\nÈ interessante notare che gli attacchi di “data poisoning” non sono sempre dannosi (Shan et al. 2023). Nightshade, uno strumento sviluppato da un team guidato dal professor Ben Zhao presso l’Università di Chicago, utilizza l’avvelenamento dei dati per aiutare gli artisti a proteggere la propria arte da scraping e violazioni del copyright da parte di modelli di IA generativa. Gli artisti possono utilizzare lo strumento per apportare modifiche impercettibili alle proprie immagini prima di caricarle online, come mostrato in Figura 17.25.\n\n\n\n\n\n\nFigura 17.25: Campioni di dati avvelenati con etichette sbagliateriguardanti coppie testo/immagine non corrispondenti. Fonte: Shan\n\n\n\nSebbene queste modifiche siano impercettibili all’occhio umano, possono compromettere significativamente le prestazioni dei modelli di intelligenza artificiale generativa quando vengono incorporate nei dati di addestramento. I modelli generativi possono essere manipolati per generare allucinazioni e immagini strane. Ad esempio, con solo 300 immagini avvelenate, i ricercatori dell’Università di Chicago potrebbero ingannare l’ultimo modello Stable Diffusion per generare immagini di cani che sembrano gatti o immagini di mucche quando vengono richieste le auto.\nMan mano che aumenta il numero di immagini avvelenate su Internet, le prestazioni dei modelli che utilizzano dati acquisiti peggioreranno in modo esponenziale. In primo luogo, i dati avvelenati sono difficili da rilevare e richiedono l’eliminazione manuale. In secondo luogo, il “veleno” si diffonde rapidamente ad altre etichette perché i modelli generativi si basano su connessioni tra parole e concetti mentre generano immagini. Quindi un’immagine avvelenata di una “macchina” potrebbe diffondersi in immagini generate associate a parole come “camion”, “treno”, “autobus”, ecc.\nD’altra parte, questo strumento può essere utilizzato in modo dannoso e può influenzare le applicazioni legittime dei modelli generativi. Ciò dimostra la natura molto impegnativa e innovativa degli attacchi di apprendimento automatico.\nFigura 17.26 mostra gli effetti di diversi livelli di avvelenamento dei dati (50 campioni, 100 campioni e 300 campioni di immagini avvelenate) sulla generazione di immagini in diverse categorie. Notare come le immagini iniziano a deformarsi e deviare dalla categoria desiderata. Ad esempio, dopo 300 campioni di avvelenamento, una richiesta di un’auto genera una mucca.\n\n\n\n\n\n\nFigura 17.26: Avvelenamento dei Dati. Fonte: Shan et al. (2023))\n\n\nShan, Shawn, Wenxin Ding, Josephine Passananti, Haitao Zheng, e Ben Y Zhao. 2023. «Prompt-Specific Poisoning Attacks on Text-to-Image Generative Models». ArXiv preprint abs/2310.13828. https://arxiv.org/abs/2310.13828.\n\n\n\n\n\n\n\n\nEsercizio 17.3: Attacchi Avvelenati\n\n\n\n\n\nPreparatevi a esplorare il lato oscuro della sicurezza dell’IA! In questo Colab, impareremo cos’è l’avvelenamento dei dati, ovvero come dati errati possono ingannare i modelli di IA e fargli prendere decisioni sbagliate. Ci concentreremo su un attacco reale contro una Support Vector Machine (SVM), osservando come cambia il comportamento dell’IA sotto attacco. Questo esercizio pratico metterà in evidenza perché proteggere i sistemi di IA è fondamentale, soprattutto man mano che diventano più integrati nelle nostre vite. Pensare come un hacker, comprendere la vulnerabilità e fare brainstorming su come difendere i sistemi di IA!\n\n\n\n\n\n\n\n\n17.4.3 Distribution Shift\n\nDefinizione e Caratteristiche\nLa “distribution shift” [slittamento della distribuzione] si riferisce al fenomeno in cui la distribuzione dei dati incontrata da un modello ML durante la distribuzione (inferenza) differisce dalla distribuzione su cui è stato addestrato, come mostrato in Figura 17.27. Questo non è tanto un attacco quanto il fatto che la robustezza del modello varierà nel tempo. In altre parole, le proprietà statistiche, i pattern o le ipotesi sottostanti dei dati possono cambiare tra le fasi di addestramento e di test.\n\n\n\n\n\n\nFigura 17.27: Le parentesi graffe racchiudono la “distribution shift” tra gli ambienti. Qui, z sta per la caratteristica spuria e y sta per la classe dell’etichetta. Fonte: Xin\n\n\n\nLe caratteristiche principali della “distribution shift” includono:\nDiscordanza di dominio: I dati di input durante l’inferenza provengono da un dominio o una distribuzione diversi rispetto ai dati di addestramento. Quando i dati di input durante l’inferenza provengono da un dominio o una distribuzione diversi dai dati di training, possono influenzare significativamente le prestazioni del modello. Questo perché il modello ha imparato pattern e relazioni specifici del dominio di training e, se applicati a un dominio diverso, tali pattern appresi potrebbero non essere validi. Ad esempio, si consideri un modello di analisi del sentiment addestrato sulle recensioni di film. Supponiamo che questo modello venga applicato per analizzare il sentiment nei tweet. In tal caso, potrebbe aver bisogno di aiuto per classificare accuratamente il sentiment perché la lingua, la grammatica e il contesto dei tweet possono differire dalle recensioni dei film. Questa discrepanza di dominio può causare scarse prestazioni e previsioni inaffidabili, limitando l’utilità pratica del modello.\nDeriva temporale: La distribuzione dei dati si evolve, portando a uno spostamento graduale o improvviso nelle caratteristiche di input. La deriva temporale è importante perché i modelli ML vengono spesso distribuiti in ambienti dinamici in cui la distribuzione dei dati può cambiare nel tempo. Se il modello non viene aggiornato o adattato a questi cambiamenti, le sue prestazioni possono gradualmente peggiorare. Ad esempio, i pattern e i comportamenti associati alle attività fraudolente possono evolversi in un sistema di rilevamento delle frodi man mano che i truffatori adattano le loro tecniche. Se il modello non viene riqualificato o aggiornato per catturare questi nuovi pattern, potrebbe non riuscire a rilevare efficacemente nuovi tipi di frode. La deriva temporale può portare a un calo dell’accuratezza e dell’affidabilità del modello nel tempo, rendendo cruciale il monitoraggio e l’affronto di questo tipo di spostamento della distribuzione.\nCambiamenti contestuali: Il contesto del modello ML può variare, determinando diverse distribuzioni di dati in base a fattori quali posizione, comportamento dell’utente o condizioni ambientali. I cambiamenti contestuali sono importanti perché i modelli ML vengono spesso distribuiti in vari contesti o ambienti che possono avere diverse distribuzioni di dati. Se il modello non riesce a generalizzarsi bene a questi diversi contesti, le sue prestazioni potrebbero deteriorarsi. Ad esempio, si consideri un modello di visione artificiale addestrato per riconoscere oggetti in un ambiente di laboratorio controllato. Quando distribuito in un contesto reale, fattori quali condizioni di illuminazione, angoli della telecamera o confusione sullo sfondo possono variare in modo significativo, determinando una “distribution shift”. Se il modello è robusto a questi cambiamenti contestuali, potrebbe essere in grado di riconoscere accuratamente gli oggetti nel nuovo ambiente, limitandone l’utilità pratica.\nDati di addestramento non rappresentativi: I dati di addestramento potrebbero catturare solo parzialmente la variabilità e la diversità dei dati del mondo reale riscontrati durante la distribuzione. I dati di training non rappresentativi possono portare a modelli parziali o distorti che funzionano male sui dati del mondo reale. Supponiamo che i dati di training debbano catturare adeguatamente la variabilità e la diversità dei dati del mondo reale. In tal caso, il modello potrebbe apprendere pattern specifici del set di training, ma deve essere meglio generalizzato a dati nuovi e mai visti. Ciò può comportare scarse prestazioni, previsioni parziali e limitata applicabilità del modello. Ad esempio, se un modello di riconoscimento facciale viene addestrato principalmente su immagini di individui di uno specifico gruppo demografico, potrebbe avere difficoltà a riconoscere accuratamente i volti di altri gruppi demografici quando viene distribuito in un contesto reale. Garantire che i dati di training siano rappresentativi e diversificati è fondamentale per creare modelli che possano essere generalizzati bene a scenari del mondo reale.\nLa “distribution shift” può manifestarsi in varie forme, come:\nCovariate shift: La distribuzione delle feature di input (covariate) cambia mentre la distribuzione condizionale della variabile target dato l’input rimane la stessa. La “covariate shift” è importante perché può influire sulla capacità del modello di fare previsioni accurate quando le feature di input (covariate) differiscono tra i dati di training e quelli di test. Anche se la relazione tra le feature di input e la variabile target rimane la stessa, un cambiamento nella distribuzione delle feature di input può influire sulle prestazioni del modello. Ad esempio, si consideri un modello addestrato per prevedere i prezzi delle case in base a caratteristiche come la metratura, il numero di camere da letto e la posizione. Supponiamo che la distribuzione di queste caratteristiche nei dati di test differisca significativamente dai dati di training (ad esempio, i dati di test contengono case con una metratura molto più ampia). In tal caso, le previsioni del modello potrebbero diventare meno accurate. È importante tenere conto dei “covariate shift” per garantire la robustezza e l’affidabilità del modello quando viene applicato a nuovi dati.\nConcept drift: La relazione tra le feature di input e la variabile target cambia nel tempo, alterando il concetto sottostante che il modello sta cercando di apprendere, come mostrato in Figura 17.28. Il “concept drift” è importante perché indica cambiamenti nella relazione fondamentale tra le feature di input e la variabile target nel tempo. Quando il concetto sottostante che il modello sta cercando di apprendere cambia, le sue prestazioni possono deteriorarsi se non vengono adattate al nuovo concetto. Ad esempio, in un modello di previsione dell’abbandono dei clienti, i fattori che influenzano l’abbandono dei clienti possono evolversi a causa delle condizioni di mercato, delle offerte della concorrenza o delle preferenze dei clienti. Se il modello non viene aggiornato per catturare questi cambiamenti, le sue previsioni potrebbero diventare meno accurate e irrilevanti. Rilevare e adattarsi al “concept drift” è fondamentale per mantenere l’efficacia del modello e l’allineamento con i concetti del mondo reale in evoluzione.\n\n\n\n\n\n\nFigura 17.28: La deriva concettuale si riferisce a un cambiamento nei pattern e nelle relazioni dei dati nel tempo. Fonte: Evidently AI\n\n\n\nGeneralizzazione del dominio: Il modello deve generalizzare a domini o distribuzioni invisibili non presenti durante l’addestramento. La generalizzazione di dominio è importante perché consente di applicare i modelli ML a nuovi domini mai visti senza richiedere un’ampia riqualificazione o adattamento. Negli scenari del mondo reale, i dati di addestramento che coprono tutti i possibili domini o distribuzioni che il modello può incontrare sono spesso irrealizzabili. Le tecniche di generalizzazione di dominio mirano ad apprendere caratteristiche o modelli invarianti al dominio che possono essere generalizzati bene a nuovi domini. Ad esempio, si consideri un modello addestrato per classificare immagini di animali. Se il modello può apprendere caratteristiche invarianti a diversi sfondi, condizioni di illuminazione o pose, può essere generalizzato bene per classificare animali in nuovi ambienti mai visti. La generalizzazione del dominio è fondamentale per creare modelli che possono essere distribuiti in contesti reali diversi e in continua evoluzione.\nLa presenza di un “distribution shift” può avere un impatto significativo sulle prestazioni e l’affidabilità dei modelli ML, poiché i modelli potrebbero aver bisogno di aiuto per generalizzare bene alla nuova distribuzione dei dati. Rilevare e adattarsi ai “distribution shift” è fondamentale per garantire la robustezza e l’utilità pratica dei sistemi ML negli scenari del mondo reale.\n\n\nMeccanismi delle Distribution Shift\nI meccanismi della “distribution shift”, come cambiamenti nelle fonti dei dati, evoluzione temporale, variazioni specifiche del dominio, bias di selezione, cicli di feedback e manipolazioni avversarie, sono importanti da comprendere perché aiutano a identificarne le cause. Comprendendo questi meccanismi, i professionisti possono sviluppare strategie mirate per mitigarne l’impatto e migliorare la robustezza del modello. Ecco alcuni meccanismi comuni:\n\n\n\n\n\n\nFigura 17.29: Evoluzione temporale. Fonte: Białek\n\n\n\nCambiamenti nelle fonti di dati: Possono verificarsi cambiamenti di distribuzione quando le fonti di dati utilizzate per l’addestramento e l’inferenza sono diverse. Ad esempio, se un modello viene addestrato sui dati di un sensore ma distribuito sui dati di un altro sensore con caratteristiche diverse, può portare a un “distribution shift”.\nEvoluzione temporale: Nel tempo, la distribuzione dei dati sottostante può evolversi a causa di cambiamenti nel comportamento dell’utente, dinamiche di mercato o altri fattori temporali. Ad esempio, in un sistema di raccomandazione, le preferenze dell’utente possono cambiare nel tempo, portando a un “distribution shift” nei dati di input, come mostrato in Figura 17.29.\nVariazioni specifiche del dominio: Domini o contesti diversi possono avere distribuzioni di dati distinte. Un modello addestrato sui dati di un dominio può generalizzare bene con un altro dominio solo con tecniche di adattamento appropriate. Ad esempio, un modello di classificazione delle immagini addestrato su scene di interni potrebbe avere difficoltà se applicato a scene in esterno.\nBias di selezione: Un “Distribution shift” può derivare da un bias di selezione durante la raccolta o il campionamento dei dati. Se i dati di training non rappresentano la popolazione reale o determinati sottogruppi sono sovrarappresentati o sottorappresentati, si può arrivare a una mancata corrispondenza tra le distribuzioni di training e di test.\nCicli di feedback: In alcuni casi, le previsioni o le azioni intraprese da un modello ML possono influenzare la futura distribuzione dei dati. Ad esempio, in un sistema di prezzi dinamici, i prezzi stabiliti dal modello possono influire sul comportamento dei clienti, determinando uno spostamento nella distribuzione dei dati nel tempo.\nManipolazioni avversarie: Gli avversari possono manipolare intenzionalmente i dati di input per creare uno spostamento della distribuzione e ingannare il modello ML. Introducendo perturbazioni attentamente studiate o generando campioni fuori distribuzione, gli aggressori possono sfruttare le vulnerabilità del modello e fargli fare previsioni errate.\nComprendere i meccanismi del “distribution shift” è importante per sviluppare strategie efficaci per rilevare e mitigare il suo impatto sui sistemi ML. Identificando le fonti e le caratteristiche dello spostamento, i professionisti possono progettare tecniche appropriate, come l’adattamento del dominio, l’apprendimento tramite trasferimento o l’apprendimento continuo, per migliorare la robustezza e le prestazioni del modello in caso di cambiamenti distributivi.\n\n\nImpatto sui Sistemi ML\nI “distribution shift” possono avere un impatto negativo significativo sulle prestazioni e l’affidabilità dei sistemi ML. Ecco alcuni modi chiave in cui il “distribution shift” può influenzare i modelli ML:\nPrestazioni predittive degradate: Quando la distribuzione dei dati riscontrata durante l’inferenza differisce dalla distribuzione di training, l’accuratezza predittiva del modello può deteriorarsi. Il modello potrebbe aver bisogno di aiuto per generalizzare bene i nuovi dati, il che porta a un aumento degli errori e a prestazioni non ottimali.\nAffidabilità e attendibilità ridotte: Il “distribution shift” può compromettere l’affidabilità e l’attendibilità dei modelli ML. Se le previsioni del modello diventano inaffidabili o incoerenti a causa dello spostamento, gli utenti potrebbero perdere fiducia negli output del sistema, il che porta a un potenziale uso improprio o non uso del modello.\nPredizioni distorte: Lo spostamento di distribuzione può introdurre “bias” [distorsioni] nelle previsioni del modello. Se i dati di training non rappresentano la distribuzione nel mondo reale o alcuni sottogruppi sono sottorappresentati, il modello potrebbe fare previsioni distorte che discriminano determinati gruppi o perpetuano pregiudizi sociali.\nMaggiore incertezza e rischio: Lo spostamento della distribuzione introduce ulteriore incertezza e rischio nel sistema ML. Il comportamento e le prestazioni del modello potrebbero diventare meno prevedibili, rendendo difficile valutarne l’affidabilità e l’idoneità per applicazioni critiche. Questa incertezza può portare a maggiori rischi operativi e potenziali guasti.\nSfide di adattabilità: I modelli ML addestrati su una distribuzione dati specifica potrebbero aver bisogno di aiuto per adattarsi ad ambienti mutevoli o nuovi domini. La mancanza di adattabilità può limitare l’utilità e l’applicabilità del modello in scenari reali dinamici in cui la distribuzione dei dati si evolve.\nDifficoltà di manutenzione e aggiornamento: Il “distribution shift” può complicare la manutenzione e l’aggiornamento dei modelli ML. Man mano che la distribuzione dei dati cambia, il modello potrebbe richiedere frequenti riqualificazioni o ottimizzazioni per mantenere le sue prestazioni. Ciò può richiedere molto tempo e risorse, soprattutto se il cambiamento avviene rapidamente o continuamente.\nVulnerabilità agli attacchi avversari: Il “distribution shift” può rendere i modelli ML più vulnerabili agli attacchi avversari. Gli avversari possono sfruttare la sensibilità del modello ai cambiamenti distributivi creando esempi avversari al di fuori della distribuzione di addestramento, facendo sì che il modello faccia previsioni errate o si comporti in modo inaspettato.\nPer mitigare l’impatto dei “distribution shift”, è fondamentale sviluppare sistemi ML robusti che rilevino e si adattino ai cambiamenti delle distribuzioni. Tecniche come l’adattamento del dominio, l’apprendimento tramite trasferimento e l’apprendimento continuo possono aiutare a migliorare la capacità di generalizzazione del modello su diverse distribuzioni. Il monitoraggio, il test e l’aggiornamento del modello ML sono inoltre necessari per garantirne le prestazioni e l’affidabilità durante i “distribution shift”.\n\n\n\n17.4.4 Rilevamento e Mitigazione\n\nAttacchi Avversari\nCome si ricorderà da quanto sopra, gli attacchi avversari rappresentano una minaccia significativa per la robustezza e l’affidabilità dei sistemi ML. Questi attacchi comportano la creazione di input attentamente progettati, noti come “esempi avversari”, per ingannare i modelli ML e fargli fare previsioni errate. Per proteggere i sistemi ML dagli attacchi avversari, è fondamentale sviluppare tecniche efficaci per rilevare e mitigare queste minacce.\n\nTecniche di Rilevamento degli Esempi Avversari\nIl rilevamento degli esempi avversari è la prima linea di difesa contro gli attacchi avversari. Sono state proposte diverse tecniche per identificare e segnalare input sospetti che potrebbero essere avversari.\nI metodi statistici mirano a rilevare gli esempi avversari analizzando le proprietà statistiche dei dati di input. Questi metodi spesso confrontano la distribuzione dei dati di input con una di riferimento, come quella dei dati di training o una nota distribuzione benigna. Tecniche come il test Kolmogorov-Smirnov (Berger e Zhou 2014) o il test Anderson-Darling possono essere utilizzate per misurare la discrepanza tra le distribuzioni e segnalare gli input che si discostano in modo significativo dalla distribuzione prevista.\n\nBerger, Vance W, e YanYan Zhou. 2014. «Kolmogorovsmirnov test: Overview». Wiley statsref: Statistics reference online.\nKernel density estimation (KDE) è una tecnica non parametrica utilizzata per stimare la funzione di densità di probabilità di un set di dati. Nel contesto del rilevamento di esempi avversari, KDE può essere utilizzato per stimare la densità di esempi benigni nello spazio di input. Gli esempi avversari spesso si trovano in regioni a bassa densità e possono essere rilevati confrontando la loro densità stimata con una soglia. Gli input con una densità stimata al di sotto della soglia vengono segnalati come potenziali esempi avversari.\nUn’altra tecnica è la compressione delle feature (Panda, Chakraborty, e Roy 2019), che riduce la complessità dello spazio di input applicando la riduzione della dimensionalità o la discretizzazione. L’idea alla base della compressione delle feature è che gli esempi avversari spesso si basano su piccole perturbazioni impercettibili che possono essere eliminate o ridotte tramite queste trasformazioni. Le incongruenze possono essere rilevate confrontando le previsioni del modello sull’input originale e sull’input compresso, indicando la presenza di esempi avversari.\n\nPanda, Priyadarshini, Indranil Chakraborty, e Kaushik Roy. 2019. «Discretization Based Solutions for Secure Machine Learning Against Adversarial Attacks». #IEEE_O_ACC# 7: 70157–68. https://doi.org/10.1109/access.2019.2919463.\nLe tecniche di stima dell’incertezza del modello mirano a quantificare la fiducia o l’incertezza associata alle previsioni di un modello. Gli esempi avversari spesso sfruttano regioni di elevata incertezza nel confine di decisione del modello. Stimando l’incertezza utilizzando tecniche come reti neurali bayesiane, stima dell’incertezza basata su dropout o metodi di ensemble, gli input con elevata incertezza possono essere contrassegnati come potenziali esempi avversari.\n\n\nStrategie di Difesa Avversarie\nUna volta rilevati gli esempi avversari, possono essere impiegate varie strategie di difesa per mitigarne l’impatto e migliorare la robustezza dei modelli ML.\nL’addestramento avversario è una tecnica che prevede l’aumento dei dati di addestramento con esempi avversari e il riaddestramento del modello su questo set di dati aumentato. Esporre il modello a esempi avversari durante l’addestramento gli insegna a classificarli correttamente e diventa più robusto agli attacchi avversari. L’addestramento avversario può essere eseguito utilizzando vari metodi di attacco, come il Fast Gradient Sign Method (FGSM) o il Projected Gradient Descent (PGD) (Madry et al. 2017).\n\nMadry, Aleksander, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, e Adrian Vladu. 2017. «Towards deep learning models resistant to adversarial attacks». arXiv preprint arXiv:1706.06083.\n\nPapernot, Nicolas, Patrick McDaniel, Xi Wu, Somesh Jha, e Ananthram Swami. 2016. «Distillation as a Defense to Adversarial Perturbations Against Deep Neural Networks». In 2016 IEEE Symposium on Security and Privacy (SP), 582–97. IEEE; IEEE. https://doi.org/10.1109/sp.2016.41.\nLa distillazione difensiva (Papernot et al. 2016) è una tecnica che addestra un secondo modello (il modello studente) per imitare il comportamento di quello originale (il modello insegnante). Il modello studente viene addestrato sulle etichette soft prodotte dal modello insegnante, che sono meno sensibili alle piccole perturbazioni. L’utilizzo del modello studente per l’inferenza può ridurre l’impatto delle perturbazioni avversarie, poiché il modello studente impara a generalizzare meglio ed è meno sensibile al rumore avversario.\nLe tecniche di pre-elaborazione e trasformazione dell’input mirano a rimuovere o mitigare l’effetto delle perturbazioni avversarie prima di alimentare l’input nel modello ML. Queste tecniche includono la rimozione del rumore dalle immagini, la compressione JPEG, il ridimensionamento casuale, il padding o l’applicazione di trasformazioni casuali ai dati di input. Riducendo l’impatto delle perturbazioni avversarie, questi passaggi di pre-elaborazione possono aiutare a migliorare la robustezza del modello agli attacchi avversari.\nI metodi ensemble combinano più modelli per fare previsioni più robuste. L’ensemble può ridurre l’impatto degli attacchi avversari utilizzando un set diversificato di modelli con diverse architetture, dati di training o iperparametri. Esempi avversari che ingannano un modello potrebbero non ingannare gli altri nell’insieme, portando a previsioni più affidabili e robuste. Le tecniche di diversificazione del modello, come l’utilizzo di diverse tecniche di pre-elaborazione o rappresentazioni delle caratteristiche per ogni modello nell’insieme, possono migliorare ulteriormente la robustezza.\n\n\nValutazione e Test della Robustezza\nCondurre valutazioni e test approfonditi per valutare l’efficacia delle tecniche di difesa avversarie e misurare la robustezza dei modelli ML.\nLe metriche di robustezza avversaria quantificano la resilienza del modello agli attacchi avversari. Queste metriche possono includere l’accuratezza del modello sugli esempi avversari, la distorsione media richiesta per ingannare il modello o le prestazioni del modello in base a diversi livelli di attacco. Confrontando queste metriche tra diversi modelli o tecniche di difesa, i professionisti possono valutare e confrontare i loro livelli di robustezza.\nI benchmark e i set di dati standardizzati per gli attacchi avversari forniscono una base comune per valutare e confrontare la robustezza dei modelli ML. Questi benchmark includono set di dati con esempi avversari pre-generati e strumenti e framework per generare attacchi avversari. Esempi di benchmark di attacchi avversari popolari includono i set di dati MNIST-C, CIFAR-10-C e ImageNet-C (Hendrycks e Dietterich 2019), che contengono versioni corrotte o perturbate dei set di dati originali.\n\nHendrycks, Dan, e Thomas Dietterich. 2019. «Benchmarking neural network robustness to common corruptions and perturbations». arXiv preprint arXiv:1903.12261.\nI professionisti possono sviluppare sistemi ML più robusti e resilienti sfruttando queste tecniche di rilevamento di esempi avversari, strategie di difesa e metodi di valutazione della robustezza. Tuttavia, è importante notare che la robustezza avversaria è un’area di ricerca in corso e nessuna tecnica singola fornisce una protezione completa contro tutti i tipi di attacchi avversari. Un approccio completo che combina più meccanismi di difesa e test regolari è essenziale per mantenere la sicurezza e l’affidabilità dei sistemi ML di fronte alle minacce avversarie in evoluzione.\n\n\n\nAvvelenamento dei Dati\nSi ricorda che il data poisoning è un attacco che prende di mira l’integrità dei dati di training utilizzati per creare modelli ML. Manipolando o corrompendo i dati di training, gli aggressori possono influenzare il comportamento del modello e fargli fare previsioni errate o eseguire azioni indesiderate. Rilevare e mitigare gli attacchi di data poisoning è fondamentale per garantire l’affidabilità e la sicurezza dei sistemi ML, come mostrato in Figura 17.30.\n\n\n\n\n\n\nFigura 17.30: Iniezione di dati dannosi. Fonte: Li\n\n\n\n\nTecniche di rilevamento delle anomalie per identificare i Dati Avvelenati\nI metodi di rilevamento statistico degli outlier identificano i dati che si discostano in modo significativo dalla maggior parte. Questi metodi presuppongono che le istanze di dati avvelenati siano probabilmente “outlier” statistici”. Tecniche come il Metodo Z-score, il Metodo di Tukey o la Distanza di Mahalanobis possono essere utilizzate per misurare la deviazione di ciascun punto dati dalla tendenza centrale del set di dati. I dati che superano una soglia predefinita vengono contrassegnati come potenziali valori anomali e considerati sospetti di avvelenamento dei dati.\nI metodi basati sul clustering raggruppano dati simili in base alle loro caratteristiche o attributi. Il presupposto è che le istanze di dati avvelenate possano formare cluster distinti o trovarsi lontano dai normali cluster di dati. Applicando algoritmi di clustering come K-means, DBSCAN o clustering gerarchico, è possibile identificare cluster anomali o dati che non appartengono a nessun cluster. Queste istanze anomale vengono poi trattate come dati potenzialmente avvelenati.\nGli autoencoder sono reti neurali addestrate per ricostruire i dati di input da una rappresentazione compressa, come mostrato in Figura 17.31. Possono essere utilizzati per il rilevamento di anomalie apprendendo i pattern normali nei dati e identificando le istanze che si discostano da essi. Durante l’addestramento, l’autoencoder viene addestrato su dati puliti e non avvelenati. Al momento dell’inferenza, viene calcolato l’errore di ricostruzione per ogni dato. I dati con errori di ricostruzione elevati sono considerati anomali e potenzialmente avvelenati, poiché non sono conformi ai pattern normali appresi.\n\n\n\n\n\n\nFigura 17.31: Autoencoder. Fonte: Dertat\n\n\n\n\n\nTecniche di Sanificazione e Preelaborazione dei Dati\nL’avvelenamento dei dati può essere evitato pulendo i dati, il che implica l’identificazione e la rimozione o la correzione di dati rumorosi, incompleti o incoerenti. Tecniche come la deduplicazione dei dati, l’imputazione dei valori mancanti e la rimozione dei valori anomali possono essere applicate per migliorare la qualità dei dati di addestramento. Eliminando o filtrando i dati sospetti o anomali, è possibile ridurre l’impatto delle istanze avvelenate.\nLa validazione dei dati implica la verifica dell’integrità e della coerenza dei dati di training. Ciò può includere il controllo della coerenza del tipo di dati, la convalida dell’intervallo e le dipendenze tra campi. Definendo e applicando le regole di validazione dei dati, i dati anomali o incoerenti indicativi di avvelenamento possono essere identificati e segnalati per ulteriori indagini.\nLa provenienza dei dati e il tracciamento della discendenza implicano il mantenimento di un registro dell’origine, delle trasformazioni e dei movimenti dei dati in tutta la pipeline ML. Documentando le fonti dei dati, i passaggi di pre-elaborazione e qualsiasi modifica apportata, i professionisti possono risalire alle anomalie o ai pattern sospetti fino alla loro origine. Ciò aiuta a identificare potenziali punti di avvelenamento dei dati e facilita il processo di indagine e mitigazione.\n\n\nTecniche di Training Robusti\nÈ possibile utilizzare tecniche di ottimizzazione robuste per modificare l’obiettivo del training per ridurre al minimo l’impatto di valori anomali o istanze avvelenate. Ciò può essere ottenuto utilizzando funzioni di perdita robuste meno sensibili ai valori estremi, come la “Huber loss” o la “modified Huber loss”. Le tecniche di regolarizzazione, come la regolarizzazione L1 o L2, possono anche aiutare a ridurre la sensibilità del modello ai dati avvelenati, limitando la complessità del modello e prevenendo l’overfitting.\nLe funzioni di “loss” [perdita] robuste sono progettate per essere meno sensibili ai valori anomali o ai dati rumorosi. Esempi includono la Huber loss modificata, la perdita di Tukey (Beaton e Tukey 1974) e la “trimmed mean loss”. Queste funzioni di perdita riducono o ignorano il contributo delle istanze anomale durante il training, riducendo il loro impatto sul processo di apprendimento del modello. Le funzioni “obiettivo” robuste, come l’obiettivo minimax o la “distributivamente robusto”, mirano a ottimizzare le prestazioni del modello negli scenari peggiori o in presenza di perturbazioni avversarie.\n\nBeaton, Albert E., e John W. Tukey. 1974. «The Fitting of Power Series, Meaning Polynomials, Illustrated on Band-Spectroscopic Data». Technometrics 16 (2): 147. https://doi.org/10.2307/1267936.\nLe tecniche di “data augmentation” comportano la generazione di esempi di addestramento aggiuntivi applicando trasformazioni o perturbazioni casuali ai dati esistenti Figura 17.32. Ciò aiuta ad aumentare la diversità e la robustezza del set di dati di addestramento. Introducendo variazioni controllate nei dati, il modello diventa meno sensibile a pattern o artefatti specifici che possono essere presenti in istanze avvelenate. Le tecniche di randomizzazione, come il sottocampionamento casuale o l’aggregazione bootstrap, possono anche aiutare a ridurre l’impatto dei dati avvelenati addestrando più modelli su diversi sottoinsiemi di dati e combinando le loro previsioni.\n\n\n\n\n\n\nFigura 17.32: Un’immagine del numero “3” nella forma originale e con applicati degli aumenti di base.\n\n\n\n\n\nApprovvigionamento di Dati Sicuro e Affidabile\nL’implementazione delle migliori pratiche di raccolta e cura dei dati può aiutare a mitigare il rischio di avvelenamento dei dati. Ciò include l’istituzione di protocolli di raccolta dati chiari, la verifica dell’autenticità e dell’affidabilità delle fonti dati e la conduzione di valutazioni regolari della qualità dei dati. L’approvvigionamento di dati da provider affidabili e rispettabili e il rispetto di pratiche di gestione dei dati sicure possono ridurre la probabilità di introdurre dati avvelenati nella pipeline di training.\nSolidi meccanismi di governance dei dati e controllo degli accessi sono essenziali per prevenire modifiche non autorizzate o manomissioni dei dati di training. Ciò implica la definizione di ruoli e responsabilità chiari per l’accesso ai dati, l’implementazione di policy di controllo degli accessi basate sul principio del privilegio minimo e il monitoraggio e il logging delle attività di accesso ai dati. Limitando l’accesso ai dati di training e mantenendo un audit trail, è possibile rilevare e investigare potenziali tentativi di avvelenamento dei dati.\nRilevare e mitigare gli attacchi di avvelenamento dei dati richiede un approccio poliedrico che combini rilevamento delle anomalie, sanificazione dei dati, tecniche di training affidabili e pratiche di approvvigionamento dei dati sicure. Implementando queste misure, i professionisti del ML possono migliorare la resilienza dei loro modelli contro l’avvelenamento dei dati e garantire l’integrità e l’affidabilità dei dati di training. Tuttavia, è importante notare che l’avvelenamento dei dati è un’area di ricerca attiva e continuano a emergere nuovi vettori di attacco e meccanismi di difesa. Rimanere informati sugli ultimi sviluppi e adottare un approccio proattivo e adattivo alla sicurezza dei dati è fondamentale per mantenere la robustezza dei sistemi ML.\n\n\n\nDistribution Shift\n\nRilevamento e Mitigazione dei “Distribution Shift”\nRicordiamo che i “distribution shift” [spostamenti di distribuzione] si verificano quando la distribuzione dei dati incontrata da un modello di machine learning (ML) durante l’implementazione differisce dalla distribuzione su cui è stato addestrato. Questi spostamenti possono avere un impatto significativo sulle prestazioni e sulla capacità di generalizzazione del modello, portando a previsioni non ottimali o errate. Rilevare e mitigare i “distribution shift” è fondamentale per garantire la robustezza e l’affidabilità dei sistemi ML in scenari reali.\n\n\nTecniche di Rilevamento per i “Distribution Shift”\nI test statistici possono essere utilizzati per confrontare le distribuzioni dei dati di training e di test per identificare differenze significative. Tecniche come il test di Kolmogorov-Smirnov o il test di Anderson-Darling misurano la discrepanza tra due distribuzioni e forniscono una valutazione quantitativa della presenza di un “distribution shift”. Applicando questi test alle funzionalità di input o alle previsioni del modello, i professionisti possono rilevare se esiste una differenza statisticamente significativa tra le distribuzioni di training e di test.\nLe metriche di divergenza quantificano la dissimilarità tra due distribuzioni di probabilità. Le metriche di divergenza comunemente utilizzate includono la Divergenza Kullback-Leibler (KL) e la Divergenza Jensen-Shannon (JS). Calcolando la divergenza tra le distribuzioni dei dati di training e di test, i professionisti possono valutare l’entità dello “spostamento della distribuzione”. Valori di divergenza elevati indicano una differenza significativa tra le distribuzioni, suggerendo la presenza di uno spostamento della distribuzione.\nLe tecniche di quantificazione dell’incertezza, come le reti neurali bayesiane o i metodi di ensemble, possono stimare l’incertezza associata alle previsioni del modello. Quando un modello viene applicato a dati da una distribuzione diversa, le sue previsioni potrebbero avere un’incertezza maggiore. Monitorando i livelli di incertezza, i professionisti possono rilevare gli spostamenti della distribuzione. Se l’incertezza supera costantemente una soglia predeterminata per i campioni di test, ciò suggerisce che il modello sta operando al di fuori della sua distribuzione addestrata.\nInoltre, i classificatori di dominio sono addestrati a distinguere tra diversi domini o distribuzioni. I professionisti possono rilevare gli spostamenti di distribuzione addestrando un classificatore a distinguere tra i domini di addestramento e di test. Se il classificatore di dominio raggiunge un’elevata accuratezza nel distinguere tra i due domini, indica una differenza significativa nelle distribuzioni sottostanti. Le prestazioni del classificatore di dominio servono come misura dello spostamento di distribuzione.\n\n\nTecniche di Mitigazione per i “Distribution Shift”\nIl “transfer learning” [trasferimento dell’apprendimento.] sfrutta le conoscenze acquisite da un dominio per migliorare le prestazioni in un altro, come mostrato in Figura 17.33. Utilizzando modelli pre-addestrati o trasferendo le feature apprese da un dominio di origine a un dominio di destinazione, il transfer learning può aiutare a mitigare l’impatto dei “distribution shift”. Il modello pre-addestrato può essere messo a punto su una piccola quantità di dati etichettati dal dominio target, consentendogli di adattarsi alla nuova distribuzione. Il transfer learning è particolarmente efficace quando i domini di origine e di destinazione condividono caratteristiche simili o quando i dati etichettati nel dominio di destinazione sono scarsi.\n\n\n\n\n\n\nFigura 17.33: Trasferimento dell’apprendimento. Fonte: Bhavsar\n\n\n\nL’apprendimento continuo, noto anche come apprendimento permanente, consente ai modelli ML di apprendere continuamente da nuove distribuzioni di dati, mantenendo al contempo le conoscenze delle distribuzioni precedenti. Tecniche come la “elastic weight consolidation (EWC)” (Kirkpatrick et al. 2017) o la “gradient episodic memory (GEM)” (Lopez-Paz e Ranzato 2017) consentono ai modelli di adattarsi alle distribuzioni di dati in evoluzione nel tempo. Queste tecniche mirano a bilanciare la plasticità del modello (capacità di apprendere da nuovi dati) con la stabilità del modello (mantenendo le conoscenze apprese in precedenza). Aggiornando gradualmente il modello con nuovi dati e mitigando l’oblio catastrofico, l’apprendimento continuo aiuta i modelli a rimanere robusti ai “distribution shift”.\n\nKirkpatrick, James, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A. Rusu, Kieran Milan, et al. 2017. «Overcoming catastrophic forgetting in neural networks». Proc. Natl. Acad. Sci. 114 (13): 3521–26. https://doi.org/10.1073/pnas.1611835114.\n\nLopez-Paz, David, e Marc’Aurelio Ranzato. 2017. «Gradient episodic memory for continual learning». Adv Neural Inf Process Syst 30.\nLe tecniche di aumento dei dati, come quelle viste in precedenza, comportano l’applicazione di trasformazioni o perturbazioni ai dati di training esistenti per aumentarne la diversità e migliorare la robustezza del modello ai “distribution shift”. Introducendo variazioni nei dati, come rotazioni, traslazioni, ridimensionamenti o aggiunta di rumore, l’aumento dei dati aiuta il modello ad apprendere caratteristiche invarianti e a generalizzare meglio a distribuzioni mai viste. Il “data augmentation” può essere eseguito durante l’addestramento e l’inferenza per migliorare la capacità del modello di gestire i “distribution shift”.\nI metodi ensemble combinano più modelli per rendere le previsioni più robuste ai “distribution shift”. Addestrando i modelli su diversi sottoinsiemi di dati, utilizzando algoritmi diversi o con diversi iperparametri, i metodi ensemble possono catturare diversi aspetti della distribuzione dei dati. Quando viene presentata una distribuzione “spostata”, l’ensemble può sfruttare i punti di forza dei singoli modelli per fare previsioni più accurate e stabili. Tecniche come il bagging, il boosting o lo stacking possono creare ensemble efficaci.\nAggiornare regolarmente i modelli con nuovi dati dalla distribuzione target è fondamentale per mitigare l’impatto dei “distribution shift”. Man mano che la distribuzione dei dati si evolve, i modelli dovrebbero essere riaddestrati o perfezionati sui dati disponibili più recenti per adattarsi ai pattern mutevoli. Il monitoraggio delle prestazioni del modello e delle caratteristiche dei dati può aiutare a rilevare quando è necessario un aggiornamento. Mantenendo aggiornati i modelli, i professionisti possono garantire che rimangano pertinenti e accurati di fronte ai distribution shift”.\nLa valutazione dei modelli utilizzando metriche robuste meno sensibili ai “distribution shift” può fornire una valutazione più affidabile delle prestazioni del modello. Metriche come l’“area under the precision-recall curve (AUPRC)” [area sotto la curva di precisione-richiamo] o il punteggio F1 sono più robuste allo squilibrio di classe e possono catturare meglio le prestazioni del modello su diverse distribuzioni. Inoltre, l’utilizzo di metriche di valutazione specifiche del dominio che si allineano con i risultati desiderati nel dominio target può fornire una misura più significativa dell’efficacia del modello.\nRilevare e mitigare i “distribution shift” è un processo continuo che richiede monitoraggio, adattamento e miglioramento continui. Utilizzando una combinazione di tecniche di rilevamento e strategie di mitigazione, i professionisti del ML possono identificare e affrontare in modo proattivo i “distribution shift”, garantendo la robustezza e l’affidabilità dei loro modelli nelle distribuzioni del mondo reale. È importante notare che i “distribution shift” possono assumere varie forme e potrebbero richiedere approcci specifici del dominio a seconda della natura dei dati e dell’applicazione. Rimanere informati sulle ultime ricerche e sulle best practice nella gestione dei “distribution shift” è essenziale per creare sistemi ML resilienti.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>IA Robusta</span>"
    ]
  },
  {
    "objectID": "contents/core/robust_ai/robust_ai.it.html#errori-software",
    "href": "contents/core/robust_ai/robust_ai.it.html#errori-software",
    "title": "17  IA Robusta",
    "section": "17.5 Errori Software",
    "text": "17.5 Errori Software\n\n17.5.1 Definizione e Caratteristiche\nGli errori software si riferiscono a difetti, errori o bug nei framework software runtime e nei componenti che supportano l’esecuzione e la distribuzione di modelli ML (Myllyaho et al. 2022). Questi guasti possono derivare da varie fonti, come errori di programmazione, difetti di progettazione o problemi di compatibilità (H. Zhang 2008), e possono avere implicazioni significative per le prestazioni, l’affidabilità e la sicurezza dei sistemi ML. Gli errori software nei framework ML presentano diverse caratteristiche chiave:\n\nMyllyaho, Lalli, Mikko Raatikainen, Tomi Männistö, Jukka K. Nurminen, e Tommi Mikkonen. 2022. «On misbehaviour and fault tolerance in machine learning systems». J. Syst. Software 183 (gennaio): 111096. https://doi.org/10.1016/j.jss.2021.111096.\n\nZhang, Hongyu. 2008. «On the Distribution of Software Faults». IEEE Trans. Software Eng. 34 (2): 301–2. https://doi.org/10.1109/tse.2007.70771.\n\nDiversità: Gli errori software possono manifestarsi in forme diverse, che vanno da semplici errori di logica e sintassi a problemi più complessi come perdite di memoria, condizioni di “race” e problemi di integrazione. La varietà di tipi di errori aumenta la sfida di rilevarli e mitigarli in modo efficace.\nPropagazione: Nei sistemi ML, gli errori software possono propagarsi attraverso i vari layer e componenti del framework. Un errore in un modulo può innescare una cascata di errori o comportamenti imprevisti in altre parti del sistema, rendendo difficile individuare la causa principale e valutare l’impatto completo dell’errore.\nIntermittenza: Alcuni errori software possono presentare un comportamento intermittente, che si verifica sporadicamente o in condizioni specifiche. Questi errori possono essere particolarmente difficili da riprodurre e correggere, poiché possono manifestarsi in modo incoerente durante i test o il normale funzionamento.\nInterazione con i modelli ML: Gli errori software nei framework ML possono interagire con i modelli addestrati in modi sottili. Ad esempio, un errore nella pipeline di preelaborazione dei dati può introdurre rumore o distorsione negli input del modello, causando prestazioni degradate o previsioni errate. Analogamente, gli errori nel componente di servizio del modello possono causare incongruenze tra gli ambienti di training e inferenza.\nImpatto sulle proprietà del sistema: Gli errori software possono compromettere varie proprietà desiderabili dei sistemi ML, come prestazioni, scalabilità, affidabilità e sicurezza. Gli errori possono causare rallentamenti, crash, output errati o vulnerabilità che gli aggressori possono sfruttare.\nDipendenza da fattori esterni: Il verificarsi e l’impatto degli errori software nei framework ML dipendono spesso da fattori esterni, come la scelta di hardware, sistema operativo, librerie e configurazioni. Problemi di compatibilità e mancate corrispondenze di versione possono introdurre errori difficili da anticipare e mitigare.\n\nComprendere le caratteristiche degli errori software nei framework ML è fondamentale per sviluppare strategie efficaci di prevenzione, rilevamento e mitigazione degli errori. Riconoscendo la diversità, la propagazione, l’intermittenza e l’impatto dei guasti software, i professionisti del ML possono progettare sistemi più robusti e affidabili, resilienti a questi problemi.\n\n\n17.5.2 Meccanismi degli Errori Software nei Framework ML\nI framework di apprendimento automatico, come TensorFlow, PyTorch e sci-kit-learn, forniscono potenti strumenti e astrazioni per la creazione e l’implementazione di modelli ML. Tuttavia, questi framework non sono immuni da errori software che possono influire sulle prestazioni, l’affidabilità e la correttezza dei sistemi ML. Esploriamo alcuni degli errori software comuni che possono verificarsi nei framework ML:\nMemory Leak e Problemi di Gestione delle Risorse: Una gestione della memoria non corretta, come il mancato rilascio di memoria o la chiusura di handle di file, può portare a perdite di memoria e all’esaurimento delle risorse nel tempo. Questo problema è aggravato dall’utilizzo inefficiente della memoria, in cui la creazione di copie non necessarie di grandi tensori o il mancato sfruttamento di strutture dati efficienti in termini di memoria può causare un consumo eccessivo di memoria e degradare le prestazioni del sistema. Inoltre, la mancata gestione corretta della memoria GPU può causare errori di “out-of-memory” o un utilizzo non ottimale delle risorse GPU, aggravando ulteriormente il problema come mostrato in Figura 17.34.\n\n\n\n\n\n\nFigura 17.34: Esempio di problemi di memoria e utilizzo non ottimale della GPU\n\n\n\nProblemi di Sincronizzazione e Concorrenza: Una sincronizzazione non corretta tra thread o processi può causare condizioni di “race”, deadlock o comportamento incoerente nei sistemi ML multi-thread o distribuiti. Questo problema è spesso legato alla gestione impropria delle operazioni asincrone, come I/O non bloccante o caricamento dati parallelo, che può causare problemi di sincronizzazione e influire sulla correttezza della pipeline ML. Inoltre, un coordinamento e una comunicazione adeguati tra nodi distribuiti in un cluster possono causare coerenza o dati obsoleti durante l’addestramento o l’inferenza, compromettendo l’affidabilità del sistema ML.\nProblemi di Compatibilità: Le discrepanze tra le versioni di framework, librerie o dipendenze ML possono introdurre problemi di compatibilità ed errori di runtime. L’aggiornamento o la modifica delle versioni delle librerie sottostanti senza testare a fondo l’impatto sul sistema ML può portare a comportamenti imprevisti o malfunzionamenti. Inoltre, le incongruenze tra gli ambienti di training e distribuzione, come differenze nell’hardware, nei sistemi operativi o nelle versioni dei pacchetti, possono causare problemi di compatibilità e influire sulla riproducibilità dei modelli ML, rendendo difficile garantire prestazioni coerenti sulle diverse piattaforme.\nInstabilità Numerica ed Errori di Precisione: Una gestione inadeguata delle instabilità numeriche, come la divisione per zero, l’underflow o l’overflow, può portare a calcoli errati o problemi di convergenza durante l’addestramento. Questo problema è aggravato da errori di precisione o arrotondamento insufficienti, che possono accumularsi nel tempo e influire sull’accuratezza dei modelli ML, specialmente nelle architetture di deep learning con molti livelli. Inoltre, un ridimensionamento o una normalizzazione impropri dei dati di input possono causare instabilità numeriche e influire sulla convergenza e sulle prestazioni degli algoritmi di ottimizzazione, con conseguenti prestazioni del modello non ottimali o inaffidabili.\nGestione degli Errori e delle Eccezioni Inadeguata: Una corretta gestione degli errori e delle eccezioni può impedire ai sistemi ML di bloccarsi o comportarsi in modo imprevisto quando si verificano condizioni eccezionali o input non validi. Non riuscire a catturare e gestire eccezioni specifiche o affidarsi alla gestione generica delle eccezioni può rendere difficile diagnosticare e recuperare gli errori in modo corretto, portando a instabilità del sistema e affidabilità ridotta. Inoltre, messaggi di errore incompleti o fuorvianti possono ostacolare la capacità di eseguire il debug e risolvere efficacemente gli errori software nei framework ML, prolungando il tempo necessario per identificare e risolvere i problemi.\n\n\n17.5.3 Impatto sui Sistemi ML\nGli errori software nei framework di apprendimento automatico possono avere impatti significativi e di vasta portata sulle prestazioni, l’affidabilità e la sicurezza dei sistemi ML. Esploriamo i vari modi in cui gli errori software possono influenzare i sistemi ML:\nDegrado delle Prestazioni e Rallentamenti del Sistema: Memory leak e gestione inefficiente delle risorse possono portare a un graduale degrado delle prestazioni nel tempo, poiché il sistema diventa sempre più vincolato dalla memoria e impiega più tempo nella garbage collection o nello swapping della memoria (Maas et al. 2024). Questo problema è aggravato da problemi di sincronizzazione e bug di concorrenza, che possono causare ritardi, riduzione della produttività e utilizzo non ottimale delle risorse di elaborazione, in particolare nei sistemi ML multi-thread o distribuiti. Inoltre, problemi di compatibilità o percorsi di codice inefficienti possono introdurre ulteriori overhead e rallentamenti, influenzando le prestazioni complessive del sistema ML.\n\nMaas, Martin, David G. Andersen, Michael Isard, Mohammad Mahdi Javanmard, Kathryn S. McKinley, e Colin Raffel. 2024. «Combining Machine Learning and Lifetime-Based Resource Management for Memory Allocation and Beyond». Commun. ACM 67 (4): 87–96. https://doi.org/10.1145/3611018.\nPrevisioni o Output Errati: Gli errori software nella pre-elaborazione dei dati, nell’ingegneria delle feature o nella valutazione del modello possono introdurre distorsioni, rumore o errori che si propagano attraverso la pipeline ML e che determinano previsioni o output errati. Nel tempo, instabilità numeriche, errori di precisione o problemi di arrotondamento possono accumularsi e portare a problemi di accuratezza o convergenza degradati nei modelli addestrati. Inoltre, gli errori nei componenti di servizio o inferenza del modello possono causare incongruenze tra gli output previsti e quelli effettivi, portando a previsioni errate o inaffidabili in produzione.\nProblemi di Affidabilità e Stabilità: Gli errori software possono causare eccezioni senza precedenti, crash o terminazioni improvvise che possono compromettere l’affidabilità e la stabilità dei sistemi ML, specialmente negli ambienti di produzione. Gli errori intermittenti o sporadici possono essere difficili da riprodurre e diagnosticare, portando a un comportamento imprevedibile e a una ridotta fiducia negli output del sistema ML. Inoltre, errori nel checkpointing, nella serializzazione del modello o nella gestione dello stato possono causare perdite di dati o incongruenze, influenzando l’affidabilità e la recuperabilità del sistema ML.\nVulnerabilità di Sicurezza: Errori software, come buffer overflow, vulnerabilità di “injection” o controllo di accesso improprio, possono introdurre rischi per la sicurezza ed esporre il sistema ML a potenziali attacchi o accessi non autorizzati. Gli avversari possono sfruttare errori nelle fasi di pre-elaborazione o estrazione delle funzionalità per manipolare i dati di input e ingannare i modelli ML, portando a comportamenti errati o dannosi. Inoltre, una protezione inadeguata dei dati sensibili, come le informazioni utente o i parametri riservati del modello, può portare a violazioni dei dati o violazioni della privacy (Q. Li et al. 2023).\n\nLi, Qinbin, Zeyi Wen, Zhaomin Wu, Sixu Hu, Naibo Wang, Yuan Li, Xu Liu, e Bingsheng He. 2023. «A Survey on Federated Learning Systems: Vision, Hype and Reality for Data Privacy and Protection». IEEE Trans. Knowl. Data Eng. 35 (4): 3347–66. https://doi.org/10.1109/tkde.2021.3124599.\nDifficoltà nella Riproduzione e nel Debug: Gli errori software possono rendere difficile la riproduzione e il debug dei problemi nei sistemi ML, soprattutto quando gli errori sono intermittenti o dipendono da condizioni di runtime specifiche. Messaggi di errore incompleti o ambigui, uniti alla complessità dei framework e dei modelli ML, possono prolungare il processo di debug e ostacolare la capacità di identificare e correggere i guasti sottostanti. Inoltre, le incongruenze tra gli ambienti di sviluppo, test e produzione possono rendere difficile la riproduzione e la diagnosi dei guasti in contesti specifici.\nMaggiori Costi di Sviluppo e Manutenzione I guasti software possono comportare maggiori costi di sviluppo e manutenzione, poiché i team dedicano più tempo e risorse al debug, alla correzione e alla validazione del sistema ML. La necessità di test estesi, monitoraggio e meccanismi di tolleranza agli errori per mitigare l’impatto degli errori software può aggiungere complessità e sovraccarico al processo di sviluppo ML. Patch, aggiornamenti e correzioni di bug frequenti per risolvere gli errori software possono interrompere il flusso di lavoro di sviluppo e richiedere sforzi aggiuntivi per garantire la stabilità e la compatibilità del sistema ML.\nComprendere il potenziale impatto degli errori software sui sistemi ML è fondamentale per dare priorità agli sforzi di test, implementare progetti di tolleranza agli errori e stabilire pratiche di monitoraggio e debug efficaci. Affrontando in modo proattivo gli errori software e le loro conseguenze, i professionisti ML possono creare sistemi ML più solidi, affidabili e sicuri che forniscono risultati accurati e affidabili.\n\n\n17.5.4 Rilevamento e Mitigazione\nRilevare e mitigare i guasti software nei framework di apprendimento automatico è essenziale per garantire l’affidabilità, le prestazioni e la sicurezza dei sistemi ML. Esploriamo varie tecniche e approcci che possono essere impiegati per identificare e risolvere efficacemente i guasti software:\nTest e Validazione Approfonditi: “Unit test” completi di singoli componenti e moduli possono verificarne la correttezza e identificare potenziali guasti nelle prime fasi dello sviluppo. I test di integrazione convalidano l’interazione e la compatibilità tra diversi componenti del framework ML, garantendo un’integrazione senza soluzione di continuità. I test sistematici di casi limite, condizioni al contorno e scenari eccezionali aiutano a scoprire guasti e vulnerabilità nascosti. Il “continuous testing” e i test di regressione come mostrato in Figura 17.35 rilevano i guasti introdotti da modifiche al codice o aggiornamenti al framework ML.\n\n\n\n\n\n\nFigura 17.35: Test di regressione automatizzati. Fonte: UTOR\n\n\n\nAnalisi Statica del Codice e Linting: L’utilizzo di strumenti di analisi statica del codice identifica automaticamente potenziali problemi di codifica, come errori di sintassi, variabili non definite o vulnerabilità di sicurezza. L’applicazione di standard di codifica e best practice tramite strumenti di “linting” mantiene la qualità del codice e riduce la probabilità di comuni errori di programmazione. L’esecuzione di revisioni regolari del codice consente l’ispezione manuale della base di codice, l’identificazione di potenziali errori e garantisce l’aderenza alle linee guida di codifica e ai principi di progettazione.\nMonitoraggio e Logging in Fase di Esecuzione: L’implementazione di meccanismi di logging completi cattura informazioni rilevanti durante l’esecuzione, come dati di input, parametri del modello ed eventi di sistema. Il monitoraggio delle metriche delle prestazioni chiave, dell’utilizzo delle risorse e dei tassi di errore aiuta a rilevare anomalie, colli di bottiglia delle prestazioni o comportamenti imprevisti. L’impiego di controlli di asserzione in fase di esecuzione e invarianti, convalida le ipotesi e rileva violazioni delle condizioni previste durante l’esecuzione del programma. L’utilizzo di strumenti di profilazione consente di identificare colli di bottiglia nelle prestazioni, memory leak o percorsi di codice inefficienti che potrebbero indicare la presenza di errori software.\nDesign Pattern a Tolleranza di Errore: L’implementazione di meccanismi di gestione degli errori e delle eccezioni consente una gestione e un ripristino controllato da condizioni eccezionali o errori di runtime. L’impiego di meccanismi di ridondanza e failover, come sistemi di backup o calcoli ridondanti, garantisce la disponibilità e l’affidabilità del sistema ML in presenza di errori. La progettazione di architetture modulari e debolmente accoppiate riduce al minimo la propagazione e l’impatto dei guasti su diversi componenti del sistema ML. L’utilizzo di meccanismi di checkpointing e ripristino (Eisenman et al. 2022) consente al sistema di riprendere da uno stato stabile noto in caso di guasti o interruzioni.\n\nEisenman, Assaf, Kiran Kumar Matam, Steven Ingram, Dheevatsa Mudigere, Raghuraman Krishnamoorthi, Krishnakumar Nair, Misha Smelyanskiy, e Murali Annavaram. 2022. «Check-N-Run: A checkpointing system for training deep learning recommendation models». In 19th USENIX Symposium on Networked Systems Design and Implementation (NSDI 22), 929–43.\nAggiornamenti e Patch Regolari: Rimanere aggiornati con le ultime versioni e patch dei framework, delle librerie e delle dipendenze ML offre vantaggi in termini di correzioni di bug, aggiornamenti di sicurezza e miglioramenti delle prestazioni. Il monitoraggio delle note di rilascio, degli avvisi di sicurezza e dei forum della community informa i professionisti su problemi noti, vulnerabilità o problemi di compatibilità nel framework ML. L’istituzione di un processo sistematico per testare e convalidare aggiornamenti e patch prima di applicarli ai sistemi di produzione garantisce stabilità e compatibilità.\nContainerizzazione e Isolamento: Sfruttando le tecnologie di containerizzazione, come Docker o Kubernetes, si incapsulano i componenti ML e le relative dipendenze in ambienti isolati. L’utilizzo della containerizzazione garantisce ambienti di runtime coerenti e riproducibili nelle fasi di sviluppo, test e produzione, riducendo la probabilità di problemi di compatibilità o errori specifici dell’ambiente. L’impiego di tecniche di isolamento, come ambienti virtuali o sandbox, impedisce che errori o vulnerabilità in un componente influiscano su altre parti del sistema ML.\nTest Automatizzati e Continuous Integration/Continuous Deployment (CI/CD): Implementare framework e script di test automatizzati, eseguire suite di test complete e individuare gli errori nelle prime fasi dello sviluppo. L’integrazione di test automatizzati nella pipeline CI/CD, come mostrato in Figura 17.36, garantisce che le modifiche al codice siano testate a fondo prima di essere unite o distribuite in produzione. L’utilizzo di sistemi di monitoraggio continuo e di allerta automatizzati rilevano e notificano a sviluppatori e operatori potenziali guasti o anomalie in tempo reale.\n\n\n\n\n\n\nFigura 17.36: Procedura di Continuous Integration/Continuous Deployment (CI/CD). Fonte: geeksforgeeks\n\n\n\nL’adozione di un approccio proattivo e sistematico al rilevamento e alla mitigazione degli errori può migliorare significativamente la robustezza, l’affidabilità e la manutenibilità dei sistemi ML. Investendo in pratiche complete di test, monitoraggio e progettazione tollerante agli errori, le organizzazioni possono ridurre al minimo l’impatto degli errori software e garantire il regolare funzionamento dei loro sistemi ML negli ambienti di produzione.\n\n\n\n\n\n\nEsercizio 17.4: Tolleranza agli Errori\n\n\n\n\n\nPreparatevi a diventare supereroi che combattono gli errori dell’IA! I problemi software possono far deragliare i sistemi di apprendimento automatico, ma in questo Colab impareremo come renderli resilienti. Simuleremo errori software per vedere come l’IA può “rompersi”, poi esploreremo tecniche per salvare i progressi del modello ML, come i checkpoint in un gioco. Vedremo come addestrare l’IA a riprendersi dopo un crash, assicurando che rimanga sulla buona strada. Questo è fondamentale per creare un’IA affidabile e degna di fiducia, soprattutto nelle applicazioni critiche. Quindi preparatevi perché questo Colab si collega direttamente al capitolo IA Robusta—passeremo dalla teoria alla risoluzione pratica dei problemi e creeremo sistemi di intelligenza artificiale in grado di gestire l’imprevisto!",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>IA Robusta</span>"
    ]
  },
  {
    "objectID": "contents/core/robust_ai/robust_ai.it.html#strumenti-e-framework",
    "href": "contents/core/robust_ai/robust_ai.it.html#strumenti-e-framework",
    "title": "17  IA Robusta",
    "section": "17.6 Strumenti e Framework",
    "text": "17.6 Strumenti e Framework\nData l’importanza di sviluppare sistemi di IA robusti, negli ultimi anni ricercatori e professionisti hanno sviluppato un’ampia gamma di strumenti e framework per comprendere come i guasti hardware si manifestano e si propagano per avere un impatto sui sistemi ML. Questi strumenti e framework svolgono un ruolo cruciale nella valutazione della resilienza dei sistemi ML ai guasti hardware simulando vari scenari di guasto e analizzandone l’impatto sulle prestazioni del sistema. Ciò consente ai progettisti di identificare potenziali vulnerabilità e sviluppare strategie di mitigazione efficaci, creando in definitiva sistemi ML più robusti e affidabili in grado di funzionare in sicurezza nonostante i guasti hardware. Questa sezione fornisce una panoramica dei modelli di guasto ampiamente utilizzati nella letteratura e degli strumenti e framework sviluppati per valutare l’impatto di tali guasti sui sistemi ML.\n\n17.6.1 Modelli di Guasto e Modelli di Errore\nCome discusso in precedenza, i guasti hardware possono manifestarsi in vari modi, tra cui guasti transitori, permanenti e intermittenti. Oltre al tipo di guasto in esame, è importante anche come si manifesta il guasto. Ad esempio, l’errore si verifica in una cella di memoria o durante il calcolo di un’unità funzionale? L’impatto è su un singolo bit o su più bit? L’errore si propaga per tutto il percorso e ha un impatto sull’applicazione (causando un errore) o viene mascherato rapidamente ed è considerato benigno? Tutti questi dettagli hanno un impatto su ciò che è noto come fault model [modello di errore], che svolge un ruolo importante nella simulazione e nella misurazione di ciò che accade a un sistema quando si verifica un errore.\nPer studiare e comprendere efficacemente l’impatto degli errori hardware sui sistemi ML, è essenziale comprendere i concetti di “fault model” e “error model”. Un “fault model” [guasto] descrive come si manifesta un errore hardware nel sistema, mentre un “error model” [modello di errore] rappresenta come l’errore si propaga e influisce sul comportamento del sistema.\nI “fault model” possono essere categorizzati in base a varie caratteristiche:\n\nDurata: I guasti transitori si verificano brevemente e poi scompaiono, mentre quelli permanenti persistono indefinitamente. I guasti intermittenti si verificano sporadicamente e possono essere difficili da diagnosticare.\nPosizione: I guasti possono verificarsi in componenti hardware, come celle di memoria, unità funzionali o interconnessioni.\nGranularità: I guastipossono interessare un singolo bit (ad esempio, bitflip) o più bit (ad esempio, errori burst) all’interno di un componente hardware.\n\nD’altro canto, gli “error model” descrivono come un guasto si propaga nel sistema e si manifesta come un errore. Un errore può causare la deviazione del sistema dal comportamento previsto, portando a risultati errati o persino a guasti del sistema. I modelli di errore possono essere definiti a diversi livelli di astrazione, da quello hardware (ad esempio, bitflip a livello di registro) al livello software (ad esempio, pesi o attivazioni corrotti in un modello ML).\nIl “fault model” (o il modello di errore, in genere la terminologia più applicabile per comprendere la robustezza di un sistema ML) svolge un ruolo importante nella simulazione e nella misura di ciò che accade a un sistema quando si verifica un guasto. Il modello scelto informa le ipotesi fatte sul sistema in fase di studio. Ad esempio, un sistema incentrato su errori transitori a bit singolo (Sangchoolie, Pattabiraman, e Karlsson 2017) non sarebbe adatto a comprendere l’impatto di errori permanenti di flip multi-bit (Wilkening et al. 2014), poiché è progettato presupponendo un modello completamente diverso.\n\nWilkening, Mark, Vilas Sridharan, Si Li, Fritz Previlon, Sudhanva Gurumurthi, e David R. Kaeli. 2014. «Calculating Architectural Vulnerability Factors for Spatial Multi-Bit Transient Faults». In 2014 47th Annual IEEE/ACM International Symposium on Microarchitecture, 293–305. IEEE; IEEE. https://doi.org/10.1109/micro.2014.15.\nInoltre, anche l’implementazione di un modello di errore è una considerazione importante, in particolare per quanto riguarda il punto in cui si dice che si verifichi un errore nello stack di elaborazione. Ad esempio, un modello di flip a bit singolo a livello di registro architetturale differisce da un modello di flip a bit singolo nel peso di un modello a livello di PyTorch. Sebbene entrambi mirino a un modello di errore simile, il primo verrebbe solitamente modellato in un simulatore architetturalmente accurato (come gem5 [binkert2011gem5]), che cattura la propagazione dell’errore rispetto al secondo, concentrandosi sulla propagazione del valore attraverso un modello.\nRicerche recenti hanno dimostrato che alcune caratteristiche dei modelli di errore possono mostrare comportamenti simili a diversi livelli di astrazione (Sangchoolie, Pattabiraman, e Karlsson 2017) (Papadimitriou e Gizopoulos 2021). Ad esempio, gli errori a bit singolo sono generalmente più problematici degli errori a bit multiplo, indipendentemente dal fatto che siano modellati a livello hardware o software. Tuttavia, altre caratteristiche, come il mascheramento degli errori (Mohanram e Touba 2003) come mostrato in Figura 17.37, potrebbero non essere sempre catturate accuratamente dai modelli a livello software, poiché possono nascondere gli effetti di sistema sottostanti.\n\nSangchoolie, Behrooz, Karthik Pattabiraman, e Johan Karlsson. 2017. «One Bit is (Not) Enough: An Empirical Study of the Impact of Single and Multiple Bit-Flip Errors». In 2017 47th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN), 97–108. IEEE; IEEE. https://doi.org/10.1109/dsn.2017.30.\n\nPapadimitriou, George, e Dimitris Gizopoulos. 2021. «Demystifying the System Vulnerability Stack: Transient Fault Effects Across the Layers». In 2021 ACM/IEEE 48th Annual International Symposium on Computer Architecture (ISCA), 902–15. IEEE; IEEE. https://doi.org/10.1109/isca52012.2021.00075.\n\nMohanram, K., e N. A. Touba. 2003. «Partial error masking to reduce soft error failure rate in logic circuits». In Proceedings. 16th IEEE Symposium on Computer Arithmetic, 433–40. IEEE; IEEE Comput. Soc. https://doi.org/10.1109/dftvs.2003.1250141.\n\n\n\n\n\n\nFigura 17.37: Esempio di mascheramento degli errori nei componenti microarchitettonici (Ko 2021)\n\n\nKo, Yohan. 2021. «Characterizing System-Level Masking Effects against Soft Errors». Electronics 10 (18): 2286. https://doi.org/10.3390/electronics10182286.\n\n\nAlcuni strumenti, come Fidelity (He, Balaprakash, e Li 2020), mirano a colmare il divario tra modelli di errore a livello hardware e software mappando i pattern tra i due livelli di astrazione (Cheng et al. 2016). Ciò consente una modellazione più accurata dei guasti hardware negli strumenti basati su software, essenziale per lo sviluppo di sistemi ML robusti e affidabili. Gli strumenti a più basso livello in genere rappresentano caratteristiche di propagazione degli errori più accurate, ma devono essere più rapidi nella simulazione di molti errori a causa della natura complessa delle progettazioni dei sistemi hardware. D’altro canto, gli strumenti a più alto livello, come quelli implementati in framework ML come PyTorch o TensorFlow, di cui parleremo presto nelle sezioni successive, sono spesso più rapidi ed efficienti per valutare la robustezza dei sistemi ML.\n\nCheng, Eric, Shahrzad Mirkhani, Lukasz G. Szafaryn, Chen-Yong Cher, Hyungmin Cho, Kevin Skadron, Mircea R. Stan, et al. 2016. «Clear: uC/u ross u-L/u ayer uE/u xploration for uA/u rchitecting uR/u esilience - Combining hardware and software techniques to tolerate soft errors in processor cores». In Proceedings of the 53rd Annual Design Automation Conference, 1–6. ACM. https://doi.org/10.1145/2897937.2897996.\nNelle sottosezioni seguenti, discuteremo vari metodi e strumenti di iniezione di guasti basati su hardware e software, evidenziandone le capacità, le limitazioni e i modelli di guasti ed errori che supportano.\n\n\n17.6.2 Injection Hardware-based di Guasti\nUno strumento di “iniezione di errori” è uno strumento che consente all’utente di implementare un particolare modello di errore, come un singolo bit flip transitorio durante l’inferenza Figura 17.38. La maggior parte degli strumenti di iniezione di errori sono basati su software, poiché sono più rapidi per gli studi di robustezza ML. Tuttavia, i metodi di iniezione di guasti basati su hardware sono ancora importanti per radicare i modelli di errore ad alto livello, poiché sono considerati il modo più accurato per studiare l’impatto dei guasti sui sistemi ML manipolando direttamente l’hardware per introdurli. Questi metodi consentono ai ricercatori di osservare il comportamento del sistema in condizioni di guasti reali. In questa sezione vengono descritti in modo più dettagliato sia gli strumenti di iniezione di errori basati su software che quelli basati su hardware.\n\n\n\n\n\n\nFigura 17.38: Gli errori hardware possono verificarsi per una serie di motivi e in momenti e/o posizioni diverse in un sistema, il che può essere esplorato quando si studia l’impatto degli errori basati sull’hardware sui sistemi (Ahmadilivani et al. 2024)\n\n\nAhmadilivani, Mohammad Hasan, Mahdi Taheri, Jaan Raik, Masoud Daneshtalab, e Maksim Jenihhin. 2024. «A Systematic Literature Review on Hardware Reliability Assessment Methods for Deep Neural Networks». ACM Comput. Surv. 56 (6): 1–39. https://doi.org/10.1145/3638242.\n\n\n\nMetodi\nDue dei metodi di iniezione di guasti basati su hardware più comuni sono quelli basati su FPGA e il test di radiazione o di fascio.\nIniezione di Guasti FPGA-based: I “Field-Programmable Gate Array (FPGA)” sono circuiti integrati riconfigurabili che possono essere programmati per implementare vari progetti hardware. Nel contesto dell’iniezione di guasti, gli FPGA offrono elevata precisione e accuratezza, poiché i ricercatori possono mirare a bit specifici o set di bit all’interno dell’hardware. Modificando la configurazione dell’FPGA, i guasti possono essere introdotti in posizioni e tempi specifici durante l’esecuzione di un modello ML. L’iniezione di guasti basata su FPGA consente un controllo dettagliato sul “fault model”, consentendo ai ricercatori di studiare l’impatto di diversi tipi di guasti, come i flip di bit singoli o gli errori multi-bit. Questo livello di controllo rende l’iniezione di guasti basata su FPGA uno strumento prezioso per comprendere la resilienza dei sistemi ML ai guasti hardware.\nTest di Radiazioni o Fasci: Il test di radiazioni o fasci (Velazco, Foucard, e Peronnard 2010) comporta l’esposizione dell’hardware che esegue un modello ML a particelle ad alta energia, come protoni o neutroni, come illustrato in Figura 17.39. Queste particelle possono causare bitflip o altri tipi di guasti nell’hardware, imitando gli effetti di quelli indotti dalle radiazioni nel mondo reale. Il test di fasci è ampiamente considerato un metodo altamente accurato per misurare il tasso di errore indotto da impatti di particelle su un’applicazione in esecuzione. Fornisce una rappresentazione realistica dei guasti in ambienti reali, in particolare in applicazioni esposte ad alti livelli di radiazioni, come sistemi spaziali o esperimenti di fisica delle particelle. Tuttavia, a differenza dell’iniezione di guasti basata su FPGA, il test di fasci potrebbe essere più preciso nel puntare a bit o componenti specifici all’interno dell’hardware, poiché potrebbe essere difficile puntare il fascio di particelle a un bit particolare nell’hardware. Nonostante sia piuttosto costoso dal punto di vista della ricerca, il test del fascio è una pratica industriale molto apprezzata per l’affidabilità.\n\nVelazco, Raoul, Gilles Foucard, e Paul Peronnard. 2010. «Combining Results of Accelerated Radiation Tests and Fault Injections to Predict the Error Rate of an Application Implemented in SRAM-Based FPGAs». IEEE Trans. Nucl. Sci. 57 (6): 3500–3505. https://doi.org/10.1109/tns.2010.2087355.\n\n\n\n\n\n\nFigura 17.39: Configurazione del test di radiazione per componenti semiconduttori (Lee et al. 2022) Fonte: JD Instrument\n\n\nLee, Minwoong, Namho Lee, Huijeong Gwon, Jongyeol Kim, Younggwan Hwang, e Seongik Cho. 2022. «Design of Radiation-Tolerant High-Speed Signal Processing Circuit for Detecting Prompt Gamma Rays by Nuclear Explosion». Electronics 11 (18): 2970. https://doi.org/10.3390/electronics11182970.\n\n\n\n\nLimitazioni\nNonostante la loro elevata accuratezza, i metodi di iniezione di guasti basati su hardware presentano diverse limitazioni che possono ostacolarne l’adozione diffusa:\nCosto: L’iniezione di guasti e il test del fascio basati su FPGA richiedono hardware e strutture specializzate, la cui configurazione e manutenzione possono essere costose. Il costo di questi metodi può rappresentare un ostacolo significativo per ricercatori e organizzazioni con risorse limitate.\nScalabilità: I metodi basati su hardware sono generalmente più lenti e meno scalabili rispetto ai metodi basati su software. L’iniezione di guasti e la raccolta di dati sull’hardware possono richiedere tempo, limitando il numero di esperimenti eseguiti in un determinato lasso di tempo. Ciò può essere particolarmente impegnativo quando si studia la resilienza di sistemi ML su larga scala o si conducono analisi statistiche che richiedono molti esperimenti di iniezione di guasti.\nFlessibilità: I metodi basati su hardware potrebbero non essere flessibili quanto quelli basati su software in termini di gamma di modelli di guasto e modelli di errore che possono supportare. Modificare la configurazione hardware o l’impostazione sperimentale per adattarsi a diversi modelli di errore può essere più impegnativo e richiedere più tempo rispetto ai metodi basati su software.\nNonostante queste limitazioni, i metodi di iniezione di errori basati su hardware rimangono strumenti essenziali per convalidare l’accuratezza dei metodi basati su software e per studiare l’impatto degli errori sui sistemi ML in contesti realistici. Combinando metodi basati su hardware e basati su software, i ricercatori possono acquisire una comprensione più completa della resilienza dei sistemi ML ai guasti hardware e sviluppare strategie di mitigazione efficaci.\n\n\n\n17.6.3 Strumenti di Injection di Guasti Software-based\nCon il rapido sviluppo di framework ML negli ultimi anni, gli strumenti di iniezione di guasti basati su software hanno guadagnato popolarità nello studio della resilienza dei sistemi ML ai guasti hardware. Questi strumenti simulano gli effetti dei guasti hardware modificando la rappresentazione software del modello ML o il grafo computazionale sottostante. L’ascesa di framework ML come TensorFlow, PyTorch e Keras ha facilitato lo sviluppo di strumenti di iniezione di guasti che sono strettamente integrati con questi framework, rendendo più facile per i ricercatori condurre esperimenti di iniezione di guasti e analizzare i risultati.\n\nVantaggi e Compromessi\nGli strumenti di iniezione di guasti basati su software offrono diversi vantaggi rispetto a quelli basati su hardware:\nVelocità: Gli strumenti basati su software sono generalmente più rapidi dei metodi basati su hardware, poiché non richiedono la modifica dell’hardware fisico o la configurazione di apparecchiature specializzate. Ciò consente ai ricercatori di condurre più esperimenti di iniezione di guasti in tempi più brevi, consentendo analisi più complete della resilienza dei sistemi ML.\nFlessibilità: Gli strumenti basati su software sono più flessibili di quelli basati su hardware in termini di gamma di modelli di guasti ed errori che possono supportare. I ricercatori possono facilmente modificare l’implementazione software dello strumento di iniezione di guasti per adattarsi a diversi modelli di guasti o per indirizzare componenti specifici del sistema ML.\nAccessibilità: Gli strumenti basati su software sono più accessibili dei metodi basati su hardware, poiché non richiedono hardware o strutture specializzate. Ciò semplifica per ricercatori e professionisti condurre esperimenti di iniezione di guasti e studiare la resilienza dei sistemi ML, anche con risorse limitate.\n\n\nLimitazioni\nGli strumenti di iniezione di guasti basati su software presentano anche alcune limitazioni rispetto ai metodi basati su hardware:\nPrecisione: Gli strumenti basati su software potrebbero non sempre catturano l’intera gamma di effetti che i guasti hardware possono avere sul sistema. Poiché questi strumenti operano a un livello di astrazione più elevato, potrebbero dover recuperare alcune delle interazioni hardware di basso livello e dei meccanismi di propagazione degli errori che possono influire sul comportamento del sistema ML.\nFedeltà: Gli strumenti basati su software potrebbero fornire un livello di fedeltà diverso rispetto ai metodi basati su hardware in termini di rappresentazione delle condizioni di guasto del mondo reale. L’accuratezza dei risultati ottenuti dagli esperimenti di iniezione di guasti basati su software potrebbe dipendere da quanto il modello software si avvicini al comportamento hardware effettivo.\n\n\nTipi di Strumenti di Iniezione di Guasti\nGli strumenti di iniezione di guasti basati su software possono essere categorizzati in base ai loro framework di destinazione o casi d’uso. Qui, discuteremo alcuni degli strumenti più popolari in ciascuna categoria:\nAres (Reagen et al. 2018), uno strumento di iniezione di guasti inizialmente sviluppato per il framework Keras nel 2018, è emerso come uno dei primi strumenti per studiare l’impatto dei guasti hardware sulle reti deep neural network (DNN) nel contesto della crescente popolarità dei framework ML a metà-fine anni 2010. Lo strumento è stato convalidato rispetto a un acceleratore DNN implementato in silicio, dimostrando la sua efficacia nella modellazione dei guasti hardware. Ares fornisce uno studio completo sull’impatto dei guasti hardware sia nei pesi che nei valori di attivazione, caratterizzando gli effetti dei flip di bit singoli e dei bit-error rate (BER) sulle strutture hardware. Successivamente, il framework Ares è stato esteso per supportare l’ecosistema PyTorch, consentendo ai ricercatori di investigare i guasti hardware in un contesto più moderno e ampliando ulteriormente la sua utilità sul campo.\n\nReagen, Brandon, Udit Gupta, Lillian Pentecost, Paul Whatmough, Sae Kyu Lee, Niamh Mulholland, David Brooks, e Gu-Yeon Wei. 2018. «Ares: A framework for quantifying the resilience of deep neural networks». In 2018 55th ACM/ESDA/IEEE Design Automation Conference (DAC), 1–6. IEEE. https://doi.org/10.1109/dac.2018.8465834.\n\nMahmoud, Abdulrahman, Neeraj Aggarwal, Alex Nobbe, Jose Rodrigo Sanchez Vicarte, Sarita V. Adve, Christopher W. Fletcher, Iuri Frosio, e Siva Kumar Sastry Hari. 2020. «PyTorchFI: A Runtime Perturbation Tool for DNNs». In 2020 50th Annual IEEE/IFIP International Conference on Dependable Systems and Networks Workshops (DSN-W), 25–31. IEEE; IEEE. https://doi.org/10.1109/dsn-w50199.2020.00014.\nPyTorchFI (Mahmoud et al. 2020), uno strumento di iniezione di guasti progettato specificamente per il framework PyTorch, è stato sviluppato nel 2020 in collaborazione con Nvidia Research. Consente l’iniezione di guasti nei pesi, nelle attivazioni e nei gradienti dei modelli PyTorch, supportando un’ampia gamma di modelli di guasti. Sfruttando le capacità di accelerazione GPU di PyTorch, PyTorchFI fornisce un’implementazione rapida ed efficiente per condurre esperimenti di iniezione di guasti su sistemi ML su larga scala, come mostrato in Figura 17.40.\n\n\n\n\n\n\nFigura 17.40: I bitflip hardware nei carichi di lavoro ML possono causare oggetti fantasma e classificazioni errate, che possono essere erroneamente utilizzati a valle da sistemi più grandi, come nella guida autonoma. Quella mostrata sopra è una versione corretta e difettosa della stessa immagine che utilizza il framework di iniezione PyTorchFI.\n\n\n\nLa velocità e la facilità d’uso dello strumento hanno portato a un’adozione diffusa nella comunità, con conseguenti molteplici progetti guidati dagli sviluppatori, come PyTorchALFI di Intel xColabs, che si concentra sulla sicurezza negli ambienti automobilistici. Gli strumenti successivi incentrati su PyTorch per l’iniezione di guasti includono Dr. DNA di Meta (Ma et al. 2024) (che facilita ulteriormente il modello di programmazione “Pythonico” per facilità d’uso) e il framework GoldenEye (Mahmoud et al. 2022), che incorpora nuovi tipi di dati numerici (come AdaptivFloat (Tambe et al. 2020) e BlockFloat nel contesto di bit flip hardware.\n\nMa, Dongning, Fred Lin, Alban Desmaison, Joel Coburn, Daniel Moore, Sriram Sankar, e Xun Jiao. 2024. «Dr. DNA: Combating Silent Data Corruptions in Deep Learning using Distribution of Neuron Activations». In Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3, 239–52. ACM. https://doi.org/10.1145/3620666.3651349.\n\nMahmoud, Abdulrahman, Thierry Tambe, Tarek Aloui, David Brooks, e Gu-Yeon Wei. 2022. «GoldenEye: A Platform for Evaluating Emerging Numerical Data Formats in DNN Accelerators». In 2022 52nd Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN), 206–14. IEEE. https://doi.org/10.1109/dsn53405.2022.00031.\n\nTambe, Thierry, En-Yu Yang, Zishen Wan, Yuntian Deng, Vijay Janapa Reddi, Alexander Rush, David Brooks, e Gu-Yeon Wei. 2020. «Algorithm-Hardware Co-Design of Adaptive Floating-Point Encodings for Resilient Deep Learning Inference». In 2020 57th ACM/IEEE Design Automation Conference (DAC), 1–6. IEEE; IEEE. https://doi.org/10.1109/dac18072.2020.9218516.\n\nChen, Zitao, Niranjhana Narayanan, Bo Fang, Guanpeng Li, Karthik Pattabiraman, e Nathan DeBardeleben. 2020. «TensorFI: A Flexible Fault Injection Framework for TensorFlow Applications». In 2020 IEEE 31st International Symposium on Software Reliability Engineering (ISSRE), 426–35. IEEE; IEEE. https://doi.org/10.1109/issre5003.2020.00047.\n\nChen, Zitao, Guanpeng Li, Karthik Pattabiraman, e Nathan DeBardeleben. 2019. «iBinFI/i: an efficient fault injector for safety-critical machine learning systems». In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis. SC ’19. New York, NY, USA: ACM. https://doi.org/10.1145/3295500.3356177.\nTensorFI (Chen et al. 2020), o TensorFlow Fault Injector, è uno strumento di iniezione di guasti sviluppato specificamente per il framework TensorFlow. Analogo ad Ares e PyTorchFI, TensorFI è considerato lo strumento all’avanguardia per gli studi di robustezza ML nell’ecosistema TensorFlow. Consente ai ricercatori di iniettare guasti nel grafo computazionale di Modelli TensorFlow e studia il loro impatto sulle prestazioni del modello, supportando un’ampia gamma di modelli di errore. Uno dei principali vantaggi di TensorFI è la sua capacità di valutare la resilienza di vari modelli ML, non solo DNN. Ulteriori progressi, come BinFi (Chen et al. 2019), forniscono un meccanismo per accelerare gli esperimenti di iniezione di errori concentrandosi sui bit “importanti” nel sistema, accelerando il processo di analisi della robustezza ML e dando priorità ai componenti critici di un modello.\nNVBitFI (T. Tsai et al. 2021), uno strumento di iniezione di errori generico sviluppato da Nvidia per le sue piattaforme GPU, opera a un più basso livello rispetto a strumenti specifici del framework come Ares, PyTorchFI e TensorFlow. Mentre questi strumenti si concentrano su varie piattaforme di deep learning per implementare ed eseguire analisi di robustezza, NVBitFI mira al codice di assemblaggio hardware sottostante per l’iniezione di guasti. Ciò consente ai ricercatori di iniettare guasti in qualsiasi applicazione in esecuzione su GPU Nvidia, rendendolo uno strumento versatile per studiare la resilienza dei sistemi ML e di altre applicazioni accelerate da GPU. Consentendo agli utenti di iniettare errori a livello di architettura, NVBitFI fornisce un modello di guasto più generico che non è limitato ai soli modelli ML. Poiché i sistemi GPU di Nvidia sono comunemente utilizzati in molti sistemi basati su ML, NVBitFI è uno strumento prezioso per un’analisi completa dell’iniezione di guasti in varie applicazioni.\n\nTsai, Timothy, Siva Kumar Sastry Hari, Michael Sullivan, Oreste Villa, e Stephen W. Keckler. 2021. «NVBitFI: Dynamic Fault Injection for GPUs». In 2021 51st Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN), 284–91. IEEE; IEEE. https://doi.org/10.1109/dsn48987.2021.00041.\n\nEsempi specifici di dominio\nSono stati sviluppati strumenti di iniezione di guasti specifici per dominio per affrontare le sfide e i requisiti unici di vari domini applicativi ML, come veicoli autonomi e robotica. Questa sezione evidenzia tre strumenti di iniezione di guasti specifici per dominio: DriveFI e PyTorchALFI per veicoli autonomi e MAVFI per “uncrewed aerial vehicles (UAV)” [veicoli aerei senza equipaggio]. Questi strumenti consentono ai ricercatori di iniettare guasti hardware nei sottosistemi di percezione, controllo e altri sistemi complessi, consentendo loro di studiare l’impatto dei guasti sulle prestazioni e sulla sicurezza del sistema. Lo sviluppo di questi strumenti di iniezione di guasti basati su software ha notevolmente ampliato le capacità della comunità ML di sviluppare sistemi più robusti e affidabili in grado di funzionare in modo sicuro ed efficace in presenza di guasti hardware.\nDriveFI (Jha et al. 2019) è uno strumento di iniezione di guasti progettato per veicoli autonomi. Consente l’iniezione di guasti hardware nelle pipeline di percezione e controllo dei sistemi di veicoli autonomi, consentendo ai ricercatori di studiare l’impatto di questi guasti sulle prestazioni e sulla sicurezza del sistema. DriveFI è stato integrato con piattaforme di guida autonoma standard del settore, come Nvidia DriveAV e Baidu Apollo, rendendolo uno strumento prezioso per valutare la resilienza dei sistemi di veicoli autonomi.\n\nJha, Saurabh, Subho Banerjee, Timothy Tsai, Siva K. S. Hari, Michael B. Sullivan, Zbigniew T. Kalbarczyk, Stephen W. Keckler, e Ravishankar K. Iyer. 2019. «ML-Based Fault Injection for Autonomous Vehicles: A Case for Bayesian Fault Injection». In 2019 49th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN), 112–24. IEEE; IEEE. https://doi.org/10.1109/dsn.2019.00025.\n\nGräfe, Ralf, Qutub Syed Sha, Florian Geissler, e Michael Paulitsch. 2023. «Large-Scale Application of Fault Injection into PyTorch Models -an Extension to PyTorchFI for Validation Efficiency». In 2023 53rd Annual IEEE/IFIP International Conference on Dependable Systems and Networks - Supplemental Volume (DSN-S), 56–62. IEEE; IEEE. https://doi.org/10.1109/dsn-s58398.2023.00025.\nPyTorchALFI (Gräfe et al. 2023) è un’estensione di PyTorchFI sviluppata da Intel xColabs per il dominio dei veicoli autonomi. Si basa sulle capacità di inserimento di guasti di PyTorchFI. Aggiunge funzionalità specificamente studiate per valutare la resilienza dei sistemi di veicoli autonomi, come la capacità di inserire guasti nei dati della telecamera e del sensore LiDAR.\nMAVFI (Hsiao et al. 2023) è uno strumento di inserimento di guasti progettato per il dominio della robotica, in particolare per i veicoli aerei senza equipaggio (UAV). MAVFI è basato sul framework Robot Operating System (ROS) e consente ai ricercatori di inserire guasti nei vari componenti di un sistema UAV, come sensori, attuatori e algoritmi di controllo. Valutando l’impatto di questi guasti sulle prestazioni e sulla stabilità del UAV, i ricercatori possono sviluppare sistemi UAV più resilienti e tolleranti ai guasti.\n\nHsiao, Yu-Shun, Zishen Wan, Tianyu Jia, Radhika Ghosal, Abdulrahman Mahmoud, Arijit Raychowdhury, David Brooks, Gu-Yeon Wei, e Vijay Janapa Reddi. 2023. «MAVFI: An End-to-End Fault Analysis Framework with Anomaly Detection and Recovery for Micro Aerial Vehicles». In 2023 Design, Automation &amp; Test in Europe Conference &amp; Exhibition (DATE), 1–6. IEEE; IEEE. https://doi.org/10.23919/date56975.2023.10137246.\nLo sviluppo di strumenti di iniezione di guasti basati su software ha notevolmente ampliato le capacità di ricercatori e professionisti di studiare la resilienza dei sistemi ML ai guasti hardware. Sfruttando la velocità, la flessibilità e l’accessibilità di questi strumenti, la comunità ML può sviluppare sistemi più robusti e affidabili in grado di funzionare in modo sicuro ed efficace in presenza di guasti hardware.\n\n\n\n\n17.6.4 Colmare il Divario tra Modelli di Errore Hardware e Software\nSebbene gli strumenti di iniezione di guasti basati su software offrano molti vantaggi in termini di velocità, flessibilità e accessibilità, potrebbero non sempre catturare accuratamente l’intera gamma di effetti che i guasti hardware possono avere sul sistema. Questo perché gli strumenti basati su software operano a un livello di astrazione più alto rispetto ai metodi basati su hardware e potrebbero non rilevare alcune delle interazioni hardware di basso livello e dei meccanismi di propagazione degli errori che possono influire sul comportamento del sistema ML.\nCome illustra Bolchini et al. (2023) nel suo lavoro, gli errori hardware possono manifestarsi in complessi pattern di distribuzione spaziale che sono difficili da replicare completamente con la sola iniezione di guasti basata su software. Identificano quattro pattern distinti: (a) singolo punto, in cui il guasto corrompe un singolo valore in una feature map; (b) stessa riga, in cui il guasto corrompe una riga parziale o intera in una singola feature map; (c) bullet wake, in cui il guasto corrompe la stessa posizione su più feature map; e (d) shatter glass, che combina gli effetti dei pattern della stessa riga e bullet wake, come mostrato in Figura 17.41. Questi intricati meccanismi di propagazione degli errori evidenziano la necessità di tecniche di iniezione di guasti consapevoli dell’hardware per valutare accuratamente la resilienza dei sistemi ML.\n\n\n\n\n\n\nFigura 17.41: Gli errori hardware possono manifestarsi in modi diversi a livello software, come classificato da Bolchini et al. (Bolchini et al. 2023)\n\n\nBolchini, Cristiana, Luca Cassano, Antonio Miele, e Alessandro Toschi. 2023. «Fast and Accurate Error Simulation for CNNs Against Soft Errors». IEEE Trans. Comput. 72 (4): 984–97. https://doi.org/10.1109/tc.2022.3184274.\n\n\nI ricercatori hanno sviluppato strumenti per affrontare questo problema colmando il divario tra modelli di errore hardware di basso livello e modelli di errore software di livello superiore. Uno di questi strumenti è Fidelity, progettato per mappare i pattern tra guasti a livello hardware e le loro manifestazioni a livello software.\n\nFidelity: Colmare il Gap\nFidelity (He, Balaprakash, e Li 2020) è uno strumento per modellare accuratamente i guasti hardware negli esperimenti di iniezione di guasti basati su software. Ciò avviene studiando attentamente la relazione tra i guasti a livello hardware e il loro impatto sulla rappresentazione software del sistema ML.\n\nHe, Yi, Prasanna Balaprakash, e Yanjing Li. 2020. «FIdelity: Efficient Resilience Analysis Framework for Deep Learning Accelerators». In 2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO), 270–81. IEEE; IEEE. https://doi.org/10.1109/micro50266.2020.00033.\nLe intuizioni chiave alla base di Fidelity sono:\n\nPropagazione dei Guasti: Fidelity modella il modo in cui gli errori si propagano attraverso l’hardware e si manifestano come errori nello stato del sistema visibili al software. Comprendendo questi pattern di propagazione, Fidelity può simulare con maggiore accuratezza gli effetti dei guasti hardware negli esperimenti basati sul software.\nEquivalenza dei Guasti: Fidelity identifica classi equivalenti di guasti hardware che producono errori simili a livello software. Ciò consente ai ricercatori di progettare modelli di guasti basati sul software che siano rappresentativi dei guasti hardware sottostanti senza la necessità di modellare ogni possibile guasto hardware singolarmente.\nApproccio a Strati: Fidelity impiega un approccio a strati alla modellazione dei guasti, in cui gli effetti dei guasti hardware vengono propagati attraverso più livelli di astrazione, dall’hardware al livello software. Questo approccio garantisce che i modelli di guasti basati sul software siano basati sul comportamento effettivo dell’hardware.\n\nIncorporando queste informazioni, Fidelity consente agli strumenti di iniezione di guasti basati su software di catturare con precisione gli effetti dei guasti hardware sui sistemi ML. Ciò è particolarmente importante per le applicazioni critiche per la sicurezza, in cui la resilienza del sistema ai guasti hardware è fondamentale.\n\n\nL’Importanza di Catturare il Vero Comportamento Hardware\nCatturare il vero comportamento hardware negli strumenti di iniezione di guasti basati su software è fondamentale per diversi motivi:\n\nPrecisione: Modellando con precisione gli effetti dei guasti hardware, gli strumenti basati su software possono fornire informazioni più affidabili sulla resilienza dei sistemi ML. Ciò è essenziale per progettare e convalidare sistemi tolleranti ai guasti che possono funzionare in modo sicuro ed efficace in presenza di guasti hardware.\nRiproducibilità: Quando gli strumenti basati su software catturano con precisione il comportamento hardware, gli esperimenti di iniezione di guasti diventano più riproducibili su diverse piattaforme e ambienti. Ciò è importante per lo studio scientifico della resilienza del sistema ML, poiché consente ai ricercatori di confrontare e convalidare i risultati su diversi studi e implementazioni.\nEfficienza: Gli strumenti basati su software che catturano il vero comportamento dell’hardware possono essere più efficienti nei loro esperimenti di iniezione di guasti concentrandosi sui modelli di guasti più rappresentativi e impattanti. Ciò consente ai ricercatori di coprire una gamma più ampia di scenari di guasti e configurazioni di sistema con risorse computazionali limitate.\nStrategie di Mitigazione: Comprendere come i guasti hardware si manifestano a livello software è fondamentale per sviluppare strategie di mitigazione efficaci. Catturando con precisione il comportamento dell’hardware, gli strumenti di iniezione di guasti basati su software possono aiutare i ricercatori a identificare i componenti più vulnerabili del sistema ML e progettare tecniche di rafforzamento mirate per migliorare la resilienza.\n\nStrumenti come Fidelity sono essenziali per far progredire lo stato dell’arte nella ricerca sulla resilienza del sistema ML. Questi strumenti consentono ai ricercatori di condurre esperimenti di iniezione di guasti più accurati, riproducibili ed efficienti colmando il divario tra modelli di errore hardware e software. Man mano che la complessità e la criticità dei sistemi ML continuano a crescere, l’importanza di catturare il vero comportamento hardware negli strumenti di iniezione di guasti basati su software diventerà sempre più evidente.\nLa ricerca in corso in quest’area cerca di perfezionare la mappatura tra modelli di errore hardware e software e di sviluppare nuove tecniche per simulare in modo efficiente i guasti hardware negli esperimenti basati su software. Man mano che questi strumenti maturano, forniranno alla comunità ML mezzi sempre più potenti e accessibili per studiare e migliorare la resilienza dei sistemi ML ai guasti hardware.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>IA Robusta</span>"
    ]
  },
  {
    "objectID": "contents/core/robust_ai/robust_ai.it.html#conclusione",
    "href": "contents/core/robust_ai/robust_ai.it.html#conclusione",
    "title": "17  IA Robusta",
    "section": "17.7 Conclusione",
    "text": "17.7 Conclusione\nSviluppare un’IA solida e resiliente è fondamentale man mano che i sistemi di apprendimento automatico diventano sempre più integrati in applicazioni critiche per la sicurezza e in ambienti reali. Questo capitolo ha esplorato le principali sfide alla robustezza dell’IA derivanti da guasti hardware, attacchi dannosi, cambiamenti di distribuzione e bug software.\nAlcune delle conclusioni principali includono quanto segue:\n\nGuasti Hardware: Guasti transitori, permanenti e intermittenti nei componenti hardware possono corrompere i calcoli e degradare le prestazioni dei modelli di apprendimento automatico se non vengono rilevati e mitigati correttamente. Tecniche come ridondanza, correzione degli errori e progetti fault-tolerant svolgono un ruolo cruciale nella creazione di sistemi ML resilienti in grado di resistere ai guasti hardware.\nRobustezza del Modello: Gli attori malintenzionati possono sfruttare le vulnerabilità nei modelli ML tramite attacchi avversari e avvelenamento dei dati, mirando a indurre classificazioni errate mirate, distorcere il comportamento appreso del modello o compromettere l’integrità e l’affidabilità del sistema. Inoltre, possono verificarsi “distribution shift” quando la distribuzione dei dati riscontrata durante l’implementazione differisce da quella osservata durante il training, con conseguente degrado delle prestazioni. L’implementazione di misure difensive, tra cui training avversario, rilevamento delle anomalie, architetture di modelli robuste e tecniche come adattamento del dominio, apprendimento per trasferimento e apprendimento continuo, è essenziale per proteggersi da queste sfide e garantire l’affidabilità e la generalizzazione del modello in ambienti dinamici.\nErrori Software: Gli errori nei framework ML, nelle librerie e negli stack software possono propagarsi, degradare le prestazioni e introdurre vulnerabilità di sicurezza. Test rigorosi, monitoraggio del runtime e adozione di “design pattern” tolleranti agli errori sono essenziali per la creazione di un’infrastruttura software robusta che supporti sistemi ML affidabili.\n\nPoiché i sistemi ML affrontano attività sempre più complesse con conseguenze nel mondo reale, dare priorità alla resilienza diventa fondamentale. Gli strumenti e i framework discussi in questo capitolo, tra cui tecniche di “fault injection” [iniezione di guasti], metodi di analisi degli errori e framework di valutazione della robustezza, forniscono ai professionisti i mezzi per testare a fondo e rafforzare i propri sistemi ML contro varie modalità di errore e condizioni avverse.\nAndando avanti, la resilienza deve essere un obiettivo centrale durante l’intero ciclo di vita dello sviluppo dell’IA, dalla raccolta dei dati e dall’addestramento del modello all’implementazione e al monitoraggio. Affrontando in modo proattivo le molteplici sfide alla robustezza, possiamo sviluppare sistemi di apprendimento automatico affidabili e sicuri, in grado di affrontare le complessità e le incertezze degli ambienti del mondo reale.\nLa ricerca futura sul ML robusto dovrebbe continuare a far progredire le tecniche per rilevare e mitigare guasti, attacchi e “shift” delle distribuzioni. Inoltre, esplorare nuovi paradigmi per lo sviluppo di architetture IA intrinsecamente resilienti, come sistemi auto-riparanti o meccanismi a prova di errore, sarà fondamentale per spingere i confini della robustezza dell’IA. Dando priorità alla resilienza e investendo nello sviluppo di sistemi di IA robusti, possiamo liberare il pieno potenziale delle tecnologie di apprendimento automatico, garantendone al contempo un’implementazione sicura, affidabile e responsabile in applicazioni del mondo reale. Mentre l’IA continua a plasmare il nostro futuro, la creazione di sistemi resilienti in grado di resistere alle sfide del mondo reale sarà un fattore determinante per il successo e l’impatto sociale di questa tecnologia trasformativa.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>IA Robusta</span>"
    ]
  },
  {
    "objectID": "contents/core/robust_ai/robust_ai.it.html#sec-robust-ai-resource",
    "href": "contents/core/robust_ai/robust_ai.it.html#sec-robust-ai-resource",
    "title": "17  IA Robusta",
    "section": "17.8 Risorse",
    "text": "17.8 Risorse\nEcco un elenco curato di risorse per supportare studenti e insegnanti nei loro percorsi di apprendimento e insegnamento. Lavoriamo continuamente per espandere questa raccolta e presto aggiungeremo nuovi esercizi.\n\n\n\n\n\n\nSlide\n\n\n\n\n\nQueste slide sono uno strumento prezioso per gli insegnanti per tenere lezioni e per gli studenti per rivedere il materiale secondo il proprio ritmo. Incoraggiamo sia gli studenti che gli insegnanti a sfruttare queste slide per migliorare la loro comprensione e facilitare un trasferimento efficace delle conoscenze.\n\nProssimamente.\n\n\n\n\n\n\n\n\n\n\nVideo\n\n\n\n\n\n\nProssimamente.\n\n\n\n\n\n\n\n\n\n\nEsercizi\n\n\n\n\n\nPer rafforzare i concetti trattati in questo capitolo, abbiamo curato una serie di esercizi che sfidano gli studenti ad applicare le proprie conoscenze e ad approfondire la propria comprensione.\n\nEsercizio 17.1\nEsercizio 17.2\nEsercizio 17.3\nEsercizio 17.4",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>IA Robusta</span>"
    ]
  },
  {
    "objectID": "contents/core/generative_ai/generative_ai.it.html",
    "href": "contents/core/generative_ai/generative_ai.it.html",
    "title": "18  IA Generativa",
    "section": "",
    "text": "Prossimamente!\nImmaginate un capitolo che si scrive da solo e si adatta alla propria curiosità, generando nuove intuizioni mentre si legge. Stiamo lavorando a qualcosa di straordinario!\nQuesto capitolo trasformerà il modo di leggere e imparare, generando dinamicamente contenuti man mano che si procede. Mentre perfezioniamo questa nuova entusiasmante funzionalità, speriamo che gli utenti si preparino per un’esperienza educativa dinamica e unica come. Segnate sul calendario la grande rivelazione e aggiungete questa pagina a quelle preferite.\nIl futuro dell’apprendimento generativo è qui! — Vijay Janapa Reddi",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>IA Generativa</span>"
    ]
  },
  {
    "objectID": "contents/core/ai_for_good/ai_for_good.it.html",
    "href": "contents/core/ai_for_good/ai_for_good.it.html",
    "title": "19  AI for Good",
    "section": "",
    "text": "19.1 Panoramica\nPer darci un quadro attorno al quale riflettere sull’intelligenza artificiale per il bene sociale, seguiremo gli “Obiettivi di Sviluppo Sostenibile delle Nazioni Unite (SDG)”. Gli SDG delle Nazioni Unite sono una raccolta di 17 obiettivi globali, mostrati in Figura 19.1, adottati dalle Nazioni Unite nel 2015 come parte dell’Agenda 2030 per lo Sviluppo Sostenibile. Gli SDG affrontano le sfide globali relative a povertà, disuguaglianza, cambiamenti climatici, degrado ambientale, prosperità, pace e giustizia.\nCiò che rende speciali gli SDG è che sono una raccolta di obiettivi interconnessi progettati per fungere da “modello condiviso per la pace e la prosperità per le persone e il pianeta, ora e in futuro”. Gli SDG enfatizzano gli aspetti ambientali, sociali ed economici interconnessi dello sviluppo sostenibile, ponendo la sostenibilità al centro.\nUno studio recente (Vinuesa et al. 2020) evidenzia l’influenza dell’IA su tutti gli aspetti dello sviluppo sostenibile, in particolare sui 17 “Sustainable Development Goals (SDG)” e 169 target definiti a livello internazionale nell’Agenda 2030 per lo Sviluppo Sostenibile. Lo studio mostra che l’IA può fungere da abilitatore per 134 target attraverso miglioramenti tecnologici, ma evidenzia anche le sfide dell’IA su alcuni target. Lo studio mostra che l’IA può avvantaggiare 67 target quando si considerano l’IA e i risultati sociali. Tuttavia, mette anche in guardia sui problemi relativi all’implementazione dell’IA in paesi con valori culturali e ricchezza diversi.\nSebbene tutte le forme di IA e apprendimento automatico abbiano il potenziale per contribuire ai “Sustainable Development Goals (SDG)” [Obiettivi di Sviluppo Sostenibile], questo capitolo si concentra su TinyML per la sua capacità unica di affrontare le sfide che si presentano in contesti con risorse limitate. I sistemi ML, in particolare quelli che si basano su infrastrutture cloud, spesso richiedono una notevole potenza di calcolo, una connettività Internet costante e un investimento finanziario sostanziale, che può limitarne l’adozione nelle regioni in via di sviluppo o nelle aree remote. Al contrario, TinyML consente soluzioni localizzate, a basso costo e a basso consumo energetico eseguendo modelli di apprendimento automatico efficienti direttamente sui microcontrollori. Queste qualità lo rendono particolarmente efficace per affrontare problemi come il monitoraggio agricolo, la diagnosi sanitaria in aree svantaggiate e la conservazione ambientale in cui l’infrastruttura potrebbe essere minima.\nConcentrandosi su TinyML, questo capitolo evidenzia un ramo dell’IA che fornisce soluzioni pratiche e localizzate in grado di funzionare indipendentemente dalle richieste di energia e connettività tipicamente associate a implementazioni ML su larga scala. TinyML si allinea bene con l’enfasi degli SDG sulla sostenibilità e l’accessibilità offrendo innovazioni scalabili che affrontano le sfide globali in contesti con risorse limitate.\nNel contesto di questo libro, TinyML potrebbe contribuire a promuovere i seguenti obiettivi SDG:\nLa portabilità, i requisiti di potenza inferiori e l’analisi in tempo reale abilitati da TinyML lo rendono adatto ad affrontare diverse sfide di sostenibilità che le regioni in via di sviluppo si trovano ad affrontare. L’ampia distribuzione di soluzioni di alimentazione ha il potenziale per fornire un monitoraggio localizzato e conveniente per aiutare a raggiungere alcuni degli Obiettivi di Sviluppo Sostenibile delle Nazioni Unite. Nelle restanti sezioni, approfondiremo il modo in cui TinyML è utile in molti settori che possono affrontare le SDG delle Nazioni Unite.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>AI for Good</span>"
    ]
  },
  {
    "objectID": "contents/core/ai_for_good/ai_for_good.it.html#panoramica",
    "href": "contents/core/ai_for_good/ai_for_good.it.html#panoramica",
    "title": "19  AI for Good",
    "section": "",
    "text": "Figura 19.1: United Nations Sustainable Development Goals (SDG). Fonte: United Nations.\n\n\n\n\n\n\nVinuesa, Ricardo, Hossein Azizpour, Iolanda Leite, Madeline Balaam, Virginia Dignum, Sami Domisch, Anna Felländer, Simone Daniela Langhans, Max Tegmark, e Francesco Fuso Nerini. 2020. «The role of artificial intelligence in achieving the Sustainable Development Goals». Nat. Commun. 11 (1): 1–10. https://doi.org/10.1038/s41467-019-14108-y.\n\n\n\n\nObiettivo 1 - Nessuna Povertà: TinyML potrebbe aiutare a fornire soluzioni a basso costo per il monitoraggio delle colture per migliorare le rese agricole nei paesi in via di sviluppo.\nObiettivo 2 - Zero Fame: TinyML potrebbe consentire un monitoraggio localizzato e preciso della salute delle colture e il rilevamento delle malattie per ridurre le perdite di raccolto.\nObiettivo 3 - Buona Salute e Benessere: TinyML potrebbe aiutare a abilitare strumenti di diagnosi medica a basso costo per la diagnosi precoce e la prevenzione delle malattie nelle aree remote.\nObiettivo 6 - Acqua Pulita e Servizi igienici: TinyML potrebbe monitorare la qualità dell’acqua e rilevare i contaminanti per garantire l’accesso all’acqua potabile.\nObiettivo 7 - Energia Pulita e Accessibile: TinyML potrebbe ottimizzare il consumo di energia e consentire la manutenzione predittiva per le infrastrutture di energia rinnovabile.\nObiettivo 11 - Città e Comunità Sostenibili: TinyML potrebbe consentire una gestione intelligente del traffico, il monitoraggio della qualità dell’aria e una gestione ottimizzata delle risorse nelle città intelligenti.\nObiettivo 13 - Azione per il Clima: TinyML potrebbe monitorare la deforestazione e tracciare gli sforzi di riforestazione. Potrebbe anche aiutare a prevedere eventi meteorologici estremi.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>AI for Good</span>"
    ]
  },
  {
    "objectID": "contents/core/ai_for_good/ai_for_good.it.html#agricoltura",
    "href": "contents/core/ai_for_good/ai_for_good.it.html#agricoltura",
    "title": "19  AI for Good",
    "section": "19.2 Agricoltura",
    "text": "19.2 Agricoltura\nL’agricoltura è essenziale per raggiungere molti degli Obiettivi di Sviluppo Sostenibile delle Nazioni Unite, tra cui l’eradicazione della fame e della malnutrizione, la promozione della crescita economica e l’uso sostenibile delle risorse naturali. TinyML può essere uno strumento prezioso per aiutare a promuovere un’agricoltura sostenibile, in particolare per i piccoli agricoltori nelle regioni in via di sviluppo.\nLe soluzioni TinyML possono fornire monitoraggio in tempo reale e analisi dei dati per la salute delle colture e le condizioni di crescita—il tutto senza dipendere dall’infrastruttura di connettività. Ad esempio, i moduli di telecamere a basso costo collegati ai microcontrollori possono monitorare malattie, parassiti e carenze nutrizionali. Gli algoritmi TinyML possono analizzare le immagini per rilevare i problemi in anticipo prima che si diffondano e danneggino i raccolti. Il monitoraggio di precisione può ottimizzare input come acqua, fertilizzanti e pesticidi—migliorando l’efficienza e la sostenibilità.\nAltri sensori, come unità GPS e accelerometri, possono tracciare le condizioni del microclima, l’umidità del suolo e il benessere del bestiame. I dati locali in tempo reale aiutano gli agricoltori a rispondere e ad adattarsi meglio ai cambiamenti sul campo. L’analisi TinyML nell’edge evita ritardi, interruzioni di rete e gli elevati costi dei dati dei sistemi basati su cloud. I sistemi localizzati consentono la personalizzazione di colture, malattie e problemi regionali specifici.\nLe applicazioni TinyML diffuse possono aiutare a digitalizzare le piccole aziende agricole per aumentare produttività, redditi e resilienza. Il basso costo dell’hardware e i requisiti minimi di connettività rendono le soluzioni accessibili. I progetti nei paesi in via di sviluppo hanno mostrato i vantaggi:\n\nIl progetto FarmBeats di Microsoft è un approccio end-to-end per abilitare l’agricoltura basata sui dati utilizzando sensori a basso costo, droni e algoritmi di visione e apprendimento automatico. Il progetto cerca di risolvere il problema dell’adozione limitata della tecnologia nell’agricoltura a causa della necessità di maggiore potenza e connettività Internet nelle aziende agricole e della limitata competenza tecnologica degli agricoltori. Il progetto tenta di aumentare la produttività agricola e ridurre i costi accoppiando i dati con la conoscenza e l’intuizione degli agricoltori sulle loro aziende agricole. Il progetto ha consentito con successo di ottenere informazioni fruibili dai dati tramite la creazione di modelli di intelligenza artificiale (IA) o apprendimento automatico (ML) basati su set di dati fusi. Figura 19.2 illustra il funzionamento interno di FarmBeats di Microsoft.\nNell’Africa subsahariana, telecamere standard e IA edge hanno ridotto le perdite per malattie della manioca dal 40% al 5%, proteggendo una coltura di base (Ramcharan et al. 2017).\nIn Indonesia, i sensori monitorano i microclimi nelle risaie, ottimizzando l’uso dell’acqua anche in caso di piogge irregolari (Tirtalistyani, Murtiningrum, e Kanwar 2022).\n\n\nRamcharan, Amanda, Kelsee Baranowski, Peter McCloskey, Babuali Ahmed, James Legg, e David P. Hughes. 2017. «Deep Learning for Image-Based Cassava Disease Detection». Front. Plant Sci. 8 (ottobre): 1852. https://doi.org/10.3389/fpls.2017.01852.\n\nTirtalistyani, Rose, Murtiningrum Murtiningrum, e Rameshwar S. Kanwar. 2022. «Indonesia Rice Irrigation System: Time for Innovation». Sustainability 14 (19): 12477. https://doi.org/10.3390/su141912477.\nCon maggiori investimenti e integrazione nei servizi di consulenza rurale, TinyML potrebbe trasformare l’agricoltura su piccola scala e migliorare i mezzi di sostentamento degli agricoltori in tutto il mondo. La tecnologia porta efficacemente i vantaggi dell’agricoltura di precisione alle regioni disconnesse più bisognose.\n\n\n\n\n\n\nFigura 19.2: Microsoft Farmbeats consente di prendere decisioni basate sui dati per migliorare la resa agricola, ridurre i costi complessivi e ridurre l’impatto ambientale della produzione agricola. Fonte: MOLD\n\n\n\n\n\n\n\n\n\nEsercizio 19.1: Modellazione della Resa delle Colture\n\n\n\n\n\nQuesto esercizio insegna come prevedere le rese delle colture in Nepal combinando dati satellitari (Sentinel-2), dati climatici (WorldClim) e misure sul campo. Si userà un algoritmo di apprendimento automatico chiamato XGBoost Regressor per creare un modello, dividere i dati per l’addestramento e il test e perfezionare i parametri del modello per ottenere le migliori prestazioni. Questo notebook getta le basi per l’implementazione di TinyML nel settore agricolo. Considerare come si potrebbe adattare questo processo per set di dati più piccoli, meno funzionalità e modelli semplificati per renderlo compatibile con i vincoli di potenza e memoria dei dispositivi TinyML.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>AI for Good</span>"
    ]
  },
  {
    "objectID": "contents/core/ai_for_good/ai_for_good.it.html#assistenza-sanitaria",
    "href": "contents/core/ai_for_good/ai_for_good.it.html#assistenza-sanitaria",
    "title": "19  AI for Good",
    "section": "19.3 Assistenza Sanitaria",
    "text": "19.3 Assistenza Sanitaria\n\n19.3.1 Espansione dell’Accesso\nLa copertura sanitaria universale e l’assistenza di qualità restano fuori dalla portata di milioni di persone in tutto il mondo. In molte regioni è necessario un numero maggiore di professionisti sanitari per accedere a diagnosi e trattamenti di base. Inoltre, è necessario migliorare le infrastrutture sanitarie come cliniche, ospedali e servizi di pubblica utilità per alimentare apparecchiature complesse. Queste lacune hanno un impatto sproporzionato sulle comunità emarginate, esacerbando le disparità sanitarie.\nTinyML offre una promettente soluzione tecnologica per aiutare ad ampliare l’accesso a un’assistenza sanitaria di qualità a livello globale. TinyML si riferisce alla capacità di implementare algoritmi di apprendimento automatico su microcontrollori, piccoli chip con potenza di elaborazione, memoria e connettività limitate. TinyML consente l’analisi dei dati in tempo reale e l’intelligenza in dispositivi compatti e a bassa potenza.\nCiò crea opportunità per strumenti medici trasformativi che sono portatili, convenienti e accessibili. Il software e l’hardware TinyML possono essere ottimizzati per funzionare anche in ambienti con risorse limitate. Ad esempio, un sistema TinyML potrebbe analizzare i sintomi o fare previsioni diagnostiche utilizzando una potenza di calcolo minima, nessuna connettività Internet continua e una batteria o una fonte di energia solare. Queste capacità possono portare screening e monitoraggio di livello medico direttamente ai pazienti meno assistiti.\n\n\n19.3.2 Diagnosi Precoce\nLa diagnosi precoce delle malattie è una delle principali applicazioni. Piccoli sensori abbinati al software TinyML possono identificare i sintomi prima che le condizioni peggiorino o compaiano segni visibili. Ad esempio, i monitoratori della tosse con apprendimento automatico embedded possono rilevare pattern acustici indicativi di malattie respiratorie, malaria o tubercolosi. Rilevare le malattie all’esordio migliora i risultati e riduce i costi sanitari.\nUn esempio dettagliato potrebbe essere il monitoraggio della polmonite nei bambini da parte di TinyML. La polmonite è una delle principali cause di morte nei bambini sotto i 5 anni e rilevarla precocemente è fondamentale. Una startup chiamata Respira xColabs ha sviluppato un sensore audio indossabile a basso costo che utilizza algoritmi TinyML per analizzare la tosse e identificare i sintomi di malattie respiratorie come la polmonite. Il dispositivo contiene un microfono e un microcontrollore che esegue un modello di rete neurale addestrato per classificare i suoni respiratori. Può identificare feature come respiro sibilante, crepitio e stridore che possono indicare la polmonite. Il dispositivo è progettato per essere altamente accessibile, dotato di un semplice cinturino, non richiede batteria o ricarica e fornisce risultati tramite luci LED e segnali acustici.\nUn altro esempio riguarda i ricercatori dell’UNIFEI in Brasile che hanno sviluppato un dispositivo a basso costo che sfrutta TinyML per monitorare i ritmi cardiaci. La loro soluzione risponde a un’esigenza critica affrontando il problema della fibrillazione atriale e di altre anomalie del ritmo cardiaco, che spesso non vengono diagnosticate a causa del costo proibitivo e della limitata disponibilità di strumenti di screening. Utilizza un microcontrollore standard che costa solo pochi dollari, insieme a un sensore di pulsazioni di base. Riducendo al minimo la complessità, il dispositivo diventa accessibile alle popolazioni con risorse insufficienti. L’algoritmo TinyML in esecuzione localmente sul microcontrollore analizza i dati delle pulsazioni in tempo reale per rilevare ritmi cardiaci irregolari. Questo dispositivo salvavita per il monitoraggio cardiaco dimostra come TinyML consenta di implementare potenti capacità di intelligenza artificiale in progetti convenienti e intuitivi.\nLa versatilità di TinyML promette anche di affrontare le malattie infettive. I ricercatori hanno proposto di applicare TinyML per identificare le zanzare che diffondono la malaria tramite i suoni del battito delle ali. Se dotati di microfoni, i piccoli microcontrollori possono eseguire modelli avanzati di classificazione audio per determinare le specie di zanzare. Questa soluzione compatta e a basso consumo produce risultati in tempo reale, adatti per l’uso in campo remoto. Rendendo l’analisi entomologica conveniente e accessibile, TinyML potrebbe rivoluzionare il monitoraggio degli insetti che mettono a rischio la salute umana. TinyML sta ampliando l’accesso all’assistenza sanitaria per le comunità vulnerabili, dalle malattie cardiache alla malaria.\n\n\n19.3.3 Controllo delle Malattie Infettive\nLe zanzare rimangono il vettore di malattie più mortale al mondo, trasmettendo malattie che infettano oltre un miliardo di persone ogni anno («Vector-borne diseases», s.d.). Malattie come la malaria, la dengue e lo Zika sono particolarmente diffuse nelle regioni con risorse limitate e prive di infrastrutture solide per il controllo delle zanzare. Il monitoraggio delle popolazioni locali di zanzare è essenziale per prevenire le epidemie e indirizzare correttamente gli interventi.\n\n«Vector-borne diseases». s.d. https://www.who.int/news-room/fact-sheets/detail/vector-borne-diseases.\nI metodi di monitoraggio tradizionali sono costosi, richiedono molta manodopera e sono difficili da implementare da remoto. La soluzione TinyML proposta supera queste barriere. Piccoli microfoni abbinati ad algoritmi di apprendimento automatico possono classificare le zanzare per specie in base a piccole differenze nelle oscillazioni delle ali. Il software TinyML funziona in modo efficiente su microcontrollori a basso costo, eliminando la necessità di connettività continua.\nUn team di ricerca collaborativo dell’Università di Khartoum e dell’ICTP sta esplorando una soluzione innovativa utilizzando TinyML. In un recente articolo, hanno presentato un dispositivo a basso costo in grado di identificare le specie di zanzare che diffondono malattie attraverso i suoni del battito delle ali (Altayeb, Zennaro, e Rovai 2022).\n\nAltayeb, Moez, Marco Zennaro, e Marcelo Rovai. 2022. «Classifying mosquito wingbeat sound using TinyML». In Proceedings of the 2022 ACM Conference on Information Technology for Social Good, 132–37. ACM. https://doi.org/10.1145/3524458.3547258.\nQuesto sistema portatile e autonomo promette molto bene per l’entomologia. I ricercatori suggeriscono che potrebbe rivoluzionare le strategie di monitoraggio degli insetti e di controllo dei vettori nelle aree remote. TinyML potrebbe rafforzare significativamente gli sforzi di eradicazione della malaria fornendo analisi delle zanzare più economiche e semplici. La sua versatilità e il fabbisogno energetico minimo lo rendono ideale per l’uso sul campo in regioni isolate e fuori dalla rete con risorse scarse ma un elevato carico di malattie.\n\n\n19.3.4 TinyML Design Contest in Healthcare\nIl primo concorso TinyML in ambito sanitario, TDC’22 (Jia et al. 2023), si è tenuto nel 2022 per motivare i team partecipanti a progettare algoritmi IA/ML per rilevare aritmie ventricolari (VA) potenzialmente letali e distribuirli su “Implantable Cardioverter Defibrillators (ICDs)”. Le VA sono la causa principale di “sudden cardiac death (SCD)” [morte cardiaca improvvisa]. Le persone ad alto rischio di SCD si affidano all’ICD per erogare un trattamento di defibrillazione adeguato e tempestivo (ad esempio, riportando il cuore al ritmo normale) quando soffrono di VA potenzialmente letali.\n\nJia, Zhenge, Dawei Li, Xiaowei Xu, Na Li, Feng Hong, Lichuan Ping, e Yiyu Shi. 2023. «Life-threatening ventricular arrhythmia detection challenge in implantable cardioverterdefibrillators». Nature Machine Intelligence 5 (5): 554–55. https://doi.org/10.1038/s42256-023-00659-9.\nUn algoritmo sul dispositivo per il rilevamento precoce e tempestivo di VA potenzialmente letali aumenterà le possibilità di sopravvivenza. L’algoritmo AI/ML proposto doveva essere implementato ed eseguito su un microcontrollore a bassissimo consumo energetico e con risorse limitate (una scheda di sviluppo da 10 dollari con un core ARM Cortex-M4 a 80 MHz, 256 kB di memoria flash e 64 kB di SRAM). I progetti presentati sono stati valutati tramite metriche misurate sul microcontrollore per (1) prestazioni di rilevamento, (2) latenza di inferenza e (3) occupazione di memoria da parte del programma di algoritmi AI/ML.\nIl campione, GaTech EIC Lab, ha ottenuto 0,972 in \\(F_\\beta\\) (punteggio F1 con un peso maggiore da richiamare), 1,747 ms di latenza e 26,39 kB di footprint di memoria con una rete neurale profonda. Un ICD con un algoritmo di rilevamento VA sul dispositivo è stato impiantato in uno studio clinico.\n\n\n\n\n\n\nEsercizio 19.2: Dati Clinici: Sbloccare Informazioni con il Riconoscimento di Entità Denominate\n\n\n\n\n\nIn questo esercizio, si imparerè il”Named Entity Recognition (NER)” [riconoscimento di entità denominate ], un potente strumento per estrarre informazioni preziose dal testo clinico. Utilizzando Spark NLP, una libreria specializzata per NLP sanitaria, esploreremo come i modelli NER come BiLSTM-CNN-Char e BERT possono identificare automaticamente importanti entità mediche come diagnosi, farmaci, risultati di test e altro ancora. Si acquisirà esperienza pratica applicando queste tecniche con un’attenzione particolare all’estrazione di dati correlati all’oncologia, aiutandoti a sbloccare informazioni sui tipi di cancro e sui dettagli del trattamento dalle cartelle cliniche dei pazienti.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>AI for Good</span>"
    ]
  },
  {
    "objectID": "contents/core/ai_for_good/ai_for_good.it.html#scienza",
    "href": "contents/core/ai_for_good/ai_for_good.it.html#scienza",
    "title": "19  AI for Good",
    "section": "19.4 Scienza",
    "text": "19.4 Scienza\nIn molti campi scientifici, i ricercatori sono limitati dalla qualità e dalla risoluzione dei dati che possono raccogliere. Spesso devono dedurre indirettamente i veri parametri di interesse utilizzando correlazioni approssimative e modelli basati su punti dati sparsi. Ciò limita l’accuratezza della comprensione scientifica e delle previsioni.\nL’emergere di TinyML apre nuove possibilità per la raccolta di misurazioni scientifiche ad alta fedeltà. Con l’apprendimento automatico embedded, piccoli sensori a basso costo possono elaborare e analizzare automaticamente i dati localmente in tempo reale. Ciò crea reti di sensori intelligenti che catturano dati sfumati su scale e frequenze molto più grandi.\nAd esempio, il monitoraggio delle condizioni ambientali per modellare il cambiamento climatico rimane una sfida a causa della necessità di dati diffusi e continui. Il progetto Ribbit dell’UC Berkeley è pioniere di una soluzione TinyML crowdsourcing (Rao 2021). Hanno sviluppato un sensore CO2 open source che utilizza un microcontrollore integrato per elaborare le misurazioni del gas. Un set di dati esteso può essere aggregato distribuendo centinaia di questi sensori a basso costo. I dispositivi TinyML compensano i fattori ambientali e forniscono letture granulari, accurate e in precedenza impossibili.\n\nRao, Ravi. 2021. «TinyML unlocks new possibilities for sustainable development technologies». www.wevolver.com. https://www.wevolver.com/article/tinyml-unlocks-new-possibilities-for-sustainable-development-technologies.\nIl potenziale di scalare massicciamente il rilevamento intelligente tramite TinyML ha profonde implicazioni scientifiche. I dati ad alta risoluzione possono portare a scoperte e capacità predittive in campi che vanno dall’ecologia alla cosmologia. Altre applicazioni potrebbero includere sensori sismici per sistemi di allerta precoce sui terremoti, monitor meteorologici distribuiti per tracciare i cambiamenti del microclima e sensori acustici per studiare le popolazioni animali.\nMan mano che i sensori e gli algoritmi continuano a migliorare, le reti TinyML potrebbero generare mappe più dettagliate che mai dei sistemi naturali. Democratizzare la raccolta di dati scientifici può accelerare la ricerca e la comprensione tra le discipline. Tuttavia, solleva nuove sfide in merito alla qualità dei dati, alla privacy e alla modellazione di incognite. TinyML indica una crescente convergenza tra intelligenza artificiale e scienze naturali per rispondere a domande fondamentali.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>AI for Good</span>"
    ]
  },
  {
    "objectID": "contents/core/ai_for_good/ai_for_good.it.html#conservazione-e-ambiente",
    "href": "contents/core/ai_for_good/ai_for_good.it.html#conservazione-e-ambiente",
    "title": "19  AI for Good",
    "section": "19.5 Conservazione e Ambiente",
    "text": "19.5 Conservazione e Ambiente\nTinyML sta emergendo come un potente strumento per la conservazione ambientale e gli sforzi di sostenibilità. Una ricerca recente ha evidenziato numerose applicazioni del tiny machine learning in ambiti quali il monitoraggio della fauna selvatica, la gestione delle risorse naturali e il monitoraggio dei cambiamenti climatici.\nUn esempio è l’utilizzo di TinyML per il monitoraggio e la protezione della fauna selvatica in tempo reale. I ricercatori hanno sviluppato dispositivi Smart Wildlife Tracker che sfruttano gli algoritmi TinyML per rilevare le attività di bracconaggio. I collari contengono sensori come telecamere, microfoni e GPS per monitorare costantemente l’ambiente circostante. I modelli di machine learning embedded analizzano i dati audio e visivi per identificare minacce come esseri umani nelle vicinanze o spari. Il rilevamento precoce del bracconaggio fornisce alle guardie forestali informazioni fondamentali per intervenire e agire.\nAltri progetti applicano TinyML per studiare il comportamento degli animali tramite sensori. Il collare intelligente per la fauna selvatica utilizza accelerometri e monitoraggio acustico per tracciare i movimenti, la comunicazione e gli stati d’animo degli elefanti (Verma 2022). I dispositivi collare TinyML a basso consumo energetico trasmettono dati approfonditi sulle attività degli elefanti evitando gravosi cambi di batteria. Ciò aiuta i ricercatori a osservare in modo discreto le popolazioni di elefanti per informare sulle strategie di conservazione.\n\nVerma, Team Dual_Boot: Swapnil. 2022. «Elephant AI». Hackster.io. https://www.hackster.io/dual\\_boot/elephant-ai-ba71e9.\nSu scala più ampia, i dispositivi TinyML distribuiti sono concepiti per creare reti di sensori dense per la modellazione ambientale. Centinaia di monitor della qualità dell’aria a basso costo potrebbero mappare l’inquinamento nelle città. I sensori sottomarini potrebbero rilevare le tossine e dare un avviso precoce di fioriture algali. Tali applicazioni sottolineano la versatilità di TinyML in ecologia, climatologia e sostenibilità.\nI ricercatori della Moulay Ismail University di Meknes in Marocco (Bamoumen et al. 2022) hanno pubblicato un sondaggio su come TinyML può essere utilizzato per risolvere i problemi ambientali. Tuttavia, valutare attentamente i benefici, i rischi e l’accesso equo sarà fondamentale man mano che TinyML espande la ricerca e la conservazione ambientale. Con una considerazione etica degli impatti, TinyML offre soluzioni basate sui dati per proteggere la biodiversità, le risorse naturali e il nostro pianeta.\n\nBamoumen, Hatim, Anas Temouden, Nabil Benamar, e Yousra Chtouki. 2022. «How TinyML Can be Leveraged to Solve Environmental Problems: A Survey». In 2022 International Conference on Innovation and Intelligence for Informatics, Computing, and Technologies (3ICT), 338–43. IEEE; IEEE. https://doi.org/10.1109/3ict56508.2022.9990661.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>AI for Good</span>"
    ]
  },
  {
    "objectID": "contents/core/ai_for_good/ai_for_good.it.html#risposta-ai-disastri",
    "href": "contents/core/ai_for_good/ai_for_good.it.html#risposta-ai-disastri",
    "title": "19  AI for Good",
    "section": "19.6 Risposta ai Disastri",
    "text": "19.6 Risposta ai Disastri\nNella risposta ai disastri, rapidità e sicurezza sono fondamentali, ma le macerie e i rottami creano ambienti pericolosi e angusti che ostacolano le attività di ricerca umana. TinyML consente ai droni agili di assistere le squadre di soccorso in questi scenari pericolosi. L’elaborazione locale dei dati tramite TinyML consente una rapida interpretazione per guidare i soccorsi.\nQuando gli edifici crollano dopo i terremoti, i piccoli droni possono rivelarsi preziosi. Dotati di algoritmi di navigazione TinyML, i droni di piccole dimensioni come il CrazyFlie con meno di 200 KB di RAM e una frequenza di clock della CPU di soli 168 MHz possono attraversare in sicurezza spazi angusti e mappare percorsi oltre la portata umana (Bardienus P. Duisterhof et al. 2019). L’elusione degli ostacoli consente a questi droni di muoversi tra detriti instabili. Questa mobilità autonoma consente loro di spazzare rapidamente aree in cui gli umani non possono accedere. Fondamentalmente, i sensori di bordo e i processori TinyML analizzano i dati in tempo reale per identificare i segni dei sopravvissuti. Le telecamere termiche possono rilevare il calore corporeo, i microfoni possono captare le richieste di aiuto e i sensori di gas possono avvisare di perdite (Bardienus P. Duisterhof et al. 2021).\n\nDuisterhof, Bardienus P, Srivatsan Krishnan, Jonathan J Cruz, Colby R Banbury, William Fu, Aleksandra Faust, Guido CHE de Croon, e Vijay Janapa Reddi. 2019. «Learning to seek: Autonomous source seeking with deep reinforcement learning onboard a nano drone microcontroller». ArXiv preprint abs/1909.11236. https://arxiv.org/abs/1909.11236.\nVideo 19.1 mostra come l’apprendimento di rinforzo profondo può essere utilizzato per consentire ai droni di cercare autonomamente fonti di luce.\n\n\n\n\n\n\nVideo 19.1: Imparare a Cercare\n\n\n\n\n\n\nVideo 19.2 è una panoramica dei droni autonomi per il rilevamento delle perdite di gas.\n\n\n\n\n\n\nVideo 19.2\n\n\n\n\n\n\nInoltre, gli sciami coordinati di droni sbloccano nuove capacità. Collaborando e condividendo informazioni, i team di droni hanno una visione completa della situazione. La copertura dei siti del disastro consente agli algoritmi TinyML di fondere e analizzare i dati da più punti di vista, amplificando la consapevolezza della situazione oltre i singoli droni (Bardienus P. Duisterhof et al. 2021).\n\nDuisterhof, Bardienus P., Shushuai Li, Javier Burgues, Vijay Janapa Reddi, e Guido C. H. E. de Croon. 2021. «Sniffy Bug: A Fully Autonomous Swarm of Gas-Seeking Nano Quadcopters in Cluttered Environments». In 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 9099–9106. IEEE; IEEE. https://doi.org/10.1109/iros51168.2021.9636217.\nAncora più importante, la ricognizione iniziale dei droni aumenta la sicurezza per i soccorritori umani. Mantenere le squadre di soccorso a una distanza di sicurezza finché i rilievi dei droni non valutano i pericoli salva vite umane. Una volta protetti, i droni possono guidare il posizionamento preciso del personale.\nCombinando mobilità agile, dati in tempo reale e coordinamento dello sciame, i droni abilitati TinyML promettono di trasformare la risposta ai disastri. La loro versatilità, velocità e sicurezza li rendono una risorsa vitale per gli sforzi di soccorso in ambienti pericolosi e inaccessibili. L’integrazione di droni autonomi con metodi tradizionali può accelerare le risposte quando è più importante.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>AI for Good</span>"
    ]
  },
  {
    "objectID": "contents/core/ai_for_good/ai_for_good.it.html#istruzione-e-sensibilizzazione",
    "href": "contents/core/ai_for_good/ai_for_good.it.html#istruzione-e-sensibilizzazione",
    "title": "19  AI for Good",
    "section": "19.7 Istruzione e Sensibilizzazione",
    "text": "19.7 Istruzione e Sensibilizzazione\nTinyML ha un immenso potenziale per aiutare ad affrontare le sfide nelle regioni in via di sviluppo, ma per realizzare i suoi benefici è necessaria un’istruzione mirata e un rafforzamento delle capacità. Riconoscendo questa necessità, i ricercatori accademici hanno guidato iniziative di sensibilizzazione per diffondere l’istruzione TinyML a livello globale.\nNel 2020, l’Università di Harvard, la Columbia University, l’International Centre for Theoretical Physics (ICTP) e l’UNIFEI hanno fondato congiuntamente la rete “TinyML for Developing Communities (TinyML4D)” (Zennaro, Plancher, e Reddi 2022). Questa rete consente alle università e ai ricercatori nei paesi in via di sviluppo di sfruttare TinyML per un impatto locale.\n\nZennaro, Marco, Brian Plancher, e V Janapa Reddi. 2022. «TinyML: Applied AI for development». In The UN 7th Multi-stakeholder Forum on Science, Technology and Innovation for the Sustainable Development Goals, 2022–05.\nUn obiettivo fondamentale è l’espansione dell’accesso all’istruzione applicata all’apprendimento automatico. La rete TinyML4D fornisce formazione, programmi di studio e risorse di laboratorio ai membri. Workshop pratici e progetti di raccolta dati offrono agli studenti esperienza pratica. I membri possono condividere le best practice e creare una comunità attraverso conferenze e collaborazioni accademiche.\nLa rete dà priorità all’abilitazione di soluzioni TinyML localmente rilevanti. I progetti affrontano sfide come agricoltura, salute e monitoraggio ambientale in base alle esigenze della comunità. Ad esempio, un’università membro in Ruanda ha sviluppato un sistema di monitoraggio delle inondazioni a basso costo utilizzando TinyML e sensori.\nTinyML4D include oltre 50 istituzioni membri in Africa, Asia e America Latina. Tuttavia, sono necessari maggiori investimenti e partnership industriali per raggiungere tutte le regioni sottoservite. La visione finale è quella di formare le nuove generazioni ad applicare eticamente TinyML per uno sviluppo sostenibile. Gli sforzi di sensibilizzazione odierni gettano le basi per democratizzare la tecnologia trasformativa per il futuro.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>AI for Good</span>"
    ]
  },
  {
    "objectID": "contents/core/ai_for_good/ai_for_good.it.html#accessibilità",
    "href": "contents/core/ai_for_good/ai_for_good.it.html#accessibilità",
    "title": "19  AI for Good",
    "section": "19.8 Accessibilità",
    "text": "19.8 Accessibilità\nLa tecnologia ha un potenziale immenso per abbattere le barriere affrontate dalle persone con disabilità e colmare le lacune nell’accessibilità. TinyML apre specificamente nuove possibilità per lo sviluppo di dispositivi di assistenza intelligenti e personalizzati.\nCon algoritmi di apprendimento automatico eseguiti localmente su microcontrollori, gli strumenti di accessibilità compatti possono funzionare in tempo reale senza dipendere dalla connettività. Il National Institute on Deafness and Other Communication Disorders (NIDCD) afferma che il 20% della popolazione mondiale ha una qualche forma di perdita dell’udito. Gli apparecchi acustici che sfruttano TinyML potrebbero riconoscere più parlanti e amplificare la voce di un target scelto in stanze affollate. Ciò consente alle persone con problemi di udito di concentrarsi su conversazioni specifiche.\nAnalogamente, i dispositivi di mobilità potrebbero utilizzare l’elaborazione della vista sul dispositivo per identificare ostacoli e caratteristiche del terreno. Ciò consente una navigazione e una sicurezza migliorate per gli ipovedenti. Aziende come Envision stanno sviluppando occhiali intelligenti, convertendo le informazioni visive in parlato, con TinyML embedded per guidare le persone non vedenti rilevando oggetti, testo e segnali stradali. Video 19.3 di seguito mostra i diversi casi di utilizzo nella vita reale degli occhiali per l’ausilio visivo Envision.\n\n\n\n\n\n\nVideo 19.3\n\n\n\n\n\n\nTinyML potrebbe persino alimentare arti protesici reattivi. Analizzando i segnali nervosi e i dati sensoriali come la tensione muscolare, protesi ed esoscheletri con ML embedded possono muoversi e regolare la presa in modo dinamico, rendendo il controllo più naturale e intuitivo. Le aziende stanno creando mani bioniche economiche e per uso quotidiano utilizzando TinyML. Per coloro che hanno difficoltà di linguaggio, i dispositivi abilitati alla voce con TinyML possono generare output vocali personalizzati da input non verbali. Pairs di Anthropic traduce i gesti in un linguaggio naturale su misura per i singoli utenti.\nAbilitando una tecnologia assistiva più personalizzabile, TinyML rende i servizi più accessibili e su misura per le esigenze individuali. E attraverso applicazioni di traduzione e interpretazione, TinyML può abbattere le barriere comunicative. App come Microsoft Translator offrono traduzioni in tempo reale basate sugli algoritmi TinyML.\nCon il suo design ponderato e inclusivo, TinyML promette più autonomia e dignità per le persone con disabilità. Tuttavia, gli sviluppatori dovrebbero coinvolgere direttamente le comunità, evitare di compromettere la privacy e considerare l’accessibilità economica per massimizzare i benefici. TinyML ha un enorme potenziale per contribuire a un mondo più giusto ed equo.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>AI for Good</span>"
    ]
  },
  {
    "objectID": "contents/core/ai_for_good/ai_for_good.it.html#infrastruttura-e-pianificazione-urbana",
    "href": "contents/core/ai_for_good/ai_for_good.it.html#infrastruttura-e-pianificazione-urbana",
    "title": "19  AI for Good",
    "section": "19.9 Infrastruttura e Pianificazione Urbana",
    "text": "19.9 Infrastruttura e Pianificazione Urbana\nCon l’aumento della popolazione urbana, le città affrontano sfide immense nella gestione efficiente di risorse e infrastrutture. TinyML presenta un potente strumento per lo sviluppo di sistemi intelligenti per ottimizzare le operazioni e la sostenibilità della città. Potrebbe rivoluzionare l’efficienza energetica negli edifici intelligenti.\nI modelli di apprendimento automatico possono imparare a prevedere e regolare l’uso di energia in base ai pattern di occupazione. I sensori miniaturizzati posizionati negli edifici possono fornire dati granulari e in tempo reale sull’utilizzo dello spazio, sulla temperatura e altro ancora (Seyedzadeh et al. 2018). Questa visibilità consente ai sistemi TinyML di ridurre al minimo gli sprechi ottimizzando riscaldamento, raffreddamento, illuminazione, ecc.\n\nSeyedzadeh, Saleh, Farzad Pour Rahimian, Ivan Glesk, e Marc Roper. 2018. «Machine learning for estimation of building energy consumption and performance: A review». Visualization in Engineering 6 (1): 1–20. https://doi.org/10.1186/s40327-018-0064-7.\nQuesti esempi dimostrano l’enorme potenziale di TinyML per infrastrutture cittadine efficienti e sostenibili. Tuttavia, gli urbanisti devono considerare privacy, sicurezza e accessibilità per garantire un’adozione responsabile. Con un’implementazione attenta, TinyML potrebbe modernizzare profondamente la vita urbana.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>AI for Good</span>"
    ]
  },
  {
    "objectID": "contents/core/ai_for_good/ai_for_good.it.html#sfide-e-considerazioni",
    "href": "contents/core/ai_for_good/ai_for_good.it.html#sfide-e-considerazioni",
    "title": "19  AI for Good",
    "section": "19.10 Sfide e Considerazioni",
    "text": "19.10 Sfide e Considerazioni\nSebbene TinyML offra immense opportunità, sarà fondamentale considerare attentamente le sfide e le implicazioni etiche man mano che l’adozione si diffonde a livello globale. I ricercatori hanno evidenziato i fattori chiave da affrontare, soprattutto quando si distribuisce TinyML nelle regioni in via di sviluppo.\nUna delle sfide più importanti è l’accesso limitato alla formazione e all’hardware (Ooko et al. 2021). Esistono solo programmi educativi su misura per TinyML e le economie emergenti hanno spesso bisogno di una solida catena di fornitura di elettronica. Saranno necessarie una formazione approfondita e partnership per coltivare le competenze e rendere i dispositivi disponibili alle comunità svantaggiate. Iniziative come la rete TinyML4D aiutano a fornire percorsi di apprendimento strutturati.\n\nOoko, Samson Otieno, Marvin Muyonga Ogore, Jimmy Nsenga, e Marco Zennaro. 2021. «TinyML in Africa: Opportunities and Challenges». In 2021 IEEE Globecom Workshops (GC Wkshps), 1–6. IEEE; IEEE. https://doi.org/10.1109/gcwkshps52748.2021.9682107.\nAnche le limitazioni dei dati pongono ostacoli. I modelli TinyML richiedono set di dati localizzati di qualità, che sono scarsi in ambienti con risorse insufficienti. La creazione di framework per il crowdsourcing dei dati in modo etico potrebbe risolvere questo problema. Tuttavia, la raccolta dei dati dovrebbe avvantaggiare direttamente le comunità locali, non solo estrarre valore.\nL’ottimizzazione dell’uso dell’energia e della connettività sarà fondamentale per la sostenibilità. Le basse esigenze di potenza di TinyML lo rendono ideale per casi d’uso fuori dalla rete. L’integrazione di batterie o energia solare può consentire un funzionamento continuo. Adattare i dispositivi per la trasmissione a bassa larghezza di banda in cui Internet è limitato massimizza anche l’impatto.\nLe barriere culturali e linguistiche complicano ulteriormente l’adozione. Le interfacce utente e i dispositivi dovrebbero tenere conto di tutti i livelli di alfabetizzazione ed evitare di escludere sottogruppi. Le soluzioni controllabili vocalmente nei dialetti locali possono migliorare l’accessibilità.\nAffrontare queste sfide richiede partnership olistiche, finanziamenti e supporto politico. Tuttavia, l’inclusione e l’etica di TinyML hanno un potenziale monumentale per elevare le popolazioni svantaggiate in tutto il mondo. Con un’implementazione ponderata, la tecnologia potrebbe democratizzare profondamente le opportunità.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>AI for Good</span>"
    ]
  },
  {
    "objectID": "contents/core/ai_for_good/ai_for_good.it.html#conclusione",
    "href": "contents/core/ai_for_good/ai_for_good.it.html#conclusione",
    "title": "19  AI for Good",
    "section": "19.11 Conclusione",
    "text": "19.11 Conclusione\nTinyML offre un’enorme opportunità di sfruttare la potenza dell’intelligenza artificiale per promuovere gli Obiettivi di sviluppo sostenibile delle Nazioni Unite e guidare l’impatto sociale a livello globale, come evidenziato da esempi in settori quali sanità, agricoltura, conservazione e altro; l’apprendimento automatico embedded sblocca nuove capacità per soluzioni accessibili e a basso costo, su misura per i contesti locali. TinyML aggira barriere come infrastrutture scadenti, connettività limitata e costi elevati che spesso escludono le comunità in via di sviluppo dalle tecnologie emergenti.\nTuttavia, realizzare il pieno potenziale di TinyML richiede una collaborazione olistica. Ricercatori, politici, aziende e stakeholder locali devono collaborare per fornire formazione, stabilire quadri etici, progettare soluzioni in comune e adattarle alle esigenze della comunità. Attraverso uno sviluppo e un’implementazione inclusivi, TinyML può mantenere la promessa di colmare le disuguaglianze e migliorare le popolazioni vulnerabili senza lasciare indietro nessuno.\nSe coltivato in modo responsabile, TinyML potrebbe democratizzare le opportunità e accelerare i progressi sulle priorità globali, dall’alleviamento della povertà alla resilienza climatica. La tecnologia rappresenta una nuova ondata di IA applicata per potenziare le società, promuovere la sostenibilità e spingere l’umanità verso una maggiore giustizia, prosperità e pace. TinyML offre uno sguardo a un futuro abilitato dall’IA accessibile a tutti.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>AI for Good</span>"
    ]
  },
  {
    "objectID": "contents/core/ai_for_good/ai_for_good.it.html#sec-ai-for-good-resource",
    "href": "contents/core/ai_for_good/ai_for_good.it.html#sec-ai-for-good-resource",
    "title": "19  AI for Good",
    "section": "19.12 Risorse",
    "text": "19.12 Risorse\nEcco un elenco curato di risorse per supportare studenti e insegnanti nei loro percorsi di apprendimento e insegnamento. Stiamo lavorando costantemente per espandere questa raccolta e presto aggiungeremo nuovi esercizi.\n\n\n\n\n\n\nSlide\n\n\n\n\n\nQueste slide sono uno strumento prezioso per gli insegnanti per tenere lezioni e per gli studenti per rivedere il materiale secondo il proprio ritmo. Incoraggiamo studenti e docenti a sfruttare queste slide per migliorare la loro comprensione e facilitare un trasferimento efficace delle conoscenze.\n\nTinyML for Social Impact.\n\n\n\n\n\n\n\n\n\n\nVideo\n\n\n\n\n\n\nVideo 19.1\nVideo 19.2\nVideo 19.3\n\n\n\n\n\n\n\n\n\n\nEsercizi\n\n\n\n\n\n\nEsercizio 19.1\nEsercizio 19.2",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>AI for Good</span>"
    ]
  },
  {
    "objectID": "contents/core/conclusion/conclusion.it.html",
    "href": "contents/core/conclusion/conclusion.it.html",
    "title": "20  Conclusione",
    "section": "",
    "text": "20.1 Panoramica\nQuesto libro esamina il campo in rapida evoluzione dei sistemi ML (Capitolo 2). Ci siamo concentrati sui sistemi perché, nonostante esistano numerose risorse sui modelli e sugli algoritmi di ML, c’è ancora molto da capire su come costruire i sistemi che li eseguono.\nPer fare un’analogia, consideriamo il processo di costruzione di un’auto. Sebbene siano disponibili molte risorse sui vari componenti di un’auto, come motore, trasmissione e sospensioni, spesso è necessario comprendere meglio come assemblare questi componenti in un veicolo funzionale. Proprio come un’auto richiede un sistema ben progettato e correttamente integrato per funzionare in modo efficiente e affidabile, anche i modelli ML richiedono un sistema robusto e costruito con cura per offrire il loro pieno potenziale. Inoltre, vi sono molte sfumature nella costruzione di sistemi ML, dato il loro caso d’uso specifico. Ad esempio, un’auto da corsa di Formula 1 deve essere assemblata in modo diverso da una normale auto Prius di tutti i giorni.\nIl nostro viaggio è iniziato tracciando la traiettoria storica del ML, dalle sue fondamenta teoriche al suo stato attuale come forza trasformativa in tutti i settori (Capitolo 3). Questo viaggio ha evidenziato i notevoli progressi nel campo, le sfide e le opportunità.\nIn questo libro, abbiamo esaminato le complessità dei sistemi ML, esaminando i componenti critici e le best practice necessarie per creare una pipeline fluida ed efficiente. Dalla preelaborazione dei dati e dalla formazione del modello alla distribuzione e al monitoraggio, abbiamo fornito approfondimenti e indicazioni per aiutare i lettori a orientarsi nel complesso panorama dello sviluppo del sistema ML.\nI sistemi ML coinvolgono flussi di lavoro complessi, che abbracciano vari argomenti, dall’ingegneria dei dati alla distribuzione del modello su sistemi diversi (Capitolo 4). Fornendo una panoramica di questi componenti del sistema ML, abbiamo mirato a mostrare l’enorme profondità e ampiezza del campo e le competenze necessarie. Comprendere le complessità dei flussi di lavoro di apprendimento automatico è fondamentale sia per i professionisti sia per i ricercatori, poiché consente loro di orientarsi in modo efficace nel panorama e di sviluppare soluzioni di apprendimento automatico solide, efficienti e di impatto.\nConcentrandoci sull’aspetto sistemico del ML, puntiamo a colmare il divario tra conoscenza teorica e implementazione pratica. Proprio come un sistema corporeo umano sano consente agli organi di funzionare in modo ottimale, un sistema ML ben progettato consente ai modelli di fornire costantemente risultati accurati e affidabili. L’obiettivo di questo libro è quello di fornire ai lettori le conoscenze e gli strumenti necessari per creare sistemi ML che mostrino la potenza dei modelli sottostanti e garantiscano un’integrazione e un funzionamento fluidi, proprio come un corpo umano ben funzionante.",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Conclusione</span>"
    ]
  },
  {
    "objectID": "contents/core/conclusion/conclusion.it.html#conoscere-limportanza-dei-dataset-ml",
    "href": "contents/core/conclusion/conclusion.it.html#conoscere-limportanza-dei-dataset-ml",
    "title": "20  Conclusione",
    "section": "20.2 Conoscere l’Importanza dei Dataset ML",
    "text": "20.2 Conoscere l’Importanza dei Dataset ML\nUno degli aspetti chiave che abbiamo sottolineato è che i dati sono la base su cui sono costruiti i sistemi ML (Capitolo 5). I dati sono il nuovo codice che programma reti neurali profonde, rendendo l’ingegneria dei dati la prima e più critica fase di qualsiasi pipeline ML. Ecco perché abbiamo iniziato la nostra esplorazione immergendoci nelle basi dell’ingegneria dei dati, riconoscendo che qualità, diversità e approvvigionamento etico sono fondamentali per creare modelli di apprendimento automatico solidi e affidabili.\nL’importanza di dati di alta qualità deve essere bilanciata. Le carenze nella qualità dei dati possono portare a conseguenze negative significative, come previsioni errate, cessazioni di progetti e persino potenziali danni alle comunità. Questi effetti a cascata, spesso chiamati “Data Cascades” [cascate di dati], evidenziano la necessità di pratiche diligenti di gestione e governance dei dati. I professionisti del ML devono dare priorità alla qualità dei dati, garantire diversità e rappresentatività e aderire a standard etici di raccolta e utilizzo dei dati. In questo modo, possiamo mitigare i rischi associati alla scarsa qualità dei dati e creare sistemi ML affidabili, sicuri e vantaggiosi per la società.",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Conclusione</span>"
    ]
  },
  {
    "objectID": "contents/core/conclusion/conclusion.it.html#esplorare-il-panorama-dei-framework-di-ia",
    "href": "contents/core/conclusion/conclusion.it.html#esplorare-il-panorama-dei-framework-di-ia",
    "title": "20  Conclusione",
    "section": "20.3 Esplorare il Panorama dei Framework di IA",
    "text": "20.3 Esplorare il Panorama dei Framework di IA\nEsistono molti framework ML diversi. Pertanto, ci siamo immersi nell’evoluzione di diversi framework ML, analizzando il funzionamento interno di quelli più popolari come TensorFlow e PyTorch e fornendo approfondimenti sui componenti principali e sulle funzionalità avanzate che li definiscono (Capitolo 6). Abbiamo anche esaminato la specializzazione di framework su misura per esigenze specifiche, come quelli progettati per l’IA embedded. Abbiamo discusso i criteri per la selezione del framework più adatto per un determinato progetto.\nLa nostra esplorazione ha anche toccato le tendenze future che dovrebbero plasmare il panorama dei framework ML nei prossimi anni. Man mano che il campo continua a evolversi, possiamo prevedere l’emergere di framework più specializzati e ottimizzati che soddisfano i requisiti unici di diversi domini e scenari di distribuzione, come abbiamo visto con TensorFlow Lite per microcontrollori. Restando al passo con questi sviluppi e comprendendo i compromessi coinvolti nella selezione del framework, possiamo prendere decisioni informate e sfruttare gli strumenti più appropriati per creare sistemi ML efficienti.\nInoltre, ci aspettiamo di vedere una crescente enfasi sull’interoperabilità dei framework e sugli sforzi di standardizzazione, come il formato ONNX (Open Neural Network Exchange). Questo formato consente di addestrare i modelli in un framework e distribuirli in un altro, facilitando una maggiore collaborazione e portabilità su diverse piattaforme e ambienti.",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Conclusione</span>"
    ]
  },
  {
    "objectID": "contents/core/conclusion/conclusion.it.html#comprendere-i-fondamenti-del-training-ml",
    "href": "contents/core/conclusion/conclusion.it.html#comprendere-i-fondamenti-del-training-ml",
    "title": "20  Conclusione",
    "section": "20.4 Comprendere i Fondamenti del Training ML",
    "text": "20.4 Comprendere i Fondamenti del Training ML\nCome professionisti ML che creano sistemi ML, è fondamentale comprendere a fondo il processo di addestramento dell’IA e le sfide del sistema nel ridimensionarlo e ottimizzarlo. Sfruttando le capacità dei moderni framework di IA e restando aggiornati con gli ultimi progressi nelle tecniche di training, possiamo creare sistemi ML robusti, efficienti e scalabili in grado di affrontare problemi del mondo reale e guidare l’innovazione in vari domini.\nAbbiamo iniziato esaminando i fondamenti della formazione AI (Capitolo 7), che comporta l’inserimento di dati nei modelli ML e la regolazione dei loro parametri per ridurre al minimo la differenza tra output previsti ed effettivi. Questo processo è computazionalmente intensivo e richiede un’attenta considerazione di vari fattori, come la scelta di algoritmi di ottimizzazione, velocità di apprendimento, dimensioni del batch e tecniche di regolarizzazione. Comprendere questi concetti è fondamentale per sviluppare pipeline di training efficaci ed efficienti.\nTuttavia, il training di modelli ML su larga scala pone sfide di sistema significative. Man mano che le dimensioni dei set di dati e la complessità dei modelli aumentano, le risorse computazionali richieste per la formazione possono diventare proibitive. Ciò ha portato allo sviluppo di tecniche di training distribuite, come il parallelismo di dati e di modelli, che consentono a più dispositivi di collaborare nel processo di training. Framework come TensorFlow e PyTorch si sono evoluti per supportare questi paradigmi di training distribuiti, consentendo ai professionisti di scalare i carichi di lavoro di training su cluster di GPU o TPU.\nOltre al training distribuito, abbiamo discusso tecniche per ottimizzare il processo di training, come il training a precisione mista e la compressione del gradiente. È importante notare che, sebbene queste tecniche possano sembrare algoritmiche, hanno un impatto significativo sulle prestazioni del sistema. La scelta degli algoritmi di training, della precisione e delle strategie di comunicazione influisce direttamente sull’utilizzo delle risorse, sulla scalabilità e sull’efficienza del sistema ML. Pertanto, è fondamentale adottare un approccio di co-progettazione algoritmo-hardware o algoritmo-sistema, in cui le scelte algoritmiche vengono effettuate in tandem con le considerazioni del sistema. Comprendendo l’interazione tra algoritmi e hardware, possiamo prendere decisioni informate che ottimizzano le prestazioni del modello e l’efficienza del sistema, portando infine a soluzioni ML più efficaci e scalabili.",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Conclusione</span>"
    ]
  },
  {
    "objectID": "contents/core/conclusion/conclusion.it.html#perseguire-lefficienza-nei-sistemi-di-ia",
    "href": "contents/core/conclusion/conclusion.it.html#perseguire-lefficienza-nei-sistemi-di-ia",
    "title": "20  Conclusione",
    "section": "20.5 Perseguire l’Efficienza nei Sistemi di IA",
    "text": "20.5 Perseguire l’Efficienza nei Sistemi di IA\nL’implementazione di modelli ML addestrati è più complessa della semplice esecuzione delle reti; l’efficienza è fondamentale (Capitolo 8). In questo capitolo sull’efficienza dell’IA, abbiamo sottolineato che l’efficienza non è solo un lusso, ma una necessità nei sistemi di intelligenza artificiale. Abbiamo approfondito i concetti chiave alla base dell’efficienza dei sistemi di IA, riconoscendo che le richieste computazionali sulle reti neurali possono essere scoraggianti, anche per i sistemi minimi. Per integrare perfettamente l’IA nei dispositivi quotidiani e nei sistemi essenziali, deve funzionare in modo ottimale entro i vincoli delle risorse limitate, mantenendo al contempo la sua efficacia.\nIn tutto il libro, abbiamo evidenziato l’importanza di perseguire l’efficienza per garantire che i modelli di IA siano semplificati, rapidi e sostenibili. Ottimizzando i modelli per l’efficienza, possiamo ampliare la loro applicabilità su varie piattaforme e scenari, consentendo all’IA di essere distribuita in ambienti con risorse limitate come sistemi embedded e dispositivi edge. Questa ricerca dell’efficienza è fondamentale per l’adozione diffusa e l’implementazione pratica delle tecnologie di IA nelle applicazioni del mondo reale.",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Conclusione</span>"
    ]
  },
  {
    "objectID": "contents/core/conclusion/conclusion.it.html#ottimizzazione-delle-architetture-dei-modelli-ml",
    "href": "contents/core/conclusion/conclusion.it.html#ottimizzazione-delle-architetture-dei-modelli-ml",
    "title": "20  Conclusione",
    "section": "20.6 Ottimizzazione delle Architetture dei Modelli ML",
    "text": "20.6 Ottimizzazione delle Architetture dei Modelli ML\nAbbiamo quindi esplorato varie architetture di modelli, dal perceptron fondamentale alle sofisticate reti di trasformatori, ciascuna adattata a specifiche attività e tipi di dati. Questa esplorazione ha messo in luce la notevole diversità e adattabilità dei modelli di apprendimento automatico, consentendo loro di affrontare vari problemi in tutti i domini.\nTuttavia, quando si distribuiscono questi modelli su sistemi, in particolare sistemi embedded con risorse limitate, l’ottimizzazione del modello diventa una necessità. L’evoluzione delle architetture di modelli, dai primi MobileNet progettati per dispositivi mobili ai più recenti modelli TinyML ottimizzati per microcontrollori, è una testimonianza della continua innovazione.\nNel capitolo sull’ottimizzazione del modello (Capitolo 9), abbiamo esaminato l’arte e la scienza dell’ottimizzazione dei modelli di apprendimento automatico per garantire che siano leggeri, efficienti ed efficaci quando distribuiti in scenari TinyML. Abbiamo esplorato tecniche come la compressione del modello, la quantizzazione e la ricerca dell’architettura, che ci consentono di ridurre l’impronta computazionale dei modelli mantenendone le prestazioni. Applicando queste tecniche di ottimizzazione, possiamo creare modelli su misura per i vincoli specifici dei sistemi embedded, consentendo l’implementazione di potenti capacità di intelligenza artificiale su dispositivi edge. Ciò apre molte possibilità per l’elaborazione e il processo decisionale intelligenti e in tempo reale in applicazioni IoT, robotica e mobile computing. Mentre continuiamo a spingere i confini dell’efficienza dell’intelligenza artificiale, ci aspettiamo di vedere soluzioni ancora più innovative per l’implementazione di modelli di apprendimento automatico in ambienti con risorse limitate.",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Conclusione</span>"
    ]
  },
  {
    "objectID": "contents/core/conclusion/conclusion.it.html#avanzamento-dellhardware-di-elaborazione-dellia",
    "href": "contents/core/conclusion/conclusion.it.html#avanzamento-dellhardware-di-elaborazione-dellia",
    "title": "20  Conclusione",
    "section": "20.7 Avanzamento dell’Hardware di Elaborazione dell’IA",
    "text": "20.7 Avanzamento dell’Hardware di Elaborazione dell’IA\nNel corso degli anni, abbiamo assistito a notevoli progressi nell’hardware ML, spinti dall’insaziabile domanda di potenza di calcolo e dalla necessità di affrontare le sfide dei vincoli di risorse nelle distribuzioni nel mondo reale (Capitolo 10). Questi progressi sono stati cruciali nel consentire l’implementazione di potenti funzionalità di intelligenza artificiale su dispositivi con risorse limitate, aprendo nuove possibilità in vari settori.\nL’accelerazione hardware specializzata è essenziale per superare questi vincoli e abilitare l’apprendimento automatico ad alte prestazioni. Gli acceleratori hardware, come GPU, FPGA e ASIC, ottimizzano le operazioni ad alta intensità di calcolo, in particolare l’inferenza, sfruttando i chip personalizzati progettati per efficienti moltiplicazioni di matrici. Questi acceleratori forniscono sostanziali accelerazioni rispetto alle CPU per uso generico, consentendo l’esecuzione in tempo reale di modelli ML avanzati su dispositivi con rigide limitazioni di dimensioni, peso e potenza.\nAbbiamo anche esplorato le varie tecniche e approcci per l’accelerazione hardware nei sistemi di apprendimento automatico embedded. Abbiamo discusso i compromessi nella selezione dell’hardware appropriato per casi d’uso specifici e l’importanza delle ottimizzazioni software per sfruttare appieno le capacità di questi acceleratori. Comprendendo questi concetti, i professionisti del ML possono prendere decisioni informate quando progettano e distribuiscono sistemi ML.\nData la pletora di soluzioni hardware ML disponibili, il benchmarking è diventato essenziale per lo sviluppo e la distribuzione di sistemi di apprendimento automatico (Capitolo 11). Il benchmarking consente agli sviluppatori di misurare e confrontare le prestazioni di diverse piattaforme hardware, architetture di modello, procedure di training e strategie di distribuzione. Utilizzando benchmark consolidati come MLPerf, i professionisti ottengono preziose informazioni sugli approcci più efficaci per un dato problema, considerando i vincoli unici dell’ambiente di distribuzione del target.\nI progressi nell’hardware ML, combinati con le informazioni ottenute dalle tecniche di benchmarking e ottimizzazione, hanno aperto la strada alla distribuzione con successo delle capacità di apprendimento automatico su vari dispositivi, dai potenti server edge ai microcontrollori con risorse limitate. Man mano che il campo continua a evolversi, ci aspettiamo di vedere soluzioni hardware e approcci di benchmarking ancora più innovativi che amplieranno ulteriormente i confini di ciò che è possibile con i sistemi di apprendimento automatico embedded.",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Conclusione</span>"
    ]
  },
  {
    "objectID": "contents/core/conclusion/conclusion.it.html#abbracciare-lapprendimento-on-device",
    "href": "contents/core/conclusion/conclusion.it.html#abbracciare-lapprendimento-on-device",
    "title": "20  Conclusione",
    "section": "20.8 Abbracciare l’Apprendimento “On-Device”",
    "text": "20.8 Abbracciare l’Apprendimento “On-Device”\nOltre ai progressi nell’hardware ML, abbiamo anche esplorato l’apprendimento “on-device, in cui i modelli possono adattarsi e apprendere direttamente sul dispositivo (Capitolo 12). Questo approccio ha implicazioni significative per la privacy e la sicurezza dei dati, poiché le informazioni sensibili possono essere elaborate localmente senza la necessità di trasmissione a server esterni.\nL’apprendimento “on-device” migliora la privacy mantenendo i dati entro i confini del dispositivo, riducendo il rischio di accessi non autorizzati o violazioni dei dati. Riduce inoltre la dipendenza dalla connettività cloud, consentendo ai modelli ML di funzionare efficacemente anche in scenari con accesso a Internet limitato o intermittente. Abbiamo discusso tecniche come l’apprendimento tramite trasferimento e l’apprendimento federato, che hanno ampliato le capacità dell’apprendimento sul dispositivo. L’apprendimento tramite trasferimento consente ai modelli di sfruttare le conoscenze acquisite da un’attività o dominio per migliorare le prestazioni su un altro, consentendo un apprendimento più efficiente ed efficace su dispositivi con risorse limitate. D’altra parte, l’apprendimento federato consente aggiornamenti collaborativi del modello su dispositivi distribuiti senza aggregazione centralizzata dei dati. Questo approccio consente a più dispositivi di contribuire all’apprendimento mantenendo i propri dati localmente, migliorando la privacy e la sicurezza.\nQuesti progressi nell’apprendimento su dispositivo hanno aperto la strada ad applicazioni di apprendimento automatico più sicure, rispettose della privacy e decentralizzate. Mentre diamo priorità alla privacy e alla sicurezza dei dati nello sviluppo di sistemi ML, ci aspettiamo di vedere soluzioni più innovative che consentano potenti capacità di IA proteggendo al contempo le informazioni sensibili e garantendo la privacy degli utenti.",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Conclusione</span>"
    ]
  },
  {
    "objectID": "contents/core/conclusion/conclusion.it.html#semplificazione-delle-operazioni-ml",
    "href": "contents/core/conclusion/conclusion.it.html#semplificazione-delle-operazioni-ml",
    "title": "20  Conclusione",
    "section": "20.9 Semplificazione delle Operazioni ML",
    "text": "20.9 Semplificazione delle Operazioni ML\nAnche se abbiamo capito bene i pezzi di cui sopra, sfide e considerazioni devono essere affrontate per garantire un’integrazione e un funzionamento di successo dei modelli ML negli ambienti di produzione. Nel capitolo MLOps (Capitolo 13) abbiamo studiato le pratiche e le architetture necessarie per sviluppare, distribuire e gestire i modelli ML durante il loro intero ciclo di vita. Abbiamo esaminato le fasi di ML, dalla raccolta dati e dal training del modello alla valutazione, distribuzione e monitoraggio continuo.\nAbbiamo appreso l’importanza dell’automazione, della collaborazione e del miglioramento continuo in MLOps. Automatizzando i processi chiave, i team possono semplificare i loro flussi di lavoro, ridurre gli errori manuali e accelerare la distribuzione dei modelli ML. La collaborazione tra team diversi, tra cui data scientist, ingegneri ed esperti di dominio, garantisce lo sviluppo e la distribuzione di successo dei sistemi ML.\nL’obiettivo finale di questo capitolo era quello di fornire ai lettori una comprensione completa della gestione dei modelli ML, dotandoli delle conoscenze e degli strumenti necessari per creare ed eseguire applicazioni ML che forniscano un valore sostenibile con successo. Adottando le “best practices” in ambito MLOps, le organizzazioni possono garantire il successo e l’impatto a lungo termine delle proprie iniziative di ML, promuovendo l’innovazione e producendo risultati significativi.",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Conclusione</span>"
    ]
  },
  {
    "objectID": "contents/core/conclusion/conclusion.it.html#garantire-sicurezza-e-privacy",
    "href": "contents/core/conclusion/conclusion.it.html#garantire-sicurezza-e-privacy",
    "title": "20  Conclusione",
    "section": "20.10 Garantire Sicurezza e Privacy",
    "text": "20.10 Garantire Sicurezza e Privacy\nNessun sistema ML è mai completo senza pensare a sicurezza e privacy. Sono di fondamentale importanza quando si sviluppano sistemi ML nel mondo reale. Poiché l’apprendimento automatico trova sempre più applicazione in domini sensibili come sanità, finanza e dati personali, salvaguardare la riservatezza e prevenire l’uso improprio di dati e modelli diventa un imperativo critico, e questi erano i concetti che abbiamo discusso in precedenza (Capitolo 14).\nPer creare sistemi ML robusti e responsabili, i professionisti devono comprendere a fondo i potenziali rischi per la sicurezza e la privacy. Questi rischi includono perdite di dati, che possono esporre informazioni sensibili; furto di modelli, in cui attori malintenzionati rubano modelli addestrati; attacchi avversari in grado di manipolare il comportamento del modello; pregiudizi nei modelli che possono portare a risultati ingiusti o discriminatori; e accesso involontario a informazioni private.\nPer mitigare questi rischi è necessaria una profonda comprensione delle best practice in materia di sicurezza e privacy. Pertanto, abbiamo sottolineato che la sicurezza e la privacy non possono essere un ripensamento: devono essere affrontate in modo proattivo in ogni fase del ciclo di vita dello sviluppo del sistema ML. Sin dalle fasi iniziali di raccolta ed etichettatura dei dati, è fondamentale garantire che i dati siano gestiti in modo sicuro e che la privacy sia protetta. Durante il training e la valutazione del modello, è possibile impiegare tecniche come la privacy differenziale e il calcolo multi-parte sicuro per salvaguardare le informazioni sensibili.\nQuando si distribuiscono modelli ML, è necessario implementare controlli di accesso, crittografia e meccanismi di monitoraggio robusti per impedire l’accesso non autorizzato e rilevare potenziali violazioni della sicurezza. Il monitoraggio e l’audit continui dei sistemi ML come parte di MLOps sono inoltre essenziali per identificare e affrontare le vulnerabilità emergenti di sicurezza o privacy.\nIntegrando considerazioni sulla sicurezza e sulla privacy in ogni fase di creazione, distribuzione e gestione dei sistemi ML, possiamo sbloccare in modo sicuro i vantaggi dell’IA proteggendo al contempo i diritti degli individui e garantendo l’uso responsabile di queste potenti tecnologie. Solo attraverso questo approccio proattivo e completo possiamo creare sistemi ML che non siano solo tecnologicamente avanzati, ma anche eticamente solidi e degni della fiducia del pubblico.",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Conclusione</span>"
    ]
  },
  {
    "objectID": "contents/core/conclusion/conclusion.it.html#sostenere-considerazioni-etiche",
    "href": "contents/core/conclusion/conclusion.it.html#sostenere-considerazioni-etiche",
    "title": "20  Conclusione",
    "section": "20.11 Sostenere Considerazioni Etiche",
    "text": "20.11 Sostenere Considerazioni Etiche\nMentre accogliamo i progressi dell’apprendimento automatico in tutti gli aspetti della nostra vita, è fondamentale tenere a mente le considerazioni etiche che plasmeranno il futuro dell’IA (Capitolo 15). Equità, trasparenza, responsabilità e privacy nei sistemi di IA saranno fondamentali man mano che diventeranno più integrati nelle nostre vite e nei nostri processi decisionali.\nPoiché i sistemi di IA stanno diventando sempre più diffusi e influenti, è importante garantire che siano progettati e implementati nel rispetto dei principi etici. Ciò significa mitigare attivamente i pregiudizi, promuovere l’equità e prevenire risultati discriminatori. Inoltre, la progettazione etica dell’IA garantisce la trasparenza nel modo in cui i sistemi di IA prendono decisioni, consentendo agli utenti di comprendere e fidarsi dei loro risultati.\nLa responsabilità è un’altra considerazione etica fondamentale. Man mano che i sistemi di IA assumono maggiori responsabilità e prendono decisioni che hanno un impatto sugli individui e sulla società, devono esserci meccanismi chiari per ritenere responsabili questi sistemi e i loro creatori. Ciò include la definizione di “framework” per l’audit e il monitoraggio dei sistemi di IA e la definizione di meccanismi di responsabilità e risarcimento in caso di danni o conseguenze indesiderate.\nQuadri etici, regolamenti e standard saranno essenziali per affrontare queste sfide etiche. Questi quadri dovrebbero guidare lo sviluppo e l’implementazione responsabili delle tecnologie di IA, assicurando che siano in linea con i valori della società e promuovano il benessere di individui e comunità.\nInoltre, discussioni e collaborazioni in corso tra ricercatori, professionisti, politici e società saranno cruciali per orientarsi nel panorama etico dell’IA. Queste conversazioni dovrebbero essere inclusive e diversificate, riunendo diverse prospettive e competenze per sviluppare soluzioni complete ed eque. Mentre andiamo avanti, è responsabilità collettiva di tutte le parti interessate dare priorità alle considerazioni etiche nello sviluppo e nell’implementazione dei sistemi di IA.",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Conclusione</span>"
    ]
  },
  {
    "objectID": "contents/core/conclusion/conclusion.it.html#promuovere-la-sostenibilità-e-lequità",
    "href": "contents/core/conclusion/conclusion.it.html#promuovere-la-sostenibilità-e-lequità",
    "title": "20  Conclusione",
    "section": "20.12 Promuovere la Sostenibilità e l’Equità",
    "text": "20.12 Promuovere la Sostenibilità e l’Equità\nLe crescenti richieste computazionali dell’apprendimento automatico, in particolare per l’addestramento di modelli di grandi dimensioni, hanno sollevato preoccupazioni circa il loro impatto ambientale dovuto all’elevato consumo energetico e alle emissioni di carbonio (Capitolo 16). Man mano che la scala e la complessità dei modelli continuano a crescere, affrontare le sfide di sostenibilità associate allo sviluppo dell’IA diventa imperativo. Per mitigare l’impatto ambientale dell’IA, lo sviluppo di algoritmi efficienti dal punto di vista energetico è fondamentale. Ciò comporta l’ottimizzazione di modelli e procedure di addestramento per ridurre al minimo i requisiti computazionali mantenendo le prestazioni. Tecniche come la compressione del modello, la quantizzazione e la ricerca efficiente dell’architettura neurale possono aiutare a ridurre il consumo energetico dei sistemi di IA.\nL’utilizzo di fonti di energia rinnovabili per alimentare l’infrastruttura di IA è un altro passo importante verso la sostenibilità. Passando a fonti di energia pulita come quella solare, eolica e idroelettrica, le emissioni di carbonio associate allo sviluppo dell’IA possono essere notevolmente ridotte. Ciò richiede uno sforzo concertato da parte della comunità dell’IA e il supporto di politici e leader del settore per investire e adottare soluzioni di energia rinnovabile. Inoltre, l’esplorazione di paradigmi di elaborazione alternativi, come l’elaborazione neuromorfica e fotonica, promette di sviluppare sistemi di intelligenza artificiale più efficienti dal punto di vista energetico. Sviluppando hardware e algoritmi che emulano i meccanismi di elaborazione del cervello, possiamo potenzialmente creare sistemi di intelligenza artificiale che siano sia potenti che sostenibili.\nLa comunità dell’intelligenza artificiale deve dare priorità alla sostenibilità come considerazione chiave nella ricerca e nello sviluppo. Ciò implica investire in iniziative di elaborazione ecologica, come lo sviluppo di hardware efficiente dal punto di vista energetico e l’ottimizzazione dei data center per ridurre il consumo di energia. Richiede inoltre la collaborazione tra discipline, riunendo esperti di IA, energia e sostenibilità per sviluppare soluzioni olistiche.\nInoltre, è importante riconoscere che l’accesso alle risorse di elaborazione dell’IA e dell’apprendimento automatico potrebbe non essere distribuito equamente tra organizzazioni e regioni. Questa disparità può portare a un divario crescente tra coloro che hanno i mezzi per sfruttare le tecnologie di IA avanzate e coloro che non li hanno. Organizzazioni come l’Organizzazione per la cooperazione e lo sviluppo economico (OCSE) stanno esplorando attivamente modi per affrontare questo problema e promuovere una maggiore equità nell’accesso e nell’adozione dell’IA. Promuovendo la cooperazione internazionale, condividendo le best practice e supportando iniziative di capacity building, possiamo garantire che i benefici dell’IA siano più ampiamente accessibili e che nessuno venga lasciato indietro nella rivoluzione dell’IA.",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Conclusione</span>"
    ]
  },
  {
    "objectID": "contents/core/conclusion/conclusion.it.html#migliorare-la-robustezza-e-la-resilienza",
    "href": "contents/core/conclusion/conclusion.it.html#migliorare-la-robustezza-e-la-resilienza",
    "title": "20  Conclusione",
    "section": "20.13 Migliorare la Robustezza e la Resilienza",
    "text": "20.13 Migliorare la Robustezza e la Resilienza\nIl capitolo su IA Robusta approfondisce i concetti fondamentali, le tecniche e gli strumenti per la creazione di sistemi ML fault-tolerant e error-resilient (Capitolo 17). In quel capitolo, abbiamo esplorato come le tecniche di IA robuste possano affrontare le sfide poste da vari tipi di guasti hardware, inclusi guasti transitori, permanenti e intermittenti, nonché problemi software come bug, difetti di progettazione ed errori di implementazione.\nUtilizzando tecniche di IA robuste, i sistemi ML possono mantenere la loro affidabilità, sicurezza e prestazioni anche in condizioni avverse. Queste tecniche consentono ai sistemi di rilevare e ripristinare i guasti, adattarsi ad ambienti mutevoli e prendere decisioni in condizioni di incertezza.\nIl capitolo consente a ricercatori e professionisti di sviluppare soluzioni di IA in grado di resistere alle complessità e alle incertezze degli ambienti del mondo reale. Fornisce approfondimenti sui principi di progettazione, sulle architetture e sugli algoritmi alla base di sistemi IA robusti e una guida pratica per l’implementazione e la convalida di questi sistemi.",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Conclusione</span>"
    ]
  },
  {
    "objectID": "contents/core/conclusion/conclusion.it.html#plasmare-il-futuro-dei-sistemi-ml",
    "href": "contents/core/conclusion/conclusion.it.html#plasmare-il-futuro-dei-sistemi-ml",
    "title": "20  Conclusione",
    "section": "20.14 Plasmare il Futuro dei Sistemi ML",
    "text": "20.14 Plasmare il Futuro dei Sistemi ML\nGuardando al futuro, la traiettoria dei sistemi ML punta verso un cambiamento di paradigma da un approccio incentrato sul modello a uno più incentrato sui dati. Questo cambiamento riconosce che la qualità e la diversità dei dati sono fondamentali per sviluppare modelli di IA solidi, affidabili ed equi.\nPrevediamo una crescente enfasi sulle tecniche di cura dei dati, etichettatura e aumento nei prossimi anni. Queste pratiche mirano a garantire che i modelli siano addestrati su dati rappresentativi di alta qualità che riflettano accuratamente le complessità e le sfumature degli scenari del mondo reale. Concentrandoci sulla qualità e sulla diversità dei dati, possiamo mitigare i rischi di modelli parziali o distorti che possono perpetuare risultati ingiusti o discriminatori.\nQuesto approccio incentrato sui dati sarà fondamentale per affrontare le sfide di pregiudizio, equità e generalizzabilità nei sistemi ML. Cercando e incorporando attivamente set di dati diversi e inclusivi, possiamo sviluppare modelli più solidi, equi e applicabili per vari contesti e popolazioni. Inoltre, l’enfasi sui dati guiderà i progressi in tecniche come il “data augmentation”, in cui i set di dati esistenti vengono ampliati e diversificati tramite sintesi, traduzione e generazione di dati. Queste tecniche possono aiutare a superare i limiti dei set di dati piccoli o sbilanciati, consentendo lo sviluppo di modelli più accurati e generalizzabili.\nNegli ultimi anni, l’IA generativa ha preso d’assalto il campo, dimostrando notevoli capacità nella creazione di immagini, video e testo realistici. Tuttavia, l’ascesa dell’IA generativa porta anche nuove sfide per i sistemi ML (Capitolo 18). A differenza dei tradizionali sistemi ML, i modelli generativi spesso richiedono più risorse computazionali e pongono sfide in termini di scalabilità ed efficienza. Inoltre, la valutazione e il benchmarking dei modelli generativi presentano difficoltà, poiché le metriche tradizionali utilizzate per le attività di classificazione potrebbero non essere direttamente applicabili. Lo sviluppo di solidi framework di valutazione per i modelli generativi è un’area di ricerca attiva.\nComprendere e affrontare queste sfide di sistema e considerazioni etiche sarà fondamentale per dare forma al futuro dell’IA generativa e al suo impatto sulla società. In qualità di professionisti e ricercatori di ML, siamo responsabili dello sviluppo delle capacità tecniche dei modelli generativi e dello sviluppo di sistemi e framework solidi in grado di mitigare i potenziali rischi e garantire l’applicazione vantaggiosa di questa potente tecnologia.",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Conclusione</span>"
    ]
  },
  {
    "objectID": "contents/core/conclusion/conclusion.it.html#applicazione-di-ai-for-good",
    "href": "contents/core/conclusion/conclusion.it.html#applicazione-di-ai-for-good",
    "title": "20  Conclusione",
    "section": "20.15 Applicazione di “AI for Good”",
    "text": "20.15 Applicazione di “AI for Good”\nIl potenziale dell’IA per essere utilizzata per il bene sociale è vasto, a condizione che vengano sviluppati e distribuiti sistemi ML responsabili su larga scala in vari casi d’uso (Capitolo 19). Per realizzare questo potenziale, è essenziale che ricercatori e professionisti si impegnino attivamente nel processo di apprendimento, sperimentazione e superamento dei limiti di ciò che è possibile.\nDurante lo sviluppo dei sistemi ML, è fondamentale ricordare i temi e le lezioni chiave esplorati in questo libro. Questi includono l’importanza della qualità e della diversità dei dati, la ricerca di efficienza e robustezza, il potenziale di TinyML e del calcolo neuromorfico e l’imperativo della sicurezza e della privacy. Queste intuizioni informano il lavoro e guidano le decisioni di coloro che sono coinvolti nello sviluppo di sistemi di IA.\nÈ importante riconoscere che lo sviluppo dell’IA non è solo un’impresa tecnica, ma anche profondamente umana. Richiede collaborazione, empatia e un impegno per comprendere le implicazioni sociali dei sistemi creati. Interagire con esperti di diversi campi, come etica, scienze sociali e politica, è essenziale per garantire che i sistemi di IA sviluppati siano tecnicamente validi, socialmente responsabili e utili. Cogliere l’opportunità di far parte di questo campo trasformativo e plasmarne il futuro è un privilegio e una responsabilità. Lavorando insieme, possiamo creare un mondo in cui i sistemi di ML fungono da strumenti per un cambiamento positivo e per migliorare la condizione umana.",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Conclusione</span>"
    ]
  },
  {
    "objectID": "contents/core/conclusion/conclusion.it.html#congratulazioni",
    "href": "contents/core/conclusion/conclusion.it.html#congratulazioni",
    "title": "20  Conclusione",
    "section": "20.16 Congratulazioni",
    "text": "20.16 Congratulazioni\nCongratulazioni per essere arrivati fin qui e buona fortuna per gli sforzi futuri! Il futuro dell’intelligenza artificiale è luminoso e pieno di infinite possibilità. Sarà emozionante vedere gli incredibili contributi che darete in questo campo.\nSentitevi liberi di contattarmi in qualsiasi momento all’indirizzo vj at eecs dot harvard dot edu.\n– Prof. Vijay Janapa Reddi, Harvard University",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Conclusione</span>"
    ]
  },
  {
    "objectID": "contents/labs/overview.it.html",
    "href": "contents/labs/overview.it.html",
    "title": "Panoramica",
    "section": "",
    "text": "Obiettivi dell’Apprendimento\nCompletando questi “lab”, speriamo che gli studenti:",
    "crumbs": [
      "LABORATORI",
      "Panoramica"
    ]
  },
  {
    "objectID": "contents/labs/overview.it.html#obiettivi-dellapprendimento",
    "href": "contents/labs/overview.it.html#obiettivi-dellapprendimento",
    "title": "Panoramica",
    "section": "",
    "text": "Consiglio\n\n\n\n\nAcquisiscano competenza nell’impostazione e distribuzione di modelli ML su dispositivi supportati, consentendo di affrontare scenari di distribuzione ML nel mondo reale con sicurezza.\nComprendano i passaggi coinvolti nell’adattamento e nella sperimentazione di modelli ML per diverse applicazioni, consentendo di ottimizzare prestazioni ed efficienza.\nApprendano tecniche di risoluzione dei problemi specifiche per le distribuzioni ML embedded, dotandoli delle competenze per superare insidie e sfide comuni.\nAcquisiscano esperienza pratica nella distribuzione di modelli TinyML su dispositivi embedded, colmando il divario tra teoria e pratica.\nEsplorino varie modalità di sensori e le loro applicazioni, ampliando la comprensione di come ML può essere sfruttato in diversi domini.\nFavoriscano una comprensione delle implicazioni e delle sfide del mondo reale associate alle distribuzioni di sistemi ML, preparandoli per progetti futuri.",
    "crumbs": [
      "LABORATORI",
      "Panoramica"
    ]
  },
  {
    "objectID": "contents/labs/overview.it.html#pubblico-target",
    "href": "contents/labs/overview.it.html#pubblico-target",
    "title": "Panoramica",
    "section": "Pubblico Target",
    "text": "Pubblico Target\nQuesti lab sono progettati per:\n\nPrincipianti nel campo dell’apprendimento automatico che hanno un vivo interesse nell’esplorare l’intersezione tra ML e sistemi embedded.\nSviluppatori e ingegneri che desiderano applicare modelli ML ad applicazioni del mondo reale utilizzando dispositivi a basso consumo e risorse limitate.\nAppassionati e ricercatori che desiderano acquisire esperienza pratica nell’implementazione dell’IA su dispositivi edge e comprendere le sfide uniche coinvolte.",
    "crumbs": [
      "LABORATORI",
      "Panoramica"
    ]
  },
  {
    "objectID": "contents/labs/overview.it.html#dispositivi-supportati",
    "href": "contents/labs/overview.it.html#dispositivi-supportati",
    "title": "Panoramica",
    "section": "Dispositivi Supportati",
    "text": "Dispositivi Supportati\nAbbiamo incluso materiali di laboratorio per tre dispositivi chiave che rappresentano diversi profili hardware e capacità.\n\nNicla Vision: Ottimizzato per applicazioni basate sulla visione come la classificazione delle immagini e il rilevamento di oggetti, ideale per casi d’uso compatti e a basso consumo.\nXIAO ESP32S3: Una scheda versatile e compatta adatta per attività di individuazione di parole chiave e rilevamento del movimento.\nRaspberry Pi: Una piattaforma flessibile per attività più intensive dal punto di vista computazionale, inclusi piccoli modelli linguistici e varie applicazioni di classificazione e rilevamento.\n\n\n\n\nEsercizio\nNicla Vision\nXIAO ESP32S3\nRaspberry Pi\n\n\n\n\nInstallation & Setup\n✓\n✓\n✓\n\n\nKeyword Spotting (KWS)\n✓\n✓\n\n\n\nImage Classification\n✓\n✓\n✓\n\n\nObject Detection\n✓\n✓\n✓\n\n\nMotion Detection\n✓\n✓\n\n\n\nSmall Language Models (SLM)\n\n\n✓",
    "crumbs": [
      "LABORATORI",
      "Panoramica"
    ]
  },
  {
    "objectID": "contents/labs/overview.it.html#struttura-del-lab",
    "href": "contents/labs/overview.it.html#struttura-del-lab",
    "title": "Panoramica",
    "section": "Struttura del Lab",
    "text": "Struttura del Lab\nOgni lab segue un approccio strutturato:\n\nIntroduzione: Esplora l’applicazione e la sua importanza in scenari reali.\nConfigurazione: Istruzioni dettagliate per configurare l’ambiente hardware e software.\nDistribuzione: Guida al training e alla distribuzione dei modelli ML pre-addestrati sui dispositivi supportati.\nEsercizi: Attività pratiche per modificare e sperimentare con i parametri del modello.\nDiscussione: Analisi dei risultati, potenziali miglioramenti e approfondimenti pratici.",
    "crumbs": [
      "LABORATORI",
      "Panoramica"
    ]
  },
  {
    "objectID": "contents/labs/overview.it.html#sequenza-dei-laboratori-consigliata",
    "href": "contents/labs/overview.it.html#sequenza-dei-laboratori-consigliata",
    "title": "Panoramica",
    "section": "Sequenza dei Laboratori Consigliata",
    "text": "Sequenza dei Laboratori Consigliata\nSe si è alle prime armi con l’ML embedded, consigliamo di iniziare con la configurazione e l’individuazione delle parole chiave prima di passare alla classificazione delle immagini e al rilevamento degli oggetti. Gli utenti di Raspberry Pi possono esplorare attività più avanzate, come piccoli modelli linguistici, dopo aver familiarizzato con le basi.",
    "crumbs": [
      "LABORATORI",
      "Panoramica"
    ]
  },
  {
    "objectID": "contents/labs/overview.it.html#risoluzione-dei-problemi-e-supporto",
    "href": "contents/labs/overview.it.html#risoluzione-dei-problemi-e-supporto",
    "title": "Panoramica",
    "section": "Risoluzione dei Problemi e Supporto",
    "text": "Risoluzione dei Problemi e Supporto\nSe si riscontrano problemi durante i laboratori, consultare i commenti sui “troubleshooting” [risoluzione dei problemi] o controllare le FAQ all’interno di ogni laboratorio. Per ulteriore assistenza, non esitate a contattare il nostro team di supporto o a interagire con i forum della community.",
    "crumbs": [
      "LABORATORI",
      "Panoramica"
    ]
  },
  {
    "objectID": "contents/labs/overview.it.html#crediti",
    "href": "contents/labs/overview.it.html#crediti",
    "title": "Panoramica",
    "section": "Crediti",
    "text": "Crediti\nUn ringraziamento speciale e un ringraziamento al Prof. Marcelo Rovai per il suo prezioso contributo allo sviluppo e al continuo perfezionamento di questi laboratori.",
    "crumbs": [
      "LABORATORI",
      "Panoramica"
    ]
  },
  {
    "objectID": "contents/labs/getting_started.it.html",
    "href": "contents/labs/getting_started.it.html",
    "title": "Guida Introduttiva",
    "section": "",
    "text": "Requisiti Hardware\nPer seguire i laboratori pratici, ci sarà bisogno del seguente hardware:\nArduino Nicla Vision è progettata su misura per applicazioni di livello professionale, offrendo funzionalità avanzate e prestazioni adatte a progetti industriali impegnativi. D’altro canto, Seeed Studio XIAO ESP32S3 Sense è rivolta a maker, hobbisti e studenti che desiderano esplorare applicazioni IA edge in un formato più accessibile e adatto ai principianti. Entrambe le schede hanno i loro punti di forza e il loro pubblico di riferimento, consentendo agli utenti di scegliere la soluzione migliore per le loro esigenze e il loro livello di competenza. Raspberry Pi è destinato a progetti di ingegneria e apprendimento automatico più avanzati.",
    "crumbs": [
      "LABORATORI",
      "Guida Introduttiva"
    ]
  },
  {
    "objectID": "contents/labs/getting_started.it.html#requisiti-hardware",
    "href": "contents/labs/getting_started.it.html#requisiti-hardware",
    "title": "Guida Introduttiva",
    "section": "",
    "text": "Scheda Arduino Nicla Vision\n\nArduino Nicla Vision è una scheda potente e compatta progettata per applicazioni audio e di visione artificiale di livello professionale. È dotata di un modulo telecamera di alta qualità, un microfono digitale e un’IMU, che la rendono adatta a progetti impegnativi in settori quali robotica, automazione e sorveglianza.\nSpecifiche di Arduino Nicla Vision\nSchema e pinout di Arduino Nicla Vision\n\nScheda XIAO ESP32S3 Sense\n\nLa scheda Seeed Studio XIAO ESP32S3 Sense è una scheda minuscola e ricca di funzionalità, progettata per maker, hobbisti e studenti interessati a esplorare applicazioni IA edge. È dotata di fotocamera, microfono e IMU, rendendo facile iniziare con progetti come classificazione delle immagini, individuazione di parole chiave e rilevamento del movimento.\nSpecifiche di XIAO ESP32S3 Sense\nSchema e pinout di XIAO ESP32S3 Sense\n\nRaspberry Pi Single Computer board\n\n\nIl Raspberry Pi è un potente e versatile computer a scheda singola che è diventato uno strumento essenziale per gli ingegneri di varie discipline. Sviluppati dalla Raspberry Pi Foundation, questi dispositivi compatti offrono una combinazione unica di convenienza, potenza di calcolo e ampie capacità GPIO (General Purpose Input/Output), rendendoli ideali per la prototipazione, lo sviluppo di sistemi embedded e progetti di ingegneria avanzata.\nDocumentazione Hardware di Raspberry Pi\nDocumentazione della Telecamera\n\n\nAccessori aggiuntivi\n\nCavo USB-C per la programmazione e l’alimentazione di XIAO\nCavo micro-USB per la programmazione e l’alimentazione di Nicla\nAlimentatore per Raspberry\nBreadboard e cavi jumper (opzionali, per collegare sensori aggiuntivi)",
    "crumbs": [
      "LABORATORI",
      "Guida Introduttiva"
    ]
  },
  {
    "objectID": "contents/labs/getting_started.it.html#requisiti-software",
    "href": "contents/labs/getting_started.it.html#requisiti-software",
    "title": "Guida Introduttiva",
    "section": "Requisiti Software",
    "text": "Requisiti Software\nPer programmare le schede e sviluppare progetti di apprendimento automatico embedded, ci sarà bisogno del seguente software:\n\nArduino IDE\n\nDownload e installazione\n\nInstall di Arduino IDE\nSeguire la guida all’installazione per il sistema operativo in uso.\nArduino CLI\nConfigurare l’IDE Arduino per le schede Arduino Nicla Vision e XIAO ESP32S3 Sense.\n\n\nOpenMV IDE (opzionale)\n\nScaricare e installare OpenMV IDE per il sistema operativo in uso.\nConfigurare l’IDE OpenMV per Arduino Nicla Vision.\n\nEdge Impulse Studio\n\nRegistrarsi per un account gratuito su Edge Impulse Studio.\nInstallare Edge Impulse CLI\nSeguire le guide per connettere le schede Arduino Nicla Vision e XIAO ESP32S3 Sense a Edge Impulse Studio.\n\nRaspberry Pi OS\n\n\nScaricare e installare Raspberry Pi Imager",
    "crumbs": [
      "LABORATORI",
      "Guida Introduttiva"
    ]
  },
  {
    "objectID": "contents/labs/getting_started.it.html#connettività-di-rete",
    "href": "contents/labs/getting_started.it.html#connettività-di-rete",
    "title": "Guida Introduttiva",
    "section": "Connettività di Rete",
    "text": "Connettività di Rete\nAlcuni progetti potrebbero richiedere la connettività Internet per la raccolta dati o la distribuzione del modello. Assicurarsi che la connessione dell’ambiente di sviluppo sia stabile tramite Wi-Fi o Ethernet. Per Raspberry Pi, è necessaria una connessione Wi-Fi o Ethernet per il funzionamento remoto senza la necessità di collegare un monitor, una tastiera e un mouse.\n\nPer Arduino Nicla Vision, si può utilizzare il modulo Wi-Fi integrato per connetterti a una rete wireless.\nPer XIAO ESP32S3 Sense, si può utilizzare il modulo Wi-Fi integrato o collegare un modulo Wi-Fi o Ethernet esterno utilizzando i pin disponibili.\nPer Raspberry Pi, si può utilizzare il modulo Wi-Fi integrato per collegare un modulo Wi-Fi o Ethernet esterno utilizzando il connettore disponibile.",
    "crumbs": [
      "LABORATORI",
      "Guida Introduttiva"
    ]
  },
  {
    "objectID": "contents/labs/getting_started.it.html#conclusione",
    "href": "contents/labs/getting_started.it.html#conclusione",
    "title": "Guida Introduttiva",
    "section": "Conclusione",
    "text": "Conclusione\nCon l’hardware e il software impostati, siamo pronti per intraprendere il viaggio di apprendimento automatico embedded. I laboratori pratici guideranno attraverso vari progetti, coprendo argomenti come classificazione delle immagini, rilevamento di oggetti, individuazione di parole chiave e classificazione del movimento.\nSe si riscontrano problemi o ci sono domande, non esitate a consultare le guide alla risoluzione dei problemi o i forum o a cercare supporto dalla community.\nTuffiamoci e sblocchiamo il potenziale dell’apprendimento automatico su (tiny) sistemi reali!",
    "crumbs": [
      "LABORATORI",
      "Guida Introduttiva"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/setup/setup.it.html",
    "href": "contents/labs/arduino/nicla_vision/setup/setup.it.html",
    "title": "Setup",
    "section": "",
    "text": "Panoramica\nLa Arduino Nicla Vision (a volte chiamata NiclaV) è una scheda di sviluppo che include due processori in grado di eseguire attività in parallelo. Fa parte di una famiglia di schede di sviluppo con lo stesso fattore di forma ma progettate per attività specifiche, come Nicla Sense ME e la Nicla Voice. Le Nicla possono eseguire in modo efficiente processi creati con TensorFlow Lite. Ad esempio, uno dei core di NiclaV esegue un algoritmo di visione artificiale al volo (inferenza), mentre l’altro esegue operazioni di basso livello come il controllo di un motore e la comunicazione o l’azione come interfaccia utente. Il modulo wireless integrato consente la gestione simultanea della connettività WiFi e Bluetooth Low Energy (BLE).",
    "crumbs": [
      "Nicla Vision",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/setup/setup.it.html#hardware",
    "href": "contents/labs/arduino/nicla_vision/setup/setup.it.html#hardware",
    "title": "Setup",
    "section": "Hardware",
    "text": "Hardware\n\nDue Core Paralleli\nIl processore centrale è il dual-core STM32H747, che include un Cortex M7 at 480 MHz e un Cortex M4 at 240 MHz. I due core comunicano tramite un meccanismo di Remote Procedure Call che consente di richiamare senza problemi le funzioni sull’altro processore. Entrambi i processori condividono tutte le periferiche on-chip e possono eseguire:\n\nSketch Arduino su Arm Mbed OS\nApplicazioni Native Mbed\nMicroPython / JavaScript tramite un interprete\nTensorFlow Lite\n\n\n\n\nMemoria\nLa memoria è fondamentale per i progetti di machine learning [apprendimento automatico] embedded. La scheda NiclaV può ospitare fino a 16 MB di QSPI Flash per l’archiviazione. Tuttavia, è essenziale considerare che la SRAM MCU è quella da utilizzare con le inferenze di machine learning; l’STM32H747 è di soli 1 MB, condiviso da entrambi i processori. Questa MCU ha anche incluso 2 MB di FLASH, principalmente per l’archiviazione del codice.\n\n\nSensori\n\nFotocamera: Una fotocamera CMOS a colori GC2145 da 2 MP.\nMicrofono: I’MP34DT05 è un microfono digitale MEMS omnidirezionale, ultracompatto, a basso consumo, costruito con un elemento di rilevamento capacitivo e l’interfaccia IC.\nIMU a 6 Assi: Dati del giroscopio 3D e dell’accelerometro 3D dall’IMU a 6 assi LSM6DSOX.\nSensore del Time of Flight: Il sensore del tempo di volo VL53L1CBV0FY aggiunge capacità di misurazione precise e a bassa potenza alla Nicla Vision. Il laser invisibile VCSEL vicino all’infrarosso (incluso il driver analogico) è incapsulato con ottica ricevente in un piccolo modulo, tutto in uno, sotto la telecamera.",
    "crumbs": [
      "Nicla Vision",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/setup/setup.it.html#installazione-di-arduino-ide",
    "href": "contents/labs/arduino/nicla_vision/setup/setup.it.html#installazione-di-arduino-ide",
    "title": "Setup",
    "section": "Installazione di Arduino IDE",
    "text": "Installazione di Arduino IDE\nSi inizia collegando la scheda (microUSB) al computer:\n\nSi installa il core Mbed OS per le schede Nicla nell’IDE Arduino. Con l’IDE aperto, si va su Tools &gt; Board &gt; Board Manager, si cerca Arduino Nicla Vision nella finestra di ricerca e si installa la scheda.\n\nPoi, si va su Tools &gt; Board &gt; Arduino Mbed OS Nicla Boards e si seleziona Arduino Nicla Vision. Con la scheda collegata alla porta USB, si dovrebbe vedere “Nicla on Port” e selezionarla.\n\nSi apre lo sketch Blink su Examples/Basic ed lo si esegue usando il pulsante “IDE Upload”. Si dovrebbe vedere il LED integrato (RGB verde) lampeggiare, il che significa che la scheda Nicla è installata correttamente e funzionante!\n\n\nTest del Microfono\nSu Arduino IDE, si va su Examples &gt; PDM &gt; PDMSerialPlotter, si apre e si esegue lo sketch. Si apre il Plotter e si guarda la rappresentazione audio dal microfono:\n\n\nVariare la frequenza del suono generato e verificare che il microfono funzioni correttamente.\n\n\n\nTest dell’IMU\nPrima di testare l’IMU, sarà necessario installare la libreria LSM6DSOX. Per farlo, si vai su Library Manager e si cerca LSM6DSOX. Si installa la libreria fornita da Arduino:\n\nPoi, si va su Examples &gt; Arduino_LSM6DSOX &gt; SimpleAccelerometer e si esegue il test dell’accelerometro (si può anche eseguire Gyro e temperatura della scheda):\n\n\n\nTest del sensore ToF (Time of Flight) [tempo di volo]\nCome fatto con l’IMU, è necessario installare la libreria ToF VL53L1X. Per farlo, si va su Library Manager e si cerca VL53L1X. Si installa la libreria fornita da Pololu:\n\nPoi, si esegue lo sketch proximity_detection.ino:\n\nSul monitor seriale, si vedrà la distanza dalla telecamera di un oggetto di fronte ad essa (max 4 m).\n\n\n\nTest della Fotocamera\nPossiamo anche testare la fotocamera utilizzando, ad esempio, il codice fornito in Examples &gt; Camera &gt; CameraCaptureRawBytes. Non possiamo vedere l’immagine direttamente, ma è possibile ottenere i dati “crudi” dell’immagine generati dalla telecamera.\nIn ogni caso, il test migliore con la telecamera è vedere un’immagine dal vivo. Per questo, useremo un altro IDE, OpenMV.",
    "crumbs": [
      "Nicla Vision",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/setup/setup.it.html#installazione-di-openmv-ide",
    "href": "contents/labs/arduino/nicla_vision/setup/setup.it.html#installazione-di-openmv-ide",
    "title": "Setup",
    "section": "Installazione di OpenMV IDE",
    "text": "Installazione di OpenMV IDE\nOpenMV IDE è il principale ambiente di sviluppo integrato con le telecamere OpenMV come quella su Nicla Vision. È dotato di un potente editor di testo, terminale di debug e visualizzatore di frame buffer con visualizzazione di istogrammi. Utilizzeremo MicroPython per programmare la telecamera.\nSi va alla pagina di OpenMV IDE, si scarica la versione corretta per il proprio sistema operativo e si seguono le istruzioni per l’installazione sul computer.\n\nL’IDE dovrebbe aprirsi, con il codice helloworld_1.py predefinito nella sua “Code Area”. In caso contrario, lo si può aprire da Files &gt; Examples &gt; HelloWord &gt; helloword.py\n\nTutti i messaggi inviati tramite una connessione seriale (utilizzando print() o i messaggi di errore) verranno visualizzati sul Serial Terminal durante l’esecuzione. L’immagine catturata da una telecamera verrà visualizzata nell’area Camera Viewer (o Frame Buffer) e nell’area Histogram immediatamente sotto Camera Viewer.\n\nPrima di collegare la Nicla all’IDE OpenMV, si deve avere la versione più recente del bootloader. Si va all’IDE Arduino, si seleziona la scheda Nicla e si apre lo sketch su Examples &gt; STM_32H747_System STM32H747_manageBootloader. Upload-are il codice sulla board. Il Serial Monitor vi guiderà.\n\nDopo aver aggiornato il bootloader, si mette la Nicla Vision in modalità bootloader premendo due volte il pulsante di reset sulla scheda. Il LED verde integrato inizierà a spegnersi e accendersi in dissolvenza [fading]. Ora si torna all’IDE OpenMV e si clicca sull’icona di connessione (Left ToolBar):\n\nUn pop-up informerà che è stata rilevata una scheda in modalità DFU e chiederà come si desidera procedere. Per prima cosa, si seleziona Install the latest release firmware (vX.Y.Z). Questa azione installerà l’ultimo firmware OpenMV su Nicla Vision.\n\nSi può lasciare l’opzione Erase internal file system deselezionata e cliccare su [OK].\nIl LED verde di Nicla inizierà a lampeggiare mentre il firmware OpenMV viene caricato sulla scheda e si aprirà una finestra del terminale che mostrerà l’avanzamento del flashing.\n\nAttendere che il LED verde smetta di lampeggiare in dissolvenza. Quando il processo termina, si vedrà un messaggio che dice “DFU firmware update complete!”. Premere [OK].\n\nQuando Nicla Vison si collega alla Barra degli Strumenti, compare un pulsante verde per la riproduzione.\n\nNotare che sul computer apparirà un’unità denominata “NO NAME”:\n\nOgni volta che si preme il pulsante [RESET] sulla board, viene automaticamente eseguito lo script main.py che vi è memorizzato. Si può caricare il codice di main.py sull’IDE (File &gt; Open File...).\n\n\nQuesto è il codice di “Blink”, che conferma che l’HW è OK.\n\nPer testare la telecamera, eseguiamo helloword_1.py. Per farlo, si seleziona lo script su File &gt; Examples &gt; HelloWorld &gt; helloword.py,\nQuando si clicca sul pulsante di riproduzione verde, lo script MicroPython (hellowolrd.py) nella “Code Area” verrà caricato ed eseguito su Nicla Vision. Sul “Camera Viewer”, si vedrà lo streaming video. Il “Serial Monitor” ci mostrerà gli FPS (fotogrammi al secondo), che dovrebbero essere circa 14 fps.\n\nEcco lo script helloworld.py:\n# Hello World Example 2\n#\n# Welcome to the OpenMV IDE! Click on the green run arrow button below to run the script!\n\nimport sensor, image, time\n\nsensor.reset()                      # Reset and initialize the sensor.\nsensor.set_pixformat(sensor.RGB565) # Set pixel format to RGB565 (or GRAYSCALE)\nsensor.set_framesize(sensor.QVGA)   # Set frame size to QVGA (320x240)\nsensor.skip_frames(time = 2000)     # Wait for settings take effect.\nclock = time.clock()                # Create a clock object to track the FPS.\n\nwhile(True):\n    clock.tick()                    # Update the FPS clock.\n    img = sensor.snapshot()         # Take a picture and return the image.\n    print(clock.fps())\nIn GitHub, si trovano gli script Python utilizzati qui.\nIl codice può essere suddiviso in due parti:\n\nSetup: Dove le librerie vengono importate, inizializzate e le variabili vengono definite e inizializzate.\nLoop: (ciclo while) parte del codice che viene eseguita continuamente. L’immagine (la variabile img) viene catturata (un frame). Ognuno di questi frame può essere utilizzato per l’inferenza nelle Applicazioni di Apprendimento Automatico.\n\nPer interrompere l’esecuzione del programma, premere il pulsante rosso [X].\n\nNota: OpenMV Cam funziona a circa la metà della velocità quando è connesso all’IDE. Gli FPS dovrebbero aumentare una volta disconnessi.\n\nIn GitHub, si trovano altri script Python. Provare a testare i sensori di bordo.",
    "crumbs": [
      "Nicla Vision",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/setup/setup.it.html#collegamento-di-nicla-vision-a-edge-impulse-studio",
    "href": "contents/labs/arduino/nicla_vision/setup/setup.it.html#collegamento-di-nicla-vision-a-edge-impulse-studio",
    "title": "Setup",
    "section": "Collegamento di Nicla Vision a Edge Impulse Studio",
    "text": "Collegamento di Nicla Vision a Edge Impulse Studio\nAvremo bisogno di Edge Impulse Studio più avanti in altri esercizi. Edge Impulse è una piattaforma di sviluppo leader per il machine learning [apprendimento automatico] su dispositivi edge.\nEdge Impulse supporta ufficialmente Nicla Vision. Quindi, per iniziare, si crea un nuovo progetto su Studio e vi si collega Nicla. Per farlo, si seguono i passaggi:\n\nScaricare l’EI Firmware più aggiornato e decomprimerlo.\nAprire il file zip sul computer e selezionare l’uploader corrispondente al sistema operativo:\n\n\n\nMettere Nicla-Vision in “Boot Mode”, premendo due volte il pulsante di reset.\nEseguire il codice batch specifico per il proprio sistema operativo per caricare il binario arduino-nicla-vision.bin sulla board.\n\nAndare sul progetto sullo Studio e nella scheda Data Acquisition tab, selezionare WebUSB (1). Si aprirà una finestra; scegliere l’opzione che mostra che Nicla is paired (2) e premere [Connect] (3).\n\nNella sezione Collect Data della scheda Data Acquisition, si possono scegliere quali dati del sensore raccogliere.\n\nPer esempio. IMU data:\n\nO Image (Camera):\n\nE così via. Si può anche testare un sensore esterno collegato all’ADC (Nicla pin 0) e agli altri sensori a bordo, come il microfono e il ToF.",
    "crumbs": [
      "Nicla Vision",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/setup/setup.it.html#espansione-della-nicla-vision-board-opzionale",
    "href": "contents/labs/arduino/nicla_vision/setup/setup.it.html#espansione-della-nicla-vision-board-opzionale",
    "title": "Setup",
    "section": "Espansione della Nicla Vision Board (opzionale)",
    "text": "Espansione della Nicla Vision Board (opzionale)\nUn ultimo elemento da esplorare è che a volte, durante la prototipazione, è essenziale sperimentare con sensori e dispositivi esterni, e un’eccellente espansione per Nicla è l’Arduino MKR Connector Carrier (Grove compatible).\nLo shield ha 14 connettori Grove: cinque ingressi analogici singoli (A0-A5), un ingresso analogico doppio (A5/A6), cinque I/O digitali singoli (D0-D4), un I/O digitale doppio (D5/D6), una I2C (TWI) e una UART (Seriale). Tutti i connettori sono compatibili con 5 V.\n\nNotare che tutti i 17 pin Nicla Vision saranno collegati agli Shield Grove, ma alcune connessioni Grove rimangono scollegate.\n\n\nQuesto shield è compatibile con MKR e può essere utilizzato con Nicla Vision e Portenta.\n\nAd esempio, supponiamo che in un progetto TinyML si desideri inviare risultati di inferenza utilizzando un dispositivo LoRaWAN e aggiungere informazioni sulla luminosità locale. Spesso, con operazioni offline, si consiglia un display locale a basso consumo come un OLED. Questa configurazione può essere visualizzata qui:\n\nIl Grove Light Sensor è collegato a uno dei singoli pin analogici (A0/PC4), il LoRaWAN device alla UART e l’OLED al connettore della I2C.\nI pin Nicla 3 (Tx) e 4 (Rx) sono collegati al connettore Serial Shield. La comunicazione UART è utilizzata con il dispositivo LoRaWan. Ecco un semplice codice per utilizzare l’UART:\n# UART Test - By: marcelo_rovai - Sat Sep 23 2023\n\nimport time\nfrom pyb import UART\nfrom pyb import LED\n\nredLED = LED(1) # built-in red LED\n\n# Init UART object.\n# Nicla Vision's UART (TX/RX pins) is on \"LP1\"\nuart = UART(\"LP1\", 9600)\n\nwhile(True):\n    uart.write(\"Hello World!\\r\\n\")\n    redLED.toggle()\n    time.sleep_ms(1000)\nPer verificare che l’UART funzioni, si deve, ad esempio, collegare un altro dispositivo come Arduino UNO, visualizzando “Hello Word” sul Serial Monitor. Ecco il codice.\n\nDi seguito è riportato il codice Hello World da utilizzare con l’OLED I2C. Il driver MicroPython SSD1306 OLED (ssd1306.py), creato da Adafruit, dovrebbe essere caricato anche su Nicla (lo script ssd1306.py si trova su GitHub).\n# Nicla_OLED_Hello_World - By: marcelo_rovai - Sat Sep 30 2023\n\n#Save on device: MicroPython SSD1306 OLED driver, I2C and SPI interfaces created by Adafruit\nimport ssd1306\n\nfrom machine import I2C\ni2c = I2C(1)\n\noled_width = 128\noled_height = 64\noled = ssd1306.SSD1306_I2C(oled_width, oled_height, i2c)\n\noled.text('Hello, World', 10, 10)\noled.show()\nInfine, ecco uno script semplice per leggere il valore ADC sul pin “PC4” (pin A0 di Nicla):\n# Light Sensor (A0) - By: marcelo_rovai - Wed Oct 4 2023\n\nimport pyb\nfrom time import sleep\n\nadc = pyb.ADC(pyb.Pin(\"PC4\"))     # create an analog object from a pin\nval = adc.read()                  # read an analog value\n\nwhile (True):\n\n    val = adc.read()  \n    print (\"Light={}\".format (val))\n    sleep (1)\nL’ADC può essere utilizzato per altre variabili del sensore, come la Temperatura.\n\nNotare che gli script sopra (scaricati da Github) presentano solo come collegare dispositivi esterni alla scheda Nicla Vision utilizzando MicroPython.",
    "crumbs": [
      "Nicla Vision",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/setup/setup.it.html#conclusione",
    "href": "contents/labs/arduino/nicla_vision/setup/setup.it.html#conclusione",
    "title": "Setup",
    "section": "Conclusione",
    "text": "Conclusione\nArduino Nicla Vision è un eccellente piccolo dispositivo per usi industriali e professionali! Tuttavia, è potente, affidabile, a basso consumo e ha sensori adatti per le applicazioni di machine learning embedded più comuni come visione, movimento, “fusion” di sensori e suono.\n\nNel repository GitHub, si trova l’ultima versione di tutti i codici utilizzati o commentati in questo esercizio pratico.",
    "crumbs": [
      "Nicla Vision",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/setup/setup.it.html#risorse",
    "href": "contents/labs/arduino/nicla_vision/setup/setup.it.html#risorse",
    "title": "Setup",
    "section": "Risorse",
    "text": "Risorse\n\nCodici Micropython\nCodici Arduino",
    "crumbs": [
      "Nicla Vision",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/image_classification/image_classification.it.html",
    "href": "contents/labs/arduino/nicla_vision/image_classification/image_classification.it.html",
    "title": "Classificazione delle Immagini",
    "section": "",
    "text": "Panoramica\nMentre iniziamo i nostri studi sul machine learning embedded o TinyML, è impossibile ignorare l’impatto trasformativo della Computer Vision (CV) e dell’Intelligenza Artificiale (IA) nelle nostre vite. Queste due discipline interconnesse ridefiniscono ciò che le macchine possono percepire e realizzare, dai veicoli autonomi e dalla robotica all’assistenza sanitaria e alla sorveglianza.\nSempre di più, ci troviamo di fronte a una rivoluzione dell’intelligenza artificiale (IA) in cui, come affermato da Gartner, Edge AI ha un potenziale di impatto molto elevato, ed è ora!\nNel “centro” del Radar c’è la Edge Computer Vision, e quando parliamo di Machine Learning (ML) applicato alla visione, la prima cosa che viene in mente è la Classificazione delle immagini, una specie di “Hello World” di ML!\nQuesto esercizio esplorerà un progetto di computer vision che utilizza Convolutional Neural Network (CNN) [Reti Neurali Convoluzionali] per la classificazione delle immagini in tempo reale. Sfruttando il robusto ecosistema di TensorFlow, implementeremo un modello MobileNet pre-addestrato e lo adatteremo per il “deployment” edge. L’attenzione sarà rivolta all’ottimizzazione del modello per un’esecuzione efficiente su hardware con risorse limitate senza sacrificare l’accuratezza.\nUtilizzeremo tecniche come la quantizzazione e la potatura per ridurre il carico computazionale. Alla fine di questo tutorial, si avrà un prototipo funzionante in grado di classificare le immagini in tempo reale, il tutto in esecuzione su un sistema embedded a basso consumo basato sulla scheda Arduino Nicla Vision.",
    "crumbs": [
      "Nicla Vision",
      "Classificazione delle Immagini"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/image_classification/image_classification.it.html#visione-artificiale",
    "href": "contents/labs/arduino/nicla_vision/image_classification/image_classification.it.html#visione-artificiale",
    "title": "Classificazione delle Immagini",
    "section": "Visione Artificiale",
    "text": "Visione Artificiale\nIn sostanza, la visione artificiale consente alle macchine di interpretare e prendere decisioni sulla base di dati visivi provenienti dal mondo esterno, imitando sostanzialmente la capacità del sistema ottico umano. Al contrario, l’intelligenza artificiale è un campo più ampio che comprende il machine learning [apprendimento automatico], elaborazione del linguaggio naturale e robotica, tra le altre tecnologie. Quando si introducono algoritmi di IA nei progetti di visione artificiale, si potenzia la capacità del sistema di comprendere, interpretare e reagire agli stimoli visivi.\nQuando si parla di progetti di visione artificiale applicati a dispositivi embedded, le applicazioni più comuni che vengono in mente sono Classificazione delle Immagini e Rilevamento degli Oggetti.\n\nEntrambi i modelli possono essere implementati su dispositivi minuscoli come Arduino Nicla Vision e utilizzati in progetti reali. In questo capitolo, parleremo della classificazione delle immagini.",
    "crumbs": [
      "Nicla Vision",
      "Classificazione delle Immagini"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/image_classification/image_classification.it.html#obiettivo-del-progetto-di-classificazione-delle-immagini",
    "href": "contents/labs/arduino/nicla_vision/image_classification/image_classification.it.html#obiettivo-del-progetto-di-classificazione-delle-immagini",
    "title": "Classificazione delle Immagini",
    "section": "Obiettivo del Progetto di Classificazione delle Immagini",
    "text": "Obiettivo del Progetto di Classificazione delle Immagini\nIl primo passo in qualsiasi progetto ML è definire l’obiettivo. In questo caso, è rilevare e classificare due oggetti specifici presenti in un’immagine. Per questo progetto, utilizzeremo due piccoli giocattoli: un robot e un piccolo pappagallo brasiliano (chiamato Periquito). Inoltre, raccoglieremo immagini di un background in cui quei due oggetti sono assenti.",
    "crumbs": [
      "Nicla Vision",
      "Classificazione delle Immagini"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/image_classification/image_classification.it.html#raccolta-dati",
    "href": "contents/labs/arduino/nicla_vision/image_classification/image_classification.it.html#raccolta-dati",
    "title": "Classificazione delle Immagini",
    "section": "Raccolta Dati",
    "text": "Raccolta Dati\nUna volta definito l’obiettivo del progetto di Machine Learning, il passaggio successivo e più cruciale è la raccolta del set di dati. Si può utilizzare Edge Impulse Studio, l’IDE OpenMV che abbiamo installato o persino il proprop telefono per l’acquisizione delle immagini. Qui, utilizzeremo l’IDE OpenMV per questo.\n\nRaccolta del Dataset con OpenMV IDE\nPer prima cosa, si crea sul computer una cartella in cui verranno salvati i dati, ad esempio “data”. Quindi, su OpenMV IDE, si va in Tools &gt; Dataset Editor e si seleziona New Dataset per avviare la raccolta di dati:\n\nL’IDE chiederà di aprire il file in cui verranno salvati i dati e di scegliere la cartella “data” che è stata creata. Notare che appariranno nuove icone sul pannello di sinistra.\n\nUtilizzando l’icona in alto (1), si inserisce il nome della prima classe, ad esempio “periquito”:\n\nEseguendo dataset_capture_script.py e cliccando sull’icona della fotocamera (2), inizierà l’acquisizione delle immagini:\n\nRipetere la stessa procedura con le altre classi\n\n\nSuggeriamo circa 60 immagini da ogni categoria. Provare ad acquisire con diverse angolazioni, sfondi e condizioni di luce.\n\nLe immagini archiviate utilizzano una dimensione del fotogramma QVGA di 320x240 e RGB565 (formato pixel colore).\nDopo aver acquisito il dataset, si chiude il Tool Dataset Editor su Tools &gt; Dataset Editor.\nSul computer, si finirà con un set di dati che contiene tre classi: periquito, robot e background.\n\nSi deve tornare a Edge Impulse Studio e caricare il set di dati nel progetto.",
    "crumbs": [
      "Nicla Vision",
      "Classificazione delle Immagini"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/image_classification/image_classification.it.html#addestramento-del-modello-con-edge-impulse-studio",
    "href": "contents/labs/arduino/nicla_vision/image_classification/image_classification.it.html#addestramento-del-modello-con-edge-impulse-studio",
    "title": "Classificazione delle Immagini",
    "section": "Addestramento del modello con Edge Impulse Studio",
    "text": "Addestramento del modello con Edge Impulse Studio\nUseremo Edge Impulse Studio per addestrare il nostro modello. Inserire le credenziali del proprio account e creare un nuovo progetto:\n\n\nQui si può clonare un progetto simile: NICLA-Vision_Image_Classification.",
    "crumbs": [
      "Nicla Vision",
      "Classificazione delle Immagini"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/image_classification/image_classification.it.html#il-dataset",
    "href": "contents/labs/arduino/nicla_vision/image_classification/image_classification.it.html#il-dataset",
    "title": "Classificazione delle Immagini",
    "section": "Il Dataset",
    "text": "Il Dataset\nUtilizzando EI Studio (o Studio), esamineremo quattro passaggi principali per avere il nostro modello pronto per l’uso sulla scheda Nicla Vision: Dataset, Impulse, Tests e Deploy (su Edge Device, in questo caso, la NiclaV).\n\nPer quanto riguarda il Dataset, è essenziale sottolineare che il nostro Dataset originale, acquisito con OpenMV IDE, sarà suddiviso in Training, Validation e Test. Il Test Set sarà suddiviso dall’inizio e una parte sarà riservata per essere utilizzata solo nella fase di Test dopo l’addestramento. Il Validation Set sarà utilizzato durante l’addestramento.\n\nSu Studio, nella scheda Data acquisition e nella sezione UPLOAD DATA, si caricano i file delle categorie scelte dal computer:\n\nLasciare in Studio la suddivisione del dataset originale in train and test e scegliere l’etichetta relativa a quei dati specifici:\n\nRipetere la procedura per tutte e tre le classi. Alla fine, si vedranno “dati grezzi” in Studio:\n\nStudio consente di esplorare i dati, mostrando una vista completa di tutti i dati nel progetto. Si possono cancellare, ispezionare o modificare le etichette cliccando sui singoli elementi di dati. Nel nostro caso, un progetto molto semplice, i dati sembrano OK.",
    "crumbs": [
      "Nicla Vision",
      "Classificazione delle Immagini"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/image_classification/image_classification.it.html#impulse-design",
    "href": "contents/labs/arduino/nicla_vision/image_classification/image_classification.it.html#impulse-design",
    "title": "Classificazione delle Immagini",
    "section": "Impulse Design",
    "text": "Impulse Design\nIn questa fase, dovremmo definire come:\n\nPre-elaborare i nostri dati, il che consiste nel ridimensionare le singole immagini e determinare la profondità di colore da utilizzare (RGB o scala di grigi) e\nSpecificare un modello, in questo caso, sarà il Transfer Learning (Images) per mettere a punto un modello di classificazione delle immagini MobileNet V2 pre-addestrato sui nostri dati. Questo metodo funziona bene anche con set di dati di immagini relativamente piccoli (circa 150 immagini nel nostro caso).\n\n\nTransfer Learning con MobileNet offre un approccio semplificato all’addestramento del modello, che è particolarmente utile per ambienti con risorse limitate e progetti con dati etichettati limitati. MobileNet, noto per la sua architettura leggera, è un modello pre-addestrato che ha già appreso funzionalità preziose da un ampio set di dati (ImageNet).\n\nSfruttando queste funzionalità apprese, si può addestrare un nuovo modello per il compito specifico con meno dati e risorse computazionali e tuttavia ottenere una precisione competitiva.\n\nQuesto approccio riduce significativamente i tempi di addestramento e i costi computazionali, rendendolo ideale per la prototipazione rapida e l’implementazione su dispositivi embedded in cui l’efficienza è fondamentale.\nSi va alla scheda Impulse Design e si crea l’impulse, definendo una dimensione dell’immagine di 96x96 e schiacciandola (forma quadrata, senza ritaglio). Si seleziona Image e i blocchi Transfer Learning. Si salva l’Impulse.\n\n\nPre-elaborazione delle Immagini\nTutte le immagini QVGA/RGB565 in ingresso verranno convertite in 27.640 feature (96x96x3).\n\nSi preme [Save parameters] e Generate all features:\n\n\n\nProgettazione del Modello\nNel 2007, Google ha introdotto MobileNetV1, una famiglia di reti neurali per la visione artificiale di uso generale progettate pensando ai dispositivi mobili per supportare la classificazione, il rilevamento e altro ancora. Le MobileNet sono modelli piccoli, a bassa latenza e a basso consumo, parametrizzati per soddisfare i vincoli di risorse di vari casi d’uso. Nel 2018, Google ha lanciato MobileNetV2: Inverted Residuals and Linear Bottlenecks.\nMobileNet V1 e MobileNet V2 mirano all’efficienza mobile e alle applicazioni di visione embedded, ma differiscono per complessità architettonica e prestazioni. Mentre entrambi utilizzano convoluzioni separabili in profondità per ridurre i costi computazionali, MobileNet V2 introduce Inverted Residual Blocks e Linear Bottlenecks per migliorare le prestazioni. Queste nuove funzionalità consentono a V2 di acquisire funzionalità più complesse utilizzando meno parametri, rendendolo più efficiente dal punto di vista computazionale e generalmente più accurato rispetto al suo predecessore. Inoltre, V2 impiega un’attivazione non lineare nello layer di espansione intermedio. Utilizza ancora un’attivazione lineare per il layer del bottleneck [collo di bottiglia], una scelta di progettazione che si è rivelata utile per preservare informazioni importanti attraverso la rete. MobileNet V2 offre un’architettura ottimizzata per una maggiore accuratezza ed efficienza e verrà utilizzata in questo progetto.\nSebbene l’architettura di base di MobileNet sia già minuscola e abbia una bassa latenza, molte volte, un caso d’uso o un’applicazione specifica potrebbe richiedere che il modello sia ancora più piccolo e veloce. MobileNets introduce un parametro semplice α (alfa) chiamato moltiplicatore di larghezza per costruire questi modelli più piccoli e meno costosi dal punto di vista computazionale. Il ruolo del moltiplicatore di larghezza α è quello di assottigliare una rete in modo uniforme a ogni layer.\nEdge Impulse Studio può utilizzare sia MobileNetV1 (immagini 96x96) che V2 (immagini 96x96 o 160x160), con diversi valori di α (da 0,05 a 1,0). Ad esempio, si otterrà la massima accuratezza con V2, immagini 160x160 e α=1,0. Naturalmente, c’è un compromesso. Maggiore è la precisione, più memoria (circa 1,3 MB di RAM e 2,6 MB di ROM) sarà necessaria per eseguire il modello, il che implica una maggiore latenza. L’ingombro minore sarà ottenuto all’altro estremo con MobileNetV1 e α=0,10 (circa 53,2 K di RAM e 101 K di ROM).\n\nPer questo progetto utilizzeremo MobileNetV2 96x96 0.1, con un costo di memoria stimato di 265,3 KB in RAM. Questo modello dovrebbe andare bene per Nicla Vision con 1 MB di SRAM. Nella scheda Transfer Learning, si seleziona questo modello:",
    "crumbs": [
      "Nicla Vision",
      "Classificazione delle Immagini"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/image_classification/image_classification.it.html#addestramento-del-modello",
    "href": "contents/labs/arduino/nicla_vision/image_classification/image_classification.it.html#addestramento-del-modello",
    "title": "Classificazione delle Immagini",
    "section": "Addestramento del Modello",
    "text": "Addestramento del Modello\nUn’altra tecnica preziosa da utilizzare con il Deep Learning è la Data Augmentation. Il “data augmentation” è un metodo per migliorare l’accuratezza dei modelli di apprendimento automatico mediante la creazione di dati artificiali aggiuntivi. Un sistema di Data Augmentation apporta piccole modifiche casuali ai dati di training (ad esempio capovolgendo, ritagliando o ruotando le immagini).\nGuardando “sotto il cofano”, qui si può vedere come Edge Impulse implementa una policy di Data Augmentation sui dati:\n# Implements the data augmentation policy\ndef augment_image(image, label):\n    # Flips the image randomly\n    image = tf.image.random_flip_left_right(image)\n\n    # Increase the image size, then randomly crop it down to\n    # the original dimensions\n    resize_factor = random.uniform(1, 1.2)\n    new_height = math.floor(resize_factor * INPUT_SHAPE[0])\n    new_width = math.floor(resize_factor * INPUT_SHAPE[1])\n    image = tf.image.resize_with_crop_or_pad(image, new_height, new_width)\n    image = tf.image.random_crop(image, size=INPUT_SHAPE)\n\n    # Vary the brightness of the image\n    image = tf.image.random_brightness(image, max_delta=0.2)\n\n    return image, label\nL’esposizione a queste variazioni durante l’addestramento può aiutare a impedire al modello di prendere scorciatoie “memorizzando” indizi superficiali nei dati di addestramento, il che significa che potrebbe riflettere meglio i pattern profondi in esame nel set di dati.\nL’ultimo layer del nostro modello avrà 12 neuroni con un dropout del 15% per prevenire l’overfitting. Ecco il risultato del Training:\n\nIl risultato è eccellente, con 77 ms di latenza, che dovrebbero tradursi in 13 fps (frame al secondo) durante l’inferenza.",
    "crumbs": [
      "Nicla Vision",
      "Classificazione delle Immagini"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/image_classification/image_classification.it.html#test-del-modello",
    "href": "contents/labs/arduino/nicla_vision/image_classification/image_classification.it.html#test-del-modello",
    "title": "Classificazione delle Immagini",
    "section": "Test del Modello",
    "text": "Test del Modello\n\nOra, si dovrebbe mettere da parte il set di dati all’inizio del progetto ed eseguire il modello addestrato usandolo come input:\n\nIl risultato è, ancora una volta, eccellente.",
    "crumbs": [
      "Nicla Vision",
      "Classificazione delle Immagini"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/image_classification/image_classification.it.html#distribuzione-del-modello",
    "href": "contents/labs/arduino/nicla_vision/image_classification/image_classification.it.html#distribuzione-del-modello",
    "title": "Classificazione delle Immagini",
    "section": "Distribuzione del modello",
    "text": "Distribuzione del modello\nA questo punto, possiamo distribuire il modello addestrato come .tflite e usare l’IDE OpenMV per eseguirlo usando MicroPython, oppure possiamo distribuirlo come C/C++ o libreria Arduino.\n\n\nLibreria Arduino\nPer prima cosa, distribuiamolo come Libreria Arduino:\n\nSi dovrebbe installare la libreria come .zip sull’IDE Arduino ed eseguire lo sketch nicla_vision_camera.ino disponibile in Examples sotto il nome della libreria.\n\nNotare che Arduino Nicla Vision ha, per default, 512KB di RAM allocati per il core M7 e altri 244 KB sullo spazio di indirizzamento dell’M4. Nel codice, questa allocazione è stata modificata in 288 kB per garantire che il modello verrà eseguito sul dispositivo (malloc_addblock((void*)0x30000000, 288 * 1024);).\n\nIl risultato è buono, con 86 ms di latenza misurata.\n\nEcco un breve video che mostra i risultati dell’inferenza: \n\n\nOpenMV\nÈ possibile distribuire il modello addestrato da utilizzare con OpenMV in due modi: come libreria e come firmware.\nCome libreria vengono generati tre file: il modello .tflite addestrato, un elenco con etichette e un semplice script MicroPython che può effettuare inferenze utilizzando il modello.\n\nEseguire questo modello come .tflite direttamente in Nicla era impossibile. Quindi, possiamo sacrificare l’accuratezza utilizzando un modello più piccolo o distribuire il modello come Firmware OpenMV (FW). Scegliendo FW, Edge Impulse Studio genera modelli, librerie e framework ottimizzati necessari per effettuare l’inferenza. Esploriamo questa opzione.\nSelezionare OpenMV Firmware nella scheda Deploy e premere [Build].\n\nSul computer si troverà un file ZIP. Lo si apre:\n\nSi usa il tool Bootloader sull’IDE OpenMV per caricare il FW sulla board:\n\nSi seleziona il file appropriato (.bin per Nicla-Vision):\n\nDopo aver completato il download, si preme OK:\n\nSe un messaggio dice che il FW è “outdated” [obsoleto], NON ESEGUIRE L’AGGIORNAMENTO. Selezionare [NO].\n\nOra, si apre lo script ei_image_classification.py che è stato scaricato da Studio e il file .bin per Nicla.\n\nEseguirlo. Puntando la telecamera sugli oggetti che vogliamo classificare, il risultato dell’inferenza verrà visualizzato sul Serial Terminal.\n\n\nModifica del Codice per Aggiungere Etichette\nIl codice fornito da Edge Impulse può essere modificato in modo da poter vedere, per motivi di test, il risultato dell’inferenza direttamente sull’immagine visualizzata sull’IDE OpenMV.\nCaricare il codice da GitHub o modificalo come di seguito:\n# Marcelo Rovai - NICLA Vision - Image Classification\n# Adapted from Edge Impulse - OpenMV Image Classification Example\n# @24Aug23\n\nimport sensor, image, time, os, tf, uos, gc\n\nsensor.reset()                         # Reset and initialize the sensor.\nsensor.set_pixformat(sensor.RGB565)    # Set pxl fmt to RGB565 (or GRAYSCALE)\nsensor.set_framesize(sensor.QVGA)      # Set frame size to QVGA (320x240)\nsensor.set_windowing((240, 240))       # Set 240x240 window.\nsensor.skip_frames(time=2000)          # Let the camera adjust.\n\nnet = None\nlabels = None\n\ntry:\n    # Load built in model\n    labels, net = tf.load_builtin_model('trained')\nexcept Exception as e:\n    raise Exception(e)\n\nclock = time.clock()\nwhile(True):\n    clock.tick()  # Starts tracking elapsed time.\n\n    img = sensor.snapshot()\n\n    # default settings just do one detection\n    for obj in net.classify(img, \n                            min_scale=1.0, \n                            scale_mul=0.8, \n                            x_overlap=0.5, \n                            y_overlap=0.5):\n        fps = clock.fps()\n        lat = clock.avg()\n\n        print(\"**********\\nPrediction:\")\n        img.draw_rectangle(obj.rect())\n        # This combines the labels and confidence values into a list of tuples\n        predictions_list = list(zip(labels, obj.output()))\n\n        max_val = predictions_list[0][1]\n        max_lbl = 'background'\n        for i in range(len(predictions_list)):\n            val = predictions_list[i][1]\n            lbl = predictions_list[i][0]\n\n            if val &gt; max_val:\n                max_val = val\n                max_lbl = lbl\n\n    # Print label with the highest probability\n    if max_val &lt; 0.5:\n        max_lbl = 'uncertain'\n    print(\"{} with a prob of {:.2f}\".format(max_lbl, max_val))\n    print(\"FPS: {:.2f} fps ==&gt; latency: {:.0f} ms\".format(fps, lat))\n\n    # Draw label with highest probability to image viewer\n    img.draw_string(\n        10, 10,\n        max_lbl + \"\\n{:.2f}\".format(max_val),\n        mono_space = False,\n        scale=2\n        )\nQui si può vedere il risultato:\n\nNotare che la latenza (136 ms) è quasi il doppio di quella che ottenuta direttamente con l’IDE Arduino. Questo perché stiamo usando l’IDE come interfaccia e anche il tempo di attesa per la fotocamera per essere pronta. Se avviamo il clock appena prima dell’inferenza:\n\nLa latenza scenderà a soli 71 ms.\n\n\nNiclaV funziona a circa la metà della velocità quando è connesso all’IDE. Gli FPS dovrebbero aumentare una volta disconnessi.\n\n\n\nPost-elaborazione con i LED\nQuando lavoriamo con l’apprendimento automatico embedded, cerchiamo dispositivi che possano procedere continuamente con l’inferenza e il risultato, eseguendo un’azione direttamente sul mondo fisico e non visualizzando il risultato su un computer connesso. Per simulare ciò, accenderemo un LED diverso per ogni possibile risultato dell’inferenza.\n\nPer ottenere ciò, dovremmo caricare il codice da GitHub o modificare l’ultimo codice per includere i LED:\n# Marcelo Rovai - NICLA Vision - Image Classification with LEDs\n# Adapted from Edge Impulse - OpenMV Image Classification Example\n# @24Aug23\n\nimport sensor, image, time, os, tf, uos, gc, pyb\n\nledRed = pyb.LED(1)\nledGre = pyb.LED(2)\nledBlu = pyb.LED(3)\n\nsensor.reset()                         # Reset and initialize the sensor.\nsensor.set_pixformat(sensor.RGB565)    # Set pixl fmt to RGB565 (or GRAYSCALE)\nsensor.set_framesize(sensor.QVGA)      # Set frame size to QVGA (320x240)\nsensor.set_windowing((240, 240))       # Set 240x240 window.\nsensor.skip_frames(time=2000)          # Let the camera adjust.\n\nnet = None\nlabels = None\n\nledRed.off()\nledGre.off()\nledBlu.off()\n\ntry:\n    # Load built in model\n    labels, net = tf.load_builtin_model('trained')\nexcept Exception as e:\n    raise Exception(e)\n\nclock = time.clock()\n\n\ndef setLEDs(max_lbl):\n\n    if max_lbl == 'uncertain':\n        ledRed.on()\n        ledGre.off()\n        ledBlu.off()\n\n    if max_lbl == 'periquito':\n        ledRed.off()\n        ledGre.on()\n        ledBlu.off()\n\n    if max_lbl == 'robot':\n        ledRed.off()\n        ledGre.off()\n        ledBlu.on()\n\n    if max_lbl == 'background':\n        ledRed.off()\n        ledGre.off()\n        ledBlu.off()\n\n\nwhile(True):\n    img = sensor.snapshot()\n    clock.tick()  # Starts tracking elapsed time.\n\n    # default settings just do one detection.\n    for obj in net.classify(img, \n                            min_scale=1.0, \n                            scale_mul=0.8, \n                            x_overlap=0.5, \n                            y_overlap=0.5):\n        fps = clock.fps()\n        lat = clock.avg()\n\n        print(\"**********\\nPrediction:\")\n        img.draw_rectangle(obj.rect())\n        # This combines the labels and confidence values into a list of tuples\n        predictions_list = list(zip(labels, obj.output()))\n\n        max_val = predictions_list[0][1]\n        max_lbl = 'background'\n        for i in range(len(predictions_list)):\n            val = predictions_list[i][1]\n            lbl = predictions_list[i][0]\n\n            if val &gt; max_val:\n                max_val = val\n                max_lbl = lbl\n\n    # Print label and turn on LED with the highest probability\n    if max_val &lt; 0.8:\n        max_lbl = 'uncertain'\n\n    setLEDs(max_lbl)\n\n    print(\"{} with a prob of {:.2f}\".format(max_lbl, max_val))\n    print(\"FPS: {:.2f} fps ==&gt; latency: {:.0f} ms\".format(fps, lat))\n\n    # Draw label with highest probability to image viewer\n    img.draw_string(\n        10, 10,\n        max_lbl + \"\\n{:.2f}\".format(max_val),\n        mono_space = False,\n        scale=2\n        )\nOra, ogni volta che una classe ottiene un risultato superiore a 0,8, il LED corrispondente si accenderà:\n\nLed Rosso 0n: incerto (nessuna classe supera 0,8)\nLed Green 0n: Periquito &gt; 0.8\nLed Blue 0n: Robot &gt; 0.8\nTutti i LED spenti: Sfondo &gt; 0.8\n\nEcco il risultato:\n\nPiù in dettaglio",
    "crumbs": [
      "Nicla Vision",
      "Classificazione delle Immagini"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/image_classification/image_classification.it.html#classificazione-delle-immagini-benchmark-non-ufficiale",
    "href": "contents/labs/arduino/nicla_vision/image_classification/image_classification.it.html#classificazione-delle-immagini-benchmark-non-ufficiale",
    "title": "Classificazione delle Immagini",
    "section": "Classificazione delle immagini Benchmark (non ufficiale)",
    "text": "Classificazione delle immagini Benchmark (non ufficiale)\nDiverse schede di sviluppo possono essere utilizzate per l’apprendimento automatico embedded (TinyML) e le più comuni per le applicazioni di Computer Vision (consumo energetico basso) sono ESP32 CAM, Seeed XIAO ESP32S3 Sense, Arduino Nicla Vison e Arduino Portenta.\n\nCogliendo l’occasione, lo stesso modello addestrato è stato distribuito su ESP-CAM, XIAO e Portenta (in questo caso, il modello è stato addestrato di nuovo, utilizzando immagini in scala di grigi per essere compatibile con la sua fotocamera). Ecco il risultato, distribuendo i modelli come Libreria di Arduino:",
    "crumbs": [
      "Nicla Vision",
      "Classificazione delle Immagini"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/image_classification/image_classification.it.html#conclusione",
    "href": "contents/labs/arduino/nicla_vision/image_classification/image_classification.it.html#conclusione",
    "title": "Classificazione delle Immagini",
    "section": "Conclusione",
    "text": "Conclusione\nPrima di finire, si tenga presente che la Computer Vision è più di una semplice classificazione delle immagini. Ad esempio, si possono sviluppare progetti Edge Machine Learning sulla visione in diverse aree, come:\n\nVeicoli Autonomi: Usa un gruppo di sensori, i dati lidar e gli algoritmi di visione artificiale per navigare e prendere decisioni.\nSanità: Diagnosi automatizzata di malattie tramite analisi delle immagini di risonanza magnetica, raggi X e TAC\nVendita al Dettaglio: Sistemi di pagamento automatizzati che identificano i prodotti mentre passano attraverso uno scanner.\nSicurezza e Sorveglianza: Riconoscimento facciale, rilevamento di anomalie e tracciamento di oggetti in video in tempo reale.\nRealtà Aumentata: Rilevamento e classificazione di oggetti per sovrapporre informazioni digitali al mondo reale.\nAutomazione Industriale: Ispezione visiva di prodotti, manutenzione predittiva e guida di robot e droni.\nAgricoltura; Monitoraggio delle colture basato su droni e raccolta automatizzata.\nElaborazione del Linguaggio Naturale: Didascalie delle immagini e risposte visive alle domande.\nRiconoscimento dei Gesti: Per giochi, traduzione del linguaggio dei segni e interazione uomo-macchina.\nRaccomandazione dei Contenuti: Sistemi di raccomandazione basati sulle immagini nell’e-commerce.",
    "crumbs": [
      "Nicla Vision",
      "Classificazione delle Immagini"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/image_classification/image_classification.it.html#risorse",
    "href": "contents/labs/arduino/nicla_vision/image_classification/image_classification.it.html#risorse",
    "title": "Classificazione delle Immagini",
    "section": "Risorse",
    "text": "Risorse\n\nCodici Micropython\nDataset\nProgetto Edge Impulse",
    "crumbs": [
      "Nicla Vision",
      "Classificazione delle Immagini"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/object_detection/object_detection.it.html",
    "href": "contents/labs/arduino/nicla_vision/object_detection/object_detection.it.html",
    "title": "Rilevamento degli Oggetti",
    "section": "",
    "text": "Panoramica\nQuesto è il seguito di CV su Nicla Vision, che ora esplora l’Object Detection sui microcontrollori.",
    "crumbs": [
      "Nicla Vision",
      "Rilevamento degli Oggetti"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/object_detection/object_detection.it.html#panoramica",
    "href": "contents/labs/arduino/nicla_vision/object_detection/object_detection.it.html#panoramica",
    "title": "Rilevamento degli Oggetti",
    "section": "",
    "text": "Object Detection e Image Classification\nIl compito principale dei modelli di Image Classification [classificazione delle immagini] è quello di produrre un elenco delle categorie di oggetti più probabili presenti in un’immagine, ad esempio, per identificare un gatto soriano subito dopo cena:\n\nMa cosa succede quando il gatto salta vicino al bicchiere di vino? Il modello riconosce ancora solo la categoria predominante nell’immagine, il gatto soriano:\n\nE cosa succede se non c’è una categoria dominante nell’immagine?\n\nIl modello identifica l’immagine soprastante in modo completamente sbagliato come un “ashcan” [?pattumiera?], probabilmente a causa delle tonalità di colore.\n\nIl modello utilizzato in tutti gli esempi precedenti è MobileNet, addestrato con un ampio set di dati, la ImageNet.\n\nPer risolvere questo problema, abbiamo bisogno di un altro tipo di modello, in cui non solo possono essere trovate più categorie (o etichette), ma anche dove si trovano gli oggetti in una determinata immagine.\nCome possiamo immaginare, tali modelli sono molto più complicati e più grandi, ad esempio, MobileNetV2 SSD FPN-Lite 320x320, addestrato con il set di dati COCO. Questo modello di rilevamento degli oggetti pre-addestrato è progettato per individuare fino a 10 oggetti all’interno di un’immagine, generando un riquadro di delimitazione per ogni oggetto rilevato. L’immagine sottostante è il risultato di un tale modello in esecuzione su un Raspberry Pi:\n\nQuei modelli utilizzati per il rilevamento degli oggetti (come MobileNet SSD o YOLO) hanno solitamente dimensioni di diversi MB, il che è OK per l’uso con Raspberry Pi ma non adatto per l’uso con dispositivi embedded, dove la RAM solitamente è inferiore a 1 M Byte.\n\n\nUna soluzione innovativa per il rilevamento degli oggetti: FOMO\nEdge Impulse ha lanciato nel 2022 FOMO (Faster Objects, More Objects), una nuova soluzione per eseguire il rilevamento di oggetti su dispositivi embedded, non solo su Nicla Vision (Cortex M7) ma anche su CPU Cortex M4F (serie Arduino Nano33 e OpenMV M4) e sui dispositivi Espressif ESP32 (ESP-CAM e XIAO ESP32S3 Sense).\nIn questo esercizio pratico, esploreremo l’uso di FOMO con Object Detection, senza entrare in molti dettagli sul modello stesso. Per saperne di più su come funziona il modello, si può esaminare l’annuncio ufficiale FOMO di Edge Impulse, dove Louis Moreau e Mat Kelcey spiegano in dettaglio come funziona.",
    "crumbs": [
      "Nicla Vision",
      "Rilevamento degli Oggetti"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/object_detection/object_detection.it.html#obiettivo-del-progetto-di-object-detection",
    "href": "contents/labs/arduino/nicla_vision/object_detection/object_detection.it.html#obiettivo-del-progetto-di-object-detection",
    "title": "Rilevamento degli Oggetti",
    "section": "Obiettivo del Progetto di Object Detection",
    "text": "Obiettivo del Progetto di Object Detection\nTutti i progetti di apprendimento automatico devono iniziare con un obiettivo dettagliato. Supponiamo di trovarci in una struttura industriale e di dover ordinare e contare ruote e scatole speciali.\n\nIn altre parole, dovremmo eseguire una classificazione multi-etichetta, in cui ogni immagine può avere tre classi:\n\nBackground [Sfondo] (nessun oggetto)\nBox [Scatola]\nWheel [Ruota]\n\nEcco alcuni campioni di immagini non etichettate che dovremmo usare per rilevare gli oggetti (ruote e scatole):\n\nSiamo interessati a quale oggetto è presente nell’immagine, alla sua posizione (centroide) e a quanti ne possiamo trovare su di essa. La dimensione dell’oggetto non viene rilevata con FOMO, come con MobileNet SSD o YOLO, in cui il Bounding Box è uno degli output del modello.\nSvilupperemo il progetto utilizzando Nicla Vision per l’acquisizione di immagini e l’inferenza del modello. Il progetto ML verrà sviluppato utilizzando Edge Impulse Studio. Ma prima di iniziare il progetto di “object detection” in Studio, creiamo un dataset grezzo (non etichettato) con immagini che contengono gli oggetti da rilevare.",
    "crumbs": [
      "Nicla Vision",
      "Rilevamento degli Oggetti"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/object_detection/object_detection.it.html#raccolta-dati",
    "href": "contents/labs/arduino/nicla_vision/object_detection/object_detection.it.html#raccolta-dati",
    "title": "Rilevamento degli Oggetti",
    "section": "Raccolta Dati",
    "text": "Raccolta Dati\nPossiamo utilizzare Edge Impulse Studio, OpenMV IDE, il telefono o altri dispositivi per l’acquisizione delle immagini. Qui, utilizzeremo di nuovo OpenMV IDE per il nostro scopo.\n\nRaccolta del Dataset con OpenMV IDE\nPer prima cosa, si crea sul computer una cartella in cui verranno salvati i dati, ad esempio “data”. Quindi, su OpenMV IDE, si va in “Tools &gt; Dataset Editor” e si seleziona “New Dataset” per avviare la raccolta di dati:\n\nEdge impulse suggerisce che gli oggetti dovrebbero essere di dimensioni simili e non sovrapposti per prestazioni migliori. Questo va bene in una struttura industriale, dove la telecamera dovrebbe essere fissa, mantenendo la stessa distanza dagli oggetti da rilevare. Nonostante ciò, proveremo anche con dimensioni e posizioni miste per vedere il risultato.\n\nNon creeremo cartelle separate per le nostre immagini perché ciascuna contiene più etichette.\n\nSi collega Nicla Vision a OpenMV IDE e si esegue dataset_capture_script.py. Cliccando sul pulsante “Capture Image” inizierà l’acquisizione delle immagini:\n\nSuggeriamo circa 50 immagini che mescolano gli oggetti e variano il numero di ciascuno che appare sulla scena. Provare ad acquisire con diverse angolazioni, sfondi e condizioni di luce.\n\nLe immagini memorizzate utilizzano una dimensione del fotogramma QVGA 320x240 e RGB565 (formato pixel a colori).\n\nDopo aver acquisito il dataset, si chiude il Tool Dataset Editor su Tools &gt; Dataset Editor.",
    "crumbs": [
      "Nicla Vision",
      "Rilevamento degli Oggetti"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/object_detection/object_detection.it.html#edge-impulse-studio",
    "href": "contents/labs/arduino/nicla_vision/object_detection/object_detection.it.html#edge-impulse-studio",
    "title": "Rilevamento degli Oggetti",
    "section": "Edge Impulse Studio",
    "text": "Edge Impulse Studio\n\nSetup del progetto\nSi va su Edge Impulse Studio, si inseriscono le proprie credenziali in Login (o si crea un account) e si avvia un nuovo progetto.\n\n\nQui si può clonare il progetto sviluppato per questa esercitazione pratica: NICLA_Vision_Object_Detection.\n\nNella “Project Dashboard”, si va in basso e su Project info e si seleziona Bounding boxes (object detection) e Nicla Vision come “Target Device”:\n\n\n\nCaricamento dei dati non etichettati\nSu Studio, si va alla scheda Data acquisition e nella sezione UPLOAD DATA, si caricano dal computer i file acquisiti.\n\n\nSi può lasciare che Studio divida automaticamente i dati tra “Train” e “Test” o farlo manualmente.\n\n\nTutte le immagini non etichettate (51) sono state caricate, ma devono comunque essere etichettate in modo appropriato prima di utilizzarle come set di dati nel progetto. Lo Studio ha uno strumento per questo scopo, che si trova al link Labeling queue (51).\nCi sono due modi per eseguire l’etichettatura assistita dall’IA su Edge Impulse Studio (versione gratuita):\n\nUtilizzando yolov5\nTracciando di oggetti tra i frame\n\n\nEdge Impulse ha lanciato una funzione di auto-labeling per i clienti Enterprise, semplificando le attività di etichettatura nei progetti di rilevamento degli oggetti.\n\nGli oggetti ordinari possono essere rapidamente identificati ed etichettati utilizzando una libreria esistente di modelli di rilevamento degli oggetti pre-addestrati da YOLOv5 (addestrati con il set di dati COCO). Ma poiché, nel nostro caso, gli oggetti non fanno parte dei set di dati COCO, dovremmo selezionare l’opzione di tracking objects. Con questa opzione, una volta disegnati i riquadri di delimitazione ed etichettate le immagini in un frame, gli oggetti verranno tracciati automaticamente da un frame all’altro, etichettando parzialmente quelli nuovi (non tutti sono etichettati correttamente).\n\nSi può usare EI uploader per importare i dati se si ha già un dataset etichettato contenente dei “bounding box”.\n\n\n\nEtichettatura del Dataset\nIniziando dalla prima immagine dei dati non etichettati, si usa il mouse per trascinare una casella attorno a un oggetto per aggiungere un’etichetta. Poi si clicca su Save labels per passare all’elemento successivo.\n\nSi continua con questo processo finché la coda non è vuota. Alla fine, tutte le immagini dovrebbero avere gli oggetti etichettati come i campioni sottostanti:\n\nPoi, si esaminano i campioni etichettati nella scheda Data acquisition. Se un’etichetta è sbagliata, la si può modificare usando il menù three dots dopo il nome del campione:\n\nSi verrà guidati a sostituire l’etichetta sbagliata, correggendo il dataset.",
    "crumbs": [
      "Nicla Vision",
      "Rilevamento degli Oggetti"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/object_detection/object_detection.it.html#impulse-design",
    "href": "contents/labs/arduino/nicla_vision/object_detection/object_detection.it.html#impulse-design",
    "title": "Rilevamento degli Oggetti",
    "section": "Impulse Design",
    "text": "Impulse Design\nIn questa fase, si deve definire come:\n\nIl Pre-processing consiste nel ridimensionare le singole immagini da 320 x 240 a 96 x 96 e nel comprimerle (forma quadrata, senza ritaglio). Successivamente, le immagini vengono convertite da RGB a scala di grigi.\nDesign a Model, in questo caso, “Object Detection”.\n\n\n\nPre-elaborazione di tutti i dataset\nIn questa sezione, si seleziona Color depth come Grayscale, che è adatta per l’uso con modelli FOMO e Save parameters.\n\nLo Studio passa automaticamente alla sezione successiva, Generate features, dove tutti i campioni saranno pre-elaborati, con conseguente set di dati con singole immagini 96x96x1 o 9.216 “feature”.\n\nL’esploratore di feature mostra che tutti i campioni evidenziano una buona separazione dopo la generazione delle feature.\n\nUno dei campioni (46) apparentemente si trova nello spazio sbagliato, ma cliccandoci sopra si può confermare che l’etichettatura è corretta.",
    "crumbs": [
      "Nicla Vision",
      "Rilevamento degli Oggetti"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/object_detection/object_detection.it.html#progettazione-addestramento-e-test-del-modello",
    "href": "contents/labs/arduino/nicla_vision/object_detection/object_detection.it.html#progettazione-addestramento-e-test-del-modello",
    "title": "Rilevamento degli Oggetti",
    "section": "Progettazione, Addestramento e Test del Modello",
    "text": "Progettazione, Addestramento e Test del Modello\nUseremo FOMO, un modello di rilevamento degli oggetti basato su MobileNetV2 (alpha 0.35) progettato per segmentare grossolanamente un’immagine in una griglia di background rispetto a oggetti di interesse (in questo caso, scatole e ruote).\nFOMO è un modello di apprendimento automatico innovativo per il rilevamento degli oggetti, che può utilizzare fino a 30 volte meno energia e memoria rispetto ai modelli tradizionali come Mobilenet SSD e YOLOv5. FOMO può funzionare su microcontrollori con meno di 200 KB di RAM. Il motivo principale per cui ciò è possibile è che mentre altri modelli calcolano le dimensioni dell’oggetto disegnando un quadrato attorno ad esso (bounding box), FOMO ignora le dimensioni dell’immagine, fornendo solo le informazioni su dove si trova l’oggetto nell’immagine, tramite le coordinate del centroide.\nCome funziona FOMO?\nFOMO prende l’immagine in scala di grigi e la divide in blocchi di pixel usando un fattore di 8. Per l’input di 96x96, la griglia è 12x12 (96/8=12). Successivamente, FOMO eseguirà un classificatore attraverso ogni blocco di pixel per calcolare la probabilità che ci sia una scatola o una ruota in ognuno di essi e, successivamente, determinerà le regioni che hanno la più alta probabilità di contenere l’oggetto (se un blocco di pixel non ha oggetti, verrà classificato come background). Dalla sovrapposizione della regione finale, FOMO fornisce le coordinate (relative alle dimensioni dell’immagine) del centroide di questa regione.\n\nPer l’addestramento, dovremmo selezionare un modello pre-addestrato. Usiamo FOMO (Faster Objects, More Objects) MobileNetV2 0.35. Questo modello utilizza circa 250 KB di RAM e 80 KB di ROM (Flash), che si adatta bene alla nostra scheda poiché ha 1 MB di RAM e ROM.\n\nPer quanto riguarda gli iperparametri di training, il modello verrà addestrato con:\n\nEpochs: 60,\nBatch size: 32\nLearning Rate: 0.001.\n\nPer la convalida durante l’addestramento, il 20% del set di dati (validation_dataset) verrà risparmiato. Per il restante 80% (train_dataset), applicheremo il “Data Augmentation”, che capovolgerà casualmente, cambierà le dimensioni e la luminosità dell’immagine e le ritaglierà, aumentando artificialmente il numero di campioni sul set di dati per l’addestramento.\nDi conseguenza, il modello termina con praticamente 1,00 nel punteggio F1, con un risultato simile quando si utilizzano i dati di test.\n\nNotare che FOMO ha aggiunto automaticamente una terza etichetta di background [sfondo] ai due precedentemente definiti (box e wheel).\n\n\n\nNelle attività di rilevamento degli oggetti, l’accuratezza non è generalmente la evaluation metric primaria. Il rilevamento degli oggetti comporta la classificazione degli oggetti e la fornitura di riquadri di delimitazione attorno a essi, il che lo rende un problema più complesso della semplice classificazione. Il problema è che non abbiamo il riquadro di delimitazione, solo i centroidi. In breve, usare l’accuratezza come metrica potrebbe essere fuorviante e potrebbe non fornire una comprensione completa delle prestazioni del modello. Per questo motivo, useremo il punteggio F1.\n\n\nModello di test con “Live Classification”\nDato che Edge Impulse supporta ufficialmente Nicla Vision, colleghiamolo allo Studio. Per farlo, si seguono i passaggi:\n\nSi effettua il download dell’last EI Firmware e lo si decomprime.\nSi apre il file zip sul computer e si seleziona l’uploader relativo al proprio sistema operativo:\n\n\n\nMettere Nicla-Vision in “Boot Mode”, premendo due volte il pulsante di reset.\nEseguire il codice batch specifico per il sistema operativo per caricare il binario (arduino-nicla-vision.bin) sulla board.\n\nSi va nella sezione Live classification su EI Studio e, tramite webUSB, si connette la Nicla Vision:\n\nUna volta connessa, si può usare la Nicla per catturare immagini reali da testare col modello addestrato su Edge Impulse Studio.\n\nUna cosa da notare è che il modello può produrre falsi positivi e falsi negativi. Questo può essere ridotto al minimo definendo una Confidence Threshold [Soglia di confidenza] (si usa il menù Three dots [tre-punti] per la configurazione). Provare con 0,8 o più.",
    "crumbs": [
      "Nicla Vision",
      "Rilevamento degli Oggetti"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/object_detection/object_detection.it.html#distribuzione-del-modello",
    "href": "contents/labs/arduino/nicla_vision/object_detection/object_detection.it.html#distribuzione-del-modello",
    "title": "Rilevamento degli Oggetti",
    "section": "Distribuzione del Modello",
    "text": "Distribuzione del Modello\nSelezionare OpenMV Firmware nella scheda Deploy e premere [Build].\n\nQuando si connette di nuovo Nicla con OpenMV IDE, proverà ad aggiornarne il FW. Scegliere invece l’opzione Load a specific firmware.\n\nSi troverà un file ZIP sul computer dallo Studio. Lo si apre:\n\nCaricare il file .bin sulla board:\n\nUna volta terminato il download, verrà visualizzato un messaggio pop-up. Premere OK e aprire lo script ei_object_detection.py scaricato da Studio.\nPrima di eseguire lo script, modifichiamo alcune righe. Notare che si può lasciare la definizione della finestra come 240 x 240 e la telecamera che cattura le immagini come QVGA/RGB. L’immagine catturata verrà pre-elaborata dal FW distribuito da Edge Impulse\n# Edge Impulse - OpenMV Object Detection Example\n\nimport sensor, image, time, os, tf, math, uos, gc\n\nsensor.reset()                         # Reset and initialize the sensor.\nsensor.set_pixformat(sensor.RGB565)    # Set pixel format to RGB565 (or GRAYSCALE)\nsensor.set_framesize(sensor.QVGA)      # Set frame size to QVGA (320x240)\nsensor.set_windowing((240, 240))       # Set 240x240 window.\nsensor.skip_frames(time=2000)          # Let the camera adjust.\n\nnet = None\nlabels = None\nRidefinire la confidenza minima, ad esempio, a 0,8 per ridurre al minimo i falsi positivi e negativi.\nmin_confidence = 0.8\nSe necessario, modificare il colore dei cerchi che saranno utilizzati per visualizzare il centroide dell’oggetto rilevato per un contrasto migliore.\ntry:\n    # Load built in model\n    labels, net = tf.load_builtin_model('trained')\nexcept Exception as e:\n    raise Exception(e)\n\ncolors = [ # Add more colors if you are detecting more than 7 types of classes at once.\n    (255, 255,   0), # background: yellow (not used)\n    (  0, 255,   0), # cube: green\n    (255,   0,   0), # wheel: red\n    (  0,   0, 255), # not used\n    (255,   0, 255), # not used\n    (  0, 255, 255), # not used\n    (255, 255, 255), # not used\n]\nMantenere il codice restante così com’è e premere il pulsante Play verde per eseguire il codice:\n\nNella vista della telecamera, possiamo vedere gli oggetti con i loro centroidi contrassegnati con 12 cerchi pixel-fixed (ogni cerchio ha un colore distinto, a seconda della sua classe). Sul terminale seriale, il modello mostra le etichette rilevate e la loro posizione sulla finestra dell’immagine (240X240).\n\nAttenzione l’origine delle coordinate è nell’angolo in alto a sinistra.\n\n\nNotare che la frequenza dei fotogrammi al secondo è di circa 8 fps (simile a quella ottenuta con il progetto Image Classification). Ciò accade perché FOMO è intelligentemente costruito su un modello CNN, non con un modello di rilevamento degli oggetti come SSD MobileNet. Ad esempio, quando si esegue un modello MobileNetV2 SSD FPN-Lite 320x320 su un Raspberry Pi 4, la latenza è circa 5 volte superiore (circa 1,5 fps)\nEcco un breve video che mostra i risultati dell’inferenza:",
    "crumbs": [
      "Nicla Vision",
      "Rilevamento degli Oggetti"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/object_detection/object_detection.it.html#conclusione",
    "href": "contents/labs/arduino/nicla_vision/object_detection/object_detection.it.html#conclusione",
    "title": "Rilevamento degli Oggetti",
    "section": "Conclusione",
    "text": "Conclusione\nFOMO è un salto significativo nello spazio di elaborazione delle immagini, come hanno affermato Louis Moreau e Mat Kelcey durante il suo lancio nel 2022:\n\nFOMO è un algoritmo rivoluzionario che porta per la prima volta il rilevamento, il tracciamento e il conteggio degli oggetti in tempo reale sui microcontrollori.\n\nEsistono molteplici possibilità per esplorare il rilevamento degli oggetti (e, più precisamente, il loro conteggio) su dispositivi embedded, ad esempio, per esplorare la il raggruppamento di sensori (telecamera + microfono) e il rilevamento degli oggetti di Nicla. Questo può essere molto utile nei progetti che coinvolgono le api, ad esempio.",
    "crumbs": [
      "Nicla Vision",
      "Rilevamento degli Oggetti"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/object_detection/object_detection.it.html#risorse",
    "href": "contents/labs/arduino/nicla_vision/object_detection/object_detection.it.html#risorse",
    "title": "Rilevamento degli Oggetti",
    "section": "Risorse",
    "text": "Risorse\n\nProgetto Edge Impulse",
    "crumbs": [
      "Nicla Vision",
      "Rilevamento degli Oggetti"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/kws/kws.it.html",
    "href": "contents/labs/arduino/nicla_vision/kws/kws.it.html",
    "title": "Keyword Spotting (KWS)",
    "section": "",
    "text": "Panoramica\nDopo aver già esplorato la scheda Nicla Vision nelle applicazioni di Image Classification e Object Detection, stiamo ora spostando la nostra attenzione sulle applicazioni attivate tramite comando vocale con un progetto su Keyword Spotting (KWS).\nCome introdotto nel tutorial Feature Engineering for Audio Classification, il Keyword Spotting (KWS) è integrato in molti sistemi di riconoscimento vocale, consentendo ai dispositivi di rispondere a parole o frasi specifiche. Sebbene questa tecnologia sia alla base di dispositivi popolari come Google Assistant o Amazon Alexa, è ugualmente applicabile e fattibile su dispositivi più piccoli e a basso consumo. Questo tutorial guiderà nell’implementazione di un sistema KWS utilizzando TinyML sulla scheda di sviluppo Nicla Vision dotata di un microfono digitale.\nIl nostro modello sarà progettato per riconoscere parole chiave che possono riattivare un dispositivo o azioni specifiche, dando loro vita con comandi vocali.",
    "crumbs": [
      "Nicla Vision",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/kws/kws.it.html#come-funziona-un-assistente-vocale",
    "href": "contents/labs/arduino/nicla_vision/kws/kws.it.html#come-funziona-un-assistente-vocale",
    "title": "Keyword Spotting (KWS)",
    "section": "Come funziona un assistente vocale?",
    "text": "Come funziona un assistente vocale?\nCome detto, gli assistenti vocali sul mercato, come Google Home o Amazon Echo-Dot, reagiscono agli umani solo quando vengono “svegliati” da parole chiave particolari come “Hey Google” sul primo e “Alexa” sul secondo.\n\nIn altre parole, il riconoscimento dei comandi vocali si basa su un modello multi-fase o Cascade Detection.\n\nFase 1: Un piccolo microprocessore all’interno di Echo Dot o Google Home ascolta continuamente, in attesa che venga individuata la parola chiave, utilizzando un modello TinyML nel device (applicazione KWS).\nFase 2: Solo quando vengono attivati dall’applicazione KWS nella Fase 1, i dati vengono inviati al cloud ed elaborati su un modello più grande.\nIl video qui sotto mostra un esempio di un Google Assistant programmato su un Raspberry Pi (Fase 2), con un Arduino Nano 33 BLE come dispositivo TinyML (Fase 1).\n\n\nPer esplorare il progetto Google Assistant di cui sopra, consultare il tutorial: Building an Intelligent Voice Assistant From Scratch.\n\nIn questo progetto KWS, ci concentreremo sulla Fase 1 (KWS o Keyword Spotting), dove utilizzeremo Nicla Vision, che ha un microfono digitale che verrà utilizzato per individuare la parola chiave.",
    "crumbs": [
      "Nicla Vision",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/kws/kws.it.html#il-progetto-pratico-kws",
    "href": "contents/labs/arduino/nicla_vision/kws/kws.it.html#il-progetto-pratico-kws",
    "title": "Keyword Spotting (KWS)",
    "section": "Il Progetto Pratico KWS",
    "text": "Il Progetto Pratico KWS\nIl diagramma seguente fornisce un’idea di come dovrebbe funzionare l’applicazione KWS finale (durante l’inferenza):\n\nLa nostra applicazione KWS riconoscerà quattro classi di suono:\n\nYES (Keyword 1)\nNO (Keyword 2)\nNOISE (nessuna parola pronunciata; è presente solo rumore di fondo)\nUNKNOW (un mix di parole diverse da YES e NO)\n\n\nPer progetti reali, è sempre consigliabile includere altri suoni oltre alle parole chiave, come “Noise” (o Background) e “Unknown”.\n\n\nIl Flusso di Lavoro del Machine Learning\nIl componente principale dell’applicazione KWS è il suo modello. Quindi, dobbiamo addestrare un modello del genere con le nostre parole chiave specifiche, rumore e altre parole (lo “unknown”):",
    "crumbs": [
      "Nicla Vision",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/kws/kws.it.html#il-dataset",
    "href": "contents/labs/arduino/nicla_vision/kws/kws.it.html#il-dataset",
    "title": "Keyword Spotting (KWS)",
    "section": "Il Dataset",
    "text": "Il Dataset\nIl componente critico di qualsiasi flusso di lavoro di apprendimento automatico è il dataset. Una volta decise le parole chiave specifiche, nel nostro caso (YES e NO), possiamo sfruttare il dataset sviluppato da Pete Warden, “Speech Commands: A Dataset for Limited-Vocabulary Speech Recognition”. Questo dataset ha 35 parole chiave (con +1.000 campioni ciascuna), come yes, no, stop e go. In parole come yes e no, possiamo ottenere 1.500 campioni.\nSi può scaricare una piccola parte del dataset da Edge Studio (Keyword spotting pre-built dataset), che include campioni dalle quattro classi che utilizzeremo in questo progetto: yes, no, noise e background. Per farlo, si seguono i passaggi seguenti:\n\nDownload del dataset delle parole chiave.\nUnzip del file in una posizione a scelta.\n\n\nCaricamento del set di dati su Edge Impulse Studio\nSi avvia un nuovo progetto su Edge Impulse Studio (EIS) e si seleziona il tool Upload Existing Data nella sezione Data Acquisition. Si scelgono i file da caricare:\n\nSi definisce l’etichetta, si seleziona Automatically split between train and test, e poi Upload data su EIS. Si ripete per tutte le classi.\n\nIl dataset apparirà ora nella sezione Data acquisition. Notare che i circa 6.000 campioni (1.500 per ogni classe) sono suddivisi in set di Train (4.800) e di Test (1.200).\n\n\n\nAcquisizione di Dati Audio Aggiuntivi\nSebbene disponiamo di molti dati dal dataset di Pete, è consigliabile raccogliere alcune parole pronunciate da noi. Lavorando con gli accelerometri, è essenziale creare un set di dati con dati acquisiti dallo stesso tipo di sensore. Nel caso del suono, questo è facoltativo perché ciò che classificheremo sono, in realtà, dati audio.\n\nLa differenza fondamentale tra suono e audio è il tipo di energia. Il suono è una perturbazione meccanica (onde sonore longitudinali) che si propagano attraverso un mezzo, causando variazioni di pressione in esso. L’audio è un segnale elettrico (analogico o digitale) che rappresenta il suono.\n\nQuando pronunciamo una parola chiave, le onde sonore devono essere convertite in dati audio. La conversione deve essere eseguita campionando il segnale generato dal microfono a una frequenza di 16 KHz con ampiezza di 16 bit per campione.\nQuindi, qualsiasi dispositivo in grado di generare dati audio con questa specifica di base (16 KHz/16 bit) funzionerà correttamente. Come device, possiamo usare NiclaV, un computer o persino il cellulare.\n\n\nUtilizzo di NiclaV ed Edge Impulse Studio\nCome abbiamo appreso nel capitolo Configurazione di Nicla Vision, EIS supporta ufficialmente Nicla Vision, semplificando l’acquisizione dei dati dai suoi sensori, incluso il microfono. Quindi, si crea un nuovo progetto su EIS e vi si collega la Nicla, seguendo questi passaggi:\n\nDownload del Firmware EIS più aggiornato e lo si decomprime.\nAprire il file zip sul computer e selezionare l’uploader corrispondente al sistema operativo:\n\n\n\nSi mette NiclaV in Boot Mode premendo due volte il pulsante di reset.\nSi carica il binario arduino-nicla-vision.bin sulla board eseguendo il codice batch corrispondente al sistema operativo.\n\nSi va sul proprio progetto EIS e nella scheda Data Acquisition tab, si seleziona WebUSB. Apparirà una finestra; scegliere l’opzione che mostra che Nicla is paired e si preme [Connect].\nSi possono scegliere quali dati del sensore raccogliere nella sezione Collect Data nella scheda Data Acquisition. Si seleziona: Built-in microphone, si definisce la label (per esempio, yes), la Frequency[16000Hz] e la Sample length (in milliseconds), per esempio [10s]. Start sampling.\n\nI dati sul dataset di Pete hanno una lunghezza di 1s, ma i campioni registrati sono lunghi 10s e devono essere suddivisi in campioni da 1s. Cliccare sui tre puntini dopo il nome del campione e selezionare Split sample.\nSi aprirà una finestra col tool Split.\n\nUna volta all’interno dello strumento, si dividono i dati in record da 1 secondo (1000 ms). Se necessario, si aggiungono o rimuovono segmenti. Questa procedura deve essere ripetuta per tutti i nuovi campioni.\n\n\nUtilizzo di uno smartphone e di EI Studio\nSi può anche utilizzare il PC o lo smartphone per acquisire dati audio, utilizzando una frequenza di campionamento di 16 KHz e una profondità di 16 bit.\nSi va su Devices, si scansiona il QR Code col telefono e si clicca sul link. Un’app, Collection, per la raccolta dei dati apparirà nel browser. Si seleziona Collecting Audio e si definisce la Label, la Length [lunghezza] dei dati catturati e la Category.\n\nSi ripete la stessa procedura usata con NiclaV.\n\nNotare che qualsiasi app, come Audacity, può essere usata per la registrazione audio, a condizione che si usino campioni di 16KHz/16-bit.",
    "crumbs": [
      "Nicla Vision",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/kws/kws.it.html#creazione-di-impulse-pre-process-definizione-del-modello",
    "href": "contents/labs/arduino/nicla_vision/kws/kws.it.html#creazione-di-impulse-pre-process-definizione-del-modello",
    "title": "Keyword Spotting (KWS)",
    "section": "Creazione di Impulse (Pre-Process / Definizione del Modello)",
    "text": "Creazione di Impulse (Pre-Process / Definizione del Modello)\nUn impulse prende dati grezzi, usa l’elaborazione del segnale per estrarre le feature e poi usa un blocco di apprendimento per classificare nuovi dati.\n\nImpulse Design\n\nInnanzitutto, prenderemo i dati con una finestra di 1 secondo, aumentando i dati e facendo scorrere quella finestra in intervalli di 500 ms. Notare che è impostata l’opzione Zero-pad data. È essenziale riempire i campioni con degli ‘zeri’ inferiori a 1 secondo (in alcuni casi, alcuni campioni possono risultare inferiori alla finestra di 1000 ms sul tool di suddivisione per evitare rumore e picchi).\nOgni campione audio di 1 secondo dovrebbe essere pre-elaborato e convertito in un’immagine (ad esempio, 13 x 49 x 1). Come discusso nel tutorial Feature Engineering for Audio Classification, utilizzeremo Audio (MFCC), che estrae le feature dai segnali audio utilizzando i Mel Frequency Cepstral Coefficients, che sono adatti alla voce umana, il nostro caso qui.\nSuccessivamente, selezioniamo il blocco Classification per costruire il nostro modello da zero utilizzando una Convolution Neural Network (CNN).\n\nIn alternativa, si può utilizzare il blocco Transfer Learning (Keyword Spotting), che ottimizza un modello di keyword spotting pre-addestrato sui dati. Questo approccio ha buone prestazioni con set di dati di parole chiave relativamente piccoli.\n\n\n\nPre-elaborazione (MFCC)\nIl passaggio successivo consiste nel creare le feature da addestrare nella fase successiva:\nPotremmo mantenere i valori di default dei parametri, ma utilizzeremo l’opzione DSP Autotune parameters.\n\nPrenderemo le Raw features (i dati audio campionati a 16 KHz e 1 secondo) e utilizzeremo il blocco di elaborazione MFCC per calcolare le Processed features. Per ogni 16.000 feature grezze (16.000 x 1 secondo), otterremo 637 feature elaborate (13 x 49).\n\nIl risultato mostra che abbiamo utilizzato solo una piccola quantità di memoria per pre-elaborare i dati (16 KB) e una latenza di 34 ms, il che è eccellente. Ad esempio, su un Arduino Nano (Cortex-M4f @ 64 MHz), lo stesso pre-processo richiederà circa 480 ms. I parametri scelti, come la FFT length [512], avranno un impatto significativo sulla latenza.\nOra, Save parameters e passiamo alla scheda Generated features, dove verranno generate le feature effettive. Utilizzando UMAP, una tecnica di riduzione delle dimensioni, Feature explorer mostra come le feature sono distribuite su un grafico bidimensionale.\n\nIl risultato sembra OK, con una separazione visivamente netta tra feature yes (in rosso) e no (in blu). Le feature unknown [sconosciute] sembrano più vicine allo spazio dei no che a quello degli yes. Questo suggerisce che la parola chiave no ha una maggiore propensione ai falsi positivi.\n\n\nAndiamo sotto il cofano\nPer comprendere meglio come viene pre-elaborato il suono grezzo, leggere il capitolo Feature Engineering for Audio Classification. Si può sperimentare con la generazione di feature MFCC scaricando questo notebook da GitHub o [Opening it In Colab]",
    "crumbs": [
      "Nicla Vision",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/kws/kws.it.html#progettazione-e-addestramento-del-modello",
    "href": "contents/labs/arduino/nicla_vision/kws/kws.it.html#progettazione-e-addestramento-del-modello",
    "title": "Keyword Spotting (KWS)",
    "section": "Progettazione e Addestramento del Modello",
    "text": "Progettazione e Addestramento del Modello\nUseremo un semplice modello di rete neurale convoluzionale (CNN), testato con convoluzioni 1D e 2D. L’architettura di base ha due blocchi di Convolution + MaxPooling (filtri ([8] e [16], rispettivamente) e un Dropout di [0.25] per 1D e [0.5] per 2D. Per l’ultimo layer, dopo Flattening, abbiamo [4] neuroni, uno per ogni classe:\n\nCome iperparametri, avremo un Learning Rate di [0.005] e un modello addestrato da [100] epoche. Includeremo anche un metodo di aumento dei dati basato su SpecAugment. Abbiamo addestrato i modelli 1D e 2D con gli stessi iperparametri. L’architettura 1D ha avuto un risultato complessivo migliore (90,5% di accuratezza rispetto all’88% del 2D, quindi useremo l’1D.\n\n\nL’utilizzo di convoluzioni 1D è più efficiente perché richiede meno parametri rispetto alle convoluzioni 2D, rendendole più adatte ad ambienti con risorse limitate.\n\nÈ anche interessante prestare attenzione alla Matrice di Confusione 1D. Lo Il punteggio F1 per yes è 95% e per il no, 91%. Ciò era previsto da ciò che abbiamo visto con Feature Explorer (no e unknown a distanza ravvicinata). Nel tentativo di migliorare il risultato, si possono esaminare attentamente i risultati dei campioni con un errore.\n\nSi ascoltino i campioni che sono andati male. Ad esempio, per yes, la maggior parte degli errori erano correlati a un sì pronunciato come “yeh”. Si possono acquisire campioni aggiuntivi e quindi riaddestrare il modello.\n\nAndiamo sotto il cofano\nPer capire cosa sta succedendo “sotto il cofano”, si può scaricare il set di dati pre-elaborato ((MFCC training data) dalla scheda Dashboard ed eseguire questo Jupyter Notebook, giocando con il codice o [Aprirlo in Colab]. Ad esempio, si può analizzare l’accuratezza per ogni epoca:",
    "crumbs": [
      "Nicla Vision",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/kws/kws.it.html#test",
    "href": "contents/labs/arduino/nicla_vision/kws/kws.it.html#test",
    "title": "Keyword Spotting (KWS)",
    "section": "Test",
    "text": "Test\nTestando il modello con i dati riservati per il training (Test Data), abbiamo ottenuto un’accuratezza di circa il 76%.\n\nIspezionando il punteggio F1, possiamo vedere che per YES abbiamo ottenuto 0.90, un risultato eccellente poiché prevediamo di utilizzare questa parola chiave come “trigger” primario per il nostro progetto KWS. Il risultato peggiore (0.70) è per UNKNOWN, il che è OK.\nPer NO, abbiamo ottenuto 0,72, come previsto, ma per migliorare questo risultato, possiamo spostare i campioni che non sono stati classificati correttamente nel set di dati di training e quindi ripetere il processo di training.\n\nClassificazione Live\nPossiamo procedere alla fase successiva del progetto, ma consideriamo anche che è possibile eseguire la Classificazione live utilizzando NiclaV o uno smartphone per catturare campioni dal vivo, testando il modello addestrato prima della distribuzione sul nostro dispositivo.",
    "crumbs": [
      "Nicla Vision",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/kws/kws.it.html#distribuzione-e-inferenza",
    "href": "contents/labs/arduino/nicla_vision/kws/kws.it.html#distribuzione-e-inferenza",
    "title": "Keyword Spotting (KWS)",
    "section": "Distribuzione e Inferenza",
    "text": "Distribuzione e Inferenza\nL’EIS impacchetterà tutte le librerie necessarie, le funzioni di pre-elaborazione e i modelli addestrati, scaricandoli sul computer. Si va alla sezione Deployment, si seleziona Arduino Library, e, in basso, si sceglie Quantized (Int8) e si preme Build.\n\nQuando si seleziona il pulsante Build, verrà creato un file zip che verrà scaricato sul computer. Sull’Arduino IDE, si va alla scheda Sketch, si seleziona l’opzione Add .ZIP Library e si sceglie il file .zip scaricato da EIS:\n\nOra è il momento di un vero test. Faremo delle inferenze completamente disconnesse da EIS. Usiamo l’esempio di codice NiclaV creato quando abbiamo distribuito la Libreria Arduino.\nNell’IDE Arduino, si va alla scheda File/Examples, si cerca il progetto e si seleziona nicla-vision/nicla-vision_microphone (o nicla-vision_microphone_continuous)\n\nPremere due volte il pulsante di reset per mettere NiclaV in modalità di avvio, caricare lo sketch sulla board e provare alcune inferenze reali:",
    "crumbs": [
      "Nicla Vision",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/kws/kws.it.html#post-elaborazione",
    "href": "contents/labs/arduino/nicla_vision/kws/kws.it.html#post-elaborazione",
    "title": "Keyword Spotting (KWS)",
    "section": "Post-elaborazione",
    "text": "Post-elaborazione\nOra che sappiamo che il modello funziona perché rileva le nostre parole chiave, modifichiamo il codice per vedere il risultato con NiclaV completamente offline (scollegato dal PC e alimentato da una batteria, un power bank o un alimentatore indipendente da 5 V).\nL’idea è che ogni volta che viene rilevata la parola chiave YES, si accende il LED verde; se si sente un NO, si accende il LED rosso, se è un UNKNOW, si accende il LED blu; e in presenza di rumore (nessuna parola chiave), i LED saranno SPENTI.\nDovremmo modificare uno degli esempi di codice. Facciamolo ora con nicla-vision_microphone_continuous.\nPrima di tutto l’inizializzazione dei LED:\n...\nvoid setup()\n{\n        // Once you finish debugging your code, you can comment or delete the Serial part of the code\n    Serial.begin(115200);\n    while (!Serial);\n    Serial.println(\"Inferencing - Nicla Vision KWS with LEDs\");\n    \n    // Pins for the built-in RGB LEDs on the Arduino NiclaV\n    pinMode(LEDR, OUTPUT);\n    pinMode(LEDG, OUTPUT);\n    pinMode(LEDB, OUTPUT);\n\n    // Ensure the LEDs are OFF by default.\n    // Note: The RGB LEDs on the Arduino Nicla Vision\n    // are ON when the pin is LOW, OFF when HIGH.\n    digitalWrite(LEDR, HIGH);\n    digitalWrite(LEDG, HIGH);\n    digitalWrite(LEDB, HIGH);\n...\n}\nCreare due funzioni, la funzione turn_off_leds(), per spegnere tutti i LED RGB\n/*\n * @brief      turn_off_leds function - turn-off all RGB LEDs\n */\nvoid turn_off_leds(){\n    digitalWrite(LEDR, HIGH);\n    digitalWrite(LEDG, HIGH);\n    digitalWrite(LEDB, HIGH);\n}\nUn’altra funzione turn_on_led() viene utilizzata per accendere i LED RGB in base al risultato più probabile del classificatore.\n/*\n * @brief      turn_on_leds function used to turn on the RGB LEDs\n * @param[in]  pred_index     \n *             no:       [0] ==&gt; Red ON\n *             noise:    [1] ==&gt; ALL OFF \n *             unknown:  [2] ==&gt; Blue ON\n *             Yes:      [3] ==&gt; Green ON\n */\nvoid turn_on_leds(int pred_index) {\n  switch (pred_index)\n  {\n    case 0:\n      turn_off_leds();\n      digitalWrite(LEDR, LOW);\n      break;\n\n    case 1:\n      turn_off_leds();\n      break;\n    \n    case 2:\n      turn_off_leds();\n      digitalWrite(LEDB, LOW);\n      break;\n\n    case 3:\n      turn_off_leds();\n      digitalWrite(LEDG, LOW);\n      break;\n  }\n}\nE modificare la parte // print the predictions del codice su loop():\n...\n\n    if (++print_results &gt;= (EI_CLASSIFIER_SLICES_PER_MODEL_WINDOW)) {\n        // print the predictions\n        ei_printf(\"Predictions \");\n        ei_printf(\"(DSP: %d ms., Classification: %d ms., Anomaly: %d ms.)\",\n            result.timing.dsp, result.timing.classification, result.timing.anomaly);\n        ei_printf(\": \\n\");\n\n        int pred_index = 0;     // Initialize pred_index\n        float pred_value = 0;   // Initialize pred_value\n\n        for (size_t ix = 0; ix &lt; EI_CLASSIFIER_LABEL_COUNT; ix++) {\n            if (result.classification[ix].value &gt; pred_value){\n                pred_index = ix;\n                pred_value = result.classification[ix].value;\n            }\n\n            // ei_printf(\"    %s: \", result.classification[ix].label);\n            // ei_printf_float(result.classification[ix].value);\n            // ei_printf(\"\\n\");\n        }\n\n        ei_printf(\"  PREDICTION: ==&gt; %s with probability %.2f\\n\", \n                  result.classification[pred_index].label, pred_value);\n        turn_on_leds (pred_index);\n\n        \n#if EI_CLASSIFIER_HAS_ANOMALY == 1\n        ei_printf(\"    anomaly score: \");\n        ei_printf_float(result.anomaly);\n        ei_printf(\"\\n\");\n#endif\n\n        print_results = 0;\n    }\n}\n\n...\nIl codice completo si trova tra i progetti GitHub.\nCaricarere lo sketch sulla board e provare alcune inferenze reali. L’idea è che il LED verde sarà ACCESO ogni volta che viene rilevata la parola chiave YES, il rosso si accenderà per un NO e qualsiasi altra parola accenderà il LED blu. Tutti i LED dovrebbero essere spenti se è presente silenzio o rumore di fondo. Ricordare che la stessa procedura può “attivare” un dispositivo esterno per eseguire un’azione desiderata invece di accendere un LED, come abbiamo visto nell’introduzione.",
    "crumbs": [
      "Nicla Vision",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/kws/kws.it.html#conclusione",
    "href": "contents/labs/arduino/nicla_vision/kws/kws.it.html#conclusione",
    "title": "Keyword Spotting (KWS)",
    "section": "Conclusione",
    "text": "Conclusione\n\nSi troveranno i notebook e i codici utilizzati in questo tutorial nel repository GitHub.\n\nPrima di concludere, considerare che la classificazione dei suoni è più di una semplice voce. Ad esempio, si possono sviluppare progetti TinyML sul suono in diverse aree, come:\n\nSicurezza (Rilevamento di vetri rotti, spari)\nIndustria (Rilevamento di Anomalie)\nMedicina (Russamento, tosse, malattie polmonari)\nNatura (Controllo degli alveari, suono degli insetti, riduzione dei sacchetti di raccolta)",
    "crumbs": [
      "Nicla Vision",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/kws/kws.it.html#risorse",
    "href": "contents/labs/arduino/nicla_vision/kws/kws.it.html#risorse",
    "title": "Keyword Spotting (KWS)",
    "section": "Risorse",
    "text": "Risorse\n\nSottoinsieme del Dataset dei Comandi Vocali di Google\nKWS MFCC Analysis Colab Notebook\nKWS_CNN_training Colab Notebook\nCodice di Post-elaborazione Arduino\nProgetto Edge Impulse",
    "crumbs": [
      "Nicla Vision",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/motion_classification/motion_classification.it.html",
    "href": "contents/labs/arduino/nicla_vision/motion_classification/motion_classification.it.html",
    "title": "Classificazione del Movimento e Rilevamento delle Anomalie",
    "section": "",
    "text": "Panoramica\nI trasporti sono la spina dorsale del commercio globale. Milioni di container vengono trasportati ogni giorno tramite vari mezzi, come navi, camion e treni, verso destinazioni in tutto il mondo. Garantire il transito sicuro ed efficiente di questi container è un compito monumentale che richiede di sfruttare la tecnologia moderna e TinyML è senza dubbio una di queste.\nIn questo tutorial, lavoreremo per risolvere problemi reali relativi al trasporto. Svilupperemo un sistema di Motion Classification e Anomaly Detection utilizzando la scheda Arduino Nicla Vision, l’IDE Arduino e Edge Impulse Studio. Questo progetto ci aiuterà a comprendere come i container subiscono forze e movimenti diversi durante le varie fasi del trasporto, come il transito terrestre e marittimo, il movimento verticale tramite carrelli elevatori e i periodi di stazionamento nei magazzini.\nAlla fine di questo tutorial, si avrà un prototipo funzionante in grado di classificare diversi tipi di movimento e rilevare anomalie durante il trasporto di container. Questa conoscenza può essere un trampolino di lancio per progetti più avanzati nel campo emergente di TinyML che coinvolge le vibrazioni.",
    "crumbs": [
      "Nicla Vision",
      "Classificazione del Movimento e Rilevamento delle Anomalie"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/motion_classification/motion_classification.it.html#panoramica",
    "href": "contents/labs/arduino/nicla_vision/motion_classification/motion_classification.it.html#panoramica",
    "title": "Classificazione del Movimento e Rilevamento delle Anomalie",
    "section": "",
    "text": "Obiettivi dell’Apprendimento\n\n\n\n\nImpostazione della scheda Arduino Nicla Vision\nRaccolta e pre-elaborazione dei dati\nCreazione del modello di classificazione del movimento\nImplementazione del rilevamento delle anomalie\nTest e Analisi nel Mondo Reale",
    "crumbs": [
      "Nicla Vision",
      "Classificazione del Movimento e Rilevamento delle Anomalie"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/motion_classification/motion_classification.it.html#installazione-e-test-dellimu",
    "href": "contents/labs/arduino/nicla_vision/motion_classification/motion_classification.it.html#installazione-e-test-dellimu",
    "title": "Classificazione del Movimento e Rilevamento delle Anomalie",
    "section": "Installazione e test dell’IMU",
    "text": "Installazione e test dell’IMU\nPer questo progetto, useremo un accelerometro. Come spiegato nel Tutorial, Configurazione di Nicla Vision, la Nicla Vision Board ha un IMU a 6 assi integrato: giroscopio 3D e accelerometro 3D, il LSM6DSOX. Verifichiamo se la libreria IMU LSM6DSOX è installata. In caso contrario, la si installa.\n\nPoi si va su Examples &gt; Arduino_LSM6DSOX &gt; SimpleAccelerometer e si esegue il test dell’accelerometro. Si può verificare se funziona aprendo l’IDE Serial Monitor o Plotter. I valori sono in g (gravità terrestre), con un range di default di +/- 4g:\n\n\nDefinizione della Frequenza di Campionamento:\nLa scelta di una frequenza di campionamento appropriata è fondamentale per catturare le caratteristiche del movimento che interessa studiare. Il teorema di campionamento di Nyquist-Shannon afferma che la frequenza di campionamento dovrebbe essere almeno il doppio della componente di frequenza più alta nel segnale per ricostruirlo correttamente. Nel contesto della classificazione del movimento e del rilevamento delle anomalie per il trasporto, la scelta della frequenza di campionamento dipenderebbe da diversi fattori:\n\nNatura del movimento: Diversi tipi di trasporto (terrestre, marittimo, ecc.) possono comportare diversi intervalli di frequenze di movimento. Movimenti più rapidi potrebbero richiedere frequenze di campionamento più elevate.\nLimitazioni hardware: La scheda Arduino Nicla Vision e tutti i sensori associati potrebbero avere limitazioni sulla velocità con cui possono campionare i dati.\nRisorse di Calcolo: Frequenze di campionamento più elevate genereranno più dati, il che potrebbe essere computazionalmente intensivo, particolarmente critico in un ambiente TinyML.\nDurata della Batteria: Una frequenza di campionamento più elevata consumerà più energia. Se il sistema funziona a batteria, questa è una considerazione importante.\nArchiviazione Dati: Un campionamento più frequente richiederà più spazio di archiviazione, un’altra considerazione cruciale per i sistemi embedded con memoria limitata.\n\nIn molte attività di riconoscimento delle attività umane, vengono comunemente utilizzate frequenze di campionamento da 50 Hz a 100 Hz circa. Dato che stiamo simulando scenari di trasporto, che in genere non sono eventi ad alta frequenza, una frequenza di campionamento in quell’intervallo (50-100 Hz) potrebbe essere un punto di partenza ragionevole.\nDefiniamo uno sketch che ci consentirà di acquisire i nostri dati con una frequenza di campionamento definita (ad esempio, 50 Hz):\n/*\n * Based on Edge Impulse Data Forwarder Example (Arduino)\n  - https://docs.edgeimpulse.com/docs/cli-data-forwarder\n * Developed by M.Rovai @11May23\n */\n\n/* Include ----------------------------------------------------------------- */\n#include &lt;Arduino_LSM6DSOX.h&gt;\n\n/* Constant defines -------------------------------------------------------- */\n#define CONVERT_G_TO_MS2 9.80665f\n#define FREQUENCY_HZ        50\n#define INTERVAL_MS         (1000 / (FREQUENCY_HZ + 1))\n\nstatic unsigned long last_interval_ms = 0;\nfloat x, y, z;\n\nvoid setup() {\n  Serial.begin(9600);\n  while (!Serial);\n\n  if (!IMU.begin()) {\n    Serial.println(\"Failed to initialize IMU!\");\n    while (1);\n  }\n}\n\nvoid loop() {\n  if (millis() &gt; last_interval_ms + INTERVAL_MS) {\n    last_interval_ms = millis();\n    \n    if (IMU.accelerationAvailable()) {\n      // Read raw acceleration measurements from the device\n      IMU.readAcceleration(x, y, z);\n\n      // converting to m/s2\n      float ax_m_s2 = x * CONVERT_G_TO_MS2;\n      float ay_m_s2 = y * CONVERT_G_TO_MS2;\n      float az_m_s2 = z * CONVERT_G_TO_MS2;\n\n      Serial.print(ax_m_s2); \n      Serial.print(\"\\t\");\n      Serial.print(ay_m_s2); \n      Serial.print(\"\\t\");\n      Serial.println(az_m_s2); \n    }\n\n  }\n}\nCaricando lo sketch e ispezionando il Serial Monitor, possiamo vedere che stiamo catturando 50 campioni al secondo.\n\n\nSi noti che con la scheda Nicla appoggiata su un tavolo (con la telecamera rivolta verso il basso), l’asse z misura circa 9.8m/s\\(^2\\), l’accelerazione terrestre prevista.",
    "crumbs": [
      "Nicla Vision",
      "Classificazione del Movimento e Rilevamento delle Anomalie"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/motion_classification/motion_classification.it.html#il-caso-di-studio-trasporto-simulato-di-container",
    "href": "contents/labs/arduino/nicla_vision/motion_classification/motion_classification.it.html#il-caso-di-studio-trasporto-simulato-di-container",
    "title": "Classificazione del Movimento e Rilevamento delle Anomalie",
    "section": "Il Caso di Studio: Trasporto Simulato di Container",
    "text": "Il Caso di Studio: Trasporto Simulato di Container\nSimuleremo il trasporto di container (o meglio di pacchi) attraverso diversi scenari per rendere questo tutorial più comprensibile e pratico. Utilizzando l’accelerometro integrato della scheda Arduino Nicla Vision, cattureremo i dati di movimento simulando manualmente le condizioni di:\n\nTrasporto Terrestre (su strada o treno)\nTrasporto Marittimo\nMovimento Verticale tramite Carrello Elevatore\nPeriodo di stazionamento (inattivo) in un Magazzino\n\n\nDalle immagini sopra, possiamo definire per la nostra simulazione che i movimenti principalmente orizzontali (asse x o y) dovrebbero essere associati alla “classe Terrestre”, i movimenti verticali (asse z) alla “classe di Sollevamento”, nessuna attività alla “classe di Inattività” e il movimento su tutti e tre gli assi alla classe Marittima.",
    "crumbs": [
      "Nicla Vision",
      "Classificazione del Movimento e Rilevamento delle Anomalie"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/motion_classification/motion_classification.it.html#raccolta-dati",
    "href": "contents/labs/arduino/nicla_vision/motion_classification/motion_classification.it.html#raccolta-dati",
    "title": "Classificazione del Movimento e Rilevamento delle Anomalie",
    "section": "Raccolta Dati",
    "text": "Raccolta Dati\nPer la raccolta dati, possiamo avere diverse opzioni. In un caso reale, possiamo avere il nostro dispositivo, ad esempio, collegato direttamente a un contenitore e i dati raccolti su un file (ad esempio .CSV) e archiviati su una scheda SD (tramite connessione SPI) o un repository offline nel computer. I dati possono anche essere inviati in remoto a un repository nelle vicinanze, come un telefono cellulare, tramite Bluetooth (come fatto in questo progetto: Sensor DataLogger). Una volta che il dataset è stato raccolto e archiviato come file .CSV, può essere caricato su Studio utilizzando lo strumento CSV Wizard.\n\nIn questo video, si possono imparare modi alternativi per inviare dati a Edge Impulse Studio.\n\n\nCollegamento del dispositivo a Edge Impulse\nCollegheremo Nicla direttamente a Edge Impulse Studio, che verrà utilizzato anche per la pre-elaborazione dei dati, l’addestramento del modello, i test e la distribuzione. Per questo, ci sono due possibilità:\n\nScaricare il firmware più recente e collegarlo direttamente alla sezione Raccolta Dati.\nUtilizzare il tool CLI Data Forwarder per acquisire i dati dal sensore e inviarli a Studio.\n\nL’opzione 1 è più semplice, come abbiamo visto nell’esercitazione Configurazione di Nicla Vision, ma l’opzione 2 darà maggiore flessibilità per quanto riguarda l’acquisizione dei dati, come la definizione della frequenza di campionamento. Facciamolo con l’ultima.\nCreare un nuovo progetto su Edge Impulse Studio (EIS) e collegarvi Nicla, seguendo questi passaggi:\n\nInstallare Edge Impulse CLI e Node.js sul computer.\nCaricare uno sketch per l’acquisizione dati (quello discusso in precedenza in questo tutorial).\nUtilizzare CLI Data Forwarder per acquisire dati dall’accelerometro di Nicla e inviarli a Studio, come mostrato in questo diagramma:\n\n\nAvviare CLI Data Forwarder sul terminale, immettendo (se è la prima volta) il seguente comando:\n$ edge-impulse-data-forwarder --clean\nQuindi, immettere le proprie credenziali EI e scegliere il progetto, le variabili (ad esempio, accX, accY e accZ) e il nome del dispositivo (ad esempio, NiclaV):\n\nAndare alla sezione Devices sul progetto EI e verificare se il dispositivo è connesso (il punto dovrebbe essere verde):\n\n\nSi può clonare il progetto sviluppato per questa esercitazione pratica: NICLA Vision Movement Classification.\n\n\n\nRaccolta Dati\nNella sezione Data Acquisition, si dovrebbe vedere che la scheda [NiclaV] è connessa. Il sensore è disponibile: [sensor with 3 axes (accX, accY, accZ)] con una frequenza di campionamento di [50Hz]. Studio suggerisce una lunghezza di campionamento di [10000] ms (10s). L’ultima cosa rimasta è definire l’etichetta del campione. Cominciamo con [terrestrial]:\n\nTerrestrial (pallet in un Camion o Treno), muovendosi orizzontalmente. Premere [Start Sample] e spostare il device orizzontalmente, mantenendo una direzione sopra il tavolo. Dopo 10s, i dati saranno caricati nello Studio. Ecco come è stato raccolto il campione:\n\nCome previsto, il movimento è stato catturato principalmente nell’asse Y (verde). Nel blu, vediamo l’asse Z, circa -10 m/s\\(^2\\) (Nicla ha la telecamera rivolta verso l’alto).\nCome discusso in precedenza, dovremmo catturare dati da tutte e quattro le classi Transportation”. Quindi, si immagini di avere un container con un accelerometro integrato nelle seguenti situazioni:\nMaritime (pallet in barche in un oceano in tempesta). Il movimento viene catturato su tutti e tre gli assi:\n\nLift (Pallet movimentati verticalmente da un Carrello elevatore). Movimento catturato solo sull’asse Z:\n\nIdle (Pallet in un magazzino). Nessun movimento rilevato dall’accelerometro:\n\nAd esempio, si possono catturare 2 minuti (dodici campioni di 10 secondi) per ciascuna delle quattro classi (per un totale di 8 minuti di dati). Utilizzando il menù tre puntini dopo ciascuno dei campioni, selezionarne 2, riservandoli per il set di Test. In alternativa, si può utilizzare lo strumento automatico Train/Test Split tool nella scheda Danger Zone della scheda Dashboard. Di seguito il set di dati risultante:\n\nUna volta acquisito il dataset, lo si può esplorare più in dettaglio utilizzando Data Explorer, uno strumento visivo per trovare valori anomali o dati etichettati in modo errato (aiutando a correggerli). Data Explorer tenta prima di estrarre feature significative dai dati (applicando l’elaborazione del segnale e gli “embedding” della rete neurale) e poi utilizza un algoritmo di riduzione della dimensionalità come PCA o t-SNE per mappare queste feature in uno spazio 2D. Questo fornisce una panoramica immediata del dataset completo.\n\nNel nostro caso, il set di dati sembra OK (buona separazione). Ma il PCA mostra che possiamo avere problemi tra marittimo (verde) e sollevamento (arancione). Ciò è prevedibile, una volta su una barca, a volte il movimento può essere solo “verticale”.",
    "crumbs": [
      "Nicla Vision",
      "Classificazione del Movimento e Rilevamento delle Anomalie"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/motion_classification/motion_classification.it.html#impulse-design",
    "href": "contents/labs/arduino/nicla_vision/motion_classification/motion_classification.it.html#impulse-design",
    "title": "Classificazione del Movimento e Rilevamento delle Anomalie",
    "section": "Impulse Design",
    "text": "Impulse Design\nIl passo successivo è la definizione del nostro Impulse, che prende i dati grezzi e usa l’elaborazione del segnale per estrarre le feature, passandole come tensore di input di un learning block [blocco di apprendimento] per classificare nuovi dati. Si va in Impulse Design e Create Impulse. Studio suggerirà la progettazione di base. Aggiungiamo anche un secondo Learning Block per Anomaly Detection.\n\nQuesto secondo modello usa un modello K-means. Se immaginiamo di poter avere le nostre classi note come cluster, qualsiasi campione che si adatterebbe potrebbe essere un “outlier”, un’anomalia come un container che rotola fuori da una nave in mare o cade da un carrello elevatore.\n\nLa frequenza di campionamento dovrebbe essere catturata automaticamente, in caso contrario, inserirla: [50]Hz. Studio suggerisce una Window Size di 2 secondi ([2000] ms) con una sliding window di [20]ms. Ciò che stiamo definendo in questa fase è che pre-elaboreremo i dati catturati (dati di serie temporali), creando un dataset tabellare (caratteristiche) che saranno l’input per un classificatore di reti neurali (DNN) e un modello di rilevamento delle anomalie (K-Means), come mostrato di seguito:\n\nAnalizziamo attentamente quei passaggi e i parametri per capire meglio cosa stiamo facendo qui.\n\nPanoramica sulla Pre-Elaborazione dei Dati\nLa pre-elaborazione dei dati consiste nell’estrapolare le feature dal set di dati acquisito con l’accelerometro, il che implica l’elaborazione e l’analisi dei dati grezzi. Gli accelerometri misurano l’accelerazione di un oggetto lungo uno o più assi (in genere tre, indicati come X, Y e Z). Queste misure possono essere utilizzate per comprendere vari aspetti del movimento dell’oggetto, come pattern di movimento e vibrazioni.\nI dati grezzi dell’accelerometro possono essere rumorosi e contenere errori o informazioni irrilevanti. Le fasi di pre-elaborazione, come il filtraggio e la normalizzazione, possono pulire e standardizzare i dati, rendendoli più adatti all’estrazione di feature. Nel nostro caso, dovremmo dividere i dati in segmenti più piccoli o windows [finestre]. Ciò può aiutare a concentrarsi su eventi o attività specifici all’interno del dataset, rendendo l’estrazione di feature più gestibile e significativa. La scelta della window size e della sovrapposizione (window increase) dipende dall’applicazione e dalla frequenza degli eventi di interesse. Come regola generale, dovremmo provare a catturare un paio di “cicli di dati”.\n\nCon un “sampling rate” (SR) [frequenza di campionamento] di 50 Hz e una dimensione della finestra di 2 secondi, otterremo 100 campioni per asse, o 300 in totale (3 assi x 2 secondi x 50 campioni). Faremo scorrere questa finestra ogni 200 ms, creando un set di dati più grande in cui ogni istanza ha 300 feature grezze.\n\n\nUna volta che i dati sono stati pre-elaborati e segmentati, si possono estrarre feature che descrivono le caratteristiche del movimento. Alcune feature tipiche estratte dai dati dell’accelerometro includono:\n\nLe feature del Time-domain descrivono le proprietà statistiche dei dati all’interno di ciascun segmento, come media, mediana, deviazione standard, asimmetria, curtosi e tasso di attraversamento dello zero.\nLe feature Frequency-domain si ottengono trasformando i dati nel dominio della frequenza utilizzando tecniche come la Fast Fourier Transform (FFT). Alcune feature tipiche del dominio della frequenza includono lo spettro di potenza, l’energia spettrale, le frequenze dominanti (ampiezza e frequenza) e l’entropia spettrale.\nLe feature del dominio Time-frequency combinano le informazioni del dominio del tempo e della frequenza, come la Short-Time Fourier Transform (STFT) o la Discrete Wavelet Transform (DWT). Possono fornire una comprensione più dettagliata di come il contenuto di frequenza del segnale cambia nel tempo.\n\nIn molti casi, il numero di feature estratte può essere elevato, il che può portare a un overfitting o a una maggiore complessità computazionale. Le tecniche di selezione delle feature, come le informazioni reciproche, i metodi basati sulla correlazione o l’analisi delle componenti principali (PCA), possono aiutare a identificare le feature più rilevanti per una determinata applicazione e ridurre la dimensionalità del dataset. Studio può aiutare con tali calcoli di importanza delle feature.\n\n\nFeature Spettrali di EI Studio\nLa pre-elaborazione dei dati è un’area impegnativa per il machine learning embedded; tuttavia, Edge Impulse aiuta a superarla con la sua fase di pre-elaborazione del segnale digitale (DSP) e, più specificamente, con lo Spectral Features Block.\nIn Studio, il dataset grezzo raccolto sarà l’input di un blocco di Spectral Analysis, che è eccellente per analizzare il movimento ripetitivo, come i dati degli accelerometri. Questo blocco eseguirà un DSP (Digital Signal Processing), estraendo feature come la FFT o le Wavelet.\nPer il nostro progetto, una volta che il segnale temporale è continuo, dovremmo usare FFT con, ad esempio, una lunghezza di [32].\nLe feature Time Domain Statistical per asse/canale sono:\n\nRMS: 1 feature\nSkewness [Asimmetria]: 1 feature\nCurtosi: 1 feature\n\nLe feature Frequency Domain Spectral features per asse/canale sono:\n\nPotenza Spettrale: 16 feature (lunghezza FFT/2)\nAsimmetria: 1 feature\nCurtosi: 1 feature\n\nQuindi, per una lunghezza FFT di 32 punti, l’output risultante dello “Spectral Analysis Block” sarà di 21 feature per asse (un totale di 63 feature).\n\nSi può scoprire di più su come viene calcolata ogni feature scaricando il notebook Edge Impulse - Spectral Features Block Analysis TinyML under the hood: Spectral Analysis o aprendolo direttamente su Google CoLab.\n\n\n\nGenerazione di feature\nUna volta capito cosa fa la pre-elaborazione, è il momento di finire il lavoro. Quindi, prendiamo i dati grezzi (tipo serie temporale) e convertiamoli in dati tabellari. Per farlo, si va alla sezione Spectral Features nella scheda Parameters, si definiscono i parametri principali come discusso nella sezione precedente ([FFT] con [32] punti), e si seleziona [Save Parameters]:\n\nNel menù in alto, si seleziona l’opzione Generate Features e il pulsante Generate Features. Ogni dato della finestra di 2 secondi verrà convertito in un punto dati di 63 feature.\n\nFeature Explorer mostrerà tali dati in 2D utilizzando UMAP. Uniform Manifold Approximation and Projection (UMAP) è una tecnica di riduzione delle dimensioni che può essere utilizzata per la visualizzazione in modo simile a t-SNE, ma è applicabile anche per la riduzione generale delle dimensioni non lineari.\n\nLa visualizzazione consente di verificare che dopo la generazione delle feature, le classi presenti mantengano la loro eccellente separazione, il che indica che il classificatore dovrebbe funzionare bene. Facoltativamente, si può analizzare quanto è importante ciascuna delle feature per una classe rispetto alle altre.",
    "crumbs": [
      "Nicla Vision",
      "Classificazione del Movimento e Rilevamento delle Anomalie"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/motion_classification/motion_classification.it.html#addestramento-dei-modelli",
    "href": "contents/labs/arduino/nicla_vision/motion_classification/motion_classification.it.html#addestramento-dei-modelli",
    "title": "Classificazione del Movimento e Rilevamento delle Anomalie",
    "section": "Addestramento dei Modelli",
    "text": "Addestramento dei Modelli\nIl nostro classificatore sarà una Dense Neural Network (DNN) che avrà 63 neuroni sul suo layer di input, due layer nascosti con 20 e 10 neuroni e un layer di output con quattro neuroni (uno per ogni classe), come mostrato qui:\n\nCome iperparametri, useremo un Learning Rate di [0.005], una dimensione Batch di [32] e [20]% di dati per la convalida per [30] epoche. Dopo l’addestramento, possiamo vedere che l’accuratezza è del 98.5%. Il costo della memoria e della latenza è esiguo.\n\nPer la “Anomaly Detection”, sceglieremo le feature suggerite che sono esattamente le più importanti nell’estrazione delle feature, più l’RMS accZ. Il numero di cluster sarà [32], come suggerito da Studio:",
    "crumbs": [
      "Nicla Vision",
      "Classificazione del Movimento e Rilevamento delle Anomalie"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/motion_classification/motion_classification.it.html#test",
    "href": "contents/labs/arduino/nicla_vision/motion_classification/motion_classification.it.html#test",
    "title": "Classificazione del Movimento e Rilevamento delle Anomalie",
    "section": "Test",
    "text": "Test\nPossiamo verificare come si comporterà il nostro modello con dati sconosciuti utilizzando il 20% dei dati lasciati da parte durante la fase di acquisizione dei dati. Il risultato è stato quasi del 95%, il che è positivo. Si può sempre lavorare per migliorare i risultati, ad esempio, per capire cosa è andato storto con uno dei risultati sbagliati. Se si tratta di una situazione unica, la si può aggiungere al set di dati di training e quindi ripeterla.\nLa soglia minima di default per un risultato considerato incerto è [0.6] per la classificazione e [0.3] per l’anomalia. Una volta che abbiamo quattro classi (la loro somma di output dovrebbe essere 1.0), si può anche impostare una soglia inferiore per una classe da considerare valida (ad esempio, 0.4). Si possono Impostare le soglie di confidenza nel menù tre puntini, oltre al pulsante Classify all.\n\nPuoi anche eseguire “Live Classification” col dispositivo (che dovrebbe essere ancora connesso a Studio).\n\nDa tenere presente che qui, si cattureranno dati reali col dispositivo e si caricheranno in Studio, dove verrà presa un’inferenza utilizzando il modello addestrato (ma il modello NON è nel device).",
    "crumbs": [
      "Nicla Vision",
      "Classificazione del Movimento e Rilevamento delle Anomalie"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/motion_classification/motion_classification.it.html#distribuzione",
    "href": "contents/labs/arduino/nicla_vision/motion_classification/motion_classification.it.html#distribuzione",
    "title": "Classificazione del Movimento e Rilevamento delle Anomalie",
    "section": "Distribuzione",
    "text": "Distribuzione\nÈ il momento di distribuire il blocco di pre-elaborazione e il modello addestrato al Nicla. Studio impacchetterà tutte le librerie necessarie, le funzioni di pre-elaborazione e i modelli addestrati, scaricandoli sul computer. Si deve selezionare l’opzione Arduino Library, e in basso si può scegliere Quantized (Int8) o Unoptimized (float32) e [Build]. Verrà creato un file Zip e scaricato sul computer.\n\nSu Arduino IDE, si va alla scheda Sketch, si seleziona Add.ZIP Library e si sceglie il file .zip scaricato da Studio. Un messaggio apparirà nel terminale IDE: Library installed.\n\nInferenza\nOra è il momento di un vero test. Faremo inferenze completamente scollegate da Studio. Modifichiamo uno degli esempi di codice creati quando si distribuisce la libreria Arduino.\nNell’IDE Arduino, si va alla scheda File/Examples e si cerca il progetto, e negli esempi, si seleziona Nicla_vision_fusion:\n\nNotare che il codice creato da Edge Impulse considera un approccio sensor fusion in cui vengono utilizzati IMU (accelerometro e giroscopio) e ToF. All’inizio del codice, ci sono le librerie relative al nostro progetto, IMU e ToF:\n/* Includes ---------------------------------------------------------------- */\n#include &lt;NICLA_Vision_Movement_Classification_inferencing.h&gt; \n#include &lt;Arduino_LSM6DSOX.h&gt; //IMU\n#include \"VL53L1X.h\" // ToF\n\nSi può mantenere il codice in questo modo per i test perché il modello addestrato utilizzerà solo le funzionalità pre-elaborate dall’accelerometro. Ma si consideri che si scriverà il codice solo con le librerie necessarie per un progetto reale.\n\nE questo è tutto!\nOra si può caricare il codice sul dispositivo e procedere con le inferenze. Si preme due volte il pulsante [RESET] di Nicla per metterlo in modalità boot (disconnetterlo da Studio se è ancora connesso) e caricare lo sketch sulla board.\nOra si devono provare diversi movimenti con la scheda (simili a quelli eseguiti durante l’acquisizione dei dati), osservando il risultato dell’inferenza di ciascuna classe sul Serial Monitor:\n\nClassi Idle e lift:\n\n\n\nMaritime e terrestrial:\n\n\nNotare che in tutte le situazioni sopra, il valore di anomaly score era inferiore a 0.0. Provare un nuovo movimento che non faceva parte del dataset originale, ad esempio, “facendo rotolare” la Nicla, rivolta verso la telecamera capovolta, come un contenitore che cade da una barca o persino un incidente in barca:\n\nRilevamento delle Anomalie\n\n\nIn questo caso, l’anomalia è molto più grande, oltre 1.00\n\n\nPost-elaborazione\nOra che sappiamo che il modello funziona poiché rileva i movimenti, suggeriamo di modificare il codice per vedere il risultato con il NiclaV completamente offline (scollegato dal PC e alimentato da una batteria, un power bank o un alimentatore indipendente da 5V).\nL’idea è di fare lo stesso del progetto KWS: se viene rilevato un movimento specifico, potrebbe accendersi un LED specifico. Ad esempio, se viene rilevato un movimento terrestre, si accenderà il LED verde; se è un movimento marittimo, si accenderà il LED rosso, se è un ascensore, si accenderà il LED blu; e se non viene rilevato alcun movimento (inattivo), i LED saranno SPENTI. Si può anche aggiungere una condizione quando viene rilevata un’anomalia, in questo caso, ad esempio, può essere utilizzato un colore bianco (tutti i LED e si accendono contemporaneamente).",
    "crumbs": [
      "Nicla Vision",
      "Classificazione del Movimento e Rilevamento delle Anomalie"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/motion_classification/motion_classification.it.html#conclusione",
    "href": "contents/labs/arduino/nicla_vision/motion_classification/motion_classification.it.html#conclusione",
    "title": "Classificazione del Movimento e Rilevamento delle Anomalie",
    "section": "Conclusione",
    "text": "Conclusione\n\nI notebook e i codici utilizzati in questo tutorial pratico si trovano nel repository GitHub.\n\nPrima di concludere, considerare che la classificazione del movimento e il rilevamento degli oggetti possono essere utilizzati in molte applicazioni in vari domini. Ecco alcune delle potenziali applicazioni:\n\nApplicazioni di Casi\n\nIndustriale e Manifatturiero\n\nManutenzione Predittiva: Rilevamento di anomalie nel movimento dei macchinari per prevedere guasti prima che si verifichino.\nControllo Qualità: Monitoraggio del movimento di linee di assemblaggio o bracci robotici per la valutazione della precisione e il rilevamento delle deviazioni dal pattern di movimenti standard.\nLogistica di Magazzino: Gestione e tracciamento del movimento delle merci con sistemi automatizzati che classificano diversi tipi di movimento e rilevano anomalie nella movimentazione.\n\n\n\nAssistenza Sanitaria\n\nMonitoraggio Pazienti: Rilevamento di cadute o movimenti anomali negli anziani o in coloro che hanno problemi di mobilità.\nRiabilitazione: Monitoraggio dei progressi dei pazienti in fase di recupero da infortuni tramite classificazione dei pattern di movimento durante le sedute di fisioterapia.\nRiconoscimento Attività: Classificazione dei tipi di attività fisica per applicazioni di fitness o monitoraggio dei pazienti.\n\n\n\nElettronica di Consumo\n\nGesture Control: Interpretazione di movimenti specifici per controllare i dispositivi, come accendere le luci con un gesto della mano.\nGaming: Miglioramento delle esperienze di gioco con input controllati dal movimento.\n\n\n\nTrasporti e Logistica\n\nTelematica dei Veicoli: Monitoraggio del movimento del veicolo per comportamenti insoliti come frenate brusche, curve strette o incidenti.\nMonitoraggio del Carico: Garantire l’integrità delle merci durante il trasporto rilevando movimenti insoliti che potrebbero indicare manomissioni o cattiva gestione.\n\n\n\nCittà Intelligenti e Infrastrutture\n\nMonitoraggio Strutturale: Rilevare vibrazioni o movimenti all’interno delle strutture che potrebbero indicare potenziali guasti o necessità di manutenzione.\nGestione del Traffico: Analizzare il flusso di pedoni o veicoli per migliorare la mobilità e la sicurezza urbana.\n\n\n\nSicurezza e Sorveglianza\n\nRilevamento Intrusi: Rilevare pattern di movimento tipici di accessi non autorizzati o altre violazioni della sicurezza.\nMonitoraggio della Fauna Selvatica: Rilevare bracconieri o movimenti anomali di animali in aree protette.\n\n\n\nAgricoltura\n\nMonitoraggio delle Attrezzature: Monitoraggio delle prestazioni e dell’utilizzo di macchinari agricoli.\nAnalisi del Comportamento Animale: Monitoraggio dei movimenti del bestiame per rilevare comportamenti che indicano problemi di salute o stress.\n\n\n\nMonitoraggio Ambientale\n\nAttività Sismica: Rilevamento di pattern di movimento irregolari che precedono terremoti o altri eventi geologicamente rilevanti.\nOceanografia: Studio di pattern di onde o movimenti marini per scopi di ricerca e sicurezza.\n\n\n\n\nCustodia per Nicla 3D\nPer applicazioni reali, come alcuni hanno descritto in precedenza, possiamo aggiungere una custodia al nostro dispositivo ed Eoin Jordan, di Edge Impulse, ha sviluppato un’ottima custodia indossabile e per la salute delle macchine per la gamma di schede Nicla. Funziona con un magnete da 10 mm, viti da 2M e una cinghia da 16mm per scenari di utilizzo per la salute umana e delle macchine. Ecco il link: Arduino Nicla Voice e Vision Wearable Case.\n\nLe applicazioni per la classificazione del movimento e il rilevamento delle anomalie sono estese e Arduino Nicla Vision è adatto per scenari in cui il basso consumo energetico e l’edge processing sono vantaggiosi. Il suo piccolo fattore di forma e l’efficienza nell’elaborazione lo rendono una scelta ideale per l’implementazione di applicazioni portatili e remote in cui l’elaborazione in tempo reale è fondamentale e la connettività potrebbe essere limitata.",
    "crumbs": [
      "Nicla Vision",
      "Classificazione del Movimento e Rilevamento delle Anomalie"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/motion_classification/motion_classification.it.html#risorse",
    "href": "contents/labs/arduino/nicla_vision/motion_classification/motion_classification.it.html#risorse",
    "title": "Classificazione del Movimento e Rilevamento delle Anomalie",
    "section": "Risorse",
    "text": "Risorse\n\nCodice Arduino\nEdge Impulse Spectral Features Block Colab Notebook\nProgetto Edge Impulse",
    "crumbs": [
      "Nicla Vision",
      "Classificazione del Movimento e Rilevamento delle Anomalie"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/setup/setup.it.html",
    "href": "contents/labs/seeed/xiao_esp32s3/setup/setup.it.html",
    "title": "Setup",
    "section": "",
    "text": "Panoramica\nXIAO ESP32S3 Sense è la scheda di sviluppo conveniente di Seeed Studio, che integra un sensore della fotocamera, un microfono digitale e il supporto per schede SD. Combinando la potenza di elaborazione ML embedded e la capacità fotografica, questa scheda di sviluppo è un ottimo strumento per iniziare con TinyML (IA vocale e visiva).\nCaratteristiche Principali di XIAO ESP32S3 Sense\nDi seguito è riportato il pinout generale della scheda:",
    "crumbs": [
      "XIAO ESP32S3",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/setup/setup.it.html#panoramica",
    "href": "contents/labs/seeed/xiao_esp32s3/setup/setup.it.html#panoramica",
    "title": "Setup",
    "section": "",
    "text": "Potente Scheda MCU: Incorpora il chip del processore Xtensa dual-core ESP32S3 a 32 bit che funziona fino a 240 MHz, più porte di sviluppo montate, supporto per Arduino/MicroPython\nFunzionalità Avanzate: Sensore della fotocamera OV2640 staccabile per una risoluzione di 1600 * 1200, compatibile con il sensore della fotocamera OV5640, integra un microfono digitale aggiuntivo\nProgetto di Alimentazione Elaborato: La capacità di gestione della carica della batteria al litio offre quattro modelli di consumo energetico, che consentono la modalità di “deep sleep” [sospensione profonda] con un consumo energetico basso fino a 14μA\nGrande Memoria per più Possibilità: Offre 8 MB di PSRAM e 8 MB di FLASH, supportando uno slot per schede SD di memoria FAT esterno da 32 GB\nPrestazioni RF Eccezionali: Supporta la comunicazione wireless duale Wi-Fi e BLE a 2,4 GHz, supporta la comunicazione remota a 100m+ se connesso all’antenna U.FL\nDesign Compatto delle Dimensioni di un Pollice: 21 x 17,5 mm, adotta il fattore di forma classico di XIAO, adatto per progetti con spazio limitato come i dispositivi indossabili\n\n\n\n\n\nPer maggiori dettagli, fare riferimento alla pagina Seeed Studio WiKi:  https://wiki.seeedstudio.com/xiao_esp32s3_getting_started/",
    "crumbs": [
      "XIAO ESP32S3",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/setup/setup.it.html#installazione-di-xiao-esp32s3-sense-su-arduino-ide",
    "href": "contents/labs/seeed/xiao_esp32s3/setup/setup.it.html#installazione-di-xiao-esp32s3-sense-su-arduino-ide",
    "title": "Setup",
    "section": "Installazione di XIAO ESP32S3 Sense su Arduino IDE",
    "text": "Installazione di XIAO ESP32S3 Sense su Arduino IDE\nSu Arduino IDE, si va su File &gt; Preferences e si inserisce l’URL:\nhttps://raw.githubusercontent.com/espressif/arduino-esp32/gh-pages/package_esp32_dev_index.json\nnel campo ==&gt; Additional Boards Manager URLs\n\nPoi si apre il gestore delle schede. Si va su Tools &gt; Board &gt; Boards Manager… e immettendo esp32. Selezionare e installare il pacchetto più aggiornato e stabile (evitare le versioni alpha) :\n\n\n⚠️ Attenzione\nLe versioni Alpha (ad esempio, 3.x-alpha) non funzionano correttamente con XIAO ed Edge Impulse. Utilizzare invece l’ultima versione stabile (ad esempio, 2.0.11).\n\nSu Tools, selezionare la Board (XIAO ESP32S3):\n\nUltimo ma non meno importante, scegliere la Porta a cui è collegato l’ESP32S3.\nEcco fatto! Il dispositivo dovrebbe funzionare. Facciamo qualche test.",
    "crumbs": [
      "XIAO ESP32S3",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/setup/setup.it.html#test-della-scheda-con-blink",
    "href": "contents/labs/seeed/xiao_esp32s3/setup/setup.it.html#test-della-scheda-con-blink",
    "title": "Setup",
    "section": "Test della scheda con BLINK",
    "text": "Test della scheda con BLINK\nXIAO ESP32S3 Sense ha un LED integrato che è collegato a GPIO21. Quindi, si può eseguire lo sketch Blink così com’è (utilizzando la costante Arduino LED_BUILTIN) o modificando di conseguenza lo sketch Blink:\n#define LED_BUILT_IN 21 \n\nvoid setup() {\n  pinMode(LED_BUILT_IN, OUTPUT); // Set the pin as output\n}\n\n// Remember that the pin work with inverted logic\n// LOW to Turn on and HIGH to turn off\nvoid loop() {\n  digitalWrite(LED_BUILT_IN, LOW); //Turn on\n  delay (1000); //Wait 1 sec\n  digitalWrite(LED_BUILT_IN, HIGH); //Turn off\n  delay (1000); //Wait 1 sec\n}\n\nNotare che i pin funzionano con logica invertita: BASSO per accendere e ALTO per spegnere.",
    "crumbs": [
      "XIAO ESP32S3",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/setup/setup.it.html#collegamento-del-modulo-sense-scheda-di-espansione",
    "href": "contents/labs/seeed/xiao_esp32s3/setup/setup.it.html#collegamento-del-modulo-sense-scheda-di-espansione",
    "title": "Setup",
    "section": "Collegamento del Modulo Sense (Scheda di Espansione)",
    "text": "Collegamento del Modulo Sense (Scheda di Espansione)\nQuando viene acquistata, la scheda di espansione è separata da quella principale, ma installarla è molto semplice. Si deve allineare il connettore sulla scheda di espansione col connettore B2B sullo XIAO ESP32S3, premerlo con forza e quando si sente un “clic”, l’installazione è completa.\nCome commentato nell’introduzione, la scheda di espansione, o la parte “sense” del dispositivo, ha una fotocamera OV2640 da 1600x1200, uno slot per schede SD e un microfono digitale.",
    "crumbs": [
      "XIAO ESP32S3",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/setup/setup.it.html#test-del-microfono",
    "href": "contents/labs/seeed/xiao_esp32s3/setup/setup.it.html#test-del-microfono",
    "title": "Setup",
    "section": "Test del Microfono",
    "text": "Test del Microfono\nCominciamo con il rilevamento del suono. Si va nei progetti GitHub e si scarica lo sketch: XIAOEsp2s3_Mic_Test poi lo si esegue sull’IDE Arduino:\n\nQuando si produce un suono, è possibile verificarlo sul Serial Plotter.\nSalvare il suono registrato (file audio .wav) su una scheda microSD.\nOra, il lettore di schede SD integrato può salvare file audio .wav. Per farlo, dobbiamo abilitare la PSRAM XIAO.\n\nESP32-S3 ha solo poche centinaia di kilobyte di RAM interna sul chip MCU. Questo può essere insufficiente per alcuni scopi, quindi fino a 16 MB di PSRAM esterna (RAM pseudo-statica) possono essere collegati al chip flash SPI. La memoria esterna è incorporata nella mappa di memoria e, con alcune restrizioni, è utilizzabile allo stesso modo della RAM dati interna.\n\nPer iniziare, inserire la scheda SD su XIAO come mostrato nella foto qui sotto (la scheda SD deve essere formattata in FAT32).\n\n\nScaricare lo sketch Wav_Record, che si può trovare su GitHub.\nPer eseguire il codice (Wav Record), è necessario utilizzare la funzione PSRAM del chip ESP-32, quindi la si deve attivare prima di caricare: Tools&gt;PSRAM: “OPI PSRAM”&gt;OPI PSRAM\n\n\n\nEseguire il codice Wav_Record.ino\nQuesto programma viene eseguito solo una volta dopo che l’utente accende il monitor seriale. Registra per 20 secondi e salva il file su una scheda microSD come “arduino_rec.wav.”\nQuando “.” viene emesso ogni 1 secondo nel monitor seriale, l’esecuzione del programma è terminata e si può riprodurre il file audio registrato con l’aiuto di un lettore di schede.\n\n\nLa qualità del suono è eccellente!\n\nLa spiegazione di come funziona il codice va oltre lo scopo di questo tutorial, ma c’è un’eccellente descrizione sulla pagina wiki.",
    "crumbs": [
      "XIAO ESP32S3",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/setup/setup.it.html#test-della-fotocamera",
    "href": "contents/labs/seeed/xiao_esp32s3/setup/setup.it.html#test-della-fotocamera",
    "title": "Setup",
    "section": "Test della Fotocamera",
    "text": "Test della Fotocamera\nPer testare la fotocamera, si deve scaricare la cartella take_photos_command da GitHub. La cartella contiene lo sketch (.ino) e due file .h con i dettagli della fotocamera.\n\nEseguire il codice: take_photos_command.ino. Aprire il Serial Monitor e inviare il comando capture per catturare e salvare l’immagine sulla scheda SD:\n\n\nVerificare che [Both NL & CR] sia selezionato sul Serial Monitor.\n\n\nEcco un esempio di una foto scattata:",
    "crumbs": [
      "XIAO ESP32S3",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/setup/setup.it.html#test-del-wifi",
    "href": "contents/labs/seeed/xiao_esp32s3/setup/setup.it.html#test-del-wifi",
    "title": "Setup",
    "section": "Test del WiFi",
    "text": "Test del WiFi\nUno dei punti di forza di XIAO ESP32S3 è la sua capacità WiFi. Quindi, testiamo la sua radio eseguendo la scansione delle reti Wi-Fi attorno ad essa. Lo si può fare eseguendo uno degli esempi di codice sulla scheda.\nSi va su Arduino IDE Examples e si cerca WiFI ==&gt; WiFIScan\nSi dovrebbero vedere le reti Wi-Fi (SSID e RSSI) nel raggio d’azione del dispositivo sul monitor seriale. Ecco cosa abbiamo ottenuto in laboratorio:\n\nUn Semplice Server WiFi (accensione/spegnimento del LED)\nTestiamo la capacità del dispositivo di comportarsi come un server WiFi. Ospiteremo una semplice pagina sul dispositivo che invia comandi per accendere e spegnere il LED integrato di XIAO.\nCome prima, si va su GitHub per scaricare la cartella usando lo sketch SimpleWiFiServer.\nPrima di eseguire lo sketch, si devono inserire le credenziali di rete:\nconst char* ssid     = \"Your credentials here\";\nconst char* password = \"Your credentials here\";\nSi può monitorare il funzionamento del server con Serial Monitor.\n\nPrendere l’indirizzo IP e inserirlo nel browser:\n\nSi vedrà una pagina con link che possono accendere e spegnere il LED integrato del XIAO.\nStreaming video sul Web\nOra che sappiamo che si possono inviare comandi dalla pagina Web al dispositivo, facciamo il contrario. Prendiamo l’immagine catturata dalla telecamera e la trasmettiamo in streaming su una pagina Web:\nScaricare da GitHub la cartella che contiene il codice: XIAO-ESP32S3-Streeming_Video.ino.\n\nRicordarsi che la cartella contiene il file .ino e un paio di file .h necessari per gestire la telecamera.\n\nInserire le credenziali ed eseguire lo sketch. Sul monitor seriale, si può trovare l’indirizzo della pagina da inserire nel browser:\n\nApri la pagina sul browser (attendere qualche secondo per avviare lo streaming). Tutto qui.\n\nMettere in streaming ciò che la telecamera “vede” può essere importante quando la si posiziona per catturare un set di dati per un progetto ML (ad esempio, usando il codice “take_phots_commands.ino”.\nNaturalmente, possiamo fare entrambe le cose contemporaneamente: mostrare ciò che la telecamera vede sulla pagina e inviare un comando per catturare e salvare l’immagine sulla scheda SD. Per questo, si può usare il codice Camera_HTTP_Server_STA, scaricabile da GitHub.\n\nIl programma eseguirà le seguenti attività:\n\nImposta la fotocamera in modalità di output JPEG.\nCrea una pagina Web (ad esempio ==&gt; http://192.168.4.119//). L’indirizzo corretto verrà visualizzato sul Serial Monitor.\nSe server.on (“/capture”, HTTP_GET, serverCapture), il programma scatta una foto e la invia al Web.\nÈ possibile ruotare l’immagine sulla pagina Web utilizzando il pulsante [ROTATE]\nIl comando [CAPTURE] visualizzerà solo l’anteprima dell’immagine sulla pagina Web, mostrandone le dimensioni sul Serial Monitor\nIl comando [SAVE] salverà un’immagine sulla scheda SD e la mostrerà sul browser.\nLe immagini salvate seguiranno una denominazione sequenziale (image1.jpg, image2.jpg.\n\n\n\nQuesto programma può catturare un set di dati di immagini con un progetto di classificazione delle immagini.\n\nEsaminate il codice; sarà più facile capire come funziona la fotocamera. Questo codice è stato sviluppato sulla base del fantastico tutorial di Rui Santos ESP32-CAM Take Photo and Display in Web Server, che invito tutti a visitare.\nUso di CameraWebServer\nNell’IDE Arduino, si va su File &gt; Examples &gt; ESP32 &gt; Camera e si seleziona CameraWebServer\nSi devono anche commentare tutti i modelli di fotocamere, eccetto i pin del modello XIAO:\n#define CAMERA_MODEL_XIAO_ESP32S3 // Has PSRAM\nNon dimenticare Tools per abilitare la PSRAM.\nInserire le credenziali wifi e caricare il codice sul dispositivo:\n\nSe il codice viene eseguito correttamente, si vedrà l’indirizzo sul monitor seriale:\n\nCopiare l’indirizzo sul browser e attendere che la pagina venga caricata. Selezionare la risoluzione della telecamera (ad esempio, QVGA) e selezionare [START STREAM]. Attendi qualche secondo/minuto, a seconda della connessione. Utilizzando il pulsante [Save], si può salvare un’immagine nell’area download del computer.\n\nEcco fatto! Si possono salvare le immagini direttamente sul computer per usarle nei progetti.",
    "crumbs": [
      "XIAO ESP32S3",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/setup/setup.it.html#conclusione",
    "href": "contents/labs/seeed/xiao_esp32s3/setup/setup.it.html#conclusione",
    "title": "Setup",
    "section": "Conclusione",
    "text": "Conclusione\nThe XIAO ESP32S3 Sense è flessibile, economico e facile da programmare. Con 8 MB di RAM, la memoria non è un problema e il dispositivo può gestire molte attività di post-elaborazione, inclusa la comunicazione.\nL’ultima versione del codice si trova nel repository GitHub: XIAO-ESP32S3-Sense.",
    "crumbs": [
      "XIAO ESP32S3",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/setup/setup.it.html#risorse",
    "href": "contents/labs/seeed/xiao_esp32s3/setup/setup.it.html#risorse",
    "title": "Setup",
    "section": "Risorse",
    "text": "Risorse\n\nXIAO ESP32S3 Code",
    "crumbs": [
      "XIAO ESP32S3",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/image_classification/image_classification.it.html",
    "href": "contents/labs/seeed/xiao_esp32s3/image_classification/image_classification.it.html",
    "title": "Classificazione delle Immagini",
    "section": "",
    "text": "Panoramica\nSempre di più, ci troviamo di fronte a una rivoluzione dell’intelligenza artificiale (IA) in cui, come affermato da Gartner, Edge AI ha un potenziale di impatto molto elevato, ed è ora!\nIn prima linea nel Radar delle Tecnologie Emergenti c’è il linguaggio universale di Edge Computer Vision. Quando esaminiamo il Machine Learning (ML) applicato alla visione, il primo concetto che ci accoglie è la classificazione delle immagini, una specie “Hello World” del ML che è sia semplice che profondo!\nSeeed Studio XIAO ESP32S3 Sense è un potente strumento che combina il supporto per fotocamera e scheda SD. Con la sua potenza di elaborazione ML embedded e la capacità di fotografia, è un ottimo punto di partenza per esplorare l’intelligenza artificiale per la visione TinyML.",
    "crumbs": [
      "XIAO ESP32S3",
      "Classificazione delle Immagini"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/image_classification/image_classification.it.html#un-progetto-tinyml-di-motion-classification-frutta-contro-verdura",
    "href": "contents/labs/seeed/xiao_esp32s3/image_classification/image_classification.it.html#un-progetto-tinyml-di-motion-classification-frutta-contro-verdura",
    "title": "Classificazione delle Immagini",
    "section": "Un progetto TinyML di Motion Classification: Frutta contro Verdura",
    "text": "Un progetto TinyML di Motion Classification: Frutta contro Verdura\n\nL’idea di base del nostro progetto sarà quella di addestrare un modello e procedere con l’inferenza su XIAO ESP32S3 Sense. Per l’addestramento, dovremmo trovare alcuni dati (in effetti, tonnellate di dati!).\nMa prima di tutto, abbiamo bisogno di un obiettivo! Cosa vogliamo classificare?\nCon TinyML, un set di tecniche associate all’inferenza di apprendimento automatico su dispositivi embedded, dovremmo limitare la classificazione a tre o quattro categorie a causa di limitazioni (principalmente della memoria). Differenziamo mele da banane e patate (si possono provare altre categorie).\nQuindi, cerchiamo un set di dati specifico che includa immagini da quelle categorie. Kaggle è un buon inizio:\nhttps://www.kaggle.com/kritikseth/fruit-and-vegetable-image-recognition\nQuesto set di dati contiene immagini dei seguenti alimenti:\n\nFrutta - banana, mela, pera, uva, arancia, kiwi, anguria, melograno, ananas, mango.\nVerdura - cetriolo, carota, peperone, cipolla, patata, limone, pomodoro, ravanello, barbabietola, cavolo, lattuga, spinaci, soia, cavolfiore, peperone, peperoncino, rapa, mais, mais dolce, patata dolce, paprika, jalepeño, zenzero, aglio, piselli, melanzane.\n\nOgni categoria è suddivisa in train [addestramento] (100 immagini), test (10 immagini) e validation (10 immagini).\n\nScaricare il dataset dal sito Web di Kaggle e metterlo sul computer.\n\n\nFacoltativamente, si possono aggiungere alcune foto fresche di banane, mele e patate direttamente dalla cucina di casa, utilizzando, ad esempio, il codice discusso nel lab della configurazione.",
    "crumbs": [
      "XIAO ESP32S3",
      "Classificazione delle Immagini"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/image_classification/image_classification.it.html#addestramento-del-modello-con-edge-impulse-studio",
    "href": "contents/labs/seeed/xiao_esp32s3/image_classification/image_classification.it.html#addestramento-del-modello-con-edge-impulse-studio",
    "title": "Classificazione delle Immagini",
    "section": "Addestramento del modello con Edge Impulse Studio",
    "text": "Addestramento del modello con Edge Impulse Studio\nUtilizzeremo Edge Impulse Studio per addestrare il modello. Come si saprà, Edge Impulse è una piattaforma di sviluppo leader per l’apprendimento automatico su dispositivi edge.\nInserire le credenziali del proprio account (o crearne uno gratuito) su Edge Impulse. Quindi, creare un nuovo progetto:\n\n\nRaccolta Dati\nSuccessivamente, nella sezione UPLOAD DATA, caricare dal computer i file delle categorie scelte:\n\nSarebbe meglio se ora si avesse il proprio set di dati di training suddiviso in tre classi di dati:\n\n\nSi possono caricare dati extra per ulteriori test del modello o dividere i dati di training. Lo lasceremo così com’è per utilizzare più dati possibili.\n\n\n\nImpulse Design\n\nUn impulso prende dati grezzi (in questo caso, immagini), estrae feature (ridimensiona le immagini) e poi utilizza un blocco di apprendimento per classificare nuovi dati.\n\nLa classificazione delle immagini è l’uso più comune del deep learning, ma per portare a termine questo compito dovrebbero essere utilizzati molti dati. Abbiamo circa 90 immagini per ogni categoria. Questo numero è sufficiente? No, per niente! Avremo bisogno di migliaia di immagini per “insegnare o modellare” per distinguere una mela da una banana. Ma possiamo risolvere questo problema riaddestrando un modello precedentemente addestrato con migliaia di immagini. Chiamiamo questa tecnica “Transfer Learning” (TL).\n\nCol TL, possiamo mettere a punto un modello di classificazione delle immagini pre-addestrato sui nostri dati, che funziona bene anche con set di dati di immagini relativamente piccoli (il nostro caso).\nQuindi, partendo dalle immagini grezze, le ridimensioneremo (96x96) pixel e le invieremo al nostro blocco Transfer Learning:\n\n\nPre-elaborazione (Generazione di Feature)\nOltre a ridimensionare le immagini, possiamo cambiarle in scala di grigi o mantenere la profondità di colore RGB effettiva. Iniziamo selezionando Grayscale. In questo modo, ognuno dei nostri campioni di dati avrà dimensione 9.216 feature (96x96x1). Con RGB, questa dimensione sarebbe tre volte più grande. Lavorare con la scala di grigi aiuta a ridurre la quantità di memoria finale necessaria per l’inferenza.\n\nRicordarsi di [Save parameters]. Questo genererà le feature da utilizzare nel training.\n\n\nProgettazione del Modello\nTransfer Learning\nNel 2007, Google ha introdotto MobileNetV1, una famiglia di reti neurali di visione artificiale di uso generale progettate pensando ai dispositivi mobili per supportare classificazione, rilevamento e altro. Le MobileNet sono modelli piccoli, a bassa latenza e a basso consumo, parametrizzati per soddisfare i vincoli di risorse di vari casi d’uso.\nSebbene l’architettura di base di MobileNet sia già minuscola e abbia una bassa latenza, spesso un caso d’uso o un’applicazione specifica potrebbe richiedere che il modello sia più piccolo e veloce. MobileNet introduce un parametro semplice α (alfa) chiamato moltiplicatore di larghezza per costruire questi modelli più piccoli e meno costosi dal punto di vista computazionale. Il ruolo del moltiplicatore di larghezza α è di assottigliare una rete in modo uniforme a ogni livello.\nEdge Impulse Studio ha MobileNet V1 (immagini 96x96) e V2 (immagini 96x96 e 160x160) disponibili, con diversi valori α (da 0,05 a 1,0). Ad esempio, si otterrà la massima accuratezza con V2, immagini 160x160 e α=1,0. Naturalmente, c’è un compromesso. Maggiore è la precisione, maggiore sarà la memoria (circa 1,3 M di RAM e 2,6 M di ROM) necessaria per eseguire il modello, il che implica una maggiore latenza.\nL’ingombro più piccolo sarà ottenuto all’altro estremo con MobileNet V1 e α=0,10 (circa 53,2 K di RAM e 101 K di ROM).\nPer questo primo passaggio, utilizzeremo MobileNet V1 e α=0,10.\n\n\n\nTraining\nData Augmentation\nUn’altra tecnica necessaria da usare con il deep learning è il data augmentation. Il data augmentation è un metodo che può aiutare a migliorare l’accuratezza dei modelli di machine learning, creando dati artificiali aggiuntivi. Un sistema di Data Augmentation apporta piccole modifiche casuali ai dati di training (ad esempio capovolgendo, ritagliando o ruotando le immagini).\nSotto, qui si può vedere come Edge Impulse implementa una policy di data Augmentation sui dati:\n# Implements the data augmentation policy\ndef augment_image(image, label):\n    # Flips the image randomly\n    image = tf.image.random_flip_left_right(image)\n\n    # Increase the image size, then randomly crop it down to\n    # the original dimensions\n    resize_factor = random.uniform(1, 1.2)\n    new_height = math.floor(resize_factor * INPUT_SHAPE[0])\n    new_width = math.floor(resize_factor * INPUT_SHAPE[1])\n    image = tf.image.resize_with_crop_or_pad(image, new_height, new_width)\n    image = tf.image.random_crop(image, size=INPUT_SHAPE)\n\n    # Vary the brightness of the image\n    image = tf.image.random_brightness(image, max_delta=0.2)\n\n    return image, label\nL’esposizione a queste variazioni durante l’addestramento può aiutare a impedire al modello di prendere scorciatoie “memorizzando” indizi superficiali nei dati di addestramento, il che significa che potrebbe riflettere meglio i pattern profondi in esame nel set di dati.\nL’ultimo layer del nostro modello avrà 16 neuroni con un dropout del 10% per prevenire l’overfitting. Ecco l’output del Training:\n\nIl risultato potrebbe essere migliore. Il modello ha raggiunto circa il 77% di accuratezza, ma la quantità di RAM prevista per essere utilizzata durante l’inferenza è relativamente piccola (circa 60 KByte), il che è molto buono.\n\n\nDeployment\nIl modello addestrato verrà distribuito come libreria Arduino .zip:\n\nApri l’IDE Arduino e in Sketch, si va su Include Library e add.ZIP Library. Selezionare il file che scaricato da Edge Impulse Studio e il gioco è fatto!\n\nNella scheda Examples su Arduino IDE, si trova un codice sketch sotto il nome del proprio progetto.\n\nAprire l’esempio Static Buffer:\n\nLa prima riga di codice è esattamente la chiamata di una libreria con tutto il necessario per eseguire l’inferenza sul dispositivo.\n#include &lt;XIAO-ESP32S3-CAM-Fruits-vs-Veggies_inferencing.h&gt;\nOvviamente, questo è un codice generico (un “template”) che ottiene solo un campione di dati grezzi (memorizzati nella variabile: features = {} ed esegue il classificatore, eseguendo l’inferenza. Il risultato viene mostrato sul monitor seriale.\nDovremmo ottenere il campione (immagine) dalla fotocamera e pre-elaborarlo (ridimensionandolo a 96x96, convertendolo in scala di grigi e appiattendolo). Questo sarà il tensore di input del nostro modello. Il tensore di output sarà un vettore con tre valori (etichette), che mostrano le probabilità di ciascuna delle classi.\n\nTornando al progetto (Tab Image), copiare uno dei Raw Data Sample:\n\n9.216 feature verranno copiate negli appunti. Questo è il tensore di input (un’immagine appiattita di 96x96x1), in questo caso, banane. “Incollare” questo tensore di input su features[] = {0xb2d77b, 0xb5d687, 0xd8e8c0, 0xeaecba, 0xc2cf67, ...}\n\nEdge Impulse ha incluso la libreria ESP NN nel suo SDK, che contiene funzioni NN (Neural Network) ottimizzate per vari chip Espressif, tra cui ESP32S3 (in esecuzione su Arduino IDE).\nQuando si esegue l’inferenza, si deve ottenere il punteggio più alto per “banana”.\n\nOttime notizie! Il nostro dispositivo gestisce un’inferenza, scoprendo che l’immagine in ingresso è una banana. Notare inoltre che il tempo di inferenza è stato di circa 317 ms, con un massimo di 3 fps se si è provato a classificare le immagini da un video.\nOra, dovremmo incorporare la telecamera e classificare le immagini in tempo reale.\nSi va su Arduino IDE Examples e si scarica dal progetto lo sketch esp32_camera:\n\nSi devono cambiare le righe dalla 32 alla 75, che definiscono il modello e i pin della telecamera, utilizzando i dati relativi al nostro modello. Copiare e incollare le righe seguenti, sostituendo le righe 32-75:\n#define PWDN_GPIO_NUM     -1 \n#define RESET_GPIO_NUM    -1 \n#define XCLK_GPIO_NUM     10 \n#define SIOD_GPIO_NUM     40 \n#define SIOC_GPIO_NUM     39\n#define Y9_GPIO_NUM       48 \n#define Y8_GPIO_NUM       11 \n#define Y7_GPIO_NUM       12 \n#define Y6_GPIO_NUM       14 \n#define Y5_GPIO_NUM       16 \n#define Y4_GPIO_NUM       18 \n#define Y3_GPIO_NUM       17 \n#define Y2_GPIO_NUM       15 \n#define VSYNC_GPIO_NUM    38 \n#define HREF_GPIO_NUM     47 \n#define PCLK_GPIO_NUM     13\nEcco il codice risultante:\n\nLo sketch modificato è scaricabile da GitHub: xiao_esp32s3_camera.\n\nNotare che si possono facoltativamente mantenere i pin come file .h come abbiamo fatto nel Setup Lab.\n\nCaricare il codice sullo XIAO ESP32S3 Sense e si sarà pronti per iniziare a classificare la frutta e la verdura! Si può controllare il risultato su Serial Monitor.",
    "crumbs": [
      "XIAO ESP32S3",
      "Classificazione delle Immagini"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/image_classification/image_classification.it.html#test-del-modello-inferenza",
    "href": "contents/labs/seeed/xiao_esp32s3/image_classification/image_classification.it.html#test-del-modello-inferenza",
    "title": "Classificazione delle Immagini",
    "section": "Test del modello (inferenza)",
    "text": "Test del modello (inferenza)\n\nScattando una foto con la fotocamera, il risultato della classificazione apparirà su Serial Monitor:\n\nAltri test:",
    "crumbs": [
      "XIAO ESP32S3",
      "Classificazione delle Immagini"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/image_classification/image_classification.it.html#test-con-un-modello-più-grande",
    "href": "contents/labs/seeed/xiao_esp32s3/image_classification/image_classification.it.html#test-con-un-modello-più-grande",
    "title": "Classificazione delle Immagini",
    "section": "Test con un Modello Più Grande",
    "text": "Test con un Modello Più Grande\nOra, passiamo all’altro lato delle dimensioni del modello. Selezioniamo un MobilinetV2 96x96 0.35, con immagini RGB in input.\n\nAnche con un modello più grande, la precisione potrebbe essere migliore e la quantità di memoria necessaria per eseguire il modello aumenta di cinque volte, con una latenza che aumenta di sette volte.\n\nNotare che le prestazioni qui sono stimate con un dispositivo più piccolo, l’ESP-EYE. L’inferenza effettiva con l’ESP32S3 dovrebbe essere migliore.\n\nPer migliorare il nostro modello, dovremo addestrare più immagini.\nAnche se il nostro modello non ha migliorato la precisione, testiamo se l’XIAO può gestire un modello così grande. Faremo un semplice test di inferenza con lo sketch Static Buffer.\nRidistribuiamo il modello. Se il compilatore EON è abilitato quando si genera la libreria, la memoria totale necessaria per l’inferenza dovrebbe essere ridotta, ma ciò non influisce sulla precisione.\n\n⚠️ Attenzione - Xiao ESP32S3 con PSRAM abilitata ha memoria sufficiente per eseguire l’inferenza, anche in un modello così grande. Mantenere il Compilatore EON NOT ENABLED.\n\n\nFacendo un’inferenza con MobilinetV2 96x96 0.35, con immagini RGB in input, la latenza è stata di 219 ms, il che è ottimo per un modello così grande.\n\nPer il test, possiamo addestrare di nuovo il modello, usando la versione più piccola di MobileNet V2, con un alpha di 0,05. È interessante che il risultato in accuratezza sia stato più alto.\n\n\nNotare che la latenza stimata per un Arduino Portenta (o Nicla), in esecuzione con un clock di 480 MHz è di 45 ms.\n\nDistribuendo il modello, abbiamo ottenuto un’inferenza di soli 135 ms, ricordando che XIAO funziona con metà del clock utilizzato da Portenta/Nicla (240 MHz):",
    "crumbs": [
      "XIAO ESP32S3",
      "Classificazione delle Immagini"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/image_classification/image_classification.it.html#esecuzione-dellinferenza-su-sensecraft-web-toolkit",
    "href": "contents/labs/seeed/xiao_esp32s3/image_classification/image_classification.it.html#esecuzione-dellinferenza-su-sensecraft-web-toolkit",
    "title": "Classificazione delle Immagini",
    "section": "Esecuzione dell’inferenza su SenseCraft-Web-Toolkit",
    "text": "Esecuzione dell’inferenza su SenseCraft-Web-Toolkit\nUna limitazione significativa della visualizzazione dell’inferenza su Arduino IDE è che non possiamo vedere su cosa punta la telecamera. Una buona alternativa è SenseCraft-Web-Toolkit, uno strumento di distribuzione del modello visivo fornito da SSCMA(Seeed SenseCraft Model Assistant). Questo strumento consente di distribuire facilmente modelli su varie piattaforme tramite semplici operazioni. Lo strumento offre un’interfaccia intuitiva e non richiede alcuna codifica.\nSeguire i seguenti passaggi per avviare SenseCraft-Web-Toolkit:\n\nAprire il sito web di SenseCraft-Web-Toolkit.\nCollega XIAO al computer:\n\n\nDopo aver collegato XIAO, selezionarlo come di seguito:\n\n\n\nSelezionare il dispositivo/Porta e premere [Connect]:\n\n\n\nSi possono provare diversi modelli di Computer Vision caricati in precedenza da Seeed Studio. Da provare e verificarli!\n\nNel nostro caso, useremo il pulsante blu in fondo alla pagina: [Upload Custom AI Model].\nMa prima, dobbiamo scaricare da Edge Impulse Studio il modello quantized.tflite.\n\nSi va sul proprio progetto su Edge Impulse Studio, oppure si clona questo:\n\n\nXIAO-ESP32S3-CAM-Fruits-vs-Veggies-v1-ESP-NN\n\n\nNella Dashboard, scaricare il modello (“block output”): Transfer learning model - TensorFlow Lite (int8 quantized).\n\n\n\nSu SenseCraft-Web-Toolkit, usare il pulsante blu in fondo alla pagina: [Upload Custom AI Model]. Si aprirà una finestra. Inserire il file del Modello scaricato sul computer da Edge Impulse Studio, scegliere un nome del modello e inserirlo con le etichette (ID: Object):\n\n\n\nNotare che si devono usare le etichette addestrate su EI Studio, inserendole in ordine alfabetico (nel nostro caso: apple, banana, potato).\n\nDopo alcuni secondi (o minuti), il modello verrà caricato sul dispositivo e l’immagine della telecamera apparirà in tempo reale nel Preview Sector:\n\nIl risultato della classificazione sarà in cima all’immagine. Si può anche selezionare la “Confidence” del cursore di inferenza Confidence.\nCliccando sul pulsante in alto (Device Log), si può aprire un Serial Monitor per seguire l’inferenza, come fatto con l’IDE Arduino:\n\nSu Device Log, si otterranno informazioni come:\n\n\nTempo di pre-elaborazione (cattura immagine e Crop): 4ms,\nTempo di inferenza (latenza modello): 106ms,\nTempo di Post-elaborazione (visualizzazione dell’immagine e inclusione dei dati): 0ms,\nTensore di output (classi), ad esempio: [[89,0]]; dove 0 è Apple (1 è banana e 2 è patata).\n\nEcco altri screenshot:",
    "crumbs": [
      "XIAO ESP32S3",
      "Classificazione delle Immagini"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/image_classification/image_classification.it.html#conclusione",
    "href": "contents/labs/seeed/xiao_esp32s3/image_classification/image_classification.it.html#conclusione",
    "title": "Classificazione delle Immagini",
    "section": "Conclusione",
    "text": "Conclusione\nXIAO ESP32S3 Sense è molto flessibile, poco costoso e facile da programmare. Il progetto dimostra il potenziale di TinyML. La memoria non è un problema; il dispositivo può gestire molte attività di post-elaborazione, tra cui la comunicazione.\nL’ultima versione del codice si trova nel repository GitHub: XIAO-ESP32S3-Sense.",
    "crumbs": [
      "XIAO ESP32S3",
      "Classificazione delle Immagini"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/image_classification/image_classification.it.html#risorse",
    "href": "contents/labs/seeed/xiao_esp32s3/image_classification/image_classification.it.html#risorse",
    "title": "Classificazione delle Immagini",
    "section": "Risorse",
    "text": "Risorse\n\nCodici XIAO ESP32S3\nDataset\nProgetto Edge Impulse",
    "crumbs": [
      "XIAO ESP32S3",
      "Classificazione delle Immagini"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.it.html",
    "href": "contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.it.html",
    "title": "Rilevamento degli Oggetti",
    "section": "",
    "text": "Panoramica\nNell’ultima sezione riguardante Computer Vision (CV) e XIAO ESP32S3, Classificazione delle immagini, abbiamo imparato come impostare e classificare le immagini con questa straordinaria scheda di sviluppo. Continuando il nostro viaggio con CV, esploreremo il Rilevamento degli oggetti sui microcontrollori.",
    "crumbs": [
      "XIAO ESP32S3",
      "Rilevamento degli Oggetti"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.it.html#panoramica",
    "href": "contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.it.html#panoramica",
    "title": "Rilevamento degli Oggetti",
    "section": "",
    "text": "Object Detection e Image Classification\nIl compito principale con i modelli di Classificazione delle immagini è identificare la categoria di oggetti più probabile presente su un’immagine, ad esempio, per classificare tra un gatto o un cane, gli “oggetti” dominanti in un’immagine:\n\nMa cosa succede se non c’è una categoria dominante nell’immagine?\n\nUn modello di classificazione delle immagini identifica l’immagine soprastante in modo completamente sbagliato come un “ashcan”, probabilmente a causa delle tonalità di colore.\n\nIl modello utilizzato nelle immagini precedenti è MobileNet, che è addestrato con un ampio set di dati, ImageNet, in esecuzione su un Raspberry Pi.\n\nPer risolvere questo problema, abbiamo bisogno di un altro tipo di modello, in cui non solo possono essere trovate più categorie (o etichette), ma anche dove si trovano gli oggetti in una determinata immagine.\nCome possiamo immaginare, tali modelli sono molto più complicati e più grandi, ad esempio, MobileNetV2 SSD FPN-Lite 320x320, addestrato con il set di dati COCO. Questo modello di rilevamento degli oggetti pre-addestrato è progettato per individuare fino a 10 oggetti all’interno di un’immagine, generando un riquadro di delimitazione per ogni oggetto rilevato. L’immagine sottostante è il risultato di un tale modello in esecuzione su un Raspberry Pi:\n\nI modelli utilizzati per il rilevamento di oggetti (come MobileNet SSD o YOLO) hanno solitamente dimensioni di diversi MB, il che è OK per l’uso con Raspberry Pi ma non è adatto per l’uso con dispositivi embedded, dove la RAM di solito ha, al massimo, pochi MB come nel caso di XIAO ESP32S3.\n\n\nUna soluzione innovativa per il Rilevamento di Oggetti: FOMO\nEdge Impulse ha lanciato nel 2022, FOMO (Faster Objects, More Objects), una nuova soluzione per eseguire il rilevamento di oggetti su dispositivi embedded, come Nicla Vision e Portenta (Cortex M7), su CPU Cortex M4F (serie Arduino Nano33 e OpenMV M4) e sui dispositivi Espressif ESP32 (ESP-CAM, ESP-EYE e XIAO ESP32S3 Sense).\nIn questo progetto pratico, esploreremo l’Object Detection utilizzando FOMO.\n\nPer saperne di più sulla FOMO, si può leggere l’annuncio ufficiale su FOMO di Edge Impulse, dove Louis Moreau e Mat Kelcey spiegano in dettaglio come funziona.",
    "crumbs": [
      "XIAO ESP32S3",
      "Rilevamento degli Oggetti"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.it.html#obiettivo-del-progetto-di-object-detection",
    "href": "contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.it.html#obiettivo-del-progetto-di-object-detection",
    "title": "Rilevamento degli Oggetti",
    "section": "Obiettivo del Progetto di Object Detection",
    "text": "Obiettivo del Progetto di Object Detection\nTutti i progetti di apprendimento automatico devono iniziare con un obiettivo dettagliato. Supponiamo di trovarci in una struttura industriale o rurale e di dover smistare e contare arance (frutti) e in particolare rane (insetti).\n\nIn altre parole, dovremmo eseguire una classificazione multi-etichetta, in cui ogni immagine può avere tre classi:\n\nBackground [Sfondo] (nessun oggetto)\nFruit\nBug\n\nEcco alcuni esempi di immagini non etichettate che dovremmo utilizzare per rilevare gli oggetti (frutti e insetti):\n\nSiamo interessati a quale oggetto è presente nell’immagine, alla sua posizione (centroide) e a quanti ne possiamo trovare su di essa. La dimensione dell’oggetto non viene rilevata con FOMO, come con MobileNet SSD o YOLO, in cui il Bounding Box è uno degli output del modello.\nSvilupperemo il progetto utilizzando XIAO ESP32S3 per l’acquisizione di immagini e l’inferenza del modello. Il progetto ML verrà sviluppato utilizzando Edge Impulse Studio. Ma prima di iniziare il progetto di “object detection” in Studio, creiamo un dataset grezzo (non etichettato) con immagini che contengono gli oggetti da rilevare.",
    "crumbs": [
      "XIAO ESP32S3",
      "Rilevamento degli Oggetti"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.it.html#raccolta-dati",
    "href": "contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.it.html#raccolta-dati",
    "title": "Rilevamento degli Oggetti",
    "section": "Raccolta Dati",
    "text": "Raccolta Dati\nSi possono catturare immagini usando XIAO, il telefono o altri dispositivi. Qui, useremo XIAO con codice dalla libreria Arduino IDE ESP32.\n\nRaccolta di Dataset con XIAO ESP32S3\nAprire Arduino IDE e selezionare la scheda XIAO_ESP32S3 (e la porta a cui è collegata). Su File &gt; Examples &gt; ESP32 &gt; Camera, selezionare CameraWebServer.\nNel pannello BOARDS MANAGER, confermare di aver installato l’ultimo pacchetto “stable”.\n\n⚠️ Attenzione\nLe versioni Alpha (ad esempio, 3.x-alpha) non funzionano correttamente con XIAO ed Edge Impulse. Utilizzare invece l’ultima versione stabile (ad esempio, 2.0.11).\n\nSi devono anche commentare tutti i modelli di fotocamere, eccetto i pin del modello XIAO:\n#define CAMERA_MODEL_XIAO_ESP32S3 // Has PSRAM\nE su Tools, abilitare la PSRAM. Inserisci le credenziali wifi e caricare il codice sul dispositivo:\n\nSe il codice viene eseguito correttamente, si vedrà l’indirizzo sul monitor seriale:\n\nCopiare l’indirizzo sul browser e attendere che la pagina venga caricata. Selezionare la risoluzione della telecamera (ad esempio, QVGA) e selezionare [START STREAM]. Attendi qualche secondo/minuto, a seconda della connessione. Si può salvare un’immagine nell’area download del computer usando il pulsante [Save].\n\nEdge impulse suggerisce che gli oggetti dovrebbero essere simili per dimensione e non sovrapposti per prestazioni migliori. Questo va bene in una struttura industriale, dove la telecamera dovrebbe essere fissa, mantenendo la stessa distanza dagli oggetti da rilevare. Nonostante ciò, proveremo anche a usare dimensioni e posizioni miste per vedere il risultato.\n\nNon abbiamo bisogno di creare cartelle separate per le nostre immagini perché ognuna contiene più etichette.\n\nSuggeriamo di usare circa 50 immagini per mescolare gli oggetti e variare il numero di ciascuno che appare sulla scena. Provare ad acquisire con diverse angolazioni, sfondi e condizioni di luce.\n\nLe immagini archiviate usano una dimensione del frame QVGA di 320x240 e RGB565 (formato pixel colore).\n\nDopo aver acquisito il dataset, [Stop Stream] e spostare le immagini in una cartella.",
    "crumbs": [
      "XIAO ESP32S3",
      "Rilevamento degli Oggetti"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.it.html#edge-impulse-studio",
    "href": "contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.it.html#edge-impulse-studio",
    "title": "Rilevamento degli Oggetti",
    "section": "Edge Impulse Studio",
    "text": "Edge Impulse Studio\n\nSetup del progetto\nSi va su Edge Impulse Studio, si inseriscono le proprie credenziali in Login (o si crea un account) e si avvia un nuovo progetto.\n\n\nQui, è possibile clonare il progetto sviluppato per questa esercitazione pratica: XIAO-ESP32S3-Sense-Object_Detection\n\nNella dashboard del progetto, andare in basso e su Project info e selezionare Bounding boxes (object detection) e Espressif ESP-EYE (il più simile alla nostra scheda) come Target Device:\n\n\n\nCaricamento dei dati non etichettati\nIn Studio, si va alla scheda Data acquisition e nella sezione UPLOAD DATA caricare i file acquisiti come cartella dal computer.\n\n\nSi può lasciare che Studio divida automaticamente i dati tra “Train” e “Test” o farlo manualmente. Caricheremo tutti come training.\n\n\nTutte le immagini non etichettate (47) sono state caricate, ma devono essere etichettate in modo appropriato prima di essere utilizzate come dataset del progetto. Studio ha uno strumento per questo scopo, che che si trova nel link Labeling queue (47).\nCi sono due modi per eseguire l’etichettatura assistita dall’IA su Edge Impulse Studio (versione gratuita):\n\nUtilizzando yolov5\nTracciando di oggetti tra i frame\n\n\nEdge Impulse ha lanciato una funzione di auto-labeling per i clienti Enterprise, semplificando le attività di etichettatura nei progetti di rilevamento degli oggetti.\n\nGli oggetti ordinari possono essere rapidamente identificati ed etichettati utilizzando una libreria esistente di modelli di rilevamento degli oggetti pre-addestrati da YOLOv5 (addestrati con il set di dati COCO). Ma poiché, nel nostro caso, gli oggetti non fanno parte dei dataset COCO, dovremmo selezionare l’opzione di “tracking” [tracciamento] degli oggetti. Con questa opzione, una volta disegnati i bounding box ed etichettate le immagini in un frame, gli oggetti verranno tracciati automaticamente da un frame all’altro, etichettando partially quelli nuovi (non tutti sono etichettati correttamente).\n\nSi può usare EI uploader per importare i dati se si ha già un dataset etichettato contenente dei “bounding box”.\n\n\n\nEtichettatura del Dataset\nIniziando dalla prima immagine dei dati non etichettati, si usa il mouse per trascinare una casella attorno a un oggetto per aggiungere un’etichetta. Poi si clicca su Save labels per passare all’elemento successivo.\n\nSi continua con questo processo finché la coda non è vuota. Alla fine, tutte le immagini dovrebbero avere gli oggetti etichettati come i campioni sottostanti:\n\nPoi, si esaminano i campioni etichettati nella scheda Data acquisition. Se una delle etichette è errata, la si può modificarla utilizzando il menù tre puntini dopo il nome del campione:\n\nSi verrà guidati a sostituire l’etichetta errata e a correggere il dataset.\n\n\n\nBilanciamento del dataset e suddivisione Train/Test\nDopo aver etichettato tutti i dati, ci siamo resi conto che la classe fruit aveva molti più campioni di bug. Quindi, sono state raccolte 11 immagini di bug nuove e aggiuntive (per un totale di 58 immagini). Dopo averle etichettate, è il momento di selezionare alcune immagini e spostarle nel dataset di test. Per farlo si usa il menù a tre punti dopo il nome dell’immagine. Sono state selezionate sei immagini, che rappresentano il 13% del set di dati totale.",
    "crumbs": [
      "XIAO ESP32S3",
      "Rilevamento degli Oggetti"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.it.html#impulse-design",
    "href": "contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.it.html#impulse-design",
    "title": "Rilevamento degli Oggetti",
    "section": "Impulse Design",
    "text": "Impulse Design\nIn questa fase, si deve definire come:\n\nIl Pre-processing consiste nel ridimensionare le singole immagini da 320 x 240 a 96 x 96 e nel ridurle (forma quadrata, senza ritaglio). In seguito, le immagini vengono convertite da RGB a scala di grigi.\nDesign a Model, in questo caso, “Object Detection”.\n\n\n\nPre-elaborazione di tutti i dataset\nIn questa sezione, selezionare Color depth come Grayscale, adatta per l’uso con modelli FOMO ed eseguire il “Save” dei parametri.\n\nStudio passa automaticamente alla sezione successiva, “Generate features”, in cui tutti i campioni verranno pre-elaborati, generando un set di dati con singole immagini 96x96x1 o 9.216 feature.\n\nL’esploratore di feature mostra che tutti i campioni evidenziano una buona separazione dopo la generazione delle feature.\n\nAlcuni campioni sembrano stare nello spazio sbagliato, ma cliccandoci sopra si conferma la corretta etichettatura.",
    "crumbs": [
      "XIAO ESP32S3",
      "Rilevamento degli Oggetti"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.it.html#progettazione-addestramento-e-test-del-modello",
    "href": "contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.it.html#progettazione-addestramento-e-test-del-modello",
    "title": "Rilevamento degli Oggetti",
    "section": "Progettazione, Addestramento e Test del Modello",
    "text": "Progettazione, Addestramento e Test del Modello\nUseremo FOMO, un modello di rilevamento degli oggetti basato su MobileNetV2 (alpha 0.35) progettato per segmentare grossolanamente un’immagine in una griglia di background rispetto a oggetti di interesse (in questo caso, scatole e ruote).\nFOMO è un modello di apprendimento automatico innovativo per il rilevamento degli oggetti, che può utilizzare fino a 30 volte meno energia e memoria rispetto ai modelli tradizionali come Mobilenet SSD e YOLOv5. FOMO può funzionare su microcontrollori con meno di 200 KB di RAM. Il motivo principale per cui ciò è possibile è che mentre altri modelli calcolano le dimensioni dell’oggetto disegnando un quadrato attorno ad esso (bounding box), FOMO ignora le dimensioni dell’immagine, fornendo solo le informazioni su dove si trova l’oggetto nell’immagine tramite le sue coordinate del centroide.\nCome funziona FOMO?\nFOMO prende l’immagine in scala di grigi e la divide in blocchi di pixel usando un fattore di 8. Per l’input di 96x96, la griglia è 12x12 (96/8=12). Successivamente, FOMO eseguirà un classificatore attraverso ogni blocco di pixel per calcolare la probabilità che ci sia un box o una ruota in ognuno di essi e, successivamente, determinerà le regioni che hanno la più alta probabilità di contenere l’oggetto (se un blocco di pixel non ha oggetti, verrà classificato come background). Dalla sovrapposizione della regione finale, FOMO fornisce le coordinate (relative alle dimensioni dell’immagine) del centroide di questa regione.\n\nPer l’addestramento, dovremmo selezionare un modello pre-addestrato. Usiamo FOMO (Faster Objects, More Objects) MobileNetV2 0.35. Questo modello utilizza circa 250 KB di RAM e 80 KB di ROM (Flash), che si adatta bene alla nostra scheda.\n\nPer quanto riguarda gli iperparametri di training, il modello verrà addestrato con:\n\nEpochs: 60\nBatch size: 32\nLearning Rate: 0.001.\n\nPer la convalida durante l’addestramento, il 20% del set di dati (validation_dataset) verrà risparmiato. Per il restante 80% (train_dataset), applicheremo il “Data Augmentation”, che capovolgerà casualmente, cambierà le dimensioni e la luminosità dell’immagine e le ritaglierà, aumentando artificialmente il numero di campioni sul set di dati per l’addestramento.\nDi conseguenza, il modello termina con un punteggio F1 complessivo dell’85%, simile al risultato ottenuto utilizzando i dati di prova (83%).\n\nNotare che FOMO ha aggiunto automaticamente una terza etichetta di background [sfondo] ai due precedentemente definiti (box e wheel).\n\n\n\nNelle attività di rilevamento di oggetti, l’accuratezza non è in genere la metrica di valutazione primaria. Il rilevamento di oggetti comporta la classificazione degli oggetti e la definizione di riquadri di delimitazione attorno a essi, il che lo rende un problema più complesso della semplice classificazione. Il problema è che non abbiamo il riquadro di delimitazione, solo i centroidi. In breve, usare l’accuratezza come metrica potrebbe essere fuorviante e potrebbe non fornire una comprensione completa delle prestazioni del modello. Per questo motivo, useremo il punteggio F1.\n\n\nModello di test con “Live Classification”\nUna volta addestrato il nostro modello, possiamo testarlo utilizzando lo strumento Live Classification. Nella sezione corrispondente, cliccare sull’icona “Connect a development board” (una piccola MCU) e scansionare il codice QR col telefono.\n\nUna volta connesso, si può usare lo smartphone per catturare immagini reali da testare col modello addestrato su Edge Impulse Studio.\n\nUna cosa da notare è che il modello può produrre falsi positivi e falsi negativi. Questo può essere ridotto al minimo definendo una “Confidence Threshold” appropriata (usare il menù “Tre puntini” per la configurazione). Provare con 0,8 o più.",
    "crumbs": [
      "XIAO ESP32S3",
      "Rilevamento degli Oggetti"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.it.html#deploying-del-modello-arduino-ide",
    "href": "contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.it.html#deploying-del-modello-arduino-ide",
    "title": "Rilevamento degli Oggetti",
    "section": "Deploying del Modello (Arduino IDE)",
    "text": "Deploying del Modello (Arduino IDE)\nSelezionare la Libreria Arduino e il modello Quantized (int8), abilitare il compilatore EON nella scheda Deploy e premere [Build].\n\nApri l’Arduino IDE e, in Sketch, andare su Include Library e aggiungere .ZIP Library. Selezionare il file che scaricato da Edge Impulse Studio e il gioco è fatto!\n\nNella scheda Examples su Arduino IDE, si trova il codice di uno sketch (esp32 &gt; esp32_camera) sotto il nome del progetto.\n\nSi devono cambiare le righe dalla 32 alla 75, che definiscono il modello e i pin della telecamera, utilizzando i dati relativi al nostro modello. Copiare e incollare le righe seguenti, sostituendo le righe 32-75:\n#define PWDN_GPIO_NUM     -1 \n#define RESET_GPIO_NUM    -1 \n#define XCLK_GPIO_NUM     10 \n#define SIOD_GPIO_NUM     40 \n#define SIOC_GPIO_NUM     39\n#define Y9_GPIO_NUM       48 \n#define Y8_GPIO_NUM       11 \n#define Y7_GPIO_NUM       12 \n#define Y6_GPIO_NUM       14 \n#define Y5_GPIO_NUM       16 \n#define Y4_GPIO_NUM       18 \n#define Y3_GPIO_NUM       17 \n#define Y2_GPIO_NUM       15 \n#define VSYNC_GPIO_NUM    38 \n#define HREF_GPIO_NUM     47 \n#define PCLK_GPIO_NUM     13\nEcco il codice risultante:\n\nCaricare il codice sul XIAO ESP32S3 Sense e si è pronti a iniziare a rilevare frutta e insetti. Si può controllare il risultato su Serial Monitor.\nBackground\n\nFruits\n\nBugs\n\nSi noti che la latenza del modello è di 143ms e il frame rate al secondo è di circa 7 fps (simile a quanto ottenuto con il progetto Image Classification). Ciò accade perché FOMO è intelligentemente costruito su un modello CNN, non con un modello di rilevamento degli oggetti come SSD MobileNet. Ad esempio, quando si esegue un modello MobileNetV2 SSD FPN-Lite 320x320 su un Raspberry Pi 4, la latenza è circa cinque volte superiore (circa 1,5 fps).",
    "crumbs": [
      "XIAO ESP32S3",
      "Rilevamento degli Oggetti"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.it.html#distribuzione-del-modello-sensecraft-web-toolkit",
    "href": "contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.it.html#distribuzione-del-modello-sensecraft-web-toolkit",
    "title": "Rilevamento degli Oggetti",
    "section": "Distribuzione del Modello (SenseCraft-Web-Toolkit)",
    "text": "Distribuzione del Modello (SenseCraft-Web-Toolkit)\nCome discusso nel capitolo Image Classification, verificare l’inferenza con i modelli di immagine su Arduino IDE è molto impegnativo perché non possiamo vedere su cosa punta la telecamera. Di nuovo, utilizziamo SenseCraft-Web Toolkit.\nSeguire i seguenti passaggi per avviare SenseCraft-Web-Toolkit:\n\nAprire il sito web di SenseCraft-Web-Toolkit.\nCollega XIAO al computer:\n\n\nDopo aver collegato XIAO, selezionarlo come di seguito:\n\n\n\nSelezionare il dispositivo/Porta e premere [Connect]:\n\n\n\nSi possono provare diversi modelli di Computer Vision caricati in precedenza da Seeed Studio. Da provare e verificarli!\n\nNel nostro caso, useremo il pulsante blu in fondo alla pagina: [Upload Custom AI Model].\nMa prima, dobbiamo scaricare da Edge Impulse Studio il modello quantized .tflite.\n\nSi va sul proprio progetto su Edge Impulse Studio, oppure si clona questo:\n\n\nXIAO-ESP32S3-CAM-Fruits-vs-Veggies-v1-ESP-NN\n\n\nSu Dashboard, scaricare il modello (“block output”): Object Detection model - TensorFlow Lite (int8 quantized)\n\n\n\nSu SenseCraft-Web-Toolkit, usare il pulsante blu in fondo alla pagina: [Upload Custom AI Model]. Si aprirà una finestra. Inserire il file del Modello scaricato sul computer da Edge Impulse Studio, scegliere un nome del modello e inserirlo con le etichette (ID: Object):\n\n\n\nNotare che si devono utilizzare le etichette apprese su EI Studio e inserirle in ordine alfabetico (nel nostro caso, background, bug, fruit).\n\nDopo alcuni secondi (o minuti), il modello verrà caricato sul dispositivo e l’immagine della telecamera apparirà in tempo reale nel Preview Sector:\n\nGli oggetti rilevati saranno contrassegnati (il centroide). È possibile selezionare l’affidabilità del cursore di inferenza Confidence e IoU, che viene utilizzata per valutare l’accuratezza delle “bounding box” previste rispetto a quelle vere.\nCliccando sul pulsante in alto (Device Log), si può aprire un Serial Monitor per seguire l’inferenza, come abbiamo fatto con l’IDE Arduino.\n\nSu Device Log, si otterranno informazioni come:\n\nTempo di pre-elaborazione (acquisizione dell’immagine e Crop): 3 ms,\nTempo di inferenza (latenza del modello): 115 ms,\nTempo di post-elaborazione (visualizzazione dell’immagine e marcatura degli oggetti): 1 ms.\nTensore di output (box), ad esempio, uno dei box: [[30,150, 20, 20, 97, 2]]; dove 30,150, 20, 20 sono le coordinate della casella (intorno al centroide); 97 è il risultato dell’inferenza e 2 è la classe (in questo caso 2: frutto).\n\n\nNotare che nell’esempio precedente, abbiamo ottenuto 5 caselle perché nessuno dei frutti ha ottenuto 3 centroidi. Una soluzione sarà la post-elaborazione, dove possiamo aggregare centroidi vicini in uno.\n\nEcco altri screenshot:",
    "crumbs": [
      "XIAO ESP32S3",
      "Rilevamento degli Oggetti"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.it.html#conclusione",
    "href": "contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.it.html#conclusione",
    "title": "Rilevamento degli Oggetti",
    "section": "Conclusione",
    "text": "Conclusione\nFOMO è un salto significativo nello spazio di elaborazione delle immagini, come hanno affermato Louis Moreau e Mat Kelcey durante il suo lancio nel 2022:\n\nFOMO è un algoritmo rivoluzionario che porta per la prima volta il rilevamento, il tracciamento e il conteggio degli oggetti in tempo reale sui microcontrollori.\n\nEsistono molteplici possibilità per esplorare il rilevamento di oggetti (e, più precisamente, il loro conteggio) su dispositivi embedded.",
    "crumbs": [
      "XIAO ESP32S3",
      "Rilevamento degli Oggetti"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.it.html#risorse",
    "href": "contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.it.html#risorse",
    "title": "Rilevamento degli Oggetti",
    "section": "Risorse",
    "text": "Risorse\n\nProgetto Edge Impulse",
    "crumbs": [
      "XIAO ESP32S3",
      "Rilevamento degli Oggetti"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/kws/kws.it.html",
    "href": "contents/labs/seeed/xiao_esp32s3/kws/kws.it.html",
    "title": "Keyword Spotting (KWS)",
    "section": "",
    "text": "Panoramica\nKeyword Spotting (KWS) è parte integrante di molti sistemi di riconoscimento vocale, consentendo ai dispositivi di rispondere a parole o frasi specifiche. Sebbene questa tecnologia sia alla base di dispositivi popolari come Google Assistant o Amazon Alexa, è ugualmente applicabile e realizzabile su dispositivi più piccoli e a basso consumo. Questo lab guiderà nell’implementazione di un sistema KWS utilizzando TinyML sulla scheda microcontrollore XIAO ESP32S3.\nThe XIAO ESP32S3, dotato del chip ESP32-S3 di Espressif, è un microcontrollore compatto e potente che offre un processore Xtensa LX7 dual-core, Wi-Fi integrato e Bluetooth. Il suo equilibrio di potenza di calcolo, efficienza energetica e connettività versatile lo rendono una piattaforma fantastica per le applicazioni TinyML. Inoltre, con la sua scheda di espansione, avremo accesso alla parte “sense” del dispositivo, che ha una fotocamera OV2640 da 1600x1200, uno slot per schede SD e un microfono digitale. Il microfono integrato e la scheda SD saranno essenziali in questo progetto.\nUtilizzeremo Edge Impulse Studio, una piattaforma potente e intuitiva che semplifica la creazione e l’implementazione di modelli di apprendimento automatico su dispositivi edge. Addestreremo un modello KWS passo dopo passo, ottimizzandolo e distribuendolo su XIAO ESP32S3 Sense.\nIl nostro modello sarà progettato per riconoscere parole chiave che possono attivare il “wake-up” [risveglio] del dispositivo o azioni specifiche (nel caso di “YES”), dando vita a progetti con comandi vocali.\nSfruttando la nostra esperienza con TensorFlow Lite per microcontrollori (il motore “sotto il cofano” di EI Studio), creeremo un sistema KWS in grado di apprendere in tempo reale sul dispositivo.\nProcedendo nel lab, analizzeremo ogni fase del processo, dalla raccolta e preparazione dei dati all’addestramento e distribuzione del modello, per fornire una comprensione completa dell’implementazione di un sistema KWS su un microcontrollore.",
    "crumbs": [
      "XIAO ESP32S3",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/kws/kws.it.html#panoramica",
    "href": "contents/labs/seeed/xiao_esp32s3/kws/kws.it.html#panoramica",
    "title": "Keyword Spotting (KWS)",
    "section": "",
    "text": "Come funziona un assistente vocale?\nKeyword Spotting (KWS) è fondamentale per molti assistenti vocali, consentendo ai dispositivi di rispondere a parole o frasi specifiche. Per iniziare, è essenziale rendersi conto che gli assistenti vocali sul mercato, come Google Home o Amazon Echo-Dot, reagiscono agli umani solo quando vengono “svegliati” da parole chiave specifiche come “Hey Google” sul primo e “Alexa” sul secondo.\n\nIn altre parole, il riconoscimento dei comandi vocali si basa su un modello multi-fase o Cascade Detection.\n\nFase 1: Un microprocessore più piccolo all’interno dell’Echo Dot o Google Home ascolta continuamente il suono, in attesa che venga individuata la parola chiave. Per tale rilevamento, viene utilizzato un modello TinyML all’edge (applicazione KWS).\nFase 2: Solo quando vengono attivati dall’applicazione KWS nella Fase 1, i dati vengono inviati al cloud ed elaborati su un modello più grande.\nIl video qui sotto mostra un esempio in cui si emula un Google Assistant su un Raspberry Pi (Fase 2), con un Arduino Nano 33 BLE come dispositivo tinyML (Fase 1).\n\n\n\nPer approfondire il progetto completo, guardare il tutorial: Building an Intelligent Voice Assistant From Scratch.\n\nIn questo lab, ci concentreremo sulla Fase 1 (KWS o Keyword Spotting), dove utilizzeremo XIAO ESP2S3 Sense, che ha un microfono digitale per individuare la parola chiave.\n\n\nIl progetto KWS\nIl diagramma seguente darà un’idea di come dovrebbe funzionare l’applicazione KWS finale (durante l’inferenza):\n\nLa nostra applicazione KWS riconoscerà quattro classi di suono:\n\nYES (Keyword 1)\nNO (Keyword 2)\nNOISE [rumore] (nessuna parola chiave pronunciata, è presente solo rumore di fondo)\nUNKNOW (un mix di parole diverse da YES e NO)\n\n\nFacoltativamente, per progetti reali, si consiglia sempre di includere parole diverse dalle parole chiave, come “Rumore” (o Sfondo) e “Sconosciuto”.\n\n\n\nIl Flusso di Lavoro del Machine Learning\nIl componente principale dell’applicazione KWS è il suo modello. Quindi, dobbiamo addestrare un modello del genere con le nostre parole chiave specifiche, rumore e altre parole (lo “unknown”):",
    "crumbs": [
      "XIAO ESP32S3",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/kws/kws.it.html#il-dataset",
    "href": "contents/labs/seeed/xiao_esp32s3/kws/kws.it.html#il-dataset",
    "title": "Keyword Spotting (KWS)",
    "section": "Il Dataset",
    "text": "Il Dataset\nIl componente critico del flusso di lavoro di apprendimento automatico è il dataset. Una volta decise le parole chiave specifiche (YES e NO), possiamo sfruttare il dataset sviluppato da Pete Warden, “Speech Commands: A Dataset for Limited-Vocabulary Speech Recognition“. Questo set di dati ha 35 parole chiave (con +1.000 campioni ciascuna), come yes, no, stop e go. In altre parole, possiamo ottenere 1.500 campioni di yes e no.\nSi può scaricare una piccola parte del dataset da Edge Studio (Keyword spotting pre-built dataset), che include campioni dalle quattro classi che utilizzeremo in questo progetto: yes, no, noise e background. Per farlo, si seguono i passaggi seguenti:\n\nDownload del dataset delle parole chiave.\nDecomprimere il file in una posizione a scelta.\n\nSebbene disponiamo di molti dati dal dataset di Pete, è consigliabile raccogliere alcune parole pronunciate da noi. Lavorando con gli accelerometri, creare un dataset con dati acquisiti dallo stesso tipo di sensore era essenziale. Nel caso del suono, è diverso perché ciò che classificheremo sono, in realtà, dati audio.\n\nLa differenza fondamentale tra suono e audio è la loro forma di energia. Il suono è energia delle onde meccaniche (onde sonore longitudinali) che si propagano attraverso un mezzo causando variazioni di pressione all’interno del mezzo. L’audio è costituito da energia elettrica (segnali analogici o digitali) che rappresentano il suono elettricamente.\n\nLe onde sonore dovrebbero essere convertite in dati audio quando pronunciamo una parola chiave. La conversione dovrebbe essere eseguita campionando il segnale generato dal microfono a 16 KHz con una profondità di 16 bit.\nQuindi, qualsiasi dispositivo in grado di generare dati audio con questa specifica di base (16Khz/16bit) funzionerà bene. Come dispositivo, possiamo usare il XIAO ESP32S3 Sense appropriato, un computer o persino il telefono cellulare.\n\nAcquisizione di Dati Audio online con Edge Impulse e uno smartphone\nNel lab “Motion Classification” e “Anomaly Detection”, colleghiamo il nostro dispositivo direttamente a Edge Impulse Studio per l’acquisizione dei dati (con una frequenza di campionamento da 50 Hz a 100 Hz). Per una frequenza così bassa, potremmo usare la funzione EI CLI Data Forwarder, ma secondo Jan Jongboom, CTO di Edge Impulse, l’audio (16 KHz)* è troppo veloce perché il data forwarder possa essere acquisito. Quindi, una volta che i dati digitali sono stati acquisiti dal microfono, possiamo trasformarli in un file WAV* da inviare a Studio tramite Data Uploader (lo stesso che faremo con il set di dati di Pete).\n\nSe vogliamo raccogliere dati audio direttamente sullo Studio, possiamo usare qualsiasi smartphone connesso online. Non esploreremo questa opzione qui, ma si può facilmente seguire la documentazione EI.\n\n\nAcquisizione (offline) di dati audio con XIAO ESP32S3 Sense\nIl microfono integrato è il MSM261D3526H1CPM, un microfono MEMS con uscita digitale PDM con Multi-mode. Internamente, è collegato all’ESP32S3 tramite un bus I2S utilizzando i pin IO41 (Clock) e IO41 (Data).\n\nCos’è I2S?\nI2S, o Inter-IC Sound, è un protocollo standard per la trasmissione di audio digitale da un dispositivo a un altro. Inizialmente è stato sviluppato da Philips Semiconductor (ora NXP Semiconductors). È comunemente utilizzato in dispositivi audio come processori di segnale digitale, processori audio digitali e, più di recente, microcontrollori con capacità audio digitali (il nostro caso qui).\nIl protocollo I2S è composto da almeno tre linee:\n\n1. Linea di clock di bit (o seriale) (BCLK o CLK): Questa linea si attiva/disattiva per indicare l’inizio di un nuovo bit di dati (pin IO42).\n2. Linea di “Word select (WS)”: Questa linea si attiva/disattiva per indicare l’inizio di una nuova parola (canale sinistro o canale destro). La frequenza del clock di Word select (WS) definisce la frequenza di campionamento. Nel nostro caso, L/R sul microfono è impostato su massa, il che significa che utilizzeremo solo il canale sinistro (mono).\n3. Data line (SD): Questa linea trasporta i dati audio (pin IO41)\nIn un flusso di dati I2S, i dati vengono inviati come una sequenza di frame, ciascuno contenente una parola del canale sinistro e una parola del canale destro. Ciò rende I2S particolarmente adatto per la trasmissione di dati audio stereo. Tuttavia, può anche essere utilizzato per audio mono o multicanale con linee dati aggiuntive.\nCominciamo a capire come catturare dati grezzi usando il microfono. Su va sul progetto GitHube si scarica lo sketch: XIAOEsp2s3_Mic_Test:\n/*\n  XIAO ESP32S3 Simple Mic Test\n*/\n\n#include &lt;I2S.h&gt;\n\nvoid setup() {\n  Serial.begin(115200);\n  while (!Serial) {\n  }\n\n\n  // start I2S at 16 kHz with 16-bits per sample\n  I2S.setAllPins(-1, 42, 41, -1, -1);\n  if (!I2S.begin(PDM_MONO_MODE, 16000, 16)) {\n    Serial.println(\"Failed to initialize I2S!\");\n    while (1); // do nothing\n  }\n}\n\nvoid loop() {\n  // read a sample\n  int sample = I2S.read();\n\n  if (sample && sample != -1 && sample != 1) {\n    Serial.println(sample);\n  }\n}\nQuesto codice è un semplice test del microfono per XIAO ESP32S3 che utilizza l’interfaccia I2S (Inter-IC Sound). Imposta l’interfaccia I2S per catturare dati audio a una frequenza di campionamento di 16 kHz con 16 bit per campione e quindi legge continuamente i campioni dal microfono e li stampa sul monitor seriale.\nAnalizziamo le parti principali del codice:\n\nInclude la libreria I2S: Questa libreria fornisce funzioni per configurare e utilizzare l’interfaccia I2S, che è uno standard per la connessione di dispositivi audio digitali.\nI2S.setAllPins(-1, 42, 41, -1, -1): Imposta i pin I2S. I parametri sono (-1, 42, 41, -1, -1), dove il secondo parametro (42) è il PIN per il clock I2S (CLK) e il terzo parametro (41) è il PIN per la linea dati I2S (DATA). Gli altri parametri sono impostati su -1, il che significa che quei pin non vengono utilizzati.\nI2S.begin(PDM_MONO_MODE, 16000, 16): Inizializza l’interfaccia I2S in modalità mono Pulse Density Modulation (PDM), con una frequenza di campionamento di 16 kHz e 16 bit per campione. Se l’inizializzazione fallisce, viene stampato un messaggio di errore e il programma si arresta.\nint sample = I2S.read(): Legge un campione audio dall’interfaccia I2S.\n\nSe il campione è valido, viene stampato sul monitor seriale e sul plotter.\nDi seguito è riportato un test “sussurrato” in due toni diversi.\n\n\n\nSalvare campioni audio registrati (dataset) come file audio .wav su una scheda microSD\nUtilizziamo il lettore di schede SD integrato per salvare i file audio .wav; dobbiamo prima abilitare la PSRAM XIAO.\n\nESP32-S3 ha solo poche centinaia di kilobyte di RAM interna sul chip MCU. Potrebbe essere insufficiente per alcuni scopi, quindi ESP32-S3 può utilizzare fino a 16 MB di PSRAM esterna (Psuedostatic RAM) collegata in parallelo con il chip flash SPI. La memoria esterna è incorporata nella mappa di memoria e, con alcune restrizioni, è utilizzabile allo stesso modo della RAM dati interna.\n\nPer iniziare, si inserisce la scheda SD sullo XIAO come mostrato nella foto qui sotto (la scheda SD deve essere formattata in FAT32).\n\nAttivare la funzione PSRAM del chip ESP-32 (Arduino IDE): Tools&gt;PSRAM: “OPI PSRAM”&gt;OPI PSRAM\n\n\nScaricare lo sketch Wav_Record_dataset, che si trova sul GitHub del progetto.\n\nQuesto codice registra l’audio usando l’interfaccia I2S della scheda Seeed XIAO ESP32S3 Sense, salva la registrazione come file .wav su una scheda SD e consente il controllo del processo di registrazione tramite comandi inviati dal monitor seriale. Il nome del file audio è personalizzabile (dovrebbe essere le etichette della classe da usare con la formazione) e possono essere effettuate più registrazioni, ciascuna salvata in un nuovo file. Il codice include anche funzionalità per aumentare il volume delle registrazioni.\nAnalizziamo le parti più essenziali:\n#include &lt;I2S.h&gt;\n#include \"FS.h\"\n#include \"SD.h\"\n#include \"SPI.h\"\nQueste sono le librerie necessarie per il programma. I2S.h consente l’input audio, FS.h fornisce capacità di gestione del file system, SD.h consente al programma di interagire con una scheda SD e SPI.h gestisce la comunicazione SPI con la scheda SD.\n#define RECORD_TIME   10  \n#define SAMPLE_RATE 16000U\n#define SAMPLE_BITS 16\n#define WAV_HEADER_SIZE 44\n#define VOLUME_GAIN 2\nQui vengono definite varie costanti per il programma.\n\nRECORD_TIME specifica la lunghezza della registrazione audio in secondi.\nSAMPLE_RATE e SAMPLE_BITS definiscono la qualità audio della registrazione.\nWAV_HEADER_SIZE specifica la dimensione dell’intestazione del file .wav.\nVOLUME_GAIN viene utilizzato per aumentare il volume della registrazione.\n\nint fileNumber = 1;\nString baseFileName;\nbool isRecording = false;\nQueste variabili tengono traccia del numero di file corrente (per creare nomi di file univoci), del nome del file di base e se il sistema sta attualmente registrando.\nvoid setup() {\n  Serial.begin(115200);\n  while (!Serial);\n  \n  I2S.setAllPins(-1, 42, 41, -1, -1);\n  if (!I2S.begin(PDM_MONO_MODE, SAMPLE_RATE, SAMPLE_BITS)) {\n    Serial.println(\"Failed to initialize I2S!\");\n    while (1);\n  }\n\n  \n  if(!SD.begin(21)){\n    Serial.println(\"Failed to mount SD Card!\");\n    while (1);\n  }\n\n  Serial.printf(\"Enter with the label name\\n\");\n}\nLa funzione di configurazione inizializza la comunicazione seriale, l’interfaccia I2S per l’ingresso audio e l’interfaccia della scheda SD. Se l’I2S non si inizializza o la scheda SD non riesce a essere montata, verrà visualizzato un messaggio di errore e l’esecuzione verrà interrotta.\nvoid loop() {\n  if (Serial.available() &gt; 0) {\n    String command = Serial.readStringUntil('\\n');\n    command.trim();\n    if (command == \"rec\") {\n      isRecording = true;\n    } else {\n      baseFileName = command;\n      fileNumber = 1; //reset file number each time a new basefile name is set\n      Serial.printf(\"Send rec for starting recording label \\n\");\n    }\n\n  }\n\n  if (isRecording && baseFileName != \"\") {\n    String fileName = \"/\" + baseFileName + \".\" + String(fileNumber) + \".wav\";\n    fileNumber++;\n    record_wav(fileName);\n    delay(1000); // delay to avoid recording multiple files at once\n    isRecording = false;\n  }\n}\nNel ciclo principale, il programma attende un comando dal monitor seriale. Se il comando è rec, il programma inizia a registrare. Altrimenti, si presume che il comando sia il nome base per i file .wav. Se sta attualmente registrando e un nome file base è impostato, registra l’audio e lo salva come file .wav. I nomi dei file vengono generati aggiungendo il numero al nome file base.\nvoid record_wav(String fileName)\n{\n  ...\n  \n  File file = SD.open(fileName.c_str(), FILE_WRITE);\n  ...\n  rec_buffer = (uint8_t *)ps_malloc(record_size);\n  ...\n\n  esp_i2s::i2s_read(esp_i2s::I2S_NUM_0, \n                    rec_buffer, \n                    record_size, \n                    &sample_size, \n                    portMAX_DELAY);\n  ...\n}\nQuesta funzione registra l’audio e lo salva come file .wav con il nome specificato. Inizia inizializzando le variabili sample_size e record_size. record_size viene calcolato in base alla frequenza di campionamento, alla dimensione e al tempo di registrazione desiderato. Analizziamo le sezioni essenziali;\nFile file = SD.open(fileName.c_str(), FILE_WRITE);\n// Write the header to the WAV file\nuint8_t wav_header[WAV_HEADER_SIZE];\ngenerate_wav_header(wav_header, record_size, SAMPLE_RATE);\nfile.write(wav_header, WAV_HEADER_SIZE);\nQuesta sezione del codice apre il file sulla scheda SD per la scrittura e poi genera l’intestazione del file .wav utilizzando la funzione generate_wav_header. Quindi scrive l’intestazione nel file.\n// PSRAM malloc for recording\nrec_buffer = (uint8_t *)ps_malloc(record_size);\nif (rec_buffer == NULL) {\n  Serial.printf(\"malloc failed!\\n\");\n  while(1) ;\n}\nSerial.printf(\"Buffer: %d bytes\\n\", ESP.getPsramSize() - ESP.getFreePsram());\nLa funzione ps_malloc alloca memoria nella PSRAM per la registrazione. Se l’allocazione fallisce (ad esempio, rec_buffer è NULL), stampa un messaggio di errore e interrompe l’esecuzione.\n// Start recording\nesp_i2s::i2s_read(esp_i2s::I2S_NUM_0, \n         rec_buffer, \n         record_size, \n         &sample_size, \n         portMAX_DELAY);\nif (sample_size == 0) {\n  Serial.printf(\"Record Failed!\\n\");\n} else {\n    Serial.printf(\"Record %d bytes\\n\", sample_size);\n  }\nLa funzione i2s_read legge i dati audio dal microfono in rec_buffer. Stampa un messaggio di errore se non vengono letti dati (sample_size è 0).\n// Increase volume\nfor (uint32_t i = 0; i &lt; sample_size; i += SAMPLE_BITS/8) {\n  (*(uint16_t *)(rec_buffer+i)) &lt;&lt;= VOLUME_GAIN;\n}\nQuesta sezione del codice aumenta il volume di registrazione spostando i valori del campione di VOLUME_GAIN.\n// Write data to the WAV file\nSerial.printf(\"Writing to the file ...\\n\");\nif (file.write(rec_buffer, record_size) != record_size)\n  Serial.printf(\"Write file Failed!\\n\");\n\nfree(rec_buffer);\nfile.close();\nSerial.printf(\"Recording complete: \\n\");\nSerial.printf(\"Send rec for a new sample or enter a new label\\n\\n\");\nInfine, i dati audio vengono scritti nel file .wav. Se l’operazione di scrittura fallisce, viene stampato un messaggio di errore. Dopo la scrittura, la memoria allocata per rec_buffer viene liberata e il file viene chiuso. La funzione termina stampando un messaggio di completamento e chiedendo all’utente di inviare un nuovo comando.\nvoid generate_wav_header(uint8_t *wav_header,  \n             uint32_t wav_size, \n             uint32_t sample_rate)\n{\n  ...\n  memcpy(wav_header, set_wav_header, sizeof(set_wav_header));\n}\nLa funzione generate_wav_header crea un’intestazione di file .wav in base ai parametri (wav_size e sample_rate). Genera un array di byte in base al formato di file .wav, che include campi per la dimensione del file, il formato audio, il numero di canali, la frequenza di campionamento, la frequenza di byte, l’allineamento dei blocchi, i bit per campione e la dimensione dei dati. L’intestazione generata viene poi copiata nell’array wav_header passato alla funzione.\nOra, caricare il codice su XIAO e ottenere campioni dalle parole chiave (yes e no). Si possono anche catturare rumore e altre parole.\nIl monitor seriale chiederà di ricevere l’etichetta da registrare.\n\nInvia l’etichetta (ad esempio, yes). Il programma attenderà un altro comando: rec\n\nE il programma inizierà a registrare nuovi campioni ogni volta che viene inviato un comando rec. I file verranno salvati come yes.1.wav, yes.2.wav, yes.3.wav, ecc., finché non verrà inviata una nuova etichetta (ad esempio, no). In questo caso, si deve inviare il comando rec per ogni nuovo campione, che verrà salvato come no.1.wav, no.2.wav, no.3.wav, ecc.\n\nAlla fine, otterremo i file salvati sulla scheda SD.\n\nI file sono pronti per essere caricati su Edge Impulse Studio\n\n\nApp di Acquisizione dei Dati Audio (offline)\nIn alternativa, si può anche usare il PC o lo smartphone per acquisire dati audio con una frequenza di campionamento di 16 KHz e una profondità di bit di 16 bit. Una buona app per questo è Voice Recorder Pro (IOS). Le registrazioni si devono salvare come file .wav e inviarle al computer.\n\n\nNotare che qualsiasi app, come Audacity, può essere usata per la registrazione audio o anche il computer.",
    "crumbs": [
      "XIAO ESP32S3",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/kws/kws.it.html#modello-di-training-con-edge-impulse-studio",
    "href": "contents/labs/seeed/xiao_esp32s3/kws/kws.it.html#modello-di-training-con-edge-impulse-studio",
    "title": "Keyword Spotting (KWS)",
    "section": "Modello di training con Edge Impulse Studio",
    "text": "Modello di training con Edge Impulse Studio\n\nCaricamento dei Dati\nQuando il dataset grezzo è definito e raccolto (dataset di Pete + parole chiave registrate), dovremmo avviare un nuovo progetto in Edge Impulse Studio:\n\nUna volta creato il progetto, selezionare lo strumento “Upload Existing Data” nella sezione “Acquisition section”. Si scelgono i file da caricare:\n\nE si caricano nello Studio (si possono dividere automaticamente i dati in train/test). Ripetere per tutte le classi e tutti i dati grezzi.\n\nI campioni appariranno ora nella sezione “Data acquisition”.\n\nTutti i dati sul dataset di Pete hanno una lunghezza di 1s, ma i campioni registrati nella sezione precedente hanno 10s e devono essere divisi in campioni da 1s per essere compatibili.\nCliccare sui tre punti dopo il nome del campione e selezionare “Split sample”.\n\nUna volta all’interno dello strumento, dividere i dati in record da 1 secondo. Se necessario, aggiungere o rimuovere segmenti:\n\nQuesta procedura deve essere ripetuta per tutti i campioni.\n\nNota: Per file audio più lunghi (minuti), prima si dividono in segmenti da 10 secondi e poi usa di nuovo lo strumento per ottenere le divisioni finali da 1 secondo.\n\nSupponiamo di non dividere automaticamente i dati in train/test durante il caricamento. In tal caso, possiamo farlo manualmente (utilizzando il menù a tre punti, spostando i campioni singolarmente) o utilizzando Perform Train / Test Split su Dashboard - Danger Zone.\n\nPossiamo facoltativamente controllare tutti i dataset utilizzando la scheda Data Explorer.\n\n\n\nCreazione di Impulse (Pre-Process / Definizione del Modello)\nUn impulse prende dati grezzi, usa l’elaborazione del segnale per estrarre le feature e poi usa un blocco di apprendimento per classificare nuovi dati.\n\nPer prima cosa, prenderemo i dati con una finestra di 1 secondo, aumentando i dati, facendo scorrere quella finestra ogni 500 ms. Notare che è impostata l’opzione Zero-pad data. È essenziale riempire con zeri i campioni inferiori a 1 secondo (in alcuni casi, si è ridotta la finestra di 1000 ms sullo strumento di divisione per evitare rumori e picchi).\nOgni campione audio di 1 secondo dovrebbe essere pre-elaborato e convertito in un’immagine (ad esempio, 13 x 49 x 1). Useremo MFCC, che estrae le caratteristiche dai segnali audio usando Mel Frequency Cepstral Coefficients, che sono ottimi per la voce umana.\n\nSuccessivamente, selezioniamo KERAS per la classificazione e costruiamo il nostro modello da zero eseguendo la classificazione delle immagini tramite la rete neurale convoluzionale).\n\n\nPre-elaborazione (MFCC)\nIl passo successivo è creare le immagini da addestrare nella fase successiva:\nPossiamo mantenere i valori dei parametri di default o sfruttare l’opzione DSP Autotuneparameters, cosa che faremo.\n\nIl risultato non impiegherà molta memoria per pre-elaborare i dati (solo 16 KB). Tuttavia, il tempo di elaborazione stimato è elevato, 675 ms per un Espressif ESP-EYE (il riferimento più vicino disponibile), con un clock di 240 KHz (lo stesso del nostro dispositivo), ma con una CPU più piccola (XTensa LX6, rispetto alla LX7 sull’ESP32S). Il tempo di inferenza reale dovrebbe essere inferiore.\nSupponiamo di dover ridurre il tempo di inferenza in seguito. In tal caso, dovremmo tornare alla fase di pre-elaborazione e, ad esempio, ridurre la lunghezza FFT a 256, modificare il numero di coefficienti o un altro parametro.\nPer ora, manteniamo i parametri definiti dallo strumento Autotuning. Salviamo i parametri e generiamo le funzionalità.\n\n\nPer andare oltre con la conversione di dati seriali temporali in immagini usando FFT, spettrogramma, ecc., si può giocare con questo CoLab: Audio Raw Data Analysis.\n\n\n\nProgettazione e Addestramento del Modello\nUseremo un modello di Rete Neurale Convoluzionale (CNN). L’architettura di base è definita con due blocchi di Conv1D + MaxPooling (rispettivamente con 8 e 16 neuroni) e un Dropout di 0,25. E sull’ultimo layer, dopo aver appiattito quattro neuroni, uno per ogni classe:\n\nCome iperparametri, avremo un Learning Rate di 0,005 e un modello che verrà addestrato per 100 epoche. Includeremo anche l’aumento dei dati, come un po’ di rumore. Il risultato sembra OK:\n\nPer capire cosa sta succedendo “sotto il cofano”, si può scaricare il dataset ed eseguire un Jupyter Notebook giocando con il codice. Ad esempio, si può analizzare l’accuratezza per ogni epoca:\n\nQuesto CoLab Notebook può spiegare come si può andare oltre: KWS Classifier Project - Looking “Under the hood Training/xiao_esp32s3_keyword_spotting_project_nn_classifier.ipynb)”.",
    "crumbs": [
      "XIAO ESP32S3",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/kws/kws.it.html#test",
    "href": "contents/labs/seeed/xiao_esp32s3/kws/kws.it.html#test",
    "title": "Keyword Spotting (KWS)",
    "section": "Test",
    "text": "Test\nTestando il modello con i dati messi da parte prima dell’addestramento (Test Data), abbiamo ottenuto un’accuratezza di circa l’87%.\n\nEsaminando il punteggio F1, possiamo vedere che per YES abbiamo ottenuto 0.95, un risultato eccellente una volta utilizzata questa parola chiave per “attivare” la nostra fase di post-elaborazione (accendere il LED integrato). Anche per NO, abbiamo ottenuto 0,90. Il risultato peggiore è per unknown, che è OK.\nPossiamo procedere con il progetto, ma è possibile eseguire Live Classification utilizzando uno smartphone prima della distribuzione sul nostro dispositivo. Si va alla sezione Live Classification e si clicca su Connect a Development board:\n\nPuntare il telefono sul codice a barre e selezionare il link.\n\nIl telefono sarà connesso allo Studio. Selezionare l’opzione Classification sull’app e, quando è in esecuzione, iniziare a testare le parole chiave, confermando che il modello funziona con dati live e reali:",
    "crumbs": [
      "XIAO ESP32S3",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/kws/kws.it.html#distribuzione-e-inferenza",
    "href": "contents/labs/seeed/xiao_esp32s3/kws/kws.it.html#distribuzione-e-inferenza",
    "title": "Keyword Spotting (KWS)",
    "section": "Distribuzione e Inferenza",
    "text": "Distribuzione e Inferenza\nStudio impacchetterà tutte le librerie necessarie, le funzioni di pre-elaborazione e i modelli addestrati, scaricandoli sul computer. Si deve selezionare l’opzione Arduino Library e, in basso, scegliere Quantized (Int8) e premere il pulsante Build.\n\nOra è il momento di un vero test. Faremo inferenze completamente scollegate da Studio. Modifichiamo uno degli esempi di codice ESP32 creati quando si distribuisce la libreria Arduino.\nNell’IDE Arduino, si va alla scheda File/Examples, si cerca il progetto e si seleziona esp32/esp32_microphone:\n\nQuesto codice è stato creato per il microfono integrato ESP-EYE, che dovrebbe essere adattato al nostro dispositivo.\nIniziare a modificare le librerie per gestire il bus I2S:\n\nDa:\n#include &lt;I2S.h&gt;\n#define SAMPLE_RATE 16000U\n#define SAMPLE_BITS 16\nInizializzare il microfono IS2 in setup(), includendo le righe:\nvoid setup()\n{\n...\n    I2S.setAllPins(-1, 42, 41, -1, -1);\n    if (!I2S.begin(PDM_MONO_MODE, SAMPLE_RATE, SAMPLE_BITS)) {\n      Serial.println(\"Failed to initialize I2S!\");\n    while (1) ;\n...\n}\nNella funzione static void capture_samples(void* arg), sostituire la riga 153 che legge i dati dal microfono I2S:\n\nDa:\n/* read data at once from i2s */\nesp_i2s::i2s_read(esp_i2s::I2S_NUM_0, \n                 (void*)sampleBuffer, \n                 i2s_bytes_to_read, \n                 &bytes_read, 100);\nNella funzione static bool microphone_inference_start(uint32_t n_samples), dovremmo commentare o eliminare le righe da 198 a 200, dove viene chiamata la funzione di inizializzazione del microfono. Ciò non è necessario perché il microfono I2S è già stato inizializzato durante setup().\n\nInfine, nella funzione static void microphone_inference_end(void), sostituire la riga 243:\n\nDa:\nstatic void microphone_inference_end(void)\n{\n    free(sampleBuffer);\n    ei_free(inference.buffer);\n}\nIl codice completo si trova tra i progetti GitHub. Caricare lo sketch sulla bacheca e provare alcune inferenze reali:",
    "crumbs": [
      "XIAO ESP32S3",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/kws/kws.it.html#post-elaborazione",
    "href": "contents/labs/seeed/xiao_esp32s3/kws/kws.it.html#post-elaborazione",
    "title": "Keyword Spotting (KWS)",
    "section": "Post-elaborazione",
    "text": "Post-elaborazione\nOra che sappiamo che il modello funziona rilevando le nostre parole chiave, modifichiamo il codice per vedere il LED interno accendersi ogni volta che viene rilevato uno YES.\nSi deve inizializzare il LED:\n#define LED_BUILT_IN 21\n...\nvoid setup()\n{\n...\n  pinMode(LED_BUILT_IN, OUTPUT); // Set the pin as output\n  digitalWrite(LED_BUILT_IN, HIGH); //Turn off\n...\n}\nE modifica la parte “// print the predictions” del codice precedente (su loop():\nint pred_index = 0;     // Initialize pred_index\nfloat pred_value = 0;   // Initialize pred_value\n\n// print the predictions\nei_printf(\"Predictions \");\nei_printf(\"(DSP: %d ms., Classification: %d ms., Anomaly: %d ms.)\",\n     result.timing.dsp, result.timing.classification, result.timing.anomaly);\nei_printf(\": \\n\");\nfor (size_t ix = 0; ix &lt; EI_CLASSIFIER_LABEL_COUNT; ix++) {\n      ei_printf(\"    %s: \", result.classification[ix].label);\n      ei_printf_float(result.classification[ix].value);\n      ei_printf(\"\\n\");\n\n      if (result.classification[ix].value &gt; pred_value){\n         pred_index = ix;\n         pred_value = result.classification[ix].value;\n      }\n}\n\n// show the inference result on LED\nif (pred_index == 3){\n    digitalWrite(LED_BUILT_IN, LOW); //Turn on\n}\nelse{\n   digitalWrite(LED_BUILT_IN, HIGH); //Turn off\n}\nIl codice completo si trova sul GitHub del progetto. Caricare lo sketch sulla scheda e provare alcune inferenze reali:\n\nL’idea è che il LED sarà ACCESO ogni volta che viene rilevata la parola chiave YES. Allo stesso modo, invece di accendere un LED, questo potrebbe essere un “trigger” per un dispositivo esterno, come abbiamo visto nell’introduzione.",
    "crumbs": [
      "XIAO ESP32S3",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/kws/kws.it.html#conclusione",
    "href": "contents/labs/seeed/xiao_esp32s3/kws/kws.it.html#conclusione",
    "title": "Keyword Spotting (KWS)",
    "section": "Conclusione",
    "text": "Conclusione\nThe Seeed XIAO ESP32S3 Sense è un tiny device gigante! Tuttavia, è potente, affidabile, non costoso, a basso consumo e ha sensori adatti per essere utilizzato nelle applicazioni di apprendimento automatico embedded più comuni come visione e suono. Anche se Edge Impulse non supporta ufficialmente XIAO ESP32S3 Sense (ancora!), ci siamo resi conto che utilizzare Studio per la formazione e l’implementazione è semplice.\n\nNel repository GitHub, si trova l’ultima versione di tutto il codice utilizzato in questo progetto e nei precedenti della serie XIAO ESP32S3.\n\nPrima di concludere, considerare che la classificazione dei suoni è più di una semplice voce. Ad esempio, si possono sviluppare progetti TinyML sul suono in diverse aree, come:\n\nSicurezza (Rilevamento vetri rotti)\nIndustria (Rilevamento di Anomalie)\nMedicina (Russare, Tosse, Malattie polmonari)\nNatura (Controllo alveari, suono degli insetti)",
    "crumbs": [
      "XIAO ESP32S3",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/kws/kws.it.html#risorse",
    "href": "contents/labs/seeed/xiao_esp32s3/kws/kws.it.html#risorse",
    "title": "Keyword Spotting (KWS)",
    "section": "Risorse",
    "text": "Risorse\n\nCodici XIAO ESP32S3\nSottoinsieme del Dataset dei Comandi Vocali di Google\nKWS MFCC Analysis Colab Notebook\nKWS CNN training Colab Notebook\nXIAO ESP32S3 Post-processing Code\nProgetto Edge Impulse",
    "crumbs": [
      "XIAO ESP32S3",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.it.html",
    "href": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.it.html",
    "title": "Classificazione del Movimento e Rilevamento delle Anomalie",
    "section": "",
    "text": "Panoramica\nThe XIAO ESP32S3 Sense, con la sua fotocamera e microfono integrati, è un dispositivo versatile. Ma cosa succede se c’è bisogno di aggiungere un altro tipo di sensore, come un IMU? Nessun problema! Una delle caratteristiche distintive di XIAO ESP32S3 sono i suoi molteplici pin che possono essere utilizzati come bus I2C (pin SDA/SCL), rendendolo una piattaforma adatta per l’integrazione di sensori.",
    "crumbs": [
      "XIAO ESP32S3",
      "Classificazione del Movimento e Rilevamento delle Anomalie"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.it.html#installazione-dellimu",
    "href": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.it.html#installazione-dellimu",
    "title": "Classificazione del Movimento e Rilevamento delle Anomalie",
    "section": "Installazione dell’IMU",
    "text": "Installazione dell’IMU\nQuando si seleziona l’IMU, il mercato offre un’ampia gamma di dispositivi, ognuno con caratteristiche e capacità uniche. Si potrebbe scegliere, ad esempio, ADXL362 (3 assi), MAX21100 (6 assi), MPU6050 (6 assi), LIS3DHTR (3 assi) o LCM20600Seeed Grove— (6 assi), che fa parte dell’IMU 9DOF (lcm20600+AK09918). Questa varietà consente di adattare la scelta alle esigenze specifiche del progetto.\nPer questo progetto, utilizzeremo un’IMU, la MPU6050 (o 6500), un’unità accelerometro/giroscopio a 6 assi a basso costo (meno di 2,00 USD).\n\nAlla fine del lab, commenteremo anche l’utilizzo dell’LCM20600.\n\nMPU-6500 è un dispositivo di Motion Tracking a 6 assi che combina un giroscopio a 3 assi, un accelerometro a 3 assi e un Digital Motion ProcessorTM (DMP) in un piccolo package da 3x3x0,9 mm. È inoltre dotato di un FIFO da 4096 byte che può ridurre il traffico sull’interfaccia del bus seriale e ridurre il consumo energetico consentendo al processore di sistema di leggere i dati del sensore in modalità burst e quindi passare a una modalità a basso consumo.\nCon il suo bus I2C dedicato al sensore, MPU-6500 accetta direttamente input da dispositivi I2C esterni. MPU-6500, con la sua integrazione a 6 assi, DMP on-chip e firmware di calibrazione runtime, consente ai produttori di eliminare la selezione, la qualificazione e l’integrazione a livello di sistema costose e complesse di dispositivi discreti, garantendo prestazioni di movimento ottimali per i consumatori. MPU-6500 è progettato anche per interfacciarsi con più sensori digitali non inerziali, come i sensori di pressione, sulla sua porta I2C ausiliaria.\n\n\nDi solito, le librerie disponibili sono per MPU6050, ma funzionano per entrambi i dispositivi.\n\nCollegamento dell’HW\nCollegare l’IMU allo XIAO secondo lo schema seguente:\n\nMPU6050 SCL –&gt; XIAO D5\nMPU6050 SDA –&gt; XIAO D4\nMPU6050 VCC –&gt; XIAO 3.3V\nMPU6050 GND –&gt; XIAO GND\n\n\nInstallare la libreria\nSi va su Arduino Library Manager e si digita MPU6050. Installare la versione più recente.\n\nScaricare lo sketch MPU6050_Acc_Data_Acquisition.in:\n/*\n * Based on I2C device class (I2Cdev) Arduino sketch for MPU6050 class \n   by Jeff Rowberg &lt;jeff@rowberg.net&gt;\n * and Edge Impulse Data Forwarder Exampe (Arduino) \n   - https://docs.edgeimpulse.com/docs/cli-data-forwarder\n * \n * Developed by M.Rovai @11May23\n */\n\n#include \"I2Cdev.h\"\n#include \"MPU6050.h\"\n#include \"Wire.h\"\n\n#define FREQUENCY_HZ        50\n#define INTERVAL_MS         (1000 / (FREQUENCY_HZ + 1))\n#define ACC_RANGE           1 // 0: -/+2G; 1: +/-4G\n\n// convert factor g to m/s^2^ ==&gt; [-32768, +32767] ==&gt; [-2g, +2g]\n#define CONVERT_G_TO_MS2    (9.81/(16384.0/(1.+ACC_RANGE))) \n\nstatic unsigned long last_interval_ms = 0;\n\nMPU6050 imu;\nint16_t ax, ay, az;\n\nvoid setup() {\n  \n    Serial.begin(115200);\n\n    \n    // initialize device\n    Serial.println(\"Initializing I2C devices...\");\n    Wire.begin();\n    imu.initialize();\n    delay(10);\n    \n//    // verify connection\n//    if (imu.testConnection()) {\n//      Serial.println(\"IMU connected\");\n//    }\n//    else {\n//      Serial.println(\"IMU Error\");\n//    }\n    delay(300);\n    \n    //Set MCU 6050 OffSet Calibration \n    imu.setXAccelOffset(-4732);\n    imu.setYAccelOffset(4703);\n    imu.setZAccelOffset(8867);\n    imu.setXGyroOffset(61);\n    imu.setYGyroOffset(-73);\n    imu.setZGyroOffset(35);\n    \n    /* Set full-scale accelerometer range.\n     * 0 = +/- 2g\n     * 1 = +/- 4g\n     * 2 = +/- 8g\n     * 3 = +/- 16g\n     */\n    imu.setFullScaleAccelRange(ACC_RANGE);\n}\n\nvoid loop() {\n\n      if (millis() &gt; last_interval_ms + INTERVAL_MS) {\n        last_interval_ms = millis();\n        \n        // read raw accel/gyro measurements from device\n        imu.getAcceleration(&ax, &ay, &az);\n\n        // converting to m/s^2^\n        float ax_m_s^2^ = ax * CONVERT_G_TO_MS2;\n        float ay_m_s^2^ = ay * CONVERT_G_TO_MS2;\n        float az_m_s^2^ = az * CONVERT_G_TO_MS2;\n\n        Serial.print(ax_m_s^2^); \n        Serial.print(\"\\t\");\n        Serial.print(ay_m_s^2^); \n        Serial.print(\"\\t\");\n        Serial.println(az_m_s^2^); \n      }\n}\nAlcuni commenti sul codice:\nNotare che i valori generati dall’accelerometro e dal giroscopio hanno un intervallo: [-32768, +32767], quindi, ad esempio, se viene utilizzato l’intervallo di default dell’accelerometro, l’intervallo in G dovrebbe essere: [-2g, +2g]. Quindi, “1G” significa 16384.\nPer la conversione in m/s2, ad esempio, si può definire quanto segue:\n#define CONVERT_G_TO_MS2 (9.81/16384.0)\nNel codice, è stata lasciata un’opzione (ACC_RANGE) da impostare su 0 (+/-2G) o 1 (+/- 4G). Useremo +/-4G; dovrebbe bastare per noi. In questo caso.\nAcquisiremo i dati dell’accelerometro a una frequenza di 50 Hz e i dati di accelerazione saranno inviati alla porta seriale come metri al secondo quadrato (m/s2).\nQuando si esegue il codice con l’IMU appoggiata sul tavolo, i dati dell’accelerometro mostrati sul monitor seriale dovrebbero essere circa 0.00, 0.00 e 9.81. Se i valori sono molto diversi, si deve calibrare l’IMU.\nL’MCU6050 può essere calibrato usando lo sketch: mcu6050-calibration.ino.\nEseguire il codice. Sul Serial Monitor verrà visualizzato quanto segue:\n\nInviare un carattere qualsiasi (nell’esempio sopra, “x”) e la calibrazione dovrebbe iniziare.\n\nNotare che c’è un messaggio MPU6050 di connessione fallita. Ignorare questo messaggio. Per qualche motivo, imu.testConnection() non restituisce un risultato corretto.\n\nAlla fine, si riceveranno i valori di offset da utilizzare su tutti gli sketch:\n\nPrendere i valori e usarli nella configurazione:\n//Set MCU 6050 OffSet Calibration \nimu.setXAccelOffset(-4732);\nimu.setYAccelOffset(4703);\nimu.setZAccelOffset(8867);\nimu.setXGyroOffset(61);\nimu.setYGyroOffset(-73);\nimu.setZGyroOffset(35);\nOra, eseguire lo sketch MPU6050_Acc_Data_Acquisition.in:\nUna volta eseguito lo sketch precedente, aprire il Serial Monitor:\n\nOppure controllare il Plotter:\n\nSpostare il dispositivo sui tre assi. Si dovrebbe vedere la variazione sul Plotter:",
    "crumbs": [
      "XIAO ESP32S3",
      "Classificazione del Movimento e Rilevamento delle Anomalie"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.it.html#il-progetto-tinyml-motion-classification",
    "href": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.it.html#il-progetto-tinyml-motion-classification",
    "title": "Classificazione del Movimento e Rilevamento delle Anomalie",
    "section": "Il Progetto TinyML Motion Classification",
    "text": "Il Progetto TinyML Motion Classification\nPer il nostro lab, simuleremo sollecitazioni meccaniche nel trasporto. Il nostro problema sarà classificare quattro classi di movimento:\n\nMaritime (Pallet in navi)\nTerrestrial (Pallet in un camion o treno)\nLift (Pallet movimentati da carrello elevatore)\nIdle (Pallet in magazzini)\n\nQuindi, per iniziare, dovremmo raccogliere dati. Quindi, gli accelerometri forniranno i dati sul pallet (o sul contenitore).\n\nDalle immagini sopra, possiamo vedere che i movimenti principalmente orizzontali dovrebbero essere associati alla “classe Terrestrial”, i movimenti verticali alla “classe Lift”, nessuna attività alla “classe Idle” e il movimento su tutti e tre gli assi alla classe Maritime.",
    "crumbs": [
      "XIAO ESP32S3",
      "Classificazione del Movimento e Rilevamento delle Anomalie"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.it.html#collegamento-del-dispositivo-a-edge-impulse",
    "href": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.it.html#collegamento-del-dispositivo-a-edge-impulse",
    "title": "Classificazione del Movimento e Rilevamento delle Anomalie",
    "section": "Collegamento del dispositivo a Edge Impulse",
    "text": "Collegamento del dispositivo a Edge Impulse\nPer la raccolta dei dati, dovremmo prima collegare il dispositivo a Edge Impulse Studio, che verrà utilizzato anche per la pre-elaborazione dei dati, l’addestramento del modello, i test e la distribuzione.\n\nSeguire le istruzioni qui per installare Node.js e Edge Impulse CLI sul computer.\n\nDal momento che XIAO ESP32S3 non è più una scheda di sviluppo completamente supportata da Edge Impulse, dovremmo, ad esempio, usare CLI Data Forwarder per acquisire dati dal nostro sensore e inviarli allo Studio, come mostrato in questo diagramma:\n\n\nIn alternativa, si possono acquisire i dati “offline”, memorizzarli su una scheda SD o inviarli al computer tramite Bluetooth o Wi-Fi. In questo video, si possono imparare modi alternativi per inviare dati a Edge Impulse Studio.\n\nCollegare il dispositivo alla porta seriale ed eseguire il codice precedente per catturare i dati IMU (Accelerometro), “stampandoli” sulla seriale. Questo consentirà a Edge Impulse Studio di “catturarli”.\nSi va alla pagina Edge Impulse e si crea un progetto.\n\n\nLa lunghezza massima per un nome di libreria Arduino è di 63 caratteri. Notare che Studio nominerà la libreria finale usando il nome del progetto e vi includerà “_inference”. Il nome scelto inizialmente non ha funzionato quando è stato provato a distribuire la libreria Arduino perché risultava in 64 caratteri. Quindi, lo si deve cambiare eliminando la parte “anomaly detection”.\n\nAvviare CLI Data Forwarder sul terminale, immettendo (se è la prima volta) il seguente comando:\nedge-impulse-data-forwarder --clean\nQuindi, immettere le credenziali EI e scegli il progetto, le variabili e i nomi dei dispositivi:\n\nSi va al progetto EI e si verifica se il dispositivo è connesso (il punto dovrebbe essere verde):",
    "crumbs": [
      "XIAO ESP32S3",
      "Classificazione del Movimento e Rilevamento delle Anomalie"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.it.html#raccolta-dati",
    "href": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.it.html#raccolta-dati",
    "title": "Classificazione del Movimento e Rilevamento delle Anomalie",
    "section": "Raccolta Dati",
    "text": "Raccolta Dati\nCome discusso in precedenza, dovremmo catturare dati da tutte e quattro le classi Transportation”. Immaginiamo di avere un contenitore con un accelerometro incorporato:\n\nOra immaginiamo che il contenitore sia su una barca, di fronte a un oceano in tempesta, su un camion, ecc.:\n\nMaritime (Pallet in navi)\n\nSpostare lo XIAO in tutte le direzioni, simulando un movimento ondulatorio della barca.\n\nTerrestrial (Pallet in un camion o treno)\n\nSpostare lo XIAO su una linea orizzontale.\n\nLift (Pallet movimentati da carrello elevatore)\n\nSpostare lo XIAO su una linea verticale.\n\nIdle (Pallet in magazzini)\n\nLasciare lo XIAO sul tavolo.\n\n\n\nDi seguito è riportato un campione (dati grezzi) di 10 secondi:\n\nAd esempio, si possono catturare 2 minuti (dodici campioni da 10 secondi ciascuno) per le quattro classi. Utilizzando i “3 punti” dopo ciascuno dei campioni, selezionarne 2, spostandoli per il set di test (o utilizzare lo strumento automatico Train/Test Split nella scheda Danger Zone della Dashboard). Di seguito, si possono vedere i dataset dei risultati:",
    "crumbs": [
      "XIAO ESP32S3",
      "Classificazione del Movimento e Rilevamento delle Anomalie"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.it.html#pre-elaborazione-dei-dati",
    "href": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.it.html#pre-elaborazione-dei-dati",
    "title": "Classificazione del Movimento e Rilevamento delle Anomalie",
    "section": "Pre-elaborazione dei Dati",
    "text": "Pre-elaborazione dei Dati\nIl tipo di dati grezzi catturato dall’accelerometro è una “serie temporale” e dovrebbe essere convertito in “dati tabulari”. Possiamo effettuare questa conversione utilizzando una finestra scorrevole sui dati campione. Ad esempio, nella figura sottostante,\n\nPossiamo vedere 10 secondi di dati dell’accelerometro catturati con un “sample rate (SR)” [frequenza di campionamento] di 50 Hz. Una finestra di 2 secondi catturerà 300 dati (3 assi x 2 secondi x 50 campioni). Faremo scorrere questa finestra ogni 200 ms, creando un set di dati più grande in cui ogni istanza ha 300 feature grezze.\n\nSi deve usare il miglior SR per il proprio caso, considerando il teorema di Nyquist, che afferma che un segnale periodico deve essere campionato a più del doppio della componente di frequenza più alta del segnale.\n\nLa pre-elaborazione dei dati è un’area impegnativa per l’apprendimento automatico embedded. Tuttavia, Edge Impulse aiuta a superare questo problema con la sua fase di pre-elaborazione dell’elaborazione del segnale digitale (DSP) e, più specificamente, le Spectral Feature.\nSu Studio, questo dataset sarà l’input di un blocco Spectral Analysis, che è eccellente per analizzare il movimento ripetitivo, come i dati degli accelerometri. Questo blocco eseguirà un DSP (Digital Signal Processing), estraendo caratteristiche come “FFT” o “Wavelets”. Nel caso più comune, FFT, le le feature Time Domain Statistical  per asse/canale sono:\n\nRMS\nSkewness\nKurtosis\n\nE la Frequency Domain Spectral features per asse/canale sono:\n\nSpectral Power [Potenza spettrale]\nSkewness\nKurtosis\n\nAd esempio, per una lunghezza FFT di 32 punti, l’output risultante del blocco di analisi spettrale sarà di 21 feature per asse (un totale di 63 feature).\nQuelle 63 feature saranno il tensore di input di un classificatore di reti neurali e il modello di rilevamento delle anomalie (K-Means).\n\nSe ne può sapere di più esplorando il laboratorio DSP Spectral Features",
    "crumbs": [
      "XIAO ESP32S3",
      "Classificazione del Movimento e Rilevamento delle Anomalie"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.it.html#progettazione-del-modello",
    "href": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.it.html#progettazione-del-modello",
    "title": "Classificazione del Movimento e Rilevamento delle Anomalie",
    "section": "Progettazione del Modello",
    "text": "Progettazione del Modello\nIl nostro classificatore sarà una Dense Neural Network (DNN) che avrà 63 neuroni sul suo layer di input, due layer nascosti con 20 e 10 neuroni e un layer di output con quattro neuroni (uno per ogni classe), come mostrato qui:",
    "crumbs": [
      "XIAO ESP32S3",
      "Classificazione del Movimento e Rilevamento delle Anomalie"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.it.html#impulse-design",
    "href": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.it.html#impulse-design",
    "title": "Classificazione del Movimento e Rilevamento delle Anomalie",
    "section": "Impulse Design",
    "text": "Impulse Design\nUn impulso prende dati grezzi, usa l’elaborazione del segnale per estrarre le caratteristiche e poi usa un blocco di apprendimento per classificare nuovi dati.\nSfruttiamo anche un secondo modello, il K-means, che può essere usato per le “Anomaly Detection” [rilevamento delle anomalie]. Se immaginiamo di poter avere le nostre classi note come cluster, qualsiasi campione che non potrebbe adattarsi potrebbe essere un valore anomalo, un’anomalia (ad esempio, un container che rotola fuori da una nave in mare).\n\n\nImmaginiamo il nostro XIAO che rotola o si muove capovolto, su un complemento di movimento diverso da quello addestrato\n\n\nDi seguito è riportato il nostro progetto Impulse finale:",
    "crumbs": [
      "XIAO ESP32S3",
      "Classificazione del Movimento e Rilevamento delle Anomalie"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.it.html#generazione-di-feature",
    "href": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.it.html#generazione-di-feature",
    "title": "Classificazione del Movimento e Rilevamento delle Anomalie",
    "section": "Generazione di feature",
    "text": "Generazione di feature\nA questo punto del nostro progetto, abbiamo definito il metodo di pre-elaborazione e il modello progettato. Ora è il momento di terminare il lavoro. Per prima cosa, prendiamo i dati grezzi (tipo serie temporale) e convertiamoli in dati tabulari. Andiamo alla scheda Spectral Features e selezioniamo Save Parameters:\n\nNel menù in alto, selezionare l’opzione Generate Features e il pulsante Generate Features. Ogni dato della finestra di 2 secondi verrà convertito in un punto dati di 63 feature.\n\nFeature Explorer mostrerà quei dati in 2D usando UMAP. Uniform Manifold Approximation and Projection (UMAP) è una tecnica di riduzione delle dimensioni che può essere usata per la visualizzazione in modo simile a t-SNE ma anche per la riduzione generale delle dimensioni non lineari.\n\nLa visualizzazione consente di verificare che le classi presentino un’eccellente separazione, il che indica che il classificatore dovrebbe funzionare bene.\n\nFacoltativamente, si può analizzare l’importanza relativa di ogni feature per una classe rispetto ad altre classi.",
    "crumbs": [
      "XIAO ESP32S3",
      "Classificazione del Movimento e Rilevamento delle Anomalie"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.it.html#training",
    "href": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.it.html#training",
    "title": "Classificazione del Movimento e Rilevamento delle Anomalie",
    "section": "Training",
    "text": "Training\nIl nostro modello ha quattro layer, come mostrato di seguito:\n\nCome iperparametri, useremo un Learning Rate di 0,005 e il 20% di dati per la convalida per 30 epoche. Dopo l’addestramento, possiamo vedere che l’accuratezza è del 97%.\n\nPer il rilevamento delle anomalie, dovremmo scegliere le feature suggerite che sono esattamente le più importanti nell’estrazione delle feature. Il numero di cluster sarà 32, come suggerito dallo Studio:",
    "crumbs": [
      "XIAO ESP32S3",
      "Classificazione del Movimento e Rilevamento delle Anomalie"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.it.html#test",
    "href": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.it.html#test",
    "title": "Classificazione del Movimento e Rilevamento delle Anomalie",
    "section": "Test",
    "text": "Test\nUtilizzando il 20% dei dati lasciati indietro durante la fase di acquisizione dati, possiamo verificare come si comporterà il nostro modello con dati sconosciuti; se non al 100% (come previsto), il risultato non è stato così buono (8%), principalmente a causa della classe “terrestrial”. Una volta che abbiamo quattro classi (il cui output dovrebbe aggiungere 1,0), possiamo impostare una soglia inferiore affinché una classe sia considerata valida (ad esempio, 0.4):\n\nOra, l’accuratezza del test salirà al 97%.\n\nSi deve anche usare il dispositivo (che è ancora connesso allo Studio) ed eseguire una “Live Classification”.\n\nNotare che qui si cattureranno dati reali con il dispositivo e si caricheranno sullo Studio, dove verrà presa un’inferenza usando il modello addestrato (ma il modello NON è nel dispositivo).",
    "crumbs": [
      "XIAO ESP32S3",
      "Classificazione del Movimento e Rilevamento delle Anomalie"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.it.html#distribuzione",
    "href": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.it.html#distribuzione",
    "title": "Classificazione del Movimento e Rilevamento delle Anomalie",
    "section": "Distribuzione",
    "text": "Distribuzione\nAdesso è il momento della magia! Studio impacchetterà tutte le librerie necessarie, le funzioni di pre-elaborazione e i modelli addestrati, scaricandoli sul computer. Si deve selezionare l’opzione “Arduino Library” e, in basso, scegliere Quantized (Int8) e Build. Verrà creato un file Zip e scaricato sul computer.\n\nSull’IDE Arduino, si va alla scheda Sketch, si seleziona l’opzione Add.ZIP Library e si sceglie il file.zip scaricato da Studio:",
    "crumbs": [
      "XIAO ESP32S3",
      "Classificazione del Movimento e Rilevamento delle Anomalie"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.it.html#inferenza",
    "href": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.it.html#inferenza",
    "title": "Classificazione del Movimento e Rilevamento delle Anomalie",
    "section": "Inferenza",
    "text": "Inferenza\nOra è il momento di un vero test. Faremo inferenze completamente scollegate dallo Studio. Modifichiamo uno degli esempi di codice creati quando si distribuisce la libreria Arduino.\nNell’IDE Arduino, si va alla scheda File/Examples e si cerca il progetto, e negli esempi, si seleziona nano_ble_sense_accelerometer:\n\nOvviamente, questa non è la propria board, ma possiamo far funzionare il codice con solo poche modifiche.\nAd esempio, all’inizio del codice, c’è la libreria relativa ad Arduino Sense IMU:\n/* Includes --------------------------------------------------------------- */\n#include &lt;XIAO-ESP32S3-Motion-Classification_inferencing.h&gt;\n#include &lt;Arduino_LSM9DS1.h&gt;\nSostituire la parte “include” con il codice relativo all’IMU:\n#include &lt;XIAO-ESP32S3-Motion-Classification_inferencing.h&gt;\n#include \"I2Cdev.h\"\n#include \"MPU6050.h\"\n#include \"Wire.h\"\nCambiare le Constant Defines\n/* Constant defines ------------------------------------------------------- */\nMPU6050 imu;\nint16_t ax, ay, az;\n\n#define ACC_RANGE           1 // 0: -/+2G; 1: +/-4G\n#define CONVERT_G_TO_MS2    (9.81/(16384/(1.+ACC_RANGE)))\n#define MAX_ACCEPTED_RANGE  (2*9.81)+(2*9.81)*ACC_RANGE\nNella funzione di configurazione, avviare l’IMU per impostare i valori di offset e il range:\n// initialize device\nSerial.println(\"Initializing I2C devices...\");\nWire.begin();\nimu.initialize();\ndelay(10);\n\n//Set MCU 6050 OffSet Calibration \nimu.setXAccelOffset(-4732);\nimu.setYAccelOffset(4703);\nimu.setZAccelOffset(8867);\nimu.setXGyroOffset(61);\nimu.setYGyroOffset(-73);\nimu.setZGyroOffset(35);\n\nimu.setFullScaleAccelRange(ACC_RANGE);\nNella funzione loop, i buffer buffer[ix], buffer[ix + 1] e buffer[ix + 2] riceveranno i dati a 3 assi catturati dall’accelerometro. Nel codice originale, si ha la riga:\nIMU.readAcceleration(buffer[ix], buffer[ix + 1], buffer[ix + 2]);\nModificarlo con questo blocco di codice:\nimu.getAcceleration(&ax, &ay, &az);       \nbuffer[ix + 0] = ax;\nbuffer[ix + 1] = ay;\nbuffer[ix + 2] = az;\nSi deve cambiare l’ordine dei due blocchi di codice seguenti. Per prima cosa, si esegue la conversione in dati grezzi in “Meters per squared second (ms2)”, seguita dal test riguardante il range di accettazione massimo (che qui è in ms2, ma su Arduino era in Gs):\nbuffer[ix + 0] *= CONVERT_G_TO_MS2;\nbuffer[ix + 1] *= CONVERT_G_TO_MS2;\nbuffer[ix + 2] *= CONVERT_G_TO_MS2;\n\nfor (int i = 0; i &lt; 3; i++) {\n     if (fabs(buffer[ix + i]) &gt; MAX_ACCEPTED_RANGE) {\n        buffer[ix + i] = ei_get_sign(buffer[ix + i]) * MAX_ACCEPTED_RANGE;\n     }\n}\nE questo è tutto! Ora si può caricare il codice sul dispositivo e procedere con le inferenze. Il codice completo è disponibile sul GitHub del progetto.\nOra si devono provare i movimenti, osservando il risultato dell’inferenza di ogni classe sulle immagini:\n\n\n\n\nE, naturalmente, qualche “anomalia”, ad esempio, capovolgere lo XIAO. Il punteggio anomalo sarà superiore a 1:",
    "crumbs": [
      "XIAO ESP32S3",
      "Classificazione del Movimento e Rilevamento delle Anomalie"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.it.html#conclusione",
    "href": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.it.html#conclusione",
    "title": "Classificazione del Movimento e Rilevamento delle Anomalie",
    "section": "Conclusione",
    "text": "Conclusione\nPer quanto riguarda l’IMU, questo progetto ha utilizzato l’MPU6050 a basso costo, ma potrebbe utilizzare anche altre IMU, ad esempio l’LCM20600 (6 assi), che fa parte del Seeed Grove - IMU 9DOF (lcm20600+AK09918). Si può sfruttare questo sensore, che ha integrato un connettore Grove, che può essere utile nel caso in cui si utilizza lo XIAO con una scheda di estensione, come mostrato di seguito:\n\nSi possono seguire le istruzioni qui per collegare l’IMU alla MCU. Notare solo che per usare l’accelerometro Grove ICM20600, è essenziale aggiornare i file I2Cdev.cpp e I2Cdev.h scaricabili dalla libreria fornita da Seeed Studio. Per farlo, si sostituiscono entrambi i file da questo link. Si trova uno sketch per testare l’IMU sul progetto GitHub: accelerometer_test.ino.\n\nNel repository GitHub del progetto, si trova l’ultima versione di tutto il codice e altri documenti: XIAO-ESP32S3 - IMU.",
    "crumbs": [
      "XIAO ESP32S3",
      "Classificazione del Movimento e Rilevamento delle Anomalie"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.it.html#risorse",
    "href": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.it.html#risorse",
    "title": "Classificazione del Movimento e Rilevamento delle Anomalie",
    "section": "Risorse",
    "text": "Risorse\n\nCodici XIAO ESP32S3\nEdge Impulse Spectral Features Block Colab Notebook\nProgetto Edge Impulse",
    "crumbs": [
      "XIAO ESP32S3",
      "Classificazione del Movimento e Rilevamento delle Anomalie"
    ]
  },
  {
    "objectID": "contents/labs/raspi/setup/setup.it.html",
    "href": "contents/labs/raspi/setup/setup.it.html",
    "title": "Setup",
    "section": "",
    "text": "Panoramica\nIl Raspberry Pi è un potente e versatile computer a scheda singola che è diventato uno strumento essenziale per gli ingegneri di varie discipline. Sviluppati dalla Raspberry Pi Foundation, questi dispositivi compatti offrono una combinazione unica di convenienza, potenza di calcolo e ampie capacità GPIO (General Purpose Input/Output), rendendoli ideali per la prototipazione, lo sviluppo di sistemi embedded e progetti di ingegneria avanzata.",
    "crumbs": [
      "Raspberry Pi",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/raspi/setup/setup.it.html#panoramica",
    "href": "contents/labs/raspi/setup/setup.it.html#panoramica",
    "title": "Setup",
    "section": "",
    "text": "Caratteristiche Principali\n\nPotenza di Calcolo: Nonostante le dimensioni ridotte, Raspberry Pi offre notevoli capacità di elaborazione, con gli ultimi modelli dotati di processori ARM multi-core e fino a 8 GB di RAM.\nInterfaccia GPIO: L’header GPIO a 40 pin consente l’interazione diretta con sensori, attuatori e altri componenti elettronici, facilitando i progetti di integrazione hardware-software.\nAmpia Connettività: Wi-Fi, Bluetooth, Ethernet e più porte USB integrate consentono diversi progetti di comunicazione e networking.\nAccesso Hardware di Basso Livello: Raspberry Pi fornisce accesso a interfacce come I2C, SPI e UART, consentendo un controllo dettagliato e la comunicazione con dispositivi esterni.\nCapacità in Tempo Reale: Con una configurazione appropriata, Raspberry Pi può essere utilizzato per applicazioni soft in tempo reale, rendendolo adatto per sistemi di controllo e attività di elaborazione del segnale.\nEfficienza Energetica: Il basso consumo energetico consente progetti alimentati a batteria e a basso consumo energetico, in particolare in modelli come Pi Zero.\n\n\n\nModelli Raspberry Pi (trattati in questo libro)\n\nRaspberry Pi Zero 2 W (Raspi-Zero):\n\nIdeale per: sistemi embedded compatti\nSpecifiche principali: CPU single-core da 1 GHz (ARM Cortex-A53), 512 MB di RAM, consumo energetico minimo\n\nRaspberry Pi 5 (Raspi-5):\n\nIdeale per: applicazioni più esigenti come edge computing, computer vision e applicazioni edgeAI, inclusi LLM.\nSpecifiche principali: CPU quad-core da 2,4 GHz (ARM Cortex A-76), fino a 8 GB di RAM, interfaccia PCIe per espansioni\n\n\n\n\nApplicazioni di Ingegneria\n\nProgettazione di Sistemi Embedded: Sviluppo e prototipi di sistemi embedded per applicazioni reali.\nIoT e Dispositivi in Rete: Creazione di dispositivi interconnessi ed esplorazione di protocolli come MQTT, CoAP e HTTP/HTTPS.\nSistemi di Controllo: Implementazione di loop di controllo feedback, controller PID e interfaccia per attuatori.\nVisione Artificiale e IA: Utilizzo di librerie come OpenCV e TensorFlow Lite per l’elaborazione delle immagini e l’apprendimento automatico in edge.\nAcquisizione e Analisi dei Dati: Raccolta dati dei sensori, analisi in tempo reale e creazione di sistemi di registrazione dei dati.\nRobotica: Creazione di controller per robot, algoritmi di pianificazione del movimento e interfaccie con driver di motori.\nElaborazione del Segnale: Analisi del segnale in tempo reale, filtraggio e applicazioni DSP.\nSicurezza di Rete: Imposta VPN, firewall ed esplora i test di penetrazione della rete.\n\nQuesto tutorial guiderà nell’impostazione dei modelli Raspberry Pi più comuni, consentendo di iniziare rapidamente il progetto di apprendimento automatico. Tratteremo la configurazione hardware, l’installazione del sistema operativo e la configurazione iniziale, concentrandoci sulla preparazione del Pi per le applicazioni di apprendimento automatico.",
    "crumbs": [
      "Raspberry Pi",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/raspi/setup/setup.it.html#panoramica-hardware",
    "href": "contents/labs/raspi/setup/setup.it.html#panoramica-hardware",
    "title": "Setup",
    "section": "Panoramica Hardware",
    "text": "Panoramica Hardware\n\nRaspberry Pi Zero 2W\n\n\nProcessore: CPU Arm Cortex-A53 quad-core a 64 bit da 1 GHz\nRAM: SDRAM da 512 MB\nWireless: LAN wireless 802.11 b/g/n da 2,4 GHz, Bluetooth 4.2, BLE\nPorte: Mini HDMI, micro USB OTG, connettore per fotocamera CSI-2\nAlimentazione: 5 V tramite porta micro USB\n\n\n\nRaspberry Pi 5\n\n\nProcessore:\n\nPi 5: CPU Arm Cortex-A76 quad-core a 64 bit @ 2,4 GHz\nPi 4: SoC Cortex-A72 quad-core (ARM v8) a 64 bit @ 1,5 GHz\n\nRAM: opzioni da 2 GB, 4 GB o 8 GB (8 GB consigliati per attività di IA)\nWireless: Wireless 802.11ac dual-band, Bluetooth 5.0\nPorte: 2 porte micro HDMI, 2 porte USB 3.0, 2 porte USB 2.0, porta fotocamera CSI, porta display DSI\nAlimentazione: 5 V CC tramite connettore USB-C (3 A)\n\n\nNei laboratori, useremo nomi diversi per riferirci al Raspberry: Raspi, Raspi-5, Raspi-Zero, ecc. Di solito, si usa Raspi quando le istruzioni o i commenti si applicano a tutti i modelli.",
    "crumbs": [
      "Raspberry Pi",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/raspi/setup/setup.it.html#installazione-del-sistema-operativo",
    "href": "contents/labs/raspi/setup/setup.it.html#installazione-del-sistema-operativo",
    "title": "Setup",
    "section": "Installazione del Sistema Operativo",
    "text": "Installazione del Sistema Operativo\n\nIl sistema operativo (SO)\nUn sistema operativo (SO) è un software fondamentale che gestisce le risorse hardware e software del computer, fornendo servizi standard per i programmi per computer. È il software principale che gira su un computer, fungendo da intermediario tra hardware e software applicativo. Il SO gestisce la memoria, i processi, i driver dei dispositivi, i file e i protocolli di sicurezza del computer.\n\nFunzioni chiave:\n\nGestione dei processi: Assegnazione del tempo della CPU a diversi programmi\nGestione della memoria: Assegnazione e liberazione della memoria in base alle necessità\nGestione del file system: Organizzazione e monitoraggio di file e directory\nGestione dei dispositivi: Comunicazione con dispositivi hardware collegati\nInterfaccia utente: Fornitura di un modo per gli utenti di interagire con il computer\n\nComponenti:\n\nKernel: Il nucleo del sistema operativo che gestisce le risorse hardware\nShell: L’interfaccia utente per interagire con il sistema operativo\nFile system: Organizzazione e gestione dell’archiviazione dei dati\nDevice driver: Software che consente al sistema operativo di comunicare con l’hardware\n\n\nIl Raspberry Pi esegue una versione specializzata di Linux progettata per sistemi embedded. Questo sistema operativo, in genere una variante di Debian chiamata Raspberry Pi OS (in precedenza Raspbian), è ottimizzato per l’architettura basata su ARM del Pi e per le risorse limitate.\n\nL’ultima versione di Raspberry Pi OS è basata su Debian Bookworm.\n\nCaratteristiche principali:\n\nLeggero: Progettato per funzionare in modo efficiente sull’hardware del Pi.\nVersatile: Supporta un’ampia gamma di applicazioni e linguaggi di programmazione.\nOpen source: Consente la personalizzazione e i miglioramenti guidati dalla comunità.\nSupporto GPIO: Consente l’interazione con sensori e altri hardware tramite i pin del Pi.\nAggiornamenti regolari: Costantemente migliorato per prestazioni e sicurezza.\n\nEmbedded Linux sul Raspberry Pi fornisce un sistema operativo completo in un pacchetto compatto, rendendolo ideale per progetti che vanno da semplici dispositivi IoT ad applicazioni di apprendimento automatico edge più complesse. La sua compatibilità con gli strumenti e le librerie Linux standard lo rende una potente piattaforma per lo sviluppo e la sperimentazione.\n\n\nInstallazione\nPer usare Raspberry Pi, avremo bisogno di un sistema operativo. Di default, Raspberry Pi verifica la presenza di un sistema operativo su qualsiasi scheda SD inserita nello slot, quindi dovremmo installare un sistema operativo usando Raspberry Pi Imager.\nRaspberry Pi Imager è uno strumento per scaricare e scrivere immagini su macOS, Windows e Linux. Include molte immagini di sistemi operativi popolari per Raspberry Pi. Useremo Imager anche per preconfigurare credenziali e impostazioni di accesso remoto.\nSeguire i passaggi per installare il sistema operativo nel Raspi.\n\nScaricare e installare Raspberry Pi Imager sul computer.\nInserire una scheda microSD nel computer (si consiglia una scheda SD da 32 GB).\nAprire Raspberry Pi Imager e selezionare il modello di Raspberry Pi.\nScegliere il sistema operativo appropriato:\n\nFor Raspi-Zero: Ad esempio, si può selezionare: Raspberry Pi OS Lite (64-bit).\n\n\n\n\nimg\n\n\n\nGrazie alla ridotta SDRAM (512 MB), il sistema operativo consigliato per Raspi-Zero è la versione a 32 bit. Tuttavia, per eseguire alcuni modelli di apprendimento automatico, come YOLOv8 di Ultralitics, dovremmo usare la versione a 64 bit. Sebbene Raspi-Zero possa eseguire un desktop, sceglieremo la versione LITE (senza Desktop) per ridurre la RAM necessaria per il normale funzionamento.\n\n\nPer Raspi-5: possiamo selezionare la versione completa a 64 bit, che include un desktop: Raspberry Pi OS (64-bit)\n\n\nSelezionare la scheda microSD come dispositivo di archiviazione.\nCliccare su Next e poi sull’icona dell’ingranaggio per accedere alle opzioni avanzate.\nImpostare hostname, nome utente e password di Raspi, configurare il WiFi e abilitare SSH (molto importante!)\n\n\n\nScrivere l’immagine sulla scheda microSD.\n\n\nNegli esempi qui, useremo nomi host diversi a seconda del dispositivo utilizzato: raspi, raspi-5, raspi-Zero, ecc. Sarebbe utile se lo si sostituisse con quello in uso.\n\n\n\nConfigurazione Iniziale\n\nInserire la scheda microSD nel Raspberry Pi.\nCollegare l’alimentazione per avviare il Raspberry Pi.\nAttendere il completamento del processo di boot iniziale (potrebbero volerci alcuni minuti).\n\n\nI comandi Linux più comuni da usare con Raspi si trovano qui o qui.",
    "crumbs": [
      "Raspberry Pi",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/raspi/setup/setup.it.html#accesso-remoto",
    "href": "contents/labs/raspi/setup/setup.it.html#accesso-remoto",
    "title": "Setup",
    "section": "Accesso Remoto",
    "text": "Accesso Remoto\n\nAccesso SSH\nIl modo più semplice per interagire con Raspi-Zero è tramite SSH (“Headless”). Si può usare un terminale (MAC/Linux), PuTTy (Windows) o qualsiasi altro.\n\nTrovare l’indirizzo IP del Raspberry Pi (ad esempio, controllando il router).\nSul computer, aprire un terminale e connettersi tramite SSH:\nssh username@[raspberry_pi_ip_address]\nIn alternativa, se non si ha l’indirizzo IP, si può provare quanto segue:\nssh username@hostname.local\nper esempio, ssh mjrovai@rpi-5.local , ssh mjrovai@raspi.local , ecc.\n\n\n\nimg\n\n\nQuando si vede il prompt:\nmjrovai@rpi-5:~ $\nSignifica che si sta interagendo da remoto con il Raspi. È una buona norma aggiornare/migliorare il sistema regolarmente. Per questo, si deve eseguire:\nsudo apt-get update\nsudo apt upgrade\nSi deve confermare l’indirizzo IP Raspi. Sul terminale, si può usare:\nhostname -I\n\n\n\n\nPer spegnere il Raspi tramite terminale:\nPer spegnere il Raspberry Pi, ci sono idee migliori che staccare semplicemente il cavo di alimentazione. Questo perché il Raspi potrebbe ancora scrivere dati sulla scheda SD, nel qual caso il semplice spegnimento potrebbe causare la perdita di dati o, peggio ancora, una scheda SD danneggiata.\nPer uno spegnimento di sicurezza, usare la riga di comando:\nsudo shutdown -h now\n\nPer evitare possibili perdite di dati e danneggiamenti della scheda SD, prima di rimuovere l’alimentazione, si deve attendere qualche secondo dopo lo spegnimento affinché il LED del Raspberry Pi smetta di lampeggiare e si spenga. Una volta che il LED si spegne, è sicuro spegnere.\n\n\n\nTrasferire file tra il Raspi e un computer\nIl trasferimento di file tra il Raspi e il nostro computer principale può essere effettuato tramite una chiavetta USB, direttamente sul terminale (con scp) o un programma FTP sulla rete.\n\nUtilizzo del Protocollo Secure Copy Protocol (scp):\n\nCopiare i file sul Raspberry Pi\nCreiamo un file di testo sul nostro computer, ad esempio, test.txt.\n\n\nSi può usare qualsiasi editor di testo. Nello stesso terminale, un’opzione è nano.\n\nPer copiare il file test.txt dal personal computer alla cartella home di un utente sul Raspberry Pi, si esegue il seguente comando dalla directory contenente test.txt, sostituendo il segnaposto &lt;username&gt; con il nome utente che si usa per accedere al Raspberry Pi e il segnaposto &lt;pi_ip_address&gt; con l’indirizzo IP del Raspberry Pi:\n$ scp test.txt &lt;username&gt;@&lt;pi_ip_address&gt;:~/\n\nNotare che ~/ significa che sposteremo il file nella ROOT del Raspi. Si può scegliere qualsiasi cartella nel Raspi. Ma si deve creare la cartella prima di eseguire scp, poiché scp non lo fa automaticamente.\n\nAd esempio, trasferiamo il file test.txt nella ROOT dell’Raspi-zero, che ha un IP di 192.168.4.210:\nscp test.txt mjrovai@192.168.4.210:~/\n\nAbbiamo usato un profilo diverso per differenziare i terminali. L’azione di cui sopra avviene sul computer. Ora, andiamo sul Raspi (utilizzando SSH) e controlliamo se il file è lì:\n\n\n\nCopiare i file dal Raspberry Pi\nPer copiare un file chiamato test.txt dalla directory home di un utente su un Raspberry Pi alla directory corrente su un altro computer, si esegue il seguente comando sul computer host:\n$ scp &lt;username&gt;@&lt;pi_ip_address&gt;:myfile.txt .\nPer esempio:\nSul Raspi, creiamo una copia del file con un altro nome:\ncp test.txt test_2.txt\nE sul computer host (in questo caso, un Mac)\nscp mjrovai@192.168.4.210:test_2.txt .\n\n\n\n\nTrasferimento di file tramite FTP\nÈ anche possibile trasferire file tramite FTP, come FileZilla FTP Client. Seguire le istruzioni, installare il programma per il proprio sistema operativo desktop e usare l’indirizzo IP Raspi come Host. Per esempio:\nsftp://192.168.4.210\ne inserire nome utente e password di Raspi . Premendo Quickconnect si apriranno due finestre, una per il desktop del computer host (destra) e un’altra per il Raspi (sinistra).",
    "crumbs": [
      "Raspberry Pi",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/raspi/setup/setup.it.html#aumentare-la-memoria-swap",
    "href": "contents/labs/raspi/setup/setup.it.html#aumentare-la-memoria-swap",
    "title": "Setup",
    "section": "Aumentare la memoria SWAP",
    "text": "Aumentare la memoria SWAP\nUtilizzando htop, un visualizzatore di processi interattivo multipiattaforma, si possono facilmente monitorare, in tempo reale, le risorse in esecuzione sul Raspi, come l’elenco dei processi, le CPU in esecuzione e la memoria utilizzata. Per lanciare hop, si immette il vomando sul terminale:\nhtop\n\nPer quanto riguarda la memoria, tra i dispositivi della famiglia Raspberry Pi, il Raspi-Zero ha la quantità più piccola di SRAM (500 MB), rispetto a una selezione da 2 GB a 8 GB sui Raspi 4 o 5. Per qualsiasi Raspi, è possibile aumentare la memoria disponibile per il sistema con “Swap”. La memoria di swap, nota anche come spazio di swap, è una tecnica utilizzata nei sistemi operativi dei computer per archiviare temporaneamente i dati dalla RAM (Random Access Memory) sulla scheda SD quando la RAM fisica è completamente utilizzata. Ciò consente al sistema operativo (SO) di continuare a funzionare anche quando la RAM è piena, il che può prevenire crash o rallentamenti del sistema.\nLa memoria di swap avvantaggia i dispositivi con RAM limitata, come il Raspi-Zero. Aumentare lo swap può aiutare a eseguire applicazioni o processi più impegnativi, ma è essenziale bilanciare questo con il potenziale impatto sulle prestazioni dell’accesso frequente al disco.\nPer default, la memoria SWAP (Swp) di Rapi-Zero è di soli 100 MB, il che è molto poco per l’esecuzione di alcune applicazioni di apprendimento automatico più complesse ed esigenti (ad esempio, YOLO). Aumentiamola a 2 MB:\nPer prima cosa, si disattiva lo swap-file:\nsudo dphys-swapfile swapoff\nPoi, si deve aprire e modificare il file /etc/dphys-swapfile. Per farlo, useremo nano:\nsudo nano /etc/dphys-swapfile\nCercare la variabile CONF_SWAPSIZE (il valore predefinito è 200) e aggiornarla a 2000:\nCONF_SWAPSIZE=2000\nE salvare il file.\nQuindi, riattivare lo swapfile e riavviare Raspi-zero:\nsudo dphys-swapfile setup\nsudo dphys-swapfile swapon\nsudo reboot\nQuando il dispositivo viene riavviato (so deve entrare di nuovo con SSH), ci si accorgerà che il valore massimo della memoria di swap mostrato in alto è ora qualcosa come 2 GB (nel nostro caso, 1.95 GB).\n\nPer mantenere in esecuzione htop, si deve aprire un’altra finestra del terminale per interagire continuamente con il Raspi.",
    "crumbs": [
      "Raspberry Pi",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/raspi/setup/setup.it.html#installazione-di-una-fotocamera",
    "href": "contents/labs/raspi/setup/setup.it.html#installazione-di-una-fotocamera",
    "title": "Setup",
    "section": "Installazione di una Fotocamera",
    "text": "Installazione di una Fotocamera\nIl Raspi è un dispositivo eccellente per applicazioni di visione artificiale; è necessaria una fotocamera. Possiamo installare una webcam USB standard sulla porta micro-USB utilizzando un adattatore USB OTG (Raspi-Zero e Raspi-5) oppure un modulo telecamera collegato alla porta Raspi CSI (Camera Serial Interface).\n\nLe webcam USB hanno generalmente una qualità inferiore rispetto ai moduli fotocamera che si collegano alla porta CSI. Inoltre, non possono essere controllate utilizzando i comandi raspistill e raspivid nel terminale o il pacchetto di registrazione picamera in Python. Tuttavia, potrebbero esserci dei motivi per collegare una telecamera USB al Raspberry Pi, ad esempio perché è molto più facile configurare più telecamere con un singolo Raspberry Pi, cavi lunghi o semplicemente perché si ha una telecamera del genere a portata di mano.\n\n\nInstallazione di una Webcam USB\n\nSpegnere il Raspi:\n\nsudo shutdown -h no\n\nCollegare la webcam USB (modulo telecamera USB 30fps,1280x720) al Raspi (in questo esempio, si usa il Raspi-Zero, ma le istruzioni funzionano per tutti i Raspi).\n\n\n\nRiaccendere ed eseguire SSH\nPer verificare che la fotocamera USB è riconosciuta, eseguire:\n\nlsusb\nSi dovrebbe vedere la fotocamera elencata nell’output.\n\n\nPer scattare una foto di prova con la fotocamera USB, si usa:\n\nfswebcam test_image.jpg\nQuesto salverà un’immagine denominata “test_image.jpg” nella directory corrente.\n\n\nDato che stiamo usando SSH per connetterci al Rapsi, dobbiamo trasferire l’immagine al computer principale in modo da poterla visualizzare. Possiamo usare FileZilla o SCP per questo:\n\nSi apre un terminale sul computer host e si esegue:\nscp mjrovai@raspi-zero.local:~/test_image.jpg .\n\nSostituire “mjrovai” col nome utente e “raspi-zero” con il nome host di Pi.\n\n\n\nSe la qualità dell’immagine non è soddisfacente, si possono regolare varie impostazioni; ad esempio, definire una risoluzione adatta a YOLO (640x640):\n\nfswebcam -r 640x640 --no-banner test_image_yolo.jpg\nQuesto cattura un’immagine ad alta risoluzione senza il banner di default.\n\nSi può anche usare una normale webcam USB:\n\nVerificandola con lsusb\n\n\nStreaming Video\nPer lo streaming video (che richiede più risorse), possiamo installare e usare mjpg-streamer:\nPer prima cosa, si installa Git:\nsudo apt install git\nOra, dovremmo installare le dipendenze necessarie per mjpg-streamer, clonare il repository e procedere con l’installazione:\nsudo apt install cmake libjpeg62-turbo-dev\ngit clone https://github.com/jacksonliam/mjpg-streamer.git\ncd mjpg-streamer/mjpg-streamer-experimental\nmake\nsudo make install\nQuindi avviare lo streaming con:\nmjpg_streamer -i \"input_uvc.so\" -o \"output_http.so -w ./www\"\nPossiamo poi accedere allo streaming aprendo un browser Web e andando su:\nhttp://&lt;your_pi_ip_address&gt;:8080. In questo caso: http://192.168.4.210:8080\nDovremmo vedere una pagina web con opzioni per visualizzare lo stream. Cliccare suk che dice “Stream” o provare ad accedere a:\nhttp://&lt;raspberry_pi_ip_address&gt;:8080/?action=stream\n\n\n\n\nInstallazione di un Modulo Fotocamera sulla porta CSI\nOra ci sono diversi moduli fotocamera Raspberry Pi. Il modello originale da 5 megapixel è stato rilasciato nel 2013, seguito da un 8-megapixel Camera Module 2 che è stato rilasciato nel 2016. L’ultimo modello di fotocamera è il 12-megapixel Camera Module 3, rilasciato nel 2023.\nLa fotocamera originale da 5 MP (Arducam OV5647) non è più disponibile da Raspberry Pi, ma può essere trovata da diversi fornitori alternativi. Di seguito è riportato un esempio di tale fotocamera su un Raspi-Zero.\n\nEcco un altro esempio di un modulo fotocamera v2, dotato di un sensore Sony IMX219 da 8 megapixel:\n\nQualsiasi modulo fotocamera funzionerà sul Raspberry Pi, ma per questo è necessario aggiornare il file configuration.txt:\nsudo nano /boot/firmware/config.txt\nIn fondo al file, ad esempio, per usare la fotocamera Arducam OV5647 da 5 MP, si deve aggiungere la riga:\ndtoverlay=ov5647,cam0\nOppure per il modulo v2, che ha la fotocamera Sony IMX219 da 8 MP:\ndtoverlay=imx219,cam0\nSalvare il file (CTRL+O [INVIO] CRTL+X) e riavviare il Raspi:\nSudo reboot\nDopo l’avvio, si può vedere se la fotocamera è elencata:\nlibcamera-hello --list-cameras\n\n\n\nlibcamera è una libreria software open source che supporta i sistemi di telecamere direttamente dal sistema operativo Linux sui processori Arm. Riduce al minimo il codice proprietario in esecuzione sulla GPU Broadcom.\n\nCatturiamo un’immagine jpeg con una risoluzione di 640 x 480 per il test e salviamola in un file denominato test_cli_camera.jpg\nrpicam-jpeg --output test_cli_camera.jpg --width 640 --height 480\nse vogliamo vedere il file salvato, dovremmo usare ls -f, che elenca tutto il contenuto della directory corrente in formato lungo. Come prima, possiamo usare scp per visualizzare l’immagine:",
    "crumbs": [
      "Raspberry Pi",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/raspi/setup/setup.it.html#esecuzione-del-desktop-raspi-da-remoto",
    "href": "contents/labs/raspi/setup/setup.it.html#esecuzione-del-desktop-raspi-da-remoto",
    "title": "Setup",
    "section": "Esecuzione del desktop Raspi da remoto",
    "text": "Esecuzione del desktop Raspi da remoto\nSebbene abbiamo interagito principalmente con il Raspberry Pi utilizzando comandi del terminale tramite SSH, possiamo accedere all’intero ambiente desktop grafico da remoto se abbiamo installato il sistema operativo Raspberry Pi completo (ad esempio, Raspberry Pi OS (64-bit). Questo può essere particolarmente utile per le attività che traggono vantaggio da un’interfaccia visuale. Per abilitare questa funzionalità, dobbiamo configurare un server VNC (Virtual Network Computing) sul Raspberry Pi. Ecco come fare:\n\nAbilitare il server VNC:\n\nConnettersi al Raspberry Pi tramite SSH.\nEsegui lo strumento di configurazione Raspberry Pi immettendo: bash  sudo raspi-config\nAndare su Interface Options utilizzando i tasti freccia.\n\n\n\nSelezionare VNC e Yes per abilitare server VNC.\n\n\n\nUscire dallo strumento di configurazione, salvando le modifiche quando richiesto.\n\n\nInstallare un VNC Viewer sul computer:\n\nScaricare e installare un’applicazione di visualizzazione VNC sul computer principale. Le opzioni più diffuse includono RealVNC Viewer, TightVNC o VNC Viewer di RealVNC. Installeremo VNC Viewer di RealVNC.\n\nUna volta installato, si deve confermare l’indirizzo IP Raspi. Ad esempio, sul terminale, si può usare:\nhostname -I\n\nConnettersi al Raspberry Pi:\n\nAprire l’applicazione di visualizzazione VNC.\n\n\n\nInserire l’indirizzo IP e il nome host del Raspberry Pi.\nQuando richiesto, inserire il nome utente e la password del Raspberry Pi.\n\n\nIl Raspberry Pi 5 Desktop dovrebbe apparire sul monitor del computer.\n\nRegolare le Impostazioni dello Schermo (se necessario):\n\nUna volta connessi, regolare la risoluzione dello schermo per una visualizzazione ottimale. Questo può essere fatto tramite le impostazioni desktop del Raspberry Pi o modificando il file config.txt.\nFacciamolo tramite le impostazioni desktop. Raggiungere il menù (l’icona Raspberry nell’angolo in alto a sinistra) e selezionare la migliore definizione dello schermo per il proprio monitor:",
    "crumbs": [
      "Raspberry Pi",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/raspi/setup/setup.it.html#aggiornamento-e-installazione-del-software",
    "href": "contents/labs/raspi/setup/setup.it.html#aggiornamento-e-installazione-del-software",
    "title": "Setup",
    "section": "Aggiornamento e Installazione del Software",
    "text": "Aggiornamento e Installazione del Software\n\nAggiorna il sistema:\nsudo apt update && sudo apt upgrade -y\nInstallare il software essenziale:\nsudo apt install python3-pip -y\nAbilita pip per i progetti Python:\nsudo rm /usr/lib/python3.11/EXTERNALLY-MANAGED",
    "crumbs": [
      "Raspberry Pi",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/raspi/setup/setup.it.html#considerazioni-specifiche-del-modello",
    "href": "contents/labs/raspi/setup/setup.it.html#considerazioni-specifiche-del-modello",
    "title": "Setup",
    "section": "Considerazioni Specifiche del Modello",
    "text": "Considerazioni Specifiche del Modello\n\nRaspberry Pi Zero (Raspi-Zero)\n\nPotenza di elaborazione limitata, ideale per progetti leggeri\nPer risparmiare risorse è meglio utilizzare una configurazione headless (SSH).\nValutare l’aumento dello spazio di swap per attività che richiedono molta memoria.\nPuò essere utilizzato per Image Classification e Object Detection Labs ma non per LLM (SLM).\n\n\n\nRaspberry Pi 4 o 5 (Raspi-4 o Raspi-5)\n\nAdatto per progetti più impegnativi, tra cui IA e apprendimento automatico.\nPuò eseguire l’intero ambiente desktop senza problemi.\nRaspi-4 può essere utilizzato per Image Classification e Object Detection Labs ma non funzionerà bene con LLM (SLM).\nPer Raspi-5, prendere in considerazione l’utilizzo di un dissipatore attivo per la gestione della temperatura durante attività intensive, come nel laboratorio LLM (SLM).\n\nRicordarsi di adattare i requisiti del progetto in base allo specifico modello di Raspberry Pi in uso. Raspi-Zero è ottimo per progetti a basso consumo e con vincoli di spazio, mentre i modelli Raspi-4 o 5 sono più adatti per attività più intensive dal punto di vista computazionale.",
    "crumbs": [
      "Raspberry Pi",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/raspi/image_classification/image_classification.it.html",
    "href": "contents/labs/raspi/image_classification/image_classification.it.html",
    "title": "Classificazione delle Immagini",
    "section": "",
    "text": "Panoramica\nLa classificazione delle immagini è un’attività fondamentale nella visione artificiale che comporta la categorizzazione di un’immagine in una delle diverse classi predefinite. È una pietra angolare dell’intelligenza artificiale, che consente alle macchine di interpretare e comprendere le informazioni visive in un modo che imita la percezione umana.\nLa classificazione delle immagini si riferisce all’assegnazione di un’etichetta o di una categoria a un’intera immagine in base al suo contenuto visivo. Questa attività è fondamentale nella visione artificiale e ha numerose applicazioni in vari settori. L’importanza della classificazione delle immagini risiede nella sua capacità di automatizzare le attività di comprensione visiva che altrimenti richiederebbero l’intervento umano.",
    "crumbs": [
      "Raspberry Pi",
      "Classificazione delle Immagini"
    ]
  },
  {
    "objectID": "contents/labs/raspi/image_classification/image_classification.it.html#panoramica",
    "href": "contents/labs/raspi/image_classification/image_classification.it.html#panoramica",
    "title": "Classificazione delle Immagini",
    "section": "",
    "text": "Applicazioni in Scenari del Mondo Reale\nLa classificazione delle immagini ha trovato la sua strada in numerose applicazioni del mondo reale, rivoluzionando vari settori:\n\nSanità: Assistenza nell’analisi delle immagini mediche, come l’identificazione di anomalie nelle radiografie o nelle risonanze magnetiche.\nAgricoltura: Monitoraggio della salute delle colture e rilevamento delle malattie delle piante tramite immagini aeree.\nAutomotive: Abilitazione di sistemi avanzati di assistenza alla guida e veicoli autonomi per riconoscere segnali stradali, pedoni e altri veicoli.\nVendita al Dettaglio: Potenziamento delle capacità di ricerca visiva e sistemi di gestione automatizzata dell’inventario.\nSicurezza e Sorveglianza: Potenziamento dei sistemi di rilevamento delle minacce e riconoscimento facciale.\nMonitoraggio Ambientale: Analisi delle immagini satellitari per studi su deforestazione, pianificazione urbana e cambiamenti climatici.\n\n\n\nVantaggi dell’Esecuzione della Classificazione su Dispositivi Edge come Raspberry Pi\nL’implementazione della classificazione delle immagini su dispositivi edge come Raspberry Pi offre diversi vantaggi interessanti:\n\nBassa latenza: L’elaborazione delle immagini in locale elimina la necessità di inviare dati ai server cloud, riducendo significativamente i tempi di risposta.\nFunzionalità Offline: La classificazione può essere eseguita senza una connessione Internet, rendendola adatta ad ambienti remoti o con problemi di connettività.\nPrivacy e Sicurezza: I dati sensibili delle immagini rimangono sul dispositivo locale, affrontando i problemi di privacy dei dati e i requisiti di conformità.\nEfficacia in Termini di Costi: Elimina la necessità di costose risorse di cloud computing, in particolare per attività di classificazione continue o ad alto volume.\nScalabilità: Consente architetture di elaborazione distribuite in cui più dispositivi possono funzionare in modo indipendente o in una rete.\nEfficienza Energetica: I modelli ottimizzati su hardware dedicato possono essere più efficienti dal punto di vista energetico rispetto alle soluzioni basate su cloud, il che è fondamentale per applicazioni alimentate a batteria o remote.\nPersonalizzazione: L’implementazione di modelli specializzati o aggiornati di frequente, su misura per casi d’uso specifici, è più gestibile.\n\nPossiamo creare soluzioni di visione artificiale più reattive, sicure ed efficienti sfruttando la potenza di dispositivi edge come Raspberry Pi per la classificazione delle immagini. Questo approccio apre nuove possibilità per integrare l’elaborazione visiva intelligente in varie applicazioni e ambienti.\nNelle sezioni seguenti, esploreremo come implementare e ottimizzare la classificazione delle immagini su Raspberry Pi, sfruttando questi vantaggi per creare sistemi di visione artificiale potenti ed efficienti.",
    "crumbs": [
      "Raspberry Pi",
      "Classificazione delle Immagini"
    ]
  },
  {
    "objectID": "contents/labs/raspi/image_classification/image_classification.it.html#impostazione-dellambiente",
    "href": "contents/labs/raspi/image_classification/image_classification.it.html#impostazione-dellambiente",
    "title": "Classificazione delle Immagini",
    "section": "Impostazione dell’Ambiente",
    "text": "Impostazione dell’Ambiente\n\nAggiornamento di Raspberry Pi\nInnanzitutto, assicurarsi che il Raspberry Pi sia aggiornato:\nsudo apt update\nsudo apt upgrade -y\n\n\nInstallazione delle Librerie Richieste\nInstallare le librerie necessarie per l’elaborazione delle immagini e l’apprendimento automatico:\nsudo apt install python3-pip\nsudo rm /usr/lib/python3.11/EXTERNALLY-MANAGED\npip3 install --upgrade pip\n\n\nImpostazione di un Ambiente Virtuale (Facoltativo ma Consigliato)\nCreare un ambiente virtuale per gestire le dipendenze:\npython3 -m venv ~/tflite\nsource ~/tflite/bin/activate\n\n\nInstallazione di TensorFlow Lite\nSiamo interessati a eseguire inferenza, ovvero l’esecuzione di un modello TensorFlow Lite su un dispositivo per effettuare previsioni basate sui dati di input. Per eseguire un’inferenza con un modello TensorFlow Lite, dobbiamo eseguirla tramite un interprete. L’interprete TensorFlow Lite è progettato per essere snello e veloce. L’interprete utilizza un ordinamento grafico statico e un allocatore di memoria personalizzato (meno dinamico) per garantire un carico, inizializzazione ed esecuzione latenza minimi.\nUtilizzeremo il runtime TensorFlow Lite per Raspberry Pi, una libreria semplificata per l’esecuzione di modelli di apprendimento automatico su dispositivi mobili e embedded, senza includere tutti i pacchetti TensorFlow.\npip install tflite_runtime --no-deps\n\nLa wheel installata: tflite_runtime-2.14.0-cp311-cp311-manylinux_2_34_aarch64.whl\n\n\n\nInstallazione di Librerie Python Aggiuntive\nInstallare le librerie Python richieste per l’uso con Image Classification:\nSe è installata un’altra versione di Numpy, disinstallarla prima.\npip3 uninstall numpy\nInstallare la versione 1.23.2, che è compatibile con tflite_runtime.\npip3 install numpy==1.23.2\npip3 install Pillow matplotlib\n\n\nCreazione di una directory di lavoro:\nSe si lavora su Raspi-Zero con il sistema operativo minimo (No Desktop), non si ha un albero di directory user-pre-defined (lo si può verificare con ls. Quindi, creiamone uno:\nmkdir Documents\ncd Documents/\nmkdir TFLITE\ncd TFLITE/\nmkdir IMG_CLASS\ncd IMG_CLASS\nmkdir models\ncd models\n\nSu Raspi-5, /Documents dovrebbe esserci.\n\nOttenere un Modello di Classificazione delle Immagini Pre-addestrato:\nUn modello pre-addestrato appropriato è fondamentale per una classificazione delle immagini di successo su dispositivi con risorse limitate come Raspberry Pi. MobileNet è progettato per applicazioni di visione mobile e embedded con un buon equilibrio tra accuratezza e velocità. Versioni: MobileNetV1, MobileNetV2, MobileNetV3. Scarichiamo la V2:\nwget https://storage.googleapis.com/download.tensorflow.org/models/\ntflite_11_05_08/mobilenet_v2_1.0_224_quant.tgz\n\ntar xzf mobilenet_v2_1.0_224_quant.tgz\nPrelevarne le etichette:\nwget https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/IMG_CLASS/models/labels.txt\nAlla fine, si dovrebbero avere i modelli nella sua directory:\n\n\nCi serviranno solo il modello mobilenet_v2_1.0_224_quant.tflite e labels.txt. Si possono eliminare gli altri file.\n\n\n\nImpostazione di Jupyter Notebook (Facoltativo)\nSe si preferisce usare Jupyter Notebook per lo sviluppo:\npip3 install jupyter\njupyter notebook --generate-config\nPer eseguire Jupyter Notebook, si lancia il comando (cambiare l’indirizzo IP per il proprio):\njupyter notebook --ip=192.168.4.210 --no-browser\nSul terminale, si può vedere l’indirizzo URL locale per aprire il notebook:\n\nVi si può accedere da un altro dispositivo inserendo l’indirizzo IP del Raspberry Pi e il token fornito in un browser Web (il token lo si può copiare dal terminale).\n\nDefinire la directory di lavoro nel Raspi e creare un nuovo notebook Python 3.\n\n\nVerifica della Configurazione\nTestare la configurazione eseguendo un semplice script Python:\nimport tflite_runtime.interpreter as tflite\nimport numpy as np\nfrom PIL import Image\n\nprint(\"NumPy:\", np.__version__)\nprint(\"Pillow:\", Image.__version__)\n\n# Try to create a TFLite Interpreter\nmodel_path = \"./models/mobilenet_v2_1.0_224_quant.tflite\"\ninterpreter = tflite.Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\nprint(\"TFLite Interpreter created successfully!\")\nSi può creare lo script Python usando nano sul terminale, salvandolo con CTRL+0 + ENTER + CTRL+X\n\nEd eseguirlo col comando:\n\nOppure lo si può lanciare direttamente sul Notebook:",
    "crumbs": [
      "Raspberry Pi",
      "Classificazione delle Immagini"
    ]
  },
  {
    "objectID": "contents/labs/raspi/image_classification/image_classification.it.html#fare-inferenze-con-mobilenet-v2",
    "href": "contents/labs/raspi/image_classification/image_classification.it.html#fare-inferenze-con-mobilenet-v2",
    "title": "Classificazione delle Immagini",
    "section": "Fare inferenze con Mobilenet V2",
    "text": "Fare inferenze con Mobilenet V2\nNell’ultima sezione, abbiamo impostato l’ambiente, incluso il download di un modello pre-addestrato popolare, Mobilenet V2, addestrato sulle immagini 224x224 di ImageNet (1,2 milioni) per 1.001 classi (1.000 categorie di oggetti più 1 sfondo). Il modello è stato convertito in un formato TensorFlow Lite compatto da 3,5 MB, rendendolo adatto allo spazio di archiviazione e alla memoria limitati di un Raspberry Pi.\n\nApriamo un nuovo notebook per seguire tutti i passaggi per classificare un’immagine:\nImportare le librerie necessarie:\nimport time\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport tflite_runtime.interpreter as tflite\nCaricare il modello TFLite e allocare i tensori:\nmodel_path = \"./models/mobilenet_v2_1.0_224_quant.tflite\"\ninterpreter = tflite.Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\nOttenere i tensori di input e output.\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\nInput details ci daranno informazioni su come il modello dovrebbe essere alimentato con un’immagine. Il profilo di (1, 224, 224, 3) ci informa che un’immagine con dimensioni (224x224x3) dovrebbe essere inserita una alla volta (Batch Dimension: 1).\n\nGli output details mostrano che l’inferenza risulterà in un array di 1.001 valori interi. Tali valori derivano dalla classificazione dell’immagine, dove ogni valore è la probabilità che quella specifica etichetta sia correlata all’immagine.\n\nEsaminiamo anche il dtype dei dettagli di input del modello\ninput_dtype = input_details[0]['dtype']\ninput_dtype\ndtype('uint8')\nQuesto mostra che l’immagine di input dovrebbe essere composta da pixel grezzi (0 - 255).\nPrendiamo un’immagine di prova. La si può trasferire dal computer o scaricarne una per testarla. Per prima cosa creiamo una cartella nella nostra directory di lavoro:\nmkdir images\ncd images\nwget https://upload.wikimedia.org/wikipedia/commons/3/3a/Cat03.jpg\nCarichiamo e visualizziamo l’immagine:\n# Load he image\nimg_path = \"./images/Cat03.jpg\"\nimg = Image.open(img_path)\n\n# Display the image\nplt.figure(figsize=(8, 8))\nplt.imshow(img)\nplt.title(\"Original Image\")\nplt.show()\n\nPossiamo vedere la dimensione dell’immagine eseguendo il comando:\nwidth, height = img.size\nQuesto ci mostra che l’immagine è RGB con una larghezza di 1600 e un’altezza di 1600 pixel. Quindi, per usare il nostro modello, dovremmo rimodellarlo in (224, 224, 3) e aggiungere una dimensione batch di 1, come definito nei dettagli di input: (1, 224, 224, 3). Il risultato dell’inferenza, come mostrato nei dettagli di output, sarà un array con una dimensione di 1001, come mostrato di seguito:\n\nQuindi, rimodelliamo l’immagine, aggiungiamo la dimensione batch e vediamo il risultato:\nimg = img.resize((input_details[0]['shape'][1], input_details[0]['shape'][2]))\ninput_data = np.expand_dims(img, axis=0)\ninput_data.shape\nLa forma di input_data è come previsto: (1, 224, 224, 3)\nConfermiamo il dtype dei dati di input:\ninput_data.dtype\ndtype('uint8')\nIl dtype dei dati di input è ‘uint8’, che è compatibile con il dtype previsto per il modello.\nUtilizzando input_data, eseguiamo l’interprete e otteniamo le previsioni (output):\ninterpreter.set_tensor(input_details[0]['index'], input_data)\ninterpreter.invoke()\npredictions = interpreter.get_tensor(output_details[0]['index'])[0]\nLa previsione è un array con 1001 elementi. Otteniamo i primi 5 indici in cui i loro elementi hanno valori elevati:\ntop_k_results = 5\ntop_k_indices = np.argsort(predictions)[::-1][:top_k_results]\ntop_k_indices\ntop_k_indices è un array con 5 elementi: array([283, 286, 282])\nQuindi, 283, 286, 282, 288 e 479 sono le classi più probabili dell’immagine. Avendo l’indice, dobbiamo trovare a quale classe è assegnato (ad esempio auto, gatto o cane). Il file di testo scaricato con il modello ha un’etichetta associata a ciascun indice da 0 a 1.000. Usiamo una funzione per caricare il file .txt come un elenco:\ndef load_labels(filename):\n    with open(filename, 'r') as f:\n        return [line.strip() for line in f.readlines()]\nE otteniamo l’elenco, stampando le etichette associate agli indici:\nlabels_path = \"./models/labels.txt\"\nlabels = load_labels(labels_path)\n\nprint(labels[286])\nprint(labels[283])\nprint(labels[282])\nprint(labels[288])\nprint(labels[479])\nDi conseguenza abbiamo:\nEgyptian cat\ntiger cat\ntabby\nlynx\ncarton\nAlmeno i quattro indici principali sono correlati ai felini. Il contenuto di prediction è la probabilità associata a ciascuna delle etichette. Come abbiamo visto nei dettagli dell’output, quei valori sono quantizzati e dovrebbero essere dequantizzati e applicare la softmax.\nscale, zero_point = output_details[0]['quantization']\ndequantized_output = (predictions.astype(np.float32) - zero_point) * scale\nexp_output = np.exp(dequantized_output - np.max(dequantized_output))\nprobabilities = exp_output / np.sum(exp_output)\nStampiamo le prime 5 probabilità:\nprint (probabilities[286])\nprint (probabilities[283])\nprint (probabilities[282])\nprint (probabilities[288])\nprint (probabilities[479])\n0.27741462\n0.3732285\n0.16919471\n0.10319158\n0.023410844\nPer chiarezza, creiamo una funzione per mettere in relazione le etichette con le probabilità:\nfor i in range(top_k_results):\n    print(\"\\t{:20}: {}%\".format(\n        labels[top_k_indices[i]],\n        (int(probabilities[top_k_indices[i]]*100))))\ntiger cat           : 37%\nEgyptian cat        : 27%\ntabby               : 16%\nlynx                : 10%\ncarton              : 2%\n\nDefinire una funzione generale di Classificazione delle Immagini\nCreiamo una funzione generale per dare un’immagine come input e otteniamo le prime 5 classi possibili:\n\ndef image_classification(img_path, model_path, labels, top_k_results=5):\n    # load the image\n    img = Image.open(img_path)\n    plt.figure(figsize=(4, 4))\n    plt.imshow(img)\n    plt.axis('off')\n\n    # Load the TFLite model\n    interpreter = tflite.Interpreter(model_path=model_path)\n    interpreter.allocate_tensors()\n    \n    # Get input and output tensors\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n    \n    # Preprocess\n    img = img.resize((input_details[0]['shape'][1], \n                      input_details[0]['shape'][2]))\n    input_data = np.expand_dims(img, axis=0)\n    \n    # Inference on Raspi-Zero\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n    \n    # Obtain results and map them to the classes\n    predictions = interpreter.get_tensor(output_details[0]['index'])[0]\n    \n    # Get indices of the top k results\n    top_k_indices = np.argsort(predictions)[::-1][:top_k_results]\n    \n    # Get quantization parameters\n    scale, zero_point = output_details[0]['quantization']\n    \n    # Dequantize the output and apply softmax\n    dequantized_output = (predictions.astype(np.float32) - zero_point) * scale\n    exp_output = np.exp(dequantized_output - np.max(dequantized_output))\n    probabilities = exp_output / np.sum(exp_output)\n    \n    print(\"\\n\\t[PREDICTION]        [Prob]\\n\")\n    for i in range(top_k_results):\n        print(\"\\t{:20}: {}%\".format(\n            labels[top_k_indices[i]],\n            (int(probabilities[top_k_indices[i]]*100))))\n\nE caricando alcune immagini per i test, abbiamo:\n\n\n\nTest con un modello addestrato da zero\nAddestriamo un modello TFLite da zero. Per questo, si può seguire il Notebook:\nCNN to classify Cifar-10 dataset\nNel notebook, abbiamo addestrato un modello utilizzando il dataset CIFAR10, che contiene 60.000 immagini da 10 classi di CIFAR (airplane, automobile, bird, cat, deer, dog, frog, horse, ship, and truck). CIFAR ha immagini a colori 32x32 (3 canali colore) in cui gli oggetti non sono centrati e possono avere l’oggetto con uno sfondo, come gli aerei che potrebbero avere un cielo nuvoloso dietro di loro! In breve, immagini piccole ma reali.\nIl modello addestrato dalla CNN (cifar10_model.keras) aveva una dimensione di 2,0 MB. Utilizzando TFLite Converter, il modello cifar10.tflite è diventato di 674 MB (circa 1/3 della dimensione originale).\n\nSul notebook Cifar 10 - Image Classification on a Raspi with TFLite (che può essere eseguito sul Raspi), possiamo seguire gli stessi passaggi che abbiamo fatto con mobilenet_v2_1.0_224_quant.tflite. Di seguito sono riportati esempi di immagini che utilizzano la General Function for Image Classification su un Raspi-Zero, come mostrato nell’ultima sezione.\n\n\n\nInstalling Picamera2\nPicamera2, una libreria Python per interagire con la fotocamera del Raspberry Pi, è basata sullo stack della fotocamera libcamera e la Raspberry Pi Foundation la mantiene. La libreria Picamera2 è supportata su tutti i modelli Raspberry Pi, dal Pi Zero al RPi 5. È già installata in tutto il sistema Raspi, ma dovremmo renderla accessibile all’interno dell’ambiente virtuale.\n\nPer prima cosa, attivare l’ambiente virtuale se non è già attivato:\nsource ~/tflite/bin/activate\nOra, creiamo un file .pth nelll’ambiente virtuale per aggiungere il percorso del sistema del pacchetto sul sito:\necho \"/usr/lib/python3/dist-packages\" &gt; $VIRTUAL_ENV/lib/python3.11/\nsite-packages/system_site_packages.pth\n\nNota: Se la versione di Python è diversa, sostituire python3.11 con la versione appropriata.\n\nDopo aver creato questo file, provare a importare picamera2 in Python:\npython3\n&gt;&gt;&gt; import picamera2\n&gt;&gt;&gt; print(picamera2.__file__)\n\nIl codice sopra mostrerà la posizione del file del modulo picamera2 stesso, dimostrando che la libreria è accessibile dall’ambiente.\n/home/mjrovai/tflite/lib/python3.11/site-packages/picamera2/__init__.py\nÈ anche possibile elencare le telecamere disponibili nel sistema:\n&gt;&gt;&gt; print(Picamera2.global_camera_info())\nNel nostro caso, con una USB installata, si è ottenuto:\n\nOra che abbiamo confermato che picamera2 funziona nell’ambiente con un indice 0, proviamo un semplice script Python per catturare un’immagine dalla fotocamera USB:\nfrom picamera2 import Picamera2\nimport time\n\n# Initialize the camera\npicam2 = Picamera2() # default is index 0\n\n# Configure the camera\nconfig = picam2.create_still_configuration(main={\"size\": (640, 480)})\npicam2.configure(config)\n\n# Start the camera\npicam2.start()\n\n# Wait for the camera to warm up\ntime.sleep(2)\n\n# Capture an image\npicam2.capture_file(\"usb_camera_image.jpg\")\nprint(\"Image captured and saved as 'usb_camera_image.jpg'\")\n\n# Stop the camera\npicam2.stop()\nUtilizzare l’editor di testo Nano, Jupyter Notebook o qualsiasi altro editor. Salvarlo come script Python (ad esempio, capture_image.py) ed eseguilo. Questo dovrebbe catturare un’immagine dalla fotocamera e salvarla come “usb_camera_image.jpg” nella stessa directory dello script.\n\nSe Jupyter è aperto, si può vedere l’immagine catturata sul computer. Altrimenti, si trasferisce il file dal Raspi al computer.\n\n\nSe si sta lavorando con un Raspi-5 con un intero desktop, si può aprire il file direttamente sul dispositivo.",
    "crumbs": [
      "Raspberry Pi",
      "Classificazione delle Immagini"
    ]
  },
  {
    "objectID": "contents/labs/raspi/image_classification/image_classification.it.html#progetto-di-classificazione-delle-immagini",
    "href": "contents/labs/raspi/image_classification/image_classification.it.html#progetto-di-classificazione-delle-immagini",
    "title": "Classificazione delle Immagini",
    "section": "Progetto di Classificazione delle Immagini",
    "text": "Progetto di Classificazione delle Immagini\nOra, svilupperemo un progetto completo di Classificazione delle Immagini utilizzando Edge Impulse Studio. Come abbiamo fatto con Movilinet V2, il modello TFLite addestrato e convertito verrà utilizzato per l’inferenza.\n\nL’Obiettivo\nIl primo passo in qualsiasi progetto ML è definire il suo obiettivo. In questo caso, è rilevare e classificare due oggetti specifici presenti in un’immagine. Per questo progetto, utilizzeremo due piccoli giocattoli: un robot e un piccolo pappagallo brasiliano (chiamato Periquito). Raccoglieremo anche immagini di un background in cui questi due oggetti sono assenti.\n\n\n\nRaccolta Dati\nUna volta definito l’obiettivo del nostro progetto di apprendimento automatico, il passaggio successivo, e più cruciale, è la raccolta del dataset. Possiamo utilizzare un telefono per l’acquisizione delle immagini, ma qui utilizzeremo il Raspi. Impostiamo un semplice server Web sul nostro Raspberry Pi per visualizzare le immagini QVGA (320 x 240) acquisite in un browser.\n\nPer prima cosa, installiamo Flask, un framework Web leggero per Python:\npip3 install flask\nCreiamo un nuovo script Python che combina l’acquisizione delle immagini con un server Web. Lo chiameremo get_img_data.py:\n\n\nfrom flask import Flask, Response, render_template_string, request, redirect, url_for\nfrom picamera2 import Picamera2\nimport io\nimport threading\nimport time\nimport os\nimport signal\n\napp = Flask(__name__)\n\n# Global variables\nbase_dir = \"dataset\"\npicam2 = None\nframe = None\nframe_lock = threading.Lock()\ncapture_counts = {}\ncurrent_label = None\nshutdown_event = threading.Event()\n\ndef initialize_camera():\n    global picam2\n    picam2 = Picamera2()\n    config = picam2.create_preview_configuration(main={\"size\": (320, 240)})\n    picam2.configure(config)\n    picam2.start()\n    time.sleep(2)  # Wait for camera to warm up\n\ndef get_frame():\n    global frame\n    while not shutdown_event.is_set():\n        stream = io.BytesIO()\n        picam2.capture_file(stream, format='jpeg')\n        with frame_lock:\n            frame = stream.getvalue()\n        time.sleep(0.1)  # Adjust as needed for smooth preview\n\ndef generate_frames():\n    while not shutdown_event.is_set():\n        with frame_lock:\n            if frame is not None:\n                yield (b'--frame\\r\\n'\n                       b'Content-Type: image/jpeg\\r\\n\\r\\n' + frame + b'\\r\\n')\n        time.sleep(0.1)  # Adjust as needed for smooth streaming\n\ndef shutdown_server():\n    shutdown_event.set()\n    if picam2:\n        picam2.stop()\n    # Give some time for other threads to finish\n    time.sleep(2)\n    # Send SIGINT to the main process\n    os.kill(os.getpid(), signal.SIGINT)\n\n@app.route('/', methods=['GET', 'POST'])\ndef index():\n    global current_label\n    if request.method == 'POST':\n        current_label = request.form['label']\n        if current_label not in capture_counts:\n            capture_counts[current_label] = 0\n        os.makedirs(os.path.join(base_dir, current_label), exist_ok=True)\n        return redirect(url_for('capture_page'))\n    return render_template_string('''\n        &lt;!DOCTYPE html&gt;\n        &lt;html&gt;\n        &lt;head&gt;\n            &lt;title&gt;Dataset Capture - Label Entry&lt;/title&gt;\n        &lt;/head&gt;\n        &lt;body&gt;\n            &lt;h1&gt;Enter Label for Dataset&lt;/h1&gt;\n            &lt;form method=\"post\"&gt;\n                &lt;input type=\"text\" name=\"label\" required&gt;\n                &lt;input type=\"submit\" value=\"Start Capture\"&gt;\n            &lt;/form&gt;\n        &lt;/body&gt;\n        &lt;/html&gt;\n    ''')\n\n@app.route('/capture')\ndef capture_page():\n    return render_template_string('''\n        &lt;!DOCTYPE html&gt;\n        &lt;html&gt;\n        &lt;head&gt;\n            &lt;title&gt;Dataset Capture&lt;/title&gt;\n            &lt;script&gt;\n                var shutdownInitiated = false;\n                function checkShutdown() {\n                    if (!shutdownInitiated) {\n                        fetch('/check_shutdown')\n                            .then(response =&gt; response.json())\n                            .then(data =&gt; {\n                                if (data.shutdown) {\n                                    shutdownInitiated = true;\n                                    document.getElementById('video-feed').src = '';\n                                    document.getElementById('shutdown-message')\n                                    .style.display = 'block';\n                                }\n\n                            });\n                    }\n\n                }\n\n                setInterval(checkShutdown, 1000);  // Check every second\n            &lt;/script&gt;\n        &lt;/head&gt;\n        &lt;body&gt;\n            &lt;h1&gt;Dataset Capture&lt;/h1&gt;\n            &lt;p&gt;Current Label: {{ label }}&lt;/p&gt;\n            &lt;p&gt;Images captured for this label: {{ capture_count }}&lt;/p&gt;\n            &lt;img id=\"video-feed\" src=\"{{ url_for('video_feed') }}\" width=\"640\" \n            height=\"480\" /&gt;\n            &lt;div id=\"shutdown-message\" style=\"display: none; color: red;\"&gt;\n                Capture process has been stopped. You can close this window.\n            &lt;/div&gt;\n            &lt;form action=\"/capture_image\" method=\"post\"&gt;\n                &lt;input type=\"submit\" value=\"Capture Image\"&gt;\n            &lt;/form&gt;\n            &lt;form action=\"/stop\" method=\"post\"&gt;\n                &lt;input type=\"submit\" value=\"Stop Capture\" \n                style=\"background-color: #ff6666;\"&gt;\n            &lt;/form&gt;\n            &lt;form action=\"/\" method=\"get\"&gt;\n                &lt;input type=\"submit\" value=\"Change Label\" \n                style=\"background-color: #ffff66;\"&gt;\n            &lt;/form&gt;\n        &lt;/body&gt;\n        &lt;/html&gt;\n    ''', label=current_label, capture_count=capture_counts.get(current_label, 0))\n\n@app.route('/video_feed')\ndef video_feed():\n    return Response(generate_frames(),\n                    mimetype='multipart/x-mixed-replace; boundary=frame')\n\n@app.route('/capture_image', methods=['POST'])\ndef capture_image():\n    global capture_counts\n    if current_label and not shutdown_event.is_set():\n        capture_counts[current_label] += 1\n        timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n        filename = f\"image_{timestamp}.jpg\"\n        full_path = os.path.join(base_dir, current_label, filename)\n        \n        picam2.capture_file(full_path)\n    \n    return redirect(url_for('capture_page'))\n\n@app.route('/stop', methods=['POST'])\ndef stop():\n    summary = render_template_string('''\n        &lt;!DOCTYPE html&gt;\n        &lt;html&gt;\n        &lt;head&gt;\n            &lt;title&gt;Dataset Capture - Stopped&lt;/title&gt;\n        &lt;/head&gt;\n        &lt;body&gt;\n            &lt;h1&gt;Dataset Capture Stopped&lt;/h1&gt;\n            &lt;p&gt;The capture process has been stopped. You can close this window.&lt;/p&gt;\n            &lt;p&gt;Summary of captures:&lt;/p&gt;\n            &lt;ul&gt;\n            {% for label, count in capture_counts.items() %}\n                &lt;li&gt;{{ label }}: {{ count }} images&lt;/li&gt;\n            {% endfor %}\n            &lt;/ul&gt;\n        &lt;/body&gt;\n        &lt;/html&gt;\n    ''', capture_counts=capture_counts)\n    \n    # Start a new thread to shutdown the server\n    threading.Thread(target=shutdown_server).start()\n    \n    return summary\n\n@app.route('/check_shutdown')\ndef check_shutdown():\n    return {'shutdown': shutdown_event.is_set()}\n\nif __name__ == '__main__':\n    initialize_camera()\n    threading.Thread(target=get_frame, daemon=True).start()\n    app.run(host='0.0.0.0', port=5000, threaded=True)\n\n\nEseguire questo script:\n```bash python3 get_img_data.py\n\n4. Accedere all'interfaccia web:\n\n    - Sul Raspberry Pi stesso (se si ha una GUI): Aprire un browser web e andare su `http://localhost:5000`\n    - Da un altro dispositivo sulla stessa rete: Aprire un browser web e andare su `http://&lt;raspberry_pi_ip&gt;:5000` (Sostituire `&lt;raspberry_pi_ip&gt;` con l'indirizzo IP del Raspberry Pi). Per esempio: `http://192.168.4.210:5000/`\n\nQuesto script Python crea un'interfaccia basata sul web per catturare e organizzare set di dati di immagini usando un Raspberry Pi e la sua fotocamera. È utile per progetti di apprendimento automatico che richiedono dati di immagini etichettati.\n\n#### Caratteristiche Principali:\n\n1. **Interfaccia Web**: Accessibile da qualsiasi dispositivo sulla stessa rete del Raspberry Pi.\n2. **Anteprima Telecamera in Tempo Reale**: Mostra un feed in tempo reale dalla telecamera.\n3. **Sistema di Etichettatura**: Consente agli utenti di immettere etichette per diverse categorie di immagini.\n4. **Archiviazione Organizzata**: Salva automaticamente le immagini in sottodirectory specifiche per etichetta.\n5. **Contatori per Etichetta**: Tiene traccia di quante immagini vengono acquisite per ogni etichetta.\n6. **Statistiche di Riepilogo**: Fornisce un riepilogo delle immagini acquisite quando si interrompe il processo di acquisizione.\n\n#### Componenti Principali:\n\n1. **Applicazione Web Flask**: Gestisce il routing e serve l'interfaccia Web.\n2. **Integrazione di Picamera2**: Controlla la telecamera Raspberry Pi.\n3. **Threaded Frame Capture**: Assicura un'anteprima live fluida.\n4. **File Management**: Organizza le immagini catturate in directory etichettate.\n\n#### Funzioni Chiave:\n\n- `initialize_camera()`: Imposta l'istanza Picamera2.\n- `get_frame()`: Cattura continuamente i frame per l'anteprima live.\n- `generate_frames()`: Genera i frame per il feed video live.\n- `shutdown_server()`: Imposta l'evento di arresto, arresta la telecamera e arresta il server Flask\n- `index()`: Gestisce la pagina di input dell'etichetta.\n- `capture_page()`: Visualizza l'interfaccia di acquisizione principale.\n- `video_feed()`: Mostra un'anteprima live per posizionare la telecamera\n- `capture_image()`: Salva un'immagine con l'etichetta corrente.\n- `stop()`: Arresta il processo di acquisizione e visualizza un riepilogo.\n\n#### Flusso di Utilizzo:\n\n1. Avviare lo script sul Raspberry Pi.\n2. Accedere all'interfaccia web da un browser.\n3. Inserire un'etichetta per le immagini da catturare e premere `Start Capture`.\n\n![](images/png/enter_label.png)\n\n4. Utilizzare l'anteprima live per posizionare la telecamera.\n5. Cliccare `Capture Image` per salvare le immagini sotto l'etichetta corrente.\n\n![](images/png/capture.png)\n\n6. Cambiare le etichette come necessario per le diverse categorie, selezionando `Change Label`.\n7. Cliccare `Stop Capture` al termine per vedere un riepilogo.\n\n![](images/png/stop.png)\n\n#### Note Tecniche:\n\n- Lo script usa il threading per gestire la cattura di frame e il web serving simultanei.\n- Le immagini vengono salvate con timestamp nei nomi dei file per renderle uniche.\n- L'interfaccia web è reattiva e accessibile da dispositivi mobili.\n\n#### Possibilità di Personalizzazione:\n\n- Regolare la risoluzione dell'immagine nella funzione `initialize_camera()`. Qui abbiamo utilizzato QVGA (320X240).\n- Modificare i modelli HTML per un aspetto diverso.\n- Aggiungere ulteriori passaggi di elaborazione o analisi delle immagini nella funzione `capture_image()`.\n\n#### Numero di campioni sul Dataset:\n\nSi ottengono circa 60 immagini da ciascuna categoria (`periquito`, `robot` e `background`). Provare ad acquisire con diverse angolazioni, sfondi e condizioni di luce. Sul Raspi, finiremo con una cartella denominata `dataset`, che contiene 3 sottocartelle *periquito*, *robot* e *background*. una per ogni classe di immagini.\n\nSi può usare `Filezilla` per trasferire il dataset creato sul computer principale. \n\n## Addestramento del modello con Edge Impulse\n\nUseremo Edge Impulse Studio per addestrare il modello.  Si va nella [Pagina di Edge Impulse](https://edgeimpulse.com/), si inseriscono le credenziali e si crea un nuovo progetto:\n\n![](images/png/new-proj-ei.png)\n\n&gt; Qui si può clonare un progetto simile: [Raspi - Img Class](https://studio.edgeimpulse.com/public/510251/live).\n\n### Dataset\n\nEsamineremo quattro passaggi principali usando EI Studio (o Studio). Questi passaggi sono fondamentali per preparare il nostro modello per l'uso sul Raspi: Dataset, Impulse, Test e Deploy (sul dispositivo Edge, in questo caso, il Raspi).\n\n&gt; Per quanto riguarda il Dataset, è essenziale sottolineare che il nostro Dataset originale, acquisito con il Raspi, sarà suddiviso in *Training*, *Validation* e *Test*. Il Test Set sarà separato dall'inizio e riservato per l'uso solo nella fase di Test dopo l'addestramento. Il Validation Set sarà utilizzato durante l'addestramento.\n\nSu Studio, seguire i passaggi per caricare i dati acquisiti:\n\n1. Si va nella scheda `Data acquisition` e nella sezione `UPLOAD DATA`, si caricano i file dal computer nelle categorie scelte.\n2. Lasciare a Studio la suddivisione del dataset originale in *train and test* e scegliere l'etichetta a riguardo\n3. Ripetere la procedura per tutte e tre le classi. Alla fine, si vedranno i \"raw data\" in Studio:\n\n![](images/png/data-Aquisition.png)\n\nStudio consente di esplorare i dati, mostrando una vista completa di tutti i quelli nel progetto. Si possono cancellare, ispezionare o modificare le etichette cliccando sui singoli elementi di dati. Nel nostro caso, un progetto semplice, i dati sembrano OK.\n\n![](images/png/data-esplorer.png)\n\n## Il Progetto Impulse\n\nIn questa fase, dovremmo definire come:\n\n- Pre-elaborare i nostri dati, il che consiste nel ridimensionare le singole immagini e determinare la `color depth` [profondità di colore] da utilizzare (sia RGB che in scala di grigi) e\n\n- Specificare un Modello. In questo caso, sarà `Transfer Learning (Images)` a mettere a punto un modello di classificazione delle immagini MobileNet V2 pre-addestrato sui nostri dati. Questo metodo funziona bene anche con set di dati di immagini relativamente piccoli (circa 180 immagini nel nostro caso).\n\nTransfer Learning con MobileNet offre un approccio semplificato all'addestramento del modello, che è particolarmente utile per ambienti con risorse limitate e progetti con dati etichettati limitati. MobileNet, noto per la sua architettura leggera, è un modello pre-addestrato che ha già appreso funzionalità preziose da un ampio set di dati (ImageNet).\n\n![](images/jpeg/model_1.jpg)\n\nSfruttando queste funzionalità apprese, possiamo addestrare un nuovo modello per il compito specifico con meno dati e risorse computazionali e raggiungere un'accuratezza competitiva.\n\n![](images/jpeg/model_2.jpg)\n\nQuesto approccio riduce significativamente i tempi di addestramento e i costi computazionali, rendendolo ideale per la prototipazione rapida e l'implementazione su dispositivi embedded in cui l'efficienza è fondamentale.\n\nSi va alla scheda Impulse Design e si crea l'*impulse*, definendo una dimensione dell'immagine di 160x160 e schiacciandola (forma quadrata, senza ritaglio). Si seleziona Image e i blocchi Transfer Learning. Si salva l'Impulse.\n\n![](images/png/impulse.png)\n\n### Pre-elaborazione delle immagini\n\nTutte le immagini QVGA/RGB565 in ingresso verranno convertite in 76.800 feature (160x160x3).\n\n![](images/png/preproc.png)\n\nPremere `Save parameters` e selezionare `Generate features` nella scheda successiva.\n\n### Progettazione del modello\n\nMobileNet è una famiglia di reti neurali convoluzionali efficienti progettate per applicazioni di visione mobile e embedded. Le caratteristiche principali di MobileNet sono:\n\n1. Leggero: Ottimizzato per dispositivi mobili e sistemi embedded con risorse di calcolo limitate.\n2. Velocità: Tempi di inferenza rapidi, adatti per applicazioni in tempo reale.\n3. Precisione: Mantiene una buona accuratezza nonostante le dimensioni compatte.\n\n[MobileNetV2](https://arxiv.org/abs/1801.04381), introdotto nel 2018, migliora l'architettura MobileNet originale. Le caratteristiche principali includono:\n\n1. Residui Invertiti: Le strutture residue invertite vengono utilizzate quando vengono create connessioni di scelta rapida tra layer di colli di bottiglia sottili.\n2. Colli di Bottiglia Lineari: Rimuove le non linearità nei layer stretti per impedire la distruzione delle informazioni.\n3. Convoluzioni Separabili in Profondità: Continua a utilizzare questa efficiente operazione da MobileNetV1.\n\nNel nostro progetto, faremo un `Transfer Learning` con `MobileNetV2 160x160 1.0`, il che significa che le immagini utilizzate per l'addestramento (e l'inferenza futura) dovrebbero avere una *input Size* [dimensione di input] di 160x160 pixel e un *Width Multiplier* [moltiplicatore di larghezza] di 1.0 (larghezza completa, non ridotta). Questa configurazione bilancia tra dimensione del modello, velocità e accuratezza.\n\n### Training del Modello\n\nUn'altra preziosa tecnica di apprendimento profondo è il **Data Augmentation**. Il \"data augmentation\" migliora l'accuratezza dei modelli di apprendimento automatico creando dati artificiali aggiuntivi. Un sistema di data augmentation apporta piccole modifiche casuali ai dati di training durante l'addestramento (ad esempio capovolgendo, ritagliando o ruotando le immagini).\n\nGuardando internamente, qui si può vedere come Edge Impulse implementa una policy di data Augmentation sui dati:\n\n``` python\n# Implements the data augmentation policy\ndef augment_image(image, label):\n    # Flips the image randomly\n    image = tf.image.random_flip_left_right(image)\n\n    # Increase the image size, then randomly crop it down to\n    # the original dimensions\n    resize_factor = random.uniform(1, 1.2)\n    new_height = math.floor(resize_factor * INPUT_SHAPE[0])\n    new_width = math.floor(resize_factor * INPUT_SHAPE[1])\n    image = tf.image.resize_with_crop_or_pad(image, new_height, new_width)\n    image = tf.image.random_crop(image, size=INPUT_SHAPE)\n\n    # Vary the brightness of the image\n    image = tf.image.random_brightness(image, max_delta=0.2)\n\n    return image, label\nL’esposizione a queste variazioni durante l’addestramento può aiutare a impedire al modello di prendere scorciatoie “memorizzando” indizi superficiali nei dati di addestramento, il che significa che potrebbe riflettere meglio i pattern profondi in esame nel set di dati.\nL’ultimo layer denso del nostro modello avrà 0 neuroni con un dropout del 10% per prevenire il sovradattamento. Ecco il risultato del Training:\n\nIl risultato è eccellente, con una latenza ragionevole di 35 ms (per un Raspi-4), che dovrebbe tradursi in circa 30 fps (fotogrammi al secondo) durante l’inferenza. Un Raspi-Zero dovrebbe essere più lento e il Raspi-5 più veloce.\n\n\nCompromesso: Accuratezza contro Velocità\nSe è necessaria un’inferenza più rapida, dovremmo addestrare il modello usando alfa più piccoli (0.35, 0.5 e 0.75) o persino ridurre le dimensioni dell’immagine in ingresso, a discapito dell’accuratezza. Tuttavia, ridurre le dimensioni dell’immagine in ingresso e diminuire l’alfa (moltiplicatore di larghezza) può accelerare l’inferenza per MobileNet V2, ma hanno compromessi diversi. Confrontiamoli:\n\nRiduzione delle Dimensioni dell’Immagine in Ingresso:\n\nPro:\n\nRiduce significativamente il costo computazionale su tutti i layer.\nRiduce l’utilizzo della memoria.\nSpesso fornisce un aumento sostanziale della velocità.\n\nContro:\n\nPotrebbe ridurre la capacità del modello di rilevare piccole caratteristiche o dettagli fini.\nPuò avere un impatto significativo sulla precisione, specialmente per le attività che richiedono un riconoscimento a grana fine.\n\n\nRiduzione di Alpha (Moltiplicatore di Larghezza):\n\nPro:\n\nRiduce il numero di parametri e calcoli nel modello.\nMantiene la risoluzione di input originale, preservando potenzialmente più dettagli.\nPuò fornire un buon equilibrio tra velocità e precisione.\n\nContro:\n\nPotrebbe non accelerare l’inferenza in modo così drastico come la riduzione delle dimensioni di input.\nPuò ridurre la capacità del modello di apprendere caratteristiche complesse.\n\nConfronto:\n\nImpatto sulla Velocità:\n\nLa riduzione delle dimensioni di input spesso fornisce un aumento di velocità più sostanziale perché riduce i calcoli in modo quadratico (dimezzando sia la larghezza che l’altezza si riducono i calcoli di circa il 75%).\nLa riduzione di Alpha fornisce una riduzione più lineare nei calcoli.\n\nImpatto sulla Precisione:\n\nLa riduzione delle dimensioni di input può avere un impatto significativo sulla precisione, specialmente quando si rilevano piccoli oggetti o dettagli fini.\nLa riduzione di alpha tende ad avere un impatto più graduale sulla precisione.\n\nArchitettura del Modello:\n\nLa modifica delle dimensioni di input non altera l’architettura del modello.\nLa modifica di alpha modifica la struttura del modello riducendo il numero di canali in ogni layer.\n\n\nRaccomandazione:\n\nSe l’applicazione non richiede il rilevamento di piccoli dettagli e può tollerare una certa perdita di accuratezza, ridurre le dimensioni di input è spesso il modo più efficace per accelerare l’inferenza.\nRidurre l’alfa potrebbe essere preferibile se mantenere la capacità di rilevare dettagli fini è fondamentale o se c’è bisogno di un compromesso più equilibrato tra velocità e accuratezza.\nPer ottenere risultati migliori, si devono sperimentare entrambi:\n\nProvare MobileNet V2 con dimensioni di input come 160x160 o 92x92\nSperimentare con valori alfa come 1.0, 0.75, 0.5 o 0.35.\n\nEseguire sempre il benchmark delle diverse configurazioni sull’hardware specifico e con il particolare set di dati per trovare l’equilibrio ottimale per il caso d’uso.\n\n\nRicordarsi che la scelta migliore dipende dai requisiti specifici di accuratezza, velocità e dalla natura delle immagini con cui si sta lavorando. Spesso vale la pena sperimentare combinazioni per trovare la configurazione ottimale per il particolare caso d’uso.\n\n\n\nTest del Modello\nOra, si deve prendere il set di dati all’inizio del progetto ed eseguire il modello addestrato usandolo come input. Di nuovo, il risultato è eccellente (92,22%).\n\n\nDistribuzione del modello\nCome abbiamo fatto nella sezione precedente, possiamo distribuire il modello addestrato come .tflite e usare Raspi per eseguirlo usando Python.\nNella scheda Dashboard, si va su Transfer learning model (int8 quantized) e si clicca sull’icona di download:\n\n\nScarichiamo anche la versione float32 per il confronto\n\nTrasferire il modello dal computer al Raspi (./models), ad esempio, usando FileZilla. Catturare, inoltre, alcune immagini per l’inferenza (./images).\nImportare le librerie necessarie:\nimport time\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport tflite_runtime.interpreter as tflite\nDefinire i path e le etichette:\nimg_path = \"./images/robot.jpg\"\nmodel_path = \"./models/ei-raspi-img-class-int8-quantized-model.tflite\"\nlabels = ['background', 'periquito', 'robot']\n\nNotare che i modelli addestrati su Edge Impulse Studio produrranno valori con indice 0, 1, 2, ecc., dove le etichette effettive seguiranno un ordine alfabetico.\n\nCaricare il modello, allocare i tensori e ottenere i dettagli dei tensori di input e output:\n# Load the TFLite model\ninterpreter = tflite.Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n# Get input and output tensors\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\nUna differenza importante da notare è che il dtype dei dettagli di input del modello è ora int8, il che significa che i valori di input vanno da -128 a +127, mentre ogni pixel della nostra immagine va da 0 a 255. Ciò significa che dovremmo pre-elaborare l’immagine per farla corrispondere. Possiamo controllare qui:\ninput_dtype = input_details[0]['dtype']\ninput_dtype\nnumpy.int8\nQuindi, apriamo l’immagine e mostriamola:\nimg = Image.open(img_path)\nplt.figure(figsize=(4, 4))\nplt.imshow(img)\nplt.axis('off')\nplt.show()\n\nEd eseguiamo la pre-elaborazione:\nscale, zero_point = input_details[0]['quantization']\nimg = img.resize((input_details[0]['shape'][1], \n                  input_details[0]['shape'][2]))\nimg_array = np.array(img, dtype=np.float32) / 255.0\nimg_array = (img_array / scale + zero_point).clip(-128, 127).astype(np.int8)\ninput_data = np.expand_dims(img_array, axis=0)\nControllando i dati di input, possiamo verificare che il tensore di input è compatibile con quanto previsto dal modello:\ninput_data.shape, input_data.dtype\n((1, 160, 160, 3), dtype('int8'))\nAdesso è il momento di effettuare l’inferenza. Calcoliamo anche la latenza del modello:\n# Inference on Raspi-Zero\nstart_time = time.time()\ninterpreter.set_tensor(input_details[0]['index'], input_data)\ninterpreter.invoke()\nend_time = time.time()\ninference_time = (end_time - start_time) * 1000  # Convert to milliseconds\nprint (\"Inference time: {:.1f}ms\".format(inference_time))\nIl modello impiegherà circa 125 ms per eseguire l’inferenza nel Raspi-Zero, che dura 3 o 4 volte più a lungo di un Raspi-5.\nOra possiamo ottenere le etichette di output e le probabilità. È anche importante notare che il modello addestrato su Edge Impulse Studio ha un softmax nel suo output (diverso dal Movilenet V2 originale) e dovremmo usare l’output grezzo del modello come “probabilità”.\n# Obtain results and map them to the classes\npredictions = interpreter.get_tensor(output_details[0]['index'])[0]\n\n# Get indices of the top k results\ntop_k_results=3\ntop_k_indices = np.argsort(predictions)[::-1][:top_k_results]\n\n# Get quantization parameters\nscale, zero_point = output_details[0]['quantization']\n\n# Dequantize the output\ndequantized_output = (predictions.astype(np.float32) - zero_point) * scale\nprobabilities = dequantized_output\n\nprint(\"\\n\\t[PREDICTION]        [Prob]\\n\")\nfor i in range(top_k_results):\n    print(\"\\t{:20}: {:.2f}%\".format(\n        labels[top_k_indices[i]],\n        probabilities[top_k_indices[i]] * 100))\n\nModifichiamo la funzione creata in precedenza in modo da poter gestire diversi tipi di modelli:\n\ndef image_classification(img_path, model_path, labels, top_k_results=3, \n                         apply_softmax=False):\n    # Load the image\n    img = Image.open(img_path)\n    plt.figure(figsize=(4, 4))\n    plt.imshow(img)\n    plt.axis('off')\n\n    # Load the TFLite model\n    interpreter = tflite.Interpreter(model_path=model_path)\n    interpreter.allocate_tensors()\n    \n    # Get input and output tensors\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n    \n    # Preprocess\n    img = img.resize((input_details[0]['shape'][1], \n                      input_details[0]['shape'][2]))\n    \n    input_dtype = input_details[0]['dtype']\n    \n    if input_dtype == np.uint8:\n        input_data = np.expand_dims(np.array(img), axis=0)\n    elif input_dtype == np.int8:\n        scale, zero_point = input_details[0]['quantization']\n        img_array = np.array(img, dtype=np.float32) / 255.0\n        img_array = (img_array / scale + zero_point).clip(-128, 127).astype(np.int8)\n        input_data = np.expand_dims(img_array, axis=0)\n    else:  # float32\n        input_data = np.expand_dims(np.array(img, dtype=np.float32), axis=0) / 255.0\n    \n    # Inference on Raspi-Zero\n    start_time = time.time()\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n    end_time = time.time()\n    inference_time = (end_time - start_time) * 1000  # Convert to milliseconds\n    \n    # Obtain results\n    predictions = interpreter.get_tensor(output_details[0]['index'])[0]\n    \n    # Get indices of the top k results\n    top_k_indices = np.argsort(predictions)[::-1][:top_k_results]\n    \n    # Handle output based on type\n    output_dtype = output_details[0]['dtype']\n    if output_dtype in [np.int8, np.uint8]:\n        # Dequantize the output\n        scale, zero_point = output_details[0]['quantization']\n        predictions = (predictions.astype(np.float32) - zero_point) * scale\n    \n    if apply_softmax:\n        # Apply softmax\n        exp_preds = np.exp(predictions - np.max(predictions))\n        probabilities = exp_preds / np.sum(exp_preds)\n    else:\n        probabilities = predictions\n    \n    print(\"\\n\\t[PREDICTION]        [Prob]\\n\")\n    for i in range(top_k_results):\n        print(\"\\t{:20}: {:.1f}%\".format(\n            labels[top_k_indices[i]],\n            probabilities[top_k_indices[i]] * 100))\n    print (\"\\n\\tInference time: {:.1f}ms\".format(inference_time))\n\nE lo si testa con immagini diverse e con il modello quantizzato int8 (160x160 alpha =1.0).\n\nScarichiamo un modello più piccolo, come quello addestrato per il Nicla Vision Lab (int8 quantized model, 96x96, alpha = 0.1), come test. Possiamo usare la stessa funzione:\n\nIl modello ha perso un po’ di accuratezza, ma è ancora OK dato che non cerca molti dettagli. Per quanto riguarda la latenza, siamo circa dieci volte più veloci su Raspi-Zero.",
    "crumbs": [
      "Raspberry Pi",
      "Classificazione delle Immagini"
    ]
  },
  {
    "objectID": "contents/labs/raspi/image_classification/image_classification.it.html#classificazione-delle-immagini-in-tempo-reale",
    "href": "contents/labs/raspi/image_classification/image_classification.it.html#classificazione-delle-immagini-in-tempo-reale",
    "title": "Classificazione delle Immagini",
    "section": "Classificazione delle Immagini in Tempo Reale",
    "text": "Classificazione delle Immagini in Tempo Reale\nSviluppiamo un’app per catturare immagini con la fotocamera USB in tempo reale, mostrandone la classificazione.\nUtilizzando nano sul terminale, salvare il codice sottostante, come img_class_live_infer.py.\n\nfrom flask import Flask, Response, render_template_string, request, jsonify\nfrom picamera2 import Picamera2\nimport io\nimport threading\nimport time\nimport numpy as np\nfrom PIL import Image\nimport tflite_runtime.interpreter as tflite\nfrom queue import Queue\n\napp = Flask(__name__)\n\n# Global variables\npicam2 = None\nframe = None\nframe_lock = threading.Lock()\nis_classifying = False\nconfidence_threshold = 0.8\nmodel_path = \"./models/ei-raspi-img-class-int8-quantized-model.tflite\"\nlabels = ['background', 'periquito', 'robot']\ninterpreter = None\nclassification_queue = Queue(maxsize=1)\n\ndef initialize_camera():\n    global picam2\n    picam2 = Picamera2()\n    config = picam2.create_preview_configuration(main={\"size\": (320, 240)})\n    picam2.configure(config)\n    picam2.start()\n    time.sleep(2)  # Wait for camera to warm up\n\ndef get_frame():\n    global frame\n    while True:\n        stream = io.BytesIO()\n        picam2.capture_file(stream, format='jpeg')\n        with frame_lock:\n            frame = stream.getvalue()\n        time.sleep(0.1)  # Capture frames more frequently\n\ndef generate_frames():\n    while True:\n        with frame_lock:\n            if frame is not None:\n                yield (b'--frame\\r\\n'\n                       b'Content-Type: image/jpeg\\r\\n\\r\\n' + frame + b'\\r\\n')\n        time.sleep(0.1)\n\ndef load_model():\n    global interpreter\n    if interpreter is None:\n        interpreter = tflite.Interpreter(model_path=model_path)\n        interpreter.allocate_tensors()\n    return interpreter\n\ndef classify_image(img, interpreter):\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n\n    img = img.resize((input_details[0]['shape'][1], \n                      input_details[0]['shape'][2]))\n    input_data = np.expand_dims(np.array(img), axis=0)\\\n                             .astype(input_details[0]['dtype'])\n\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    predictions = interpreter.get_tensor(output_details[0]['index'])[0]\n    # Handle output based on type\n    output_dtype = output_details[0]['dtype']\n    if output_dtype in [np.int8, np.uint8]:\n        # Dequantize the output\n        scale, zero_point = output_details[0]['quantization']\n        predictions = (predictions.astype(np.float32) - zero_point) * scale\n    return predictions\n\ndef classification_worker():\n    interpreter = load_model()\n    while True:\n        if is_classifying:\n            with frame_lock:\n                if frame is not None:\n                    img = Image.open(io.BytesIO(frame))\n            predictions = classify_image(img, interpreter)\n            max_prob = np.max(predictions)\n            if max_prob &gt;= confidence_threshold:\n                label = labels[np.argmax(predictions)]\n            else:\n                label = 'Uncertain'\n            classification_queue.put({'label': label, \n                                      'probability': float(max_prob)})\n        time.sleep(0.1)  # Adjust based on your needs\n\n@app.route('/')\ndef index():\n    return render_template_string('''\n        &lt;!DOCTYPE html&gt;\n        &lt;html&gt;\n        &lt;head&gt;\n            &lt;title&gt;Image Classification&lt;/title&gt;\n            &lt;script \n                src=\"https://code.jquery.com/jquery-3.6.0.min.js\"&gt;\n            &lt;/script&gt;\n            &lt;script&gt;\n                function startClassification() {\n                    $.post('/start');\n                    $('#startBtn').prop('disabled', true);\n                    $('#stopBtn').prop('disabled', false);\n                }\n\n                function stopClassification() {\n                    $.post('/stop');\n                    $('#startBtn').prop('disabled', false);\n                    $('#stopBtn').prop('disabled', true);\n                }\n\n                function updateConfidence() {\n                    var confidence = $('#confidence').val();\n                    $.post('/update_confidence', {confidence: confidence});\n                }\n\n                function updateClassification() {\n                    $.get('/get_classification', function(data) {\n                        $('#classification').text(data.label + ': ' \n                        + data.probability.toFixed(2));\n                    });\n                }\n\n                $(document).ready(function() {\n                    setInterval(updateClassification, 100);  \n                    // Update every 100ms\n                });\n            &lt;/script&gt;\n        &lt;/head&gt;\n        &lt;body&gt;\n            &lt;h1&gt;Image Classification&lt;/h1&gt;\n            &lt;img src=\"{{ url_for('video_feed') }}\" width=\"640\" height=\"480\" /&gt;\n            &lt;br&gt;\n            &lt;button id=\"startBtn\" onclick=\"startClassification()\"&gt;\n            Start Classification&lt;/button&gt;\n            &lt;button id=\"stopBtn\" onclick=\"stopClassification()\" disabled&gt;\n            Stop Classification&lt;/button&gt;\n            &lt;br&gt;\n            &lt;label for=\"confidence\"&gt;Confidence Threshold:&lt;/label&gt;\n            &lt;input type=\"number\" id=\"confidence\" name=\"confidence\" min=\"0\" \n            max=\"1\" step=\"0.1\" value=\"0.8\" onchange=\"updateConfidence()\"&gt;\n            &lt;br&gt;\n            &lt;div id=\"classification\"&gt;Waiting for classification...&lt;/div&gt;\n        &lt;/body&gt;\n        &lt;/html&gt;\n    ''')\n\n@app.route('/video_feed')\ndef video_feed():\n    return Response(generate_frames(),\n                    mimetype='multipart/x-mixed-replace; boundary=frame')\n\n@app.route('/start', methods=['POST'])\ndef start_classification():\n    global is_classifying\n    is_classifying = True\n    return '', 204\n\n@app.route('/stop', methods=['POST'])\ndef stop_classification():\n    global is_classifying\n    is_classifying = False\n    return '', 204\n\n@app.route('/update_confidence', methods=['POST'])\ndef update_confidence():\n    global confidence_threshold\n    confidence_threshold = float(request.form['confidence'])\n    return '', 204\n\n@app.route('/get_classification')\ndef get_classification():\n    if not is_classifying:\n        return jsonify({'label': 'Not classifying', 'probability': 0})\n    try:\n        result = classification_queue.get_nowait()\n    except Queue.Empty:\n        result = {'label': 'Processing', 'probability': 0}\n    return jsonify(result)\n\nif __name__ == '__main__':\n    initialize_camera()\n    threading.Thread(target=get_frame, daemon=True).start()\n    threading.Thread(target=classification_worker, daemon=True).start()\n    app.run(host='0.0.0.0', port=5000, threaded=True)\n\nSul terminale lanciare:\npython3 img_class_live_infer.py\nE accedere all’interfaccia web:\n\nSul Raspberry Pi stesso (se si ha una GUI): si apre un browser web e si va suhttp://localhost:5000\nDa un altro dispositivo sulla stessa rete: aprire un browser web e andare su http://&lt;raspberry_pi_ip&gt;:5000 (Sostituire &lt;raspberry_pi_ip&gt; con l’indirizzo IP del Raspberry Pi). Per esempio: http://192.168.4.210:5000/\n\nEcco alcuni screenshot dell’app in esecuzione su un desktop esterno\n\nQui si può vedere l’app in esecuzione su YouTube:\n\nIl codice crea un’applicazione web per la classificazione delle immagini in tempo reale utilizzando un Raspberry Pi, il suo modulo fotocamera e un modello TensorFlow Lite. L’applicazione utilizza Flask per fornire un’interfaccia web in cui è possibile visualizzare il feed della fotocamera e vedere i risultati della classificazione in tempo reale.\n\nComponenti Chiave:\n\nApplicazione Web Flask: Fornisce l’interfaccia utente e gestisce le richieste.\nPiCamera2: Cattura le immagini dal modulo fotocamera Raspberry Pi.\nTensorFlow Lite: Esegue il modello di classificazione delle immagini.\nThreading: Gestisce le operazioni simultanee per prestazioni fluide.\n\n\n\nCaratteristiche Principali:\n\nVisualizzazione feed telecamera live\nClassificazione immagini in tempo reale\nSoglia di confidenza regolabile\nAvvia/Arresta classificazione su richiesta\n\n\n\nStruttura del Codice:\n\nImportazioni e Setup:\n\nFlask per applicazione web\nPiCamera2 per controllo telecamera\nTensorFlow Lite per l’inferenza\nThreading e Queue per operazioni concorrenti\n\nVariabili Globali:\n\nGestione telecamera e frame\nControllo classificazione\nInformazioni modello ed etichetta\n\nFunzioni della Telecamera:\n\ninitialize_camera(): Imposta PiCamera2\nget_frame(): Cattura continuamente frame\ngenerate_frames(): Genera frame per il feed web\n\nFunzioni del Modello:\n\nload_model(): Carica il modello TFLite\nclassify_image(): Esegue l’inferenza su una singola immagine\n\nWorker per la Classificazione:\n\nGira in un thread separato\nClassifica continuamente i frame quando è attivo\nAaggiorna una coda con i risultati più recenti\n\nRoute di Flask:\n\n/: Serve la pagina HTML principale\n/video_feed: Trasmette il feed della telecamera\n/start and /stop: Controlla la classificazione\n/update_confidence: Regola la soglia di confidenza\n/get_classification: Restituisce l’ultimo risultato di classificazione\n\nTemplate HTML:\n\nVisualizza il feed della telecamera e la classificazione dei risultati\nFornisce controlli per avviare/arrestare e regolare le impostazioni\n\nEsecuzione Principale:\n\nInizializza la fotocamera e avvia i thread necessari\nEsegue l’applicazione Flask\n\n\n\n\nConcetti Chiave:\n\nOperazioni Concorrenti: Utilizzo di thread per gestire l’acquisizione e la classificazione della telecamera separatamente dal server Web.\nAggiornamenti in Tempo Reale: Aggiornamenti frequenti dei risultati della classificazione senza ricaricare la pagina.\nRiutilizzo del Modello: Caricamento del modello TFLite una volta e il riutilizzo per l’efficienza.\nConfigurazione Flessibile: Consente agli utenti di regolare la soglia di confidenza al volo.\n\n\n\nUso:\n\nAssicurarsi che tutte le dipendenze siano installate.\nEseguire lo script su un Raspberry Pi con un modulo telecamera.\nAccedere all’interfaccia Web da un browser utilizzando l’indirizzo IP del Raspberry Pi.\nAvviare la classificazione e regolare le impostazioni in base alle esigenze.",
    "crumbs": [
      "Raspberry Pi",
      "Classificazione delle Immagini"
    ]
  },
  {
    "objectID": "contents/labs/raspi/image_classification/image_classification.it.html#conclusione",
    "href": "contents/labs/raspi/image_classification/image_classification.it.html#conclusione",
    "title": "Classificazione delle Immagini",
    "section": "Conclusione:",
    "text": "Conclusione:\nLa classificazione delle immagini è emersa come un’applicazione potente e versatile dell’apprendimento automatico, con implicazioni significative per vari campi, dall’assistenza sanitaria al monitoraggio ambientale. Questo capitolo ha dimostrato come implementare un sistema di classificazione delle immagini robusto su dispositivi edge come Raspi-Zero e Raspi-5, mostrando il potenziale per l’intelligenza in tempo reale sul dispositivo.\nAbbiamo esplorato l’intera pipeline di un progetto di classificazione delle immagini, dalla raccolta dati e dall’addestramento del modello tramite Edge Impulse Studio all’implementazione e all’esecuzione di inferenze su un Raspi. Il processo ha evidenziato diversi punti chiave:\n\nL’importanza di una corretta raccolta dati e pre-elaborazione per l’addestramento efficace dei modelli.\nLa potenza dell’apprendimento tramite trasferimento, che ci consente di sfruttare modelli pre-addestrati come MobileNet V2 per un addestramento efficiente con dati limitati.\nI compromessi tra accuratezza del modello e velocità di inferenza, particolarmente cruciali per i dispositivi edge.\nL’implementazione della classificazione in tempo reale tramite un’interfaccia basata sul Web, che dimostra applicazioni pratiche.\n\nLa capacità di eseguire questi modelli su dispositivi edge come Raspi apre numerose possibilità per applicazioni IoT, sistemi autonomi e soluzioni di monitoraggio in tempo reale. Consente una latenza ridotta, una migliore privacy e il funzionamento in ambienti con connettività limitata.\nCome abbiamo visto, anche con i vincoli computazionali dei dispositivi edge, è possibile ottenere risultati impressionanti in termini di accuratezza e velocità. La flessibilità di regolare i parametri del modello, come le dimensioni di input e i valori alfa, consente una messa a punto precisa per soddisfare requisiti di progetto specifici.\nGuardando al futuro, il campo dell’intelligenza artificiale edge e della classificazione delle immagini continua a evolversi rapidamente. I progressi nelle tecniche di compressione dei modelli, nell’accelerazione hardware e nelle architetture di reti neurali più efficienti promettono di espandere ulteriormente le capacità dei dispositivi edge nelle attività di visione artificiale.\nQuesto progetto funge da base per applicazioni di visione artificiale più complesse e incoraggia un’ulteriore esplorazione nell’entusiasmante mondo dell’intelligenza artificiale edge e dell’IoT. Che si tratti di automazione industriale, applicazioni per la casa intelligente o monitoraggio ambientale, le competenze e i concetti trattati qui forniscono un solido punto di partenza per un’ampia gamma di progetti innovativi.",
    "crumbs": [
      "Raspberry Pi",
      "Classificazione delle Immagini"
    ]
  },
  {
    "objectID": "contents/labs/raspi/image_classification/image_classification.it.html#risorse",
    "href": "contents/labs/raspi/image_classification/image_classification.it.html#risorse",
    "title": "Classificazione delle Immagini",
    "section": "Risorse",
    "text": "Risorse\n\nEsempio di Dataset\nImpostazione del Notebook di Prova su un Raspi\nNotebook di Classificazione delle Immagini su un Raspi\nCNN per classificare il dataset Cifar-10 su CoLab\nCifar 10 - Classificazione delle Immagini su un Raspi\nScript Python\nProgetto Edge Impulse",
    "crumbs": [
      "Raspberry Pi",
      "Classificazione delle Immagini"
    ]
  },
  {
    "objectID": "contents/labs/raspi/object_detection/object_detection.it.html",
    "href": "contents/labs/raspi/object_detection/object_detection.it.html",
    "title": "Rilevamento degli Oggetti",
    "section": "",
    "text": "Panoramica\nSulla base della nostra esplorazione della classificazione delle immagini, ora rivolgiamo la nostra attenzione a un’attività di visione artificiale più avanzata: il rilevamento degli oggetti. Mentre la classificazione delle immagini assegna una singola etichetta a un’intera immagine, il rilevamento degli oggetti va oltre identificando e localizzando più oggetti all’interno di una singola immagine. Questa capacità apre molte nuove applicazioni e sfide, in particolare nell’edge computing e nei dispositivi IoT come Raspberry Pi.\nIl rilevamento degli oggetti combina le attività di classificazione e localizzazione. Non solo determina quali oggetti sono presenti in un’immagine, ma individua anche le loro posizioni, ad esempio disegnando riquadri di delimitazione attorno a essi. Questa complessità aggiunta rende il rilevamento degli oggetti uno strumento più potente per comprendere le scene visive, ma richiede anche modelli e tecniche di training più sofisticati.\nNell’IA edge, dove lavoriamo con risorse computazionali limitate, l’implementazione di modelli di rilevamento degli oggetti efficienti diventa cruciale. Le sfide che abbiamo affrontato con la classificazione delle immagini, ovvero bilanciare le dimensioni del modello, la velocità di inferenza e l’accuratezza, sono amplificate nel rilevamento degli oggetti. Tuttavia, i vantaggi sono anche più significativi, poiché il rilevamento degli oggetti consente un’analisi dei dati visivi più sfumata e dettagliata.\nAlcune applicazioni del rilevamento di oggetti su dispositivi edge includono:\nMettendo le mani nel rilevamento di oggetti, ci baseremo sui concetti e sulle tecniche esplorate nella classificazione delle immagini. Esamineremo le architetture di rilevamento di oggetti più diffuse progettate per l’efficienza, come:\nEsploreremo quei modelli di rilevamento di oggetti utilizzando\nIn questo laboratorio, tratteremo i fondamenti del rilevamento degli oggetti e le sue differenze rispetto alla classificazione delle immagini. Impareremo anche come addestrare, mettere a punto, testare, ottimizzare e distribuire le architetture di rilevamento degli oggetti più diffuse utilizzando un dataset creato da zero.",
    "crumbs": [
      "Raspberry Pi",
      "Rilevamento degli Oggetti"
    ]
  },
  {
    "objectID": "contents/labs/raspi/object_detection/object_detection.it.html#panoramica",
    "href": "contents/labs/raspi/object_detection/object_detection.it.html#panoramica",
    "title": "Rilevamento degli Oggetti",
    "section": "",
    "text": "Sistemi di sorveglianza e sicurezza\nVeicoli autonomi e droni\nControllo industriale della qualità\nMonitoraggio della fauna selvatica\nApplicazioni di realtà aumentata\n\n\n\n“Single Stage Detectors” [Rilevatori a stadio singolo], come MobileNet ed EfficientDet,\nFOMO (Faster Objects, More Objects), e\nYOLO (You Only Look Once).\n\n\nPer saperne di più sui modelli di rilevamento di oggetti, seguire il tutorial A Gentle Introduction to Object Recognition With Deep Learning.\n\n\n\nTensorFlow Lite Runtime (ora modificato in LiteRT),\nEdge Impulse Linux Python SDK e\nUltralitics\n\n\n\n\nFondamenti dell’Object Detection\nIl rilevamento degli oggetti si basa sulle fondamenta della classificazione delle immagini, ma ne amplia notevolmente le capacità. Per comprendere il rilevamento degli oggetti, è fondamentale innanzitutto riconoscere le sue principali differenze rispetto alla classificazione delle immagini:\n\nClassificazione delle Immagini vs. Rilevamento degli Oggetti\nClassificazione delle Immagini:\n\nAssegna una singola etichetta a un’intera immagine\nRisponde alla domanda: “Qual è l’oggetto o la scena principale di questa immagine?”\nGenera una singola previsione di classe per l’intera immagine\n\nRilevamento degli oggetti:\n\nIdentifica e individua più oggetti all’interno di un’immagine\nRisponde alle domande: “Quali oggetti sono presenti in questa immagine e dove si trovano?”\nGenera più previsioni, ciascuna composta da un’etichetta di classe e un riquadro di delimitazione\n\nPer visualizzare questa differenza, prendiamo in considerazione un esempio: \nQuesto diagramma illustra la differenza critica: la classificazione delle immagini fornisce un’unica etichetta per l’intera immagine, mentre il rilevamento degli oggetti identifica più oggetti, le loro classi e le loro posizioni all’interno dell’immagine.\n\n\nComponenti Chiave del Rilevamento degli Oggetti\nI sistemi di rilevamento degli oggetti sono in genere costituiti da due componenti principali:\n\nLocalizzazione degli Oggetti: Questo componente identifica dove si trovano gli oggetti nell’immagine. In genere genera riquadri di delimitazione, regioni rettangolari che racchiudono ogni oggetto rilevato.\nClassificazione degli Oggetti: Questo componente determina la classe o la categoria di ogni oggetto rilevato, in modo simile alla classificazione delle immagini ma applicato a ogni regione localizzata.\n\n\n\nSfide nel Rilevamento degli Oggetti\nIl rilevamento degli oggetti presenta diverse sfide oltre a quelle della classificazione delle immagini:\n\nOggetti multipli: Un’immagine può contenere più oggetti di varie classi, dimensioni e posizioni.\nScale variabili: Gli oggetti possono apparire a diverse dimensioni all’interno dell’immagine.\nOcclusione: Gli oggetti possono essere parzialmente nascosti o sovrapposti.\nDisordine di sfondo: Distinguere gli oggetti da sfondi complessi può essere difficile.\nPrestazioni in tempo reale: Molte applicazioni richiedono tempi di inferenza rapidi, soprattutto su dispositivi edge.\n\n\n\nApprocci al Rilevamento di Oggetti\nEsistono due approcci principali al rilevamento di oggetti:\n\nRilevatori a due stadi: Prima propongono le regioni di interesse e poi classificano ciascuna regione. Esempi includono R-CNN e le sue varianti (Fast R-CNN, Faster R-CNN).\nRilevatori a stadio singolo: Questi prevedono bounding box (o centroidi) e probabilità di classe in un passaggio “forward” [in avanti] della rete. Esempi includono YOLO (You Only Look Once), EfficientDet, SSD (Single Shot Detector) e FOMO (Faster Objects, More Objects). Questi sono spesso più veloci e più adatti per dispositivi edge come Raspberry Pi.\n\n\n\nMetriche di Valutazione\nIl rilevamento degli oggetti utilizza metriche diverse rispetto alla classificazione delle immagini:\n\nIntersection over Union (IoU): Misura la sovrapposizione tra bounding box previsti e “ground truth” [reali].\nMean Average Precision (mAP): Combina precisione e richiamo in tutte le classi e soglie IoU.\nFrames Per Second (FPS): Misura la velocità di rilevamento, fondamentale per le applicazioni in tempo reale su dispositivi edge.",
    "crumbs": [
      "Raspberry Pi",
      "Rilevamento degli Oggetti"
    ]
  },
  {
    "objectID": "contents/labs/raspi/object_detection/object_detection.it.html#panoramica-dei-modelli-di-rilevamento-degli-oggetti-pre-addestrati",
    "href": "contents/labs/raspi/object_detection/object_detection.it.html#panoramica-dei-modelli-di-rilevamento-degli-oggetti-pre-addestrati",
    "title": "Rilevamento degli Oggetti",
    "section": "Panoramica dei Modelli di Rilevamento degli Oggetti Pre-Addestrati",
    "text": "Panoramica dei Modelli di Rilevamento degli Oggetti Pre-Addestrati\nCome abbiamo visto nell’introduzione, data un’immagine o un flusso video, un modello di rilevamento degli oggetti può identificare quale, di un set noto di oggetti, potrebbe essere presente e fornire informazioni sulle loro posizioni all’interno dell’immagine.\n\nSi possono testare alcuni modelli comuni online visitando Object Detection - MediaPipe Studio\n\nSu Kaggle, possiamo trovare i modelli tflite pre-addestrati più comuni da usare con Raspi, ssd_mobilenet_v1 e EfficientDet. Quei modelli sono stati addestrati sul dataset COCO (Common Objects in Context), con oltre 200.000 immagini etichettate in 91 categorie. Scaricare i modelli e caricali nella cartella ./models in Raspi.\n\nIn alternativa, si possono trovare i modelli e le etichette COCO su GitHub.\n\nPer la prima parte di questo laboratorio, ci concentreremo su un modello SSD-Mobilenet V1 300x300 pre-addestrato e lo confronteremo con EfficientDet-lite0 320x320, anch’esso addestrato utilizzando il set di dati COCO 2017. Entrambi i modelli sono stati convertiti in un formato TensorFlow Lite (4,2 MB per SSD Mobilenet e 4,6 MB per EfficientDet).\n\nSSD-Mobilenet V2 o V3 è consigliato per progetti di “transfer learning”, ma una volta che il modello TFLite V1 sarà disponibile al pubblico, lo useremo per questa panoramica.\n\n\n\nImpostazione dell’Ambiente TFLite\nDovremmo confermare i passaggi eseguiti nell’ultimo laboratorio pratico, Image Classification, come segue:\n\nAggiornamento di Raspberry Pi\nInstallazione delle Librerie Richieste\nImpostazione di un Ambiente Virtuale (Facoltativo ma Consigliato)\n\nsource ~/tflite/bin/activate\n\nInstallazione di TensorFlow Lite Runtime\nInstallazione di Additional Python Libraries (all’interno dell’ambiente)\n\n\n\nCreazione di una Directory di Lavoro:\nConsiderando che abbiamo creato la cartella Documents/TFLITE nell’ultimo Lab, creiamo ora le cartelle specifiche per questo lab di rilevamento degli oggetti:\ncd Documents/TFLITE/\nmkdir OBJ_DETECT\ncd OBJ_DETECT\nmkdir images\nmkdir models\ncd models\n\n\nInferenza e Post-Elaborazione\nApriamo un nuovo notebook per seguire tutti i passaggi per rilevare oggetti su un’immagine:\nImportare le librerie necessarie:\nimport time\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport tflite_runtime.interpreter as tflite\nCaricare il modello TFLite e allocare i tensori:\nmodel_path = \"./models/ssd-mobilenet-v1-tflite-default-v1.tflite\"\ninterpreter = tflite.Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\nOttenere i tensori di input e output.\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\nInput details ci informeranno su come il modello dovrebbe essere alimentato con un’immagine. La forma di (1, 300, 300, 3) con un dtype di uint8 ci dice che un’immagine non normalizzata (intervallo di valori pixel da 0 a 255) con dimensioni (300x300x3) dovrebbe essere inserita una alla volta (Batch Dimension: 1).\nGli output details includono non solo le etichette (“classes”) e le probabilità (“scores”), ma anche la posizione relativa della finestra dei bounding box (“box”) su dove si trova l’oggetto nell’immagine e il numero di oggetti rilevati (“num_detections”). I dettagli di output ci dicono anche che il modello può rilevare un massimo di 10 oggetti nell’immagine.\n\nQuindi, per l’esempio precedente, utilizzando la stessa immagine di gatto utilizzata col Lab di Classificazione Immagine alla ricerca dell’output, abbiamo una probabilità del 76% di aver trovato un oggetto con un ID classe di 16 su un’area delimitata da un bounding box di [0.028011084, 0.020121813, 0.9886069, 0.802299]. Questi quattro numeri sono correlati a ymin, xmin, ymax e xmax, le coordinate del box.\nConsiderando che y va dall’alto (ymin) al basso (ymax) e x va da sinistra (xmin) a destra (xmax), abbiamo, di fatto, le coordinate dell’angolo superiore/sinistro e di quello inferiore/destro. Con entrambi i bordi e conoscendo la forma dell’immagine, è possibile disegnare un rettangolo attorno all’oggetto, come mostrato nella figura sottostante:\n\nSuccessivamente, dovremmo scoprire cosa significa un ID di classe uguale a 16. Aprendo il file coco_labels.txt, come un elenco, ogni elemento ha un indice associato e ispezionando l’indice 16, otteniamo, come previsto, cat. La probabilità è il valore restituito dal punteggio.\nCarichiamo ora alcune immagini con più oggetti su di esse per il test.\nimg_path = \"./images/cat_dog.jpeg\"\norig_img = Image.open(img_path)\n\n# Display the image\nplt.figure(figsize=(8, 8))\nplt.imshow(orig_img)\nplt.title(\"Original Image\")\nplt.show()\n\nIn base ai dettagli di input, pre-elaboriamo l’immagine, modificandone la forma ed espandendone le dimensioni:\nimg = orig_img.resize((input_details[0]['shape'][1], \n                  input_details[0]['shape'][2]))\ninput_data = np.expand_dims(img, axis=0)\ninput_data.shape, input_data.dtype\nLa nuova forma input_data è(1, 300, 300, 3) con un dtype di uint8, che è compatibile con quanto previsto dal modello.\nUtilizzando input_data, eseguiamo l’interprete, misuriamo la latenza e otteniamo l’output:\nstart_time = time.time()\ninterpreter.set_tensor(input_details[0]['index'], input_data)\ninterpreter.invoke()\nend_time = time.time()\ninference_time = (end_time - start_time) * 1000  # Convert to milliseconds\nprint (\"Inference time: {:.1f}ms\".format(inference_time))\nCon una latenza di circa 800 ms, possiamo ottenere 4 output distinti:\nboxes = interpreter.get_tensor(output_details[0]['index'])[0] \nclasses = interpreter.get_tensor(output_details[1]['index'])[0]  \nscores = interpreter.get_tensor(output_details[2]['index'])[0]   \nnum_detections = int(interpreter.get_tensor(output_details[3]['index'])[0])\nDa una rapida occhiata, possiamo vedere che il modello ha rilevato 2 oggetti con un punteggio superiore a 0,5:\nfor i in range(num_detections):\n    if scores[i] &gt; 0.5:  # Confidence threshold\n        print(f\"Object {i}:\")\n        print(f\"  Bounding Box: {boxes[i]}\")\n        print(f\"  Confidence: {scores[i]}\")\n        print(f\"  Class: {classes[i]}\")\n\nE possiamo anche visualizzare i risultati:\nplt.figure(figsize=(12, 8))\nplt.imshow(orig_img)\nfor i in range(num_detections):\n    if scores[i] &gt; 0.5:  # Adjust threshold as needed\n        ymin, xmin, ymax, xmax = boxes[i]\n        (left, right, top, bottom) = (xmin * orig_img.width, \n                                      xmax * orig_img.width, \n                                      ymin * orig_img.height, \n                                      ymax * orig_img.height)\n        rect = plt.Rectangle((left, top), right-left, bottom-top, \n                             fill=False, color='red', linewidth=2)\n        plt.gca().add_patch(rect)\n        class_id = int(classes[i])\n        class_name = labels[class_id]\n        plt.text(left, top-10, f'{class_name}: {scores[i]:.2f}', \n                 color='red', fontsize=12, backgroundcolor='white')\n\n\n\nEfficientDet\nEfficientDet non è tecnicamente un modello SSD (Single Shot Detector), ma condivide alcune somiglianze e si basa su idee provenienti da SSD e altre architetture di rilevamento degli oggetti:\n\nEfficientDet:\n\nSviluppato dai ricercatori di Google nel 2019\nUtilizza EfficientNet come rete backbone\nUtilizza una nuova “bi-directional feature pyramid network (BiFPN)” [rete piramidale di feature bidirezionale]\nUtilizza il ridimensionamento composto per ridimensionare in modo efficiente la rete backbone e i componenti di rilevamento degli oggetti.\n\nSomiglianze con SSD:\n\nEntrambi sono rilevatori a stadio singolo, il che significa che eseguono la localizzazione e la classificazione degli oggetti in un singolo passaggio “forward” [in avanti].\nEntrambi utilizzano mappe di feature multiscala per rilevare oggetti a diverse scale.\n\nDifferenze principali:\n\nBackbone: SSD in genere utilizza VGG o MobileNet, mentre EfficientDet utilizza EfficientNet.\nFusione di funzionalità: SSD utilizza una semplice piramide di funzionalità, mentre EfficientDet utilizza il più avanzato BiFPN.\nMetodo di ridimensionamento: EfficientDet introduce il ridimensionamento composto per tutti i componenti della rete\n\nVantaggi di EfficientDet:\n\nIn genere, raggiunge migliori compromessi tra accuratezza ed efficienza rispetto a SSD e molti altri modelli di rilevamento di oggetti.\nUn ridimensionamento più flessibile consente una famiglia di modelli con diversi compromessi tra dimensioni e prestazioni.\n\n\nSebbene EfficientDet non sia un modello SSD, può essere visto come un’evoluzione delle architetture di rilevamento a fase singola, che incorpora tecniche più avanzate per migliorare efficienza e precisione. Quando si utilizza EfficientDet, possiamo aspettarci strutture di output simili a SSD (ad esempio, bounding box e punteggi di classe).\n\nSu GitHub, si trova un altro notebook che esplora il modello EfficientDet che abbiamo realizzato con SSD MobileNet.",
    "crumbs": [
      "Raspberry Pi",
      "Rilevamento degli Oggetti"
    ]
  },
  {
    "objectID": "contents/labs/raspi/object_detection/object_detection.it.html#progetto-di-rilevamento-di-oggetti",
    "href": "contents/labs/raspi/object_detection/object_detection.it.html#progetto-di-rilevamento-di-oggetti",
    "title": "Rilevamento degli Oggetti",
    "section": "Progetto di Rilevamento di Oggetti",
    "text": "Progetto di Rilevamento di Oggetti\nOra, svilupperemo un progetto completo di classificazione delle immagini dalla raccolta dei dati all’addestramento e all’implementazione. Come abbiamo fatto con il progetto di classificazione delle immagini, il modello addestrato e convertito verrà utilizzato per l’inferenza.\nUtilizzeremo lo stesso set di dati per addestrare 3 modelli: SSD-MobileNet V2, FOMO e YOLO.\n\nL’Obiettivo\nTutti i progetti di Machine Learning devono iniziare con un obiettivo. Supponiamo di trovarci in una struttura industriale e di dover ordinare e contare ruote e scatole speciali.\n\nIn altre parole, dovremmo eseguire una classificazione multi-etichetta, in cui ogni immagine può avere tre classi:\n\nBackground (nessun oggetto)\nBox [Scatola]\nWheel [Ruota]\n\n\n\nRaccolta Dati Grezzi\nUna volta definito l’obiettivo del nostro progetto di apprendimento automatico, il passaggio successivo, e più cruciale, è la raccolta del dataset. Possiamo utilizzare un telefono, il Raspi o un mix per creare il set di dati grezzi (senza etichette). Usiamo la semplice app Web sul nostro Raspberry Pi per visualizzare le immagini QVGA (320 x 240) catturate in un browser.\nDa GitHub, si prende lo script Python get_img_data.py e lo si apre nel terminale:\npython3 get_img_data.py\nAccedere all’interfaccia web:\n\nSul Raspberry Pi stesso (se si ha una GUI): si apre un browser web e si va suhttp://localhost:5000\nDa un altro dispositivo sulla stessa rete: aprire un browser web e andare su http://&lt;raspberry_pi_ip&gt;:5000 (Sostituire &lt;raspberry_pi_ip&gt; con l’indirizzo IP del Raspberry Pi). Per esempio: http://192.168.4.210:5000/\n\nLo script Python crea un’interfaccia basata sul Web per catturare e organizzare set di dati di immagini utilizzando un Raspberry Pi e la sua fotocamera. È utile per progetti di apprendimento automatico che richiedono dati di immagini etichettati o meno, come nel nostro caso.\nAccedere all’interfaccia Web da un browser, inserire un’etichetta generica per le immagini da catturare e premere Start Capture.\n\n\nNotare che le immagini da catturare avranno più etichette che dovranno essere definite in seguito.\n\nUtilizzare l’anteprima live per posizionare la fotocamera e cliccare su Capture Image per salvare le immagini sotto l’etichetta corrente (in questo caso, box-wheel.\n\nQuando abbiamo abbastanza immagini, possiamo premere Stop Capture. Le immagini catturate vengono salvate nella cartella dataset/box-wheel:\n\n\nAcquisire circa 60 immagini. Provare ad acquisire con diverse angolazioni, sfondi e condizioni di luce. Filezilla può trasferire il dataset raw creato sul computer principale.\n\n\n\nEtichettatura dei Dati\nIl passaggio successivo in un progetto di Object Detect è creare un dataset etichettato. Dovremmo etichettare le immagini del dataset raw, creando riquadri di delimitazione attorno agli oggetti di ogni immagine (box e ruota). Possiamo usare strumenti di etichettatura come LabelImg, CVAT, Roboflow, o persino Edge Impulse Studio. Dopo aver esplorato lo strumento Edge Impulse in altri laboratori, usiamo Roboflow qui.\n\nStiamo usando Roboflow (versione gratuita) qui per due motivi principali. 1) Possiamo avere un auto-labeler e 2) Il dataset annotato è disponibile in diversi formati e può essere utilizzato sia su Edge Impulse Studio (lo useremo per MobileNet V2 e FOMO train) sia su CoLab (YOLOv8 train), ad esempio. Avendo il dataset annotato su Edge Impulse (account gratuito), non è possibile utilizzarlo per il training su altre piattaforme.\n\nDovremmo caricare il dataset grezzo su Roboflow. Creare un account gratuito lì e avviare un nuovo progetto, ad esempio (“box-versus-wheel”).\n\n\nNon entreremo nei dettagli del processo Roboflow dato che sono disponibili molti tutorial.\n\n\nAnnotazione\nUna volta creato il progetto e caricato il dataset, si devono effettuare le annotazioni utilizzando lo strumento “Auto-Label”. Notare che si può anche caricare immagini con solo uno sfondo, che dovrebbero essere salvate senza annotazioni.\n\nUna volta annotate tutte le immagini, si devono dividere in training, validation e testing.\n\n\n\nPre-elaborazione dei Dati\nL’ultimo passaggio con il dataset è la pre-elaborazione per generare una versione finale per il training. Ridimensioniamo tutte le immagini a 320x320 e generiamo versioni aumentate di ogni immagine (augmentation) per creare nuovi esempi di training da cui il nostro modello può imparare.\nPer l’augmentation, ruoteremo le immagini (+/-15o), ritaglieremo e varieremo la luminosità e l’esposizione.\n\nAlla fine del processo avremo 153 immagini.\n\nOra si deve esportare il dataset annotato in un formato che Edge Impulse, Ultralitics e altri framework/strumenti possano comprendere, ad esempio YOLOv8. Scarichiamo una versione compressa del dataset sul nostro desktop.\n\nQui è possibile rivedere come è stato strutturato il dataset\n\nCi sono 3 cartelle separate, una per ogni suddivisione (train/test/valid). Per ognuna di esse ci sono 2 sottocartelle, images e labels. Le immagini sono archiviate come image_id.jpg e images_id.txt, dove “image_id” è univoco per ogni immagine.\nIl formato del file delle etichette sarà class_id coordinate del riquadro di delimitazione, dove nel nostro caso class_id sarà 0 per box e 1 per wheel. L’ID numerico (o, 1, 2…) seguirà l’ordine alfabetico del nome della classe.\nIl file data.yaml contiene informazioni sul set di dati come i nomi delle classi (names: ['box', 'wheel']) seguendo il formato YOLO.\nE questo è tutto! Siamo pronti per iniziare l’addestramento utilizzando Edge Impulse Studio (come faremo nel passaggio successivo), Ultralytics (come faremo quando discuteremo di YOLO) o persino l’addestramento da zero su CoLab (come abbiamo fatto col dataset Cifar-10 nel laboratorio di classificazione delle immagini).\n\nIl dataset pre-elaborato si trova sul sito Roboflow, o qui:",
    "crumbs": [
      "Raspberry Pi",
      "Rilevamento degli Oggetti"
    ]
  },
  {
    "objectID": "contents/labs/raspi/object_detection/object_detection.it.html#addestramento-di-un-modello-ssd-mobilenet-su-edge-impulse-studio",
    "href": "contents/labs/raspi/object_detection/object_detection.it.html#addestramento-di-un-modello-ssd-mobilenet-su-edge-impulse-studio",
    "title": "Rilevamento degli Oggetti",
    "section": "Addestramento di un Modello SSD MobileNet su Edge Impulse Studio",
    "text": "Addestramento di un Modello SSD MobileNet su Edge Impulse Studio\nSi va su Edge Impulse Studio, si inseriscono le proprie credenziali in Login (o si crea un account) e si avvia un nuovo progetto.\n\nQui, è possibile clonare il progetto sviluppato per questo laboratorio pratico: Raspi - Object Detection.\n\nNella scheda Dashboard del progetto, si va in basso e su Project info e per “Labeling method” si seleziona Bounding boxes (object detection)\n\nCaricamento dei dati annotati\nSu Studio, si va alla scheda Data acquisition e nella sezione UPLOAD DATA si carica dal computer il set di dati non elaborato.\nPossiamo usare l’opzione Select a folder, scegliendo, ad esempio, la cartella train nel computer, che contiene due sottocartelle, images e labels. Selezionare Image label format, “YOLO TXT”, caricare nella categoria Training e premere Upload data.\n\nRipetere il processo per i dati di test (caricare entrambe le cartelle, test e convalida). Alla fine del processo di caricamento, si ottiene il set di dati annotato di 153 immagini suddivise in train/test (84%/16%).\n\nNotare che le etichette saranno archiviate nei file di etichette 0 e 1, che sono equivalenti a box e wheel.\n\n\n\n\nImpulse Design\nLa prima cosa da definire quando entriamo nella fase Create impulse è descrivere il dispositivo target per la distribuzione. Apparirà una finestra pop-up. Selezioneremo Raspberry 4, un dispositivo intermedio tra Raspi-Zero e Raspi-5.\n\nQuesta scelta non interferirà con l’addestramento; ci darà solo un’idea della latenza del modello su quel target specifico.\n\n\nIn questa fase, si deve definire come:\n\nIl pre-processing consiste nel ridimensionare le singole immagini. Nel nostro caso, le immagini sono state pre-elaborate su Roboflow, a 320x320, quindi teniamole. La modifica delle dimensioni non avrà importanza qui perché le immagini sono già quadrate. Se si carica un’immagine rettangolare, la si deve schiacciare (forma quadrata, senza ritagliarla). In seguito, si potrebbe definire se le immagini vengono convertite da RGB a scala di grigi o meno.\nDesign a Model, in questo caso, “Object Detection”.\n\n\n\n\nPre-elaborazione di tutti i dataset\nNella sezione Image, selezionare Color depth come RGB e premere Save parameters.\n\nLo Studio passa automaticamente alla sezione successiva, Generate features, dove tutti i campioni verranno pre-elaborati, ottenendo 480 oggetti: 207 box e 273 ruote.\n\nL’esploratore di feature mostra che tutti i campioni evidenziano una buona separazione dopo la generazione delle feature.\n\n\nProgettazione, Addestramento e Test del Modello\nPer l’addestramento, dovremmo selezionare un modello pre-addestrato. Usiamo MobileNetV2 SSD FPN-Lite (solo 320x320) . È un modello di rilevamento oggetti pre-addestrato progettato per individuare fino a 10 oggetti all’interno di un’immagine, generando un riquadro di delimitazione per ogni oggetto rilevato. Il modello è di circa 3,7 MB. Supporta un input RGB a 320x320px.\nPer quanto riguarda gli iperparametri di training, il modello verrà addestrato con:\n\nEpochs: 25\nBatch size: 32\nLearning Rate: 0.15.\n\nPer la convalida durante l’addestramento, il 20% del set di dati (validation_dataset) verrà risparmiato.\n\nDi conseguenza, il modello termina con un punteggio di precisione complessivo (basato su COCO mAP) dell’88,8%, superiore al risultato ottenuto utilizzando i dati di test (83,3%).\n\n\nDistribuzione del modello\nAbbiamo due modi per distribuire il modello:\n\nModello TFLite, che consente di distribuire il modello addestrato come .tflite affinché Raspi lo esegua tramite Python.\nLinux (AARCH64), un binario per Linux (AARCH64), implementa il protocollo Edge Impulse Linux, che consente di eseguire i modelli su qualsiasi scheda di sviluppo basata su Linux, ad esempio con SDK per Python. Consulta la documentazione per maggiori informazioni e le istruzioni di configurazione.\n\nDistribuiamo il modello TFLite. Nella scheda Dashboard, si va su Transfer learning model (int8 quantized) e si clicca sull’icona di download:\n\nTrasferire il modello dal computer alla cartella Raspi ./models e catturare o ottenere alcune immagini per l’inferenza e salvale nella cartella ./images.\n\n\nInferenza e Post-Elaborazione\nL’inferenza può essere fatta come discusso nella Panoramica dei Modelli di Rilevamento degli Oggetti Pre-Addestrati. Iniziamo un nuovo notebook per seguire tutti i passaggi per rilevare cubi e ruote su un’immagine.\nImportare le librerie necessarie:\nimport time\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nfrom PIL import Image\nimport tflite_runtime.interpreter as tflite\nDefinire il percorso del modello e delle etichette:\nmodel_path = \"./models/ei-raspi-object-detection-SSD-MobileNetv2-320x0320-\\\nint8.lite\"\nlabels = ['box', 'wheel']\n\nRicordare che il modello restituirà l’ID della classe come valori (0 e 1), seguendo un ordine alfabetico in base ai nomi delle classi.\n\nCaricare il modello, allocare i tensori e ottenere i dettagli dei tensori di input e output:\n# Load the TFLite model\ninterpreter = tflite.Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n# Get input and output tensors\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\nUna differenza fondamentale da notare è che il dtype dei dettagli di input del modello è ora int8, il che significa che i valori di input vanno da -128 a +127, mentre ogni pixel della nostra immagine raw va da 0 a 256. Ciò significa che dovremmo pre-elaborare l’immagine per farla corrispondere. Possiamo controllare qui:\ninput_dtype = input_details[0]['dtype']\ninput_dtype\nnumpy.int8\nQuindi, apriamo l’immagine e mostriamola:\n# Load the image\nimg_path = \"./images/box_2_wheel_2.jpg\"\norig_img = Image.open(img_path)\n\n# Display the image\nplt.figure(figsize=(6, 6))\nplt.imshow(orig_img)\nplt.title(\"Original Image\")\nplt.show()\n\nEd eseguiamo la pre-elaborazione:\nscale, zero_point = input_details[0]['quantization']\nimg = orig_img.resize((input_details[0]['shape'][1], \n                  input_details[0]['shape'][2]))\nimg_array = np.array(img, dtype=np.float32) / 255.0\nimg_array = (img_array / scale + zero_point).clip(-128, 127).astype(np.int8)\ninput_data = np.expand_dims(img_array, axis=0)\nControllando i dati di input, possiamo verificare che il tensore di input è compatibile con quanto previsto dal modello:\ninput_data.shape, input_data.dtype\n((1, 320, 320, 3), dtype('int8'))\nAdesso è il momento di effettuare l’inferenza. Calcoliamo anche la latenza del modello:\n# Inference on Raspi-Zero\nstart_time = time.time()\ninterpreter.set_tensor(input_details[0]['index'], input_data)\ninterpreter.invoke()\nend_time = time.time()\ninference_time = (end_time - start_time) * 1000  # Convert to milliseconds\nprint (\"Inference time: {:.1f}ms\".format(inference_time))\nIl modello impiegherà circa 600 ms per eseguire l’inferenza nel Raspi-Zero, che è circa 5 volte più lungo di un Raspi-5.\nOra possiamo ottenere le classi di output degli oggetti rilevati, le coordinate dei suoi bounding box e le probabilità.\nboxes = interpreter.get_tensor(output_details[1]['index'])[0]  \nclasses = interpreter.get_tensor(output_details[3]['index'])[0]  \nscores = interpreter.get_tensor(output_details[0]['index'])[0]        \nnum_detections = int(interpreter.get_tensor(output_details[2]['index'])[0])\nfor i in range(num_detections):\n    if scores[i] &gt; 0.5:  # Confidence threshold\n        print(f\"Object {i}:\")\n        print(f\"  Bounding Box: {boxes[i]}\")\n        print(f\"  Confidence: {scores[i]}\")\n        print(f\"  Class: {classes[i]}\")\n\nDai risultati, possiamo vedere che sono stati rilevati 4 oggetti: due con ID classe 0 (box) e due con ID classe 1 (wheel), cosa è corretto!\nVisualizziamo il risultato per un threshold [soglia] di 0,5\nthreshold = 0.5\nplt.figure(figsize=(6,6))\nplt.imshow(orig_img)\nfor i in range(num_detections):\n    if scores[i] &gt; threshold:  \n        ymin, xmin, ymax, xmax = boxes[i]\n        (left, right, top, bottom) = (xmin * orig_img.width, \n                                      xmax * orig_img.width, \n                                      ymin * orig_img.height, \n                                      ymax * orig_img.height)\n        rect = plt.Rectangle((left, top), right-left, bottom-top, \n                             fill=False, color='red', linewidth=2)\n        plt.gca().add_patch(rect)\n        class_id = int(classes[i])\n        class_name = labels[class_id]\n        plt.text(left, top-10, f'{class_name}: {scores[i]:.2f}', \n                 color='red', fontsize=12, backgroundcolor='white')\n\nMa cosa succede se riduciamo la soglia a 0.3, ad esempio?\n\nCominciamo a vedere falsi positivi e rilevazioni multiple, in cui il modello rileva lo stesso oggetto più volte con diversi livelli di confidenza e bounding box leggermente diversi.\nDi solito, a volte, dobbiamo regolare la soglia su valori più piccoli per catturare tutti gli oggetti, evitando falsi negativi, che porterebbero a rilevazioni multiple.\nPer migliorare i risultati di rilevamento, dovremmo implementare Non-Maximum Suppression (NMS), che aiuta a eliminare le bounding box sovrapposte e mantiene solo il rilevamento più affidabile.\nPer questo, creiamo una funzione generale denominata non_max_suppression(), con il ruolo di perfezionare i risultati di rilevamento degli oggetti eliminando le bounding box ridondanti e sovrapposte. Ciò è possibile selezionando in modo iterativo la rilevazione con il punteggio di confidenza più elevato e rimuovendo altre rilevazioni significativamente sovrapposte in base a una soglia di “Intersection over Union (IoU)” [intersezione su unione].\ndef non_max_suppression(boxes, scores, threshold):\n    # Convert to corner coordinates\n    x1 = boxes[:, 0]\n    y1 = boxes[:, 1]\n    x2 = boxes[:, 2]\n    y2 = boxes[:, 3]\n\n    areas = (x2 - x1 + 1) * (y2 - y1 + 1)\n    order = scores.argsort()[::-1]\n\n    keep = []\n    while order.size &gt; 0:\n        i = order[0]\n        keep.append(i)\n        xx1 = np.maximum(x1[i], x1[order[1:]])\n        yy1 = np.maximum(y1[i], y1[order[1:]])\n        xx2 = np.minimum(x2[i], x2[order[1:]])\n        yy2 = np.minimum(y2[i], y2[order[1:]])\n\n        w = np.maximum(0.0, xx2 - xx1 + 1)\n        h = np.maximum(0.0, yy2 - yy1 + 1)\n        inter = w * h\n        ovr = inter / (areas[i] + areas[order[1:]] - inter)\n\n        inds = np.where(ovr &lt;= threshold)[0]\n        order = order[inds + 1]\n\n    return keep\nCome funziona:\n\nSorting: Inizia ordinando tutte le rilevazioni in base ai loro punteggi di confidenza, dal più alto al più basso.\nSelection: Seleziona la casella con il punteggio più alto e la aggiunge all’elenco finale delle rilevazioni.\nComparison: Questa casella selezionata viene confrontata con tutte le restanti caselle con punteggio più basso.\nElimination: Qualsiasi casella che si sovrapponga in modo significativo (oltre la soglia IoU) alla casella selezionata viene eliminata.\nIteration: Questo processo si ripete con la casella con il punteggio più alto successivo finché tutte le caselle non vengono elaborate.\n\nOra, possiamo definire una funzione di visualizzazione più precisa che prenderà in considerazione una soglia IoU, rilevando solo gli oggetti selezionati dalla funzione non_max_suppression:\ndef visualize_detections(image, boxes, classes, scores, \n                         labels, threshold, iou_threshold):\n    if isinstance(image, Image.Image):\n        image_np = np.array(image)\n    else:\n        image_np = image\n\n    height, width = image_np.shape[:2]\n    \n    # Convert normalized coordinates to pixel coordinates\n    boxes_pixel = boxes * np.array([height, width, height, width])\n    \n    # Apply NMS\n    keep = non_max_suppression(boxes_pixel, scores, iou_threshold)\n    \n    # Set the figure size to 12x8 inches\n    fig, ax = plt.subplots(1, figsize=(12, 8))\n\n    ax.imshow(image_np)\n    \n    for i in keep:\n        if scores[i] &gt; threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            rect = patches.Rectangle((xmin * width, ymin * height),\n                                     (xmax - xmin) * width,\n                                     (ymax - ymin) * height,\n                                     linewidth=2, edgecolor='r', facecolor='none')\n            ax.add_patch(rect)\n            class_name = labels[int(classes[i])]\n            ax.text(xmin * width, ymin * height - 10,\n                    f'{class_name}: {scores[i]:.2f}', color='red',\n                    fontsize=12, backgroundcolor='white')\n\n    plt.show()\nOra possiamo creare una funzione che chiamerà le altre, eseguendo l’inferenza su qualsiasi immagine:\ndef detect_objects(img_path, conf=0.5, iou=0.5):\n    orig_img = Image.open(img_path)\n    scale, zero_point = input_details[0]['quantization']\n    img = orig_img.resize((input_details[0]['shape'][1], \n                      input_details[0]['shape'][2]))\n    img_array = np.array(img, dtype=np.float32) / 255.0\n    img_array = (img_array / scale + zero_point).clip(-128, 127).\\\n    astype(np.int8)\n    input_data = np.expand_dims(img_array, axis=0)\n    \n    # Inference on Raspi-Zero\n    start_time = time.time()\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n    end_time = time.time()\n    inference_time = (end_time - start_time) * 1000  # Convert to ms\n    print (\"Inference time: {:.1f}ms\".format(inference_time))\n    \n    # Extract the outputs\n    boxes = interpreter.get_tensor(output_details[1]['index'])[0]  \n    classes = interpreter.get_tensor(output_details[3]['index'])[0]  \n    scores = interpreter.get_tensor(output_details[0]['index'])[0]        \n    num_detections = int(interpreter.get_tensor(output_details[2]['index'])[0])\n\n    visualize_detections(orig_img, boxes, classes, scores, labels, \n                         threshold=conf, \n                         iou_threshold=iou)\nOra, eseguendo il codice, ottenendo di nuovo la stessa immagine con una soglia di confidenza di 0.3, ma con un piccolo IoU:\nimg_path = \"./images/box_2_wheel_2.jpg\"\ndetect_objects(img_path, conf=0.3,iou=0.05)",
    "crumbs": [
      "Raspberry Pi",
      "Rilevamento degli Oggetti"
    ]
  },
  {
    "objectID": "contents/labs/raspi/object_detection/object_detection.it.html#training-di-un-modello-fomo-su-edge-impulse-studio",
    "href": "contents/labs/raspi/object_detection/object_detection.it.html#training-di-un-modello-fomo-su-edge-impulse-studio",
    "title": "Rilevamento degli Oggetti",
    "section": "Training di un modello FOMO su Edge Impulse Studio",
    "text": "Training di un modello FOMO su Edge Impulse Studio\nL’inferenza con il modello SSD MobileNet ha funzionato bene, ma la latenza era significativamente alta. L’inferenza variava da 0.5 a 1.3 secondi su un Raspi-Zero, il che significa circa o meno di 1 FPS (1 frame al secondo). Un’alternativa per accelerare il processo è usare FOMO (Faster Objects, More Objects).\nQuesto nuovo algoritmo di apprendimento automatico consente di contare più oggetti e di trovare la loro posizione in un’immagine in tempo reale utilizzando fino a 30 volte meno potenza di elaborazione e memoria rispetto a MobileNet SSD o YOLO. Il motivo principale per cui ciò è possibile è che mentre altri modelli calcolano le dimensioni dell’oggetto disegnando un quadrato attorno ad esso (bounding box), FOMO ignora le dimensioni dell’immagine, fornendo solo le informazioni sulla posizione dell’oggetto nell’immagine tramite le sue coordinate del centroide.\n\nCome funziona FOMO?\nIn una tipica pipeline di rilevamento degli oggetti, la prima fase consiste nell’estrarre le feature dall’immagine di input. FOMO sfrutta MobileNetV2 per eseguire questa attività. MobileNetV2 elabora l’immagine di input per produrre una feature map che cattura le caratteristiche essenziali, come texture, forme e bordi degli oggetti, in modo computazionalmente efficiente.\n\nUna volta estratte queste feature, l’architettura più semplice di FOMO, focalizzata sul rilevamento del punto centrale, interpreta la feature map per determinare dove si trovano gli oggetti nell’immagine. L’output è una griglia di celle, in cui ogni cella rappresenta se è stato rilevato o meno un centro dell’oggetto. Il modello restituisce uno o più punteggi di confidenza per ogni cella, indicando la probabilità che un oggetto sia presente.\nVediamo come funziona su un’immagine.\nFOMO divide l’immagine in blocchi di pixel usando un fattore di 8. Per l’input di 96x96, la griglia è 12x12 (96/8=12). Per un 160x160, la griglia sarà 20x20 e così via. Successivamente, FOMO eseguirà un classificatore attraverso ogni blocco di pixel per calcolare la probabilità che ci sia un box o una ruota in ognuno di essi e, successivamente, determinerà le regioni che hanno la più alta probabilità di contenere l’oggetto (se un blocco di pixel non ha oggetti, verrà classificato come background). Dalla sovrapposizione della regione finale, FOMO fornisce le coordinate (relative alle dimensioni dell’immagine) del centroide di questa regione.\n\nCompromesso Tra Velocità e Precisione:\n\nRisoluzione della Griglia: FOMO utilizza una griglia di risoluzione fissa, il che significa che ogni cella può rilevare se un oggetto è presente in quella parte dell’immagine. Sebbene non fornisca un’elevata precisione di localizzazione, fa un compromesso essendo veloce e computazionalmente leggero, il che è fondamentale per i dispositivi edge.\nRilevamento Multi-Oggetto: Poiché ogni cella è indipendente, FOMO può rilevare più oggetti contemporaneamente in un’immagine identificando più centri.\n\n\n\nImpulse Design, nuovo Training e Test\nTornare a Edge Impulse Studio e nella scheda Experiments, creare un altro impulso. Ora, le immagini di input dovrebbero essere 160x160 (questa è la dimensione di input prevista per MobilenetV2).\n\nNella scheda Image, generare le feature e spostarsi nella scheda Object detection.\nDovremmo selezionare un modello pre-addestrato per l’addestramento. Usiamo FOMO (Faster Objects, More Objects) MobileNetV2 0.35.\n\nPer quanto riguarda gli iperparametri di training, il modello verrà addestrato con:\n\nEpochs: 30\nBatch size: 32\nLearning Rate: 0.001.\n\nPer la convalida durante l’addestramento, il 20% del set di dati (validation_dataset) verrà risparmiato. Non applicheremo il “Data Augmentation” per il restante 80% (train_dataset) perché il dataset è già stato aumentato durante la fase di etichettatura su Roboflow.\nDi conseguenza, il modello termina con un punteggio F1 complessivo del 93,3% con una latenza impressionante di 8 ms (Raspi-4), circa 60 volte inferiore a quella ottenuta con SSD MovileNetV2.\n\n\nNotare che FOMO ha aggiunto automaticamente una terza etichetta di background ai due box (0) e ruote (1) definite in precedenza.\n\nNella scheda Model testing, possiamo vedere che l’accuratezza era del 94%. Ecco uno dei risultati del campione di test:\n\n\nNelle attività di rilevamento di oggetti, l’accuratezza non è in genere la metrica di valutazione primaria. Il rilevamento di oggetti comporta la classificazione degli oggetti e la definizione di riquadri di delimitazione attorno a essi, il che lo rende un problema più complesso della semplice classificazione. Il problema è che non abbiamo il riquadro di delimitazione, solo i centroidi. In breve, usare l’accuratezza come metrica potrebbe essere fuorviante e potrebbe non fornire una comprensione completa delle prestazioni del modello.\n\n\n\nDistribuzione del modello\nCome abbiamo fatto nella sezione precedente, possiamo distribuire il modello addestrato come TFLite o Linux (AARCH64). Ora facciamolo come Linux (AARCH64), un binario che implementa il protocollo Edge Impulse Linux.\nEdge Impulse per i modelli Linux viene fornito in formato .eim. Questo eseguibile contiene il nostro “impulso completo” creato in Edge Impulse Studio. L’impulso è costituito dai blocchi di elaborazione del segnale e da qualsiasi blocco di apprendimento e anomalia che abbiamo aggiunto e addestrato. È compilato con ottimizzazioni per il nostro processore o GPU (ad esempio, istruzioni NEON su core ARM), più un semplice layer IPC (su un socket Unix).\nNella scheda Deploy, selezionare l’opzione Linux (AARCH64), il modello int8 e premere Build.\n\nIl modello verrà scaricato automaticamente sul computer.\nSul nostro Raspi, creiamo una nuova area di lavoro:\ncd ~\ncd Documents\nmkdir EI_Linux\ncd EI_Linux\nmkdir models\nmkdir images\nRinominare il modello per facilitarne l’identificazione:\nAd esempio, raspi-object-detection-linux-aarch64-FOMO-int8.eim e trasferirlo nella nuova cartella Raspi ./models e catturare o ottenere alcune immagini per l’inferenza e salvarle nella cartella ./images.\n\n\nInferenza e Post-Elaborazione\nL’inferenza verrà effettuata utilizzando Linux Python SDK. Questa libreria consente di eseguire modelli di apprendimento automatico e raccogliere dati dei sensori su macchine Linux utilizzando Python. L’SDK è open source e ospitato su GitHub: edgeimpulse/linux-sdk-python.\nImpostiamo un ambiente virtuale per lavorare con Linux Python SDK\npython3 -m venv ~/eilinux\nsource ~/eilinux/bin/activate\nE installare tutte le librerie necessarie:\nsudo apt-get update\nsudo apt-get install libatlas-base-dev libportaudio0 libportaudio2\nsudo apt-get installlibportaudiocpp0 portaudio19-dev\n\npip3 install edge_impulse_linux -i https://pypi.python.org/simple\npip3 install Pillow matplotlib pyaudio opencv-contrib-python\n\nsudo apt-get install portaudio19-dev\npip3 install pyaudio \npip3 install opencv-contrib-python\nConsentire al modello di essere eseguibile.\nchmod +x raspi-object-detection-linux-aarch64-FOMO-int8.eim\nInstallare Jupiter Notebook nel nuovo ambiente\npip3 install jupyter\nEseguire un notebook in locale (su Raspi-4 o 5 con desktop)\njupyter notebook\no sul browser del computer:\njupyter notebook --ip=192.168.4.210 --no-browser\nAvviamo un nuovo notebook seguendo tutti i passaggi per rilevare cubi e ruote su un’immagine utilizzando il modello FOMO e l’SDK Python di Edge Impulse Linux.\nImportare le librerie necessarie:\nimport sys, time\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nfrom PIL import Image\nimport cv2\nfrom edge_impulse_linux.image import ImageImpulseRunner\nDefinire il percorso del modello e delle etichette:\nmodel_file = \"raspi-object-detection-linux-aarch64-int8.eim\"\nmodel_path = \"models/\"+ model_file # Trained ML model from Edge Impulse\nlabels = ['box', 'wheel']\n\nRicordare che il modello restituirà l’ID della classe come valori (0 e 1), seguendo un ordine alfabetico in base ai nomi delle classi.\n\nCaricare e inizializzare il modello:\n# Load the model file\nrunner = ImageImpulseRunner(model_path)\n\n# Initialize model\nmodel_info = runner.init()\nIl model_info conterrà informazioni critiche sul modello. Tuttavia, a differenza dell’interprete TFLite, la libreria EI Linux Python SDK preparerà ora il modello per l’inferenza.\nQuindi, apriamo l’immagine e mostriamola (ora, per compatibilità, useremo OpenCV, la libreria CV utilizzata internamente da EI. OpenCV legge l’immagine come BGR, quindi dovremo convertirla in RGB:\n# Load the image\nimg_path = \"./images/1_box_1_wheel.jpg\"\norig_img = cv2.imread(img_path)\nimg_rgb = cv2.cvtColor(orig_img, cv2.COLOR_BGR2RGB)\n\n# Display the image\nplt.imshow(img_rgb)\nplt.title(\"Original Image\")\nplt.show()\n\nOra otterremo le feature e l’immagine preelaborata (ritagliata) utilizzando runner:\nfeatures, cropped = runner.get_features_from_image_auto_studio_setings(img_rgb)\nEd eseguiamo l’inferenza. Calcoliamo anche la latenza del modello:\nres = runner.classify(features)\nOtteniamo le classi di output degli oggetti rilevati, i loro centroidi dei bounding box e le probabilità.\nprint('Found %d bounding boxes (%d ms.)' % (\n  len(res[\"result\"][\"bounding_boxes\"]), \n  res['timing']['dsp'] + res['timing']['classification']))\nfor bb in res[\"result\"][\"bounding_boxes\"]:\n    print('\\t%s (%.2f): x=%d y=%d w=%d h=%d' % (\n      bb['label'], bb['value'], bb['x'], \n      bb['y'], bb['width'], bb['height']))\nFound 2 bounding boxes (29 ms.)\n    1 (0.91): x=112 y=40 w=16 h=16\n    0 (0.75): x=48 y=56 w=8 h=8\nI risultati mostrano che sono stati rilevati due oggetti: uno con ID classe 0 (box) e uno con ID classe 1 (wheel), il che è corretto!\nVisualizziamo il risultato (Il threshold [soglia] è 0.5, il valore predefinito impostato durante il test del modello su Edge Impulse Studio).\nprint('\\tFound %d bounding boxes (latency: %d ms)' % (\n  len(res[\"result\"][\"bounding_boxes\"]), \n  res['timing']['dsp'] + res['timing']['classification']))\nplt.figure(figsize=(5,5))\nplt.imshow(cropped)\n\n# Go through each of the returned bounding boxes\nbboxes = res['result']['bounding_boxes']\nfor bbox in bboxes:\n\n    # Get the corners of the bounding box\n    left = bbox['x']\n    top = bbox['y']\n    width = bbox['width']\n    height = bbox['height']\n    \n    # Draw a circle centered on the detection\n    circ = plt.Circle((left+width//2, top+height//2), 5, \n                     fill=False, color='red', linewidth=3)\n    plt.gca().add_patch(circ)\n    class_id = int(bbox['label'])\n    class_name = labels[class_id]\n    plt.text(left, top-10, f'{class_name}: {bbox[\"value\"]:.2f}', \n              color='red', fontsize=12, backgroundcolor='white')\nplt.show()",
    "crumbs": [
      "Raspberry Pi",
      "Rilevamento degli Oggetti"
    ]
  },
  {
    "objectID": "contents/labs/raspi/object_detection/object_detection.it.html#esplorazione-di-un-modello-yolo-tramite-ultralitics",
    "href": "contents/labs/raspi/object_detection/object_detection.it.html#esplorazione-di-un-modello-yolo-tramite-ultralitics",
    "title": "Rilevamento degli Oggetti",
    "section": "Esplorazione di un Modello YOLO tramite Ultralitics",
    "text": "Esplorazione di un Modello YOLO tramite Ultralitics\nPer questo laboratorio, esploreremo YOLOv8. Ultralytics YOLOv8 è una versione dell’acclamato modello di rilevamento di oggetti in tempo reale e segmentazione delle immagini, YOLO. YOLOv8 è basato su progressi all’avanguardia nel deep learning e nella visione artificiale, offrendo prestazioni senza pari in termini di velocità e precisione. Il suo design semplificato lo rende adatto a varie applicazioni e facilmente adattabile a diverse piattaforme hardware, dai dispositivi edge alle API cloud.\n\nA proposito del modello YOLO\nIl modello YOLO (You Only Look Once) è un algoritmo di rilevamento di oggetti altamente efficiente e ampiamente utilizzato, noto per le sue capacità di elaborazione in tempo reale. A differenza dei tradizionali sistemi di rilevamento di oggetti che riutilizzano classificatori o localizzatori per eseguire il rilevamento, YOLO inquadra il problema del rilevamento come un singolo compito di regressione. Questo approccio innovativo consente a YOLO di prevedere simultaneamente più bounding box e le relative probabilità di classe da immagini complete in un’unica valutazione, aumentando notevolmente la sua velocità.\n\nCaratteristiche Principali:\n\nArchitettura “ingle Network”:\n\nYOLO impiega una singola rete neurale per elaborare l’intera immagine. Questa rete divide l’immagine in una griglia e, per ogni cella della griglia, prevede direttamente i bounding box e le relative probabilità di classe. Questa formazione end-to-end migliora la velocità e semplifica l’architettura del modello.\n\nElaborazione in Tempo Reale:\n\nUna delle caratteristiche distintive di YOLO è la sua capacità di eseguire il rilevamento di oggetti in tempo reale. A seconda della versione e dell’hardware, YOLO può elaborare immagini con frame al secondo (FPS) elevati. Ciò lo rende ideale per applicazioni che richiedono un rilevamento di oggetti rapido e accurato, come videosorveglianza, guida autonoma e analisi di eventi sportivi in diretta.\n\nEvoluzione delle Versioni:\n\nNel corso degli anni, YOLO ha subito miglioramenti significativi, da YOLOv1 all’ultimo YOLOv10. Ogni iterazione ha introdotto miglioramenti in termini di accuratezza, velocità ed efficienza. YOLOv8, ad esempio, incorpora progressi nell’architettura di rete, metodologie di training migliorate e un supporto migliore per vari hardware, garantendo prestazioni più robuste.\nSebbene YOLOv10 sia il membro più recente della famiglia con prestazioni incoraggianti in base alla sua documentazione, è stato appena rilasciato (maggio 2024) e non è completamente integrato con la libreria Ultralitycs. Al contrario, l’analisi della curva di precisione-richiamo suggerisce che YOLOv8 generalmente supera YOLOv9, catturando una percentuale maggiore di veri positivi e riducendo al minimo i falsi positivi in modo più efficace (per maggiori dettagli, vedere questo articolo). Quindi, questo laboratorio si basa su YOLOv8n.\n\n\nPrecisione ed Efficienza:\n\nMentre le prime versioni di YOLO barattavano un po’ di precisione per la velocità, le versioni recenti hanno fatto notevoli progressi nell’equilibrare entrambi. I modelli più recenti sono più veloci e precisi, rilevano piccoli oggetti (come le api) e funzionano bene su set di dati complessi.\n\nAmpia Gamma di Applicazioni:\n\nLa versatilità di YOLO ha portato alla sua adozione in numerosi campi. Viene utilizzato nei sistemi di monitoraggio del traffico per rilevare e contare i veicoli, nelle applicazioni di sicurezza per identificare potenziali minacce e nella tecnologia agricola per monitorare raccolti e bestiame. La sua applicazione si estende a qualsiasi dominio che richieda un rilevamento efficiente e accurato degli oggetti.\n\nComunità e Sviluppo:\n\nYOLO continua a evolversi ed è supportato da una solida comunità di sviluppatori e ricercatori (essendo YOLOv8 molto forte). Le implementazioni open source e l’ampia documentazione lo hanno reso accessibile per la personalizzazione e l’integrazione in vari progetti. Framework di deep learning popolari come Darknet, TensorFlow e PyTorch supportano YOLO, ampliandone ulteriormente l’applicabilità.\nUltralytics YOLOv8 non solo può Detect [rilevare] (il nostro caso qui) ma anche Segmentare e mettere in posa con Pose modelli pre-addestrati sul set di dati COCO e YOLOv8 Classifica modelli pre-addestrati sul set di dati ImageNet. La modalità Track è disponibile per tutti i modelli Detect, Segment e Pose.\n\n\n\n\nAttività supportate da Ultralytics YOLO\n\n\n\n\n\n\nInstallazione\nSul nostro Raspi, disattiviamo l’ambiente corrente per creare una nuova area di lavoro:\ndeactivate\ncd ~\ncd Documents/\nmkdir YOLO\ncd YOLO\nmkdir models\nmkdir images\nImpostiamo un Virtual Environment per lavorare con Ultralytics YOLOv8\npython3 -m venv ~/yolo\nsource ~/yolo/bin/activate\nE installiamo i pacchetti Ultralytics per l’inferenza locale sul Raspi\n\nAggiorniamo l’elenco dei pacchetti, installiamo pip ed eseguiamo l’aggiornamento all’ultima versione:\n\nsudo apt update\nsudo apt install python3-pip -y\npip install -U pip\n\nInstalliamo il pacchetto pip ultralytics con le dipendenze opzionali:\n\npip install ultralytics[export]\n\nRiavviamo il dispositivo:\n\nsudo reboot\n\n\nTest di YOLO\nDopo l’avvio di Raspi-Zero, attiviamo l’ambiente yolo, andiamo alla directory di lavoro,\nsource ~/yolo/bin/activate\ncd /Documents/YOLO\ned eseguiamo l’inferenza su un’immagine che verrà scaricata dal sito web di Ultralytics, che utilizza il modello YOLOV8n (il più piccolo della famiglia) nel Terminal (CLI):\nyolo predict model='yolov8n' source='https://ultralytics.com/images/bus.jpg'\n\nLa famiglia di modelli YOLO è pre-addestrata con il dataset COCO.\n\nIl risultato dell’inferenza apparirà nel terminale. Nell’immagine (bus.jpg), sono state rilevate 4 persone, 1 autobus e 1 segnale di stop:\n\nInoltre, abbiamo ricevuto un messaggio che indica Results saved to runs/detect/predict. Ispezionando quella directory, possiamo vedere una nuova immagine salvata (bus.jpg). Scarichiamola dal Raspi-Zero sul desktop per l’ispezione:\n\nQuindi, Ultrayitics YOLO è installato correttamente sul nostro Raspi. Ma, su Raspi-Zero, un problema è l’elevata latenza per questa inferenza, circa 18 secondi, anche con il modello più in miniatura della famiglia (YOLOv8n).\n\n\nEsportazione del Modello in formato NCNN\nL’implementazione di modelli di visione artificiale su dispositivi edge con potenza di calcolo limitata, come Raspi-Zero, può causare problemi di latenza. Un’alternativa è quella di utilizzare un formato ottimizzato per prestazioni ottimali. Ciò garantisce che anche i dispositivi con potenza di elaborazione limitata possano gestire bene attività di visione artificiale avanzate.\nDi tutti i formati di esportazione del modello supportati da Ultralytics, NCNN è un framework di elaborazione inferenziale di reti neurali ad alte prestazioni ottimizzato per piattaforme mobili. Fin dall’inizio della progettazione, NCNN è stato profondamente attento all’implementazione e all’uso su telefoni cellulari e non aveva dipendenze di terze parti. È multipiattaforma e funziona più velocemente di tutti i framework open source noti (come TFLite).\nNCNN offre le migliori prestazioni di inferenza quando si lavora con dispositivi Raspberry Pi. NCNN è altamente ottimizzato per piattaforme mobili embedded (come l’architettura ARM).\nQuindi, convertiamo il nostro modello e rieseguiamo l’inferenza:\n\nEsportiamo un modello PyTorch YOLOv8n in formato NCNN, creando: ‘/yolov8n_ncnn_model’\n\nyolo export model=yolov8n.pt format=ncnn\n\nEseguiamo l’inferenza con il modello esportato (ora la sorgente potrebbe essere l’immagine bus.jpg scaricata dal sito Web nella directory corrente nell’ultima inferenza):\n\nyolo predict model='./yolov8n_ncnn_model' source='bus.jpg'\n\nLa prima inferenza, quando il modello viene caricato, di solito ha una latenza elevata (circa 17 s), ma dalla seconda, è possibile notare che l’inferenza scende a circa 2 s.\n\n\n\nEsplorare YOLO con Python\nPer iniziare, chiamiamo l’interprete Python in modo da poter esplorare il funzionamento del modello YOLO, riga per riga:\npython3\nOra, dovremmo chiamare la libreria YOLO da Ultralitics e caricare il modello:\nfrom ultralytics import YOLO\nmodel = YOLO('yolov8n_ncnn_model')\nQuindi, eseguire l’inferenza su un’immagine (usiamo di nuovo bus.jpg):\nimg = 'bus.jpg'\nresult = model.predict(img, save=True, imgsz=640, conf=0.5, iou=0.3)\n\nPossiamo verificare che il risultato è quasi identico a quello che otteniamo eseguendo l’inferenza a livello di terminale (CLI), tranne per il fatto che la fermata dell’autobus non è stata rilevata con il modello NCNN ridotto. Notare che la latenza è stata ridotta.\nAnalizziamo il contenuto di “result”.\nAd esempio, possiamo vedere result[0].boxes.data, che mostra il risultato principale dell’inferenza, che è un profilo tensoriale (4, 6). Ogni riga è uno degli oggetti rilevati, ovvero le prime 4 colonne, le coordinate delle bounding box, la quinta, la confidenza e la sesta, la classe (in questo caso, 0: person e 5: bus):\n\nPossiamo accedere a diversi risultati di inferenza separatamente, come il tempo di inferenza, e stamparli in un formato migliore:\ninference_time = int(result[0].speed['inference'])\nprint(f\"Inference Time: {inference_time} ms\")\nOppure possiamo avere il numero totale di oggetti rilevati:\nprint(f'Number of objects: {len (result[0].boxes.cls)}')\n\nCon Python, possiamo creare un output dettagliato che soddisfi le nostre esigenze (vedere Model Prediction with Ultralytics YOLO per maggiori dettagli). Eseguiamo uno script Python invece di inserirlo manualmente riga per riga nell’interprete, come mostrato di seguito. Usiamo nano come editor di testo. Per prima cosa, dovremmo creare uno script Python vuoto denominato, ad esempio, yolov8_tests.py:\nnano yolov8_tests.py\nSi inseriscono le righe di codice:\nfrom ultralytics import YOLO\n\n# Load the YOLOv8 model\nmodel = YOLO('yolov8n_ncnn_model')\n\n# Run inference\nimg = 'bus.jpg'\nresult = model.predict(img, save=False, imgsz=640, conf=0.5, iou=0.3)\n\n# print the results\ninference_time = int(result[0].speed['inference'])\nprint(f\"Inference Time: {inference_time} ms\")\nprint(f'Number of objects: {len (result[0].boxes.cls)}')\n\nE si inseriscono i comandi: [CTRL+O] + [INVIO] +[CTRL+X] per salvare lo script Python.\nEseguire lo script:\npython yolov8_tests.py\nIl risultato è lo stesso dell’esecuzione dell’inferenza a livello di terminale (CLI) e con l’interprete Python nativo.\n\nChiamare la libreria YOLO e caricare il modello per l’inferenza per la prima volta richiede molto tempo, ma le inferenze successive saranno molto più veloci. Ad esempio, la prima singola inferenza può richiedere diversi secondi, ma in seguito il tempo di inferenza dovrebbe essere ridotto a meno di 1 secondo.\n\n\n\nAddestramento di YOLOv8 su un Dataset Personalizzato\nTorniamo al nostro set di dati “Boxe versus Wheel”, etichettato su Roboflow. Nell’opzione Download Dataset, invece dell’opzione Download a zip to computer eseguita per l’addestramento su Edge Impulse Studio, opteremo per Show download code. Questa opzione aprirà una finestra pop-up con un frammento di codice che dovrebbe essere incollato nel nostro notebook di training.\n\nPer l’addestramento, adattiamo uno degli esempi pubblici disponibili da Ultralitytics ed eseguiamolo su Google Colab. Di seguito, si può trovare il mio da adattare al progetto:\n\nYOLOv8 Box versus Wheel Dataset Training [Open In Colab]\n\n\nPunti critici sul Notebook:\n\nEseguirlo con GPU (NVidia T4 è gratuito)\nInstallare Ultralytics tramite PIP.\n\nOra, si può importare YOLO e caricare il dataset sul CoLab, incollando il codice di download che riceviamo da Roboflow. Notare che il dataset verrà montato in /content/datasets/:\n\n\n\nÈ essenziale verificare e modificare il file data.yaml con il path corretto per le immagini (copiare il percorso su ogni cartella images).\n\nnames:\n- box\n- wheel\nnc: 2\nroboflow:\n  license: CC BY 4.0\n  project: box-versus-wheel-auto-dataset\n  url: https://universe.roboflow.com/marcelo-rovai-riila/box-versus-wheel-auto-dataset/dataset/5\n  version: 5\n  workspace: marcelo-rovai-riila\ntest: /content/datasets/Box-versus-Wheel-auto-dataset-5/test/images\ntrain: /content/datasets/Box-versus-Wheel-auto-dataset-5/train/images\nval: /content/datasets/Box-versus-Wheel-auto-dataset-5/valid/images\n\nDefinire i principali iperparametri da modificare rispetto ai valori di default, ad esempio:\nMODEL = 'yolov8n.pt'\nIMG_SIZE = 640\nEPOCHS = 25 # For a final project, you should consider at least 100 epochs\nEseguire il training (utilizzando la CLI):\n!yolo task=detect mode=train model={MODEL} data={dataset.location}/data.yaml epochs={EPOCHS} imgsz={IMG_SIZE} plots=True\n\n\n\nimage-20240910111319804\n\n\nL’addestramento del modello ha richiesto alcuni minuti e ha prodotto risultati eccellenti (mAP50 pari a 0,995). Al termine del training, tutti i risultati vengono salvati nella cartella elencata, ad esempio: /runs/detect/train/. Lì si trova, ad esempio, la matrice di confusione.\n\n\n\nNotare che il modello addestrato (best.pt) viene salvato nella cartella /runs/detect/train/weights/. Ora, si deve convalidare il modello addestrato con valid/images.\n\n!yolo task=detect mode=val model={HOME}/runs/detect/train/weights/best.pt data={dataset.location}/data.yaml\nI risultati erano simili al training.\n\nOra, si deve eseguire l’inferenza sulle immagini lasciate a parte per il test\n\n!yolo task=detect mode=predict model={HOME}/runs/detect/train/weights/best.pt conf=0.25 source={dataset.location}/test/images save=True\nI risultati dell’inferenza vengono salvati nella cartella runs/detect/predict. Vediamone alcuni:\n\n\nSi consiglia di esportare i risultati di train, validation e test per un Drive su Google. Per farlo, si deve montare l’unità.\nfrom google.colab import drive\ndrive.mount('/content/gdrive')\ne copiare il contenuto della cartella /runs in una cartella che si deve creare nel Drive, ad esempio:\n!scp -r /content/runs '/content/gdrive/MyDrive/10_UNIFEI/Box_vs_Wheel_Project'\n\n\n\n\nInferenza col modello addestrato, usando Raspi\nScaricare il modello addestrato /runs/detect/train/weights/best.pt nel computer. Utilizzando l’FTP FileZilla, trasferiamo best.pt nella cartella dei modelli Raspi (prima del trasferimento, si può cambiare il nome del modello, ad esempio, box_wheel_320_yolo.pt).\nCon FileZilla FTP, trasferiamo alcune immagini dal set di dati di prova a .\\YOLO\\images:\nTorniamo alla cartella YOLO e utilizziamo l’interprete Python:\ncd ..\npython\nCome prima, importeremo la libreria YOLO e definiremo il nostro modello convertito per rilevare le api:\nfrom ultralytics import YOLO\nmodel = YOLO('./models/box_wheel_320_yolo.pt')\nOra, definiamo un’immagine e chiamiamo l’inferenza (stavolta salveremo il risultato dell’immagine per la verifica esterna):\nimg = './images/1_box_1_wheel.jpg'\nresult = model.predict(img, save=True, imgsz=320, conf=0.5, iou=0.3)\nRipetiamo per diverse immagini. Il risultato dell’inferenza viene salvato sulla variabile result e l’immagine elaborata su runs/detect/predict8\n\nCon FileZilla FTP, possiamo inviare il risultato dell’inferenza al Desktop per la verifica:\n\nPossiamo vedere che il risultato dell’inferenza è eccellente! Il modello è stato addestrato in base al modello base più piccolo della famiglia YOLOv8 (YOLOv8n). Il problema è la latenza, circa 1 secondo (o 1 FPS su Raspi-Zero). Naturalmente, possiamo ridurre questa latenza e convertire il modello in TFLite o NCNN.",
    "crumbs": [
      "Raspberry Pi",
      "Rilevamento degli Oggetti"
    ]
  },
  {
    "objectID": "contents/labs/raspi/object_detection/object_detection.it.html#rilevamento-di-oggetti-su-un-live-streaming",
    "href": "contents/labs/raspi/object_detection/object_detection.it.html#rilevamento-di-oggetti-su-un-live-streaming",
    "title": "Rilevamento degli Oggetti",
    "section": "Rilevamento di Oggetti su un Live Streaming",
    "text": "Rilevamento di Oggetti su un Live Streaming\nTutti i modelli esplorati in questo laboratorio possono rilevare oggetti in tempo reale utilizzando una telecamera. L’immagine catturata dovrebbe essere l’input per il modello addestrato e convertito. Per Raspi-4 o 5 con un desktop, OpenCV può catturare i frame e visualizzare il risultato dell’inferenza.\nTuttavia, è anche possibile creare un live streaming con una webcam per rilevare oggetti in tempo reale. Ad esempio, iniziamo con lo script sviluppato per l’app Image Classification e adattiamolo per un’Applicazione Web di Rilevamento di Oggetti in Tempo Reale Utilizzando TensorFlow Lite e Flask.\nQuesta versione dell’app funzionerà per tutti i modelli TFLite. Verificare che il modello sia nella cartella corretta, ad esempio:\nmodel_path = \"./models/ssd-mobilenet-v1-tflite-default-v1.tflite\"\nScaricare lo script Python object_detection_app.py da GitHub.\nE sul terminale, si esegue:\npython3 object_detection_app.py\nE accedere all’interfaccia web:\n\nSul Raspberry Pi stesso (se si ha una GUI): si apre un browser web e si va suhttp://localhost:5000\nDa un altro dispositivo sulla stessa rete: aprire un browser web e andare su http://&lt;raspberry_pi_ip&gt;:5000 (Sostituire &lt;raspberry_pi_ip&gt; con l’indirizzo IP del Raspberry Pi). Per esempio: http://192.168.4.210:5000/\n\nEcco alcuni screenshot dell’app in esecuzione su un desktop esterno\n\nDiamo un’occhiata a una descrizione tecnica dei moduli chiave utilizzati nell’applicazione di rilevamento degli oggetti:\n\nTensorFlow Lite (tflite_runtime):\n\nScopo: Inferenza efficiente di modelli di machine learning su dispositivi edge.\nPerché: TFLite offre dimensioni ridotte del modello e prestazioni ottimizzate rispetto a TensorFlow completo, il che è fondamentale per dispositivi con risorse limitate come Raspberry Pi. Supporta l’accelerazione hardware e la quantizzazione, migliorando ulteriormente l’efficienza.\nFunzioni chiave: Interpreter per caricare ed eseguire il modello, get_input_details() e get_output_details() per l’interfaccia col modello.\n\nFlask:\n\nScopo: Framework web leggero per la creazione del server backend.\nPerché: La semplicità e la flessibilità di Flask lo rendono ideale per lo sviluppo e la distribuzione rapidi di applicazioni web. Richiede meno risorse rispetto a framework più grandi, adatto a dispositivi edge.\nComponenti chiave: decoratori di route per definire endpoint API, oggetti Response per lo streaming video, render_template_string per servire HTML dinamico.\n\nPicamera2:\n\nScopo: Interfaccia con il modulo fotocamera Raspberry Pi.\nPerché: Picamera2 è la libreria più recente per il controllo delle fotocamere Raspberry Pi, che offre prestazioni e funzionalità migliorate rispetto alla libreria Picamera originale.\nFunzioni chiave: create_preview_configuration() per impostare la fotocamera, capture_file() per catturare i fotogrammi.\n\nPIL (Python Imaging Library):\n\nScopo: Elaborazione e manipolazione delle immagini.\nPerché: PIL fornisce un’ampia gamma di funzionalità di elaborazione delle immagini. Viene utilizzato qui per ridimensionare le immagini, disegnare riquadri di delimitazione e convertire tra formati di immagine.\nClassi chiave: Image per caricare e manipolare immagini, ImageDraw per disegnare forme e testo sulle immagini.\n\nNumPy:\n\nScopo: Operazioni array efficienti e calcolo numerico.\nPerché: le operazioni su array di NumPy sono molto più veloci delle liste Python pure, il che è fondamentale per elaborare in modo efficiente i dati delle immagini e gli input/output del modello.\nFunzioni chiave: array() per creare array, expand_dims() per aggiungere dimensioni agli array.\n\nThreading:\n\nScopo: Esecuzione simultanea di attività.\nPerché: Il threading consente l’acquisizione simultanea di frame, il rilevamento di oggetti e il funzionamento del server Web, cruciali per il mantenimento delle prestazioni in tempo reale.\nComponenti chiave: La classe Thread crea thread di esecuzione separati e Lock viene utilizzato per la sincronizzazione dei thread.\n\nio.BytesIO:\n\nScopo: Stream binari in memoria.\nPerché: Consente una gestione efficiente dei dati delle immagini in memoria senza bisogno di file temporanei, migliorando la velocità e riducendo le operazioni di I/O.\n\ntime:\n\nScopo: Funzioni correlate al tempo.\nPerché: Utilizzato per aggiungere ritardi (time.sleep()) per controllare la frequenza dei fotogrammi e per le misure delle prestazioni.\n\njQuery (client-side):\n\nScopo: Manipolazione DOM semplificata e richieste AJAX.\nPerché: Semplifica l’aggiornamento dinamico dell’interfaccia web e la comunicazione con il server senza ricaricamenti di pagina.\nFunzioni chiave: .get() e .post() per richieste AJAX, metodi di manipolazione DOM per l’aggiornamento dell’interfaccia utente.\n\n\nPer quanto riguarda l’architettura del sistema dell’app principale:\n\nMain Thread: Esegue il server Flask, gestisce le richieste HTTP e serve l’interfaccia web.\nCamera Thread: Cattura continuamente i frame dalla fotocamera.\nDetection Thread: Elabora i frame tramite il modello TFLite per il rilevamento degli oggetti.\nFrame Buffer: Spazio di memoria condiviso (protetto da blocchi) che memorizza i frame più recenti e i risultati del rilevamento.\n\nE il flusso di dati dell’app, possiamo descriverlo in breve:\n\nLa fotocamera cattura il frame → Frame Buffer\nIl thread di rilevamento legge dal Frame Buffer → Elabora tramite il modello TFLite → Aggiorna i risultati del rilevamento nel Frame Buffer\nFlask indirizza l’accesso al Frame Buffer per fornire i risultati più recenti del frame e del rilevamento\nIl client Web riceve gli aggiornamenti tramite AJAX e aggiorna l’interfaccia utente\n\nQuesta architettura consente un rilevamento efficiente degli oggetti in tempo reale, mantenendo al contempo un’interfaccia Web reattiva in esecuzione su un dispositivo edge con risorse limitate come un Raspberry Pi. Il threading e le librerie efficienti come TFLite e PIL consentono al sistema di elaborare i frame video in tempo reale, mentre Flask e jQuery forniscono un modo intuitivo per interagire con essi.\nSi può testare l’app con un altro modello pre-elaborato, come EfficientDet, modificando la riga dell’app:\nmodel_path = \"./models/lite-model_efficientdet_lite0_detection_metadata_1.tflite\"\n\nPer usare l’app per il modello SSD-MobileNetV2, addestrato su Edge Impulse Studio con il set di dati “Box versus Wheel”, il codice dovrebbe essere adattato anche in base ai dettagli di input, come abbiamo esplorato sul suo notebook.",
    "crumbs": [
      "Raspberry Pi",
      "Rilevamento degli Oggetti"
    ]
  },
  {
    "objectID": "contents/labs/raspi/object_detection/object_detection.it.html#conclusione",
    "href": "contents/labs/raspi/object_detection/object_detection.it.html#conclusione",
    "title": "Rilevamento degli Oggetti",
    "section": "Conclusione",
    "text": "Conclusione\nQuesto laboratorio ha esplorato l’implementazione del rilevamento di oggetti su dispositivi edge come Raspberry Pi, dimostrando la potenza e il potenziale dell’esecuzione di attività avanzate di computer vision su hardware con risorse limitate. Abbiamo trattato diversi aspetti essenziali:\n\nConfronto tra Modelli: Abbiamo esaminato diversi modelli di rilevamento oggetti, tra cui SSD-MobileNet, EfficientDet, FOMO e YOLO, confrontandone le prestazioni e i compromessi sui dispositivi edge.\nTraining e Deployment: Utilizzando un set di dati personalizzato di scatole e ruote (etichettato su Roboflow), abbiamo esaminato il processo di addestramento dei modelli utilizzando Edge Impulse Studio e Ultralytics e la loro distribuzione su Raspberry Pi.\nTecniche di Ottimizzazione: Per migliorare la velocità di inferenza sui dispositivi edge, abbiamo esplorato vari metodi di ottimizzazione, come la quantizzazione del modello (TFLite int8) e la conversione del formato (ad esempio, in NCNN).\nApplicazioni in Tempo Reale: Il laboratorio ha esemplificato un’applicazione Web di rilevamento oggetti in tempo reale, dimostrando come questi modelli possono essere integrati in sistemi pratici e interattivi.\nConsiderazioni sulle Prestazioni: Durante il laboratorio abbiamo discusso l’equilibrio tra accuratezza del modello e velocità di inferenza, un aspetto fondamentale per le applicazioni di IA edge.\n\nLa capacità di eseguire il rilevamento di oggetti su dispositivi edge apre numerose possibilità in vari ambiti, dall’agricoltura di precisione, all’automazione industriale e al controllo di qualità, alle applicazioni per la casa intelligente e al monitoraggio ambientale. Elaborando i dati localmente, questi sistemi possono offrire latenza ridotta, privacy migliorata e funzionamento in ambienti con connettività limitata.\nGuardando al futuro, le potenziali aree di ulteriore esplorazione includono: - Implementazione di pipeline multi-modello per attività più complesse - Esplorazione di opzioni di accelerazione hardware per Raspberry Pi - Integrazione del rilevamento di oggetti con altri sensori per sistemi AI edge più completi - Sviluppo di soluzioni edge-to-cloud che sfruttano sia l’elaborazione locale che le risorse cloud\nIl rilevamento di oggetti su dispositivi edge può creare sistemi intelligenti e reattivi che portano la potenza dell’IA direttamente nel mondo fisico, aprendo nuove frontiere nel modo in cui interagiamo e comprendiamo il nostro ambiente.",
    "crumbs": [
      "Raspberry Pi",
      "Rilevamento degli Oggetti"
    ]
  },
  {
    "objectID": "contents/labs/raspi/object_detection/object_detection.it.html#risorse",
    "href": "contents/labs/raspi/object_detection/object_detection.it.html#risorse",
    "title": "Rilevamento degli Oggetti",
    "section": "Risorse",
    "text": "Risorse\n\nDataset (“Box versus Wheel”)\nSSD-MobileNet Notebook on a Raspi\nEfficientDet Notebook on a Raspi\nFOMO - EI Linux Notebook on a Raspi\nYOLOv8 Box versus Wheel Dataset Training on CoLab\nEdge Impulse Project - SSD MobileNet and FOMO\nScript Python\nModelli",
    "crumbs": [
      "Raspberry Pi",
      "Rilevamento degli Oggetti"
    ]
  },
  {
    "objectID": "contents/labs/raspi/llm/llm.it.html",
    "href": "contents/labs/raspi/llm/llm.it.html",
    "title": "Small Language Models (SLM)",
    "section": "",
    "text": "Panoramica\nNell’area in rapida crescita dell’intelligenza artificiale, l’edge computing offre l’opportunità di decentralizzare le capacità tradizionalmente riservate a server potenti e centralizzati. Questo laboratorio esplora l’integrazione pratica di piccole versioni di modelli linguistici tradizionali di grandi dimensioni (LLM) in un Raspberry Pi 5, trasformando questo dispositivo edge in un hub di IA in grado di elaborare dati in tempo reale e in loco.\nMan mano che i modelli linguistici di grandi dimensioni crescono in dimensioni e complessità, gli Small Language Model (SLM) offrono un’alternativa interessante per i dispositivi edge, raggiungendo un equilibrio tra prestazioni ed efficienza delle risorse. Eseguendo questi modelli direttamente su Raspberry Pi, possiamo creare applicazioni reattive e rispettose della privacy che funzionano anche in ambienti con connettività Internet limitata o assente.\nQuesto laboratorio guiderà attraverso la configurazione, l’ottimizzazione e lo sfruttamento degli SLM su Raspberry Pi. Esploreremo l’installazione e l’utilizzo di Ollama. Questo framework open source ci consente di eseguire LLM localmente sulle nostre macchine (i nostri desktop o dispositivi edge come Raspberry Pi o NVidia Jetson). Ollama è progettato per essere efficiente, scalabile e facile da usare, il che lo rende una buona opzione per distribuire modelli di IA come Microsoft Phi, Google Gemma, Meta Llama e LLaVa (Multimodal). Integreremo alcuni di questi modelli in progetti utilizzando l’ecosistema Python, esplorandone il potenziale in scenari del mondo reale (o almeno indicheremo questa direzione).",
    "crumbs": [
      "Raspberry Pi",
      "Small Language Models (SLM)"
    ]
  },
  {
    "objectID": "contents/labs/raspi/llm/llm.it.html#setup",
    "href": "contents/labs/raspi/llm/llm.it.html#setup",
    "title": "Small Language Models (SLM)",
    "section": "Setup",
    "text": "Setup\nNei laboratori precedenti avremmo potuto usare qualsiasi modello Raspi, ma qui la scelta deve ricadere sul Raspberry Pi 5 (Raspi-5). È una piattaforma robusta che aggiorna sostanzialmente l’ultima versione 4, dotata di Broadcom BCM2712, una CPU Arm Cortex-A76 quad-core a 64 bit da 2,4 GHz con Cryptographic Extension e capacità di caching migliorate. Vanta una GPU VideoCore VII, due uscite HDMI® 4Kp60 con HDR e un decoder HEVC 4Kp60. Le opzioni di memoria includono 4 GB e 8 GB di SDRAM LPDDR4X ad alta velocità, con 8 GB come nostra scelta per eseguire SLM. Presenta inoltre un’archiviazione espandibile tramite uno slot per schede microSD e un’interfaccia PCIe 2.0 per periferiche veloci come gli SSD M.2 (Solid State Drive).\n\nPer applicazioni SSL reali, gli SSD sono un’opzione migliore delle schede SD.\n\nA proposito, come ha spiegato Alasdair Allan, l’inferenza diretta sulla CPU Raspberry Pi 5, senza accelerazione GPU, è ora alla pari con le prestazioni del Coral TPU.\n\nPer maggiori informazioni, consultare l’articolo completo: Benchmarking TensorFlow e TensorFlow Lite su Raspberry Pi 5.\n\nRaspberry Pi Active Cooler\nSuggeriamo di installare un Active Cooler, una soluzione di raffreddamento clip-on dedicata per Raspberry Pi 5 (Raspi-5), per questo laboratorio. Combina un dissipatore di calore in alluminio con una ventola di raffreddamento a temperatura controllata per mantenere il Raspi-5 in funzione comodamente sotto carichi pesanti, come l’esecuzione di SLM.\n\nL’Active Cooler ha dei pad termici pre-applicati per il trasferimento del calore ed è montato direttamente sulla scheda Raspberry Pi 5 tramite perni a molla. Il firmware Raspberry Pi lo gestisce attivamente: a 60 °C, la ventola del soffiatore viene accesa; a 67,5 °C, la velocità della ventola verrà aumentata; e infine, a 75 °C, la ventola aumenterà fino alla massima velocità. La ventola del soffiatore rallenterà automaticamente quando la temperatura scenderà al di sotto di questi limiti.\n\n\nPer evitare il surriscaldamento, tutte le schede Raspberry Pi iniziano a limitare il processore quando la temperatura raggiunge gli 80 °C e a limitarlo ulteriormente quando raggiunge la temperatura massima di 85 °C (maggiori dettagli qui).",
    "crumbs": [
      "Raspberry Pi",
      "Small Language Models (SLM)"
    ]
  },
  {
    "objectID": "contents/labs/raspi/llm/llm.it.html#generative-ai-genai",
    "href": "contents/labs/raspi/llm/llm.it.html#generative-ai-genai",
    "title": "Small Language Models (SLM)",
    "section": "Generative AI (GenAI)",
    "text": "Generative AI (GenAI)\nGenerative AI è un sistema di IA in grado di creare nuovi contenuti originali su vari supporti come testo, immagini, audio e video. Questi sistemi apprendono modelli da dati esistenti e utilizzano tale conoscenza per generare nuovi output che in precedenza non esistevano. Large Language Models (LLM), Small Language Models (SLM) e modelli multimodali possono essere tutti considerati tipi di GenAI quando utilizzati per attività generative.\nGenAI fornisce il framework concettuale per la creazione di contenuti basati sull’intelligenza artificiale, con gli LLM che fungono da potenti generatori di testo per uso generale. Gli SLM adattano questa tecnologia all’edge computing, mentre i modelli multimodali estendono le capacità di GenAI a diversi tipi di dati. Insieme, rappresentano uno spettro di tecnologie di intelligenza artificiale generativa, ciascuna con i suoi punti di forza e applicazioni, che guidano collettivamente la creazione e la comprensione di contenuti basati sull’IA.\n\nLarge Language Models (LLM)\nI “Large Language Model (LLM)” sono sistemi avanzati di intelligenza artificiale che comprendono, elaborano e generano testi simili a quelli umani. Questi modelli sono caratterizzati dalla loro enorme scala in termini di quantità di dati su cui vengono addestrati e numero di parametri che contengono. Gli aspetti critici degli LLM includono:\n\nDimensioni: Gli LLM contengono in genere miliardi di parametri. Ad esempio, GPT-3 ha 175 miliardi di parametri, mentre alcuni modelli più recenti superano un trilione di parametri.\nDati di Addestramento: Vengono addestrati su grandi quantità di dati di testo, spesso inclusi libri, siti Web e altre fonti diverse, che ammontano a centinaia di gigabyte o persino terabyte di testo.\nArchitettura: La maggior parte degli LLM utilizza architetture basate su trasformatori, che consentono loro di elaborare e generare testo prestando attenzione a diverse parti dell’input contemporaneamente.\nCapacità: Gli LLM possono eseguire un’ampia gamma di attività linguistiche senza una messa a punto specifica, tra cui:\n\nGenerazione di testo\nTraduzione\nRiepilogo\nRisposte a domande\nGenerazione di codice\nRagionamento logico\n\nApprendimento a Intervalli: Spesso possono comprendere ed eseguire nuove attività con esempi o istruzioni minimi.\nRichiede molte Risorse: A causa delle loro dimensioni, gli LLM richiedono in genere risorse di elaborazione significative per funzionare, spesso necessitando di potenti GPU o TPU.\nSviluppo Continuo: Il campo degli LLM è in rapida evoluzione, con nuovi modelli e tecniche che emergono costantemente.\nConsiderazioni Etiche: L’uso degli LLM solleva importanti questioni su pregiudizi, disinformazione e impatto ambientale della formazione di modelli così grandi.\nApplicazioni: Gli LLM sono utilizzati in vari campi, tra cui la creazione di contenuti, il servizio clienti, l’assistenza alla ricerca e lo sviluppo di software.\nLimitazioni: Nonostante la loro potenza, gli LLM possono produrre informazioni errate o distorte e non hanno capacità di vera comprensione o ragionamento.\n\nDobbiamo notare che utilizziamo modelli di grandi dimensioni oltre al testo, chiamandoli modelli multimodali. Questi modelli integrano ed elaborano informazioni da più tipi di input contemporaneamente. Sono progettati per comprendere e generare contenuti in varie forme di dati, come testo, immagini, audio e video.\nCertamente. Definiamo modelli “open” [aperti] e “closed” [chiusi] nel contesto di modelli di linguaggio e IA:\n\n\nModelli Chiusi e Aperti:\nModelli Chiusi, detti anche modelli proprietari, sono modelli di IA il cui funzionamento interno, codice e dati di addestramento non sono divulgati pubblicamente. Esempi: GPT-4 (di OpenAI), Claude (di Anthropic), Gemini (di Google).\nModelli Aperti, noti anche come modelli open source, sono modelli di intelligenza artificiale il cui codice sottostante, architettura e spesso dati di addestramento sono disponibili e accessibili al pubblico. Esempi: Gemma (di Google), LLaMA (di Meta) e Phi (di Microsoft).\nI modelli aperti sono particolarmente rilevanti per l’esecuzione di modelli su dispositivi edge come Raspberry Pi in quanto possono essere più facilmente adattati, ottimizzati e distribuiti in ambienti con risorse limitate. Tuttavia, è fondamentale verificare le loro Licenze. I modelli aperti sono dotati di varie licenze open source che possono influenzare il loro utilizzo in applicazioni commerciali, mentre i modelli chiusi hanno termini di servizio chiari, seppur restrittivi.\n\n\n\nAdattato da https://arxiv.org/pdf/2304.13712\n\n\n\n\nSmall Language Models (SLM)\nNel contesto dell’edge computing su dispositivi come Raspberry Pi, gli LLM su larga scala sono in genere troppo grandi e dispendiosi in termini di risorse per essere eseguiti direttamente. Questa limitazione ha spinto lo sviluppo di modelli più piccoli ed efficienti, come gli Small Language Models (SLM).\nGli SLM sono versioni compatte degli LLM progettate per essere eseguite in modo efficiente su dispositivi con risorse limitate come smartphone, dispositivi IoT e computer a scheda singola come Raspberry Pi. Questi modelli sono significativamente più piccoli in termini di dimensioni e requisiti di calcolo rispetto alle loro controparti più grandi, pur mantenendo impressionanti capacità di comprensione e generazione del linguaggio.\nLe caratteristiche principali degli SLM includono:\n\nConteggio ridotto dei parametri: In genere vanno da poche centinaia di milioni a qualche miliardo di parametri, rispetto ai miliardi a due cifre nei modelli più grandi.\nMeno ingombro di memoria: Richiedono, al massimo, pochi gigabyte di memoria anziché decine o centinaia di gigabyte.\nTempo di inferenza più rapido: Possono generare risposte in millisecondi o secondi su dispositivi edge.\nEfficienza energetica: Consumano meno energia, rendendoli adatti per dispositivi alimentati a batteria.\nTutela della privacy: Abilitano l’elaborazione sul dispositivo senza inviare dati ai server cloud.\nFunzionalità offline: Funzionano senza una connessione Internet.\n\nGli SLM raggiungono le loro dimensioni compatte attraverso varie tecniche come la distillazione della conoscenza, la potatura del modello e la quantizzazione. Sebbene non possano eguagliare le ampie capacità dei modelli più grandi, gli SLM eccellono in compiti e domini specifici, il che li rende ideali per applicazioni mirate su dispositivi edge.\n\nIn genere prenderemo in considerazione gli SLM, modelli linguistici con meno di 5 miliardi di parametri quantizzati a 4 bit.\n\nEsempi di SLM includono versioni compresse di modelli come Meta Llama, Microsoft PHI e Google Gemma. Questi modelli consentono un’ampia gamma di attività di elaborazione del linguaggio naturale direttamente sui dispositivi edge, dalla classificazione del testo e analisi del sentiment alle risposte alle domande e alla generazione di testo limitato.\nPer maggiori informazioni sugli SLM, il documento, LLM Pruning and Distillation in Practice: The Minitron Approach, fornisce un approccio che applica il pruning e la distillazione per ottenere SLM da LLM. E, SMALL LANGUAGE MODELS: SURVEY, MEASUREMENTS, AND INSIGHTS, presenta un’indagine e un’analisi complete di Small Language Models (SLM), che sono modelli linguistici con da 100 milioni a 5 miliardi di parametri progettati per dispositivi con risorse limitate.",
    "crumbs": [
      "Raspberry Pi",
      "Small Language Models (SLM)"
    ]
  },
  {
    "objectID": "contents/labs/raspi/llm/llm.it.html#ollama",
    "href": "contents/labs/raspi/llm/llm.it.html#ollama",
    "title": "Small Language Models (SLM)",
    "section": "Ollama",
    "text": "Ollama\n\n\n\nlogo di ollama\n\n\nOllama è un framework open source che ci consente di eseguire modelli linguistici (LM), grandi o piccoli, localmente sulle nostre macchine. Ecco alcuni punti critici su Ollama:\n\nEsecuzione del Modello Locale: Ollama consente di eseguire LM su personal computer o dispositivi edge come Raspi-5, eliminando la necessità di chiamate API basate su cloud.\nFacilità d’Uso: Fornisce una semplice interfaccia a riga di comando per scaricare, eseguire e gestire diversi modelli linguistici.\nVarietà di Modelli: Ollama supporta vari LLM, tra cui Phi, Gemma, Llama, Mistral e altri modelli open source.\nPersonalizzazione: Gli utenti possono creare e condividere modelli personalizzati su misura per esigenze o domini specifici.\nLeggero: Progettato per essere efficiente e funzionare su hardware di livello consumer.\nIntegrazione API: Offre un’API che consente l’integrazione con altre applicazioni e servizi.\nIncentrato sulla Privacy: Eseguendo i modelli localmente, affronta i problemi di privacy associati all’invio di dati a server esterni.\nMultipiattaforma: Disponibile per sistemi macOS, Windows e Linux (il nostro caso, qui).\nSviluppo Attivo: Aggiornato regolarmente con nuove funzionalità e supporto per i modelli.\nGuidato dalla Community: Trae vantaggio dai contributi della community e dalla condivisione dei modelli.\n\nPer saperne di più su cosa sia Ollama e come funziona sotto internamente, si può guardare questo breve video di Matt Williams, uno dei fondatori di Ollama:\n\n\nMatt ha un corso completamente gratuito su Ollama che consigliamo: \n\n\nInstallazione di Ollama\nImpostiamo e attiviamo un ambiente virtuale per lavorare con Ollama:\npython3 -m venv ~/ollama\nsource ~/ollama/bin/activate\nEd eseguiamo il comando per installare Ollama:\ncurl -fsSL https://ollama.com/install.sh | sh\nDi conseguenza, un’API verrà eseguita in background su 127.0.0.1:11434. D’ora in poi, possiamo eseguire Ollama tramite il terminale. Per iniziare, verifichiamo la versione di Ollama, che ci dirà anche se è installata correttamente:\nollama -v\n\nNella pagina della Libreria Ollama, possiamo trovare i modelli supportati da Ollama. Ad esempio, filtrando per Most popular, possiamo vedere Meta Llama, Google Gemma, Microsoft Phi, LLaVa, ecc.\n\n\nMeta Llama 3.2 1B/3B\n\nInstalliamo ed eseguiamo il nostro primo piccolo modello linguistico, Llama 3.2 1B (e 3B). La serie Meta Llama 3.2 comprende un set di modelli linguistici generativi multilingue disponibili in dimensioni di parametri pari a 1 miliardo e 3 miliardi. Questi modelli sono progettati per elaborare input di testo e generare output di testo. Le varianti sintonizzate sulle istruzioni all’interno di questa raccolta sono specificamente ottimizzate per applicazioni conversazionali multilingue, tra cui attività che comportano il recupero e la sintesi delle informazioni con un approccio agentico. Rispetto a molti modelli di chat open source e proprietari esistenti, i modelli sintonizzati sulle istruzioni Llama 3.2 dimostrano prestazioni superiori su benchmark di settore ampiamente utilizzati.\nI modelli 1B e 3B sono stati potati da Llama 8B, quindi i logit dai modelli 8B e 70B sono stati utilizzati come target a livello di token (distillazione a livello di token). La distillazione della conoscenza è stata utilizzata per recuperare le prestazioni (sono stati addestrati con 9 trilioni di token). Il modello 1B ha 1,24B, quantizzati a intero (Q8_0), e i parametri 3B, 3,12B, con una quantizzazione Q4_0, che termina con una dimensione di 1,3 GB e 2 GB, rispettivamente. La sua finestra di contesto è di 131.072 token.\n\nInstallare ed eseguire il Model\nollama run llama3.2:1b\nEseguendo il modello col comando precedente, dovremmo avere il prompt di Ollama disponibile per inserire una domanda e iniziare a chattare con il modello LLM; ad esempio,\n&gt;&gt;&gt; What is the capital of France?\nQuasi immediatamente, otteniamo la risposta corretta:\nThe capital of France is Paris.\nUtilizzando l’opzione --verbose quando si richiama il modello, verranno generate diverse statistiche sulle sue prestazioni (Il modello eseguirà il polling solo la prima volta che eseguiamo il comando).\n\nOgni metrica fornisce informazioni su come il modello elabora gli input e genera gli output. Ecco una ripartizione del significato di ogni metrica:\n\nTotal Duration (2.620170326s): Questo è il tempo completo impiegato dall’inizio del comando al completamento della risposta. Comprende il caricamento del modello, l’elaborazione del prompt di input e la generazione della risposta.\nLoad Duration (39.947908ms): Questa durata indica il tempo necessario per caricare il modello o i componenti necessari nella memoria. Se questo valore è minimo, può suggerire che il modello è stato precaricato o che è stata richiesta solo una configurazione minima.\nPrompt Eval Count (32 tokens): Il numero di token nel prompt di input. In NLP, i token sono in genere parole o sotto-parole, quindi questo conteggio include tutti i token che il modello ha valutato per comprendere e rispondere alla query.\nPrompt Eval Duration (1.644773s): Misura il tempo impiegato dal modello per valutare o elaborare il prompt di input. Rappresenta la maggior parte della durata totale, il che implica che comprendere la query e preparare una risposta è la parte più dispendiosa in termini di tempo del processo.\nPrompt Eval Rate (19.46 tokens/s): Questa frequenza indica la rapidità con cui il modello elabora i token dal prompt di input. Riflette la velocità del modello in termini di comprensione del linguaggio naturale.\nEval Count (8 token(s)): Questo è il numero di token nella risposta del modello, che in questo caso era “The capital of France is Paris.”\nEval Duration (889.941ms): Questo è il tempo impiegato per generare l’output in base all’input valutato. È molto più breve della valutazione del prompt, il che suggerisce che generare la risposta è meno complesso o computazionalmente intensivo rispetto alla comprensione del prompt.\nEval Rate (8.99 tokens/s): Simile alla frequenza di valutazione del prompt, indica la velocità con cui il modello genera token di output. È una metrica fondamentale per comprendere l’efficienza del modello nella generazione di output.\n\nQuesta ripartizione dettagliata può aiutare a comprendere le richieste computazionali e le caratteristiche delle prestazioni dell’esecuzione di SLM come Llama su dispositivi edge come Raspberry Pi 5. Mostra che mentre la valutazione del prompt richiede più tempo, la generazione effettiva delle risposte è relativamente più rapida. Questa analisi è fondamentale per ottimizzare le prestazioni e diagnosticare potenziali colli di bottiglia nelle applicazioni in tempo reale.\nCaricando ed eseguendo il modello 3B, possiamo vedere la differenza nelle prestazioni per lo stesso prompt;\n\nIl tasso di valutazione è inferiore, 5,3 token/s rispetto ai 9 token/s del modello più piccolo.\nQuando si chiede\n&gt;&gt;&gt; What is the distance between Paris and Santiago, Chile?\nIl modello 1B ha risposto 9,841 kilometers (6,093 miles), il che è impreciso, e il modello 3B ha risposto 7,300 miles (11,700 km), il che è vicino alla distanza corretta (11,642 km).\nChiediamo le coordinate di Parigi:\n&gt;&gt;&gt; what is the latitude and longitude of Paris?\nThe latitude and longitude of Paris are 48.8567° N (48°55' \n42\" N) and 2.3510° E (2°22' 8\" E), respectively.\n\nSia i modelli 1B che 3B hanno dato risposte corrette.\n\n\nGoogle Gemma 2 2B\nInstalliamo Gemma 2, un modello efficiente e ad alte prestazioni disponibile in tre dimensioni: 2B, 9B e 27B. Installeremo Gemma 2 2B, un modello leggero addestrato con 2 trilioni di token che produce risultati sproporzionati imparando da modelli più grandi tramite distillazione. Il modello ha 2,6 miliardi di parametri e una quantizzazione Q4_0, che termina con una dimensione di 1,6 GB. La sua finestra di contesto è di 8.192 token.\n\nInstallare ed eseguire il Model\nollama run gemma2:2b --verbose\nEseguendo il modello col comando precedente, dovremmo avere il prompt di Ollama disponibile per inserire una domanda e iniziare a chattare con il modello LLM; ad esempio,\n&gt;&gt;&gt; What is the capital of France?\nQuasi immediatamente, otteniamo la risposta corretta:\nThe capital of France is **Paris**. 🗼\nE le statistiche.\n\nPossiamo vedere che Gemma 2:2B ha più o meno le stesse prestazioni di Lama 3.2:3B, ma con meno parametri.\nAltri esempi:\n&gt;&gt;&gt; What is the distance between Paris and Santiago, Chile?\n\nThe distance between Paris, France and Santiago, Chile is \napproximately **7,000 miles (11,267 kilometers)**. \n\nKeep in mind that this is a straight-line distance, and actual \ntravel distance can vary depending on the chosen routes and any \nstops along the way. ✈️`\nInoltre, una buona risposta ma meno precisa di Llama3.2:3B.\n&gt;&gt;&gt; what is the latitude and longitude of Paris?\n\nYou got it! Here are the latitudes and longitudes of Paris, \nFrance:\n\n* **Latitude:** 48.8566° N (north)\n* **Longitude:** 2.3522° E (east) \n\nLet me know if you'd like to explore more about Paris or its \nlocation! 🗼🇫🇷\nUna risposta buona e precisa (un po’ più prolissa delle risposte del lama).\n\n\nMicrosoft Phi3.5 3.8B\nPrendiamo un modello più grande (ma comunque “tiny”), il PHI3.5, un modello aperto all’avanguardia leggero da 3,8B di Microsoft. Il modello appartiene alla famiglia di modelli Phi-3 e supporta la lunghezza del contesto token 128K e le lingue: arabo, cinese, ceco, danese, olandese, inglese, finlandese, francese, tedesco, ebraico, ungherese, italiano, giapponese, coreano, norvegese, polacco, portoghese, russo, spagnolo, svedese, tailandese, turco e ucraino.\nLa dimensione del modello, in termini di byte, dipenderà dal formato di quantizzazione specifico utilizzato. La dimensione può variare dalla quantizzazione a 2 bit (q2_k) di 1,4 GB (prestazioni più elevate/qualità inferiore) alla quantizzazione a 16 bit (fp-16) di 7,6 GB (prestazioni più basse/qualità più elevata).\nEseguiamo la quantizzazione a 4 bit (Q4_0), che richiederà 2,2 GB di RAM, con un compromesso intermedio per quanto riguarda la qualità dell’output e le prestazioni.\nollama run phi3.5:3.8b --verbose\n\nSi può usare run o pull per scaricare il modello. Ciò che accade è che Ollama tiene nota dei modelli estratti e, una volta che PHI3 non esiste, prima di eseguirlo, Ollama lo estrae.\n\nImmettiamo lo stesso prompt usato prima:\n&gt;&gt;&gt; What is the capital of France?\n\n\nThe capital of France is Paris. It' extradites significant \nhistorical, cultural, and political importance to the country as \nwell as being a major European city known for its art, fashion, \ngastronomy, and culture. Its influence extends beyond national \nborders, with millions of tourists visiting each year from around \nthe globe. The Seine River flows through Paris before it reaches \nthe broader English Channel at Le Havre. Moreover, France is one \nof Europe's leading economies with its capital playing a key role \n\n...\nLa risposta è stata molto “verbosa”, specifichiamo un prompt migliore:\n\nIn questo caso, la risposta è stata comunque più lunga di quanto ci aspettassimo, con una velocità di valutazione di 2,25 token/s, più del doppio di quella di Gemma e Llama.\n\nScegliere il prompt più appropriato è una delle competenze più importanti da utilizzare con gli LLM, indipendentemente dalle dimensioni.\n\nQuando abbiamo posto le stesse domande su distanza e latitudine/longitudine, non abbiamo ottenuto una buona risposta per una distanza di 13,507 kilometers (8,429 miles), ma andava bene per le coordinate. Di nuovo, avrebbe potuto essere meno verbosa (più di 200 token per ogni risposta).\nPossiamo usare qualsiasi modello come assistente poiché la loro velocità è relativamente decente, ma al 24 settembre (2023), Llama2:3B è una scelta migliore. Si dovrebbero provare altri modelli, a seconda delle esigenze. 🤗 Open LLM Leaderboard può dare un’idea sui migliori modelli in termini di dimensioni, benchmark, licenza, ecc.\n\nIl miglior modello da usare è quello adatto alle specifiche necessità. Inoltre, si tenga presente che questo campo si evolve con nuovi modelli ogni giorno,\n\n\n\nModelli Multimodali\nI modelli multimodali sono sistemi di intelligenza artificiale (IA) in grado di elaborare e comprendere informazioni provenienti da più fonti, come immagini, testo, audio e video. Nel nostro contesto, gli LLM multimodali possono elaborare vari input, tra cui testo, immagini e audio, come prompt e convertire tali prompt in vari output, non solo il tipo di sorgente.\nQui lavoreremo con LLaVA-Phi-3, un modello LLaVA ottimizzato da Phi 3 Mini 4k. Ha solidi benchmark di prestazioni che sono alla pari con il modello originale LLaVA (Large Language and Vision Assistant).\nLLaVA-Phi-3 è un modello multimodale di grandi dimensioni addestrato end-to-end progettato per comprendere e generare contenuti in base a input visivi (immagini) e istruzioni testuali. Combina le capacità di un codificatore visivo e di un modello linguistico per elaborare e rispondere a input multimodali.\nInstalliamo il modello:\nollama run llava-phi3:3.8b --verbose\nCominciamo con un input di testo:\n&gt;&gt;&gt; You are a helpful AI assistant. What is the capital of France?\n\nAs an AI language model, I can tell you that the capital of France \nis Paris. It's not only the largest city in the country but also \nserves as its political and administrative center. Paris is known \nfor its iconic landmarks such as the Eiffel Tower, Notre-Dame \nCathedral, and the Louvre Museum. The city has a rich history, \nbeautiful architecture, and is widely considered to be one of the \nmost romantic cities in the world.\nLa risposta ha richiesto circa 30 secondi, con una velocità di valutazione di 3,93 token/s! Niente male!\nMa vediamo di immettere un’immagine come input. Per questo, creiamo una directory per lavorare:\ncd Documents/\nmkdir OLLAMA\ncd OLLAMA\nScarichiamo un’immagine 640x320 da Internet, per esempio (Wikipedia: Paris, France):\n\nUtilizzando FileZilla, ad esempio, carichiamo l’immagine nella cartella OLLAMA sul Raspi-5 e chiamiamola image_test_1.jpg. Dovremmo avere l’intero path dell’immagine (possiamo usare pwd per ottenerlo).\n/home/mjrovai/Documents/OLLAMA/image_test_1.jpg\nSe si usa un desktop, si può copiare il path dell’immagine cliccando sull’immagine con il tasto destro del mouse.\n\nImmettiamo questo prompt:\n&gt;&gt;&gt; Describe the image /home/mjrovai/Documents/OLLAMA/image_test_1.jpg\nIl risultato è stato ottimo, ma la latenza complessiva è stata significativa: quasi 4 minuti per eseguire l’inferenza.\n\n\n\nIspezione delle risorse locali\nUtilizzando htop, possiamo monitorare le risorse in esecuzione sul nostro dispositivo.\nhtop\nDurante il periodo in cui il modello è in esecuzione, possiamo ispezionare le risorse:\n\nTutte e quattro le CPU funzionano a quasi il 100% della loro capacità e la memoria utilizzata con il modello caricato è di 3.24GB. Uscendo da Ollama, la memoria scende a circa 377 MB (senza desktop).\nÈ anche essenziale monitorare la temperatura. Quando si esegue Raspberry con un desktop, è possibile visualizzare la temperatura sulla barra delle applicazioni:\n\nSe si è “headless”, la temperatura può essere monitorata col comando:\nvcgencmd measure_temp\nSe non si fa nulla, la temperatura è di circa 50°C per le CPU in esecuzione all’1%. Durante l’inferenza, con le CPU al 100%, la temperatura può salire fino a quasi 70°C. Questo è OK e significa che il dissipatore attivo funziona, mantenendo la temperatura al di sotto di 80°C / 85°C (il suo limite).",
    "crumbs": [
      "Raspberry Pi",
      "Small Language Models (SLM)"
    ]
  },
  {
    "objectID": "contents/labs/raspi/llm/llm.it.html#libreria-python-ollama",
    "href": "contents/labs/raspi/llm/llm.it.html#libreria-python-ollama",
    "title": "Small Language Models (SLM)",
    "section": "Libreria Python Ollama",
    "text": "Libreria Python Ollama\nFinora, abbiamo esplorato la capacità di chat degli SLM utilizzando la riga di comando su un terminale. Tuttavia, vogliamo integrare quei modelli nei nostri progetti, quindi Python sembra essere la strada giusta. La buona notizia è che Ollama ha una libreria del genere.\nLa libreria Python Ollama semplifica l’interazione con modelli LLM avanzati, consentendo risposte e capacità più sofisticate, oltre a fornire il modo più semplice per integrare progetti Python 3.8+ con Ollama.\nPer una migliore comprensione di come creare app usando Ollama con Python, possiamo seguire i video di Matt Williams, come quello qui sotto:\n\nInstallazione:\nNel terminale, eseguire il comando:\npip install ollama\nAvremo bisogno di un editor di testo o di un IDE per creare uno script Python. Se si esegue Raspberry OS su un desktop, diverse opzioni, come Thonny e Geany, sono già state installate di default (accessibili tramite [Menu][Programming]). Si possono scaricare altri IDE, come Visual Studio Code, da [Menu][Recommended Software]. Quando si apre la finestra, si va su [Programming], e si seleziona l’opzione che si preferisce e si preme [Apply].\n\nSe si preferisce usare Jupyter Notebook per lo sviluppo:\npip install jupyter\njupyter notebook --generate-config\nPer eseguire Jupyter Notebook, si lancia il comando (cambiare l’indirizzo IP per il proprio):\njupyter notebook --ip=192.168.4.209 --no-browser\nSul terminale, si può vedere l’indirizzo URL locale per aprire il notebook:\n\nPossiamo accedervi da un altro computer inserendo l’indirizzo IP del Raspberry Pi e il token fornito in un browser web (dovremmo copiarlo dal terminale).\nNella nostra directory di lavoro nel Raspi, creeremo un nuovo notebook Python 3.\nEntriamo con uno script molto semplice per verificare i modelli installati:\nimport ollama\nollama.list()\nTutti i modelli verranno stampati come un dizionario, ad esempio:\n{'name': 'gemma2:2b',\n   'model': 'gemma2:2b',\n   'modified_at': '2024-09-24T19:30:40.053898094+01:00',\n   'size': 1629518495,\n   'digest': '8ccf136fdd5298f3ffe2d69862750ea7fb56555fa4d5b18c04e3fa4d82ee09d7',\n   'details': {'parent_model': '',\n    'format': 'gguf',\n    'family': 'gemma2',\n    'families': ['gemma2'],\n    'parameter_size': '2.6B',\n    'quantization_level': 'Q4_0'}}]}\nRipetiamo una delle domande che abbiamo fatto prima, ma ora usando ollama.generate() dalla libreria Python Ollama. Questa API genererà una risposta per il prompt specificato con il modello fornito. Questo è un endpoint di streaming, quindi ci saranno una serie di risposte. L’oggetto di risposta finale includerà statistiche e dati aggiuntivi dalla richiesta.\nMODEL = 'gemma2:2b'\nPROMPT = 'What is the capital of France?'\n\nres = ollama.generate(model=MODEL, prompt=PROMPT)\nprint (res)\nNel caso in cui si stia eseguendo il codice come script Python, lo si deve salvare, ad esempio, test_ollama.py. Si può usare l’IDE per eseguirlo o farlo direttamente sul terminale. Da ricordare, inoltre, che si deve sempre chiamare il modello e definirlo quando si esegue uno script stand-alone.\npython test_ollama.py\nDi conseguenza, avremo la risposta del modello in un formato JSON:\n{'model': 'gemma2:2b', 'created_at': '2024-09-25T14:43:31.869633807Z', \n'response': 'The capital of France is **Paris**. 🇫🇷 \\n', 'done': True, \n'done_reason': 'stop', 'context': [106, 1645, 108, 1841, 603, 573, 6037, 576,\n6081, 235336, 107, 108, 106, 2516, 108, 651, 6037, 576, 6081, 603, 5231, 29437, \n168428, 235248, 244304, 241035, 235248, 108], 'total_duration': 24259469458, \n'load_duration': 19830013859, 'prompt_eval_count': 16, 'prompt_eval_duration': \n1908757000, 'eval_count': 14, 'eval_duration': 2475410000}\nCome possiamo vedere, vengono generate diverse informazioni, come:\n\nresponse: il testo di output principale generato dal modello in risposta al nostro prompt.\n\nThe capital of France is **Paris**. 🇫🇷\n\ncontext: gli ID token che rappresentano l’input e il contesto utilizzati dal modello. I token sono rappresentazioni numeriche del testo utilizzate per l’elaborazione dal modello linguistico.\n\n[106, 1645, 108, 1841, 603, 573, 6037, 576, 6081, 235336, 107, 108, 106, 2516, 108, 651, 6037, 576, 6081, 603, 5231, 29437, 168428, 235248, 244304, 241035, 235248, 108]\n\n\nLe Metriche delle Prestazioni:\n\ntotal_duration: Tempo totale impiegato per l’operazione in nanosecondi. In questo caso, circa 24,26 secondi.\nload_duration: Tempo impiegato per caricare il modello o i componenti in nanosecondi. Circa 19,83 secondi.\nprompt_eval_duration: Tempo impiegato per valutare il prompt in nanosecondi. Circa 16 nanosecondi.\neval_count: Numero di token valutati durante la generazione. Qui, 14 token.\neval_duration: Tempo impiegato dal modello per generare la risposta in nanosecondi. Circa 2,5 secondi.\n\nMa ciò che vogliamo è la semplice ‘response’ e, forse per l’analisi, la durata totale dell’inferenza, quindi modifichiamo il codice per estrarlo dal dizionario:\nprint(f\"\\n{res['response']}\")\nprint(f\"\\n [INFO] Total Duration: {(res['total_duration']/1e9):.2f} seconds\")\nOra, abbiamo:\nThe capital of France is **Paris**. 🇫🇷 \n\n [INFO] Total Duration: 24.26 seconds\nUtilizzo di Ollama.chat()\nUn altro modo per ottenere la nostra risposta è utilizzare ollama.chat(), che genera il messaggio successivo in una chat con un modello fornito. Questo è un endpoint di streaming, quindi si verificheranno una serie di risposte. Lo streaming può essere disabilitato utilizzando \"stream\": false. L’oggetto di risposta finale includerà anche statistiche e dati aggiuntivi dalla richiesta.\nPROMPT_1 = 'What is the capital of France?'\n\nresponse = ollama.chat(model=MODEL, messages=[\n{'role': 'user','content': PROMPT_1,},])\nresp_1 = response['message']['content']\nprint(f\"\\n{resp_1}\")\nprint(f\"\\n [INFO] Total Duration: {(res['total_duration']/1e9):.2f} seconds\")\nLa risposta è la stessa di prima.\nUna considerazione importante è che usando ollama.generate(), la risposta è “chiara” dalla “memoria” del modello dopo la fine dell’inferenza (usata solo una volta), ma se vogliamo mantenere una conversazione, dobbiamo usare ollama.chat(). Vediamolo in azione:\nPROMPT_1 = 'What is the capital of France?'\nresponse = ollama.chat(model=MODEL, messages=[\n{'role': 'user','content': PROMPT_1,},])\nresp_1 = response['message']['content']\nprint(f\"\\n{resp_1}\")\nprint(f\"\\n [INFO] Total Duration: {(response['total_duration']/1e9):.2f} seconds\")\n\nPROMPT_2 = 'and of Italy?'\nresponse = ollama.chat(model=MODEL, messages=[\n{'role': 'user','content': PROMPT_1,},\n{'role': 'assistant','content': resp_1,},\n{'role': 'user','content': PROMPT_2,},])\nresp_2 = response['message']['content']\nprint(f\"\\n{resp_2}\")\nprint(f\"\\n [INFO] Total Duration: {(response['total_duration']/1e9):.2f} seconds\")\nNel codice sopra, stiamo eseguendo due query e il secondo prompt considera il risultato del primo.\nEcco come ha risposto il modello:\nThe capital of France is **Paris**. 🇫🇷 \n\n [INFO] Total Duration: 2.82 seconds\n\nThe capital of Italy is **Rome**. 🇮🇹 \n\n [INFO] Total Duration: 4.46 seconds\nOttenere una descrizione dell’immagine:\nAllo stesso modo in cui abbiamo utilizzato il modello LlaVa-PHI-3 con la riga di comando per analizzare un’immagine, lo stesso può essere fatto qui con Python. Usiamo la stessa immagine di Parigi, ma ora con ollama.generate():\nMODEL = 'llava-phi3:3.8b'\nPROMPT = \"Describe this picture\"\n\nwith open('image_test_1.jpg', 'rb') as image_file:\n    img = image_file.read()\n\nresponse = ollama.generate(\n    model=MODEL,\n    prompt=PROMPT,\n    images= [img]\n)\nprint(f\"\\n{response['response']}\")\nprint(f\"\\n [INFO] Total Duration: {(res['total_duration']/1e9):.2f} seconds\")\nEcco il risultato:\nThis image captures the iconic cityscape of Paris, France. The vantage point \nis high, providing a panoramic view of the Seine River that meanders through \nthe heart of the city. Several bridges arch gracefully over the river, \nconnecting different parts of the city. The Eiffel Tower, an iron lattice \nstructure with a pointed top and two antennas on its summit, stands tall in the \nbackground, piercing the sky. It is painted in a light gray color, contrasting \nagainst the blue sky speckled with white clouds.\n\nThe buildings that line the river are predominantly white or beige, their uniform\ncolor palette broken occasionally by red roofs peeking through. The Seine River \nitself appears calm and wide, reflecting the city's architectural beauty in its \nsurface. On either side of the river, trees add a touch of green to the urban \nlandscape.\n\nThe image is taken from an elevated perspective, looking down on the city. This \nviewpoint allows for a comprehensive view of Paris's beautiful architecture and \nlayout. The relative positions of the buildings, bridges, and other structures \ncreate a harmonious composition that showcases the city's charm.\n\nIn summary, this image presents a serene day in Paris, with its architectural \nmarvels - from the Eiffel Tower to the river-side buildings - all bathed in soft \ncolors under a clear sky.\n\n [INFO] Total Duration: 256.45 seconds\nIl modello ha impiegato circa 4 minuti (256,45 s) per restituire una descrizione dettagliata dell’immagine.\n\nNel notebook 10-Ollama_Python_Library, è possibile trovare gli esperimenti con la libreria Python Ollama.\n\n\nChiamata di Funzione\nFinora, possiamo osservare che utilizzando la risposta del modello in una variabile, possiamo incorporarla efficacemente in progetti reali. Tuttavia, sorge un problema importante quando il modello fornisce risposte diverse allo stesso input. Ad esempio, supponiamo di aver bisogno solo del nome della capitale di un paese e delle sue coordinate come risposta del modello negli esempi precedenti, senza ulteriori informazioni, anche quando si utilizzano modelli dettagliati come Microsoft Phi. Per garantire risposte coerenti, possiamo utilizzare la “chiamata di funzione Ollama”, che è completamente compatibile con l’API OpenAI.\n\nMa cos’è esattamente la “chiamata di funzione”?\nNell’intelligenza artificiale moderna, la chiamata di funzione con Large Language Models (LLM) consente a questi modelli di eseguire azioni che vanno oltre la generazione di testo. Integrandosi con funzioni o API esterne, gli LLM possono accedere a dati in tempo reale, automatizzare attività e interagire con vari sistemi.\nAd esempio, invece di rispondere semplicemente a una query sul meteo, un LLM può chiamare un’API meteo per recuperare le condizioni attuali e fornire informazioni accurate e aggiornate. Questa capacità migliora la pertinenza e l’accuratezza delle risposte del modello e lo rende uno strumento potente per guidare flussi di lavoro e automatizzare i processi, trasformandolo in un partecipante attivo nelle applicazioni del mondo reale.\nPer maggiori dettagli sulla “Function Calling” chiamata di funzione, c’è questo video realizzato da Marvin Prison:\n\n\n\nCreiamo un progetto.\nVogliamo creare un’app in cui l’utente inserisce il nome di un Paese e ottiene, come output, la distanza in km dalla capitale di tale Paese e la posizione dell’app (per semplicità, useremo Santiago del Cile come posizione dell’app).\n\nUna volta che l’utente inserisce il nome di un paese, il modello restituirà il nome della sua capitale (come stringa) e la latitudine e la longitudine di tale città (in float). Utilizzando queste coordinate, possiamo usare una semplice libreria Python (haversine) per calcolare la distanza tra quei 2 punti.\nL’idea di questo progetto è dimostrare una combinazione di interazione del modello linguistico, gestione dei dati strutturati con Pydantic e calcoli geospaziali utilizzando la formula di Haversine (informatica tradizionale).\nPer prima cosa, installiamo alcune librerie. Oltre a Haversine, la principale è la libreria Python OpenAI, che fornisce un comodo accesso all’API REST OpenAI da qualsiasi applicazione Python 3.7+. L’altra è Pydantic (e instructor), una libreria robusta di convalida dei dati e gestione delle impostazioni progettata da Python per migliorare la robustezza e l’affidabilità della nostra base di codice. In breve, Pydantic ci aiuterà a garantire che la risposta del nostro modello sia sempre coerente.\npip install haversine\npip install openai \npip install pydantic \npip install instructor\nOra, dovremmo creare uno script Python progettato per interagire con il nostro modello (LLM) per determinare le coordinate della capitale di un paese e calcolare la distanza da Santiago del Cile a quella capitale.\nDiamo un’occhiata al codice:\n\n\n\n1. Importazione delle Librerie\nimport sys\nfrom haversine import haversine\nfrom openai import OpenAI\nfrom pydantic import BaseModel, Field\nimport instructor\n\nsys: Fornisce accesso a parametri e funzioni specifici del sistema. Viene utilizzato per ottenere argomenti dalla riga di comando.\nhaversine: Una funzione della libreria haversine che calcola la distanza tra due punti geografici utilizzando la formula Haversine.\nopenAI: Un modulo per interagire con l’API OpenAI (anche se viene utilizzato insieme a una configurazione locale, Ollama). Qui tutto è offline.\npydantic: Fornisce la convalida dei dati e la gestione delle impostazioni utilizzando annotazioni di tipo Python. Viene utilizzato per definire la struttura dei dati di risposta previsti.\ninstructor: Un modulo viene utilizzato per applicare patch al client OpenAI per funzionare in una modalità specifica (probabilmente correlata alla gestione dei dati strutturati).\n\n\n\n2. Definizione di Input e Modello\ncountry = sys.argv[1]       # Get the country from command-line arguments\nMODEL = 'phi3.5:3.8b'     # The name of the model to be used\nmylat = -33.33              # Latitude of Santiago de Chile\nmylon = -70.51              # Longitude of Santiago de Chile\n\ncountry: In uno script Python, è possibile ottenere il nome del paese dagli argomenti della riga di comando. In un notebook Jupyter, possiamo immettere il suo nome, ad esempio,\n\ncountry = \"France\"\n\nMODEL: Specifica il modello utilizzato, che è, in questo esempio, phi3.5.\nmylat e mylon: Coordinate di Santiago del Cile, utilizzate come punto di partenza per il calcolo della distanza.\n\n\n\n3. Definizione della Struttura dei Dati di Risposta\nclass CityCoord(BaseModel):\n    city: str = Field(..., description=\"Name of the city\")\n    lat: float = Field(..., description=\"Decimal Latitude of the city\")\n    lon: float = Field(..., description=\"Decimal Longitude of the city\")\n\nCityCoord: Un modello Pydantic che definisce la struttura prevista della risposta dal LLM. Si aspetta tre campi: city (nome della città), lat (latitudine) e lon (longitudine).\n\n\n\n4. Impostazione del Client OpenAI\nclient = instructor.patch(\n    OpenAI(\n        base_url=\"http://localhost:11434/v1\",  # Local API base URL (Ollama)\n        api_key=\"ollama\",                      # API key (not used)\n    ),\n    mode=instructor.Mode.JSON,                 # Mode for structured JSON output\n)\n\nOpenAI: Questa configurazione inizializza un client OpenAI con un URL di base locale e una chiave API (ollama). Utilizza un server locale.\ninstructor.patch: Applica patch al client OpenAI per funzionare in modalità JSON, abilitando un output strutturato che corrisponde al modello Pydantic.\n\n\n\n5. Generazione della Risposta\nresp = client.chat.completions.create(\n    model=MODEL,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": f\"return the decimal latitude and decimal longitude \\\n            of the capital of the {country}.\"\n        }\n\n    ],\n    response_model=CityCoord,\n    max_retries=10\n)\n\nclient.chat.completions.create: Chiama l’LLM per generare una risposta.\nmodel: Specifica il modello da utilizzare (llava-phi3).\nmessages: Contiene il prompt per l’LLM, che chiede latitudine e longitudine della capitale del paese specificato.\nresponse_model: Indica che la risposta deve essere conforme al modello CityCoord.\nmax_retries: Numero massimo di tentativi di ripetizione se la richiesta fallisce.\n\n\n\n6. Calcolo della Distanza\ndistance = haversine((mylat, mylon), (resp.lat, resp.lon), unit='km')\nprint(f\"Santiago de Chile is about {int(round(distance, -1)):,} \\\n        kilometers away from {resp.city}.\")\n\nhaversine: Calcola la distanza tra Santiago del Cile e la capitale restituita dall’LLM utilizzando le rispettive coordinate.\n(mylat, mylon): Coordinate di Santiago del Cile.\nresp.city: Nome della capitale del paese\n(resp.lat, resp.lon): Le coordinate della capitale sono fornite dalla risposta dell’LLM.\nunit=‘km’: Specifica che la distanza deve essere calcolata in chilometri.\nprint: Restituisce la distanza, arrotondata ai 10 chilometri più vicini, con migliaia di separatori per una migliore leggibilità.\n\nEsecuzione del codice\nSe inseriamo paesi diversi, ad esempio Francia, Colombia e Stati Uniti, possiamo notare che riceviamo sempre le stesse informazioni strutturate:\nSantiago de Chile is about 8,060 kilometers away from Washington, D.C..\nSantiago de Chile is about 4,250 kilometers away from Bogotá.\nSantiago de Chile is about 11,630 kilometers away from Paris.\nEseguendo il codice come script, il risultato verrà stampato sul terminale:\n\nE i calcoli sono piuttosto buoni!\n\n\nNel notebook 20-Ollama_Function_Calling, è possibile trovare esperimenti con tutti i modelli installati.\n\n\n\nAggiunta di immagini\nOra è il momento raccogliere il tutto! Modifichiamo lo script in modo che invece di immettere il nome del paese (come testo), l’utente immetta un’immagine e l’applicazione (basata su SLM) restituisca la città nell’immagine e la sua posizione geografica. Con quei dati, possiamo calcolare la distanza come prima.\n\nPer semplicità, implementeremo questo nuovo codice in due passaggi. Innanzitutto, LLM analizzerà l’immagine e creerà una descrizione (testo). Questo testo verrà passato a un’altra istanza, dove il modello estrarrà le informazioni necessarie per il passaggio.\nInizieremo ad importare le librerie\nimport sys\nimport time\nfrom haversine import haversine\nimport ollama\nfrom openai import OpenAI\nfrom pydantic import BaseModel, Field\nimport instructor\nPossiamo vedere l’immagine se si esegue il codice su Jupyter Notebook. Per questo dobbiamo anche importare:\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\nTali librerie non sono necessarie se eseguiamo il codice come uno script.\n\nOra, definiamo il modello e le coordinate locali:\nMODEL = 'llava-phi3:3.8b'\nmylat = -33.33\nmylon = -70.51\nPossiamo effettuare il download di una nuova immagine, ad esempio Machu Picchu da Wikipedia. Sul Notebook possiamo vederla:\n# Load the image\nimg_path = \"image_test_3.jpg\"\nimg = Image.open(img_path)\n\n# Display the image\nplt.figure(figsize=(8, 8))\nplt.imshow(img)\nplt.axis('off')\n#plt.title(\"Image\")\nplt.show()\n\nOra, definiamo una funzione che riceverà l’immagine e che return the decimal latitude and decimal longitude of the city in the image, its name, and what country it is located [restituirà la latitudine decimale e la longitudine decimale della città nell’immagine, il suo nome e il paese in cui si trova].\ndef image_description(img_path):\n    with open(img_path, 'rb') as file:\n        response = ollama.chat(\n            model=MODEL,\n            messages=[\n              {\n                'role': 'user',\n                'content': '''return the decimal latitude and decimal longitude \n                              of the city in the image, its name, and \n                              what country it is located''',\n                'images': [file.read()],\n              },\n            ],\n            options = {\n              'temperature': 0,\n              }\n\n      )\n\n    #print(response['message']['content'])\n    return response['message']['content']\n\nPossiamo stampare l’intera risposta per debug.\n\nLa descrizione dell’immagine generata per la funzione verrà passata di nuovo come prompt per il modello.\nstart_time = time.perf_counter()  # Start timing\n\nclass CityCoord(BaseModel):\n    city: str = Field(..., description=\"Name of the city in the image\")\n    country: str = Field(..., description=\"\"\"Name of the country where\"\n                                             the city in the image is located\n                                             \"\"\")\n    lat: float = Field(..., description=\"\"\"Decimal Latitude of the city in\"\n                                            the image\"\"\")\n    lon: float = Field(..., description=\"\"\"Decimal Longitude of the city in\"\n                                           the image\"\"\")\n\n# enables `response_model` in create call\nclient = instructor.patch(\n    OpenAI(\n        base_url=\"http://localhost:11434/v1\",\n        api_key=\"ollama\"\n    ),\n    mode=instructor.Mode.JSON,\n)\n\nimage_description = image_description(img_path)\n# Send this description to the model\nresp = client.chat.completions.create(\n    model=MODEL,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": image_description,\n        }\n\n    ],\n    response_model=CityCoord,\n    max_retries=10,\n    temperature=0,\n)\nSe stampiamo la descrizione dell’immagine, otterremo:\nThe image shows the ancient city of Machu Picchu, located in Peru. The city is\nperched on a steep hillside and consists of various structures made of stone. It \nis surrounded by lush greenery and towering mountains. The sky above is blue with\nscattered clouds. \n\nMachu Picchu's latitude is approximately 13.5086° S, and its longitude is around\n72.5494° W.\nE la seconda risposta del modello (resp) sarà:\nCityCoord(city='Machu Picchu', country='Peru', lat=-13.5086, lon=-72.5494)\nOra possiamo effettuare un “Post-Processing”, calcolando la distanza e preparando la risposta finale:\ndistance = haversine((mylat, mylon), (resp.lat, resp.lon), unit='km')\n\nprint(f\"\\n The image shows {resp.city}, with lat:{round(resp.lat, 2)} and \\\n      long: {round(resp.lon, 2)}, located in {resp.country} and about \\\n            {int(round(distance, -1)):,} kilometers away from \\\n            Santiago, Chile.\\n\")\n\nend_time = time.perf_counter()  # End timing\nelapsed_time = end_time - start_time  # Calculate elapsed time\nprint(f\" [INFO] ==&gt; The code (running {MODEL}), took {elapsed_time:.1f} \\\n      seconds to execute.\\n\")\nE otterremo:\nThe image shows Machu Picchu, with lat:-13.16 and long: -72.54, located in Peru\n and about 2,250 kilometers away from Santiago, Chile.\n\n [INFO] ==&gt; The code (running llava-phi3:3.8b), took 491.3 seconds to execute.\nNel notebook 30-Function_Calling_with_images è possibile trovare gli esperimenti con più immagini.\nScarichiamo ora lo script calc_distance_image.py da GitHub ed eseguiamolo sul terminale con il comando:\npython calc_distance_image.py /home/mjrovai/Documents/OLLAMA/image_test_3.jpg\nImmettere la patch completa dell’immagine di Machu Picchu come argomento. Otterremo lo stesso risultato precedente.\n\nPer wuanto riguarda Parigi?\n\nNaturalmente, ci sono molti modi per ottimizzare il codice utilizzato qui. Tuttavia, l’idea è di esplorare il notevole potenziale della function calling con SLM nell’edge, consentendo a tali modelli di integrarsi con funzioni o API esterne. Andando oltre la generazione di testo, gli SLM possono accedere a dati in tempo reale, automatizzare attività e interagire con vari sistemi.",
    "crumbs": [
      "Raspberry Pi",
      "Small Language Models (SLM)"
    ]
  },
  {
    "objectID": "contents/labs/raspi/llm/llm.it.html#slm-tecniche-di-ottimizzazione",
    "href": "contents/labs/raspi/llm/llm.it.html#slm-tecniche-di-ottimizzazione",
    "title": "Small Language Models (SLM)",
    "section": "SLM: Tecniche di Ottimizzazione",
    "text": "SLM: Tecniche di Ottimizzazione\nI Large Language Model (LLM) hanno rivoluzionato l’elaborazione del linguaggio naturale, ma la loro distribuzione e ottimizzazione presentano sfide uniche. Un problema significativo è la tendenza degli LLM (e più in particolare degli SLM) a generare informazioni che sembrano plausibili ma di fatto errate, un fenomeno noto come allucinazione. Ciò si verifica quando i modelli producono contenuti che sembrano coerenti ma non sono basati sulla verità o sui fatti del mondo reale.\nAltre sfide includono le immense risorse computazionali richieste per l’addestramento e l’esecuzione di questi modelli, la difficoltà nel mantenere aggiornate le conoscenze all’interno del modello e la necessità di adattamenti specifici per dominio. Problemi di privacy sorgono anche quando si gestiscono dati sensibili durante l’addestramento o l’inferenza. Inoltre, garantire prestazioni coerenti in diverse attività e mantenere un uso etico di questi potenti strumenti presentano sfide continue. Affrontare questi problemi è fondamentale per l’implementazione efficace e responsabile di LLM in applicazioni del mondo reale.\nLe tecniche fondamentali per migliorare le prestazioni e l’efficienza di LLM (e SLM) sono Fine-tuning, Prompt engineering e Retrieval-Augmented Generation (RAG).\n\nFine-tuning, sebbene richieda più risorse, offre un modo per specializzare LLM per domini o attività particolari. Questo processo comporta un ulteriore addestramento del modello su set di dati attentamente curati, consentendogli di adattare la sua vasta conoscenza generale ad applicazioni specifiche. Il Fine-tuning può portare a miglioramenti sostanziali nelle prestazioni, specialmente in campi specializzati o per casi d’uso unici.\nPrompt engineering è in prima linea nell’ottimizzazione LLM. Creando attentamente prompt di input, possiamo guidare i modelli per produrre output più accurati e pertinenti. Questa tecnica comporta la strutturazione di query che sfruttano le conoscenze e le capacità pre-addestrate del modello, spesso incorporando esempi o istruzioni specifiche per modellare la risposta desiderata.\nRetrieval-Augmented Generation (RAG) rappresenta un altro approccio potente per migliorare le prestazioni LLM. Questo metodo combina la vasta conoscenza incorporata nei modelli pre-addestrati con la capacità di accedere e incorporare informazioni esterne aggiornate. Recuperando dati pertinenti per integrare il processo decisionale del modello, RAG può migliorare significativamente l’accuratezza e ridurre la probabilità di generare informazioni obsolete o false.\n\nPer le applicazioni edge, è più utile concentrarsi su tecniche come RAG che possono migliorare le prestazioni del modello senza dover effettuare un “fine-tuning” sul dispositivo. Esploriamolo.",
    "crumbs": [
      "Raspberry Pi",
      "Small Language Models (SLM)"
    ]
  },
  {
    "objectID": "contents/labs/raspi/llm/llm.it.html#implementazione-del-rag",
    "href": "contents/labs/raspi/llm/llm.it.html#implementazione-del-rag",
    "title": "Small Language Models (SLM)",
    "section": "Implementazione del RAG",
    "text": "Implementazione del RAG\nIn un’interazione di base tra un utente e un modello linguistico, l’utente pone una domanda, che viene inviata come prompt al modello. Il modello genera una risposta basata esclusivamente sulla sua conoscenza pre-addestrata. In un processo RAG, c’è un passaggio aggiuntivo tra la domanda dell’utente e la risposta del modello. La domanda dell’utente innesca un processo di recupero da una “knowledge base”.\n\n\nUn semplice progetto RAG\nEcco i passaggi per implementare una Retrieval Augmented Generation (RAG) di base:\n\nDeterminare il tipo di documenti che si utilizzeranno: I tipi migliori sono documenti da cui possiamo ottenere testo pulito e non oscurato. I PDF possono essere problematici perché sono progettati per la stampa, non per estrarre testo sensato. Per lavorare con i PDF, dovremmo ottenere il documento di origine o utilizzare strumenti per gestirli.\nSuddividere il testo in blocchi: Non possiamo archiviare il testo come un unico lungo flusso a causa delle limitazioni delle dimensioni del contesto e della potenziale confusione. La suddivisione in blocchi comporta la suddivisione del testo in parti più piccole. Il testo in blocchi ha molti modi, come conteggio dei caratteri, token, parole, paragrafi o sezioni. È anche possibile sovrapporre i blocchi.\nCrea embedding: Gli “embedding” [incorporamenti] sono rappresentazioni numeriche del testo che catturano il significato semantico. Creiamo incorporamenti passando ogni blocco di testo attraverso un particolare modello di embedding. Il modello genera un vettore, la cui lunghezza dipende dal modello di embedding utilizzato. Dovremmo estrarre uno (o più) modelli di embedding da Ollama, per eseguire questa attività. Ecco alcuni esempi di modelli di embedding disponibili su Ollama.\n\n\n\nModello\nDimensione del parametro\nDimensione dell’Embedding\n\n\n\n\nmxbai-embed-large\n334M\n1024\n\n\nnomic-embed-text\n137M\n768\n\n\nall-minilm\n23M\n384\n\n\n\n\nIn genere, dimensioni dell’embedding maggiori catturano informazioni più sfumate sull’input. Tuttavia, richiedono anche più risorse per l’elaborazione e un numero maggiore di parametri dovrebbe aumentare la latenza (ma anche la qualità della risposta).\n\nMemorizzare i blocchi e gli embedding in un database vettoriale: Avremo bisogno di un modo per trovare in modo efficiente i blocchi di testo più rilevanti per un dato prompt, ed è qui che entra in gioco un database vettoriale. Useremo Chromadb, un database vettoriale open source nativo IA, che semplifica la creazione di RAG creando conoscenze, fatti e competenze collegabili per LLM. Vengono memorizzati sia embedding che il testo sorgente per ogni blocco.\nCreare il prompt: Quando abbiamo una domanda, creiamo un embedding e interroghiamo il database vettoriale per i blocchi più simili. Poi, selezioniamo i primi risultati e includiamo il loro testo nel prompt.\n\nL’obiettivo di RAG è fornire al modello le informazioni più rilevanti dai nostri documenti, consentendogli di generare risposte più accurate e informative. Quindi, implementiamo un semplice esempio di un SLM che incorpora un set particolare di fatti sulle api (“Bee Facts”).\nAll’interno dell’ambiente ollama, si inserisce il comando nel terminale per l’installazione di Chromadb:\npip install ollama chromadb\nTiriamo fuori un modello di embedding intermedio, nomic-embed-text\nollama pull nomic-embed-text\nE creiamo una directory di lavoro:\ncd Documents/OLLAMA/\nmkdir RAG-simple-bee\ncd RAG-simple-bee/\nCreiamo un nuovo notebook Jupyter, 40-RAG-simple-bee per qualche esplorazione:\nImportare le librerie necessarie:\nimport ollama\nimport chromadb\nimport time\nE definiamo i modelli aor:\nEMB_MODEL = \"nomic-embed-text\"\nMODEL = 'llama3.2:3B'\nInizialmente, dovrebbe essere creata una “knowledge base” sui fatti sulle api. Ciò comporta la raccolta di documenti rilevanti e la loro conversione in embedding vettoriali. Questi embedding vengono poi archiviati in un database vettoriale, consentendo in seguito ricerche di similarità efficienti. Si immette il “document”, una base di “fatti sulle api” come un elenco:\ndocuments = [\n    \"Bee-keeping, also known as apiculture, involves the maintenance of bee \\\n    colonies, typically in hives, by humans.\",\n    \"The most commonly kept species of bees is the European honey bee (Apis \\\n    mellifera).\",\n    \n    ...\n    \n    \"There are another 20,000 different bee species in the world.\",  \n    \"Brazil alone has more than 300 different bee species, and the \\\n    vast majority, unlike western honey bees, don’t sting.\", \n    \"Reports written in 1577 by Hans Staden, mention three native bees \\\n    used by indigenous people in Brazil.\",\n    \"The indigenous people in Brazil used bees for medicine and food purposes\",\n    \"From Hans Staden report: probable species: mandaçaia (Melipona \\\n    quadrifasciata), mandaguari (Scaptotrigona postica) and jataí-amarela \\\n    (Tetragonisca angustula).\"\n]\n\nNon abbiamo bisogno di “suddividere” il documento qui perché useremo ogni elemento dell’elenco e un blocco.\n\nOra creeremo il nostro database di embedding vettoriale bee_facts e memorizzeremo il documento in esso:\nclient = chromadb.Client()\ncollection = client.create_collection(name=\"bee_facts\")\n\n# store each document in a vector embedding database\nfor i, d in enumerate(documents):\n  response = ollama.embeddings(model=EMB_MODEL, prompt=d)\n  embedding = response[\"embedding\"]\n  collection.add(\n    ids=[str(i)],\n    embeddings=[embedding],\n    documents=[d]\n  )\nra che abbiamo creato la nostra “Knowledge Base”, possiamo iniziare a fare query, recuperando dati da essa:\n\nQuery utente: Il processo inizia quando un utente pone una domanda, come “Quante api ci sono in una colonia? Chi depone le uova e in che quantità? E per quanto riguarda parassiti e malattie comuni?”\nprompt = \"How many bees are in a colony? Who lays eggs and how much? How about\\\n          common pests and diseases?\"\nQuery Embedding: La domanda dell’utente viene convertita in un embedding vettoriale utilizzando lo stesso modello di embedding  utilizzato per la knowledge base.\nresponse = ollama.embeddings(\n  prompt=prompt,\n  model=EMB_MODEL\n)\nRecupero di Documenti Pertinenti: Il sistema esegue una ricerca nella knowledge base utilizzando il “query embedding” per trovare i documenti più pertinenti (in questo caso, i 5 più probabili). Ciò avviene tramite una ricerca di similarità, che confronta la “query embedding” con gli embedding di documenti nel database.\nresults = collection.query(\n  query_embeddings=[response[\"embedding\"]],\n  n_results=5\n)\ndata = results['documents']\nPrompt Augmentation: Le informazioni rilevanti recuperate vengono combinate con la query utente originale per creare un prompt “aumentato”. Questo prompt ora contiene la domanda dell’utente e i fatti pertinenti dalla knowledge base.\nprompt=f\"Using this data: {data}. Respond to this prompt: {prompt}\",\nGenerazione di Risposte: Il prompt aumentato viene poi immesso in un modello linguistico, in questo caso il modello llama3.2:3b. Il modello utilizza questo contesto arricchito per generare una risposta completa. Parametri come temperatura, top_k e top_p vengono impostati per controllare la casualità e la qualità della risposta generata.\noutput = ollama.generate(\n  model=MODEL,\n  prompt=f\"Using this data: {data}. Respond to this prompt: {prompt}\",\n\n  options={\n    \"temperature\": 0.0,\n    \"top_k\":10,\n    \"top_p\":0.5                          }\n)\nResponse Delivery: Infine, il sistema restituisce all’utente la risposta generata.\nprint(output['response'])\nBased on the provided data, here are the answers to your questions:\n\n1. How many bees are in a colony?\nA typical bee colony can contain between 20,000 and 80,000 bees.\n\n2. Who lays eggs and how much?\nThe queen bee lays up to 2,000 eggs per day during peak seasons.\n\n3. What about common pests and diseases?\nCommon pests and diseases that affect bees include varroa mites, hive beetles,\nand foulbrood.\nCreiamo una funzione che ci aiuti a rispondere a nuove domande:\ndef rag_bees(prompt, n_results=5, temp=0.0, top_k=10, top_p=0.5):\n    start_time = time.perf_counter()  # Start timing\n    \n    # generate an embedding for the prompt and retrieve the data \n    response = ollama.embeddings(\n      prompt=prompt,\n      model=EMB_MODEL\n    )\n\n    \n    results = collection.query(\n      query_embeddings=[response[\"embedding\"]],\n      n_results=n_results\n    )\n\n    data = results['documents']\n    \n    # generate a response combining the prompt and data retrieved\n    output = ollama.generate(\n      model=MODEL,\n      prompt=f\"Using this data: {data}. Respond to this prompt: {prompt}\",\n\n      options={\n        \"temperature\": temp,\n        \"top_k\": top_k,\n        \"top_p\": top_p                          }\n    )\n\n    \n    print(output['response'])\n\n    \n    end_time = time.perf_counter()  # End timing\n    elapsed_time = round((end_time - start_time), 1)  # Calculate elapsed time\n    \n    print(f\"\\n [INFO] ==&gt; The code for model: {MODEL}, took {elapsed_time}s \\\n          to generate the answer.\\n\")\nOra possiamo creare delle query e chiamare la funzione:\nprompt = \"Are bees in Brazil?\"\nrag_bees(prompt)\nYes, bees are found in Brazil. According to the data, Brazil has more than 300\ndifferent bee species, and indigenous people in Brazil used bees for medicine and\nfood purposes. Additionally, reports from 1577 mention three native bees used by\nindigenous people in Brazil.\n\n [INFO] ==&gt; The code for model: llama3.2:3b, took 22.7s to generate the answer.\nA proposito, se il modello utilizzato supporta più lingue, possiamo utilizzarlo (ad esempio, il portoghese), anche se il set di dati è stato creato in inglese:\nprompt = \"Existem abelhas no Brazil?\"\nrag_bees(prompt)\nSim, existem abelhas no Brasil! De acordo com o relato de Hans Staden, há três \nespécies de abelhas nativas do Brasil que foram mencionadas: mandaçaia (Melipona\nquadrifasciata), mandaguari (Scaptotrigona postica) e jataí-amarela (Tetragonisca\nangustula). Além disso, o Brasil é conhecido por ter mais de 300 espécies diferentes de abelhas, a maioria das quais não é agressiva e não põe veneno.\n\n [INFO] ==&gt; The code for model: llama3.2:3b, took 54.6s to generate the answer.\n\n\nAndando Oltre\nI piccoli modelli LLM testati hanno funzionato bene sull’edge, sia nel testo che con le immagini, ma ovviamente avevano un’elevata latenza per quanto riguarda quest’ultima. Una combinazione di modelli specifici e dedicati può portare a risultati migliori; ad esempio, in casi reali, un modello di rilevamento degli oggetti (come YOLO) può ottenere una descrizione generale e un conteggio degli oggetti su un’immagine che, una volta passati a un LLM, possono aiutare a estrarre informazioni e azioni essenziali.\nSecondo Avi Baum, CTO di Hailo,\n\nNel vasto panorama dell’intelligenza artificiale (IA), uno dei viaggi più intriganti è stata l’evoluzione dell’IA nell’edge. Questo viaggio ci ha portato dalla classica visione artificiale ai regni dell’IA discriminativa, dell’IA potenziata e ora, alla rivoluzionaria frontiera dell’IA generativa. Ogni passo ci ha avvicinato a un futuro in cui i sistemi intelligenti si integrano perfettamente con la nostra vita quotidiana, offrendo un’esperienza immersiva non solo di percezione ma anche di creazione nel palmo della nostra mano.",
    "crumbs": [
      "Raspberry Pi",
      "Small Language Models (SLM)"
    ]
  },
  {
    "objectID": "contents/labs/raspi/llm/llm.it.html#conclusione",
    "href": "contents/labs/raspi/llm/llm.it.html#conclusione",
    "title": "Small Language Models (SLM)",
    "section": "Conclusione",
    "text": "Conclusione\nQuesto laboratorio ha dimostrato come un Raspberry Pi 5 può essere trasformato in un potente hub AI in grado di eseguire “large language model (LLM)” per analisi e approfondimenti dei dati in tempo reale e in loco utilizzando Ollama e Python. La versatilità e la potenza del Raspberry Pi, unite alle capacità di LLM leggeri come Llama 3.2 e LLaVa-Phi-3-mini, lo rendono un’eccellente piattaforma per applicazioni di edge computing.\nIl potenziale di esecuzione di LLM sull’edge si estende ben oltre la semplice elaborazione dei dati, come negli esempi di questo laboratorio. Ecco alcuni suggerimenti innovativi per l’utilizzo di questo progetto:\n1. Smart Home Automation:\n\nIntegrare gli SLM per interpretare i comandi vocali o analizzare i dati dei sensori per l’automazione domestica intelligente. Ciò potrebbe includere il monitoraggio e il controllo in tempo reale di dispositivi domestici, sistemi di sicurezza e gestione energetica, tutti elaborati localmente senza fare affidamento sui servizi cloud.\n\n2. Raccolta e Analisi dei Dati sul Campo:\n\nDistribuire gli SLM su Raspberry Pi in configurazioni remote o mobili per la raccolta e l’analisi dei dati in tempo reale. Può essere utilizzato in agricoltura per monitorare la salute delle colture, negli studi ambientali per il monitoraggio della fauna selvatica o nella risposta ai disastri per la consapevolezza della situazione e la gestione delle risorse.\n\n3. Strumenti Didattici:\n\nCreare strumenti didattici interattivi che sfruttano gli SLM per fornire feedback immediato, traduzione linguistica e tutoraggio. Può essere particolarmente utile nelle regioni in via di sviluppo con accesso limitato a tecnologie avanzate e connettività Internet.\n\n4. Applicazioni Sanitarie:\n\nUtilizzare gli SLM per la diagnosi medica e il monitoraggio dei pazienti. Possono fornire analisi in tempo reale dei sintomi e suggerire potenziali trattamenti. Può essere integrato in piattaforme di telemedicina o dispositivi sanitari portatili.\n\n5. Intelligence Aziendale Locale:\n\nImplementare gli SLM in ambienti di vendita al dettaglio o di piccole imprese per analizzare il comportamento dei clienti, gestire l’inventario e ottimizzare le operazioni. La capacità di elaborare i dati localmente garantisce la privacy e riduce la dipendenza dai servizi esterni.\n\n6. IoT Industriale:\n\nIntegrare gli SLM nei sistemi IoT industriali per manutenzione predittiva, controllo qualità e ottimizzazione dei processi. Raspberry Pi può fungere da unità di elaborazione dati localizzata, riducendo la latenza e migliorando l’affidabilità dei sistemi automatizzati.\n\n7. Veicoli Autonomi:\n\nUtilizzare gli SLM per elaborare dati sensoriali da veicoli autonomi, consentendo decisioni e navigazione in tempo reale. Questo può essere applicato a droni, robot e auto a guida autonoma per una maggiore autonomia e sicurezza.\n\n8. Patrimonio Culturale e Turismo:\n\nImplementare gli SLM per fornire siti del patrimonio culturale e guide museali interattive e informative. I visitatori possono utilizzare questi sistemi per ottenere informazioni e approfondimenti in tempo reale, migliorando la loro esperienza senza connettività Internet.\n\n9. Progetti Artistici e Creativi:\n\nUtilizzare SLM per analizzare e generare contenuti creativi, come musica, arte e letteratura. Questo può promuovere progetti innovativi nei settori creativi e consentire esperienze interattive uniche in mostre e spettacoli.\n\n10. Tecnologie Assistenziali Personalizzate:\n\nSviluppare tecnologie assistenziali per persone con disabilità, fornendo supporto personalizzato e adattivo tramite testo in tempo reale, traduzione di lingue e altri strumenti accessibili.",
    "crumbs": [
      "Raspberry Pi",
      "Small Language Models (SLM)"
    ]
  },
  {
    "objectID": "contents/labs/raspi/llm/llm.it.html#risorse",
    "href": "contents/labs/raspi/llm/llm.it.html#risorse",
    "title": "Small Language Models (SLM)",
    "section": "Risorse",
    "text": "Risorse\n\n10-Ollama_Python_Library notebook\n20-Ollama_Function_Calling notebook\n30-Function_Calling_with_images notebook\n40-RAG-simple-bee notebook\ncalc_distance_image python script",
    "crumbs": [
      "Raspberry Pi",
      "Small Language Models (SLM)"
    ]
  },
  {
    "objectID": "contents/labs/shared/kws_feature_eng/kws_feature_eng.it.html",
    "href": "contents/labs/shared/kws_feature_eng/kws_feature_eng.it.html",
    "title": "KWS Feature Engineering",
    "section": "",
    "text": "Panoramica\nIn questo tutorial pratico, l’enfasi è posta sul ruolo critico che la “feature engineering” [ingegneria delle funzionalità] svolge nell’ottimizzazione delle prestazioni dei modelli di machine learning applicati alle attività di classificazione audio, come il riconoscimento vocale. È essenziale essere consapevoli che le prestazioni di qualsiasi modello di apprendimento automatico dipendono in larga misura dalla qualità delle feature utilizzate e ci occuperemo della meccanica “sotto il cofano” dell’estrazione delle feature, concentrandoci principalmente sui Mel-frequency Cepstral Coefficient (MFCC), una pietra miliare nel campo dell’elaborazione del segnale audio.\nI modelli di apprendimento automatico, in particolare gli algoritmi tradizionali, non comprendono le onde audio. Comprendono i numeri disposti in modo significativo, ovvero le feature. Queste feature incapsulano le caratteristiche del segnale audio, rendendo più facile per i modelli distinguere tra suoni diversi.",
    "crumbs": [
      "Lab Condivisi",
      "KWS Feature Engineering"
    ]
  },
  {
    "objectID": "contents/labs/shared/kws_feature_eng/kws_feature_eng.it.html#panoramica",
    "href": "contents/labs/shared/kws_feature_eng/kws_feature_eng.it.html#panoramica",
    "title": "KWS Feature Engineering",
    "section": "",
    "text": "Questo tutorial si occuperà della generazione di feature specificamente per la classificazione audio. Ciò può essere particolarmente interessante per l’applicazione di machine learning a una varietà di dati audio, sia per il riconoscimento vocale, la categorizzazione musicale, la classificazione degli insetti basata sui suoni del battito delle ali o altre attività di analisi del suono",
    "crumbs": [
      "Lab Condivisi",
      "KWS Feature Engineering"
    ]
  },
  {
    "objectID": "contents/labs/shared/kws_feature_eng/kws_feature_eng.it.html#il-kws",
    "href": "contents/labs/shared/kws_feature_eng/kws_feature_eng.it.html#il-kws",
    "title": "KWS Feature Engineering",
    "section": "Il KWS",
    "text": "Il KWS\nL’applicazione TinyML più comune è Keyword Spotting (KWS), un sottoinsieme del campo più ampio del riconoscimento vocale. Mentre il riconoscimento vocale generale trascrive tutte le parole pronunciate in testo, il Keyword Spotting si concentra sul rilevamento di “parole chiave” o “wake word” specifiche in un flusso audio continuo. Il sistema è addestrato a riconoscere queste parole chiave come frasi o parole predefinite, come yes o no. In breve, KWS è una forma specializzata di riconoscimento vocale con il suo set di sfide e requisiti.\nEcco un tipico processo KWS che utilizza MFCC Feature Converter:\n\n\nApplicazioni di KWS\n\nAssistenti Vocali: In dispositivi come Alexa di Amazon o Google Home, KWS viene utilizzato per rilevare la wake word (“Alexa” o “Hey Google”) per attivare il dispositivo.\nComandi Attivati Tramite Voce: In contesti automobilistici o industriali, KWS può essere utilizzato per avviare comandi specifici come “Avvia motore” o “Spegni luci”.\nSistemi di Sicurezza: I sistemi di sicurezza attivati tramite voce possono utilizzare KWS per autenticare gli utenti in base a una passphrase pronunciata.\nServizi di Telecomunicazione: Le linee del servizio clienti possono utilizzare KWS per instradare le chiamate in base a parole chiave pronunciate.\n\n\n\nDifferenze dal Riconoscimento Vocale Generale\n\nEfficienza Computazionale: KWS è solitamente progettato per essere meno intensivo dal punto di vista computazionale rispetto al riconoscimento vocale completo, poiché deve riconoscere solo un piccolo set di frasi.\nElaborazione in Tempo Reale: KWS spesso funziona in tempo reale ed è ottimizzato per il rilevamento a bassa latenza delle parole chiave.\nVincoli di Risorse: I modelli KWS sono spesso progettati per essere leggeri, in modo da poter essere eseguiti su dispositivi con risorse computazionali limitate, come microcontrollori o telefoni cellulari.\nAttività Mirata: Mentre i modelli di riconoscimento vocale generali sono addestrati per gestire un’ampia gamma di vocabolario e accenti, i modelli KWS sono ottimizzati per riconoscere parole chiave specifiche, spesso in modo accurato in ambienti rumorosi.",
    "crumbs": [
      "Lab Condivisi",
      "KWS Feature Engineering"
    ]
  },
  {
    "objectID": "contents/labs/shared/kws_feature_eng/kws_feature_eng.it.html#panoramica-sui-segnali-audio",
    "href": "contents/labs/shared/kws_feature_eng/kws_feature_eng.it.html#panoramica-sui-segnali-audio",
    "title": "KWS Feature Engineering",
    "section": "Panoramica sui Segnali Audio",
    "text": "Panoramica sui Segnali Audio\nComprendere le proprietà di base dei segnali audio è fondamentale per un’estrazione efficace delle feature [caratteristiche] e, in ultima analisi, per applicare con successo gli algoritmi di apprendimento automatico nelle attività di classificazione audio. I segnali audio sono forme d’onda complesse che catturano le fluttuazioni della pressione dell’aria nel tempo. Questi segnali possono essere caratterizzati da diversi attributi fondamentali: frequenza di campionamento, frequenza e ampiezza.\n\nFrequenza e Ampiezza: La Frequenza si riferisce al numero di oscillazioni che una forma d’onda subisce per unità di tempo e si misura anche in Hz. Nel contesto dei segnali audio, frequenze diverse corrispondono a pitch [toni] diversi. L’Ampiezza, d’altra parte, misura la grandezza delle oscillazioni e si correla con l’intensità del suono. Sia la frequenza che l’ampiezza sono feature essenziali che catturano le qualità tonali e ritmiche dei segnali audio.\nFrequenza di Campionamento: La frequenza di campionamento, spesso indicata in Hertz (Hz), definisce il numero di campioni presi al secondo durante la digitalizzazione di un segnale analogico. Una frequenza di campionamento più elevata consente una rappresentazione digitale più accurata del segnale, ma richiede anche più risorse di elaborazione. Le frequenze di campionamento tipiche includono 44,1 kHz per l’audio di qualità CD e 16 kHz o 8 kHz per le attività di riconoscimento vocale. Comprendere i compromessi nella selezione di una frequenza di campionamento appropriata è essenziale per bilanciare accuratezza ed efficienza computazionale. In generale, con i progetti TinyML, lavoriamo con 16 KHz. Sebbene i toni musicali possano essere uditi a frequenze fino a 20 kHz, la voce raggiunge il massimo a 8 kHz. I sistemi telefonici tradizionali utilizzano una frequenza di campionamento di 8 kHz.\n\n\nPer una rappresentazione accurata del segnale, la frequenza di campionamento deve essere almeno il doppio della frequenza più alta presente nel segnale.\n\n\nDominio del Tempo e Dominio della Frequenza: I segnali audio possono essere analizzati nei domini del tempo e della frequenza. Nel dominio del tempo, un segnale è rappresentato come una forma d’onda in cui l’ampiezza è tracciata in funzione del tempo. Questa rappresentazione aiuta a osservare feature temporali come inizio e durata, ma le caratteristiche tonali del segnale non sono ben evidenziate. Al contrario, una rappresentazione del dominio della frequenza fornisce una vista delle frequenze costituenti del segnale e delle rispettive ampiezze, in genere ottenute tramite una trasformata di Fourier. Questo è prezioso per le attività che richiedono la comprensione del contenuto spettrale del segnale, come l’identificazione di note musicali o fonemi vocali (il nostro caso).\n\nL’immagine seguente mostra le parole YES e NO con rappresentazioni tipiche nei domini del tempo (audio grezzo) e della frequenza:\n\n\nPerché l’Audio Grezzo No?\nSebbene l’utilizzo diretto di dati audio grezzi per attività di apprendimento automatico possa sembrare allettante, questo approccio presenta diverse sfide che lo rendono meno adatto alla creazione di modelli solidi ed efficienti.\nL’utilizzo di dati audio grezzi per il Keyword Spotting (KWS), ad esempio, su dispositivi TinyML pone delle sfide dovute alla sua elevata dimensionalità (utilizzando una frequenza di campionamento di 16 kHz), alla complessità computazionale per l’acquisizione di feature temporali, alla suscettibilità al rumore e alla mancanza di feature semanticamente significative, rendendo le tecniche di estrazione di caratteristiche come MFCC una scelta più pratica per applicazioni con risorse limitate.\nEcco alcuni dettagli aggiuntivi sui problemi critici associati all’utilizzo di audio grezzi:\n\nAlta Dimensionalità: I segnali audio, in particolare quelli campionati ad alte velocità, generano grandi quantità di dati. Ad esempio, una clip audio di 1 secondo campionata a 16 kHz avrà 16.000 punti singoli. I dati ad alta dimensionalità aumentano la complessità computazionale, portando a tempi di training più lunghi e costi computazionali più elevati, rendendoli poco pratici per ambienti con risorse limitate. Inoltre, l’ampia gamma dinamica dei segnali audio richiede una quantità significativa di bit per campione, mentre trasmette poche informazioni utili.\nDipendenze Temporali: I segnali audio grezzi hanno strutture temporali che i semplici modelli di apprendimento automatico potrebbero trovare difficili da catturare. Sebbene le reti neurali ricorrenti come LSTM possano modellare tali dipendenze, sono computazionalmente intensive e difficili da addestrare su dispositivi di piccole dimensioni.\nRumore e Variabilità: I segnali audio grezzi spesso contengono rumore di fondo e altri elementi non essenziali che influenzano le prestazioni del modello. Inoltre, lo stesso suono può avere caratteristiche diverse in base a vari fattori come la distanza dal microfono, l’orientamento della sorgente sonora e le proprietà acustiche dell’ambiente, aggiungendo complessità ai dati.\nMancanza di Significato Semantico: L’audio grezzo non contiene intrinsecamente feature semanticamente significative per le attività di classificazione. Feature come tono, tempo e feature spettrali, che possono essere cruciali per il riconoscimento vocale, non sono direttamente accessibili dai dati della forma d’onda grezza.\nRidondanza del Segnale: I segnali audio spesso contengono informazioni ridondanti, con alcune porzioni del segnale che contribuiscono poco o per niente al compito da svolgere. Questa ridondanza può rendere l’apprendimento inefficiente e potenzialmente portare a un overfitting.\n\nPer queste ragioni, tecniche di estrazione di feature come i Mel-frequency Cepstral Coefficients (MFCCs), le Mel-Frequency Energies (MFEs) e gli spettrogrammi semplici sono comunemente utilizzate per trasformare i dati audio grezzi in un formato più gestibile e informativo. Queste feature catturano le caratteristiche essenziali del segnale audio riducendo dimensionalità e rumore, facilitando un apprendimento automatico più efficace.",
    "crumbs": [
      "Lab Condivisi",
      "KWS Feature Engineering"
    ]
  },
  {
    "objectID": "contents/labs/shared/kws_feature_eng/kws_feature_eng.it.html#panoramica-sugli-mfcc",
    "href": "contents/labs/shared/kws_feature_eng/kws_feature_eng.it.html#panoramica-sugli-mfcc",
    "title": "KWS Feature Engineering",
    "section": "Panoramica sugli MFCC",
    "text": "Panoramica sugli MFCC\n\nCosa sono gli MFCC?\nI Mel-frequency Cepstral Coefficients (MFCC) sono un set di feature derivate dal contenuto spettrale di un segnale audio. Si basano sulle percezioni uditive umane e sono comunemente utilizzati per catturare le feature fonetiche di un segnale audio. Gli MFCC vengono calcolati tramite un processo in più fasi che include pre-enfasi, inquadratura, windowing, applicazione della Fast Fourier Transform (FFT) per convertire il segnale nel dominio della frequenza e, infine, applicazione della Discrete Cosine Transform (DCT). Il risultato è una rappresentazione compatta delle caratteristiche spettrali del segnale audio originale.\nL’immagine seguente mostra le parole YES e NO nella loro rappresentazione MFCC:\n\n\nQuesto video spiega i Mel Frequency Cepstral Coefficients (MFCC) e come calcolarli.\n\n\n\nPerché gli MFCC sono importanti?\nGli MFCC sono fondamentali per diversi motivi, in particolare nel contesto di Keyword Spotting (KWS) e TinyML:\n\nRiduzione della Dimensionalità: Gli MFCC catturano le caratteristiche spettrali essenziali del segnale audio riducendo significativamente la dimensionalità dei dati, rendendoli ideali per applicazioni TinyML con risorse limitate.\nRobustezza: Gli MFCC sono meno sensibili al rumore e alle variazioni di tono e ampiezza, offrendo un set di funzionalità più stabile e robusto per le attività di classificazione audio.\nModellazione del Sistema Uditivo Umano: La scala Mel negli MFCC approssima la risposta dell’orecchio umano a diverse frequenze, rendendoli pratici per il riconoscimento vocale in cui è desiderata una percezione simile a quella umana.\nEfficienza Computazionale: Il processo di calcolo degli MFCC è efficiente dal punto di vista computazionale, rendendolo adatto per applicazioni in tempo reale su hardware con risorse computazionali limitate.\n\nIn sintesi, gli MFCC offrono un equilibrio tra ricchezza di informazioni ed efficienza computazionale, rendendoli popolari per le attività di classificazione audio, in particolare in ambienti limitati come TinyML.\n\n\nCalcolo degli MFCC\nIl calcolo dei Mel-frequency Cepstral Coefficients (MFCCs) comporta diversi passaggi chiave. Esaminiamoli, che sono particolarmente importanti per le attività di Keyword Spotting (KWS) sui dispositivi TinyML.\n\nPre-enfasi: Il primo passaggio è la pre-enfasi, che viene applicata per accentuare le componenti ad alta frequenza del segnale audio e bilanciare lo spettro di frequenza. Ciò si ottiene applicando un filtro che amplifica la differenza tra campioni consecutivi. La formula per la pre-enfasi è: y(t) = x(t) - \\(\\alpha\\) x(t-1), dove \\(\\alpha\\) è il fattore di pre-enfasi, in genere intorno a 0.97.\nFraming: I segnali audio sono divisi in frame brevi (la frame length), in genere da 20 a 40 millisecondi. Ciò si basa sul presupposto che le frequenze in un segnale siano stazionarie per un breve periodo. Il framing aiuta ad analizzare il segnale in slot di tempo così piccoli. Il frame stride (o step) sposterà un frame e quello adiacente. Questi step potrebbero essere sequenziali o sovrapposti.\nWindowing: Ogni frame viene quindi sottoposto a windowing per ridurre al minimo le discontinuità ai confini del frame. Una funzione window comunemente utilizzata è la finestra di Hamming. Il windowing prepara il segnale per una trasformata di Fourier riducendo al minimo gli effetti alle estremità. L’immagine sottostante mostra tre frame (10, 20 e 30) e i campioni di tempo dopo il windowing (notare che la lunghezza del frame e il frame stride sono di 20 ms):\n\n\n\nFast Fourier Transform (FFT) La Fast Fourier Transform (FFT) viene applicata a ciascun frame sottoposto a windowing per convertirlo dal dominio del tempo al dominio della frequenza. La FFT ci fornisce una rappresentazione a valori complessi che include sia informazioni di ampiezza che di fase. Tuttavia, per gli MFCC, solo l’ampiezza viene utilizzata per calcolare lo spettro di potenza. Lo spettro di potenza è il quadrato dello spettro di ampiezza e misura l’energia presente in ogni componente di frequenza.\n\n\nLo spettro di potenza \\(P(f)\\) di un segnale \\(x(t)\\) è definito come \\(P(f) = |X(f)|^2\\), dove \\(X(f)\\) è la trasformata di Fourier di \\(x(t)\\). Elevando al quadrato l’ampiezza della trasformata di Fourier, mettiamo in risalto le frequenze più forti rispetto a quelle più deboli, catturando così caratteristiche spettrali più rilevanti del segnale audio. Ciò è importante in applicazioni come la classificazione audio, il riconoscimento vocale e il Keyword Spotting (KWS), in cui l’attenzione è rivolta all’identificazione di distinti pattern di frequenza che caratterizzano diverse classi di audio o fonemi nel parlato.\n\n\n\nMel Filter Banks: Il dominio di frequenza viene poi mappato sulla scala Mel, che approssima la risposta dell’orecchio umano a diverse frequenze. L’idea è di estrarre più feature (più filtri) nelle frequenze più basse e meno in quelle alte. Pertanto, funziona bene sui suoni distinti dall’orecchio umano. In genere, da 20 a 40 filtri triangolari estraggono le energie di frequenza Mel. Queste energie vengono poi trasformate in logaritmo per convertire i fattori moltiplicativi in quelli additivi, rendendoli più adatti per un’ulteriore elaborazione.\n\n\n\nDiscrete Cosine Transform (DCT): L’ultimo passaggio consiste nell’applicare la Trasformata discreta del coseno (DCT) alle energie del log Mel. La DCT aiuta a de-correlare le energie, comprimendo efficacemente i dati e mantenendo solo le feature più discriminanti. Di solito, vengono mantenuti i primi 12-13 coefficienti DCT, formando il vettore finale delle feature MFCC.",
    "crumbs": [
      "Lab Condivisi",
      "KWS Feature Engineering"
    ]
  },
  {
    "objectID": "contents/labs/shared/kws_feature_eng/kws_feature_eng.it.html#pratica-con-python",
    "href": "contents/labs/shared/kws_feature_eng/kws_feature_eng.it.html#pratica-con-python",
    "title": "KWS Feature Engineering",
    "section": "Pratica con Python",
    "text": "Pratica con Python\nApplichiamo quanto discusso lavorando su un campione audio reale. Aprire il notebook su Google CoLab ed estrarre le funzionalità MLCC sui campioni audio: [Open In Colab]",
    "crumbs": [
      "Lab Condivisi",
      "KWS Feature Engineering"
    ]
  },
  {
    "objectID": "contents/labs/shared/kws_feature_eng/kws_feature_eng.it.html#conclusione",
    "href": "contents/labs/shared/kws_feature_eng/kws_feature_eng.it.html#conclusione",
    "title": "KWS Feature Engineering",
    "section": "Conclusione",
    "text": "Conclusione\nQuale Tecnica di Estrazione delle Feature dovremmo usare?\nI Mel-frequency Cepstral Coefficient (MFCC), le Mel-Frequency Energie (MFE), o lo Spettrogramma sono tecniche per rappresentare dati audio, che sono spesso utili in diversi contesti.\nIn generale, gli MFCC sono più focalizzati sulla cattura dell’inviluppo dello spettro di potenza, il che li rende meno sensibili ai dettagli spettrali a grana fine ma più robusti al rumore. Questo è spesso auspicabile per le attività relative al parlato. D’altro canto, gli spettrogrammi o MFE conservano informazioni di frequenza più dettagliate, il che può essere vantaggioso in attività che richiedono discriminazione basata su contenuti spettrali a grana fine.\n\nGli MFCC sono particolarmente efficaci per:\n\nRiconoscimento Vocale: Gli MFCC sono eccellenti per identificare il contenuto fonetico nei segnali vocali.\nIdentificazione del Parlante: Possono essere utilizzati per distinguere tra diversi parlanti in base alle caratteristiche della voce.\nRiconoscimento delle Emozioni: Gli MFCC possono catturare le sfumature nel parlato indicative di stati emotivi.\nKeyword Spotting: Specialmente in TinyML, dove la bassa complessità computazionale e le piccole dimensioni delle funzionalità sono cruciali.\n\n\n\nGli spettrogrammi o MFE sono spesso più adatti per:\n\nAnalisi Musicale: Gli spettrogrammi possono catturare strutture armoniche e timbriche nella musica, il che è essenziale per attività come la classificazione del genere, il riconoscimento degli strumenti o la trascrizione musicale.\nClassificazione dei Suoni Ambientali: Nel riconoscimento di suoni ambientali non vocali (ad esempio pioggia, vento, traffico), lo spettrogramma completo può fornire feature più discriminanti.\nIdentificazione del Canto degli Uccelli: I dettagli intricati dei richiami degli uccelli sono spesso meglio catturati utilizzando gli spettrogrammi.\nElaborazione del Segnale Bioacustico: In applicazioni come l’analisi dei richiami dei delfini o dei pipistrelli, le informazioni di frequenza a grana fine in uno spettrogramma possono essere essenziali.\nAudio Quality Assurance: Gli spettrogrammi sono spesso utilizzati nell’analisi audio professionale per identificare rumori indesiderati, clic o altri artefatti.",
    "crumbs": [
      "Lab Condivisi",
      "KWS Feature Engineering"
    ]
  },
  {
    "objectID": "contents/labs/shared/kws_feature_eng/kws_feature_eng.it.html#risorse",
    "href": "contents/labs/shared/kws_feature_eng/kws_feature_eng.it.html#risorse",
    "title": "KWS Feature Engineering",
    "section": "Risorse",
    "text": "Risorse\n\nNotebook Audio_Data_Analysis Colab",
    "crumbs": [
      "Lab Condivisi",
      "KWS Feature Engineering"
    ]
  },
  {
    "objectID": "contents/labs/shared/dsp_spectral_features_block/dsp_spectral_features_block.it.html",
    "href": "contents/labs/shared/dsp_spectral_features_block/dsp_spectral_features_block.it.html",
    "title": "Blocco delle Feature Spettrali DSP",
    "section": "",
    "text": "Panoramica\nI progetti TinyML correlati al movimento (o alle vibrazioni) coinvolgono dati da IMU (solitamente accelerometri e giroscopi). Questi dataset di tipo temporale dovrebbero essere pre-elaborati prima di inserirli in un training di modello di apprendimento automatico, che è un’area impegnativa per l’apprendimento automatico embedded. Tuttavia, Edge Impulse aiuta a superare questa complessità con la sua fase di pre-elaborazione dell’elaborazione del segnale digitale (DSP) e, più specificamente, il Spectral Features Block per i sensori inerziali.\nMa come funziona internamente? Analizziamolo nel dettaglio.",
    "crumbs": [
      "Lab Condivisi",
      "Blocco delle Feature Spettrali DSP"
    ]
  },
  {
    "objectID": "contents/labs/shared/dsp_spectral_features_block/dsp_spectral_features_block.it.html#estrazione-delle-feature-di-revisione",
    "href": "contents/labs/shared/dsp_spectral_features_block/dsp_spectral_features_block.it.html#estrazione-delle-feature-di-revisione",
    "title": "Blocco delle Feature Spettrali DSP",
    "section": "Estrazione delle Feature di Revisione",
    "text": "Estrazione delle Feature di Revisione\nL’estrazione di feature [caratteristiche] da un dataset catturato con sensori inerziali, come gli accelerometri, comporta l’elaborazione e l’analisi dei dati grezzi. Gli accelerometri misurano l’accelerazione di un oggetto lungo uno o più assi (in genere tre, indicati come X, Y e Z). Queste misure possono essere utilizzate per comprendere vari aspetti del movimento dell’oggetto, come pattern di movimento e vibrazioni. Ecco una panoramica di alto livello del processo:\nRaccolta dati: Per prima cosa, dobbiamo raccogliere i dati dagli accelerometri. A seconda dell’applicazione, i dati possono essere raccolti a diverse frequenze di campionamento. È essenziale assicurarsi che la frequenza di campionamento sia sufficientemente alta da catturare le dinamiche rilevanti del movimento studiato (la frequenza di campionamento dovrebbe essere almeno il doppio della massima frequenza presente nel segnale).\nPre-elaborazione dei dati: I dati grezzi dell’accelerometro possono essere rumorosi e contenere errori o informazioni irrilevanti. I passaggi di pre-elaborazione, come il filtraggio e la normalizzazione, possono aiutare a pulire e standardizzare i dati, rendendoli più adatti all’estrazione di feature.\n\nStudio non esegue la normalizzazione o la standardizzazione, quindi a volte, quando si lavora con Sensor Fusion [gruppi di sensori], potrebbe essere necessario eseguire questo passaggio prima di caricare i dati in Studio. Ciò è particolarmente cruciale nei progetti di gruppi di sensori, come visto in questo tutorial, Sensor Data Fusion with Spresense and CommonSense.\n\nSegmentazione: A seconda della natura dei dati e dell’applicazione, potrebbe essere necessario dividere i dati in segmenti più piccoli o finestre. Ciò può aiutare a concentrarsi su eventi o attività specifici all’interno del dataset, rendendo l’estrazione di feature più gestibile e significativa. La scelta della window size [dimensione della finestra] e della sovrapposizione (window span) dipende dall’applicazione e dalla frequenza degli eventi di interesse. Come regola generale, dovremmo provare a catturare un paio di “cicli di dati”.\nEstrazione delle feature: Una volta che i dati sono stati pre-elaborati e segmentati, è possibile estrarre feature che descrivono le caratteristiche del movimento. Alcune feature tipiche estratte dai dati dell’accelerometro includono:\n\nLe feature del Time-domain descrivono le proprietà statistiche dei dati all’interno di ciascun segmento, come media, mediana, deviazione standard, asimmetria, curtosi e tasso di attraversamento dello zero.\nLe feature Frequency-domain si ottengono trasformando i dati nel dominio della frequenza utilizzando tecniche come la Fast Fourier Transform (FFT). Alcune feature tipiche del dominio della frequenza includono lo spettro di potenza, l’energia spettrale, le frequenze dominanti (ampiezza e frequenza) e l’entropia spettrale.\nLe feature del dominio Time-frequency combinano le informazioni del dominio del tempo e della frequenza, come la Short-Time Fourier Transform (STFT) o la Discrete Wavelet Transform (DWT). Possono fornire una comprensione più dettagliata di come il contenuto di frequenza del segnale cambia nel tempo.\n\nIn molti casi, il numero di feature estratte può essere elevato, il che può portare a un overfitting o a una maggiore complessità computazionale. Le tecniche di selezione delle feature, come le informazioni reciproche, i metodi basati sulla correlazione o l’analisi delle componenti principali (PCA), possono aiutare a identificare le feature più rilevanti per una determinata applicazione e ridurre la dimensionalità del dataset. Studio può aiutare con tali calcoli rilevanti per le feature.\nEsploriamo più in dettaglio un tipico progetto di classificazione del movimento TinyML trattato in questa serie di esercitazioni pratiche.",
    "crumbs": [
      "Lab Condivisi",
      "Blocco delle Feature Spettrali DSP"
    ]
  },
  {
    "objectID": "contents/labs/shared/dsp_spectral_features_block/dsp_spectral_features_block.it.html#un-progetto-tinyml-di-motion-classification",
    "href": "contents/labs/shared/dsp_spectral_features_block/dsp_spectral_features_block.it.html#un-progetto-tinyml-di-motion-classification",
    "title": "Blocco delle Feature Spettrali DSP",
    "section": "Un progetto TinyML di Motion Classification",
    "text": "Un progetto TinyML di Motion Classification\n\nNel progetto, Motion Classification and Anomaly Detection, abbiamo simulato sollecitazioni meccaniche nel trasporto, dove il nostro problema era classificare quattro classi di movimento:\n\nMaritime (Pallet in navi)\nTerrestrial (pallet in un Camion o Treno)\nLift [Sollevamento] (pallet movimentati da Carrello elevatore)\nIdle (pallet in Magazzini)\n\nGli accelerometri hanno fornito i dati sul pallet (o il container).\n\nDi seguito è riportato un campione (dati grezzi) di 10 secondi, acquisito con una frequenza di campionamento di 50 Hz:\n\n\nIl risultato è simile quando questa analisi viene eseguita su un altro set di dati con lo stesso principio, utilizzando una frequenza di campionamento diversa, 62.5Hz invece di 50Hz.",
    "crumbs": [
      "Lab Condivisi",
      "Blocco delle Feature Spettrali DSP"
    ]
  },
  {
    "objectID": "contents/labs/shared/dsp_spectral_features_block/dsp_spectral_features_block.it.html#pre-elaborazione-dei-dati",
    "href": "contents/labs/shared/dsp_spectral_features_block/dsp_spectral_features_block.it.html#pre-elaborazione-dei-dati",
    "title": "Blocco delle Feature Spettrali DSP",
    "section": "Pre-elaborazione dei Dati",
    "text": "Pre-elaborazione dei Dati\nI dati grezzi acquisiti dall’accelerometro (dati di “serie temporali”) devono essere convertiti in “dati tabellari” utilizzando uno dei tipici metodi di estrazione delle feature descritti nell’ultima sezione.\nDovremmo segmentare i dati utilizzando una finestra scorrevole sui dati campione per l’estrazione delle feature. Il progetto ha acquisito i dati dell’accelerometro ogni 10 secondi con una frequenza di campionamento di 62.5 Hz. Una finestra di 2 secondi acquisisce 375 punti dati (3 assi x 2 secondi x 62.5 campioni). La finestra scorre ogni 80 ms, creando un dataset più grande in cui ogni istanza ha 375 “feature grezze”.\n\nSu Studio, la versione precedente (V1) dello Spectral Analysis Block estraeva come caratteristiche del dominio del tempo solo l’RMS e, per il dominio della frequenza, i picchi e la frequenza (utilizzando FFT) e le caratteristiche di potenza (PSD) del segnale nel tempo, risultando in un set di dati tabellari fisso di 33 caratteristiche (11 per ogni asse),\n\nQuelle 33 feature erano il tensore di input di un classificatore di reti neurali.\nNel 2022, Edge Impulse ha rilasciato la versione 2 dello Spectral Analysis block, che esploreremo qui.\n\nEdge Impulse - Spectral Analysis Block V.2 funzionamento\nNella Versione 2, le caratteristiche statistiche del dominio del tempo per asse/canale sono:\n\nRMS\nSkewness\nKurtosis\n\nE le caratteristiche spettrali del dominio della frequenza per asse/canale sono:\n\nSpectral Power [Potenza spettrale]\nSkewness [asimmetria] (nella prossima versione)\nCurtosi (nella prossima versione)\n\nIn questo link possiamo avere maggiori dettagli sull’estrazione delle feature.\n\nClonare il progetto pubblico. Si può anche seguire la spiegazione, giocando col codice usando il mio Google CoLab Notebook: Edge Impulse Spectral Analysis Block Notebook.\n\nSi inizia importando le librerie:\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport math\nfrom scipy.stats import skew, kurtosis\nfrom scipy import signal\nfrom scipy.signal import welch\nfrom scipy.stats import entropy\nfrom sklearn import preprocessing\nimport pywt\n\nplt.rcParams['figure.figsize'] = (12, 6)\nplt.rcParams['lines.linewidth'] = 3\nDal progetto studiato, scegliamo un campione di dati dagli accelerometri come di seguito:\n\nDimensione della finestra di 2 secondi: [2,000] ms\nFrequenza di campionamento: [62.5] Hz\nSceglieremo il filtro [None] (per semplicità) e una\nLunghezza FFT: [16].\n\nf =  62.5 # Hertz\nwind_sec = 2 # seconds\nFFT_Lenght = 16\naxis = ['accX', 'accY', 'accZ']\nn_sensors = len(axis)\n\nSelezionando Raw Features nella scheda Studio Spectral Analysis, possiamo copiare tutti i 375 punti dati di una particolare finestra di 2 secondi negli appunti.\n\nIncollare i punti dati in una nuova variabile data:\ndata=[-5.6330, 0.2376, 9.8701, -5.9442, 0.4830, 9.8701, -5.4217, ...]\nNo_raw_features = len(data)\nN = int(No_raw_features/n_sensors)\nLe feature grezze totali sono 375, ma lavoreremo con ogni asse singolarmente, dove N= 125 (numero di campioni per asse).\nVogliamo capire come Edge Impulse ottiene le feature elaborate.\n\nQuindi, si devono anche incollare le feature elaborate su una variabile (per confrontare le feature calcolate in Python con quelle fornite da Studio):\nfeatures = [2.7322, -0.0978, -0.3813, 2.3980, 3.8924, 24.6841, 9.6303, ...]\nN_feat = len(features)\nN_feat_axis = int(N_feat/n_sensors)\nIl numero totale di feature elaborate è 39, il che significa 13 feature/asse.\nOsservando attentamente queste 13 feature, ne troveremo 3 per il dominio del tempo (RMS, Skewness e Kurtosis):\n\n[rms] [skew] [kurtosis]\n\ne 10 per il dominio della frequenza (ci torneremo più avanti).\n\n[spectral skew][spectral kurtosis][Spectral Power 1] ... [Spectral Power 8]\n\nSuddivisione dei dati grezzi per sensore\nI dati hanno campioni da tutti gli assi; dividiamoli e tracciamo un grafico separatamente:\ndef plot_data(sensors, axis, title):\n    [plt.plot(x, label=y) for x,y in zip(sensors, axis)]\n    plt.legend(loc='lower right')\n    plt.title(title)\n    plt.xlabel('#Sample')\n    plt.ylabel('Value')\n    plt.box(False)\n    plt.grid()\n    plt.show()\n\naccX = data[0::3]\naccY = data[1::3]\naccZ = data[2::3]\nsensors = [accX, accY, accZ] \nplot_data(sensors, axis, 'Raw Features')\n\nSottrazione della media\nSuccessivamente, dovremmo sottrarre la media dai dati. La sottrazione della media da un set di dati è una comune fase di pre-elaborazione dei dati in statistica e apprendimento automatico. Lo scopo della sottrazione della media dai dati è di centrare i dati attorno allo zero. Questo è importante perché può rivelare pattern e relazioni che potrebbero essere nascosti se i dati non sono centrati.\nEcco alcuni motivi specifici per cui la sottrazione della media può essere utile:\n\nSemplifica l’analisi: Centrando i dati, la media diventa zero, rendendo alcuni calcoli più semplici e facili da interpretare.\nRimuove la distorsione: Se i dati sono distorti, sottraendo la media è possibile rimuoverli e consentire un’analisi più accurata.\nPuò rivelare pattern: Centrare i dati può aiutare a scoprire pattern che potrebbero essere nascosti se i dati non sono centrati. Ad esempio, centrare i dati può aiutare a identificare i trend nel tempo se si analizza un set di dati di serie temporali.\nPuò migliorare le prestazioni: in alcuni algoritmi di apprendimento automatico, centrare i dati può migliorare le prestazioni riducendo l’influenza dei valori anomali e rendendo i dati più facilmente confrontabili. Nel complesso, sottrarre la media è una tecnica semplice ma potente che può essere utilizzata per migliorare l’analisi e l’interpretazione dei dati.\n\ndtmean = [(sum(x)/len(x)) for x in sensors]\n[print('mean_'+x+'= ', round(y, 4)) for x,y in zip(axis, dtmean)][0]\n\naccX = [(x - dtmean[0]) for x in accX]\naccY = [(x - dtmean[1]) for x in accY]\naccZ = [(x - dtmean[2]) for x in accZ]\nsensors = [accX, accY, accZ]\n\nplot_data(sensors, axis, 'Raw Features - Subctract the Mean')",
    "crumbs": [
      "Lab Condivisi",
      "Blocco delle Feature Spettrali DSP"
    ]
  },
  {
    "objectID": "contents/labs/shared/dsp_spectral_features_block/dsp_spectral_features_block.it.html#feature-statistiche-del-dominio-del-tempo",
    "href": "contents/labs/shared/dsp_spectral_features_block/dsp_spectral_features_block.it.html#feature-statistiche-del-dominio-del-tempo",
    "title": "Blocco delle Feature Spettrali DSP",
    "section": "Feature Statistiche del Dominio del Tempo",
    "text": "Feature Statistiche del Dominio del Tempo\nCalcolo RMS\nIl valore RMS di un set di valori (o di una forma d’onda a tempo continuo) è la radice quadrata della media aritmetica dei quadrati dei valori o del quadrato della funzione che definisce la forma d’onda continua. In fisica, il valore RMS di una corrente elettrica è definito come il “valore della corrente continua che dissipa la stessa potenza in un resistore”.\nNel caso di un set di n valori {𝑥1, 𝑥2, …, 𝑥𝑛}, l’RMS è:\n\n\nNOTARE che il valore RMS è diverso per i dati grezzi originali e dopo aver sottratto la media\n\n# Using numpy and standartized data (subtracting mean)\nrms = [np.sqrt(np.mean(np.square(x))) for x in sensors]\nPossiamo confrontare i valori RMS calcolati qui con quelli presentati da Edge Impulse:\n[print('rms_'+x+'= ', round(y, 4)) for x,y in zip(axis, rms)][0]\nprint(\"\\nCompare with Edge Impulse result features\")\nprint(features[0:N_feat:N_feat_axis])\nrms_accX=  2.7322\nrms_accY=  0.7833\nrms_accZ=  0.1383\nRispetto alle caratteristiche dei risultati di Edge Impulse:\n[2.7322, 0.7833, 0.1383]\nCalcolo di asimmetria e curtosi\nIn statistica, asimmetria e curtosi sono due modi per misurare la forma di una distribuzione.\nQui possiamo vedere la distribuzione dei valori del sensore:\nfig, axes = plt.subplots(nrows=1, ncols=3, figsize=(13, 4))\nsns.kdeplot(accX, fill=True, ax=axes[0])\nsns.kdeplot(accY, fill=True, ax=axes[1])\nsns.kdeplot(accZ, fill=True, ax=axes[2])\naxes[0].set_title('accX')\naxes[1].set_title('accY')\naxes[2].set_title('accZ')\nplt.suptitle('IMU Sensors distribution', fontsize=16, y=1.02)\nplt.show()\n\nLa Skewness [asimmetria] è una misura dell’asimmetria di una distribuzione. Questo valore può essere positivo o negativo.\n\n\nUn’asimmetria negativa indica che la coda si trova sul lato sinistro della distribuzione, che si estende verso valori più negativi.\nUn’asimmetria positiva indica che la coda si trova sul lato destro della distribuzione, che si estende verso valori più positivi.\nUn valore zero indica che non c’è alcuna asimmetria nella distribuzione, il che significa che la distribuzione è perfettamente simmetrica.\n\nskew = [skew(x, bias=False) for x in sensors]\n[print('skew_'+x+'= ', round(y, 4)) for x,y in zip(axis, skew)][0]\nprint(\"\\nCompare with Edge Impulse result features\")\nfeatures[1:N_feat:N_feat_axis]\nskew_accX=  -0.099\nskew_accY=  0.1756\nskew_accZ=  6.9463\nRispetto alle caratteristiche dei risultati di Edge Impulse:\n[-0.0978, 0.1735, 6.8629]\nLa Kurtosis è una misura che indica se una distribuzione è a coda pesante o a coda leggera rispetto a una distribuzione normale.\n\n\nLa curtosi di una distribuzione normale è zero.\nSe una data distribuzione ha una curtosi negativa, si dice che è platicurtica, il che significa che tende a produrre meno valori anomali e meno estremi rispetto alla distribuzione normale.\nSe una data distribuzione ha una curtosi positiva, si dice che è leptocurtica, il che significa che tende a produrre più valori anomali rispetto alla distribuzione normale.\n\nkurt = [kurtosis(x, bias=False) for x in sensors]\n[print('kurt_'+x+'= ', round(y, 4)) for x,y in zip(axis, kurt)][0]\nprint(\"\\nCompare with Edge Impulse result features\")\nfeatures[2:N_feat:N_feat_axis]\nkurt_accX=  -0.3475\nkurt_accY=  1.2673\nkurt_accZ=  68.1123\nRispetto alle caratteristiche dei risultati di Edge Impulse:\n[-0.3813, 1.1696, 65.3726]",
    "crumbs": [
      "Lab Condivisi",
      "Blocco delle Feature Spettrali DSP"
    ]
  },
  {
    "objectID": "contents/labs/shared/dsp_spectral_features_block/dsp_spectral_features_block.it.html#feature-spettrali",
    "href": "contents/labs/shared/dsp_spectral_features_block/dsp_spectral_features_block.it.html#feature-spettrali",
    "title": "Blocco delle Feature Spettrali DSP",
    "section": "Feature spettrali",
    "text": "Feature spettrali\nIl segnale filtrato viene trasmesso alla sezione di potenza spettrale, che calcola la FFT per generare le caratteristiche spettrali.\nPoiché la finestra campionata è solitamente più grande della dimensione FFT, la finestra verrà suddivisa in frame (o “sotto-finestre”) e la FFT verrà calcolata su ogni frame.\nFFT length - La dimensione della FFT. Questo determina il numero di bin FFT e la risoluzione dei picchi di frequenza che possono essere separati. Un numero basso significa che più segnali saranno mediati insieme nello stesso bin FFT, ma riduce anche il numero di feature e la dimensione del modello. Un numero alto separerà più segnali in bin separati, generando un modello più grande.\n\nIl numero totale di feature di potenza spettrale varierà a seconda di come si impostano i parametri del filtro e di FFT. Senza filtro, il numero di feature è 1/2 della lunghezza FFT.\n\nSpectral Power - Metodo di Welch\nDovremmo usare il metodo di Welch per dividere il segnale nel dominio della frequenza in “bin” e calcolare lo spettro di potenza per ogni bin. Questo metodo divide il segnale in segmenti sovrapposti, applica una funzione finestra a ogni segmento, calcola il periodogramma di ogni segmento usando DFT e ne fa la media per ottenere una stima più uniforme dello spettro di potenza.\n# Function used by Edge Impulse instead of scipy.signal.welch().\ndef welch_max_hold(fx, sampling_freq, nfft, n_overlap):\n    n_overlap = int(n_overlap)\n    spec_powers = [0 for _ in range(nfft//2+1)]\n    ix = 0\n    while ix &lt;= len(fx):\n        # Slicing truncates if end_idx &gt; len, and rfft will auto-zero pad\n        fft_out = np.abs(np.fft.rfft(fx[ix:ix+nfft], nfft))\n        spec_powers = np.maximum(spec_powers, fft_out**2/nfft)\n        ix = ix + (nfft-n_overlap)\n    return np.fft.rfftfreq(nfft, 1/sampling_freq), spec_powers\nApplicazione della funzione di cui sopra a 3 segnali:\nfax,Pax = welch_max_hold(accX, fs, FFT_Lenght, 0)\nfay,Pay = welch_max_hold(accY, fs, FFT_Lenght, 0)\nfaz,Paz = welch_max_hold(accZ, fs, FFT_Lenght, 0)\nspecs = [Pax, Pay, Paz ]\nPossiamo tracciare lo spettro di potenza P(f):\nplt.plot(fax,Pax, label='accX')\nplt.plot(fay,Pay, label='accY')\nplt.plot(faz,Paz, label='accZ')\nplt.legend(loc='upper right')\nplt.xlabel('Frequency (Hz)')\n#plt.ylabel('PSD [V**2/Hz]')\nplt.ylabel('Power')\nplt.title('Power spectrum P(f) using Welch's method')\nplt.grid()\nplt.box(False)\nplt.show()\n\nOltre allo spettro di potenza, possiamo anche includere l’asimmetria e la curtosi delle feature nel dominio della frequenza (dovrebbero essere disponibili in una nuova versione):\nspec_skew = [skew(x, bias=False) for x in specs]\nspec_kurtosis = [kurtosis(x, bias=False) for x in specs]\nElenchiamo ora tutte le feature spettrali per asse e confrontarli con EI:\nprint(\"EI Processed Spectral features (accX): \")\nprint(features[3:N_feat_axis][0:])\nprint(\"\\nCalculated features:\")\nprint (round(spec_skew[0],4))\nprint (round(spec_kurtosis[0],4))\n[print(round(x, 4)) for x in Pax[1:]][0]\nEI Processed Spectral features (accX):\n2.398, 3.8924, 24.6841, 9.6303, 8.4867, 7.7793, 2.9963, 5.6242, 3.4198, 4.2735\nCalculated features:\n2.9069 8.5569 24.6844 9.6304 8.4865 7.7794 2.9964 5.6242 3.4198 4.2736\nprint(\"EI Processed Spectral features (accY): \")\nprint(features[16:26][0:]) #13: 3+N_feat_axis;  26 = 2x N_feat_axis\nprint(\"\\nCalculated features:\")\nprint (round(spec_skew[1],4))\nprint (round(spec_kurtosis[1],4))\n[print(round(x, 4)) for x in Pay[1:]][0]\nEI Processed Spectral features (accY):\n0.9426, -0.8039, 5.429, 0.999, 1.0315, 0.9459, 1.8117, 0.9088, 1.3302, 3.112\nCalculated features:\n1.1426 -0.3886 5.4289 0.999 1.0315 0.9458 1.8116 0.9088 1.3301 3.1121\nprint(\"EI Processed Spectral features (accZ): \")\nprint(features[29:][0:]) #29: 3+(2*N_feat_axis);\nprint(\"\\nCalculated features:\")\nprint (round(spec_skew[2],4))\nprint (round(spec_kurtosis[2],4))\n[print(round(x, 4)) for x in Paz[1:]][0]\nEI Processed Spectral features (accZ):\n0.3117, -1.3812, 0.0606, 0.057, 0.0567, 0.0976, 0.194, 0.2574, 0.2083, 0.166\nCalculated features:\n0.3781 -1.4874 0.0606 0.057 0.0567 0.0976 0.194 0.2574 0.2083 0.166",
    "crumbs": [
      "Lab Condivisi",
      "Blocco delle Feature Spettrali DSP"
    ]
  },
  {
    "objectID": "contents/labs/shared/dsp_spectral_features_block/dsp_spectral_features_block.it.html#dominio-tempo-frequenza",
    "href": "contents/labs/shared/dsp_spectral_features_block/dsp_spectral_features_block.it.html#dominio-tempo-frequenza",
    "title": "Blocco delle Feature Spettrali DSP",
    "section": "Dominio tempo-frequenza",
    "text": "Dominio tempo-frequenza\n\nWavelet\nWavelet è una tecnica potente per analizzare segnali con feature transitorie o bruschi cambiamenti, come picchi o bordi, che sono difficili da interpretare con i metodi tradizionali basati su Fourier.\nLe trasformazioni wavelet funzionano scomponendo un segnale in diverse componenti di frequenza e analizzandole individualmente. La trasformazione si ottiene convolvendo il segnale con una funzione wavelet, una piccola forma d’onda centrata su un tempo e una frequenza specifici. Questo processo scompone efficacemente il segnale in diverse bande di frequenza, ciascuna delle quali può essere analizzata separatamente.\nUno dei vantaggi fondamentali delle trasformazioni wavelet è che consentono l’analisi tempo-frequenza, il che significa che possono rivelare il contenuto di frequenza di un segnale mentre cambia nel tempo. Ciò le rende particolarmente utili per analizzare segnali non stazionari, che variano nel tempo.\nLe wavelet hanno molte applicazioni pratiche, tra cui la compressione di segnali e immagini, il denoising [rimozione del rumore], l’estrazione di feature e l’elaborazione di immagini.\nSelezioniamo Wavelet sul blocco Spectral Features nello stesso progetto:\n\nType: Wavelet\nWavelet Decomposition Level: 1\nWavelet: bior1.3\n\n\nLa Funzione Wavelet\nwavelet_name='bior1.3'\nnum_layer = 1\n\nwavelet = pywt.Wavelet(wavelet_name)\n[phi_d,psi_d,phi_r,psi_r,x] = wavelet.wavefun(level=5)\nplt.plot(x, psi_d, color='red')\nplt.title('Wavelet Function')\nplt.ylabel('Value')\nplt.xlabel('Time')\nplt.grid()\nplt.box(False)\nplt.show()\n\nCome abbiamo fatto prima, copiamo e incolliamo le Processed Features:\n\nfeatures = [3.6251, 0.0615, 0.0615, -7.3517, -2.7641, 2.8462, 5.0924, ...]\nN_feat = len(features)\nN_feat_axis = int(N_feat/n_sensors)\nEdge Impulse calcola la Discrete Wavelet Transform (DWT) per ciascuno dei livelli di Wavelet Decomposition selezionati. Dopodiché, le feature verranno estratte.\nNel caso di Wavelet, le feature estratte sono valori statistici di base, valori di incrocio ed entropia. Ci sono, in totale, 14 feature per layer come di seguito:\n\n[11] Feature Statistiche: n5, n25, n75, n95, media, mediana, deviazione standard (std), varianza (var) radice quadrata media (rms), curtosi e asimmetria (skew).\n[2] Feature di attraversamento: Il tasso di attraversamento dello zero (zcross) e il tasso di attraversamento medio (mcross) sono rispettivamente i tempi in cui il segnale attraversa la linea di base (y = 0) e il livello medio (y = u) per unità di tempo\n[1] Feature di Complessità: L’entropia è una misura caratteristica della complessità del segnale\n\nTutti i 14 valori sopra indicati vengono calcolati per ogni Layer (incluso L0, il segnale originale)\n\nIl numero totale di feature varia a seconda di come si imposta il filtro e del numero di layer. Ad esempio, con il filtro [None] e Level[1], il numero di feature per asse sarà 14 x 2 (L0 e L1) = 28. Per i tre assi, avremo un totale di 84 feature.\n\n\n\nAnalisi Wavelet\nL’analisi wavelet scompone il segnale (accX, accY, e accZ) in diverse componenti di frequenza utilizzando un set di filtri, che separano queste componenti in componenti a bassa frequenza (parti del segnale che variano lentamente e contengono pattern a lungo termine), come accX_l1, accY_l1, accZ_l1 e componenti ad alta frequenza (parti del segnale che variano rapidamente e contengono modelli a breve termine), come accX_d1, accY_d1, accZ_d1, consentendo l’estrazione di feature per ulteriori analisi o classificazioni.\nVerranno utilizzati solo le componenti a bassa frequenza (coefficienti di approssimazione o cA). In questo esempio, assumiamo un solo livello (Single-level Discrete Wavelet Transform), in cui la funzione restituirà una tupla. Con una decomposizione multilivello, la “Multilevel 1D Discrete Wavelet Transform”, il risultato sarà un elenco (per i dettagli, vedere: Discrete Wavelet Transform (DWT) )\n(accX_l1, accX_d1) = pywt.dwt(accX, wavelet_name)\n(accY_l1, accY_d1) = pywt.dwt(accY, wavelet_name)\n(accZ_l1, accZ_d1) = pywt.dwt(accZ, wavelet_name)\nsensors_l1 = [accX_l1, accY_l1, accZ_l1]\n\n# Plot power spectrum versus frequency\nplt.plot(accX_l1, label='accX')\nplt.plot(accY_l1, label='accY')\nplt.plot(accZ_l1, label='accZ')\nplt.legend(loc='lower right')\nplt.xlabel('Time')\nplt.ylabel('Value')\nplt.title('Wavelet Approximation')\nplt.grid()\nplt.box(False)\nplt.show()\n\n\n\nEstrazione delle Feature\nCominciamo con le feature statistiche di base. Notare che applichiamo la funzione sia per i segnali originali che per i cAs risultanti dal DWT:\ndef calculate_statistics(signal):\n    n5 = np.percentile(signal, 5)\n    n25 = np.percentile(signal, 25)\n    n75 = np.percentile(signal, 75)\n    n95 = np.percentile(signal, 95)\n    median = np.percentile(signal, 50)\n    mean = np.mean(signal)\n    std = np.std(signal)\n    var = np.var(signal)\n    rms = np.sqrt(np.mean(np.square(signal)))\n    return [n5, n25, n75, n95, median, mean, std, var, rms]\n \nstat_feat_l0 = [calculate_statistics(x) for x in sensors]\nstat_feat_l1 = [calculate_statistics(x) for x in sensors_l1]\nAsimmetria e Curtosi:\nskew_l0 = [skew(x, bias=False) for x in sensors]\nskew_l1 = [skew(x, bias=False) for x in sensors_l1]\nkurtosis_l0 = [kurtosis(x, bias=False) for x in sensors]\nkurtosis_l1 = [kurtosis(x, bias=False) for x in sensors_l1]\nZero crossing (zcross) è il numero di volte in cui il coefficiente wavelet attraversa l’asse dello zero. Può essere utilizzato per misurare il contenuto di frequenza del segnale poiché i segnali ad alta frequenza tendono ad avere più attraversamenti per lo zero rispetto ai segnali a bassa frequenza.\nMean crossing (mcross), d’altra parte, è il numero di volte in cui il coefficiente wavelet attraversa la media del segnale. Può essere utilizzato per misurare l’ampiezza poiché i segnali ad alta ampiezza tendono ad avere più attraversamenti medi rispetto ai segnali a bassa ampiezza.\ndef getZeroCrossingRate(arr):\n    my_array = np.array(arr)\n    zcross = float(\"{0:.2f}\".format((((my_array[:-1] * my_array[1:]) &lt; 0).su    m())/len(arr)))\n    return zcross\n\ndef getMeanCrossingRate(arr):\n    mcross = getZeroCrossingRate(np.array(arr) - np.mean(arr))\n    return mcross\n\ndef calculate_crossings(list):\n    zcross=[]\n    mcross=[]\n    for i in range(len(list)):\n        zcross_i = getZeroCrossingRate(list[i])\n        zcross.append(zcross_i)\n        mcross_i = getMeanCrossingRate(list[i])\n        mcross.append(mcross_i)\n    return zcross, mcross\n\ncross_l0 = calculate_crossings(sensors)\ncross_l1 = calculate_crossings(sensors_l1)\nNell’analisi wavelet, l’entropia si riferisce al grado di disordine o casualità nella distribuzione dei coefficienti wavelet. Qui, abbiamo utilizzato l’entropia di Shannon, che misura l’incertezza o la casualità di un segnale. Viene calcolata come la somma negativa delle probabilità dei diversi possibili risultati del segnale moltiplicata per il loro logaritmo in base 2. Nel contesto dell’analisi wavelet, l’entropia di Shannon può essere utilizzata per misurare la complessità del segnale, con valori più alti che indicano una maggiore complessità.\ndef calculate_entropy(signal, base=None):\n    value, counts = np.unique(signal, return_counts=True)\n    return entropy(counts, base=base)\n\nentropy_l0 = [calculate_entropy(x) for x in sensors]\nentropy_l1 = [calculate_entropy(x) for x in sensors_l1]\nElenchiamo ora tutte le feature wavelet e creiamo un elenco per layer.\nL1_features_names = [\"L1-n5\", \"L1-n25\", \"L1-n75\", \"L1-n95\", \"L1-median\", \"L1-mean\", \"L1-std\", \"L1-var\", \"L1-rms\", \"L1-skew\", \"L1-Kurtosis\", \"L1-zcross\", \"L1-mcross\", \"L1-entropy\"]\n\nL0_features_names = [\"L0-n5\", \"L0-n25\", \"L0-n75\", \"L0-n95\", \"L0-median\", \"L0-mean\", \"L0-std\", \"L0-var\", \"L0-rms\", \"L0-skew\", \"L0-Kurtosis\", \"L0-zcross\", \"L0-mcross\", \"L0-entropy\"]\n\nall_feat_l0 = []\nfor i in range(len(axis)):\n    feat_l0 = stat_feat_l0[i]+[skew_l0[i]]+[kurtosis_l0[i]]+[cross_l0[0][i]]+[cross_l0[1][i]]+[entropy_l0[i]]\n    [print(axis[i]+' '+x+'= ', round(y, 4)) for x,y in zip(L0_features_names, feat_l0)][0]\n    all_feat_l0.append(feat_l0)\nall_feat_l0 = [item for sublist in all_feat_l0 for item in sublist]\nprint(f\"\\nAll L0 Features = {len(all_feat_l0)}\")\n\nall_feat_l1 = []\nfor i in range(len(axis)):\nfeat_l1 = stat_feat_l1[i]+[skew_l1[i]]+[kurtosis_l1[i]]+[cross_l1[0][i]]+[cross_l1[1][i]]+[entropy_l1[i]]\n[print(axis[i]+' '+x+'= ', round(y, 4)) for x,y in zip(L1_features_names, feat_l1)][0]\nall_feat_l1.append(feat_l1)\nall_feat_l1 = [item for sublist in all_feat_l1 for item in sublist]\nprint(f\"\\nAll L1 Features = {len(all_feat_l1)}\")",
    "crumbs": [
      "Lab Condivisi",
      "Blocco delle Feature Spettrali DSP"
    ]
  },
  {
    "objectID": "contents/labs/shared/dsp_spectral_features_block/dsp_spectral_features_block.it.html#conclusione",
    "href": "contents/labs/shared/dsp_spectral_features_block/dsp_spectral_features_block.it.html#conclusione",
    "title": "Blocco delle Feature Spettrali DSP",
    "section": "Conclusione",
    "text": "Conclusione\nEdge Impulse Studio è una potente piattaforma online in grado di gestire per noi l’attività di pre-elaborazione. Tuttavia, data la nostra prospettiva ingegneristica, vogliamo capire cosa sta succedendo “sotto il cofano”. Questa conoscenza ci aiuterà a trovare le migliori opzioni e gli iperparametri per ottimizzare i nostri progetti.\nDaniel Situnayake ha scritto nel suo blog: “I dati grezzi dei sensori sono altamente dimensionali e rumorosi. Gli algoritmi di elaborazione del segnale digitale ci aiutano a separare il segnale dal rumore. Il DSP è una parte essenziale dell’ingegneria embedded e molti processori edge hanno un’accelerazione integrata per il DSP. Come ingegnere ML, imparare il DSP di base dà dei superpoteri per gestire dati di serie temporali ad alta frequenza nei propri modelli”. Consiglio di leggere l’eccellente post di Dan nella sua interezza: nn to cpp: What you need to know about porting deep learning models to the edge [Tutto ciò che devi sapere sul porting dei modelli di deep learning verso l’edge].",
    "crumbs": [
      "Lab Condivisi",
      "Blocco delle Feature Spettrali DSP"
    ]
  },
  {
    "objectID": "references.it.html",
    "href": "references.it.html",
    "title": "Riferimenti",
    "section": "",
    "text": "Abadi, Martin, Andy Chu, Ian Goodfellow, H. Brendan McMahan, Ilya\nMironov, Kunal Talwar, and Li Zhang. 2016. “Deep Learning with\nDifferential Privacy.” In Proceedings of the 2016 ACM SIGSAC\nConference on Computer and Communications Security, 308–18. CCS\n’16. New York, NY, USA: ACM. https://doi.org/10.1145/2976749.2978318.\n\n\nAbdelkader, Ahmed, Michael J. Curry, Liam Fowl, Tom Goldstein, Avi\nSchwarzschild, Manli Shu, Christoph Studer, and Chen Zhu. 2020.\n“Headless Horseman: Adversarial Attacks on Transfer\nLearning Models.” In ICASSP 2020 - 2020 IEEE International\nConference on Acoustics, Speech and Signal Processing (ICASSP),\n3087–91. IEEE. https://doi.org/10.1109/icassp40776.2020.9053181.\n\n\nAddepalli, Sravanti, B. S. Vivek, Arya Baburaj, Gaurang Sriramanan, and\nR. Venkatesh Babu. 2020. “Towards Achieving Adversarial Robustness\nby Enforcing Feature Consistency Across Bit Planes.” In 2020\nIEEE/CVF Conference on Computer Vision and Pattern Recognition\n(CVPR), 1020–29. IEEE. https://doi.org/10.1109/cvpr42600.2020.00110.\n\n\nAdolf, Robert, Saketh Rama, Brandon Reagen, Gu-yeon Wei, and David\nBrooks. 2016. “Fathom: Reference Workloads for Modern\nDeep Learning Methods.” In 2016 IEEE International Symposium\non Workload Characterization (IISWC), 1–10. IEEE; IEEE. https://doi.org/10.1109/iiswc.2016.7581275.\n\n\nAgarwal, Alekh, Alina Beygelzimer, Miroslav Dudı́k, John Langford, and\nHanna M. Wallach. 2018. “A Reductions Approach to Fair\nClassification.” In Proceedings of the 35th International\nConference on Machine Learning, ICML 2018, Stockholmsmässan, Stockholm,\nSweden, July 10-15, 2018, edited by Jennifer G. Dy and Andreas\nKrause, 80:60–69. Proceedings of Machine Learning Research. PMLR. http://proceedings.mlr.press/v80/agarwal18a.html.\n\n\nAgnesina, Anthony, Puranjay Rajvanshi, Tian Yang, Geraldo Pradipta,\nAustin Jiao, Ben Keller, Brucek Khailany, and Haoxing Ren. 2023.\n“AutoDMP: Automated DREAMPlace-Based Macro\nPlacement.” In Proceedings of the 2023 International\nSymposium on Physical Design, 149–57. ACM. https://doi.org/10.1145/3569052.3578923.\n\n\nAgrawal, Dakshi, Selcuk Baktir, Deniz Karakoyunlu, Pankaj Rohatgi, and\nBerk Sunar. 2007. “Trojan Detection Using IC\nFingerprinting.” In 2007 IEEE Symposium on Security and\nPrivacy (SP ’07), 296–310. Springer; IEEE. https://doi.org/10.1109/sp.2007.36.\n\n\nAhmadilivani, Mohammad Hasan, Mahdi Taheri, Jaan Raik, Masoud\nDaneshtalab, and Maksim Jenihhin. 2024. “A Systematic Literature\nReview on Hardware Reliability Assessment Methods for Deep Neural\nNetworks.” ACM Comput. Surv. 56 (6): 1–39. https://doi.org/10.1145/3638242.\n\n\nAledhari, Mohammed, Rehma Razzak, Reza M. Parizi, and Fahad Saeed. 2020.\n“Federated Learning: A Survey on Enabling\nTechnologies, Protocols, and Applications.” #IEEE_O_ACC#\n8: 140699–725. https://doi.org/10.1109/access.2020.3013541.\n\n\nAlghamdi, Wael, Hsiang Hsu, Haewon Jeong, Hao Wang, Peter Michalak,\nShahab Asoodeh, and Flavio Calmon. 2022. “Beyond Adult and\nCOMPAS: Fair Multi-Class Prediction via\nInformation Projection.” Adv. Neur. In. 35: 38747–60.\n\n\nAltayeb, Moez, Marco Zennaro, and Marcelo Rovai. 2022.\n“Classifying Mosquito Wingbeat Sound Using\nTinyML.” In Proceedings of the 2022 ACM\nConference on Information Technology for Social Good, 132–37. ACM.\nhttps://doi.org/10.1145/3524458.3547258.\n\n\nAmiel, Frederic, Christophe Clavier, and Michael Tunstall. 2006.\n“Fault Analysis of DPA-Resistant Algorithms.” In Fault\nDiagnosis and Tolerance in Cryptography, 223–36. Springer; Springer\nBerlin Heidelberg. https://doi.org/10.1007/11889700\\_20.\n\n\nAnsel, Jason, Edward Yang, Horace He, Natalia Gimelshein, Animesh Jain,\nMichael Voznesensky, Bin Bao, et al. 2024. “PyTorch\n2: Faster Machine Learning Through Dynamic Python Bytecode\nTransformation and Graph Compilation.” In Proceedings of the\n29th ACM International Conference on Architectural Support for\nProgramming Languages and Operating Systems, Volume 2, edited by\nHanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence\nd’Alché-Buc, Emily B. Fox, and Roman Garnett, 8024–35. ACM. https://doi.org/10.1145/3620665.3640366.\n\n\nAnthony, Lasse F. Wolff, Benjamin Kanding, and Raghavendra Selvan. 2020.\nICML Workshop on Challenges in Deploying and monitoring Machine Learning\nSystems.\n\n\nAntol, Stanislaw, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv\nBatra, C. Lawrence Zitnick, and Devi Parikh. 2015. “VQA: Visual\nQuestion Answering.” In 2015 IEEE International Conference on\nComputer Vision (ICCV), 2425–33. IEEE. https://doi.org/10.1109/iccv.2015.279.\n\n\nAntonakakis, Manos, Tim April, Michael Bailey, Matt Bernhard, Elie\nBursztein, Jaime Cochran, Zakir Durumeric, et al. 2017.\n“Understanding the Mirai Botnet.” In 26th USENIX\nSecurity Symposium (USENIX Security 17), 1093–1110.\n\n\nArdila, Rosana, Megan Branson, Kelly Davis, Michael Kohler, Josh Meyer,\nMichael Henretty, Reuben Morais, Lindsay Saunders, Francis Tyers, and\nGregor Weber. 2020. “Common Voice: A\nMassively-Multilingual Speech Corpus.” In Proceedings of the\nTwelfth Language Resources and Evaluation Conference, 4218–22.\nMarseille, France: European Language Resources Association. https://aclanthology.org/2020.lrec-1.520.\n\n\nArifeen, Tooba, Abdus Sami Hassan, and Jeong-A Lee. 2020.\n“Approximate Triple Modular Redundancy: A\nSurvey.” #IEEE_O_ACC# 8: 139851–67. https://doi.org/10.1109/access.2020.3012673.\n\n\nAsonov, D., and R. Agrawal. n.d. “Keyboard Acoustic\nEmanations.” In IEEE Symposium on Security and Privacy, 2004.\nProceedings. 2004, 3–11. IEEE; IEEE. https://doi.org/10.1109/secpri.2004.1301311.\n\n\nAteniese, Giuseppe, Luigi V. Mancini, Angelo Spognardi, Antonio Villani,\nDomenico Vitali, and Giovanni Felici. 2015. “Hacking Smart\nMachines with Smarter Ones: How to Extract Meaningful Data from Machine\nLearning Classifiers.” International Journal of Security and\nNetworks 10 (3): 137. https://doi.org/10.1504/ijsn.2015.071829.\n\n\nAttia, Zachi I., Alan Sugrue, Samuel J. Asirvatham, Michael J. Ackerman,\nSuraj Kapa, Paul A. Friedman, and Peter A. Noseworthy. 2018.\n“Noninvasive Assessment of Dofetilide Plasma Concentration Using a\nDeep Learning (Neural Network) Analysis of the Surface\nElectrocardiogram: A Proof of Concept Study.” PLOS ONE\n13 (8): e0201059. https://doi.org/10.1371/journal.pone.0201059.\n\n\nAygun, Sercan, Ece Olcay Gunes, and Christophe De Vleeschouwer. 2021.\n“Efficient and Robust Bitstream Processing in Binarised Neural\nNetworks.” Electron. Lett. 57 (5): 219–22. https://doi.org/10.1049/ell2.12045.\n\n\nBai, Tao, Jinqi Luo, Jun Zhao, Bihan Wen, and Qian Wang. 2021.\n“Recent Advances in Adversarial Training for Adversarial\nRobustness.” arXiv Preprint arXiv:2102.01356.\n\n\nBains, Sunny. 2020. “The Business of Building Brains.”\nNature Electronics 3 (7): 348–51. https://doi.org/10.1038/s41928-020-0449-1.\n\n\nBamoumen, Hatim, Anas Temouden, Nabil Benamar, and Yousra Chtouki. 2022.\n“How TinyML Can Be Leveraged to Solve Environmental\nProblems: A Survey.” In 2022 International\nConference on Innovation and Intelligence for Informatics, Computing,\nand Technologies (3ICT), 338–43. IEEE; IEEE. https://doi.org/10.1109/3ict56508.2022.9990661.\n\n\nBanbury, Colby, Vijay Janapa Reddi, Peter Torelli, Jeremy Holleman, Nat\nJeffries, Csaba Kiraly, Pietro Montino, et al. 2021. “MLPerf Tiny\nBenchmark.” arXiv Preprint arXiv:2106.07597, June. http://arxiv.org/abs/2106.07597v4.\n\n\nBank, Dor, Noam Koenigstein, and Raja Giryes. 2023.\n“Autoencoders.” Machine Learning for Data Science\nHandbook: Data Mining and Knowledge Discovery Handbook, 353–74.\n\n\nBannon, Pete, Ganesh Venkataramanan, Debjit Das Sarma, and Emil Talpes.\n2019. “Computer and Redundancy Solution for the Full Self-Driving\nComputer.” In 2019 IEEE Hot Chips 31 Symposium (HCS),\n1–22. IEEE Computer Society; IEEE. https://doi.org/10.1109/hotchips.2019.8875645.\n\n\nBarenghi, Alessandro, Guido M. Bertoni, Luca Breveglieri, Mauro\nPellicioli, and Gerardo Pelosi. 2010. “Low Voltage Fault Attacks\nto AES.” In 2010 IEEE International Symposium on\nHardware-Oriented Security and Trust (HOST), 7–12. IEEE; IEEE. https://doi.org/10.1109/hst.2010.5513121.\n\n\nBarroso, Luiz André, Urs Hölzle, and Parthasarathy Ranganathan. 2019.\nThe Datacenter as a Computer: Designing Warehouse-Scale\nMachines. Springer International Publishing. https://doi.org/10.1007/978-3-031-01761-2.\n\n\nBau, David, Bolei Zhou, Aditya Khosla, Aude Oliva, and Antonio Torralba.\n2017. “Network Dissection: Quantifying\nInterpretability of Deep Visual Representations.” In 2017\nIEEE Conference on Computer Vision and Pattern Recognition (CVPR),\n3319–27. IEEE. https://doi.org/10.1109/cvpr.2017.354.\n\n\nBeaton, Albert E., and John W. Tukey. 1974. “The Fitting of Power\nSeries, Meaning Polynomials, Illustrated on Band-Spectroscopic\nData.” Technometrics 16 (2): 147. https://doi.org/10.2307/1267936.\n\n\nBeck, Nathaniel, and Simon Jackman. 1998. “Beyond Linearity by\nDefault: Generalized Additive Models.” Am. J.\nPolit. Sci. 42 (2): 596. https://doi.org/10.2307/2991772.\n\n\nBender, Emily M., and Batya Friedman. 2018. “Data Statements for\nNatural Language Processing: Toward Mitigating System Bias\nand Enabling Better Science.” Transactions of the Association\nfor Computational Linguistics 6 (December): 587–604. https://doi.org/10.1162/tacl_a_00041.\n\n\nBerger, Vance W, and YanYan Zhou. 2014.\n“Kolmogorovsmirnov Test:\nOverview.” Wiley Statsref: Statistics Reference\nOnline.\n\n\nBeyer, Lucas, Olivier J. Hénaff, Alexander Kolesnikov, Xiaohua Zhai, and\nAäron van den Oord. 2020. “Are We Done with ImageNet?”\nArXiv Preprint abs/2006.07159 (June). http://arxiv.org/abs/2006.07159v1.\n\n\nBhagoji, Arjun Nitin, Warren He, Bo Li, and Dawn Song. 2018.\n“Practical Black-Box Attacks on Deep Neural Networks Using\nEfficient Query Mechanisms.” In Proceedings of the European\nConference on Computer Vision (ECCV), 154–69.\n\n\nBhardwaj, Kshitij, Marton Havasi, Yuan Yao, David M. Brooks, José Miguel\nHernández-Lobato, and Gu-Yeon Wei. 2020. “A Comprehensive\nMethodology to Determine Optimal Coherence Interfaces for\nMany-Accelerator SoCs.” In Proceedings of the\nACM/IEEE International Symposium on Low Power Electronics and\nDesign, 145–50. ACM. https://doi.org/10.1145/3370748.3406564.\n\n\nBianco, Simone, Remi Cadene, Luigi Celona, and Paolo Napoletano. 2018.\n“Benchmark Analysis of Representative Deep Neural Network\nArchitectures.” IEEE Access 6: 64270–77. https://doi.org/10.1109/access.2018.2877890.\n\n\nBiega, Asia J., Peter Potash, Hal Daumé, Fernando Diaz, and Michèle\nFinck. 2020. “Operationalizing the Legal Principle of Data\nMinimization for Personalization.” In Proceedings of the 43rd\nInternational ACM SIGIR Conference on Research and Development in\nInformation Retrieval, edited by Jimmy Huang, Yi Chang, Xueqi\nCheng, Jaap Kamps, Vanessa Murdock, Ji-Rong Wen, and Yiqun Liu, 399–408.\nACM. https://doi.org/10.1145/3397271.3401034.\n\n\nBiggio, Battista, Blaine Nelson, and Pavel Laskov. 2012.\n“Poisoning Attacks Against Support Vector Machines.” In\nProceedings of the 29th International Conference on Machine\nLearning, ICML 2012, Edinburgh, Scotland, UK, June 26 - July 1,\n2012. icml.cc / Omnipress. http://icml.cc/2012/papers/880.pdf.\n\n\nBiggs, John, James Myers, Jedrzej Kufel, Emre Ozer, Simon Craske, Antony\nSou, Catherine Ramsdale, Ken Williamson, Richard Price, and Scott White.\n2021. “A Natively Flexible 32-Bit Arm Microprocessor.”\nNature 595 (7868): 532–36. https://doi.org/10.1038/s41586-021-03625-w.\n\n\nBinkert, Nathan, Bradford Beckmann, Gabriel Black, Steven K. Reinhardt,\nAli Saidi, Arkaprava Basu, Joel Hestness, et al. 2011. “The Gem5\nSimulator.” ACM SIGARCH Computer Architecture News 39\n(2): 1–7. https://doi.org/10.1145/2024716.2024718.\n\n\nBohr, Adam, and Kaveh Memarzadeh. 2020. “The Rise of Artificial\nIntelligence in Healthcare Applications.” In Artificial\nIntelligence in Healthcare, 25–60. Elsevier. https://doi.org/10.1016/b978-0-12-818438-7.00002-2.\n\n\nBolchini, Cristiana, Luca Cassano, Antonio Miele, and Alessandro Toschi.\n2023. “Fast and Accurate Error Simulation for CNNs\nAgainst Soft Errors.” IEEE Trans. Comput. 72 (4):\n984–97. https://doi.org/10.1109/tc.2022.3184274.\n\n\nBondi, Elizabeth, Ashish Kapoor, Debadeepta Dey, James Piavis, Shital\nShah, Robert Hannaford, Arvind Iyer, Lucas Joppa, and Milind Tambe.\n2018. “Near Real-Time Detection of Poachers from Drones in\nAirSim.” In Proceedings of the Twenty-Seventh\nInternational Joint Conference on Artificial Intelligence, edited\nby Jérôme Lang, 5814–16. International Joint Conferences on Artificial\nIntelligence Organization. https://doi.org/10.24963/ijcai.2018/847.\n\n\nBourtoule, Lucas, Varun Chandrasekaran, Christopher A. Choquette-Choo,\nHengrui Jia, Adelin Travers, Baiwu Zhang, David Lie, and Nicolas\nPapernot. 2021. “Machine Unlearning.” In 2021 IEEE\nSymposium on Security and Privacy (SP), 141–59. IEEE; IEEE. https://doi.org/10.1109/sp40001.2021.00019.\n\n\nBreier, Jakub, Xiaolu Hou, Dirmanto Jap, Lei Ma, Shivam Bhasin, and Yang\nLiu. 2018. “DeepLaser: Practical Fault Attack on Deep Neural\nNetworks.” ArXiv Preprint abs/1806.05859 (June). http://arxiv.org/abs/1806.05859v2.\n\n\nBrown, Tom B., Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,\nPrafulla Dhariwal, Arvind Neelakantan, et al. 2020. “Language\nModels Are Few-Shot Learners.” In Advances in Neural\nInformation Processing Systems 33: Annual Conference on Neural\nInformation Processing Systems 2020, NeurIPS 2020, December 6-12, 2020,\nVirtual, edited by Hugo Larochelle, Marc’Aurelio Ranzato, Raia\nHadsell, Maria-Florina Balcan, and Hsuan-Tien Lin. https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html.\n\n\nBuolamwini, Joy, and Timnit Gebru. 2018. “Gender Shades:\nIntersectional Accuracy Disparities in Commercial Gender\nClassification.” In Conference on Fairness, Accountability\nand Transparency, 77–91. PMLR.\n\n\nBurnet, David, and Richard Thomas. 1989. “Spycatcher: The\nCommodification of Truth.” Journal of Law and Society 16\n(2): 210. https://doi.org/10.2307/1410360.\n\n\nBurr, Geoffrey W., Matthew J. BrightSky, Abu Sebastian, Huai-Yu Cheng,\nJau-Yi Wu, Sangbum Kim, Norma E. Sosa, et al. 2016. “Recent\nProgress in Phase-Change?Pub _Newline ?Memory\nTechnology.” IEEE Journal on Emerging and Selected Topics in\nCircuits and Systems 6 (2): 146–62. https://doi.org/10.1109/jetcas.2016.2547718.\n\n\nBushnell, Michael L, and Vishwani D Agrawal. 2002. “Built-in\nSelf-Test.” Essentials of Electronic Testing for Digital,\nMemory and Mixed-Signal VLSI Circuits, 489–548.\n\n\nBuyya, Rajkumar, Anton Beloglazov, and Jemal Abawajy. 2010.\n“Energy-Efficient Management of Data Center Resources for Cloud\nComputing: A Vision, Architectural Elements, and Open\nChallenges.” https://arxiv.org/abs/1006.0308.\n\n\nCai, Carrie J., Emily Reif, Narayan Hegde, Jason Hipp, Been Kim, Daniel\nSmilkov, Martin Wattenberg, et al. 2019. “Human-Centered Tools for\nCoping with Imperfect Algorithms During Medical Decision-Making.”\nIn Proceedings of the 2019 CHI Conference on Human Factors in\nComputing Systems, edited by Jennifer G. Dy and Andreas Krause,\n80:2673–82. Proceedings of Machine Learning Research. ACM. https://doi.org/10.1145/3290605.3300234.\n\n\nCai, Han, Chuang Gan, Ligeng Zhu, and Song Han 0003. 2020.\n“TinyTL: Reduce Memory, Not Parameters for Efficient on-Device\nLearning.” In Advances in Neural Information Processing\nSystems 33: Annual Conference on Neural Information Processing Systems\n2020, NeurIPS 2020, December 6-12, 2020, Virtual, edited by Hugo\nLarochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan,\nand Hsuan-Tien Lin. https://proceedings.neurips.cc/paper/2020/hash/81f7acabd411274fcf65ce2070ed568a-Abstract.html.\n\n\nCai, Han, Ligeng Zhu, and Song Han. 2019.\n“ProxylessNAS: Direct Neural\nArchitecture Search on Target Task and Hardware.” In 7th\nInternational Conference on Learning Representations, ICLR 2019, New\nOrleans, LA, USA, May 6-9, 2019. OpenReview.net. https://openreview.net/forum?id=HylVB3AqYm.\n\n\nCalvo, Rafael A, Dorian Peters, Karina Vold, and Richard M Ryan. 2020.\n“Supporting Human Autonomy in AI Systems:\nA Framework for Ethical Enquiry.” Ethics of\nDigital Well-Being: A Multidisciplinary Approach, 31–54.\n\n\nCarlini, Nicholas, Pratyush Mishra, Tavish Vaidya, Yuankai Zhang, Micah\nSherr, Clay Shields, David Wagner, and Wenchao Zhou. 2016. “Hidden\nVoice Commands.” In 25th USENIX Security Symposium (USENIX\nSecurity 16), 513–30.\n\n\nCarlini, Nicolas, Jamie Hayes, Milad Nasr, Matthew Jagielski, Vikash\nSehwag, Florian Tramer, Borja Balle, Daphne Ippolito, and Eric Wallace.\n2023. “Extracting Training Data from Diffusion Models.” In\n32nd USENIX Security Symposium (USENIX Security 23), 5253–70.\n\n\nCarta, Salvatore, Alessandro Sebastian Podda, Diego Reforgiato Recupero,\nand Roberto Saia. 2020. “A Local Feature Engineering Strategy to\nImprove Network Anomaly Detection.” Future Internet 12\n(10): 177. https://doi.org/10.3390/fi12100177.\n\n\nCavoukian, Ann. 2009. “Privacy by Design.” Office of\nthe Information and Privacy Commissioner.\n\n\nCenci, Marcelo Pilotto, Tatiana Scarazzato, Daniel Dotto Munchen, Paula\nCristina Dartora, Hugo Marcelo Veit, Andrea Moura Bernardes, and Pablo\nR. Dias. 2021. “Eco-Friendly\nElectronicsA Comprehensive Review.”\nAdv. Mater. Technol. 7 (2): 2001263. https://doi.org/10.1002/admt.202001263.\n\n\nChallenge, WEF Net-Zero. 2021. “The Supply Chain\nOpportunity.” In World Economic Forum: Geneva,\nSwitzerland.\n\n\nChandola, Varun, Arindam Banerjee, and Vipin Kumar. 2009. “Anomaly\nDetection: A Survey.” ACM Comput. Surv. 41 (3): 1–58. https://doi.org/10.1145/1541880.1541882.\n\n\nChapelle, O., B. Scholkopf, and A. Zien Eds. 2009.\n“Semi-Supervised Learning (Chapelle, O.\nEt Al., Eds.; 2006) [Book Reviews].” IEEE Trans.\nNeural Networks 20 (3): 542–42. https://doi.org/10.1109/tnn.2009.2015974.\n\n\nChen, Chaofan, Oscar Li, Daniel Tao, Alina Barnett, Cynthia Rudin, and\nJonathan Su. 2019. “This Looks Like That: Deep\nLearning for Interpretable Image Recognition.” In Advances in\nNeural Information Processing Systems 32: Annual Conference on Neural\nInformation Processing Systems 2019, NeurIPS 2019, December 8-14, 2019,\nVancouver, BC, Canada, edited by Hanna M. Wallach, Hugo Larochelle,\nAlina Beygelzimer, Florence d’Alché-Buc, Emily B. Fox, and Roman\nGarnett, 8928–39. https://proceedings.neurips.cc/paper/2019/hash/adf7ee2dcf142b0e11888e72b43fcb75-Abstract.html.\n\n\nChen, Emma, Shvetank Prakash, Vijay Janapa Reddi, David Kim, and Pranav\nRajpurkar. 2023. “A Framework for Integrating Artificial\nIntelligence for Clinical Care with Continuous Therapeutic\nMonitoring.” Nature Biomedical Engineering, November. https://doi.org/10.1038/s41551-023-01115-0.\n\n\nChen, H.-W. 2006. “Gallium, Indium, and Arsenic Pollution of\nGroundwater from a Semiconductor Manufacturing Area of\nTaiwan.” B. Environ. Contam. Tox. 77 (2):\n289–96. https://doi.org/10.1007/s00128-006-1062-3.\n\n\nChen, Tianqi, Thierry Moreau, Ziheng Jiang, Lianmin Zheng, Eddie Yan,\nHaichen Shen, Meghan Cowan, et al. 2018. “TVM: An Automated\nEnd-to-End Optimizing Compiler for Deep Learning.” In 13th\nUSENIX Symposium on Operating Systems Design and Implementation (OSDI\n18), 578–94.\n\n\nChen, Tianqi, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. 2016.\n“Training Deep Nets with Sublinear Memory Cost.” ArXiv\nPreprint abs/1604.06174 (April). http://arxiv.org/abs/1604.06174v2.\n\n\nChen, Zhiyong, and Shugong Xu. 2023. “Learning\nDomain-Heterogeneous Speaker Recognition Systems with Personalized\nContinual Federated Learning.” EURASIP Journal on Audio,\nSpeech, and Music Processing 2023 (1): 33. https://doi.org/10.1186/s13636-023-00299-2.\n\n\nChen, Zitao, Guanpeng Li, Karthik Pattabiraman, and Nathan DeBardeleben.\n2019. “iBinFI/i: An Efficient Fault\nInjector for Safety-Critical Machine Learning Systems.” In\nProceedings of the International Conference for High Performance\nComputing, Networking, Storage and Analysis. SC ’19. New York, NY,\nUSA: ACM. https://doi.org/10.1145/3295500.3356177.\n\n\nChen, Zitao, Niranjhana Narayanan, Bo Fang, Guanpeng Li, Karthik\nPattabiraman, and Nathan DeBardeleben. 2020.\n“TensorFI: A Flexible Fault Injection\nFramework for TensorFlow Applications.” In 2020\nIEEE 31st International Symposium on Software Reliability Engineering\n(ISSRE), 426–35. IEEE; IEEE. https://doi.org/10.1109/issre5003.2020.00047.\n\n\nCheng, Eric, Shahrzad Mirkhani, Lukasz G. Szafaryn, Chen-Yong Cher,\nHyungmin Cho, Kevin Skadron, Mircea R. Stan, et al. 2016. “Clear:\nuC/u Ross u-l/u Ayer uE/u Xploration for uA/u Rchitecting uR/u Esilience\n- Combining Hardware and Software Techniques to Tolerate Soft Errors in\nProcessor Cores.” In Proceedings of the 53rd Annual Design\nAutomation Conference, 1–6. ACM. https://doi.org/10.1145/2897937.2897996.\n\n\nCheng, Yu, Duo Wang, Pan Zhou, and Tao Zhang. 2018. “Model\nCompression and Acceleration for Deep Neural Networks: The\nPrinciples, Progress, and Challenges.” IEEE Signal Process\nMag. 35 (1): 126–36. https://doi.org/10.1109/msp.2017.2765695.\n\n\nChi, Ping, Shuangchen Li, Cong Xu, Tao Zhang, Jishen Zhao, Yongpan Liu,\nYu Wang, and Yuan Xie. 2016. “Prime: A Novel Processing-in-Memory\nArchitecture for Neural Network Computation in ReRAM-Based Main\nMemory.” ACM SIGARCH Computer Architecture News 44 (3):\n27–39. https://doi.org/10.1145/3007787.3001140.\n\n\nChollet, François. 2018. “Introduction to Keras.” March\n9th.\n\n\nChristiano, Paul F., Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg,\nand Dario Amodei. 2017. “Deep Reinforcement Learning from Human\nPreferences.” In Advances in Neural Information Processing\nSystems 30: Annual Conference on Neural Information Processing Systems\n2017, December 4-9, 2017, Long Beach, CA, USA, edited by Isabelle\nGuyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S.\nV. N. Vishwanathan, and Roman Garnett, 4299–4307. https://proceedings.neurips.cc/paper/2017/hash/d5e2c0adad503c91f91df240d0cd4e49-Abstract.html.\n\n\nChu, Grace, Okan Arikan, Gabriel Bender, Weijun Wang, Achille Brighton,\nPieter-Jan Kindermans, Hanxiao Liu, Berkin Akin, Suyog Gupta, and Andrew\nHoward. 2021. “Discovering Multi-Hardware Mobile Models via\nArchitecture Search.” In 2021 IEEE/CVF Conference on Computer\nVision and Pattern Recognition Workshops (CVPRW), 3022–31. IEEE. https://doi.org/10.1109/cvprw53098.2021.00337.\n\n\nChua, L. 1971. “Memristor-the Missing Circuit Element.”\n#IEEE_J_CT# 18 (5): 507–19. https://doi.org/10.1109/tct.1971.1083337.\n\n\nChung, Jae-Won, Yile Gu, Insu Jang, Luoxi Meng, Nikhil Bansal, and\nMosharaf Chowdhury. 2023. “Perseus: Removing Energy\nBloat from Large Model Training.” ArXiv Preprint\nabs/2312.06902. https://arxiv.org/abs/2312.06902.\n\n\nCohen, Maxime C., Ruben Lobel, and Georgia Perakis. 2016. “The\nImpact of Demand Uncertainty on Consumer Subsidies for Green Technology\nAdoption.” Manage. Sci. 62 (5): 1235–58. https://doi.org/10.1287/mnsc.2015.2173.\n\n\nColeman, Cody, Edward Chou, Julian Katz-Samuels, Sean Culatana, Peter\nBailis, Alexander C. Berg, Robert Nowak, Roshan Sumbaly, Matei Zaharia,\nand I. Zeki Yalniz. 2022. “Similarity Search for Efficient Active\nLearning and Search of Rare Concepts.” Proceedings of the\nAAAI Conference on Artificial Intelligence 36 (6): 6402–10. https://doi.org/10.1609/aaai.v36i6.20591.\n\n\nColeman, Cody, Daniel Kang, Deepak Narayanan, Luigi Nardi, Tian Zhao,\nJian Zhang, Peter Bailis, Kunle Olukotun, Chris Ré, and Matei Zaharia.\n2019. “Analysis of DAWNBench, a Time-to-Accuracy Machine Learning\nPerformance Benchmark.” ACM SIGOPS Operating Systems\nReview 53 (1): 14–25. https://doi.org/10.1145/3352020.3352024.\n\n\nConstantinescu, Cristian. 2008. “Intermittent Faults and Effects\non Reliability of Integrated Circuits.” In 2008 Annual\nReliability and Maintainability Symposium, 370–74. IEEE; IEEE. https://doi.org/10.1109/rams.2008.4925824.\n\n\nCooper, Tom, Suzanne Fallender, Joyann Pafumi, Jon Dettling, Sebastien\nHumbert, and Lindsay Lessard. 2011. “A Semiconductor Company’s\nExamination of Its Water Footprint Approach.” In Proceedings\nof the 2011 IEEE International Symposium on Sustainable Systems and\nTechnology, 1–6. IEEE; IEEE. https://doi.org/10.1109/issst.2011.5936865.\n\n\nCope, Gord. 2009. “Pure Water, Semiconductors and the\nRecession.” Global Water Intelligence 10 (10).\n\n\nCourbariaux, Matthieu, Itay Hubara, Daniel Soudry, Ran El-Yaniv, and\nYoshua Bengio. 2016. “Binarized Neural Networks:\nTraining Deep Neural Networks with Weights and Activations\nConstrained to+ 1 or-1.” arXiv Preprint\narXiv:1602.02830.\n\n\nCrankshaw, Daniel, Xin Wang, Guilio Zhou, Michael J Franklin, Joseph E\nGonzalez, and Ion Stoica. 2017. “Clipper: A {Low-Latency} Online Prediction Serving System.”\nIn 14th USENIX Symposium on Networked Systems Design and\nImplementation (NSDI 17), 613–27.\n\n\nD’ignazio, Catherine, and Lauren F Klein. 2023. Data Feminism.\nMIT press.\n\n\nDarvish Rouhani, Bita, Azalia Mirhoseini, and Farinaz Koushanfar. 2017.\n“TinyDL: Just-in-Time Deep Learning Solution for Constrained\nEmbedded Systems.” In 2017 IEEE International Symposium on\nCircuits and Systems (ISCAS), 1–4. IEEE. https://doi.org/10.1109/iscas.2017.8050343.\n\n\nDavarzani, Samaneh, David Saucier, Purva Talegaonkar, Erin Parker, Alana\nTurner, Carver Middleton, Will Carroll, et al. 2023. “Closing the\nWearable Gap: Footankle\nKinematic Modeling via Deep Learning Models Based on a Smart Sock\nWearable.” Wearable Technologies 4. https://doi.org/10.1017/wtc.2023.3.\n\n\nDavid, Robert, Jared Duke, Advait Jain, Vijay Janapa Reddi, Nat\nJeffries, Jian Li, Nick Kreeger, et al. 2021. “Tensorflow Lite\nMicro: Embedded Machine Learning for Tinyml Systems.”\nProceedings of Machine Learning and Systems 3: 800–811.\n\n\nDavies, Emma. 2011. “Endangered Elements: Critical\nThinking.” https://www.rsc.org/images/Endangered\\%20Elements\\%20-\\%20Critical\\%20Thinking\\_tcm18-196054.pdf.\n\n\nDavies, Mike, Narayan Srinivasa, Tsung-Han Lin, Gautham Chinya,\nYongqiang Cao, Sri Harsha Choday, Georgios Dimou, et al. 2018.\n“Loihi: A Neuromorphic Manycore Processor with\non-Chip Learning.” IEEE Micro 38 (1): 82–99. https://doi.org/10.1109/mm.2018.112130359.\n\n\nDavies, Mike, Andreas Wild, Garrick Orchard, Yulia Sandamirskaya,\nGabriel A. Fonseca Guerra, Prasad Joshi, Philipp Plank, and Sumedh R.\nRisbud. 2021. “Advancing Neuromorphic Computing with Loihi:\nA Survey of Results and Outlook.” Proc.\nIEEE 109 (5): 911–34. https://doi.org/10.1109/jproc.2021.3067593.\n\n\nDavis, Jacqueline, Daniel Bizo, Andy Lawrence, Owen Rogers, and Max\nSmolaks. 2022. “Uptime Institute Global Data Center Survey\n2022.” Uptime Institute.\n\n\nDayarathna, Miyuru, Yonggang Wen, and Rui Fan. 2016. “Data Center\nEnergy Consumption Modeling: A Survey.” IEEE\nCommunications Surveys &Amp; Tutorials 18 (1): 732–94. https://doi.org/10.1109/comst.2015.2481183.\n\n\nDean, Jeffrey, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Quoc\nV. Le, Mark Z. Mao, et al. 2012. “Large Scale Distributed Deep\nNetworks.” In Advances in Neural Information Processing\nSystems 25: 26th Annual Conference on Neural Information Processing\nSystems 2012. Proceedings of a Meeting Held December 3-6, 2012, Lake\nTahoe, Nevada, United States, edited by Peter L. Bartlett, Fernando\nC. N. Pereira, Christopher J. C. Burges, Léon Bottou, and Kilian Q.\nWeinberger, 1232–40. https://proceedings.neurips.cc/paper/2012/hash/6aca97005c68f1206823815f66102863-Abstract.html.\n\n\nDeng, Jia, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Fei-Fei Li.\n2009. “ImageNet: A Large-Scale\nHierarchical Image Database.” In 2009 IEEE Conference on\nComputer Vision and Pattern Recognition, 248–55. IEEE. https://doi.org/10.1109/cvpr.2009.5206848.\n\n\nDesai, Tanvi, Felix Ritchie, Richard Welpton, et al. 2016. “Five\nSafes: Designing Data Access for Research.” Economics Working\nPaper Series 1601: 28.\n\n\nDevlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.\n“None.” In Proceedings of the 2019 Conference of the\nNorth, 4171–86. Minneapolis, Minnesota: Association for\nComputational Linguistics. https://doi.org/10.18653/v1/n19-1423.\n\n\nDhar, Sauptik, Junyao Guo, Jiayi (Jason) Liu, Samarth Tripathi, Unmesh\nKurup, and Mohak Shah. 2021. “A Survey of on-Device Machine\nLearning: An Algorithms and Learning Theory Perspective.” ACM\nTransactions on Internet of Things 2 (3): 1–49. https://doi.org/10.1145/3450494.\n\n\nDong, Xin, Barbara De Salvo, Meng Li, Chiao Liu, Zhongnan Qu, H. T.\nKung, and Ziyun Li. 2022. “SplitNets:\nDesigning Neural Architectures for Efficient Distributed\nComputing on Head-Mounted Systems.” In 2022 IEEE/CVF\nConference on Computer Vision and Pattern Recognition (CVPR),\n12549–59. IEEE. https://doi.org/10.1109/cvpr52688.2022.01223.\n\n\nDongarra, Jack J. 2009. “The Evolution of High Performance\nComputing on System z.” IBM J. Res. Dev. 53: 3–4.\n\n\nDuarte, Javier, Nhan Tran, Ben Hawks, Christian Herwig, Jules Muhizi,\nShvetank Prakash, and Vijay Janapa Reddi. 2022.\n“FastML Science Benchmarks: Accelerating\nReal-Time Scientific Edge Machine Learning.” ArXiv\nPreprint abs/2207.07958. https://arxiv.org/abs/2207.07958.\n\n\nDuchi, John C., Elad Hazan, and Yoram Singer. 2010. “Adaptive\nSubgradient Methods for Online Learning and Stochastic\nOptimization.” In COLT 2010 - the 23rd Conference on Learning\nTheory, Haifa, Israel, June 27-29, 2010, edited by Adam Tauman\nKalai and Mehryar Mohri, 257–69. Omnipress. http://colt2010.haifa.il.ibm.com/papers/COLT2010proceedings.pdf#page=265.\n\n\nDuisterhof, Bardienus P, Srivatsan Krishnan, Jonathan J Cruz, Colby R\nBanbury, William Fu, Aleksandra Faust, Guido CHE de Croon, and Vijay\nJanapa Reddi. 2019. “Learning to Seek: Autonomous\nSource Seeking with Deep Reinforcement Learning Onboard a Nano Drone\nMicrocontroller.” ArXiv Preprint abs/1909.11236. https://arxiv.org/abs/1909.11236.\n\n\nDuisterhof, Bardienus P., Shushuai Li, Javier Burgues, Vijay Janapa\nReddi, and Guido C. H. E. de Croon. 2021. “Sniffy Bug:\nA Fully Autonomous Swarm of Gas-Seeking Nano Quadcopters in\nCluttered Environments.” In 2021 IEEE/RSJ International\nConference on Intelligent Robots and Systems (IROS), 9099–9106.\nIEEE; IEEE. https://doi.org/10.1109/iros51168.2021.9636217.\n\n\nDürr, Marc, Gunnar Nissen, Kurt-Wolfram Sühs, Philipp Schwenkenbecher,\nChristian Geis, Marius Ringelstein, Hans-Peter Hartung, et al. 2021.\n“CSF Findings in Acute NMDAR and LGI1 Antibody–Associated\nAutoimmune Encephalitis.” Neurology Neuroimmunology &Amp;\nNeuroinflammation 8 (6). https://doi.org/10.1212/nxi.0000000000001086.\n\n\nDwork, Cynthia, Frank McSherry, Kobbi Nissim, and Adam Smith. 2006.\n“Calibrating Noise to Sensitivity in Private Data\nAnalysis.” In Theory of Cryptography, edited by Shai\nHalevi and Tal Rabin, 265–84. Berlin, Heidelberg: Springer Berlin\nHeidelberg. https://doi.org/10.1007/11681878\\_14.\n\n\nDwork, Cynthia, and Aaron Roth. 2013. “The Algorithmic Foundations\nof Differential Privacy.” Foundations and Trends® in\nTheoretical Computer Science 9 (3-4): 211–407. https://doi.org/10.1561/0400000042.\n\n\nEbrahimi, Khosrow, Gerard F. Jones, and Amy S. Fleischer. 2014. “A\nReview of Data Center Cooling Technology, Operating Conditions and the\nCorresponding Low-Grade Waste Heat Recovery Opportunities.”\nRenewable Sustainable Energy Rev. 31 (March): 622–38. https://doi.org/10.1016/j.rser.2013.12.007.\n\n\nEgwutuoha, Ifeanyi P., David Levy, Bran Selic, and Shiping Chen. 2013.\n“A Survey of Fault Tolerance Mechanisms and Checkpoint/Restart\nImplementations for High Performance Computing Systems.” The\nJournal of Supercomputing 65 (3): 1302–26. https://doi.org/10.1007/s11227-013-0884-0.\n\n\nEisenman, Assaf, Kiran Kumar Matam, Steven Ingram, Dheevatsa Mudigere,\nRaghuraman Krishnamoorthi, Krishnakumar Nair, Misha Smelyanskiy, and\nMurali Annavaram. 2022. “Check-n-Run: A Checkpointing\nSystem for Training Deep Learning Recommendation Models.” In\n19th USENIX Symposium on Networked Systems Design and Implementation\n(NSDI 22), 929–43.\n\n\nEldan, Ronen, and Mark Russinovich. 2023. “Who’s Harry Potter?\nApproximate Unlearning in LLMs.” ArXiv Preprint\nabs/2310.02238 (October). http://arxiv.org/abs/2310.02238v2.\n\n\nEl-Rayis, A. O. 2014. “Reconfigurable Architectures for the Next\nGeneration of Mobile Device Telecommunications Systems.” :\nhttps://www.researchgate.net/publication/292608967.\n\n\nEshraghian, Jason K., Max Ward, Emre O. Neftci, Xinxin Wang, Gregor\nLenz, Girish Dwivedi, Mohammed Bennamoun, Doo Seok Jeong, and Wei D. Lu.\n2023. “Training Spiking Neural Networks Using Lessons from Deep\nLearning.” Proc. IEEE 111 (9): 1016–54. https://doi.org/10.1109/jproc.2023.3308088.\n\n\nEsteva, Andre, Brett Kuprel, Roberto A. Novoa, Justin Ko, Susan M.\nSwetter, Helen M. Blau, and Sebastian Thrun. 2017.\n“Dermatologist-Level Classification of Skin Cancer with Deep\nNeural Networks.” Nature 542 (7639): 115–18. https://doi.org/10.1038/nature21056.\n\n\nEykholt, Kevin, Ivan Evtimov, Earlence Fernandes, Bo Li, Amir Rahmati,\nChaowei Xiao, Atul Prakash, Tadayoshi Kohno, and Dawn Song. 2017.\n“Robust Physical-World Attacks on Deep Learning Models.”\nArXiv Preprint abs/1707.08945. https://arxiv.org/abs/1707.08945.\n\n\nFahim, Farah, Benjamin Hawks, Christian Herwig, James Hirschauer, Sergo\nJindariani, Nhan Tran, Luca P. Carloni, et al. 2021. “Hls4ml:\nAn Open-Source Codesign Workflow to Empower Scientific\nLow-Power Machine Learning Devices.” https://arxiv.org/abs/2103.05579.\n\n\nFarah, Martha J. 2005. “Neuroethics: The Practical\nand the Philosophical.” Trends Cogn. Sci. 9 (1): 34–40.\nhttps://doi.org/10.1016/j.tics.2004.12.001.\n\n\nFarwell, James P., and Rafal Rohozinski. 2011. “Stuxnet and the\nFuture of Cyber War.” Survival 53 (1): 23–40. https://doi.org/10.1080/00396338.2011.555586.\n\n\nFowers, Jeremy, Kalin Ovtcharov, Michael Papamichael, Todd Massengill,\nMing Liu, Daniel Lo, Shlomi Alkalay, et al. 2018. “A Configurable\nCloud-Scale DNN Processor for Real-Time\nAI.” In 2018 ACM/IEEE 45th Annual International\nSymposium on Computer Architecture (ISCA), 1–14. IEEE; IEEE. https://doi.org/10.1109/isca.2018.00012.\n\n\nFrancalanza, Adrian, Luca Aceto, Antonis Achilleos, Duncan Paul Attard,\nIan Cassar, Dario Della Monica, and Anna Ingólfsdóttir. 2017. “A\nFoundation for Runtime Monitoring.” In International\nConference on Runtime Verification, 8–29. Springer.\n\n\nFrankle, Jonathan, and Michael Carbin. 2019. “The Lottery Ticket\nHypothesis: Finding Sparse, Trainable Neural\nNetworks.” In 7th International Conference on Learning\nRepresentations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.\nOpenReview.net. https://openreview.net/forum?id=rJl-b3RcF7.\n\n\nFriedman, Batya. 1996. “Value-Sensitive Design.”\nInteractions 3 (6): 16–23. https://doi.org/10.1145/242485.242493.\n\n\nFurber, Steve. 2016. “Large-Scale Neuromorphic Computing\nSystems.” J. Neural Eng. 13 (5): 051001. https://doi.org/10.1088/1741-2560/13/5/051001.\n\n\nFursov, Ivan, Matvey Morozov, Nina Kaploukhaya, Elizaveta Kovtun,\nRodrigo Rivera-Castro, Gleb Gusev, Dmitry Babaev, Ivan Kireev, Alexey\nZaytsev, and Evgeny Burnaev. 2021. “Adversarial Attacks on Deep\nModels for Financial Transaction Records.” In Proceedings of\nthe 27th ACM SIGKDD Conference on Knowledge Discovery &Amp; Data\nMining, 2868–78. ACM. https://doi.org/10.1145/3447548.3467145.\n\n\nGale, Trevor, Erich Elsen, and Sara Hooker. 2019. “The State of\nSparsity in Deep Neural Networks.” ArXiv Preprint\nabs/1902.09574. https://arxiv.org/abs/1902.09574.\n\n\nGandolfi, Karine, Christophe Mourtel, and Francis Olivier. 2001.\n“Electromagnetic Analysis: Concrete Results.” In\nCryptographic Hardware and Embedded Systems — CHES 2001,\n251–61. Springer; Springer Berlin Heidelberg. https://doi.org/10.1007/3-540-44709-1\\_21.\n\n\nGannot, G., and M. Ligthart. 1994. “Verilog HDL Based\nFPGA Design.” In International Verilog HDL\nConference, 86–92. IEEE. https://doi.org/10.1109/ivc.1994.323743.\n\n\nGao, Yansong, Said F. Al-Sarawi, and Derek Abbott. 2020. “Physical\nUnclonable Functions.” Nature Electronics 3 (2): 81–91.\nhttps://doi.org/10.1038/s41928-020-0372-5.\n\n\nGates, Byron D. 2009. “Flexible Electronics.”\nScience 323 (5921): 1566–67. https://doi.org/10.1126/science.1171230.\n\n\nGebru, Timnit, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman\nVaughan, Hanna Wallach, Hal Daumé III, and Kate Crawford. 2021.\n“Datasheets for Datasets.” Commun. ACM 64 (12):\n86–92. https://doi.org/10.1145/3458723.\n\n\nGeiger, Atticus, Hanson Lu, Thomas Icard, and Christopher Potts. 2021.\n“Causal Abstractions of Neural Networks.” In Advances\nin Neural Information Processing Systems 34: Annual Conference on Neural\nInformation Processing Systems 2021, NeurIPS 2021, December 6-14, 2021,\nVirtual, edited by Marc’Aurelio Ranzato, Alina Beygelzimer, Yann N.\nDauphin, Percy Liang, and Jennifer Wortman Vaughan, 9574–86. https://proceedings.neurips.cc/paper/2021/hash/4f5c422f4d49a5a807eda27434231040-Abstract.html.\n\n\nGholami, Dong Kim, Mahoney Yao, and Keutzer. 2021. “A Survey of\nQuantization Methods for Efficient Neural Network Inference).”\nArXiv Preprint. https://arxiv.org/abs/2103.13630.\n\n\nGlorot, Xavier, and Yoshua Bengio. 2010. “Understanding the\nDifficulty of Training Deep Feedforward Neural Networks.” In\nProceedings of the Thirteenth International Conference on Artificial\nIntelligence and Statistics, 249–56. http://proceedings.mlr.press/v9/glorot10a.html.\n\n\nGnad, Dennis R. E., Fabian Oboril, and Mehdi B. Tahoori. 2017.\n“Voltage Drop-Based Fault Attacks on FPGAs Using Valid\nBitstreams.” In 2017 27th International Conference on Field\nProgrammable Logic and Applications (FPL), 1–7. IEEE; IEEE. https://doi.org/10.23919/fpl.2017.8056840.\n\n\nGoodfellow, Ian, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David\nWarde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2020.\n“Generative Adversarial Networks.” Commun. ACM 63\n(11): 139–44. https://doi.org/10.1145/3422622.\n\n\nGoodyear, Victoria A. 2017. “Social Media, Apps and Wearable\nTechnologies: Navigating Ethical Dilemmas and\nProcedures.” Qualitative Research in Sport, Exercise and\nHealth 9 (3): 285–302. https://doi.org/10.1080/2159676x.2017.1303790.\n\n\nGoogle. n.d. “Information Quality Content Moderation.” https://blog.google/documents/83/.\n\n\nGordon, Ariel, Elad Eban, Ofir Nachum, Bo Chen, Hao Wu, Tien-Ju Yang,\nand Edward Choi. 2018. “MorphNet: Fast\n&Amp; Simple Resource-Constrained Structure Learning of Deep\nNetworks.” In 2018 IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 1586–95. IEEE. https://doi.org/10.1109/cvpr.2018.00171.\n\n\nGräfe, Ralf, Qutub Syed Sha, Florian Geissler, and Michael Paulitsch.\n2023. “Large-Scale Application of Fault Injection into\nPyTorch Models -an Extension to PyTorchFI for\nValidation Efficiency.” In 2023 53rd Annual IEEE/IFIP\nInternational Conference on Dependable Systems and Networks -\nSupplemental Volume (DSN-s), 56–62. IEEE; IEEE. https://doi.org/10.1109/dsn-s58398.2023.00025.\n\n\nGreengard, Samuel. 2021. The Internet of Things. The MIT Press.\nhttps://doi.org/10.7551/mitpress/13937.001.0001.\n\n\nGrossman, Elizabeth. 2007. High Tech Trash: Digital\nDevices, Hidden Toxics, and Human Health. Island press.\n\n\nGruslys, Audrunas, Rémi Munos, Ivo Danihelka, Marc Lanctot, and Alex\nGraves. 2016. “Memory-Efficient Backpropagation Through\nTime.” In Advances in Neural Information Processing Systems\n29: Annual Conference on Neural Information Processing Systems 2016,\nDecember 5-10, 2016, Barcelona, Spain, edited by Daniel D. Lee,\nMasashi Sugiyama, Ulrike von Luxburg, Isabelle Guyon, and Roman Garnett,\n4125–33. https://proceedings.neurips.cc/paper/2016/hash/a501bebf79d570651ff601788ea9d16d-Abstract.html.\n\n\nGu, Ivy. 2023. “Deep Learning Model Compression (Ii) by Ivy Gu\nMedium.” https://ivygdy.medium.com/deep-learning-model-compression-ii-546352ea9453.\n\n\nGujarati, Arpan, Reza Karimi, Safya Alzayat, Wei Hao, Antoine Kaufmann,\nYmir Vigfusson, and Jonathan Mace. 2020. “Serving DNNs Like\nClockwork: Performance Predictability from the Bottom Up.” In\n14th USENIX Symposium on Operating Systems Design and Implementation\n(OSDI 20), 443–62. https://www.usenix.org/conference/osdi20/presentation/gujarati.\n\n\nGuo, Chuan, Jacob Gardner, Yurong You, Andrew Gordon Wilson, and Kilian\nWeinberger. 2019. “Simple Black-Box Adversarial Attacks.”\nIn International Conference on Machine Learning, 2484–93. PMLR.\n\n\nGuo, Yutao, Hao Wang, Hui Zhang, Tong Liu, Zhaoguang Liang, Yunlong Xia,\nLi Yan, et al. 2019. “Mobile Photoplethysmographic Technology to\nDetect Atrial Fibrillation.” Journal of the American College\nof Cardiology 74 (19): 2365–75. https://doi.org/10.1016/j.jacc.2019.08.019.\n\n\nGupta, Maanak, Charankumar Akiri, Kshitiz Aryal, Eli Parker, and\nLopamudra Praharaj. 2023. “From ChatGPT to ThreatGPT: Impact of\nGenerative AI in Cybersecurity and Privacy.” IEEE Access\n11: 80218–45. https://doi.org/10.1109/access.2023.3300381.\n\n\nGupta, Maya, Andrew Cotter, Jan Pfeifer, Konstantin Voevodski, Kevin\nCanini, Alexander Mangylov, Wojciech Moczydlowski, and Alexander Van\nEsbroeck. 2016. “Monotonic Calibrated Interpolated Look-up\nTables.” The Journal of Machine Learning Research 17\n(1): 3790–3836.\n\n\nGupta, Udit, Mariam Elgamal, Gage Hills, Gu-Yeon Wei, Hsien-Hsin S. Lee,\nDavid Brooks, and Carole-Jean Wu. 2022. “Act: Designing\nSustainable Computer Systems with an Architectural Carbon Modeling\nTool.” In Proceedings of the 49th Annual International\nSymposium on Computer Architecture, 784–99. ACM. https://doi.org/10.1145/3470496.3527408.\n\n\nGwennap, Linley. n.d. “Certus-NX Innovates\nGeneral-Purpose FPGAs.”\n\n\nHaensch, Wilfried, Tayfun Gokmen, and Ruchir Puri. 2019. “The Next\nGeneration of Deep Learning Hardware: Analog\nComputing.” Proc. IEEE 107 (1): 108–22. https://doi.org/10.1109/jproc.2018.2871057.\n\n\nHamming, R. W. 1950. “Error Detecting and Error Correcting\nCodes.” Bell Syst. Tech. J. 29 (2): 147–60. https://doi.org/10.1002/j.1538-7305.1950.tb00463.x.\n\n\nHan, Song, Huizi Mao, and William J Dally. 2015. “Deep\nCompression: Compressing Deep Neural Networks with Pruning,\nTrained Quantization and Huffman Coding.” arXiv Preprint\narXiv:1510.00149.\n\n\nHan, Song, Huizi Mao, and William J. Dally. 2016. “Deep\nCompression: Compressing Deep Neural Networks with Pruning,\nTrained Quantization and Huffman Coding.” https://arxiv.org/abs/1510.00149.\n\n\nHandlin, Oscar. 1965. “Science and Technology in Popular\nCulture.” Daedalus-Us., 156–70.\n\n\nHardt, Moritz, Eric Price, and Nati Srebro. 2016. “Equality of\nOpportunity in Supervised Learning.” In Advances in Neural\nInformation Processing Systems 29: Annual Conference on Neural\nInformation Processing Systems 2016, December 5-10, 2016, Barcelona,\nSpain, edited by Daniel D. Lee, Masashi Sugiyama, Ulrike von\nLuxburg, Isabelle Guyon, and Roman Garnett, 3315–23. https://proceedings.neurips.cc/paper/2016/hash/9d2682367c3935defcb1f9e247a97c0d-Abstract.html.\n\n\nHawks, Benjamin, Javier Duarte, Nicholas J. Fraser, Alessandro\nPappalardo, Nhan Tran, and Yaman Umuroglu. 2021. “Ps and Qs: Quantization-aware Pruning for Efficient Low\nLatency Neural Network Inference.” Frontiers in Artificial\nIntelligence 4 (July). https://doi.org/10.3389/frai.2021.676564.\n\n\nHazan, Avi, and Elishai Ezra Tsur. 2021. “Neuromorphic Analog\nImplementation of Neural Engineering Framework-Inspired Spiking Neuron\nfor High-Dimensional Representation.” Front. Neurosci.\n15 (February): 627221. https://doi.org/10.3389/fnins.2021.627221.\n\n\nHe, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2015.\n“Delving Deep into Rectifiers: Surpassing Human-Level Performance\non ImageNet Classification.” In 2015 IEEE International\nConference on Computer Vision (ICCV), 1026–34. IEEE. https://doi.org/10.1109/iccv.2015.123.\n\n\n———. 2016. “Deep Residual Learning for Image Recognition.”\nIn 2016 IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR), 770–78. IEEE. https://doi.org/10.1109/cvpr.2016.90.\n\n\nHe, Yi, Prasanna Balaprakash, and Yanjing Li. 2020.\n“FIdelity: Efficient Resilience Analysis\nFramework for Deep Learning Accelerators.” In 2020 53rd\nAnnual IEEE/ACM International Symposium on Microarchitecture\n(MICRO), 270–81. IEEE; IEEE. https://doi.org/10.1109/micro50266.2020.00033.\n\n\nHe, Yi, Mike Hutton, Steven Chan, Robert De Gruijl, Rama Govindaraju,\nNishant Patil, and Yanjing Li. 2023. “Understanding and Mitigating\nHardware Failures in Deep Learning Training Systems.” In\nProceedings of the 50th Annual International Symposium on Computer\nArchitecture, 1–16. IEEE; ACM. https://doi.org/10.1145/3579371.3589105.\n\n\nHébert-Johnson, Úrsula, Michael P. Kim, Omer Reingold, and Guy N.\nRothblum. 2018. “Multicalibration: Calibration for\nthe (Computationally-Identifiable) Masses.” In\nProceedings of the 35th International Conference on Machine\nLearning, ICML 2018, Stockholmsmässan, Stockholm, Sweden, July 10-15,\n2018, edited by Jennifer G. Dy and Andreas Krause, 80:1944–53.\nProceedings of Machine Learning Research. PMLR. http://proceedings.mlr.press/v80/hebert-johnson18a.html.\n\n\nHegde, Sumant. 2023. “An Introduction to Separable Convolutions -\nAnalytics Vidhya.” https://www.analyticsvidhya.com/blog/2021/11/an-introduction-to-separable-convolutions/.\n\n\nHenderson, Peter, Jieru Hu, Joshua Romoff, Emma Brunskill, Dan Jurafsky,\nand Joelle Pineau. 2020. “Towards the Systematic Reporting of the\nEnergy and Carbon Footprints of Machine Learning.” The\nJournal of Machine Learning Research 21 (1): 10039–81.\n\n\nHendrycks, Dan, and Thomas Dietterich. 2019. “Benchmarking Neural\nNetwork Robustness to Common Corruptions and Perturbations.”\narXiv Preprint arXiv:1903.12261.\n\n\nHendrycks, Dan, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn\nSong. 2021. “Natural Adversarial Examples.” In 2021\nIEEE/CVF Conference on Computer Vision and Pattern Recognition\n(CVPR), 15257–66. IEEE. https://doi.org/10.1109/cvpr46437.2021.01501.\n\n\nHennessy, John L., and David A. Patterson. 2019. “A New Golden Age\nfor Computer Architecture.” Commun. ACM 62 (2): 48–60.\nhttps://doi.org/10.1145/3282307.\n\n\nHeyndrickx, Wouter, Lewis Mervin, Tobias Morawietz, Noé Sturm, Lukas\nFriedrich, Adam Zalewski, Anastasia Pentina, et al. 2023.\n“Melloddy: Cross-Pharma Federated Learning at Unprecedented Scale\nUnlocks Benefits in Qsar Without Compromising Proprietary\nInformation.” Journal of Chemical Information and\nModeling 64 (7): 2331–44. https://pubs.acs.org/doi/10.1021/acs.jcim.3c00799.\n\n\nHimmelstein, Gracie, David Bates, and Li Zhou. 2022. “Examination\nof Stigmatizing Language in the Electronic Health Record.”\nJAMA Network Open 5 (1): e2144967. https://doi.org/10.1001/jamanetworkopen.2021.44967.\n\n\nHinton, Geoffrey. 2005. “Van Nostrand’s Scientific Encyclopedia.” Wiley.\nhttps://doi.org/10.1002/0471743984.vse0673.\n\n\n———. 2017. “Overview of Minibatch Gradient Descent.”\nUniversity of Toronto; University Lecture.\n\n\nHo Yoon, Jung, Hyung-Suk Jung, Min Hwan Lee, Gun Hwan Kim, Seul Ji Song,\nJun Yeong Seok, Kyung Jean Yoon, et al. 2012. “Frontiers in\nElectronic Materials.” Wiley. https://doi.org/10.1002/9783527667703.ch67.\n\n\nHoefler, Torsten, Dan Alistarh, Tal Ben-Nun, Nikoli Dryden, and\nAlexandra Peste. 2021. “Sparsity in Deep Learning: Pruning and\nGrowth for Efficient Inference and Training in Neural Networks,”\nJanuary. http://arxiv.org/abs/2102.00554v1.\n\n\nHolland, Sarah, Ahmed Hosny, Sarah Newman, Joshua Joseph, and Kasia\nChmielinski. 2020. “The Dataset Nutrition Label: A Framework to\nDrive Higher Data Quality Standards.” In Data Protection and\nPrivacy. Hart Publishing. https://doi.org/10.5040/9781509932771.ch-001.\n\n\nHong, Sanghyun, Nicholas Carlini, and Alexey Kurakin. 2023.\n“Publishing Efficient on-Device Models Increases Adversarial\nVulnerability.” In 2023 IEEE Conference on Secure and\nTrustworthy Machine Learning (SaTML), abs 1603 5279:271–90. IEEE;\nIEEE. https://doi.org/10.1109/satml54575.2023.00026.\n\n\nHooker, Sara. 2021. “The Hardware Lottery.”\nCommunications of the ACM 64 (12): 58–65. https://doi.org/10.1145/3467017.\n\n\nHosseini, Hossein, Sreeram Kannan, Baosen Zhang, and Radha Poovendran.\n2017. “Deceiving Google’s Perspective Api Built for Detecting\nToxic Comments.” ArXiv Preprint abs/1702.08138. https://arxiv.org/abs/1702.08138.\n\n\nHoward, Andrew G., Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun\nWang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. 2017.\n“MobileNets: Efficient Convolutional\nNeural Networks for Mobile Vision Applications.” ArXiv\nPreprint. https://arxiv.org/abs/1704.04861.\n\n\nHsiao, Yu-Shun, Zishen Wan, Tianyu Jia, Radhika Ghosal, Abdulrahman\nMahmoud, Arijit Raychowdhury, David Brooks, Gu-Yeon Wei, and Vijay\nJanapa Reddi. 2023. “MAVFI: An\nEnd-to-End Fault Analysis Framework with Anomaly Detection and Recovery\nfor Micro Aerial Vehicles.” In 2023 Design, Automation\n&Amp; Test in Europe Conference &Amp; Exhibition (DATE),\n1–6. IEEE; IEEE. https://doi.org/10.23919/date56975.2023.10137246.\n\n\nHsu, Liang-Ching, Ching-Yi Huang, Yen-Hsun Chuang, Ho-Wen Chen, Ya-Ting\nChan, Heng Yi Teah, Tsan-Yao Chen, Chiung-Fen Chang, Yu-Ting Liu, and\nYu-Min Tzou. 2016. “Accumulation of Heavy Metals and Trace\nElements in Fluvial Sediments Received Effluents from Traditional and\nSemiconductor Industries.” Scientific Reports 6 (1):\n34250. https://doi.org/10.1038/srep34250.\n\n\nHu, Jie, Li Shen, and Gang Sun. 2018. “Squeeze-and-Excitation\nNetworks.” In 2018 IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 7132–41. IEEE. https://doi.org/10.1109/cvpr.2018.00745.\n\n\nHu, Yang, Jie Jiang, Lifu Zhang, Yunfeng Shi, and Jian Shi. 2023.\n“Halide Perovskite Semiconductors.” Wiley. https://doi.org/10.1002/9783527829026.ch13.\n\n\nHuang, Tsung-Ching, Kenjiro Fukuda, Chun-Ming Lo, Yung-Hui Yeh, Tsuyoshi\nSekitani, Takao Someya, and Kwang-Ting Cheng. 2011.\n“Pseudo-CMOS: A Design Style for\nLow-Cost and Robust Flexible Electronics.” IEEE Trans.\nElectron Devices 58 (1): 141–50. https://doi.org/10.1109/ted.2010.2088127.\n\n\nHutter, Michael, Jorn-Marc Schmidt, and Thomas Plos. 2009.\n“Contact-Based Fault Injections and Power Analysis on RFID\nTags.” In 2009 European Conference on Circuit Theory and\nDesign, 409–12. IEEE; IEEE. https://doi.org/10.1109/ecctd.2009.5275012.\n\n\nIandola, Forrest N, Song Han, Matthew W Moskewicz, Khalid Ashraf,\nWilliam J Dally, and Kurt Keutzer. 2016. “SqueezeNet:\nAlexnet-level Accuracy with 50x Fewer\nParameters and 0.5 MB Model Size.” ArXiv\nPreprint abs/1602.07360. https://arxiv.org/abs/1602.07360.\n\n\nIgnatov, Andrey, Radu Timofte, William Chou, Ke Wang, Max Wu, Tim\nHartley, and Luc Van Gool. 2018. “AI Benchmark:\nRunning Deep Neural Networks on Android\nSmartphones,” 0–0.\n\n\nImani, Mohsen, Abbas Rahimi, and Tajana S. Rosing. 2016.\n“Resistive Configurable Associative Memory for Approximate\nComputing.” In Proceedings of the 2016 Design, Automation\n&Amp; Test in Europe Conference &Amp; Exhibition (DATE),\n1327–32. IEEE; Research Publishing Services. https://doi.org/10.3850/9783981537079_0454.\n\n\nIntelLabs. 2023. “Knowledge Distillation - Neural Network\nDistiller.” https://intellabs.github.io/distiller/knowledge_distillation.html.\n\n\nIppolito, Daphne, Florian Tramer, Milad Nasr, Chiyuan Zhang, Matthew\nJagielski, Katherine Lee, Christopher Choquette Choo, and Nicholas\nCarlini. 2023. “Preventing Generation of Verbatim Memorization in\nLanguage Models Gives a False Sense of Privacy.” In\nProceedings of the 16th International Natural Language Generation\nConference, 5253–70. Association for Computational Linguistics. https://doi.org/10.18653/v1/2023.inlg-main.3.\n\n\nIrimia-Vladu, Mihai. 2014.\n““Green” Electronics:\nBiodegradable and Biocompatible Materials and Devices for\nSustainable Future.” Chem. Soc. Rev. 43 (2): 588–610. https://doi.org/10.1039/c3cs60235d.\n\n\nIsscc. 2014. “Computing’s Energy Problem (and What We Can Do about\nIt).” https://ieeexplore.ieee.org/document/6757323.\n\n\nJacob, Benoit, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang,\nAndrew Howard, Hartwig Adam, and Dmitry Kalenichenko. 2018.\n“Quantization and Training of Neural Networks for Efficient\nInteger-Arithmetic-Only Inference.” In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recognition,\n2704–13.\n\n\nJaderberg, Max, Valentin Dalibard, Simon Osindero, Wojciech M.\nCzarnecki, Jeff Donahue, Ali Razavi, Oriol Vinyals, et al. 2017.\n“Population Based Training of Neural Networks.” arXiv\nPreprint arXiv:1711.09846, November. http://arxiv.org/abs/1711.09846v2.\n\n\nJanapa Reddi, Vijay, Alexander Elium, Shawn Hymel, David Tischler,\nDaniel Situnayake, Carl Ward, Louis Moreau, et al. 2023. “Edge\nImpulse: An MLOps Platform for Tiny Machine Learning.”\nProceedings of Machine Learning and Systems 5.\n\n\nJha, A. R. 2014. Rare Earth Materials: Properties and\nApplications. CRC Press. https://doi.org/10.1201/b17045.\n\n\nJha, Saurabh, Subho Banerjee, Timothy Tsai, Siva K. S. Hari, Michael B.\nSullivan, Zbigniew T. Kalbarczyk, Stephen W. Keckler, and Ravishankar K.\nIyer. 2019. “ML-Based Fault Injection for Autonomous\nVehicles: A Case for Bayesian Fault\nInjection.” In 2019 49th Annual IEEE/IFIP International\nConference on Dependable Systems and Networks (DSN), 112–24. IEEE;\nIEEE. https://doi.org/10.1109/dsn.2019.00025.\n\n\nJia, Yangqing, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan\nLong, Ross Girshick, Sergio Guadarrama, and Trevor Darrell. 2014.\n“Caffe: Convolutional Architecture for Fast Feature\nEmbedding.” In Proceedings of the 22nd ACM International\nConference on Multimedia, 675–78. ACM. https://doi.org/10.1145/2647868.2654889.\n\n\nJia, Zhe, Marco Maggioni, Benjamin Staiger, and Daniele P. Scarpazza.\n2018. “Dissecting the NVIDIA Volta\nGPU Architecture via Microbenchmarking.” ArXiv\nPreprint. https://arxiv.org/abs/1804.06826.\n\n\nJia, Zhenge, Dawei Li, Xiaowei Xu, Na Li, Feng Hong, Lichuan Ping, and\nYiyu Shi. 2023. “Life-Threatening Ventricular Arrhythmia Detection\nChallenge in Implantable\nCardioverterdefibrillators.” Nature Machine\nIntelligence 5 (5): 554–55. https://doi.org/10.1038/s42256-023-00659-9.\n\n\nJia, Zhihao, Matei Zaharia, and Alex Aiken. 2019. “Beyond Data and\nModel Parallelism for Deep Neural Networks.” In Proceedings\nof Machine Learning and Systems 2019, MLSys 2019, Stanford, CA, USA,\nMarch 31 - April 2, 2019, edited by Ameet Talwalkar, Virginia\nSmith, and Matei Zaharia. mlsys.org. https://proceedings.mlsys.org/book/265.pdf.\n\n\nJin, Yilun, Xiguang Wei, Yang Liu, and Qiang Yang. 2020. “Towards\nUtilizing Unlabeled Data in Federated Learning: A Survey and\nProspective.” arXiv Preprint arXiv:2002.11545, February.\nhttp://arxiv.org/abs/2002.11545v2.\n\n\nJohnson-Roberson, Matthew, Charles Barto, Rounak Mehta, Sharath Nittur\nSridhar, Karl Rosaen, and Ram Vasudevan. 2017. “Driving in the\nMatrix: Can Virtual Worlds Replace Human-Generated\nAnnotations for Real World Tasks?” In 2017 IEEE International\nConference on Robotics and Automation (ICRA), 746–53. Singapore,\nSingapore: IEEE. https://doi.org/10.1109/icra.2017.7989092.\n\n\nJouppi, Norman P., Cliff Young, Nishant Patil, David Patterson, Gaurav\nAgrawal, Raminder Bajwa, Sarah Bates, et al. 2017a. “In-Datacenter\nPerformance Analysis of a Tensor Processing Unit.” In\nProceedings of the 44th Annual International Symposium on Computer\nArchitecture, 1–12. ISCA ’17. New York, NY, USA: ACM. https://doi.org/10.1145/3079856.3080246.\n\n\n———, et al. 2017b. “In-Datacenter Performance Analysis of a Tensor\nProcessing Unit.” In Proceedings of the 44th Annual\nInternational Symposium on Computer Architecture, 1–12. ISCA ’17.\nNew York, NY, USA: ACM. https://doi.org/10.1145/3079856.3080246.\n\n\nJouppi, Norm, George Kurian, Sheng Li, Peter Ma, Rahul Nagarajan, Lifeng\nNai, Nishant Patil, et al. 2023. “TPU V4:\nAn Optically Reconfigurable Supercomputer for Machine\nLearning with Hardware Support for Embeddings.” In\nProceedings of the 50th Annual International Symposium on Computer\nArchitecture. ISCA ’23. New York, NY, USA: ACM. https://doi.org/10.1145/3579371.3589350.\n\n\nJoye, Marc, and Michael Tunstall. 2012. Fault Analysis in\nCryptography. Springer Berlin Heidelberg. https://doi.org/10.1007/978-3-642-29656-7.\n\n\nKairouz, Peter, Sewoong Oh, and Pramod Viswanath. 2015. “Secure\nMulti-Party Differential Privacy.” In Advances in Neural\nInformation Processing Systems 28: Annual Conference on Neural\nInformation Processing Systems 2015, December 7-12, 2015, Montreal,\nQuebec, Canada, edited by Corinna Cortes, Neil D. Lawrence, Daniel\nD. Lee, Masashi Sugiyama, and Roman Garnett, 2008–16. https://proceedings.neurips.cc/paper/2015/hash/a01610228fe998f515a72dd730294d87-Abstract.html.\n\n\nKalamkar, Dhiraj, Dheevatsa Mudigere, Naveen Mellempudi, Dipankar Das,\nKunal Banerjee, Sasikanth Avancha, Dharma Teja Vooturi, et al. 2019.\n“A Study of BFLOAT16 for Deep Learning\nTraining.” https://arxiv.org/abs/1905.12322.\n\n\nKao, Sheng-Chun, Geonhwa Jeong, and Tushar Krishna. 2020.\n“ConfuciuX: Autonomous Hardware Resource\nAssignment for DNN Accelerators Using Reinforcement\nLearning.” In 2020 53rd Annual IEEE/ACM International\nSymposium on Microarchitecture (MICRO), 622–36. IEEE; IEEE. https://doi.org/10.1109/micro50266.2020.00058.\n\n\nKao, Sheng-Chun, and Tushar Krishna. 2020. “Gamma: Automating the\nHW Mapping of DNN Models on Accelerators via Genetic Algorithm.”\nIn Proceedings of the 39th International Conference on\nComputer-Aided Design, 1–9. ACM. https://doi.org/10.1145/3400302.3415639.\n\n\nKaplan, Jared, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin\nChess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario\nAmodei. 2020. “Scaling Laws for Neural Language Models.”\nArXiv Preprint abs/2001.08361. https://arxiv.org/abs/2001.08361.\n\n\nKarargyris, Alexandros, Renato Umeton, Micah J. Sheller, Alejandro\nAristizabal, Johnu George, Anna Wuest, Sarthak Pati, et al. 2023.\n“Federated Benchmarking of Medical Artificial Intelligence with\nMedPerf.” Nature Machine Intelligence 5 (7): 799–810. https://doi.org/10.1038/s42256-023-00652-2.\n\n\nKaur, Harmanpreet, Harsha Nori, Samuel Jenkins, Rich Caruana, Hanna\nWallach, and Jennifer Wortman Vaughan. 2020. “Interpreting\nInterpretability: Understanding Data Scientists’ Use of\nInterpretability Tools for Machine Learning.” In Proceedings\nof the 2020 CHI Conference on Human Factors in Computing Systems,\nedited by Regina Bernhaupt, Florian ’Floyd’Mueller, David Verweij, Josh\nAndres, Joanna McGrenere, Andy Cockburn, Ignacio Avellino, et al., 1–14.\nACM. https://doi.org/10.1145/3313831.3376219.\n\n\nKawazoe Aguilera, Marcos, Wei Chen, and Sam Toueg. 1997.\n“Heartbeat: A Timeout-Free Failure Detector for\nQuiescent Reliable Communication.” In Distributed Algorithms:\n11th International Workshop, WDAG’97 Saarbrücken, Germany, September\n2426, 1997 Proceedings 11, 126–40. Springer.\n\n\nKhan, Mohammad Emtiyaz, and Siddharth Swaroop. 2021.\n“Knowledge-Adaptation Priors.” In Advances in Neural\nInformation Processing Systems 34: Annual Conference on Neural\nInformation Processing Systems 2021, NeurIPS 2021, December 6-14, 2021,\nVirtual, edited by Marc’Aurelio Ranzato, Alina Beygelzimer, Yann N.\nDauphin, Percy Liang, and Jennifer Wortman Vaughan, 19757–70. https://proceedings.neurips.cc/paper/2021/hash/a4380923dd651c195b1631af7c829187-Abstract.html.\n\n\nKiela, Douwe, Max Bartolo, Yixin Nie, Divyansh Kaushik, Atticus Geiger,\nZhengxuan Wu, Bertie Vidgen, et al. 2021. “Dynabench: Rethinking\nBenchmarking in NLP.” In Proceedings of the 2021 Conference\nof the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, 4110–24. Online:\nAssociation for Computational Linguistics. https://doi.org/10.18653/v1/2021.naacl-main.324.\n\n\nKim, Jungrae, Michael Sullivan, and Mattan Erez. 2015. “Bamboo\nECC: Strong, Safe, and Flexible Codes for\nReliable Computer Memory.” In 2015 IEEE 21st International\nSymposium on High Performance Computer Architecture (HPCA), 101–12.\nIEEE; IEEE. https://doi.org/10.1109/hpca.2015.7056025.\n\n\nKim, Sunju, Chungsik Yoon, Seunghon Ham, Jihoon Park, Ohun Kwon, Donguk\nPark, Sangjun Choi, Seungwon Kim, Kwonchul Ha, and Won Kim. 2018.\n“Chemical Use in the Semiconductor Manufacturing Industry.”\nInt. J. Occup. Env. Heal. 24 (3-4): 109–18. https://doi.org/10.1080/10773525.2018.1519957.\n\n\nKingma, Diederik P., and Jimmy Ba. 2014. “Adam: A Method for\nStochastic Optimization.” Edited by Yoshua Bengio and Yann LeCun,\nDecember. http://arxiv.org/abs/1412.6980v9.\n\n\nKirkpatrick, James, Razvan Pascanu, Neil Rabinowitz, Joel Veness,\nGuillaume Desjardins, Andrei A. Rusu, Kieran Milan, et al. 2017.\n“Overcoming Catastrophic Forgetting in Neural Networks.”\nProc. Natl. Acad. Sci. 114 (13): 3521–26. https://doi.org/10.1073/pnas.1611835114.\n\n\nKo, Yohan. 2021. “Characterizing System-Level Masking Effects\nAgainst Soft Errors.” Electronics 10 (18): 2286. https://doi.org/10.3390/electronics10182286.\n\n\nKocher, Paul, Jann Horn, Anders Fogh, Daniel Genkin, Daniel Gruss,\nWerner Haas, Mike Hamburg, et al. 2019a. “Spectre Attacks:\nExploiting Speculative Execution.” In 2019 IEEE Symposium on\nSecurity and Privacy (SP), 1–19. IEEE. https://doi.org/10.1109/sp.2019.00002.\n\n\n———, et al. 2019b. “Spectre Attacks: Exploiting Speculative\nExecution.” In 2019 IEEE Symposium on Security and Privacy\n(SP), 1–19. IEEE. https://doi.org/10.1109/sp.2019.00002.\n\n\nKocher, Paul, Joshua Jaffe, and Benjamin Jun. 1999. “Differential\nPower Analysis.” In Advances in Cryptology — CRYPTO’ 99,\n388–97. Springer; Springer Berlin Heidelberg. https://doi.org/10.1007/3-540-48405-1\\_25.\n\n\nKocher, Paul, Joshua Jaffe, Benjamin Jun, and Pankaj Rohatgi. 2011.\n“Introduction to Differential Power Analysis.” Journal\nof Cryptographic Engineering 1 (1): 5–27. https://doi.org/10.1007/s13389-011-0006-y.\n\n\nKoh, Pang Wei, Thao Nguyen, Yew Siang Tang, Stephen Mussmann, Emma\nPierson, Been Kim, and Percy Liang. 2020. “Concept Bottleneck\nModels.” In Proceedings of the 37th International Conference\non Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event,\n119:5338–48. Proceedings of Machine Learning Research. PMLR. http://proceedings.mlr.press/v119/koh20a.html.\n\n\nKoh, Pang Wei, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin\nZhang, Akshay Balsubramani, Weihua Hu, et al. 2021. “WILDS: A\nBenchmark of in-the-Wild Distribution Shifts.” In Proceedings\nof the 38th International Conference on Machine Learning, ICML 2021,\n18-24 July 2021, Virtual Event, edited by Marina Meila and Tong\nZhang, 139:5637–64. Proceedings of Machine Learning Research. PMLR. http://proceedings.mlr.press/v139/koh21a.html.\n\n\nKoren, Yehuda, Robert Bell, and Chris Volinsky. 2009. “Matrix\nFactorization Techniques for Recommender Systems.”\nComputer 42 (8): 30–37. https://doi.org/10.1109/mc.2009.263.\n\n\nKrishna, Adithya, Srikanth Rohit Nudurupati, Chandana D G, Pritesh\nDwivedi, André van Schaik, Mahesh Mehendale, and Chetan Singh Thakur.\n2023. “RAMAN: A Re-Configurable and\nSparse TinyML Accelerator for Inference on Edge.” https://arxiv.org/abs/2306.06493.\n\n\nKrishnamoorthi. 2018. “Quantizing Deep Convolutional Networks for\nEfficient Inference: A Whitepaper.” ArXiv\nPreprint. https://arxiv.org/abs/1806.08342.\n\n\nKrishnan, Rayan, Pranav Rajpurkar, and Eric J. Topol. 2022.\n“Self-Supervised Learning in Medicine and Healthcare.”\nNat. Biomed. Eng. 6 (12): 1346–52. https://doi.org/10.1038/s41551-022-00914-1.\n\n\nKrishnan, Srivatsan, Natasha Jaques, Shayegan Omidshafiei, Dan Zhang,\nIzzeddin Gur, Vijay Janapa Reddi, and Aleksandra Faust. 2022.\n“Multi-Agent Reinforcement Learning for Microprocessor Design\nSpace Exploration.” https://arxiv.org/abs/2211.16385.\n\n\nKrishnan, Srivatsan, Amir Yazdanbakhsh, Shvetank Prakash, Jason Jabbour,\nIkechukwu Uchendu, Susobhan Ghosh, Behzad Boroujerdian, et al. 2023.\n“ArchGym: An Open-Source Gymnasium for\nMachine Learning Assisted Architecture Design.” In\nProceedings of the 50th Annual International Symposium on Computer\nArchitecture, 1–16. ACM. https://doi.org/10.1145/3579371.3589049.\n\n\nKrizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. 2012.\n“ImageNet Classification with Deep Convolutional\nNeural Networks.” In Advances in Neural Information\nProcessing Systems 25: 26th Annual Conference on Neural Information\nProcessing Systems 2012. Proceedings of a Meeting Held December 3-6,\n2012, Lake Tahoe, Nevada, United States, edited by Peter L.\nBartlett, Fernando C. N. Pereira, Christopher J. C. Burges, Léon Bottou,\nand Kilian Q. Weinberger, 1106–14. https://proceedings.neurips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html.\n\n\n———. 2017. “ImageNet Classification with Deep\nConvolutional Neural Networks.” Edited by F. Pereira, C. J.\nBurges, L. Bottou, and K. Q. Weinberger. Commun. ACM 60 (6):\n84–90. https://doi.org/10.1145/3065386.\n\n\nKung, Hsiang Tsung, and Charles E Leiserson. 1979. “Systolic\nArrays (for VLSI).” In Sparse Matrix Proceedings\n1978, 1:256–82. Society for industrial; applied mathematics\nPhiladelphia, PA, USA.\n\n\nKurth, Thorsten, Shashank Subramanian, Peter Harrington, Jaideep Pathak,\nMorteza Mardani, David Hall, Andrea Miele, Karthik Kashinath, and Anima\nAnandkumar. 2023. “FourCastNet:\nAccelerating Global High-Resolution Weather Forecasting\nUsing Adaptive Fourier Neural Operators.” In\nProceedings of the Platform for Advanced Scientific Computing\nConference, 1–11. ACM. https://doi.org/10.1145/3592979.3593412.\n\n\nKuzmin, Andrey, Mart Van Baalen, Yuwei Ren, Markus Nagel, Jorn Peters,\nand Tijmen Blankevoort. 2022. “FP8 Quantization:\nThe Power of the Exponent.” https://arxiv.org/abs/2208.09225.\n\n\nKuznetsova, Alina, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan\nKrasin, Jordi Pont-Tuset, Shahab Kamali, et al. 2020. “The Open\nImages Dataset V4: Unified Image Classification, Object\nDetection, and Visual Relationship Detection at Scale.”\nInternational Journal of Computer Vision 128 (7): 1956–81.\n\n\nKwon, Jisu, and Daejin Park. 2021. “Hardware/Software\nCo-Design for TinyML Voice-Recognition Application on\nResource Frugal Edge Devices.” Applied Sciences 11 (22):\n11073. https://doi.org/10.3390/app112211073.\n\n\nKwon, Sun Hwa, and Lin Dong. 2022. “Flexible Sensors and Machine\nLearning for Heart Monitoring.” Nano Energy 102\n(November): 107632. https://doi.org/10.1016/j.nanoen.2022.107632.\n\n\nKwon, Young D., Rui Li, Stylianos I. Venieris, Jagmohan Chauhan,\nNicholas D. Lane, and Cecilia Mascolo. 2023. “TinyTrain:\nResource-Aware Task-Adaptive Sparse Training of DNNs at the Data-Scarce\nEdge.” ArXiv Preprint abs/2307.09988 (July). http://arxiv.org/abs/2307.09988v2.\n\n\nLai, Liangzhen, Naveen Suda, and Vikas Chandra. 2018a. “Cmsis-Nn:\nEfficient Neural Network Kernels for Arm Cortex-m\nCpus.” ArXiv Preprint abs/1801.06601. https://arxiv.org/abs/1801.06601.\n\n\n———. 2018b. “CMSIS-NN:\nEfficient Neural Network Kernels for Arm Cortex-m\nCPUs.” https://arxiv.org/abs/1801.06601.\n\n\nLakkaraju, Himabindu, and Osbert Bastani. 2020.\n“”How Do i Fool You?”:\nManipulating User Trust via Misleading Black Box Explanations.”\nIn Proceedings of the AAAI/ACM Conference on AI, Ethics, and\nSociety, 79–85. ACM. https://doi.org/10.1145/3375627.3375833.\n\n\nLam, Remi, Alvaro Sanchez-Gonzalez, Matthew Willson, Peter Wirnsberger,\nMeire Fortunato, Ferran Alet, Suman Ravuri, et al. 2023. “Learning\nSkillful Medium-Range Global Weather Forecasting.”\nScience 382 (6677): 1416–21. https://doi.org/10.1126/science.adi2336.\n\n\nLannelongue, Loı̈c, Jason Grealey, and Michael Inouye. 2021. “Green\nAlgorithms: Quantifying the Carbon Footprint of\nComputation.” Adv. Sci. 8 (12): 2100707. https://doi.org/10.1002/advs.202100707.\n\n\nLeCun, Yann, John Denker, and Sara Solla. 1989. “Optimal Brain\nDamage.” Adv Neural Inf Process Syst 2.\n\n\nLee, Minwoong, Namho Lee, Huijeong Gwon, Jongyeol Kim, Younggwan Hwang,\nand Seongik Cho. 2022. “Design of Radiation-Tolerant High-Speed\nSignal Processing Circuit for Detecting Prompt Gamma Rays by Nuclear\nExplosion.” Electronics 11 (18): 2970. https://doi.org/10.3390/electronics11182970.\n\n\nLeRoy Poff, N, MM Brinson, and JW Day. 2002. “Aquatic Ecosystems\n& Global Climate Change.” Pew Center on Global Climate\nChange.\n\n\nLi, En, Liekang Zeng, Zhi Zhou, and Xu Chen. 2020. “Edge\nAI: On-demand Accelerating Deep\nNeural Network Inference via Edge Computing.” IEEE Trans.\nWireless Commun. 19 (1): 447–57. https://doi.org/10.1109/twc.2019.2946140.\n\n\nLi, Guanpeng, Siva Kumar Sastry Hari, Michael Sullivan, Timothy Tsai,\nKarthik Pattabiraman, Joel Emer, and Stephen W. Keckler. 2017.\n“Understanding Error Propagation in Deep Learning Neural Network\n(DNN) Accelerators and Applications.” In\nProceedings of the International Conference for High Performance\nComputing, Networking, Storage and Analysis, 1–12. ACM. https://doi.org/10.1145/3126908.3126964.\n\n\nLi, Jingzhen, Igbe Tobore, Yuhang Liu, Abhishek Kandwal, Lei Wang, and\nZedong Nie. 2021. “Non-Invasive Monitoring of Three Glucose Ranges\nBased on ECG by Using DBSCAN-CNN.” IEEE Journal of Biomedical\nand Health Informatics 25 (9): 3340–50. https://doi.org/10.1109/jbhi.2021.3072628.\n\n\nLi, Mu, David G. Andersen, Alexander J. Smola, and Kai Yu. 2014.\n“Communication Efficient Distributed Machine Learning with the\nParameter Server.” In Advances in Neural Information\nProcessing Systems 27: Annual Conference on Neural Information\nProcessing Systems 2014, December 8-13 2014, Montreal, Quebec,\nCanada, edited by Zoubin Ghahramani, Max Welling, Corinna Cortes,\nNeil D. Lawrence, and Kilian Q. Weinberger, 19–27. https://proceedings.neurips.cc/paper/2014/hash/1ff1de774005f8da13f42943881c655f-Abstract.html.\n\n\nLi, Qinbin, Zeyi Wen, Zhaomin Wu, Sixu Hu, Naibo Wang, Yuan Li, Xu Liu,\nand Bingsheng He. 2023. “A Survey on Federated Learning Systems:\nVision, Hype and Reality for Data Privacy and\nProtection.” IEEE Trans. Knowl. Data Eng. 35 (4):\n3347–66. https://doi.org/10.1109/tkde.2021.3124599.\n\n\nLi, Tian, Anit Kumar Sahu, Ameet Talwalkar, and Virginia Smith. 2020.\n“Federated Learning: Challenges, Methods, and Future\nDirections.” IEEE Signal Processing Magazine 37 (3):\n50–60. https://doi.org/10.1109/msp.2020.2975749.\n\n\nLi, Xiang, Tao Qin, Jian Yang, and Tie-Yan Liu. 2016. “LightRNN:\nMemory and Computation-Efficient Recurrent Neural Networks.” In\nAdvances in Neural Information Processing Systems 29: Annual\nConference on Neural Information Processing Systems 2016, December 5-10,\n2016, Barcelona, Spain, edited by Daniel D. Lee, Masashi Sugiyama,\nUlrike von Luxburg, Isabelle Guyon, and Roman Garnett, 4385–93. https://proceedings.neurips.cc/paper/2016/hash/c3e4035af2a1cde9f21e1ae1951ac80b-Abstract.html.\n\n\nLi, Yuhang, Xin Dong, and Wei Wang. 2020. “Additive Powers-of-Two\nQuantization: An Efficient Non-Uniform Discretization for\nNeural Networks.” In 8th International Conference on Learning\nRepresentations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30,\n2020. OpenReview.net. https://openreview.net/forum?id=BkgXT24tDS.\n\n\nLi, Zhizhong, and Derek Hoiem. 2018. “Learning Without\nForgetting.” IEEE Trans. Pattern Anal. Mach. Intell. 40\n(12): 2935–47. https://doi.org/10.1109/tpami.2017.2773081.\n\n\nLi, Zhuohan, Lianmin Zheng, Yinmin Zhong, Vincent Liu, Ying Sheng, Xin\nJin, Yanping Huang, et al. 2023. “{AlpaServe}:\nStatistical Multiplexing with Model Parallelism for Deep Learning\nServing.” In 17th USENIX Symposium on Operating Systems\nDesign and Implementation (OSDI 23), 663–79.\n\n\nLin, Ji, Wei-Ming Chen, Yujun Lin, John Cohn, Chuang Gan, and Song Han.\n2020. “MCUNet: Tiny Deep Learning on\nIoT Devices.” In Advances in Neural Information\nProcessing Systems 33: Annual Conference on Neural Information\nProcessing Systems 2020, NeurIPS 2020, December 6-12, 2020,\nVirtual, edited by Hugo Larochelle, Marc’Aurelio Ranzato, Raia\nHadsell, Maria-Florina Balcan, and Hsuan-Tien Lin. https://proceedings.neurips.cc/paper/2020/hash/86c51678350f656dcc7f490a43946ee5-Abstract.html.\n\n\nLin, Ji, Ligeng Zhu, Wei-Ming Chen, Wei-Chen Wang, Chuang Gan, and Song\nHan. 2022. “On-Device Training Under 256kb Memory.”\nAdv. Neur. In. 35: 22941–54.\n\n\nLin, Ji, Ligeng Zhu, Wei-Ming Chen, Wei-Chen Wang, and Song Han. 2023.\n“Tiny Machine Learning: Progress and Futures Feature.”\nIEEE Circuits Syst. Mag. 23 (3): 8–34. https://doi.org/10.1109/mcas.2023.3302182.\n\n\nLin, Tsung-Yi, Michael Maire, Serge Belongie, James Hays, Pietro Perona,\nDeva Ramanan, Piotr Dollár, and C Lawrence Zitnick. 2014.\n“Microsoft Coco: Common Objects in Context.”\nIn Computer VisionECCV 2014: 13th European Conference,\nZurich, Switzerland, September 6-12, 2014, Proceedings, Part v 13,\n740–55. Springer.\n\n\nLindgren, Simon. 2023. Handbook of Critical Studies of Artificial\nIntelligence. Edward Elgar Publishing.\n\n\nLindholm, Andreas, Dave Zachariah, Petre Stoica, and Thomas B. Schon.\n2019. “Data Consistency Approach to Model Validation.”\n#IEEE_O_ACC# 7: 59788–96. https://doi.org/10.1109/access.2019.2915109.\n\n\nLindholm, Erik, John Nickolls, Stuart Oberman, and John Montrym. 2008.\n“NVIDIA Tesla: A Unified Graphics and\nComputing Architecture.” IEEE Micro 28 (2): 39–55. https://doi.org/10.1109/mm.2008.31.\n\n\nLin, Tang Tang, Dang Yang, and Han Gan. 2023. “AWQ:\nActivation-aware Weight Quantization for\nLLM Compression and Acceleration.” ArXiv\nPreprint. https://arxiv.org/abs/2306.00978.\n\n\nLiu, Yanan, Xiaoxia Wei, Jinyu Xiao, Zhijie Liu, Yang Xu, and Yun Tian.\n2020. “Energy Consumption and Emission Mitigation Prediction Based\non Data Center Traffic and PUE for Global Data\nCenters.” Global Energy Interconnection 3 (3): 272–82.\nhttps://doi.org/10.1016/j.gloei.2020.07.008.\n\n\nLiu, Yingcheng, Guo Zhang, Christopher G. Tarolli, Rumen Hristov, Stella\nJensen-Roberts, Emma M. Waddell, Taylor L. Myers, et al. 2022.\n“Monitoring Gait at Home with Radio Waves in Parkinson’s Disease:\nA Marker of Severity, Progression, and Medication Response.”\nScience Translational Medicine 14 (663): eadc9669. https://doi.org/10.1126/scitranslmed.adc9669.\n\n\nLoh, Gabriel H. 2008. “3D-Stacked Memory\nArchitectures for Multi-Core Processors.” ACM SIGARCH\nComputer Architecture News 36 (3): 453–64. https://doi.org/10.1145/1394608.1382159.\n\n\nLopez-Paz, David, and Marc’Aurelio Ranzato. 2017. “Gradient\nEpisodic Memory for Continual Learning.” Adv Neural Inf\nProcess Syst 30.\n\n\nLou, Yin, Rich Caruana, Johannes Gehrke, and Giles Hooker. 2013.\n“Accurate Intelligible Models with Pairwise Interactions.”\nIn Proceedings of the 19th ACM SIGKDD International Conference on\nKnowledge Discovery and Data Mining, edited by Inderjit S. Dhillon,\nYehuda Koren, Rayid Ghani, Ted E. Senator, Paul Bradley, Rajesh Parekh,\nJingrui He, Robert L. Grossman, and Ramasamy Uthurusamy, 623–31. ACM. https://doi.org/10.1145/2487575.2487579.\n\n\nLowy, Andrew, Rakesh Pavan, Sina Baharlouei, Meisam Razaviyayn, and\nAhmad Beirami. 2021. “Fermi: Fair Empirical Risk\nMinimization via Exponential Rényi Mutual Information.”\n\n\nLubana, Ekdeep Singh, and Robert P Dick. 2020. “A Gradient Flow\nFramework for Analyzing Network Pruning.” arXiv Preprint\narXiv:2009.11839.\n\n\nLuebke, David. 2008. “CUDA: Scalable\nParallel Programming for High-Performance Scientific Computing.”\nIn 2008 5th IEEE International Symposium on Biomedical Imaging: From\nNano to Macro, 836–38. IEEE. https://doi.org/10.1109/isbi.2008.4541126.\n\n\nLundberg, Scott M., and Su-In Lee. 2017. “A Unified Approach to\nInterpreting Model Predictions.” In Advances in Neural\nInformation Processing Systems 30: Annual Conference on Neural\nInformation Processing Systems 2017, December 4-9, 2017, Long Beach, CA,\nUSA, edited by Isabelle Guyon, Ulrike von Luxburg, Samy Bengio,\nHanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett,\n4765–74. https://proceedings.neurips.cc/paper/2017/hash/8a20a8621978632d76c43dfd28b67767-Abstract.html.\n\n\nMa, Dongning, Fred Lin, Alban Desmaison, Joel Coburn, Daniel Moore,\nSriram Sankar, and Xun Jiao. 2024. “Dr.\nDNA: Combating Silent Data Corruptions in Deep\nLearning Using Distribution of Neuron Activations.” In\nProceedings of the 29th ACM International Conference on\nArchitectural Support for Programming Languages and Operating Systems,\nVolume 3, 239–52. ACM. https://doi.org/10.1145/3620666.3651349.\n\n\nMaas, Martin, David G. Andersen, Michael Isard, Mohammad Mahdi\nJavanmard, Kathryn S. McKinley, and Colin Raffel. 2024. “Combining\nMachine Learning and Lifetime-Based Resource Management for Memory\nAllocation and Beyond.” Commun. ACM 67 (4): 87–96. https://doi.org/10.1145/3611018.\n\n\nMaass, Wolfgang. 1997. “Networks of Spiking Neurons:\nThe Third Generation of Neural Network Models.”\nNeural Networks 10 (9): 1659–71. https://doi.org/10.1016/s0893-6080(97)00011-7.\n\n\nMadry, Aleksander, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras,\nand Adrian Vladu. 2017. “Towards Deep Learning Models Resistant to\nAdversarial Attacks.” arXiv Preprint arXiv:1706.06083.\n\n\nMahmoud, Abdulrahman, Neeraj Aggarwal, Alex Nobbe, Jose Rodrigo Sanchez\nVicarte, Sarita V. Adve, Christopher W. Fletcher, Iuri Frosio, and Siva\nKumar Sastry Hari. 2020. “PyTorchFI: A\nRuntime Perturbation Tool for DNNs.” In 2020\n50th Annual IEEE/IFIP International Conference on Dependable Systems and\nNetworks Workshops (DSN-w), 25–31. IEEE; IEEE. https://doi.org/10.1109/dsn-w50199.2020.00014.\n\n\nMahmoud, Abdulrahman, Siva Kumar Sastry Hari, Christopher W. Fletcher,\nSarita V. Adve, Charbel Sakr, Naresh Shanbhag, Pavlo Molchanov, Michael\nB. Sullivan, Timothy Tsai, and Stephen W. Keckler. 2021.\n“Optimizing Selective Protection for CNN\nResilience.” In 2021 IEEE 32nd International Symposium on\nSoftware Reliability Engineering (ISSRE), 127–38. IEEE. https://doi.org/10.1109/issre52982.2021.00025.\n\n\nMahmoud, Abdulrahman, Thierry Tambe, Tarek Aloui, David Brooks, and\nGu-Yeon Wei. 2022. “GoldenEye: A\nPlatform for Evaluating Emerging Numerical Data Formats in\nDNN Accelerators.” In 2022 52nd Annual IEEE/IFIP\nInternational Conference on Dependable Systems and Networks (DSN),\n206–14. IEEE. https://doi.org/10.1109/dsn53405.2022.00031.\n\n\nMarković, Danijela, Alice Mizrahi, Damien Querlioz, and Julie Grollier.\n2020. “Physics for Neuromorphic Computing.” Nature\nReviews Physics 2 (9): 499–510. https://doi.org/10.1038/s42254-020-0208-2.\n\n\nMartin, C. Dianne. 1993. “The Myth of the Awesome Thinking\nMachine.” Commun. ACM 36 (4): 120–33. https://doi.org/10.1145/255950.153587.\n\n\nMarulli, Fiammetta, Stefano Marrone, and Laura Verde. 2022.\n“Sensitivity of Machine Learning Approaches to Fake and Untrusted\nData in Healthcare Domain.” Journal of Sensor and Actuator\nNetworks 11 (2): 21. https://doi.org/10.3390/jsan11020021.\n\n\nMaslej, Nestor, Loredana Fattorini, Erik Brynjolfsson, John Etchemendy,\nKatrina Ligett, Terah Lyons, James Manyika, et al. 2023.\n“Artificial Intelligence Index Report 2023.” ArXiv\nPreprint abs/2310.03715. https://arxiv.org/abs/2310.03715.\n\n\nMattson, Peter, Vijay Janapa Reddi, Christine Cheng, Cody Coleman, Greg\nDiamos, David Kanter, Paulius Micikevicius, et al. 2020a. “MLPerf:\nAn Industry Standard Benchmark Suite for Machine Learning\nPerformance.” IEEE Micro 40 (2): 8–16. https://doi.org/10.1109/mm.2020.2974843.\n\n\n———, et al. 2020b. “MLPerf: An Industry\nStandard Benchmark Suite for Machine Learning Performance.”\nIEEE Micro 40 (2): 8–16. https://doi.org/10.1109/mm.2020.2974843.\n\n\nMazumder, Mark, Sharad Chitlangia, Colby Banbury, Yiping Kang, Juan\nManuel Ciro, Keith Achorn, Daniel Galvez, et al. 2021.\n“Multilingual Spoken Words Corpus.” In Thirty-Fifth\nConference on Neural Information Processing Systems Datasets and\nBenchmarks Track (Round 2).\n\n\nMcCarthy, John. 1981. “Epistemological Problems of Artificial\nIntelligence.” In Readings in Artificial Intelligence,\n459–65. Elsevier. https://doi.org/10.1016/b978-0-934613-03-3.50035-0.\n\n\nMcMahan, Brendan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise\nAgüera y Arcas. 2017a. “Communication-Efficient Learning of Deep\nNetworks from Decentralized Data.” In Proceedings of the 20th\nInternational Conference on Artificial Intelligence and Statistics,\nAISTATS 2017, 20-22 April 2017, Fort Lauderdale, FL, USA, edited by\nAarti Singh and Xiaojin (Jerry) Zhu, 54:1273–82. Proceedings of Machine\nLearning Research. PMLR. http://proceedings.mlr.press/v54/mcmahan17a.html.\n\n\n———. 2017b. “Communication-Efficient Learning of Deep Networks\nfrom Decentralized Data.” In Artificial Intelligence and\nStatistics, 1273–82. PMLR. http://proceedings.mlr.press/v54/mcmahan17a.html.\n\n\nMiller, Charlie. 2019. “Lessons Learned from Hacking a\nCar.” IEEE Design &Amp; Test 36 (6): 7–9. https://doi.org/10.1109/mdat.2018.2863106.\n\n\nMiller, Charlie, and Chris Valasek. 2015. “Remote Exploitation of\nan Unaltered Passenger Vehicle.” Black Hat USA 2015 (S\n91): 1–91.\n\n\nMiller, D. A. B. 2000. “Optical Interconnects to Silicon.”\n#IEEE_J_JSTQE# 6 (6): 1312–17. https://doi.org/10.1109/2944.902184.\n\n\nMills, Andrew, and Stephen Le Hunte. 1997. “An Overview of\nSemiconductor Photocatalysis.” J. Photochem. Photobiol.,\nA 108 (1): 1–35. https://doi.org/10.1016/s1010-6030(97)00118-4.\n\n\nMirhoseini, Azalia, Anna Goldie, Mustafa Yazgan, Joe Wenjie Jiang,\nEbrahim Songhori, Shen Wang, Young-Joon Lee, et al. 2021. “A Graph\nPlacement Methodology for Fast Chip Design.” Nature 594\n(7862): 207–12. https://doi.org/10.1038/s41586-021-03544-w.\n\n\nMishra, Asit K., Jorge Albericio Latorre, Jeff Pool, Darko Stosic, Dusan\nStosic, Ganesh Venkatesh, Chong Yu, and Paulius Micikevicius. 2021.\n“Accelerating Sparse Deep Neural Networks.” CoRR\nabs/2104.08378. https://arxiv.org/abs/2104.08378.\n\n\nMittal, Sparsh, Gaurav Verma, Brajesh Kaushik, and Farooq A. Khanday.\n2021. “A Survey of SRAM-Based in-Memory Computing\nTechniques and Applications.” J. Syst. Architect. 119\n(October): 102276. https://doi.org/10.1016/j.sysarc.2021.102276.\n\n\nModha, Dharmendra S., Filipp Akopyan, Alexander Andreopoulos,\nRathinakumar Appuswamy, John V. Arthur, Andrew S. Cassidy, Pallab Datta,\net al. 2023. “Neural Inference at the Frontier of Energy, Space,\nand Time.” Science 382 (6668): 329–35. https://doi.org/10.1126/science.adh1174.\n\n\nMohanram, K., and N. A. Touba. 2003. “Partial Error Masking to\nReduce Soft Error Failure Rate in Logic Circuits.” In\nProceedings. 16th IEEE Symposium on Computer Arithmetic,\n433–40. IEEE; IEEE Comput. Soc. https://doi.org/10.1109/dftvs.2003.1250141.\n\n\nMonyei, Chukwuka G., and Kirsten E. H. Jenkins. 2018. “Electrons\nHave No Identity: Setting Right Misrepresentations in\nGoogle and Apple’s Clean Energy Purchasing.”\nEnergy Research &Amp; Social Science 46 (December): 48–51.\nhttps://doi.org/10.1016/j.erss.2018.06.015.\n\n\nMoshawrab, Mohammad, Mehdi Adda, Abdenour Bouzouane, Hussein Ibrahim,\nand Ali Raad. 2023. “Reviewing Federated Learning Aggregation\nAlgorithms; Strategies, Contributions, Limitations and Future\nPerspectives.” Electronics 12 (10): 2287. https://doi.org/10.3390/electronics12102287.\n\n\nMukherjee, S. S., J. Emer, and S. K. Reinhardt. 2005. “The Soft\nError Problem: An Architectural Perspective.” In\n11th International Symposium on High-Performance Computer\nArchitecture, 243–47. IEEE; IEEE. https://doi.org/10.1109/hpca.2005.37.\n\n\nMunshi, Aaftab. 2009. “The OpenCL\nSpecification.” In 2009 IEEE Hot Chips 21 Symposium\n(HCS), 1–314. IEEE. https://doi.org/10.1109/hotchips.2009.7478342.\n\n\nMusk, Elon et al. 2019. “An Integrated Brain-Machine Interface\nPlatform with Thousands of Channels.” J. Med. Internet\nRes. 21 (10): e16194. https://doi.org/10.2196/16194.\n\n\nMyllyaho, Lalli, Mikko Raatikainen, Tomi Männistö, Jukka K. Nurminen,\nand Tommi Mikkonen. 2022. “On Misbehaviour and Fault Tolerance in\nMachine Learning Systems.” J. Syst. Software 183\n(January): 111096. https://doi.org/10.1016/j.jss.2021.111096.\n\n\nNakano, Jane. 2021. The Geopolitics of Critical Minerals Supply\nChains. JSTOR.\n\n\nNarayanan, Arvind, and Vitaly Shmatikov. 2006. “How to Break\nAnonymity of the Netflix Prize Dataset.” CoRR. http://arxiv.org/abs/cs/0610105.\n\n\nNg, Davy Tsz Kit, Jac Ka Lok Leung, Kai Wah Samuel Chu, and Maggie Shen\nQiao. 2021. “AI Literacy: Definition,\nTeaching, Evaluation and Ethical Issues.” Proceedings of the\nAssociation for Information Science and Technology 58 (1): 504–9.\n\n\nNgo, Richard, Lawrence Chan, and Sören Mindermann. 2022. “The\nAlignment Problem from a Deep Learning Perspective.” ArXiv\nPreprint abs/2209.00626. https://arxiv.org/abs/2209.00626.\n\n\nNguyen, Ngoc-Bao, Keshigeyan Chandrasegaran, Milad Abdollahzadeh, and\nNgai-Man Cheung. 2023. “Re-Thinking Model Inversion Attacks\nAgainst Deep Neural Networks.” In 2023 IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR), 16384–93. IEEE. https://doi.org/10.1109/cvpr52729.2023.01572.\n\n\nNorrie, Thomas, Nishant Patil, Doe Hyun Yoon, George Kurian, Sheng Li,\nJames Laudon, Cliff Young, Norman Jouppi, and David Patterson. 2021.\n“The Design Process for Google’s Training Chips:\nTpuv2 and TPUv3.” IEEE Micro\n41 (2): 56–63. https://doi.org/10.1109/mm.2021.3058217.\n\n\nNorthcutt, Curtis G, Anish Athalye, and Jonas Mueller. 2021.\n“Pervasive Label Errors in Test Sets Destabilize Machine Learning\nBenchmarks.” arXiv. https://doi.org/https://doi.org/10.48550/arXiv.2103.14749\narXiv-issued DOI via DataCite.\n\n\nObermeyer, Ziad, Brian Powers, Christine Vogeli, and Sendhil\nMullainathan. 2019. “Dissecting Racial Bias in an Algorithm Used\nto Manage the Health of Populations.” Science 366\n(6464): 447–53. https://doi.org/10.1126/science.aax2342.\n\n\nOecd. 2023. “A Blueprint for Building National Compute Capacity\nfor Artificial Intelligence.” 350. Organisation for Economic\nCo-Operation; Development (OECD). https://doi.org/10.1787/876367e3-en.\n\n\nOlah, Chris, Nick Cammarata, Ludwig Schubert, Gabriel Goh, Michael\nPetrov, and Shan Carter. 2020. “Zoom in: An\nIntroduction to Circuits.” Distill 5 (3): e00024–001. https://doi.org/10.23915/distill.00024.001.\n\n\nOliynyk, Daryna, Rudolf Mayer, and Andreas Rauber. 2023. “I Know\nWhat You Trained Last Summer: A Survey on Stealing Machine Learning\nModels and Defences.” ACM Computing Surveys 55 (14s):\n1–41. https://doi.org/10.1145/3595292.\n\n\nOoko, Samson Otieno, Marvin Muyonga Ogore, Jimmy Nsenga, and Marco\nZennaro. 2021. “TinyML in Africa:\nOpportunities and Challenges.” In 2021 IEEE\nGlobecom Workshops (GC Wkshps), 1–6. IEEE; IEEE. https://doi.org/10.1109/gcwkshps52748.2021.9682107.\n\n\nOprea, Alina, Anoop Singhal, and Apostol Vassilev. 2022.\n“Poisoning Attacks Against Machine Learning: Can\nMachine Learning Be Trustworthy?” Computer 55 (11):\n94–99. https://doi.org/10.1109/mc.2022.3190787.\n\n\nPan, Sinno Jialin, and Qiang Yang. 2010. “A Survey on Transfer\nLearning.” IEEE Transactions on Knowledge and Data\nEngineering 22 (10): 1345–59. https://doi.org/10.1109/tkde.2009.191.\n\n\nPanda, Priyadarshini, Indranil Chakraborty, and Kaushik Roy. 2019.\n“Discretization Based Solutions for Secure Machine Learning\nAgainst Adversarial Attacks.” #IEEE_O_ACC# 7: 70157–68.\nhttps://doi.org/10.1109/access.2019.2919463.\n\n\nPapadimitriou, George, and Dimitris Gizopoulos. 2021.\n“Demystifying the System Vulnerability Stack:\nTransient Fault Effects Across the Layers.” In\n2021 ACM/IEEE 48th Annual International Symposium on Computer\nArchitecture (ISCA), 902–15. IEEE; IEEE. https://doi.org/10.1109/isca52012.2021.00075.\n\n\nPapernot, Nicolas, Patrick McDaniel, Xi Wu, Somesh Jha, and Ananthram\nSwami. 2016. “Distillation as a Defense to Adversarial\nPerturbations Against Deep Neural Networks.” In 2016 IEEE\nSymposium on Security and Privacy (SP), 582–97. IEEE; IEEE. https://doi.org/10.1109/sp.2016.41.\n\n\nParrish, Alicia, Hannah Rose Kirk, Jessica Quaye, Charvi Rastogi, Max\nBartolo, Oana Inel, Juan Ciro, et al. 2023. “Adversarial Nibbler:\nA Data-Centric Challenge for Improving the Safety of\nText-to-Image Models.” ArXiv Preprint abs/2305.14384. https://arxiv.org/abs/2305.14384.\n\n\nPatterson, David A, and John L Hennessy. 2016. Computer Organization\nand Design ARM Edition: The Hardware Software\nInterface. Morgan kaufmann.\n\n\nPatterson, David, Joseph Gonzalez, Urs Holzle, Quoc Le, Chen Liang,\nLluis-Miquel Munguia, Daniel Rothchild, David R. So, Maud Texier, and\nJeff Dean. 2022. “The Carbon Footprint of Machine Learning\nTraining Will Plateau, Then Shrink.” Computer 55 (7):\n18–28. https://doi.org/10.1109/mc.2022.3148714.\n\n\nPeters, Dorian, Rafael A. Calvo, and Richard M. Ryan. 2018.\n“Designing for Motivation, Engagement and Wellbeing in Digital\nExperience.” Front. Psychol. 9 (May): 797. https://doi.org/10.3389/fpsyg.2018.00797.\n\n\nPhillips, P Jonathon, Carina A Hahn, Peter C Fontana, David A\nBroniatowski, and Mark A Przybocki. 2020. “Four Principles of\nExplainable Artificial Intelligence.” Gaithersburg,\nMaryland 18.\n\n\nPlank, James S. 1997. “A Tutorial on\nReedSolomon Coding for Fault-Tolerance in\nRAID-Like Systems.” Software: Practice and\nExperience 27 (9): 995–1012.\n\n\nPont, Michael J, and Royan HL Ong. 2002. “Using Watchdog Timers to\nImprove the Reliability of Single-Processor Embedded Systems:\nSeven New Patterns and a Case Study.” In\nProceedings of the First Nordic Conference on Pattern Languages of\nPrograms, 159–200. Citeseer.\n\n\nPrakash, Shvetank, Tim Callahan, Joseph Bushagour, Colby Banbury, Alan\nV. Green, Pete Warden, Tim Ansell, and Vijay Janapa Reddi. 2023.\n“CFU Playground: Full-stack Open-Source Framework for Tiny Machine\nLearning (TinyML) Acceleration on\nFPGAs.” In 2023 IEEE International Symposium on\nPerformance Analysis of Systems and Software (ISPASS). Vol.\nabs/2201.01863. IEEE. https://doi.org/10.1109/ispass57527.2023.00024.\n\n\nPrakash, Shvetank, Matthew Stewart, Colby Banbury, Mark Mazumder, Pete\nWarden, Brian Plancher, and Vijay Janapa Reddi. 2023. “Is\nTinyML Sustainable? Assessing the Environmental Impacts of\nMachine Learning on Microcontrollers.” ArXiv Preprint.\nhttps://arxiv.org/abs/2301.11899.\n\n\nPsoma, Sotiria D., and Chryso Kanthou. 2023. “Wearable Insulin\nBiosensors for Diabetes Management: Advances and Challenges.”\nBiosensors 13 (7): 719. https://doi.org/10.3390/bios13070719.\n\n\nPushkarna, Mahima, Andrew Zaldivar, and Oddur Kjartansson. 2022.\n“Data Cards: Purposeful and Transparent Dataset\nDocumentation for Responsible AI.” In 2022 ACM\nConference on Fairness, Accountability, and Transparency. ACM. https://doi.org/10.1145/3531146.3533231.\n\n\nPutnam, Andrew, Adrian M. Caulfield, Eric S. Chung, Derek Chiou, Kypros\nConstantinides, John Demme, Hadi Esmaeilzadeh, et al. 2014. “A\nReconfigurable Fabric for Accelerating Large-Scale Datacenter\nServices.” ACM SIGARCH Computer Architecture News 42\n(3): 13–24. https://doi.org/10.1145/2678373.2665678.\n\n\nQi, Chen, Shibo Shen, Rongpeng Li, Zhifeng Zhao, Qing Liu, Jing Liang,\nand Honggang Zhang. 2021. “An Efficient Pruning Scheme of Deep\nNeural Networks for Internet of Things Applications.” EURASIP\nJournal on Advances in Signal Processing 2021 (1): 31. https://doi.org/10.1186/s13634-021-00744-4.\n\n\nQian, Yu, Xuegong Zhou, Hao Zhou, and Lingli Wang. 2024. “An\nEfficient Reinforcement Learning Based Framework for Exploring Logic\nSynthesis.” ACM Trans. Des. Autom. Electron. Syst. 29\n(2): 1–33. https://doi.org/10.1145/3632174.\n\n\nR. V., Rashmi, and Karthikeyan A. 2018. “Secure Boot of Embedded\nApplications - a Review.” In 2018 Second International\nConference on Electronics, Communication and Aerospace Technology\n(ICECA), 291–98. IEEE. https://doi.org/10.1109/iceca.2018.8474730.\n\n\nRachwan, John, Daniel Zügner, Bertrand Charpentier, Simon Geisler,\nMorgane Ayle, and Stephan Günnemann. 2022. “Winning the Lottery\nAhead of Time: Efficient Early Network Pruning.” In\nInternational Conference on Machine Learning, 18293–309. PMLR.\n\n\nRaina, Rajat, Anand Madhavan, and Andrew Y. Ng. 2009. “Large-Scale\nDeep Unsupervised Learning Using Graphics Processors.” In\nProceedings of the 26th Annual International Conference on Machine\nLearning, edited by Andrea Pohoreckyj Danyluk, Léon Bottou, and\nMichael L. Littman, 382:873–80. ACM International Conference Proceeding\nSeries. ACM. https://doi.org/10.1145/1553374.1553486.\n\n\nRamaswamy, Vikram V., Sunnie S. Y. Kim, Ruth Fong, and Olga Russakovsky.\n2023a. “Overlooked Factors in Concept-Based Explanations:\nDataset Choice, Concept Learnability, and Human\nCapability.” In 2023 IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR), 10932–41. IEEE. https://doi.org/10.1109/cvpr52729.2023.01052.\n\n\nRamaswamy, Vikram V, Sunnie SY Kim, Ruth Fong, and Olga Russakovsky.\n2023b. “UFO: A Unified Method for\nControlling Understandability and Faithfulness Objectives in\nConcept-Based Explanations for CNNs.” ArXiv\nPreprint abs/2303.15632. https://arxiv.org/abs/2303.15632.\n\n\nRamcharan, Amanda, Kelsee Baranowski, Peter McCloskey, Babuali Ahmed,\nJames Legg, and David P. Hughes. 2017. “Deep Learning for\nImage-Based Cassava Disease Detection.” Front. Plant\nSci. 8 (October): 1852. https://doi.org/10.3389/fpls.2017.01852.\n\n\nRamesh, Aditya, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss,\nAlec Radford, Mark Chen, and Ilya Sutskever. 2021. “Zero-Shot\nText-to-Image Generation.” In Proceedings of the 38th\nInternational Conference on Machine Learning, ICML 2021, 18-24 July\n2021, Virtual Event, edited by Marina Meila and Tong Zhang,\n139:8821–31. Proceedings of Machine Learning Research. PMLR. http://proceedings.mlr.press/v139/ramesh21a.html.\n\n\nRanganathan, Parthasarathy. 2011. “From Microprocessors to\nNanostores: Rethinking Data-Centric Systems.”\nComputer 44 (1): 39–48. https://doi.org/10.1109/mc.2011.18.\n\n\nRao, Ravi. 2021. “TinyML Unlocks New Possibilities\nfor Sustainable Development Technologies.”\nWww.wevolver.com. https://www.wevolver.com/article/tinyml-unlocks-new-possibilities-for-sustainable-development-technologies.\n\n\nRashid, Layali, Karthik Pattabiraman, and Sathish Gopalakrishnan. 2012.\n“Intermittent Hardware Errors Recovery: Modeling and\nEvaluation.” In 2012 Ninth International Conference on\nQuantitative Evaluation of Systems, 220–29. IEEE; IEEE. https://doi.org/10.1109/qest.2012.37.\n\n\n———. 2015. “Characterizing the Impact of Intermittent Hardware\nFaults on Programs.” IEEE Trans. Reliab. 64 (1):\n297–310. https://doi.org/10.1109/tr.2014.2363152.\n\n\nRatner, Alex, Braden Hancock, Jared Dunnmon, Roger Goldman, and\nChristopher Ré. 2018. “Snorkel MeTaL: Weak\nSupervision for Multi-Task Learning.” In Proceedings of the\nSecond Workshop on Data Management for End-to-End Machine Learning.\nACM. https://doi.org/10.1145/3209889.3209898.\n\n\nReagen, Brandon, Udit Gupta, Lillian Pentecost, Paul Whatmough, Sae Kyu\nLee, Niamh Mulholland, David Brooks, and Gu-Yeon Wei. 2018. “Ares:\nA Framework for Quantifying the Resilience of Deep Neural\nNetworks.” In 2018 55th ACM/ESDA/IEEE Design Automation\nConference (DAC), 1–6. IEEE. https://doi.org/10.1109/dac.2018.8465834.\n\n\nReagen, Brandon, Jose Miguel Hernandez-Lobato, Robert Adolf, Michael\nGelbart, Paul Whatmough, Gu-Yeon Wei, and David Brooks. 2017. “A\nCase for Efficient Accelerator Design Space Exploration via\nBayesian Optimization.” In 2017 IEEE/ACM\nInternational Symposium on Low Power Electronics and Design\n(ISLPED), 1–6. IEEE; IEEE. https://doi.org/10.1109/islped.2017.8009208.\n\n\nReddi, Sashank J., Satyen Kale, and Sanjiv Kumar. 2019. “On the\nConvergence of Adam and Beyond.” arXiv Preprint\narXiv:1904.09237, April. http://arxiv.org/abs/1904.09237v1.\n\n\nReddi, Vijay Janapa, Christine Cheng, David Kanter, Peter Mattson,\nGuenther Schmuelling, Carole-Jean Wu, Brian Anderson, et al. 2020.\n“MLPerf Inference Benchmark.” In 2020\nACM/IEEE 47th Annual International Symposium on Computer Architecture\n(ISCA), 446–59. IEEE; IEEE. https://doi.org/10.1109/isca45697.2020.00045.\n\n\nReddi, Vijay Janapa, and Meeta Sharma Gupta. 2013. Resilient\nArchitecture Design for Voltage Variation. Springer International\nPublishing. https://doi.org/10.1007/978-3-031-01739-1.\n\n\nReis, G. A., J. Chang, N. Vachharajani, R. Rangan, and D. I. August.\n2005. “SWIFT: Software Implemented Fault\nTolerance.” In International Symposium on Code Generation and\nOptimization, 243–54. IEEE; IEEE. https://doi.org/10.1109/cgo.2005.34.\n\n\nRibeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. 2016.\n“” Why Should i Trust You?” Explaining\nthe Predictions of Any Classifier.” In Proceedings of the\n22nd ACM SIGKDD International Conference on Knowledge Discovery and Data\nMining, 1135–44.\n\n\nRobbins, Herbert, and Sutton Monro. 1951. “A Stochastic\nApproximation Method.” The Annals of Mathematical\nStatistics 22 (3): 400–407. https://doi.org/10.1214/aoms/1177729586.\n\n\nRombach, Robin, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and\nBjorn Ommer. 2022. “High-Resolution Image Synthesis with Latent\nDiffusion Models.” In 2022 IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR). IEEE. https://doi.org/10.1109/cvpr52688.2022.01042.\n\n\nRomero, Francisco, Qian Li 0027, Neeraja J. Yadwadkar, and Christos\nKozyrakis. 2021. “INFaaS: Automated Model-Less Inference\nServing.” In 2021 USENIX Annual Technical Conference (USENIX\nATC 21), 397–411. https://www.usenix.org/conference/atc21/presentation/romero.\n\n\nRosenblatt, Frank. 1957. The Perceptron, a Perceiving and\nRecognizing Automaton Project Para. Cornell Aeronautical\nLaboratory.\n\n\nRoskies, Adina. 2002. “Neuroethics for the New Millenium.”\nNeuron 35 (1): 21–23. https://doi.org/10.1016/s0896-6273(02)00763-8.\n\n\nRuder, Sebastian. 2016. “An Overview of Gradient Descent\nOptimization Algorithms.” ArXiv Preprint abs/1609.04747\n(September). http://arxiv.org/abs/1609.04747v2.\n\n\nRudin, Cynthia. 2019. “Stop Explaining Black Box Machine Learning\nModels for High Stakes Decisions and Use Interpretable Models\nInstead.” Nature Machine Intelligence 1 (5): 206–15. https://doi.org/10.1038/s42256-019-0048-x.\n\n\nRumelhart, David E., Geoffrey E. Hinton, and Ronald J. Williams. 1986.\n“Learning Representations by Back-Propagating Errors.”\nNature 323 (6088): 533–36. https://doi.org/10.1038/323533a0.\n\n\nRussakovsky, Olga, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh,\nSean Ma, Zhiheng Huang, et al. 2015. “ImageNet Large\nScale Visual Recognition Challenge.” Int. J. Comput.\nVision 115 (3): 211–52. https://doi.org/10.1007/s11263-015-0816-y.\n\n\nRussell, Stuart. 2021. “Human-Compatible Artificial\nIntelligence.” Human-Like Machine Intelligence, 3–23.\n\n\nRyan, Richard M., and Edward L. Deci. 2000. “Self-Determination\nTheory and the Facilitation of Intrinsic Motivation, Social Development,\nand Well-Being.” Am. Psychol. 55 (1): 68–78. https://doi.org/10.1037/0003-066x.55.1.68.\n\n\nSamajdar, Ananda, Yuhao Zhu, Paul Whatmough, Matthew Mattina, and Tushar\nKrishna. 2018. “Scale-Sim: Systolic Cnn Accelerator\nSimulator.” ArXiv Preprint abs/1811.02883. https://arxiv.org/abs/1811.02883.\n\n\nSambasivan, Nithya, Shivani Kapania, Hannah Highfill, Diana Akrong,\nPraveen Paritosh, and Lora M Aroyo. 2021a.\n““Everyone Wants to Do the Model Work,\nNot the Data Work”: Data Cascades in\nHigh-Stakes AI.” In Proceedings of the 2021 CHI\nConference on Human Factors in Computing Systems, 1–15.\n\n\n———. 2021b. “‘Everyone Wants to Do the Model Work, Not the\nData Work’: Data Cascades in High-Stakes AI.” In\nProceedings of the 2021 CHI Conference on Human Factors in Computing\nSystems. ACM. https://doi.org/10.1145/3411764.3445518.\n\n\nSangchoolie, Behrooz, Karthik Pattabiraman, and Johan Karlsson. 2017.\n“One Bit Is (Not) Enough: An Empirical\nStudy of the Impact of Single and Multiple Bit-Flip Errors.” In\n2017 47th Annual IEEE/IFIP International Conference on Dependable\nSystems and Networks (DSN), 97–108. IEEE; IEEE. https://doi.org/10.1109/dsn.2017.30.\n\n\nSchäfer, Mike S. 2023. “The Notorious GPT:\nScience Communication in the Age of Artificial\nIntelligence.” Journal of Science Communication 22 (02):\nY02. https://doi.org/10.22323/2.22020402.\n\n\nSchizas, Nikolaos, Aristeidis Karras, Christos Karras, and Spyros\nSioutas. 2022. “TinyML for Ultra-Low Power\nAI and Large Scale IoT Deployments:\nA Systematic Review.” Future Internet 14\n(12): 363. https://doi.org/10.3390/fi14120363.\n\n\nSchuman, Catherine D., Shruti R. Kulkarni, Maryam Parsa, J. Parker\nMitchell, Prasanna Date, and Bill Kay. 2022. “Opportunities for\nNeuromorphic Computing Algorithms and Applications.” Nature\nComputational Science 2 (1): 10–19. https://doi.org/10.1038/s43588-021-00184-y.\n\n\nSchwartz, Daniel, Jonathan Michael Gomes Selman, Peter Wrege, and\nAndreas Paepcke. 2021. “Deployment of Embedded\nEdge-AI for Wildlife Monitoring in Remote Regions.”\nIn 2021 20th IEEE International Conference on Machine Learning and\nApplications (ICMLA), 1035–42. IEEE; IEEE. https://doi.org/10.1109/icmla52953.2021.00170.\n\n\nSchwartz, Roy, Jesse Dodge, Noah A. Smith, and Oren Etzioni. 2020.\n“Green AI.” Commun. ACM 63 (12):\n54–63. https://doi.org/10.1145/3381831.\n\n\nSegal, Mark, and Kurt Akeley. 1999. “The OpenGL\nGraphics System: A Specification (Version 1.1).”\n\n\nSegura Anaya, L. H., Abeer Alsadoon, N. Costadopoulos, and P. W. C.\nPrasad. 2017. “Ethical Implications of User Perceptions of\nWearable Devices.” Sci. Eng. Ethics 24 (1): 1–28. https://doi.org/10.1007/s11948-017-9872-8.\n\n\nSeide, Frank, and Amit Agarwal. 2016. “Cntk: Microsoft’s\nOpen-Source Deep-Learning Toolkit.” In Proceedings of the\n22nd ACM SIGKDD International Conference on Knowledge Discovery and Data\nMining, 2135–35. ACM. https://doi.org/10.1145/2939672.2945397.\n\n\nSelvaraju, Ramprasaath R., Michael Cogswell, Abhishek Das, Ramakrishna\nVedantam, Devi Parikh, and Dhruv Batra. 2017.\n“Grad-CAM: Visual Explanations from Deep\nNetworks via Gradient-Based Localization.” In 2017 IEEE\nInternational Conference on Computer Vision (ICCV), 618–26. IEEE.\nhttps://doi.org/10.1109/iccv.2017.74.\n\n\nSeong, Nak Hee, Dong Hyuk Woo, Vijayalakshmi Srinivasan, Jude A. Rivers,\nand Hsien-Hsin S. Lee. 2010. “SAFER: Stuck-at-fault Error Recovery for\nMemories.” In 2010 43rd Annual IEEE/ACM International\nSymposium on Microarchitecture, 115–24. IEEE; IEEE. https://doi.org/10.1109/micro.2010.46.\n\n\nSeyedzadeh, Saleh, Farzad Pour Rahimian, Ivan Glesk, and Marc Roper.\n2018. “Machine Learning for Estimation of Building Energy\nConsumption and Performance: A Review.”\nVisualization in Engineering 6 (1): 1–20. https://doi.org/10.1186/s40327-018-0064-7.\n\n\nShalev-Shwartz, Shai, Shaked Shammah, and Amnon Shashua. 2017. “On\na Formal Model of Safe and Scalable Self-Driving Cars.” ArXiv\nPreprint abs/1708.06374. https://arxiv.org/abs/1708.06374.\n\n\nShan, Shawn, Wenxin Ding, Josephine Passananti, Haitao Zheng, and Ben Y\nZhao. 2023. “Prompt-Specific Poisoning Attacks on Text-to-Image\nGenerative Models.” ArXiv Preprint abs/2310.13828. https://arxiv.org/abs/2310.13828.\n\n\nShastri, Bhavin J., Alexander N. Tait, T. Ferreira de Lima, Wolfram H.\nP. Pernice, Harish Bhaskaran, C. D. Wright, and Paul R. Prucnal. 2021.\n“Photonics for Artificial Intelligence and Neuromorphic\nComputing.” Nat. Photonics 15 (2): 102–14. https://doi.org/10.1038/s41566-020-00754-y.\n\n\nSheaffer, Jeremy W, David P Luebke, and Kevin Skadron. 2007. “A\nHardware Redundancy and Recovery Mechanism for Reliable Scientific\nComputation on Graphics Processors.” In Graphics\nHardware, 2007:55–64. Citeseer.\n\n\nShehabi, Arman, Sarah Smith, Dale Sartor, Richard Brown, Magnus Herrlin,\nJonathan Koomey, Eric Masanet, Nathaniel Horner, Inês Azevedo, and\nWilliam Lintner. 2016. “United States Data Center Energy Usage\nReport.”\n\n\nShen, Sheng, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami,\nMichael W. Mahoney, and Kurt Keutzer. 2020. “Q-BERT:\nHessian Based Ultra Low Precision Quantization of\nBERT.” Proceedings of the AAAI Conference on\nArtificial Intelligence 34 (05): 8815–21. https://doi.org/10.1609/aaai.v34i05.6409.\n\n\nSheng, Victor S., and Jing Zhang. 2019. “Machine Learning with\nCrowdsourcing: A Brief Summary of the Past Research and\nFuture Directions.” Proceedings of the AAAI Conference on\nArtificial Intelligence 33 (01): 9837–43. https://doi.org/10.1609/aaai.v33i01.33019837.\n\n\nShi, Hongrui, and Valentin Radu. 2022. “Data Selection for\nEfficient Model Update in Federated Learning.” In Proceedings\nof the 2nd European Workshop on Machine Learning and Systems,\n72–78. ACM. https://doi.org/10.1145/3517207.3526980.\n\n\nShneiderman, Ben. 2020. “Bridging the Gap Between Ethics and\nPractice: Guidelines for Reliable, Safe, and Trustworthy Human-Centered\nAI Systems.” ACM Trans. Interact. Intell. Syst. 10 (4):\n1–31. https://doi.org/10.1145/3419764.\n\n\n———. 2022. Human-Centered AI. Oxford University\nPress.\n\n\nShokri, Reza, Marco Stronati, Congzheng Song, and Vitaly Shmatikov.\n2017. “Membership Inference Attacks Against Machine Learning\nModels.” In 2017 IEEE Symposium on Security and Privacy\n(SP), 3–18. IEEE; IEEE. https://doi.org/10.1109/sp.2017.41.\n\n\nSiddik, Md Abu Bakar, Arman Shehabi, and Landon Marston. 2021.\n“The Environmental Footprint of Data Centers in the United\nStates.” Environ. Res. Lett. 16 (6): 064017. https://doi.org/10.1088/1748-9326/abfba1.\n\n\nSilvestro, Daniele, Stefano Goria, Thomas Sterner, and Alexandre\nAntonelli. 2022. “Improving Biodiversity Protection Through\nArtificial Intelligence.” Nature Sustainability 5 (5):\n415–24. https://doi.org/10.1038/s41893-022-00851-6.\n\n\nSingh, Narendra, and Oladele A. Ogunseitan. 2022. “Disentangling\nthe Worldwide Web of e-Waste and Climate Change Co-Benefits.”\nCircular Economy 1 (2): 100011. https://doi.org/10.1016/j.cec.2022.100011.\n\n\nSkorobogatov, Sergei. 2009. “Local Heating Attacks on Flash Memory\nDevices.” In 2009 IEEE International Workshop on\nHardware-Oriented Security and Trust, 1–6. IEEE; IEEE. https://doi.org/10.1109/hst.2009.5225028.\n\n\nSkorobogatov, Sergei P., and Ross J. Anderson. 2002. “Optical\nFault Induction Attacks.” In Cryptographic Hardware and\nEmbedded Systems-CHES 2002: 4th International Workshop Redwood Shores,\nCA, USA, August 13–15, 2002 Revised Papers 4, 2–12. Springer. https://doi.org/10.1007/3-540-36400-5\\_2.\n\n\nSmilkov, Daniel, Nikhil Thorat, Been Kim, Fernanda Viégas, and Martin\nWattenberg. 2017. “Smoothgrad: Removing Noise by\nAdding Noise.” ArXiv Preprint abs/1706.03825. https://arxiv.org/abs/1706.03825.\n\n\nSnoek, Jasper, Hugo Larochelle, and Ryan P. Adams. 2012.\n“Practical Bayesian Optimization of Machine Learning\nAlgorithms.” In Advances in Neural Information Processing\nSystems 25: 26th Annual Conference on Neural Information Processing\nSystems 2012. Proceedings of a Meeting Held December 3-6, 2012, Lake\nTahoe, Nevada, United States, edited by Peter L. Bartlett, Fernando\nC. N. Pereira, Christopher J. C. Burges, Léon Bottou, and Kilian Q.\nWeinberger, 2960–68. https://proceedings.neurips.cc/paper/2012/hash/05311655a15b75fab86956663e1819cd-Abstract.html.\n\n\nSrivastava, Nitish, Geoffrey E. Hinton, Alex Krizhevsky, Ilya Sutskever,\nand Ruslan Salakhutdinov. 2014. “Dropout: A Simple Way to Prevent\nNeural Networks from Overfitting.” J. Mach. Learn. Res.\n15 (1): 1929–58. https://doi.org/10.5555/2627435.2670313.\n\n\nStrubell, Emma, Ananya Ganesh, and Andrew McCallum. 2019. “Energy\nand Policy Considerations for Deep Learning in NLP.”\nIn Proceedings of the 57th Annual Meeting of the Association for\nComputational Linguistics, 3645–50. Florence, Italy: Association\nfor Computational Linguistics. https://doi.org/10.18653/v1/p19-1355.\n\n\nSuda, Naveen, Vikas Chandra, Ganesh Dasika, Abinash Mohanty, Yufei Ma,\nSarma Vrudhula, Jae-sun Seo, and Yu Cao. 2016.\n“Throughput-Optimized OpenCL-Based FPGA\nAccelerator for Large-Scale Convolutional Neural Networks.” In\nProceedings of the 2016 ACM/SIGDA International Symposium on\nField-Programmable Gate Arrays, 16–25. ACM. https://doi.org/10.1145/2847263.2847276.\n\n\nSudhakar, Soumya, Vivienne Sze, and Sertac Karaman. 2023. “Data\nCenters on Wheels: Emissions from Computing Onboard\nAutonomous Vehicles.” IEEE Micro 43 (1): 29–39. https://doi.org/10.1109/mm.2022.3219803.\n\n\nSze, Vivienne, Yu-Hsin Chen, Tien-Ju Yang, and Joel S. Emer. 2017.\n“Efficient Processing of Deep Neural Networks: A\nTutorial and Survey.” Proc. IEEE 105 (12): 2295–2329. https://doi.org/10.1109/jproc.2017.2761740.\n\n\nSzegedy, Christian, Wojciech Zaremba, Ilya Sutskever, Joan Bruna,\nDumitru Erhan, Ian J. Goodfellow, and Rob Fergus. 2014.\n“Intriguing Properties of Neural Networks.” In 2nd\nInternational Conference on Learning Representations, ICLR 2014, Banff,\nAB, Canada, April 14-16, 2014, Conference Track Proceedings, edited\nby Yoshua Bengio and Yann LeCun. http://arxiv.org/abs/1312.6199.\n\n\nTambe, Thierry, En-Yu Yang, Zishen Wan, Yuntian Deng, Vijay Janapa\nReddi, Alexander Rush, David Brooks, and Gu-Yeon Wei. 2020.\n“Algorithm-Hardware Co-Design of Adaptive Floating-Point Encodings\nfor Resilient Deep Learning Inference.” In 2020 57th ACM/IEEE\nDesign Automation Conference (DAC), 1–6. IEEE; IEEE. https://doi.org/10.1109/dac18072.2020.9218516.\n\n\nTan, Mingxing, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler,\nAndrew Howard, and Quoc V. Le. 2019. “MnasNet: Platform-aware Neural Architecture Search for\nMobile.” In 2019 IEEE/CVF Conference on Computer Vision and\nPattern Recognition (CVPR), 2820–28. IEEE. https://doi.org/10.1109/cvpr.2019.00293.\n\n\nTan, Mingxing, and Quoc V. Le. 2023. “Demystifying Deep\nLearning.” Wiley. https://doi.org/10.1002/9781394205639.ch6.\n\n\nTang, Xin, Yichun He, and Jia Liu. 2022. “Soft Bioelectronics for\nCardiac Interfaces.” Biophysics Reviews 3 (1). https://doi.org/10.1063/5.0069516.\n\n\nTang, Xin, Hao Shen, Siyuan Zhao, Na Li, and Jia Liu. 2023.\n“Flexible Braincomputer Interfaces.”\nNature Electronics 6 (2): 109–18. https://doi.org/10.1038/s41928-022-00913-9.\n\n\nTarun, Ayush K, Vikram S Chundawat, Murari Mandal, and Mohan\nKankanhalli. 2022. “Deep Regression Unlearning.” ArXiv\nPreprint abs/2210.08196 (October). http://arxiv.org/abs/2210.08196v2.\n\n\nTeam, The Theano Development, Rami Al-Rfou, Guillaume Alain, Amjad\nAlmahairi, Christof Angermueller, Dzmitry Bahdanau, Nicolas Ballas, et\nal. 2016. “Theano: A Python Framework for Fast\nComputation of Mathematical Expressions.” https://arxiv.org/abs/1605.02688.\n\n\n“The Ultimate Guide to Deep Learning Model Quantization and\nQuantization-Aware Training.” n.d. https://deci.ai/quantization-and-quantization-aware-training/.\n\n\nThompson, Neil C., Kristjan Greenewald, Keeheon Lee, and Gabriel F.\nManso. 2021. “Deep Learning’s Diminishing Returns:\nThe Cost of Improvement Is Becoming Unsustainable.”\nIEEE Spectr. 58 (10): 50–55. https://doi.org/10.1109/mspec.2021.9563954.\n\n\nTill, Aaron, Andrew L. Rypel, Andrew Bray, and Samuel B. Fey. 2019.\n“Fish Die-Offs Are Concurrent with Thermal Extremes in North\nTemperate Lakes.” Nat. Clim. Change 9 (8): 637–41. https://doi.org/10.1038/s41558-019-0520-y.\n\n\nTirtalistyani, Rose, Murtiningrum Murtiningrum, and Rameshwar S. Kanwar.\n2022. “Indonesia Rice Irrigation System:\nTime for Innovation.” Sustainability 14\n(19): 12477. https://doi.org/10.3390/su141912477.\n\n\nTokui, Seiya, Ryosuke Okuta, Takuya Akiba, Yusuke Niitani, Toru Ogawa,\nShunta Saito, Shuji Suzuki, Kota Uenishi, Brian Vogel, and Hiroyuki\nYamazaki Vincent. 2019. “Chainer: A Deep Learning Framework for\nAccelerating the Research Cycle.” In Proceedings of the 25th\nACM SIGKDD International Conference on Knowledge Discovery &Amp;\nData Mining, 5:1–6. ACM. https://doi.org/10.1145/3292500.3330756.\n\n\nTramèr, Florian, Pascal Dupré, Gili Rusak, Giancarlo Pellegrino, and Dan\nBoneh. 2019. “AdVersarial: Perceptual Ad Blocking\nMeets Adversarial Machine Learning.” In Proceedings of the\n2019 ACM SIGSAC Conference on Computer and Communications Security,\n2005–21. ACM. https://doi.org/10.1145/3319535.3354222.\n\n\nTran, Cuong, Ferdinando Fioretto, Jung-Eun Kim, and Rakshit Naidu. 2022.\n“Pruning Has a Disparate Impact on Model Accuracy.” Adv\nNeural Inf Process Syst 35: 17652–64.\n\n\nTsai, Min-Jen, Ping-Yi Lin, and Ming-En Lee. 2023. “Adversarial\nAttacks on Medical Image Classification.” Cancers 15\n(17): 4228. https://doi.org/10.3390/cancers15174228.\n\n\nTsai, Timothy, Siva Kumar Sastry Hari, Michael Sullivan, Oreste Villa,\nand Stephen W. Keckler. 2021. “NVBitFI:\nDynamic Fault Injection for GPUs.” In\n2021 51st Annual IEEE/IFIP International Conference on Dependable\nSystems and Networks (DSN), 284–91. IEEE; IEEE. https://doi.org/10.1109/dsn48987.2021.00041.\n\n\nTschand, Arya, Arun Tejusve Raghunath Rajan, Sachin Idgunji, Anirban\nGhosh, Jeremy Holleman, Csaba Kiraly, Pawan Ambalkar, et al. 2024.\n“MLPerf Power: Benchmarking the Energy Efficiency of Machine\nLearning Systems from Microwatts to Megawatts for Sustainable\nAI.” arXiv Preprint arXiv:2410.12032, October. http://arxiv.org/abs/2410.12032v1.\n\n\nUddin, Mueen, and Azizah Abdul Rahman. 2012. “Energy Efficiency\nand Low Carbon Enabler Green IT Framework for Data Centers\nConsidering Green Metrics.” Renewable Sustainable Energy\nRev. 16 (6): 4078–94. https://doi.org/10.1016/j.rser.2012.03.014.\n\n\nUn, and World Economic Forum. 2019. A New Circular Vision for\nElectronics, Time for a Global Reboot. PACE - Platform for\nAccelerating the Circular Economy. https://www3.weforum.org/docs/WEF\\_A\\_New\\_Circular\\_Vision\\_for\\_Electronics.pdf.\n\n\nValenzuela, Christine L, and Pearl Y Wang. 2000. “A Genetic\nAlgorithm for VLSI Floorplanning.” In Parallel\nProblem Solving from Nature PPSN VI: 6th International Conference Paris,\nFrance, September 1820, 2000 Proceedings 6, 671–80.\nSpringer.\n\n\nVan Noorden, Richard. 2016. “ArXiv Preprint Server\nPlans Multimillion-Dollar Overhaul.” Nature 534 (7609):\n602–2. https://doi.org/10.1038/534602a.\n\n\nVangal, Sriram, Somnath Paul, Steven Hsu, Amit Agarwal, Saurabh Kumar,\nRam Krishnamurthy, Harish Krishnamurthy, James Tschanz, Vivek De, and\nChris H. Kim. 2021. “Wide-Range Many-Core SoC Design\nin Scaled CMOS: Challenges and\nOpportunities.” IEEE Trans. Very Large Scale Integr. VLSI\nSyst. 29 (5): 843–56. https://doi.org/10.1109/tvlsi.2021.3061649.\n\n\nVaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion\nJones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017.\n“Attention Is All You Need.” Adv Neural Inf Process\nSyst 30.\n\n\n“Vector-Borne Diseases.” n.d.\nhttps://www.who.int/news-room/fact-sheets/detail/vector-borne-diseases.\n\n\nVelazco, Raoul, Gilles Foucard, and Paul Peronnard. 2010.\n“Combining Results of Accelerated Radiation Tests and Fault\nInjections to Predict the Error Rate of an Application Implemented in\nSRAM-Based FPGAs.” IEEE Trans.\nNucl. Sci. 57 (6): 3500–3505. https://doi.org/10.1109/tns.2010.2087355.\n\n\nVerma, Naveen, Hongyang Jia, Hossein Valavi, Yinqi Tang, Murat Ozatay,\nLung-Yen Chen, Bonan Zhang, and Peter Deaville. 2019. “In-Memory\nComputing: Advances and Prospects.” IEEE\nSolid-State Circuits Mag. 11 (3): 43–55. https://doi.org/10.1109/mssc.2019.2922889.\n\n\nVerma, Team Dual_Boot: Swapnil. 2022. “Elephant\nAI.” Hackster.io. https://www.hackster.io/dual\\_boot/elephant-ai-ba71e9.\n\n\nVinuesa, Ricardo, Hossein Azizpour, Iolanda Leite, Madeline Balaam,\nVirginia Dignum, Sami Domisch, Anna Felländer, Simone Daniela Langhans,\nMax Tegmark, and Francesco Fuso Nerini. 2020. “The Role of\nArtificial Intelligence in Achieving the Sustainable Development\nGoals.” Nat. Commun. 11 (1): 1–10. https://doi.org/10.1038/s41467-019-14108-y.\n\n\nVivet, Pascal, Eric Guthmuller, Yvain Thonnart, Gael Pillonnet, Cesar\nFuguet, Ivan Miro-Panades, Guillaume Moritz, et al. 2021.\n“IntAct: A 96-Core Processor with Six\nChiplets 3D-Stacked on an Active Interposer with\nDistributed Interconnects and Integrated Power Management.”\nIEEE J. Solid-State Circuits 56 (1): 79–97. https://doi.org/10.1109/jssc.2020.3036341.\n\n\nWachter, Sandra, Brent Mittelstadt, and Chris Russell. 2017.\n“Counterfactual Explanations Without Opening the Black Box:\nAutomated Decisions and the GDPR.”\nSSRN Electronic Journal 31: 841. https://doi.org/10.2139/ssrn.3063289.\n\n\nWald, Peter H., and Jeffrey R. Jones. 1987. “Semiconductor\nManufacturing: An Introduction to Processes and\nHazards.” Am. J. Ind. Med. 11 (2): 203–21. https://doi.org/10.1002/ajim.4700110209.\n\n\nWan, Zishen, Aqeel Anwar, Yu-Shun Hsiao, Tianyu Jia, Vijay Janapa Reddi,\nand Arijit Raychowdhury. 2021. “Analyzing and Improving Fault\nTolerance of Learning-Based Navigation Systems.” In 2021 58th\nACM/IEEE Design Automation Conference (DAC), 841–46. IEEE; IEEE. https://doi.org/10.1109/dac18074.2021.9586116.\n\n\nWan, Zishen, Yiming Gan, Bo Yu, S Liu, A Raychowdhury, and Y Zhu. 2023.\n“Vpp: The Vulnerability-Proportional Protection\nParadigm Towards Reliable Autonomous Machines.” In\nProceedings of the 5th International Workshop on Domain Specific\nSystem Architecture (DOSSA), 1–6.\n\n\nWang, LingFeng, and YaQing Zhan. 2019a. “A Conceptual Peer Review\nModel for arXiv and Other Preprint\nDatabases.” Learn. Publ. 32 (3): 213–19. https://doi.org/10.1002/leap.1229.\n\n\n———. 2019b. “A Conceptual Peer Review Model for arXiv and Other Preprint Databases.”\nLearn. Publ. 32 (3): 213–19. https://doi.org/10.1002/leap.1229.\n\n\nWang, Tianzhe, Kuan Wang, Han Cai, Ji Lin, Zhijian Liu, Hanrui Wang,\nYujun Lin, and Song Han. 2020. “APQ:\nJoint Search for Network Architecture, Pruning and\nQuantization Policy.” In 2020 IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), 2075–84. IEEE. https://doi.org/10.1109/cvpr42600.2020.00215.\n\n\nWarden, Pete. 2018. “Speech Commands: A Dataset for\nLimited-Vocabulary Speech Recognition.” arXiv Preprint\narXiv:1804.03209.\n\n\nWarden, Pete, and Daniel Situnayake. 2019. Tinyml:\nMachine Learning with Tensorflow Lite on Arduino and\nUltra-Low-Power Microcontrollers. O’Reilly Media.\n\n\nWeik, Martin H. 1955. A Survey of Domestic Electronic Digital\nComputing Systems. Ballistic Research Laboratories.\n\n\nWess, Matthias, Matvey Ivanov, Christoph Unger, and Anvesh Nookala.\n2020. “ANNETTE: Accurate Neural Network\nExecution Time Estimation with Stacked Models.” IEEE. https://doi.org/10.1109/ACCESS.2020.3047259.\n\n\nWiener, Norbert. 1960. “Some Moral and Technical Consequences of\nAutomation: As Machines Learn They May Develop Unforeseen Strategies at\nRates That Baffle Their Programmers.” Science 131\n(3410): 1355–58. https://doi.org/10.1126/science.131.3410.1355.\n\n\nWilkening, Mark, Vilas Sridharan, Si Li, Fritz Previlon, Sudhanva\nGurumurthi, and David R. Kaeli. 2014. “Calculating Architectural\nVulnerability Factors for Spatial Multi-Bit Transient Faults.” In\n2014 47th Annual IEEE/ACM International Symposium on\nMicroarchitecture, 293–305. IEEE; IEEE. https://doi.org/10.1109/micro.2014.15.\n\n\nWinkler, Harald, Franck Lecocq, Hans Lofgren, Maria Virginia Vilariño,\nSivan Kartha, and Joana Portugal-Pereira. 2022. “Examples of\nShifting Development Pathways: Lessons on How to Enable\nBroader, Deeper, and Faster Climate Action.” Climate\nAction 1 (1). https://doi.org/10.1007/s44168-022-00026-1.\n\n\nWong, H.-S. Philip, Heng-Yuan Lee, Shimeng Yu, Yu-Sheng Chen, Yi Wu,\nPang-Shiu Chen, Byoungil Lee, Frederick T. Chen, and Ming-Jinn Tsai.\n2012. “MetalOxide\nRRAM.” Proc. IEEE 100 (6): 1951–70. https://doi.org/10.1109/jproc.2012.2190369.\n\n\nWu, Bichen, Kurt Keutzer, Xiaoliang Dai, Peizhao Zhang, Yanghan Wang,\nFei Sun, Yiming Wu, Yuandong Tian, Peter Vajda, and Yangqing Jia. 2019.\n“FBNet: Hardware-aware\nEfficient ConvNet Design via Differentiable Neural\nArchitecture Search.” In 2019 IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), 10734–42. IEEE. https://doi.org/10.1109/cvpr.2019.01099.\n\n\nWu, Carole-Jean, David Brooks, Kevin Chen, Douglas Chen, Sy Choudhury,\nMarat Dukhan, Kim Hazelwood, et al. 2019. “Machine Learning at\nFacebook: Understanding Inference at the Edge.” In 2019 IEEE\nInternational Symposium on High Performance Computer Architecture\n(HPCA), 331–44. IEEE; IEEE. https://doi.org/10.1109/hpca.2019.00048.\n\n\nWu, Carole-Jean, Ramya Raghavendra, Udit Gupta, Bilge Acun, Newsha\nArdalani, Kiwan Maeng, Gloria Chang, et al. 2022. “Sustainable Ai:\nEnvironmental Implications, Challenges and\nOpportunities.” Proceedings of Machine Learning and\nSystems 4: 795–813.\n\n\nWu, Zhang Judd, and Micikevicius Isaev. 2020. “Integer\nQuantization for Deep Learning Inference: Principles and\nEmpirical Evaluation).” ArXiv Preprint. https://arxiv.org/abs/2004.09602.\n\n\nXiao, Seznec Lin, Demouth Wu, and Han. 2022.\n“SmoothQuant: Accurate and Efficient\nPost-Training Quantization for Large Language Models.” ArXiv\nPreprint. https://arxiv.org/abs/2211.10438.\n\n\nXie, Cihang, Mingxing Tan, Boqing Gong, Jiang Wang, Alan L. Yuille, and\nQuoc V. Le. 2020. “Adversarial Examples Improve Image\nRecognition.” In 2020 IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR), 816–25. IEEE. https://doi.org/10.1109/cvpr42600.2020.00090.\n\n\nXie, Saining, Ross Girshick, Piotr Dollar, Zhuowen Tu, and Kaiming He.\n2017. “Aggregated Residual Transformations for Deep Neural\nNetworks.” In 2017 IEEE Conference on Computer Vision and\nPattern Recognition (CVPR), 1492–1500. IEEE. https://doi.org/10.1109/cvpr.2017.634.\n\n\nXinyu, Chen. n.d.\n\n\nXiong, Siyu, Guoqing Wu, Xitian Fan, Xuan Feng, Zhongcheng Huang, Wei\nCao, Xuegong Zhou, et al. 2021. “MRI-Based Brain\nTumor Segmentation Using FPGA-Accelerated Neural\nNetwork.” BMC Bioinf. 22 (1): 421. https://doi.org/10.1186/s12859-021-04347-6.\n\n\nXiu, Liming. 2019. “Time Moore: Exploiting Moore’s Law from the Perspective of Time.”\nIEEE Solid-State Circuits Mag. 11 (1): 39–55. https://doi.org/10.1109/mssc.2018.2882285.\n\n\nXu, Chen, Jianqiang Yao, Zhouchen Lin, Wenwu Ou, Yuanbin Cao, Zhirong\nWang, and Hongbin Zha. 2018. “Alternating Multi-Bit Quantization\nfor Recurrent Neural Networks.” In 6th International\nConference on Learning Representations, ICLR 2018, Vancouver, BC,\nCanada, April 30 - May 3, 2018, Conference Track Proceedings.\nOpenReview.net. https://openreview.net/forum?id=S19dR9x0b.\n\n\nXu, Hu, Saining Xie, Xiaoqing Ellen Tan, Po-Yao Huang, Russell Howes,\nVasu Sharma, Shang-Wen Li, Gargi Ghosh, Luke Zettlemoyer, and Christoph\nFeichtenhofer. 2023. “Demystifying CLIP Data.” ArXiv\nPreprint abs/2309.16671 (September). http://arxiv.org/abs/2309.16671v4.\n\n\nXu, Ying, Xu Zhong, Antonio Jimeno Yepes, and Jey Han Lau. 2021.\n“Grey-Box Adversarial Attack and Defence for\nSentiment Classification.” arXiv Preprint\narXiv:2103.11576.\n\n\nXu, Zheng, Yanxiang Zhang, Galen Andrew, Christopher A. Choquette-Choo,\nPeter Kairouz, H. Brendan McMahan, Jesse Rosenstock, and Yuanbo Zhang.\n2023. “Federated Learning of Gboard Language Models with\nDifferential Privacy.” ArXiv Preprint abs/2305.18465\n(May). http://arxiv.org/abs/2305.18465v2.\n\n\nYang, Tien-Ju, Yonghui Xiao, Giovanni Motta, Françoise Beaufays, Rajiv\nMathews, and Mingqing Chen. 2023. “Online Model Compression for\nFederated Learning with Large Models.” In ICASSP 2023 - 2023\nIEEE International Conference on Acoustics, Speech and Signal Processing\n(ICASSP), 1–5. IEEE; IEEE. https://doi.org/10.1109/icassp49357.2023.10097124.\n\n\nYao, Zhewei, Zhen Dong, Zhangcheng Zheng, Amir Gholami, Jiali Yu, Eric\nTan, Leyuan Wang, et al. 2021. “Hawq-V3: Dyadic\nNeural Network Quantization.” In International Conference on\nMachine Learning, 11875–86. PMLR.\n\n\nYe, Linfeng, and Shayan Mohajer Hamidi. 2021. “Thundernna:\nA White Box Adversarial Attack.” arXiv Preprint\narXiv:2111.12305.\n\n\nYeh, Y. C. 1996. “Triple-Triple Redundant 777 Primary Flight\nComputer.” In 1996 IEEE Aerospace Applications Conference.\nProceedings, 1:293–307. IEEE; IEEE. https://doi.org/10.1109/aero.1996.495891.\n\n\nYik, Jason, Korneel Van den Berghe, Douwe den Blanken, Younes Bouhadjar,\nMaxime Fabre, Paul Hueber, Denis Kleyko, et al. 2023. “NeuroBench:\nA Framework for Benchmarking Neuromorphic Computing Algorithms and\nSystems,” April. http://arxiv.org/abs/2304.04640v3.\n\n\nYou, Jie, Jae-Won Chung, and Mosharaf Chowdhury. 2023. “Zeus:\nUnderstanding and Optimizing GPU Energy\nConsumption of DNN Training.” In 20th USENIX\nSymposium on Networked Systems Design and Implementation (NSDI 23),\n119–39. Boston, MA: USENIX Association. https://www.usenix.org/conference/nsdi23/presentation/you.\n\n\nYou, Yang, Zhao Zhang, Cho-Jui Hsieh, James Demmel, and Kurt Keutzer.\n2017. “ImageNet Training in Minutes,” September. http://arxiv.org/abs/1709.05011v10.\n\n\nYoung, Tom, Devamanyu Hazarika, Soujanya Poria, and Erik Cambria. 2018.\n“Recent Trends in Deep Learning Based Natural Language Processing\n[Review Article].” IEEE Comput. Intell.\nMag. 13 (3): 55–75. https://doi.org/10.1109/mci.2018.2840738.\n\n\nYu, Yuan, Martı́n Abadi, Paul Barham, Eugene Brevdo, Mike Burrows, Andy\nDavis, Jeff Dean, et al. 2018. “Dynamic Control Flow in\nLarge-Scale Machine Learning.” In Proceedings of the\nThirteenth EuroSys Conference, 265–83. ACM. https://doi.org/10.1145/3190508.3190551.\n\n\nZafrir, Ofir, Guy Boudoukh, Peter Izsak, and Moshe Wasserblat. 2019.\n“Q8BERT: Quantized 8Bit\nBERT.” In 2019 Fifth Workshop on Energy\nEfficient Machine Learning and Cognitive Computing - NeurIPS Edition\n(EMC2-NIPS), 36–39. IEEE; IEEE. https://doi.org/10.1109/emc2-nips53020.2019.00016.\n\n\nZeiler, Matthew D. 2012. “ADADELTA: An Adaptive Learning Rate\nMethod,” December, 119–49. https://doi.org/10.1002/9781118266502.ch6.\n\n\nZennaro, Marco, Brian Plancher, and V Janapa Reddi. 2022.\n“TinyML: Applied AI for\nDevelopment.” In The UN 7th Multi-Stakeholder Forum on\nScience, Technology and Innovation for the Sustainable Development\nGoals, 2022–05.\n\n\nZhang, Chengliang, Minchen Yu, Wei Wang 0030, and Feng Yan 0001. 2019.\n“MArk: Exploiting Cloud Services for Cost-Effective, SLO-Aware\nMachine Learning Inference Serving.” In 2019 USENIX Annual\nTechnical Conference (USENIX ATC 19), 1049–62. https://www.usenix.org/conference/atc19/presentation/zhang-chengliang.\n\n\nZhang, Chen, Peng Li, Guangyu Sun, Yijin Guan, Bingjun Xiao, and Jason\nOptimizing Cong. 2015. “FPGA-Based Accelerator Design\nfor Deep Convolutional Neural Networks Proceedings of the 2015\nACM.” In SIGDA International Symposium on\nField-Programmable Gate Arrays-FPGA, 15:161–70.\n\n\nZhang, Dan, Safeen Huda, Ebrahim Songhori, Kartik Prabhu, Quoc Le, Anna\nGoldie, and Azalia Mirhoseini. 2022. “A Full-Stack Search\nTechnique for Domain Optimized Deep Learning Accelerators.” In\nProceedings of the 27th ACM International Conference on\nArchitectural Support for Programming Languages and Operating\nSystems, 27–42. ASPLOS ’22. New York, NY, USA: ACM. https://doi.org/10.1145/3503222.3507767.\n\n\nZhang, Dongxia, Xiaoqing Han, and Chunyu Deng. 2018. “Review on\nthe Research and Practice of Deep Learning and Reinforcement Learning in\nSmart Grids.” CSEE Journal of Power and Energy Systems 4\n(3): 362–70. https://doi.org/10.17775/cseejpes.2018.00520.\n\n\nZhang, Hongyu. 2008. “On the Distribution of Software\nFaults.” IEEE Trans. Software Eng. 34 (2): 301–2. https://doi.org/10.1109/tse.2007.70771.\n\n\nZhang, Jeff Jun, Tianyu Gu, Kanad Basu, and Siddharth Garg. 2018.\n“Analyzing and Mitigating the Impact of Permanent Faults on a\nSystolic Array Based Neural Network Accelerator.” In 2018\nIEEE 36th VLSI Test Symposium (VTS), 1–6. IEEE; IEEE. https://doi.org/10.1109/vts.2018.8368656.\n\n\nZhang, Jeff, Kartheek Rangineni, Zahra Ghodsi, and Siddharth Garg. 2018.\n“ThUnderVolt: Enabling Aggressive\nVoltage Underscaling and Timing Error Resilience for Energy Efficient\nDeep Learning Accelerators.” In 2018 55th ACM/ESDA/IEEE\nDesign Automation Conference (DAC), 1–6. IEEE. https://doi.org/10.1109/dac.2018.8465918.\n\n\nZhang, Li Lyna, Yuqing Yang, Yuhang Jiang, Wenwu Zhu, and Yunxin Liu.\n2020. “Fast Hardware-Aware Neural Architecture Search.” In\n2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition\nWorkshops (CVPRW). IEEE. https://doi.org/10.1109/cvprw50498.2020.00354.\n\n\nZhang, Qingxue, Dian Zhou, and Xuan Zeng. 2017. “Highly Wearable\nCuff-Less Blood Pressure and Heart Rate Monitoring with Single-Arm\nElectrocardiogram and Photoplethysmogram Signals.” BioMedical\nEngineering OnLine 16 (1): 23. https://doi.org/10.1186/s12938-017-0317-z.\n\n\nZhang, Tunhou, Hsin-Pai Cheng, Zhenwen Li, Feng Yan, Chengyu Huang, Hai\nHelen Li, and Yiran Chen. 2020. “AutoShrink:\nA Topology-Aware NAS for Discovering Efficient\nNeural Architecture.” In The Thirty-Fourth AAAI Conference on\nArtificial Intelligence, AAAI 2020, the Thirty-Second Innovative\nApplications of Artificial Intelligence Conference, IAAI 2020, the Tenth\nAAAI Symposium on Educational Advances in Artificial Intelligence, EAAI\n2020, New York, NY, USA, February 7-12, 2020, 6829–36. AAAI Press.\nhttps://aaai.org/ojs/index.php/AAAI/article/view/6163.\n\n\nZhao, Mark, and G. Edward Suh. 2018. “FPGA-Based Remote Power\nSide-Channel Attacks.” In 2018 IEEE Symposium on Security and\nPrivacy (SP), 229–44. IEEE; IEEE. https://doi.org/10.1109/sp.2018.00049.\n\n\nZhao, Yue, Meng Li, Liangzhen Lai, Naveen Suda, Damon Civin, and Vikas\nChandra. 2018. “Federated Learning with Non-IID Data.”\nArXiv Preprint abs/1806.00582 (June). http://arxiv.org/abs/1806.00582v2.\n\n\nZhou, Bolei, Yiyou Sun, David Bau, and Antonio Torralba. 2018.\n“Interpretable Basis Decomposition for Visual Explanation.”\nIn Proceedings of the European Conference on Computer Vision\n(ECCV), 119–34.\n\n\nZhou, Chuteng, Fernando Garcia Redondo, Julian Büchel, Irem Boybat,\nXavier Timoneda Comas, S. R. Nandakumar, Shidhartha Das, Abu Sebastian,\nManuel Le Gallo, and Paul N. Whatmough. 2021.\n“AnalogNets: Ml-hw\nCo-Design of Noise-Robust TinyML Models and Always-on\nAnalog Compute-in-Memory Accelerator.” https://arxiv.org/abs/2111.06503.\n\n\nZhou, Peng, Xintong Han, Vlad I. Morariu, and Larry S. Davis. 2018.\n“Learning Rich Features for Image Manipulation Detection.”\nIn 2018 IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 1053–61. IEEE. https://doi.org/10.1109/cvpr.2018.00116.\n\n\nZhu, Hongyu, Mohamed Akrout, Bojian Zheng, Andrew Pelegris, Anand\nJayarajan, Amar Phanishayee, Bianca Schroeder, and Gennady Pekhimenko.\n2018. “Benchmarking and Analyzing Deep Neural Network\nTraining.” In 2018 IEEE International Symposium on Workload\nCharacterization (IISWC), 88–100. IEEE; IEEE. https://doi.org/10.1109/iiswc.2018.8573476.\n\n\nZhu, Ligeng, Lanxiang Hu, Ji Lin, Wei-Ming Chen, Wei-Chen Wang, Chuang\nGan, and Song Han. 2023. “PockEngine:\nSparse and Efficient Fine-Tuning in a Pocket.” In\n56th Annual IEEE/ACM International Symposium on\nMicroarchitecture. ACM. https://doi.org/10.1145/3613424.3614307.\n\n\nZhuang, Fuzhen, Zhiyuan Qi, Keyu Duan, Dongbo Xi, Yongchun Zhu, Hengshu\nZhu, Hui Xiong, and Qing He. 2021. “A Comprehensive Survey on\nTransfer Learning.” Proceedings of the IEEE 109 (1):\n43–76. https://doi.org/10.1109/jproc.2020.3004555.\n\n\nZoph, Barret, and Quoc V. Le. 2016. “Neural Architecture Search\nwith Reinforcement Learning,” November, 367–92. https://doi.org/10.1002/9781394217519.ch17.",
    "crumbs": [
      "RIFERIMENTI",
      "Riferimenti"
    ]
  },
  {
    "objectID": "contents/labs/labs.it.html",
    "href": "contents/labs/labs.it.html",
    "title": "LABORATORI",
    "section": "",
    "text": "Questa pagina è stata lasciata intenzionalmente vuota.",
    "crumbs": [
      "LABORATORI"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/nicla_vision.it.html",
    "href": "contents/labs/arduino/nicla_vision/nicla_vision.it.html",
    "title": "Nicla Vision",
    "section": "",
    "text": "Prerequisiti",
    "crumbs": [
      "Nicla Vision"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/nicla_vision.it.html#prerequisiti",
    "href": "contents/labs/arduino/nicla_vision/nicla_vision.it.html#prerequisiti",
    "title": "Nicla Vision",
    "section": "",
    "text": "Nicla Vision Board: Si deve avere la scheda Nicla Vision.\nCavo USB: Per collegare la scheda al computer.\nRete: Con accesso a Internet per scaricare il software necessario.",
    "crumbs": [
      "Nicla Vision"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/nicla_vision.it.html#setup",
    "href": "contents/labs/arduino/nicla_vision/nicla_vision.it.html#setup",
    "title": "Nicla Vision",
    "section": "Setup",
    "text": "Setup\n\nSetup di Nicla Vision",
    "crumbs": [
      "Nicla Vision"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/nicla_vision.it.html#esercizi",
    "href": "contents/labs/arduino/nicla_vision/nicla_vision.it.html#esercizi",
    "title": "Nicla Vision",
    "section": "Esercizi",
    "text": "Esercizi\n\n\n\nModalità\nAttività\nDescrizione\nLink\n\n\n\n\nVisione\nClassificazione delle Immagini\nImparare a classificare le immagini\nLink\n\n\nVisione\nRilevamento degli Oggetti\nImplementare il rilevamento degli oggetti\nLink\n\n\nSuono\nIndividuazione delle Parole Chiave\nEsplorare i sistemi di riconoscimento vocale\nLink\n\n\nIMU\nClassificazione del Movimento e Rilevamento delle Anomalie\nClassifica i Dati di Movimento e Rileva le Anomalie\nLink",
    "crumbs": [
      "Nicla Vision"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/xiao_esp32s3.it.html",
    "href": "contents/labs/seeed/xiao_esp32s3/xiao_esp32s3.it.html",
    "title": "XIAO ESP32S3",
    "section": "",
    "text": "Prerequisiti",
    "crumbs": [
      "XIAO ESP32S3"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/xiao_esp32s3.it.html#prerequisiti",
    "href": "contents/labs/seeed/xiao_esp32s3/xiao_esp32s3.it.html#prerequisiti",
    "title": "XIAO ESP32S3",
    "section": "",
    "text": "XIAO ESP32S3 Sense Board: Si deve avere la scheda XIAO ESP32S3 Sense.\nUSB-C Cable: Per collegare la scheda al computer.\nRete: Con accesso a Internet per scaricare il software necessario.\nSD Card e SD card Adapter: Per salvare audio e immagini (opzionale).",
    "crumbs": [
      "XIAO ESP32S3"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/xiao_esp32s3.it.html#setup",
    "href": "contents/labs/seeed/xiao_esp32s3/xiao_esp32s3.it.html#setup",
    "title": "XIAO ESP32S3",
    "section": "Setup",
    "text": "Setup\n\nSetup della XIAO ESP32S3",
    "crumbs": [
      "XIAO ESP32S3"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/xiao_esp32s3.it.html#esercizi",
    "href": "contents/labs/seeed/xiao_esp32s3/xiao_esp32s3.it.html#esercizi",
    "title": "XIAO ESP32S3",
    "section": "Esercizi",
    "text": "Esercizi\n\n\n\nModalità\nAttività\nDescrizione\nLink\n\n\n\n\nVisione\nClassificazione delle immagini\nImpara a classificare le immagini\nLink\n\n\nVisione\nRilevamento di Oggetti\nImplementa il rilevamento oggetti\nLink\n\n\nSuono\nIndividuazione Parole Chiave\nEsplora sistemi di riconoscimento vocale\nLink\n\n\nIMU\nClassificazione del Movimento e Rilevamento Anomalie\nClassifica i dati di movimento e rileva anomalie\nLink",
    "crumbs": [
      "XIAO ESP32S3"
    ]
  },
  {
    "objectID": "contents/labs/raspi/raspi.it.html",
    "href": "contents/labs/raspi/raspi.it.html",
    "title": "Raspberry Pi",
    "section": "",
    "text": "Prerequisiti",
    "crumbs": [
      "Raspberry Pi"
    ]
  },
  {
    "objectID": "contents/labs/raspi/raspi.it.html#prerequisiti",
    "href": "contents/labs/raspi/raspi.it.html#prerequisiti",
    "title": "Raspberry Pi",
    "section": "",
    "text": "Raspberry Pi: Si deve avere almeno una delle seguenti schede: Raspberry Pi Zero 2W, Raspberry Pi 4 o 5 per Vision Labs e Raspberry 5 per GenAi Lab.\nAdattatore di Alimentazione: Per alimentare le schede.\n\nRaspberry Pi Zero 2-W: 2,5 W con un adattatore Micro-USB\nRaspberry Pi 4 o 5: 3,5 W con un adattatore USB-C\n\nRete: Con accesso a Internet per scaricare il software necessario e controllare le schede da remoto.\nScheda SD (minimo 32 GB) e un Adattatore per Schede SD: Per il sistema operativo Raspberry Pi.",
    "crumbs": [
      "Raspberry Pi"
    ]
  },
  {
    "objectID": "contents/labs/raspi/raspi.it.html#setup",
    "href": "contents/labs/raspi/raspi.it.html#setup",
    "title": "Raspberry Pi",
    "section": "Setup",
    "text": "Setup\n\nConfigurazione di Raspberry Pi",
    "crumbs": [
      "Raspberry Pi"
    ]
  },
  {
    "objectID": "contents/labs/raspi/raspi.it.html#esercizi",
    "href": "contents/labs/raspi/raspi.it.html#esercizi",
    "title": "Raspberry Pi",
    "section": "Esercizi",
    "text": "Esercizi\n\n\n\nModalità\nAttività\nDescrizione\nLink\n\n\n\n\nVisione\nClassificazione delle Immagini\nImparare a classificare le immagini\nLink\n\n\nVisione\nRilevamento degli Oggetti\nImplementare il rilevamento degli oggetti\nLink\n\n\nGenAI\nSmall Language Models\nDeploy SLMs at the Edge\nLink",
    "crumbs": [
      "Raspberry Pi"
    ]
  },
  {
    "objectID": "contents/labs/shared/shared.it.html",
    "href": "contents/labs/shared/shared.it.html",
    "title": "Lab Condivisi",
    "section": "",
    "text": "I lab in questa sezione coprono argomenti e tecniche applicabili a diverse piattaforme hardware. Questi laboratori sono progettati per essere indipendenti da schede specifiche, consentendo di concentrarsi sui concetti fondamentali e sugli algoritmi utilizzati nelle applicazioni (tiny) ML.\nEsplorando questi “shared lab”, si otterrà una comprensione più approfondita delle sfide e delle soluzioni comuni nell’apprendimento automatico embedded. Le conoscenze e le competenze acquisite qui saranno preziose indipendentemente dall’hardware specifico con cui lavorerà in futuro.\n\n\n\nEsercizio\nNicla Vision\nXIAO ESP32S3\n\n\n\n\nKWS Feature Engineering\n✔ Link\n✔ Link\n\n\nBlocco delle Feature Spettrali DSP\n✔ Link\n✔ Link",
    "crumbs": [
      "Lab Condivisi"
    ]
  }
]