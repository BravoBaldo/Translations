[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning Systems",
    "section": "",
    "text": "Prefazione\nBenvenuti in Machine Learning Systems. Questo libro √® la porta d‚Äôaccesso al mondo frenetico dei sistemi di intelligenza artificiale. √à un‚Äôestensione del corso CS249r alla Harvard University.\nAbbiamo creato questo libro open source come sforzo collaborativo per riunire spunti di studenti, professionisti e la pi√π ampia comunit√† di professionisti dell‚ÄôIA. Il nostro obiettivo √® sviluppare una guida completa che esplori le complessit√† dei sistemi di intelligenza artificiale e le loro numerose applicazioni.\nQuesto non √® un libro statico; √® un documento vivo e pulsante. Lo stiamo rendendo open source e lo aggiorniamo costantemente per soddisfare le esigenze in continua evoluzione di questo campo dinamico. Contiene un ricco mix di conoscenze specialistiche che guideranno attraverso la complessa interazione tra algoritmi all‚Äôavanguardia e i principi fondamentali che li fanno funzionare. Stiamo preparando il terreno per il prossimo grande balzo nell‚Äôinnovazione dell‚ÄôIA.",
    "crumbs": [
      "Prefazione"
    ]
  },
  {
    "objectID": "index.html#perch√©-abbiamo-scritto-questo-libro",
    "href": "index.html#perch√©-abbiamo-scritto-questo-libro",
    "title": "Machine Learning Systems",
    "section": "Perch√© Abbiamo Scritto Questo Libro",
    "text": "Perch√© Abbiamo Scritto Questo Libro\nViviamo in un‚Äôepoca in cui la tecnologia √® in continua evoluzione. La collaborazione aperta e la condivisione delle conoscenze sono gli elementi costitutivi della vera innovazione. Questo √® lo spirito alla base di questo lavoro. Andiamo oltre il tradizionale modello di libro di testo per creare un hub di conoscenza vivo, in modo che possiamo tutti condividere e imparare gli uni dagli altri.\nIl libro si concentra sui principi e sui casi di studio dei sistemi di IA, con l‚Äôobiettivo di fornire una comprensione approfondita che aiuter√† a navigare nel panorama in continua evoluzione dei sistemi di IA. Mantenendolo ‚Äúopen source‚Äù, non stiamo solo rendendo accessibile l‚Äôapprendimento, ma stiamo anche invitando nuove idee e miglioramenti continui. In breve, stiamo costruendo una comunit√† in cui la conoscenza √® libera di crescere e illuminare la strada verso la tecnologia AI globale.",
    "crumbs": [
      "Prefazione"
    ]
  },
  {
    "objectID": "index.html#cosa-c√®-da-sapere",
    "href": "index.html#cosa-c√®-da-sapere",
    "title": "Machine Learning Systems",
    "section": "Cosa c‚Äô√® da Sapere",
    "text": "Cosa c‚Äô√® da Sapere\nPer immergersi in questo libro, non si dev‚Äôessere un esperto di AI. Tutto ci√≤ di cui c‚Äô√® bisogno √® una conoscenza di base dei concetti di informatica e la curiosit√† di esplorare su come funzionano i sistemi AI. √à qui che avviene l‚Äôinnovazione e una conoscenza di base della programmazione e delle strutture dati sar√† la bussola.",
    "crumbs": [
      "Prefazione"
    ]
  },
  {
    "objectID": "index.html#dichiarazione-di-trasparenza-dei-contenuti",
    "href": "index.html#dichiarazione-di-trasparenza-dei-contenuti",
    "title": "Machine Learning Systems",
    "section": "Dichiarazione di Trasparenza dei Contenuti",
    "text": "Dichiarazione di Trasparenza dei Contenuti\nQuesto libro √® un progetto guidato dalla comunit√†, con contenuti generati da numerosi collaboratori nel tempo. Il processo di creazione dei contenuti potrebbe aver coinvolto vari strumenti di editing, tra cui la tecnologia AI generativa. In qualit√† di autore principale, editore e curatore, il Prof.¬†Vijay Janapa Reddi mantiene la supervisione umana e la supervisione editoriale per garantire che il contenuto sia accurato e pertinente. Tuttavia, nessuno √® perfetto, quindi potrebbero comunque esserci delle inesattezze. Apprezziamo molto i feedback e invitiamo a fornire correzioni e suggerimenti. Questo approccio collaborativo √® fondamentale per migliorare e mantenere la qualit√† del contenuto e rendere le informazioni accessibili a livello globale.",
    "crumbs": [
      "Prefazione"
    ]
  },
  {
    "objectID": "index.html#per-dare-una-mano",
    "href": "index.html#per-dare-una-mano",
    "title": "Machine Learning Systems",
    "section": "Per dare una mano",
    "text": "Per dare una mano\nSe si √® interessati a contribuire, le linee guida si trovano qui.",
    "crumbs": [
      "Prefazione"
    ]
  },
  {
    "objectID": "index.html#contatti",
    "href": "index.html#contatti",
    "title": "Machine Learning Systems",
    "section": "Contatti",
    "text": "Contatti\nCi sono domande o feedback? Si √® liberi di inviare una e-mail al Prof.¬†Vijay Janapa Reddi direttamente, oppure si pu√≤ avviare un thread di discussione su GitHub.",
    "crumbs": [
      "Prefazione"
    ]
  },
  {
    "objectID": "index.html#collaboratori",
    "href": "index.html#collaboratori",
    "title": "Machine Learning Systems",
    "section": "Collaboratori",
    "text": "Collaboratori\nUn grande ringraziamento a tutti coloro che hanno contribuito a rendere questo libro quello che √®! L‚Äôelenco completo dei singoli collaboratori √® qui e ulteriori dettagli sullo stile GitHub qui. Benvenuti come collaboratori!",
    "crumbs": [
      "Prefazione"
    ]
  },
  {
    "objectID": "index.html#copyright",
    "href": "index.html#copyright",
    "title": "Machine Learning Systems",
    "section": "Copyright",
    "text": "Copyright\nQuesto libro √® open source e sviluppato in modo collaborativo tramite GitHub. Salvo diversa indicazione, questo lavoro √® concesso in licenza con Creative Commons Attribuzione-Non commerciale-Condividi allo stesso modo 4.0 Internazionale (CC BY-NC-SA 4.0 CC BY-SA 4.0). Il testo completo della licenza si trova qui.\nI collaboratori di questo progetto hanno dedicato i loro contributi al pubblico dominio o con la stessa licenza aperta del progetto originale. Sebbene i contributi siano collaborativi, ogni collaboratore mantiene il copyright sui rispettivi contributi.\nPer i dettagli sulla paternit√†, i contributi e come contribuire, consultare il repository del progetto su GitHub.\nTutti i marchi e i marchi registrati menzionati in questo libro sono di propriet√† dei rispettivi proprietari.\nLe informazioni fornite in questo libro sono ritenute accurate e affidabili. Tuttavia, gli autori, i curatori e gli editori non possono essere ritenuti responsabili per eventuali danni causati o presumibilmente causati, direttamente o indirettamente, dalle informazioni contenute nel presente libro.",
    "crumbs": [
      "Prefazione"
    ]
  },
  {
    "objectID": "contents/core/acknowledgements/acknowledgements.it.html",
    "href": "contents/core/acknowledgements/acknowledgements.it.html",
    "title": "Ringraziamenti",
    "section": "",
    "text": "Agenzie e Aziende Finanziatrici\nSiamo grati per il supporto di varie agenzie di finanziamento e aziende che hanno sostenuto gli assistenti didattici coinvolti in questo lavoro. Le seguenti organizzazioni hanno svolto un ruolo cruciale nel dare vita a questo progetto:",
    "crumbs": [
      "Ringraziamenti"
    ]
  },
  {
    "objectID": "contents/core/acknowledgements/acknowledgements.it.html#collaboratori",
    "href": "contents/core/acknowledgements/acknowledgements.it.html#collaboratori",
    "title": "Ringraziamenti",
    "section": "Collaboratori",
    "text": "Collaboratori\nEsprimiamo la nostra sincera gratitudine alla comunit√† open source di studenti, educatori e collaboratori. Ogni contributo, che si tratti di una sezione di capitolo o di una correzione di una sola parola, ha migliorato significativamente la qualit√† di questa risorsa. Riconosciamo anche coloro che hanno condiviso intuizioni, identificato problemi e fornito feedback preziosi dietro le quinte.\nUn elenco completo di tutti i collaboratori di GitHub, aggiornato automaticamente con ogni nuovo contributo, √® disponibile di seguito. Per quelli interessati a contribuire ulteriormente, consultare la nostra pagina GitHub per maggiori informazioni.\n\n\n\n\n\n\n\n\nVijay Janapa Reddi\n\n\njasonjabbour\n\n\nIkechukwu Uchendu\n\n\nNaeem Khoshnevis\n\n\nMarcelo Rovai\n\n\n\n\nSara Khosravi\n\n\nKai Kleinbard\n\n\nDouwe den Blanken\n\n\nMatthew Stewart\n\n\nshanzehbatool\n\n\n\n\nElias Nuwara\n\n\nJared Ping\n\n\nItai Shapira\n\n\nMaximilian Lam\n\n\nJayson Lin\n\n\n\n\nSophia Cho\n\n\nAndrea\n\n\nJeffrey Ma\n\n\nAlex Rodriguez\n\n\nKorneel Van den Berghe\n\n\n\n\nColby Banbury\n\n\nZishen Wan\n\n\nAbdulrahman Mahmoud\n\n\nSrivatsan Krishnan\n\n\nDivya Amirtharaj\n\n\n\n\nEmeka Ezike\n\n\nAghyad Deeb\n\n\nHaoran Qiu\n\n\nmarin-llobet\n\n\nEmil Njor\n\n\n\n\nAditi Raju\n\n\nJared Ni\n\n\nMichael Schnebly\n\n\noishib\n\n\nELSuitorHarvard\n\n\n\n\nHenry Bae\n\n\nJae-Won Chung\n\n\nYu-Shun Hsiao\n\n\nMark Mazumder\n\n\nMarco Zennaro\n\n\n\n\nEura Nofshin\n\n\nAndrew Bass\n\n\nPong Trairatvorakul\n\n\nJennifer Zhou\n\n\nShvetank Prakash\n\n\n\n\nAlex Oesterling\n\n\nArya Tschand\n\n\nBruno Scaglione\n\n\nGauri Jain\n\n\nAllen-Kuang\n\n\n\n\nFin Amin\n\n\nFatima Shah\n\n\nThe Random DIY\n\n\ngnodipac886\n\n\nSercan Ayg√ºn\n\n\n\n\nBaldassarre Cesarano\n\n\nAbenezer\n\n\nBilge Acun\n\n\nyanjingl\n\n\nYang Zhou\n\n\n\n\nabigailswallow\n\n\nJason Yik\n\n\nhappyappledog\n\n\nCurren Iyer\n\n\nEmmanuel Rassou\n\n\n\n\nSonia Murthy\n\n\nShreya Johri\n\n\nJessica Quaye\n\n\nVijay Edupuganti\n\n\nCostin-Andrei Oncescu\n\n\n\n\nAnnie Laurie Cook\n\n\nJothi Ramaswamy\n\n\nBatur Arslan\n\n\nFatima Shah\n\n\na-saraf\n\n\n\n\nsonghan\n\n\nZishen",
    "crumbs": [
      "Ringraziamenti"
    ]
  },
  {
    "objectID": "contents/core/about/about.it.html",
    "href": "contents/core/about/about.it.html",
    "title": "Informazioni sul Libro",
    "section": "",
    "text": "Panoramica\nBenvenuti a questo libro collaborativo, sviluppato come parte del corso CS249r Machine Learning Systems presso l‚ÄôUniversit√† di Harvard. L‚Äôobiettivo √® quello di fornire una risorsa completa per educatori e studenti che desiderano comprendere i sistemi di apprendimento automatico. Questo libro viene costantemente aggiornato per incorporare le ultime intuizioni e strategie didattiche efficaci.",
    "crumbs": [
      "Informazioni sul Libro"
    ]
  },
  {
    "objectID": "contents/core/about/about.it.html#cosa-c√®-nel-libro",
    "href": "contents/core/about/about.it.html#cosa-c√®-nel-libro",
    "title": "Informazioni sul Libro",
    "section": "Cosa c‚Äô√® nel Libro",
    "text": "Cosa c‚Äô√® nel Libro\nEsploriamo le basi tecniche dei sistemi di apprendimento automatico, le sfide della creazione e distribuzione di questi sistemi nel continuum informatico e la vasta gamma di applicazioni che consentono. Un aspetto unico di questo libro √® la sua funzione di canale verso opere accademiche fondamentali e documenti di ricerca accademica, mirati ad arricchire la comprensione del lettore e incoraggiare un‚Äôesplorazione pi√π approfondita dell‚Äôargomento. Questo approccio cerca di colmare il divario tra materiali pedagogici e tendenze di ricerca all‚Äôavanguardia, offrendo una guida completa che √® al passo con l‚Äôevoluzione del campo dell‚Äôapprendimento automatico applicato.\nPer migliorare l‚Äôesperienza di apprendimento, abbiamo incluso una variet√† di materiali supplementari. In tutto il libro, si troveranno slide che riassumono i concetti chiave, video che forniscono spiegazioni e dimostrazioni approfondite, esercizi che rafforzano la comprensione ed esercizi pratici che offrono esperienza pratica con gli strumenti e le tecniche discussi. Queste risorse aggiuntive sono progettate per soddisfare diversi stili di apprendimento e contribuire ad acquisire una comprensione pi√π profonda e pratica dell‚Äôargomento.",
    "crumbs": [
      "Informazioni sul Libro"
    ]
  },
  {
    "objectID": "contents/core/about/about.it.html#argomenti-esplorati",
    "href": "contents/core/about/about.it.html#argomenti-esplorati",
    "title": "Informazioni sul Libro",
    "section": "Argomenti Esplorati",
    "text": "Argomenti Esplorati\nQuesto libro di testo offre un‚Äôesplorazione completa di vari aspetti dei sistemi di apprendimento automatico, coprendo l‚Äôintero flusso di lavoro end-to-end. Partendo dai concetti fondamentali, procede attraverso aree essenziali come l‚Äôingegneria dei dati, i framework di IA e l‚Äôaddestramento dei modelli.\nPer migliorare l‚Äôesperienza di apprendimento, abbiamo incluso una vasta gamma di materiali supplementari. Queste risorse sono costituite da slide che riassumono i concetti chiave, video che forniscono spiegazioni e dimostrazioni dettagliate, esercizi progettati per rafforzare la comprensione e laboratori che offrono esperienza pratica con gli strumenti e le tecniche discussi.\nI lettori acquisiranno informazioni sull‚Äôottimizzazione dei modelli per l‚Äôefficienza, l‚Äôimplementazione dell‚ÄôIA su diverse piattaforme hardware e il benchmarking delle prestazioni. Il libro approfondisce anche argomenti avanzati, tra cui sicurezza, privacy, IA responsabile e sostenibile, IA robusta e IA generativa. Inoltre, esamina l‚Äôimpatto sociale dell‚ÄôIA, concludendo con un‚Äôenfasi sui contributi positivi che l‚ÄôIA pu√≤ apportare alla societ√†.",
    "crumbs": [
      "Informazioni sul Libro"
    ]
  },
  {
    "objectID": "contents/core/about/about.it.html#principali-risultati-dellapprendimento",
    "href": "contents/core/about/about.it.html#principali-risultati-dellapprendimento",
    "title": "Informazioni sul Libro",
    "section": "Principali Risultati dell‚ÄôApprendimento",
    "text": "Principali Risultati dell‚ÄôApprendimento\nI lettori acquisiranno competenze nel training e nell‚Äôimplementazione di modelli di reti neurali profonde su diverse piattaforme, oltre a comprendere le sfide pi√π ampie coinvolte nella loro progettazione, sviluppo e implementazione. Nello specifico, dopo aver completato questo libro, gli studenti saranno in grado di:\n\n\n\n\n\n\nConsiglio\n\n\n\n\nSpiegare i concetti fondamentali e la loro rilevanza per i sistemi di IA.\nDescrivere i componenti fondamentali e l‚Äôarchitettura dei sistemi di IA.\nConfrontare e mettere a contrasto varie piattaforme hardware per l‚Äôimplementazione dell‚ÄôIA, selezionando le opzioni appropriate per casi d‚Äôuso specifici.\nProgettare e implementare processi di training per modelli di IA su sistemi diversi.\nApplicare tecniche di ottimizzazione per migliorare le prestazioni e l‚Äôefficienza dei modelli di IA.\nAnalizzare le applicazioni di IA nel mondo reale e le loro strategie di implementazione.\nValutare le attuali sfide nei sistemi di IA e prevedere le tendenze future nel settore.\nSviluppare un progetto completo basato sull‚Äôapprendimento automatico, dalla concezione all‚Äôimplementazione.\nRisolvere i problemi comuni nell‚Äôaddestramento e nell‚Äôimplementazione dei modelli di IA.\nValutare criticamente le implicazioni etiche e gli impatti sociali dei sistemi di IA.",
    "crumbs": [
      "Informazioni sul Libro"
    ]
  },
  {
    "objectID": "contents/core/about/about.it.html#prerequisiti-per-i-lettori",
    "href": "contents/core/about/about.it.html#prerequisiti-per-i-lettori",
    "title": "Informazioni sul Libro",
    "section": "Prerequisiti per i Lettori",
    "text": "Prerequisiti per i Lettori\n\nCompetenze di programmazione di base: Consigliamo di avere una certa esperienza di programmazione, idealmente in Python. Una conoscenza delle variabili, tipi di dati e strutture di controllo faciliter√† l‚Äôinterazione col libro.\nAlcune Conoscenze di Machine Learning: Sebbene non sia obbligatorio, una conoscenza di base dei concetti di apprendimento automatico aiuter√† ad assorbire il materiale pi√π facilmente. Se si √® nuovi nel campo, il libro fornisce sufficienti informazioni di base per mettersi al passo.\nConoscenza di Base dei Sistemi: Si consiglia un livello di conoscenza di base dei sistemi a livello universitario junior o senior. Sar√† utile comprendere l‚Äôarchitettura di sistema, i sistemi operativi e le reti di base.\nProgrammazione Python (Facoltativo): Se si ha familiarit√† con Python, si trover√† pi√π facile interagire con le sezioni di codifica del libro. Conoscere librerie come NumPy, scikit-learn e TensorFlow sar√† particolarmente utile.\nVoglia di Imparare: Il libro √® progettato per essere accessibile a un vasto pubblico, con diversi livelli di competenza tecnica. La volont√† di sfidare se stessi e di impegnarsi in esercizi pratici aiuter√† a trarne il massimo vantaggio.\nDisponibilit√† delle Risorse: Per gli aspetti pratici, ci sar√† bisogno di un computer con Python e le librerie pertinenti installate. L‚Äôaccesso facoltativo a schede di sviluppo o hardware specifico sar√† utile anche per sperimentare la distribuzione del modello di apprendimento automatico.\n\nSoddisfacendo questi prerequisiti, si sar√† ben posizionati per approfondire la comprensione dei sistemi di apprendimento automatico, impegnarsi in esercizi di codifica e persino implementare applicazioni pratiche su vari dispositivi.",
    "crumbs": [
      "Informazioni sul Libro"
    ]
  },
  {
    "objectID": "contents/core/about/about.it.html#chi-dovrebbe-leggerlo",
    "href": "contents/core/about/about.it.html#chi-dovrebbe-leggerlo",
    "title": "Informazioni sul Libro",
    "section": "Chi Dovrebbe Leggerlo",
    "text": "Chi Dovrebbe Leggerlo\nQuesto libro √® progettato per individui in diverse fasi del loro percorso con i sistemi di machine learning, dai principianti a quelli pi√π avanzati nel settore. Introduce concetti fondamentali e avanza verso argomenti complessi rilevanti per la comunit√† di apprendimento automatico e vaste aree di ricerca. I principali destinatari di questo libro includono:\n\nStudenti di Informatica e Ingegneria Elettrica: Studenti senior e laureati troveranno questo libro particolarmente prezioso. Introduce le tecniche essenziali per la progettazione e la creazione di sistemi di apprendimento automatico, concentrandosi sulle conoscenze fondamentali piuttosto che sui dettagli esaustivi, spesso al centro dell‚Äôinsegnamento in classe. Questo libro fornir√† il background e il contesto necessari, consentendo agli insegnanti di esplorare argomenti avanzati in modo pi√π approfondito. Una caratteristica essenziale √® la sua prospettiva end-to-end, che viene spesso trascurata nei programmi tradizionali.\nIngegneri di Sistema: Questo libro funge da guida per gli ingegneri che cercano di comprendere le complessit√† dei sistemi e delle applicazioni intelligenti, in particolare che coinvolgono l‚Äôapprendimento automatico. Comprende i framework concettuali e i componenti pratici che compongono un sistema ML, estendendosi oltre le aree specifiche che si potrebbero incontrare nel proprio ruolo professionale.\nRicercatori e Accademici: Per i ricercatori, questo libro affronta le sfide distinte dell‚Äôesecuzione di algoritmi di apprendimento automatico su diverse piattaforme. Man mano che l‚Äôefficienza acquisisce importanza, una solida comprensione dei sistemi, al di l√† dei soli algoritmi, √® fondamentale per sviluppare modelli pi√π efficienti. Il libro fa riferimento a documenti seminali, indirizzando i ricercatori verso lavori che hanno influenzato il campo e stabilendo connessioni tra varie aree con implicazioni significative per la loro ricerca.",
    "crumbs": [
      "Informazioni sul Libro"
    ]
  },
  {
    "objectID": "contents/core/about/about.it.html#come-orientarsi-in-questo-libro",
    "href": "contents/core/about/about.it.html#come-orientarsi-in-questo-libro",
    "title": "Informazioni sul Libro",
    "section": "Come Orientarsi in Questo Libro",
    "text": "Come Orientarsi in Questo Libro\nPer ottenere il massimo da questo libro, consigliamo un approccio di apprendimento strutturato che sfrutti le varie risorse fornite. Ogni capitolo include slide, video, esercizi e laboratori per soddisfare diversi stili di apprendimento e rafforzare la comprensione.\n\nI Fondamenti (Capitoli 1-3): Si inizia costruendo una solida base con i primi capitoli, che forniscono un‚Äôintroduzione all‚ÄôIA embedded e trattano argomenti fondamentali come sistemi di IA e deep learning.\nFlusso di Lavoro (Capitoli 4-6): Con questa base, si passa ai capitoli incentrati sugli aspetti pratici del processo di creazione del modello AI come flussi di lavoro, ingegneria dei dati e framework.\nTraining (Capitoli 7-10): Questi capitoli offrono approfondimenti su come addestrare efficacemente i modelli AI, comprese tecniche per efficienza, ottimizzazioni e accelerazione.\nDeployment (Capitoli 11-13): Si esamina come distribuire l‚ÄôIA sui dispositivi e monitorarne l‚Äôoperativit√† tramite metodi come benchmarking, on-device learning e MLOps.\nArgomenti Avanzati (Capitoli 14-18): Si esaminano criticamente argomenti come sicurezza, privacy, etica, sostenibilit√†, robustezza e IA generativa.\nImpatto Sociale (Capitolo 19): Esplora le applicazioni positive e il potenziale dell‚ÄôIA per il bene della societ√†.\nConclusione (Capitolo 20): Riflessioni sui principali risultati e sulle direzioni future dei sistemi di IA.\n\nSebbene il libro sia progettato per un apprendimento progressivo, incoraggiamo un approccio di apprendimento interconnesso che consente di navigare tra i capitoli in base ai propri interessi e alle proprie esigenze. In tutto il libro si trovano casi di studio ed esercizi pratici che aiuteranno a mettere in relazione la teoria con le applicazioni del mondo reale. Consigliamo inoltre di partecipare a forum e gruppi per partecipare a discussioni, discutere concetti e condividere approfondimenti con altri studenti. Rivedere regolarmente i capitoli pu√≤ aiutare a rafforzare l‚Äôapprendimento e offrire nuove prospettive sui concetti trattati. Adottando questo approccio strutturato ma flessibile e interagendo attivamente con i contenuti e la community, si far√† un‚Äôesperienza di apprendimento appagante e arricchente che massimizza la comprensione.",
    "crumbs": [
      "Informazioni sul Libro"
    ]
  },
  {
    "objectID": "contents/core/about/about.it.html#approfondimenti-capitolo-per-capitolo",
    "href": "contents/core/about/about.it.html#approfondimenti-capitolo-per-capitolo",
    "title": "Informazioni sul Libro",
    "section": "Approfondimenti Capitolo per Capitolo",
    "text": "Approfondimenti Capitolo per Capitolo\nEcco uno sguardo pi√π da vicino a cosa tratta ogni capitolo. Abbiamo strutturato il libro in sei sezioni principali: Nozioni Fondamentali, Flusso di lavoro, Training, Deployment, Argomenti avanzati e Impatto. Queste sezioni riflettono da vicino i componenti principali di una tipica pipeline di machine learning, dalla comprensione dei concetti di base al la deploy e alla manutenzione dei sistemi di intelligenza artificiale in applicazioni del mondo reale. Organizzando il contenuto in questo modo, puntiamo a fornire una progressione logica che rispecchi il processo effettivo di sviluppo e implementazione dei sistemi di intelligenza artificiale.\n\nNozioni Fondamentali\nNella sezione Nozioni Fondamentali, poniamo le basi per comprendere l‚Äôintelligenza artificiale. Questo √® ben lungi dall‚Äôessere un‚Äôimmersione profonda negli algoritmi, ma puntiamo a introdurre concetti chiave, fornire una panoramica dei sistemi di apprendimento automatico e approfondire i principi e gli algoritmi di deep learning che alimentano le applicazioni di IA nei loro sistemi associati. Questa sezione fornisce le conoscenze essenziali necessarie per comprendere i capitoli successivi.\n\nIntroduzione: Questo capitolo prepara il terreno, fornendo una panoramica dell‚Äôintelligenza artificiale e gettando le basi per i capitoli successivi.\nSistemi di ML: Introduciamo le basi dei sistemi di machine learning [apprendimento automatico], le piattaforme in cui gli algoritmi di intelligenza artificiale sono ampiamente applicati.\nAvvio al Deep Learning: Questo capitolo offre una breve introduzione agli algoritmi e ai principi alla base delle applicazioni di IA nei sistemi di apprendimento automatico.\n\n\n\nWorkflow\nLa sezione Workflow [Flusso di lavoro] guida attraverso gli aspetti pratici della creazione di modelli AI. Analizziamo il flusso di lavoro AI, discutiamo le ‚Äúbest practice‚Äù di data engineering e passiamo in rassegna i framework AI pi√π diffusi. Alla fine di questa sezione, si avr√† una chiara comprensione dei passaggi coinvolti nello sviluppo di applicazioni AI competenti e degli strumenti disponibili per semplificare il processo.\n\nWorkflow IA: Questo capitolo analizza il flusso di lavoro di apprendimento automatico, offrendo approfondimenti sui passaggi che portano ad applicazioni AI competenti.\nIngegneria dei Dati: Ci concentriamo sull‚Äôimportanza dei dati nei sistemi di IA, discutendo su come gestire e organizzare efficacemente i dati.\nFramework di IA: Questo capitolo esamina diversi framework per lo sviluppo di modelli di apprendimento automatico, guidando nella scelta di quello pi√π adatto ai propri progetti.\n\n\n\nTraining\nNella sezione Training, esploriamo tecniche per il training efficiente e affidabile di modelli di IA. Trattiamo strategie per raggiungere efficienza, ottimizzazioni dei modelli e il ruolo dell‚Äôhardware specializzato nell‚Äôaccelerazione IA. Questa sezione fornisce le conoscenze necessarie per sviluppare modelli ad alte prestazioni che possono essere integrati perfettamente nei sistemi di IA.\n\nTraining IA: Questo capitolo approfondisce il training [addestramento] dei modelli, esplorando tecniche per sviluppare modelli efficienti e affidabili.\nIA Efficiente: Qui, discutiamo strategie per raggiungere l‚Äôefficienza nelle applicazioni di IA, dall‚Äôottimizzazione delle risorse computazionali al miglioramento delle prestazioni.\nOttimizzazioni dei Modelli: Esploriamo vari percorsi per ottimizzare i modelli di IA per un‚Äôintegrazione senza soluzione di continuit√† nei sistemi di IA.\nAccelerazione dell‚ÄôIA: Discutiamo il ruolo dell‚Äôhardware specializzato nel migliorare le prestazioni dei sistemi di IA.\n\n\n\nDeployment\nLa sezione Deployment [distribuzione] si concentra sulle sfide e sulle soluzioni per l‚Äôimplementazione di modelli di IA. Discutiamo metodi di benchmarking per valutare le prestazioni del sistema AI, tecniche per l‚Äôapprendimento ‚Äúon-device‚Äù per migliorare l‚Äôefficienza e la privacy e i processi coinvolti nelle operazioni di ML. Questa sezione fornisce le competenze per implementare e gestire in modo efficace le funzionalit√† di IA nei sistemi di intelligenza artificiale.\n\nBenchmark dell‚ÄôIA: Questo capitolo si concentra su come valutare i sistemi di IA tramite metodi di benchmarking sistematici.\nApprendimento On-Device: Esploriamo tecniche per l‚Äôapprendimento localizzato, che migliora sia l‚Äôefficienza che la privacy.\nOperazioni di ML: Questo capitolo esamina i processi coinvolti nell‚Äôintegrazione, nel monitoraggio e nella manutenzione senza soluzione di continuit√† delle funzionalit√† di IA.\n\n\n\nArgomenti Avanzati\nNella sezione Argomenti avanzati, studieremo le problematiche critiche che circondano l‚ÄôIA. Affrontiamo le preoccupazioni relative a privacy e sicurezza, esploriamo i principi etici dell‚Äôintelligenza artificiale responsabile, discutiamo strategie per uno sviluppo sostenibile dell‚Äôintelligenza artificiale, esaminiamo tecniche per la creazione di modelli di intelligenza artificiale solidi e introduciamo l‚Äôentusiasmante campo dell‚Äôintelligenza artificiale generativa. Questa sezione amplia la comprensione del complesso panorama dell‚ÄôIA e prepara ad affrontarne le sfide.\n\nSicurezza e Privacy: Man mano che l‚Äôintelligenza artificiale diventa sempre pi√π onnipresente, questo capitolo affronta gli aspetti cruciali della privacy e della sicurezza nei sistemi di IA.\nIA Responsabile: Discutiamo i principi etici che guidano l‚Äôuso responsabile dell‚Äôintelligenza artificiale, concentrandoci sulla correttezza, responsabilit√† e trasparenza.\nIA Sostenibile: Questo capitolo esplora pratiche e strategie per un‚Äôintelligenza artificiale sostenibile, garantendo fattibilit√† a lungo termine e un impatto ambientale ridotto.\nIA Robusta: Parliamo di tecniche per sviluppare modelli di IA affidabili e robusti che possano funzionare in modo coerente in varie condizioni.\nIA Generativa: Questo capitolo esplora gli algoritmi e le tecniche alla base dell‚ÄôIA generativa, aprendo strade all‚Äôinnovazione e alla creativit√†.\n\n\n\nImpatto Sociale\nLa sezione Impatto Sociale evidenzia il potenziale trasformativo dell‚ÄôIA in vari domini. Presentiamo applicazioni reali di TinyML in sanit√†, agricoltura, conservazione e altre aree in cui l‚ÄôIA sta facendo una positiva differenza. Questa sezione invoglia a sfruttare la potenza dell‚ÄôAI per il bene della societ√† e a contribuire allo sviluppo di soluzioni di impatto.\n\nAI for Good: Evidenziamo applicazioni positive di TinyML in aree come sanit√†, agricoltura e la conservazione.\n\n\n\nChiusura\nNella sezione Chiusura, riflettiamo sugli insegnamenti chiave del libro e guardiamo al futuro dell‚ÄôIA. Sintetizziamo i concetti trattati, discutiamo le tendenze emergenti e forniamo indicazioni su come proseguire nel percorso di apprendimento in questo campo in rapida evoluzione. Questa sezione lascia con una comprensione completa dell‚ÄôIA e l‚Äôentusiasmo di applicare le conoscenze in modi innovativi.\n\nConclusione: Il libro si conclude con una riflessione sugli apprendimenti chiave e sulle direzioni future nel campo dell‚ÄôIA.",
    "crumbs": [
      "Informazioni sul Libro"
    ]
  },
  {
    "objectID": "contents/core/about/about.it.html#apprendimento-personalizzato",
    "href": "contents/core/about/about.it.html#apprendimento-personalizzato",
    "title": "Informazioni sul Libro",
    "section": "Apprendimento Personalizzato",
    "text": "Apprendimento Personalizzato\nSappiamo che i lettori hanno interessi diversi; alcuni potrebbero voler comprendere i fondamenti, mentre altri sono desiderosi di approfondire argomenti avanzati come l‚Äôaccelerazione hardware o l‚Äôetica dell‚Äôintelligenza artificiale. Per aiutare a navigare nel libro in modo pi√π efficace, abbiamo creato una guida alla lettura basata sulla persona, su misura per i propri interessi e obiettivi specifici. Questa guida aiuta a identificare il lettore che meglio corrisponde ai propri interessi. Ogni persona rappresenta un profilo di lettore distinto con obiettivi specifici. Selezionando la persona che in sintonia, ci si pu√≤ concentrare sui capitoli e sulle sezioni pi√π pertinenti alle proprie esigenze.\n\n\n\n\n\n\n\n\n\nPersona\nDescrizione\nCapitoli\nFocus\n\n\n\n\nIl Principiante del TinyML\n√à nuovo nel campo del TinyML e impaziente di imparare le basi.\n1-3, 8, 9, 10, 12\nComprendere i fondamenti, ottenere informazioni su ML efficiente, e ottimizzato e scoprire l‚Äôaddestramento sul dispositivo.\n\n\nL‚ÄôAppassionato di EdgeML\nHa una certa conoscenza di TinyML ed √® interessato a esplorare il mondo pi√π ampio di EdgeML.\n1-3, 8, 9, 10, 12, 13\nCostruire solide basi, approfondire le complessit√† del ML efficiente ed esplorare gli aspetti operativi dei sistemi embedded.\n\n\nIl Visionario Informatico\n√à affascinato dalla visione artificiale e dalle sue applicazioni in TinyML ed EdgeML.\n1-3, 5, 8-10, 12, 13, 17, 20\nIniziare dalle basi, esplorare l‚Äôingegneria dei dati e studiare i metodi per ottimizzare i modelli ML. Scoprire la robustezza e il futuro dei sistemi di ML.\n\n\nIl Maestro dei Dati\n√à appassionato di dati e del loro ruolo cruciale nei sistemi ML.\n1-5, 8-13\nAcquisire una comprensione completa del ruolo dei dati nei sistemi ML, esplorare il flusso di lavoro ML e approfondire le considerazioni sull‚Äôottimizzazione del modello e sulla distribuzione.\n\n\nL‚ÄôEroe dell‚ÄôHardware\n√à entusiasta degli aspetti hardware dei sistemi di ML e di come influenzano le prestazioni del modello.\n1-3, 6, 8-10, 12, 14, 17, 20\nCostruire solide basi nei sistemi e nei framework ML, esplorare le sfide dell‚Äôottimizzazione dei modelli per l‚Äôefficienza, la progettazione congiunta hardware-software e gli aspetti della sicurezza.\n\n\nIl campione della Sostenibilit√†\n√à per la sostenibilit√† e vuol imparare a sviluppare sistemi di IA ecocompatibili.\n1-3, 8-10, 12, 15, 16, 20\nIniziare con i fondamenti dei sistemi ML e TinyML, esplorare le tecniche di ottimizzazione e scoprire le pratiche di IA responsabili e sostenibili.\n\n\nIl Ricercatore di Etica nell‚ÄôIA\n√à preoccupato per le implicazioni etiche dell‚ÄôIA e vuol garantire uno sviluppo e un‚Äôimplementazione responsabili.\n1-3, 5, 7, 12, 14-16, 19, 20\nOttenere informazioni sulle considerazioni etiche che circondano l‚ÄôIA, tra cui correttezza, privacy, sostenibilit√† e pratiche di sviluppo responsabili.\n\n\nL‚Äôingegnere ML Full-Stack\n√à un esperto di ML esperto e vuole approfondire la comprensione dell‚Äôintero stack del sistema ML.\nL‚Äôintero libro\nComprendere il processo end-to-end di creazione e distribuzione di sistemi di ML, dall‚Äôingegneria dei dati e dall‚Äôottimizzazione dei modelli all‚Äôaccelerazione hardware e alle considerazioni etiche.",
    "crumbs": [
      "Informazioni sul Libro"
    ]
  },
  {
    "objectID": "contents/core/about/about.it.html#unirsi-alla-community",
    "href": "contents/core/about/about.it.html#unirsi-alla-community",
    "title": "Informazioni sul Libro",
    "section": "Unirsi alla Community",
    "text": "Unirsi alla Community\nL‚Äôapprendimento nel mondo frenetico dell‚Äôintelligenza artificiale √® un viaggio collaborativo. Ci siamo prefissati di coltivare una vivace comunit√† di studenti, innovatori e collaboratori. Esplorando i concetti e impegnandosi con gli esercizi, incoraggiamo a condividere le intuizioni ed esperienze personali. Che si tratti di un approccio innovativo, di un‚Äôapplicazione interessante o di una domanda stimolante, i contributi dei singoli possono arricchire l‚Äôecosistema di apprendimento. Partecipare alle discussioni, offrire e cercare indicazioni e collaborare a progetti per promuovere una cultura di crescita e apprendimento reciproci. Condividendo la conoscenza, si svolge un ruolo importante nel promuovere una comunit√† connessa, informata e potenziata a livello globale.",
    "crumbs": [
      "Informazioni sul Libro"
    ]
  },
  {
    "objectID": "contents/ai/socratiq.it.html",
    "href": "contents/ai/socratiq.it.html",
    "title": "SocratiQ AI",
    "section": "",
    "text": "Assistente di apprendimento IA\nBenvenuti a SocratiQ (si pronuncia ‚ÄúSocratic‚Äù), un assistente di apprendimento IA perfettamente integrato in questa risorsa. Ispirato al metodo di insegnamento socratico, che enfatizza domande e risposte ponderate per stimolare il pensiero critico‚ÄîSocratiQ fa parte del nostro esperimento con ci√≤ che chiamiamo Apprendimento Generativo. Combinando quiz interattivi, assistenza personalizzata e feedback in tempo reale, SocratiQ √® pensato per rafforzare la comprensione e aiutare a creare nuove connessioni. Nota: SocratiQ √® ancora un ‚Äúwork in progress‚Äù e accogliamo con favore i feedback.\nSi ascolti questo podcast generato dall‚ÄôIA su SocratiQ, creato utilizzando le note di questa pagina con NotebookLM di Google.\nSi pu√≤ abilitare SocratiQ cliccando sul pulsante qui sotto:\nL‚Äôobiettivo di SocratiQ √® adattarsi alle proprie esigenze, generando domande mirate e impegnandosi in un dialogo significativo sul materiale didattico. A differenza dello studio tradizionale sui libri di testo, SocratiQ offre un‚Äôesperienza di apprendimento interattiva e personalizzata che pu√≤ aiutare a comprendere meglio e a ricordare concetti complessi.",
    "crumbs": [
      "SocratiQ AI"
    ]
  },
  {
    "objectID": "contents/ai/socratiq.it.html#assistente-di-apprendimento-ia",
    "href": "contents/ai/socratiq.it.html#assistente-di-apprendimento-ia",
    "title": "SocratiQ AI",
    "section": "",
    "text": "SocratiQ: OFF\n\n\n\n\n\n\n\n\n\nURL per l‚ÄôAccesso Diretto\n\n\n\n\n\nSi pu√≤ controllare direttamente SocratiQ aggiungendo i parametri ?socratiq= all‚ÄôURL:\n\nPer attivare: mlsysbook.ai/?socratiq=true\nPer disattivare: mlsysbook.ai/?socratiq=false\n\nQuesto d√† un rapido accesso per attivare/disattivare la funzionalit√† di SocratiQ direttamente dalla barra degli indirizzi del browser se ci si trova su una pagina e non si vuol tornare qui per attivare/disattivare la funzionalit√†.",
    "crumbs": [
      "SocratiQ AI"
    ]
  },
  {
    "objectID": "contents/ai/socratiq.it.html#guida-rapida",
    "href": "contents/ai/socratiq.it.html#guida-rapida",
    "title": "SocratiQ AI",
    "section": "Guida Rapida",
    "text": "Guida Rapida\n\nAbilitare SocratiQ utilizzando il pulsante in basso o i parametri URL\nUtilizzare la scorciatoia da tastiera (Cmd/Ctrl + /) per aprire SocratiQ in qualsiasi momento\nImpostare il proprio livello accademico in ‚ÄúSettings‚Äù\nIniziare a imparare! Cercare i pulsanti dei quiz alla fine delle sezioni\n\nNotare che questa √® una funzionalit√† sperimentale. Stiamo sperimentando l‚Äôidea di creare un‚Äôesperienza di apprendimento dinamica e personalizzata sfruttando la potenza dell‚ÄôIA generativa. Ci auguriamo che questo approccio trasformi il modo in cui si interagisce e assorbono i concetti complessi.\n\n\n\n\n\n\nAvviso\n\n\n\nLe Risposte dell‚ÄôIA: Sebbene SocratiQ utilizzi un‚ÄôIA avanzata per generare quiz e fornire assistenza, come tutti i sistemi di IA, pu√≤ occasionalmente fornire risposte imperfette o incomplete. Tuttavia, l‚Äôabbiamo progettato e testato per garantire che sia efficace nel supportare il percorso di apprendimento. Se non si √® sicuri di una risposta, si faccia riferimento al contenuto del libro di testo o si chieda consiglio all‚Äôistruttore.\n\n\nUna volta abilitato SocratiQ, sar√† sempre disponibile quando visitando questo sito.\nPuoi accedere a SocratiQ in qualsiasi momento utilizzando una scorciatoia da tastiera mostrata in Figura¬†1, che richiama l‚Äôinterfaccia mostrata in Figura¬†2.\n\n\n\n\n\n\nFigura¬†1: Scorciatoia da tastiera per SocratiQ.\n\n\n\n\n\n\n\n\n\nFigura¬†2: L‚Äôinterfaccia principale di SocratiQ, che mostra i componenti chiave dell‚Äôassistente di apprendimento IA.",
    "crumbs": [
      "SocratiQ AI"
    ]
  },
  {
    "objectID": "contents/ai/socratiq.it.html#panoramica-dei-pulsanti",
    "href": "contents/ai/socratiq.it.html#panoramica-dei-pulsanti",
    "title": "SocratiQ AI",
    "section": "Panoramica dei Pulsanti",
    "text": "Panoramica dei Pulsanti\nLa barra di navigazione superiore fornisce un rapido accesso alle seguenti funzionalit√†:\n\nPer regolare le impostazioni in qualsiasi momento.\nPer tenere traccia dei progressi visualizzando la dashboard.\nPer avviare o salvare le conversazioni con SocratiQ.\n\n\n\n\n\n\n\nFigura¬†3: Visualizzazione del men√π di navigazione superiore.",
    "crumbs": [
      "SocratiQ AI"
    ]
  },
  {
    "objectID": "contents/ai/socratiq.it.html#sec-socratiq-settings",
    "href": "contents/ai/socratiq.it.html#sec-socratiq-settings",
    "title": "SocratiQ AI",
    "section": "Personalizzazione dell‚ÄôApprendimento",
    "text": "Personalizzazione dell‚ÄôApprendimento\nPrima di immergersi nello studio, ci si prenda un momento per configurare SocratiQ in base al proprio livello accademico. Questa configurazione iniziale assicura che tutte le interazioni, dalle domande del quiz alle spiegazioni, siano personalizzate in base alle proprie conoscenze di base. La Figura¬†4 mostra dove regolare queste preferenze.\n\n\n\n\n\n\nFigura¬†4: Il pannello delle impostazioni in cui personalizzare SocratiQ in base al proprio livello accademico.\n\n\n\nSi pu√≤ aumentare qualsiasi risposta AI SocratiQ utilizzando il men√π a discesa nella parte superiore di ogni messaggio.\n\n\n\n\n\n\nFigura¬†5: Ripetere un messaggio AI scegliendo un nuovo livello di esperienza.",
    "crumbs": [
      "SocratiQ AI"
    ]
  },
  {
    "objectID": "contents/ai/socratiq.it.html#sec-socratiq-learning",
    "href": "contents/ai/socratiq.it.html#sec-socratiq-learning",
    "title": "SocratiQ AI",
    "section": "Apprendimento con SocratiQ",
    "text": "Apprendimento con SocratiQ\n\nI Quiz\nMan mano che si procede in ogni sezione del libro di testo, si ha la possibilit√† di chiedere a SocratiQ di generare automaticamente quiz su misura per rafforzare i concetti chiave. Questi quiz sono opportunamente inseriti alla fine di ogni sottosezione principale (ad esempio, 1.1, 1.2, 1.3 e cos√¨ via), come illustrato in Figura¬†6.\n\n\n\n\n\n\nFigura¬†6: I quiz vengono generati alla fine di ogni sezione.\n\n\n\nOgni quiz √® in genere composto da 3-5 domande a risposta multipla e richiede solo 1-2 minuti per essere completato. Queste domande sono progettate per valutare la comprensione del materiale trattato nella sezione precedente, come mostrato in Figura¬†7 (a).\nDopo aver inviato le risposte, SocratiQ fornisce un feedback immediato insieme a spiegazioni dettagliate per ogni domanda, come mostrato in Figura¬†7 (b).\n\n\n\n\n\n\n\n\n\n\n\n(a) Esempio di domande del quiz generate dall‚ÄôIA.\n\n\n\n\n\n\n\n\n\n\n\n(b) Esempio di feedback e spiegazioni generate dall‚ÄôIA per i quiz.\n\n\n\n\n\n\n\nFigura¬†7: SocratiQ utilizza un Large Language Model (LLM) per generare e valutare automaticamente i quiz.\n\n\n\n\n\nEsempio di Flusso di Apprendimento\n\nLeggere una sezione\nSelezionare un testo impegnativo ‚Üí Chiedere spiegazioni a SocratiQ\nFare il quiz sulla sezione\nEsaminare i suggerimenti sui contenuti correlati\nMonitorare i progressi nella dashboard\n\n\n\nOttenere aiuto con i concetti\nQuando si incontrano concetti impegnativi, SocratiQ offre due potenti modi per ottenere aiuto. Innanzitutto, si pu√≤ selezionare qualsiasi testo dal libro di testo e chiedere una spiegazione dettagliata, come dimostrato in Figura¬†8.\n\n\n\n\n\n\nFigura¬†8: Selezione di un testo specifico per chiedere chiarimenti.\n\n\n\nDopo aver selezionato il testo, si possono porre domande al riguardo e SocratiQ fornir√† spiegazioni dettagliate in base a quel contesto, come illustrato in Figura¬†9.\n\n\n\n\n\n\nFigura¬†9: Esempio di come SocratiQ fornisce spiegazioni basate sul testo selezionato.\n\n\n\nLa Figura¬†11 mostra la risposta alla richiesta in Figura¬†9.\nInoltre, si pu√≤ anche fare riferimento a Sezioni, come mostrato in Figura¬†10, Sottosezioni e parole chiave direttamente mentre si conversa con SocratiQ. Usare il simbolo @ per fare riferimento a una sezione, sottosezione o parola chiave. Si pu√≤ anche cliccare sul pulsante + Context proprio sopra l‚Äôinput.\n\n\n\n\n\n\nFigura¬†10: Riferimento a diverse sezioni del libro di testo.\n\n\n\n\n\n\n\n\n\nFigura¬†11: Una sessione di chat interattiva con SocratiQ, che mostra come ottenere chiarimenti sui concetti.\n\n\n\nPer migliorare l‚Äôesperienza di apprendimento, SocratiQ non si limita a rispondere alle domande, ma suggerisce anche contenuti correlati dal libro di testo che potrebbero essere utili per una comprensione pi√π approfondita, come mostrato in Figura¬†12.\n\n\n\n\n\n\nFigura¬†12: SocratiQ suggerisce contenuti correlati in base alle domande per migliorare la comprensione.",
    "crumbs": [
      "SocratiQ AI"
    ]
  },
  {
    "objectID": "contents/ai/socratiq.it.html#sec-socratiq-dashboard",
    "href": "contents/ai/socratiq.it.html#sec-socratiq-dashboard",
    "title": "SocratiQ AI",
    "section": "Monitoraggio dei Progressi",
    "text": "Monitoraggio dei Progressi\n\nDashboard delle Prestazioni\nSocratiQ mantiene un registro completo del percorso di apprendimento. La dashboard dei progressi (Figura¬†13) mostra le statistiche delle prestazioni nei quiz, le serie di apprendimento e i badge dei risultati. Questa dashboard si aggiorna in tempo reale.\n\n\n\n\n\n\nFigura¬†13: La dashboard dei progressi mostra le statistiche di apprendimento e i risultati.\n\n\n\nContinuando a interagire col materiale e a completare i quiz, si otterranno vari badge che riconoscono i progressi, come mostrato in Figura¬†14.\n\n\nüèÖ Badge dei Risultati\nContinuando a progredire nei quiz, si otterranno badge speciali per contrassegnare i risultati! Ecco cosa si pu√≤ guadagnare:\n\n\n\nBadge\nNome\nCome Guadagnare\n\n\n\n\nüéØ\nFirst Steps\nCompletare il primo quiz\n\n\nüî¢\nOn a Streak\nMantenere una sequenza di punteggi perfetti\n\n\nüèÜ\nQuiz Medalist\nCompletare 10 quiz\n\n\nüèÜüèÜ\nQuiz Champion\nCompletare 20 quiz\n\n\nüèÜüèÜüèÜ\nQuiz Legend\nCompletare 30 quiz\n\n\nüèÜüèÜüèÜüèÜ x n\nQuiz AGI Super Human\nCompletare 40 quiz o pi√π\n\n\n\n\n\n\n\n\n\nConsiglio\n\n\n\nContinuare a risolvere quiz per collezionare tutti i badge e migliorare il percorso di apprendimento! I badge attuali appariranno nella dashboard delle statistiche del quiz.\n\n\n\n\n\n\n\n\nFigura¬†14: Esempi di badge di risultati che si possono guadagnare tramite un impegno costante.\n\n\n\nSe si desidera un registro dei propri progressi, si pu√≤ generare un report PDF. Mostrer√† i progressi, le prestazioni medie e tutte le domande a cui si √® risposto. Il PDF viene generato con un hash univoco e pu√≤ essere convalidato in modo univoco.\n\n\n\n\n\n\nFigura¬†15: Si pu√≤ cliccare sul pulsante ‚ÄúDownload Report‚Äù per visualizzare il report. Si pu√≤ verificare che il PDF sia stato creato da SocratiQ cliccando sul pulsante di verifica e caricando il PDF generato.",
    "crumbs": [
      "SocratiQ AI"
    ]
  },
  {
    "objectID": "contents/ai/socratiq.it.html#archiviazione-dati",
    "href": "contents/ai/socratiq.it.html#archiviazione-dati",
    "title": "SocratiQ AI",
    "section": "Archiviazione Dati",
    "text": "Archiviazione Dati\n\n\n\n\n\n\nImportante\n\n\n\nNota importante: Tutti i dati sui progressi sono archiviati localmente nel browser. La cancellazione della cronologia o della cache del browser canceller√† l‚Äôintera cronologia di apprendimento, inclusi punteggi dei quiz, serie e badge guadagnati.\n\n\nSi possono anche eliminare tutte le conversazioni salvate cliccando sul pulsante ‚ÄúNew Chat‚Äù nella barra di navigazione.\n\n\n\n\n\n\nFigura¬†16: Caricare o elimina le chat precedenti o avviare una nuova chat.",
    "crumbs": [
      "SocratiQ AI"
    ]
  },
  {
    "objectID": "contents/ai/socratiq.it.html#requisiti-tecnici",
    "href": "contents/ai/socratiq.it.html#requisiti-tecnici",
    "title": "SocratiQ AI",
    "section": "Requisiti Tecnici",
    "text": "Requisiti Tecnici\nPer usare SocratiQ in modo efficace, c‚Äô√® bisogno di:\n\nBrowser Chrome o Safari\nJavaScript abilitato\nConnessione Internet stabile",
    "crumbs": [
      "SocratiQ AI"
    ]
  },
  {
    "objectID": "contents/ai/socratiq.it.html#problemi-comuni-e-risoluzione-dei-problemi",
    "href": "contents/ai/socratiq.it.html#problemi-comuni-e-risoluzione-dei-problemi",
    "title": "SocratiQ AI",
    "section": "Problemi Comuni e Risoluzione dei Problemi",
    "text": "Problemi Comuni e Risoluzione dei Problemi\n\nSe SocratiQ non risponde: Aggiornare la pagina\nSe i quiz non si caricano: Controlla la connessione Internet\nSe i progressi non vengono salvati: Assicurarsi che i cookie siano abilitati\n\nPer problemi persistenti, contattare l‚Äôindirizzo vj[@]eecs.harvard.edu.",
    "crumbs": [
      "SocratiQ AI"
    ]
  },
  {
    "objectID": "contents/ai/socratiq.it.html#fornire-feedback",
    "href": "contents/ai/socratiq.it.html#fornire-feedback",
    "title": "SocratiQ AI",
    "section": "Fornire Feedback",
    "text": "Fornire Feedback\nI feedback ci aiutano a migliorare SocratiQ. Si possono segnalare problemi tecnici, suggerire miglioramenti alle domande dei quiz o condividere idee sulle risposte dell‚ÄôIA utilizzando i pulsanti di feedback presenti nell‚Äôinterfaccia.\nSi pu√≤ inviare un GitHub issue, oppure se si preferisce lasciare un feedback tramite Google Form, lo si pu√≤ fare tramite questo link:\nInvia un Feedback\nNota: SocratiQ √® progettato per aiutare a imparare in modo efficace. Partecipando costantemente ai quiz, ponendo domande quando necessario e monitorando i propri progressi, si otterr√† il massimo da questo assistente di apprendimento AI.",
    "crumbs": [
      "SocratiQ AI"
    ]
  },
  {
    "objectID": "contents/core/introduction/introduction.it.html",
    "href": "contents/core/introduction/introduction.it.html",
    "title": "1¬† Introduzione",
    "section": "",
    "text": "1.1 Perch√© i Sistemi di Machine Learning Sono Importanti\nL‚Äôintelligenza artificiale √® ovunque. Si pensi alla routine mattutina: Ci si sveglia con una sveglia intelligente basata sull‚ÄôIA che ha appreso i propri schemi di sonno. Il telefono suggerisce il percorso per andare al lavoro, avendo appreso dai pattern del traffico. Durante il tragitto, l‚Äôapp musicale crea automaticamente una playlist che si pensa piacer√†. Al lavoro, il client di posta elettronica filtra lo spam e d√† priorit√† ai messaggi importanti. Durante il giorno, lo smartwatch monitora l‚Äôattivit√†, suggerendo quando muoversi o fare esercizio. La sera, il servizio di streaming consiglia programmi che potrebbero piacere, mentre i dispositivi smart home regolano l‚Äôilluminazione e la temperatura in base alle preferenze apprese.\nMa queste comodit√† quotidiane sono solo l‚Äôinizio. L‚ÄôIA sta trasformando il mondo in modi straordinari. Oggi, i sistemi di IA rilevano tumori in fase iniziale con una precisione senza precedenti, prevedono e tracciano eventi meteorologici estremi per salvare vite e accelerano la scoperta di farmaci simulando milioni di interazioni molecolari. I veicoli autonomi percorrono strade cittadine complesse elaborando dati di sensori in tempo reale da decine di fonti. I modelli linguistici si impegnano in conversazioni sofisticate, traducono tra centinaia di lingue e aiutano gli scienziati ad analizzare vasti database di ricerca. Nei laboratori scientifici, i sistemi di IA stanno facendo scoperte rivoluzionarie, dalla previsione di strutture proteiche che sbloccano nuovi trattamenti medici all‚Äôidentificazione di materiali promettenti per celle solari e batterie di nuova generazione. Anche nei campi creativi, l‚ÄôIA collabora con artisti e musicisti per esplorare nuove forme di espressione, spingendo i confini della creativit√† umana.\nQuesta non √® fantascienza, √® la realt√† di come l‚Äôintelligenza artificiale, in particolare i sistemi di apprendimento automatico, si sia intrecciata nel tessuto della nostra vita quotidiana. All‚Äôinizio degli anni ‚Äô90, Mark Weiser, un pioniere dell‚Äôinformatica, ha introdotto il mondo a un concetto rivoluzionario che avrebbe cambiato per sempre il modo in cui interagiamo con la tecnologia. Questa visione √® stata riassunta in modo sintetico nel suo articolo fondamentale, ‚ÄúThe Computer for the 21st Century‚Äù (Figura¬†1.1). Weiser immaginava un futuro in cui l‚Äôinformatica sarebbe stata perfettamente integrata nei nostri ambienti, diventando una parte invisibile e integrante della vita quotidiana.\nHa definito questo concetto ‚Äúubiquitous computing‚Äù, promettendo un mondo in cui la tecnologia ci avrebbe servito senza richiedere la nostra costante attenzione o interazione. Oggi ci ritroviamo a vivere nel futuro immaginato da Weiser, in gran parte reso possibile dai sistemi di apprendimento automatico. La vera essenza della sua visione, ovvero creare un ambiente intelligente in grado di anticipare le nostre esigenze e agire per nostro conto, √® diventata realt√† attraverso lo sviluppo e l‚Äôimplementazione di sistemi di ML che abbracciano interi ecosistemi, dai potenti data center cloud ai dispositivi edge fino ai pi√π piccoli sensori IoT.\nEppure la maggior parte di noi raramente pensa ai sistemi complessi che rendono possibile tutto questo. Dietro ciascuna di queste interazioni apparentemente semplici si nasconde una sofisticata infrastruttura di dati, algoritmi e risorse informatiche che lavorano insieme. Comprendere come funzionano questi sistemi, le loro capacit√†, limitazioni e requisiti, √® diventato sempre pi√π critico man mano che si integrano sempre di pi√π nel nostro mondo.\nPer apprezzare l‚Äôentit√† di questa trasformazione e la complessit√† dei moderni sistemi di machine learning, dobbiamo capire come siamo arrivati fin qui. Il viaggio dall‚Äôintelligenza artificiale primitiva agli odierni sistemi ML onnipresenti √® una storia non solo di evoluzione tecnologica, ma anche di prospettive mutevoli su ci√≤ che √® possibile e ci√≤ che √® necessario per rendere l‚ÄôIA pratica e affidabile.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduzione</span>"
    ]
  },
  {
    "objectID": "contents/core/introduction/introduction.it.html#perch√©-i-sistemi-di-machine-learning-sono-importanti",
    "href": "contents/core/introduction/introduction.it.html#perch√©-i-sistemi-di-machine-learning-sono-importanti",
    "title": "1¬† Introduzione",
    "section": "",
    "text": "Figura¬†1.1: Ubiquitous computing as envisioned di Mark Weiser.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduzione</span>"
    ]
  },
  {
    "objectID": "contents/core/introduction/introduction.it.html#levoluzione-dellia",
    "href": "contents/core/introduction/introduction.it.html#levoluzione-dellia",
    "title": "1¬† Introduzione",
    "section": "1.2 L‚Äôevoluzione dell‚ÄôIA",
    "text": "1.2 L‚Äôevoluzione dell‚ÄôIA\nL‚Äôevoluzione dell‚ÄôIA, rappresentata nella cronologia mostrata in Figura¬†1.2, evidenzia traguardi chiave come lo sviluppo del perceptron1 nel 1957 da parte di Frank Rosenblatt, un elemento fondamentale per le moderne reti neurali. Si immagini di entrare in un laboratorio informatico nel 1965. Si troveranno mainframe delle dimensioni di una stanza che eseguono programmi in grado di dimostrare teoremi matematici di base o di giocare a semplici giochi come il tris. Questi primi sistemi di intelligenza artificiale, pur essendo rivoluzionari per l‚Äôepoca, erano ben lontani dagli odierni sistemi di apprendimento automatico in grado di rilevare il cancro nelle immagini mediche o di comprendere il linguaggio umano. La cronologia mostra la progressione dalle prime innovazioni come il chatbot ELIZA nel 1966, a importanti innovazioni come Deep Blue di IBM che ha sconfitto il campione di scacchi Garry Kasparov nel 1997. I progressi pi√π recenti includono l‚Äôintroduzione di GPT-3 di OpenAI nel 2020 e GPT-4 nel 2023, dimostrando la drammatica evoluzione e la crescente complessit√† dei sistemi di IA nel corso dei decenni.\n1¬†La prima rete neurale artificiale, un modello semplice che potrebbe imparare a classificare schemi visivi, simile a un singolo neurone che prende una decisione s√¨/no in base ai suoi input.\n\n\n\n\n\nFigura¬†1.2: Pietre miliari nell‚ÄôIA dal 1950 al 2020. Fonte: IEEE Spectrum\n\n\n\nEsploriamo come siamo arrivati fin qui.\n\n1.2.1 IA Simbolica (1956-1974)\nLa storia del machine learning inizia alla storica conferenza di Dartmouth del 1956, dove pionieri come John McCarthy, Marvin Minsky e Claude Shannon coniarono per primi il termine ‚Äúintelligenza artificiale‚Äù. Il loro approccio si basava su un‚Äôidea convincente: l‚Äôintelligenza poteva essere ridotta alla manipolazione dei simboli. Si consideri il sistema STUDENT di Daniel Bobrow del 1964, uno dei primi programmi di IA in grado di risolvere problemi di algebra:\n\n\n\n\n\n\nEsempio: STUDENT (1964)\n\n\n\nProblema: \"Se il numero di clienti che Tom ottiene √® il doppio del \nquadrato del 20% del numero di pubblicit√† che gestisce e \nil numero di pubblicit√† √® 45, qual √® il numero di\nclienti che Tom ottiene?\"\n \nSTUDENT dovrebbe:\n\n1. Analizzare il testo\n2. Convertirlo in equazioni algebriche\n3. Risolvere l'equazione: n = 2(0.2 √ó 45)¬≤\n4. Fornire la risposta: 162 clienti\n\n\nLe prime IA come STUDENT soffrivano di una limitazione fondamentale: potevano gestire solo input che corrispondevano esattamente ai loro schemi e regole pre-programmati. Si immagini un traduttore linguistico che funziona solo quando le frasi seguono una struttura grammaticale perfetta: anche piccole variazioni come cambiare l‚Äôordine delle parole, usare sinonimi o schemi di linguaggio naturali causerebbero il fallimento di STUDENT. Questa ‚Äúfragilit√†‚Äù significava che, mentre queste soluzioni potevano apparire intelligenti quando gestivano casi molto specifici per cui erano state progettate, si sarebbero completamente guastate quando si trovavano di fronte anche a piccole variazioni o complessit√† del mondo reale. Questa limitazione non era solo un inconveniente tecnico, ma rivelava un problema pi√π profondo con gli approcci basati su regole all‚ÄôIA: non potevano realmente comprendere o generalizzare dalla loro programmazione, potevano solo abbinare e manipolare i modelli esattamente come specificato.\n\n\n1.2.2 Sistemi Esperti (anni ‚Äô70-‚Äô80)\nVerso la met√† degli anni ‚Äô70, i ricercatori si resero conto che l‚ÄôIA generale era troppo ambiziosa. Si concentraronoinvece, sull‚Äôacquisizione di conoscenze esperte umane in domini specifici. MYCIN, sviluppato a Stanford, √® stato uno dei primi sistemi esperti su larga scala progettati per diagnosticare le infezioni del sangue:\n\n\n\n\n\n\nEsempio: MYCIN (1976)\n\n\n\nEsempio di regola da MYCIN:\nIF\n    L'infezione √® una batteriemia primaria\n    Il sito della coltura √® uno dei siti sterili\n    Il presunto portale di ingresso √® il tratto gastrointestinale\nTHEN\n    Ci sono prove suggestive (0,7) che l'infezione √® batterioide\n\n\nMentre MYCIN ha rappresentato un importante progresso nell‚ÄôIA medica con le sue 600 regole esperte per la diagnosi delle infezioni del sangue, ha rivelato sfide fondamentali che ancora oggi affliggono l‚Äôapprendimento automatico. Ottenere la conoscenza del dominio da esperti umani e convertirla in regole precise si √® rivelato incredibilmente dispendioso in termini di tempo e difficile: i medici spesso non riuscivano a spiegare esattamente come prendevano le decisioni. MYCIN ha lottato con informazioni incerte o incomplete, a differenza dei medici umani che potevano fare ipotesi istruite. Forse la cosa pi√π importante √® che la manutenzione e l‚Äôaggiornamento della base di regole sono diventati esponenzialmente pi√π complessi con la crescita di MYCIN: l‚Äôaggiunta di nuove regole spesso entrava in conflitto con quelle esistenti e la conoscenza medica stessa continuava a evolversi. Queste stesse sfide di acquisizione della conoscenza, gestione dell‚Äôincertezza e manutenzione rimangono preoccupazioni centrali nell‚Äôapprendimento automatico moderno, anche se ora utilizziamo approcci tecnici diversi per affrontarle.\n\n\n1.2.3 Apprendimento Statistico: Un Cambio di Paradigma (anni ‚Äô90)\nGli anni ‚Äô90 hanno segnato una trasformazione radicale nell‚Äôintelligenza artificiale, poich√© il campo si √® spostato dalle regole codificate a mano verso approcci di apprendimento statistico. Questa non √® stata una scelta semplice: √® stata guidata da tre fattori convergenti che hanno reso i metodi statistici possibili e potenti. La rivoluzione digitale ha significato che enormi quantit√† di dati erano improvvisamente disponibili per addestrare gli algoritmi. La legge di Moore2 ha fornito la potenza di calcolo necessaria per elaborare questi dati in modo efficace. E i ricercatori hanno sviluppato nuovi algoritmi come le ‚ÄúSupport Vector Machines‚Äù [macchine a vettori di supporto] e reti neurali migliorate che potevano effettivamente apprendere modelli da questi dati anzich√© seguire regole pre-programmate. Questa combinazione ha cambiato radicalmente il modo in cui abbiamo costruito l‚ÄôIA: invece di cercare di codificare direttamente la conoscenza umana, ora potevamo lasciare che le macchine scoprissero automaticamente i pattern da esempi, portando a un‚ÄôIA pi√π solida e adattabile.\n2¬†L‚Äôosservazione fatta dal co-fondatore di Intel Gordon Moore nel 1965 secondo cui il numero di transistor su un microchip raddoppia circa ogni due anni, mentre il costo si dimezza. Questa crescita esponenziale della potenza di calcolo √® stata un fattore chiave dei progressi nell‚Äôapprendimento automatico, sebbene il ritmo abbia iniziato a rallentare negli ultimi anni.Si consideri come si √® evoluto il filtraggio dello spam via e-mail:\n\n\n\n\n\n\nEsempio: Sistemi di Rilevamento Precoce dello Spam\n\n\n\nBasati su regole (anni '80):\nIF contains(\"viagra\") OR contains(\"winner\") THEN spam\n\nStatistici (anni '90):\nP(spam|word) = (frequency in spam emails) / (total frequency)\n\nCombinati usando Naive Bayes:\nP(spam|email) ‚àù P(spam) √ó ‚àè P(word|spam)\n\n\nIl passaggio agli approcci statistici ha cambiato radicalmente il nostro modo di pensare alla creazione di IA introducendo tre concetti fondamentali che rimangono importanti ancora oggi. In primo luogo, la qualit√† e la quantit√† dei dati di addestramento sono diventati importanti quanto gli algoritmi stessi: l‚ÄôIA poteva apprendere solo i modelli presenti nei suoi esempi di addestramento. In secondo luogo, avevamo bisogno di metodi rigorosi per valutare quanto bene l‚ÄôIA funzionasse effettivamente, portando a metriche che potessero misurare il successo e confrontare diversi approcci. In terzo luogo, abbiamo scoperto una tensione intrinseca tra precisione (avere ragione quando facciamo una previsione) e richiamo (catturare tutti i casi che dovremmo trovare), costringendo i progettisti a fare compromessi espliciti in base alle esigenze della loro applicazione. Ad esempio, un filtro antispam potrebbe tollerare un po‚Äô di spam per evitare di bloccare e-mail importanti, mentre la diagnosi medica potrebbe dover catturare ogni potenziale caso anche se ci√≤ significa pi√π falsi allarmi.\nTabella¬†1.1 racchiude il percorso evolutivo degli approcci di IA di cui abbiamo discusso finora, evidenziando i punti di forza e le capacit√† chiave emersi con ogni nuovo paradigma. Spostandoci da sinistra a destra nella tabella, possiamo osservare diverse tendenze importanti. Parleremo di apprendimento ‚Äúsuperficiale‚Äù e ‚Äúprofondo‚Äù in seguito, ma √® utile comprendere i compromessi tra gli approcci trattati finora.\n\n\n\nTabella¬†1.1: Evoluzione dell‚ÄôIA - Aspetti Chiave Positivi\n\n\n\n\n\n\n\n\n\n\n\n\nAspetto\nIA simbolica\nSistemi esperti\nApprendimento statistico\nApprendimento superficiale/profondo\n\n\n\n\nPunto di forza\nRagionamento logico\nCompetenza di dominio\nVersatilit√†\nRiconoscimento di pattern\n\n\nMiglior caso d‚Äôuso\nProblemi ben definiti e basati su regole\nProblemi di dominio specifici\nVari problemi di dati strutturati\nProblemi di dati complessi e non strutturati\n\n\nGestione dei dati\nDati minimi necessari\nBasato sulla conoscenza del dominio\nDati moderati richiesti\nElaborazione dati su larga scala\n\n\nAdattabilit√†\nRegole fisse\nAdattabilit√† specifica del dominio\nAdattabile a vari domini\nAltamente adattabile a diverse attivit√†\n\n\nComplessit√† del problema\nSemplice, basato sulla logica\nComplicato, specifico del dominio\nComplesso, strutturato\nAltamente complesso, non strutturato\n\n\n\n\n\n\nLa tabella funge da ponte tra i primi approcci di cui abbiamo parlato e gli sviluppi pi√π recenti nell‚Äôapprendimento superficiale e profondo che esploreremo in seguito. Pone le basi per comprendere perch√© determinati approcci hanno acquisito importanza in epoche diverse e come ogni nuovo paradigma si √® basato e ha affrontato i limiti dei suoi predecessori. Inoltre, illustra come i punti di forza degli approcci precedenti continuino a influenzare e migliorare le moderne tecniche di IA, in particolare nell‚Äôera dei modelli di fondazione.\n\n\n1.2.4 Apprendimento Superficiale (anni 2000)\nGli anni 2000 hanno segnato un periodo affascinante nella storia del machine learning che ora chiamiamo l‚Äôera dello ‚Äú‚Äúshallow learning‚Äù [apprendimento superficiale]. Per capire perch√© √® ‚Äúsuperficiale‚Äù, si immagini di costruire una casa: il ‚Äúdeep learning‚Äù [apprendimento profondo] (che √® arrivato dopo) √® come avere pi√π squadre di costruzione che lavorano a diversi livelli contemporaneamente, ciascuna squadra impara dal lavoro delle squadre sottostanti. Al contrario, l‚Äôapprendimento superficiale in genere aveva solo uno o due livelli di elaborazione, come avere solo una squadra di fondazione e una squadra di intelaiatura.\nDurante questo periodo, diversi potenti algoritmi hanno dominato il panorama dell‚Äôapprendimento automatico. Ognuno ha portato punti di forza unici a problemi diversi: I ‚Äúdecision trees‚Äù [alberi decisionali] hanno fornito risultati interpretabili prendendo decisioni molto simili a un diagramma di flusso. I ‚ÄúK-nearest neighbors‚Äù hanno fatto previsioni trovando esempi simili nei dati passati, come chiedere consiglio ai vicini pi√π esperti. La regressione lineare e logistica hanno offerto modelli semplici e interpretabili che hanno funzionato bene per molti problemi del mondo reale. Le ‚ÄúSupport Vector Machine (SVM)‚Äù eccellevano nel trovare confini complessi tra categorie usando il ‚Äútrucco del kernel‚Äù: Si immagini di poter districare una ciotola di spaghetti in linee rette sollevandola in una dimensione superiore. Questi algoritmi hanno costituito la base della macchina pratica.\nSi consideri una tipica soluzione di visione artificiale del 2005:\n\n\n\n\n\n\nEsempio: Pipeline di Visione Artificiale Tradizionale\n\n\n\n1. Estrazione Manuale delle Feature\n   - SIFT (Scale-Invariant Feature Transform)\n   - HOG (Histogram of Oriented Gradients)\n   - Filtri di Gabor\n2. Selezione/Ingegnerizzazione delle Feature\n3. Modello di Apprendimento \"Superficiale\" (ad esempio, SVM)\n4. Post-elaborazione\n\n\nCi√≤ che ha reso questa era unica √® stato il suo approccio ibrido: caratteristiche ingegnerizzate dall‚Äôuomo combinate con l‚Äôapprendimento statistico. Avevano solide basi matematiche (i ricercatori potevano dimostrare perch√© funzionavano). Hanno funzionato bene anche con dati limitati. Erano efficienti dal punto di vista computazionale. Hanno prodotto risultati affidabili e riproducibili.\nPrendiamo l‚Äôesempio del rilevamento dei volti, in cui l‚Äôalgoritmo Viola-Jones (2001) ha ottenuto prestazioni in tempo reale utilizzando semplici ‚Äúfeature‚Äù [caratteristiche] rettangolari e una cascata di classificatori. Questo algoritmo ha alimentato il rilevamento dei volti delle fotocamere digitali per quasi un decennio.\n\n\n1.2.5 Deep Learning (2012-Oggi)\nMentre le ‚ÄúSupport Vector Machine‚Äù eccellevano nel trovare confini complessi tra categorie usando trasformazioni matematiche, il deep learning ha adottato un approccio radicalmente diverso ispirato all‚Äôarchitettura del cervello umano. Il deep learning √® costruito da ‚Äúlayer‚Äù [strati] di neuroni artificiali, dove ogni layer impara a trasformare i suoi dati di input in rappresentazioni sempre pi√π astratte. Si immagini di elaborare un‚Äôimmagine di un gatto: il primo layer potrebbe imparare a rilevare semplici bordi e contrasti, il layer successivo li combina in forme e texture di base, un altro layer potrebbe riconoscere baffi e orecchie a punta e quelli finali assemblano queste caratteristiche nel concetto di ‚Äúgatto‚Äù. A differenza dei metodi di apprendimento superficiali che richiedevano agli esseri umani di progettare attentamente le feature, le reti di deep learning possono scoprire automaticamente feature utili direttamente dai dati grezzi. Questa capacit√† di apprendere rappresentazioni gerarchiche, da semplici a complesse, da concrete ad astratte, √® ci√≤ che rende il deep learning ‚Äúdeep‚Äù [profondo] e si √® rivelato un approccio straordinariamente potente per la gestione di dati complessi del mondo reale come immagini, discorsi e testo.\nNel 2012, una rete neurale profonda chiamata AlexNet, mostrata in Figura¬†1.3, ha raggiunto una svolta nella competizione ImageNet che avrebbe trasformato il campo del machine learning. La sfida era formidabile: classificare correttamente 1,2 milioni di immagini ad alta risoluzione in 1.000 categorie diverse. Mentre gli approcci precedenti hanno lottato con tassi di errore superiori al 25%, AlexNet ha raggiunto un tasso di errore del 15,3%, superando notevolmente tutti i metodi esistenti.\n\n\n\n\n\n\nFigura¬†1.3: Architettura della rete neurale profonda per Alexnet. Fonte: Krizhevsky, Sutskever, e Hinton (2017)\n\n\nKrizhevsky, Alex, Ilya Sutskever, e Geoffrey E. Hinton. 2017. ¬´ImageNet classification with deep convolutional neural networks¬ª. Communications of the ACM 60 (6): 84‚Äì90. https://doi.org/10.1145/3065386.\n\n\nIl successo di AlexNet non √® stato solo un risultato tecnico, √® stato un momento spartiacque che ha dimostrato la fattibilit√† pratica del deep learning. Ha dimostrato che con dati sufficienti, potenza di calcolo e innovazioni architettoniche, le reti neurali potevano superare le funzionalit√† progettate a mano e i metodi di apprendimento superficiale che avevano dominato il campo per decenni. Questo singolo risultato ha innescato un‚Äôesplosione di ricerca e applicazioni nel deep learning che continua ancora oggi.\nDa questa base, il deep learning √® entrato in un‚Äôera di portata senza precedenti. Verso la fine del 2010, aziende come Google, Facebook e OpenAI stavano addestrando reti neurali migliaia di volte pi√π grandi di AlexNet3. Questi modelli massicci, spesso chiamati ‚Äúfoundation models‚Äù [modelli di base], hanno portato il deep learning a nuovi livelli. GPT-3, rilasciato nel 2020, conteneva 175 miliardi di parametri4‚Äîsi immagini uno studente che potesse leggere l‚Äôintera Wikipedia pi√π volte e apprendere modelli da ogni articolo. Questi modelli hanno mostrato capacit√† straordinarie: scrivere testi simili a quelli umani, impegnarsi in conversazioni, generare immagini da descrizioni e persino scrivere codice per computer. L‚Äôintuizione chiave era semplice ma potente: man mano che ingrandivamo le reti neurali e fornivamo loro pi√π dati, queste diventavano in grado di risolvere attivit√† sempre pi√π complesse. Tuttavia, questa scala ha portato sfide di sistema senza precedenti: come si addestrano in modo efficiente modelli che richiedono migliaia di GPU che lavorano in parallelo? Come si archiviano e si forniscono modelli di centinaia di gigabyte di dimensioni? Come si gestiscono gli enormi set di dati necessari per l‚Äôaddestramento?\n3¬†Una rivoluzionaria rete neurale profonda del 2012 che ha vinto la ImageNet competition con un ampio margine e ha contribuito a innescare la rivoluzione del deep learning.4¬†Simile a come le connessioni neurali del cervello diventano pi√π forti man mano che si apprende una nuova abilit√†, avere pi√π parametri significa generalmente che il modello pu√≤ apprendere schemi pi√π complessi.5¬†Un tipo di rete neurale appositamente progettata per l‚Äôelaborazione di immagini, ispirata al funzionamento del sistema visivo umano. La parte ‚Äúconvoluzionale‚Äù si riferisce al modo in cui analizza le immagini in piccoli blocchi, in modo simile a come i nostri occhi si concentrano su diverse parti di una scena.La rivoluzione del deep learning del 2012 non √® emersa dal nulla, ma √® stata costruita sulla ricerca sulle reti neurali risalente agli anni ‚Äô50. La storia inizia con il Perceptron di Frank Rosenblatt nel 1957, che ha catturato l‚Äôimmaginazione dei ricercatori mostrando come un semplice neurone artificiale potesse imparare a classificare gli schemi. Sebbene potesse gestire solo problemi linearmente separabili, una limitazione drammaticamente evidenziata dal libro del 1969 di Minsky e Papert ‚ÄúPerceptrons‚Äù, ha introdotto il concetto fondamentale di reti neurali addestrabili. Gli anni ‚Äô80 portarono altre importanti scoperte: Rumelhart, Hinton e Williams introdussero la backpropagation nel 1986, fornendo un modo sistematico per addestrare reti multi-layer, mentre Yann LeCun ne dimostr√≤ l‚Äôapplicazione pratica nel riconoscimento di cifre scritte a mano utilizzando ‚Äúconvolutional neural networks (CNN)‚Äù [reti neurali convoluzionali]5.\n\n\n\n\n\n\nVideo¬†1.1: Demo di Rete Convoluzionale del 1989\n\n\n\n\n\n\nTuttavia, queste reti sono rimaste in gran parte inattive negli anni ‚Äô90 e 2000, non perch√© le idee fossero sbagliate, ma perch√© erano avanti coi tempi: il campo mancava di tre ingredienti importanti: dati sufficienti per addestrare reti complesse, potenza di calcolo sufficiente per elaborare questi dati e le innovazioni tecniche necessarie per addestrare efficacemente reti molto profonde.\nIl campo ha dovuto attendere la convergenza dei big data, hardware di elaborazione migliore e innovazioni algoritmiche prima che il potenziale del deep learning potesse essere sbloccato. Questo lungo periodo di gestazione aiuta a spiegare perch√© il momento ImageNet del 2012 √® stato meno una rivoluzione improvvisa e pi√π il culmine di decenni di ricerca accumulata che ha finalmente trovato il suo momento. Come esploreremo nelle sezioni seguenti, questa evoluzione ha portato a due sviluppi significativi nel campo. In primo luogo, ha dato origine alla definizione del campo dell‚Äôingegneria dei sistemi di apprendimento automatico, una disciplina che insegna come colmare il divario tra progressi teorici e implementazione pratica. In secondo luogo, ha reso necessaria una definizione pi√π completa dei sistemi di apprendimento automatico, che comprenda non solo gli algoritmi, ma anche i dati e l‚Äôinfrastruttura informatica. Le attuali sfide di scala riecheggiano molte delle stesse domande fondamentali su metodi di calcolo, dati e apprendimento con cui i ricercatori si sono confrontati sin dall‚Äôinizio del campo, ma ora all‚Äôinterno di un quadro pi√π complesso e interconnesso.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduzione</span>"
    ]
  },
  {
    "objectID": "contents/core/introduction/introduction.it.html#lascesa-dellingegneria-dei-sistemi-di-ml",
    "href": "contents/core/introduction/introduction.it.html#lascesa-dellingegneria-dei-sistemi-di-ml",
    "title": "1¬† Introduzione",
    "section": "1.3 L‚ÄôAscesa dell‚ÄôIngegneria dei Sistemi di ML",
    "text": "1.3 L‚ÄôAscesa dell‚ÄôIngegneria dei Sistemi di ML\nLa storia che abbiamo tracciato, dai primi giorni del Perceptron alla rivoluzione del deep learning, √® stata in gran parte una storia di innovazioni algoritmiche. Ogni epoca ha portato nuove intuizioni matematiche e approcci di modellazione che hanno ampliato i confini di ci√≤ che l‚ÄôIA poteva raggiungere. Ma qualcosa di importante √® cambiato nell‚Äôultimo decennio: il successo dei sistemi di IA √® diventato sempre pi√π dipendente non solo dalle innovazioni algoritmiche, ma anche da un‚Äôingegneria sofisticata.\nQuesto cambiamento rispecchia l‚Äôevoluzione dell‚Äôinformatica e dell‚Äôingegneria alla fine degli anni ‚Äô60 e all‚Äôinizio degli anni ‚Äô70. Durante quel periodo, man mano che i sistemi informatici diventavano pi√π complessi, emerse una nuova disciplina: la ‚ÄúComputer Engineering‚Äù [ingegneria informatica]. Questo campo colm√≤ il divario tra l‚Äôesperienza hardware dell‚ÄôIngegneria Elettrica e l‚Äôattenzione dell‚ÄôInformatica su algoritmi e software. L‚ÄôIngegneria Informatica nacque perch√© le sfide della progettazione e della costruzione di sistemi informatici complessi richiedevano un approccio integrato che nessuna delle due discipline poteva affrontare completamente da sola.\nOggi, stiamo assistendo a una transizione simile nel campo dell‚ÄôIA. Mentre l‚ÄôInformatica continua a spingere i confini degli algoritmi di ML e l‚ÄôIngegneria Elettrica fa progredire l‚Äôhardware di IA specializzato, nessuna delle due discipline affronta completamente i principi di ingegneria necessari per distribuire, ottimizzare e sostenere i sistemi di ML su larga scala. Questa lacuna evidenzia la necessit√† di una nuova disciplina: la ‚ÄúMachine Learning Systems Engineering‚Äù [ingegneria dei sistemi di apprendimento automatico].\nNon esiste una definizione esplicita di cosa sia questo campo oggi, ma pu√≤ essere ampiamente definito come tale:\n\n\n\n\n\n\nDefinizione di Machine Learning Systems Engineering\n\n\n\nLa ‚ÄúMachine Learning Systems Engineering (MLSysEng)‚Äù √® la disciplina di progettazione, implementazione e gestione di sistemi di intelligenza artificiale su scale di elaborazione, dai dispositivi embedded con risorse limitate ai computer su scala industriale. Questo campo integra i principi delle discipline ingegneristiche che spaziano dall‚Äôhardware al software per creare sistemi affidabili, efficienti e ottimizzati per il loro contesto di distribuzione. Comprende il ciclo di vita completo delle applicazioni AI: dall‚Äôingegneria dei requisiti e dalla raccolta dati allo sviluppo di modelli, all‚Äôintegrazione di sistemi, alla distribuzione, al monitoraggio e alla manutenzione. Il campo enfatizza i principi ingegneristici di progettazione sistematica, vincoli di risorse, requisiti di prestazioni e affidabilit√† operativa.\n\n\nConsideriamo l‚Äôesplorazione spaziale. Mentre gli astronauti si avventurano in nuove frontiere ed esplorano le vaste incognite dell‚Äôuniverso, le loro scoperte sono possibili solo grazie ai complessi sistemi di ingegneria che li supportano: i razzi che li sollevano nello spazio, i sistemi di supporto vitale che li mantengono in vita e le reti di comunicazione che li mantengono connessi alla Terra. Allo stesso modo, mentre i ricercatori di IA spingono i confini di ci√≤ che √® possibile con gli algoritmi di apprendimento, le loro scoperte diventano realt√† pratica solo attraverso un‚Äôattenta ingegneria dei sistemi. I moderni sistemi di IA necessitano di un‚Äôinfrastruttura solida per raccogliere e gestire i dati, di potenti sistemi di elaborazione per addestrare i modelli e di piattaforme di distribuzione affidabili per servire milioni di utenti.\nQuesta emergenza dell‚Äôingegneria dei sistemi di apprendimento automatico come disciplina importante riflette una realt√† pi√π ampia: trasformare gli algoritmi di IA in sistemi del mondo reale richiede di colmare il divario tra possibilit√† teoriche e implementazione pratica. Non basta avere un algoritmo brillante se non si riesce a raccogliere ed elaborare in modo efficiente i dati necessari, distribuirne il calcolo su centinaia di macchine, servirlo in modo affidabile a milioni di utenti o monitorarne le prestazioni in produzione.\nComprendere questa interazione tra algoritmi e ingegneria √® diventato fondamentale per i moderni professionisti dell‚ÄôIA. Mentre i ricercatori continuano a spingere i confini di ci√≤ che √® algoritmicamente possibile, gli ingegneri stanno affrontando la complessa sfida di far funzionare questi algoritmi in modo affidabile ed efficiente nel mondo reale. Questo ci porta a una domanda fondamentale: cos‚Äô√® esattamente un sistema di ‚Äúmachine learning‚Äù [apprendimento automatico] e cosa lo rende diverso dai tradizionali sistemi software?",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduzione</span>"
    ]
  },
  {
    "objectID": "contents/core/introduction/introduction.it.html#definizione-di-un-sistema-ml",
    "href": "contents/core/introduction/introduction.it.html#definizione-di-un-sistema-ml",
    "title": "1¬† Introduzione",
    "section": "1.4 Definizione di un sistema ML",
    "text": "1.4 Definizione di un sistema ML\nNon esiste una definizione univoca e universalmente accettata di un sistema di apprendimento automatico. Questa ambiguit√† deriva dal fatto che diversi professionisti, ricercatori e settori spesso fanno riferimento ai sistemi di apprendimento automatico in contesti diversi e con ambiti diversi. Alcuni potrebbero concentrarsi esclusivamente sugli aspetti algoritmici, mentre altri potrebbero includere l‚Äôintera pipeline dalla raccolta dati all‚Äôimplementazione del modello. Questo uso approssimativo del termine riflette la natura multidisciplinare e in rapida evoluzione del campo.\nData questa diversit√† di prospettive, √® importante stabilire una definizione chiara e completa che comprenda tutti questi aspetti. In questo libro, adottiamo un approccio olistico ai sistemi di apprendimento automatico, considerando non solo gli algoritmi ma anche l‚Äôintero ecosistema in cui operano. Pertanto, definiamo un sistema di apprendimento automatico come segue:\n\n\n\n\n\n\nDefinizione di un Sistema di Machine Learning\n\n\n\nUn sistema di ‚Äúmachine learning‚Äù [apprendimento automatico] √® un sistema di elaborazione integrato che comprende tre componenti principali: (1) dati che guidano il comportamento algoritmico, (2) algoritmi di apprendimento che estraggono modelli da questi dati e (3) infrastruttura di elaborazione che consente sia il processo di apprendimento (ad esempio, addestramento) sia l‚Äôapplicazione della conoscenza appresa (ad esempio, inferenza/servizio). Insieme, questi componenti creano un sistema di elaborazione in grado di fare previsioni, generare contenuti o intraprendere azioni in base a modelli appresi.\n\n\nIl nucleo di qualsiasi sistema di apprendimento automatico √® costituito da tre componenti interrelati, come illustrato in Figura¬†1.4: modelli/algoritmi, dati e infrastruttura informatica. Questi componenti formano una dipendenza triangolare in cui ogni elemento plasma fondamentalmente le possibilit√† degli altri. L‚Äôarchitettura del modello detta sia le richieste computazionali per l‚Äôaddestramento e l‚Äôinferenza, sia il volume e la struttura dei dati richiesti per un apprendimento efficace. La scala e la complessit√† dei dati influenzano l‚Äôinfrastruttura necessaria per l‚Äôarchiviazione e l‚Äôelaborazione, determinando contemporaneamente quali architetture del modello sono fattibili. Le capacit√† dell‚Äôinfrastruttura stabiliscono limiti pratici sia sulla scala del modello che sulla capacit√† di elaborazione dei dati, creando un framework all‚Äôinterno del quale devono operare gli altri componenti.\n\n\n\n\n\n\nFigura¬†1.4: I sistemi di apprendimento automatico coinvolgono algoritmi, dati e calcoli, tutti interconnessi tra loro.\n\n\n\nCiascuno di questi componenti ha uno scopo distinto ma interconnesso:\n\nAlgoritmi: Modelli matematici e metodi che apprendono pattern dai dati per fare previsioni o decisioni\nDati: Processi e infrastrutture per la raccolta, l‚Äôarchiviazione, l‚Äôelaborazione, la gestione e la fornitura di dati sia per l‚Äôaddestramento che per l‚Äôinferenza.\nCalcolo: Infrastruttura hardware e software che consente l‚Äôaddestramento, la fornitura e il funzionamento efficienti di modelli su larga scala.\n\nL‚Äôinterdipendenza di questi componenti significa che nessun singolo elemento pu√≤ funzionare in modo isolato. L‚Äôalgoritmo pi√π sofisticato non pu√≤ apprendere senza dati o risorse di elaborazione su cui eseguire. I set di dati pi√π grandi sono inutili senza algoritmi per estrarre modelli o infrastrutture per elaborarli. E l‚Äôinfrastruttura di elaborazione pi√π potente non serve a nulla senza algoritmi da eseguire o dati da elaborare.\nPer illustrare queste relazioni, possiamo fare un paragone con l‚Äôesplorazione spaziale. Gli sviluppatori di algoritmi sono come gli astronauti: esplorano nuove frontiere e fanno scoperte. I team di data science funzionano come specialisti del controllo missione, assicurando il flusso costante di informazioni e risorse critiche necessarie per far funzionare la missione. Gli ingegneri delle infrastrutture informatiche sono come gli ingegneri missilistici: progettano e costruiscono i sistemi che rendono possibile la missione. Proprio come una missione spaziale richiede l‚Äôintegrazione perfetta di astronauti, controllo missione e sistemi missilistici, un sistema di apprendimento automatico richiede l‚Äôattenta orchestrazione di algoritmi, dati e infrastrutture informatiche.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduzione</span>"
    ]
  },
  {
    "objectID": "contents/core/introduction/introduction.it.html#il-ciclo-di-vita-dei-sistemi-ml",
    "href": "contents/core/introduction/introduction.it.html#il-ciclo-di-vita-dei-sistemi-ml",
    "title": "1¬† Introduzione",
    "section": "1.5 Il ciclo di vita dei sistemi ML",
    "text": "1.5 Il ciclo di vita dei sistemi ML\nI sistemi software tradizionali seguono un ciclo di vita prevedibile in cui gli sviluppatori scrivono istruzioni esplicite che i computer devono eseguire. Questi sistemi sono basati su decenni di consolidate pratiche di ingegneria del software. I sistemi di controllo delle versioni mantengono cronologie precise delle modifiche del codice. Le pipeline di integrazione e distribuzione continue automatizzano i processi di test e rilascio. Gli strumenti di analisi statica misurano la qualit√† del codice e identificano potenziali problemi. Questa infrastruttura consente uno sviluppo, un test e una distribuzione affidabili di sistemi software, seguendo principi ben definiti di ingegneria del software.\nI sistemi di apprendimento automatico rappresentano una deviazione fondamentale da questo paradigma tradizionale. Mentre i sistemi tradizionali eseguono una logica di programmazione esplicita, i sistemi di apprendimento automatico derivano il loro comportamento da pattern nei dati. Questo passaggio dal codice ai dati come driver principale del comportamento del sistema introduce nuove complessit√†.\nCome illustrato in Figura¬†1.5, il ciclo di vita ML √® costituito da fasi interconnesse dalla raccolta dati al monitoraggio del modello, con cicli di feedback per il miglioramento continuo quando le prestazioni si degradano o i modelli necessitano di miglioramenti.\n\n\n\n\n\n\nFigura¬†1.5: Il tipico ciclo di vita di un sistema di apprendimento automatico.\n\n\n\nA differenza del codice sorgente, che cambia solo quando gli sviluppatori lo modificano, i dati riflettono la natura dinamica del mondo reale. Le modifiche nelle distribuzioni dei dati possono alterare silenziosamente il comportamento del sistema. Gli strumenti tradizionali di ingegneria del software, progettati per sistemi basati su codice deterministico, si dimostrano insufficienti per la gestione di questi sistemi dipendenti dai dati. Ad esempio, i sistemi di controllo delle versioni che eccellono nel tracciare modifiche discrete del codice hanno difficolt√† a gestire grandi set di dati in evoluzione. I framework di test progettati per output deterministici devono essere adattati per previsioni probabilistiche. Questa natura dipendente dai dati crea un ciclo di vita pi√π dinamico, che richiede un monitoraggio e un adattamento continui per mantenere la pertinenza del sistema man mano che i modelli di dati del mondo reale si evolvono.\nPer comprendere il ciclo di vita del sistema di apprendimento automatico √® necessario esaminarne le fasi distinte. Ogni fase presenta requisiti unici sia dal punto di vista dell‚Äôapprendimento che da quello dell‚Äôinfrastruttura. Questa duplice considerazione, delle esigenze di apprendimento e del supporto dei sistemi, √® estremamente importante per la creazione di sistemi di machine learning efficaci.\nTuttavia, le varie fasi del ciclo di vita ML in produzione non sono isolate; sono, infatti, profondamente interconnesse. Questa interconnessione pu√≤ creare circoli virtuosi o viziosi. In un circolo virtuoso, dati di alta qualit√† consentono un apprendimento efficace, infrastrutture robuste supportano un‚Äôelaborazione efficiente e sistemi ben progettati facilitano la raccolta di dati ancora migliori. Tuttavia, in un circolo vizioso, una scarsa qualit√† dei dati mina l‚Äôapprendimento, infrastrutture inadeguate ostacolano l‚Äôelaborazione e le limitazioni del sistema impediscono il miglioramento della raccolta dati: ogni problema aggrava gli altri.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduzione</span>"
    ]
  },
  {
    "objectID": "contents/core/introduction/introduction.it.html#lo-spettro-dei-sistemi-ml",
    "href": "contents/core/introduction/introduction.it.html#lo-spettro-dei-sistemi-ml",
    "title": "1¬† Introduzione",
    "section": "1.6 Lo Spettro dei Sistemi ML",
    "text": "1.6 Lo Spettro dei Sistemi ML\nLa complessit√† della gestione dei sistemi di machine learning diventa ancora pi√π evidente se consideriamo l‚Äôampio spettro in cui il ML viene distribuito oggi. I sistemi di ML esistono su scale molto diverse e in ambienti diversi, ognuno dei quali presenta sfide e vincoli unici.\nDa un lato, abbiamo sistemi di ML basati su cloud in esecuzione in enormi data center. Questi sistemi, come i grandi modelli linguistici o i motori di raccomandazione, elaborano petabyte di dati e servono milioni di utenti contemporaneamente. Possono sfruttare risorse di elaborazione virtualmente illimitate, ma devono gestire un‚Äôenorme complessit√† operativa e costi.\nDall‚Äôaltro lato, troviamo sistemi TinyML in esecuzione su microcontrollori e dispositivi embedded. Questi sistemi devono eseguire attivit√† di ML con gravi vincoli di memoria, potenza di elaborazione e consumo energetico. Si immagini un dispositivo per la casa intelligente, come Alexa o Google Assistant, che deve riconoscere i comandi vocali utilizzando meno energia di una lampadina a LED, o un sensore che deve rilevare anomalie mentre funziona a batteria per mesi o addirittura anni.\nTra questi estremi, troviamo una ricca variet√† di sistemi di ML adattati a diversi contesti. I sistemi Edge ML avvicinano il calcolo alle fonti dei dati, riducendo i requisiti di latenza e larghezza di banda e gestendo al contempo le risorse di elaborazione locali. I sistemi ML mobili devono bilanciare capacit√† sofisticate con limitazioni di durata della batteria e del processore su smartphone e tablet. I sistemi ML aziendali spesso operano entro vincoli aziendali specifici, concentrandosi su attivit√† particolari e integrandosi con l‚Äôinfrastruttura esistente. Alcune organizzazioni impiegano approcci ibridi, distribuendo le capacit√† ML su pi√π livelli per bilanciare vari requisiti.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduzione</span>"
    ]
  },
  {
    "objectID": "contents/core/introduction/introduction.it.html#implicazioni-del-sistema-di-ml-sul-ciclo-di-vita-ml",
    "href": "contents/core/introduction/introduction.it.html#implicazioni-del-sistema-di-ml-sul-ciclo-di-vita-ml",
    "title": "1¬† Introduzione",
    "section": "1.7 Implicazioni del Sistema di ML sul Ciclo di Vita ML",
    "text": "1.7 Implicazioni del Sistema di ML sul Ciclo di Vita ML\nLa diversit√† dei sistemi ML nell‚Äôintero spettro rappresenta una complessa interazione di requisiti, vincoli e compromessi. Queste decisioni hanno un impatto fondamentale su ogni fase del ciclo di vita ML di cui abbiamo parlato in precedenza, dalla raccolta dati al funzionamento continuo.\nI requisiti di prestazioni spesso guidano le decisioni architettoniche iniziali. Le applicazioni sensibili alla latenza, come i veicoli autonomi o il rilevamento delle frodi in tempo reale, potrebbero richiedere architetture edge o embedded nonostante i loro vincoli di risorse. Al contrario, le applicazioni che richiedono un‚Äôenorme potenza di calcolo per l‚Äôaddestramento, come i grandi modelli linguistici, gravitano naturalmente verso architetture cloud centralizzate. Tuttavia, le mere prestazioni sono solo una considerazione in uno spazio decisionale complesso.\nLa gestione delle risorse varia notevolmente tra le architetture. I sistemi cloud devono ottimizzare l‚Äôefficienza dei costi su larga scala, bilanciando costosi cluster GPU, sistemi di archiviazione e larghezza di banda di rete. I sistemi edge affrontano limiti di risorse fissate e devono gestire attentamente l‚Äôelaborazione e l‚Äôarchiviazione locali. I sistemi mobili ed embedded operano con i vincoli pi√π rigorosi, in cui ogni byte di memoria e milliwatt di potenza sono importanti. Queste considerazioni sulle risorse influenzano direttamente sia la progettazione del modello che l‚Äôarchitettura del sistema.\nLa complessit√† operativa aumenta con la distribuzione del sistema. Mentre le architetture cloud centralizzate traggono vantaggio da strumenti di distribuzione maturi e servizi gestiti, i sistemi edge e ibridi devono gestire la complessit√† della gestione del sistema distribuito. Questa complessit√† si manifesta durante l‚Äôintero ciclo di vita del ML, dalla raccolta dati e dal controllo delle versioni alla distribuzione e al monitoraggio del modello. Come discusso nell‚Äôesame del debito tecnico, questa complessit√† operativa pu√≤ aumentare nel tempo se non gestita attentamente.\nLe considerazioni sui dati spesso introducono pressioni concorrenti. I requisiti sulla privacy o le normative sulla sovranit√† dei dati potrebbero spingere verso architetture edge o integrate, mentre la necessit√† di dati di formazione su larga scala potrebbe favorire approcci cloud. Anche la velocit√† e il volume dei dati influenzano le scelte architettoniche: i dati dei sensori in tempo reale potrebbero richiedere l‚Äôelaborazione edge per gestire la larghezza di banda, mentre l‚Äôanalisi batch potrebbe essere pi√π adatta all‚Äôelaborazione cloud.\nI requisiti di evoluzione e manutenzione devono essere considerati fin dall‚Äôinizio. Le architetture cloud offrono flessibilit√† per l‚Äôevoluzione del sistema, ma possono comportare costi continui significativi. I sistemi edge ed embedded potrebbero essere pi√π difficili da aggiornare, ma potrebbero offrire un sovraccarico operativo inferiore. Il ciclo continuo dei sistemi ML di cui abbiamo parlato in precedenza diventa particolarmente impegnativo nelle architetture distribuite, dove l‚Äôaggiornamento dei modelli e il mantenimento dello stato di salute del sistema richiedono un‚Äôattenta orchestrazione su pi√π piani.\nQuesti compromessi sono raramente semplici scelte binarie. I moderni sistemi ML adottano spesso approcci ibridi, bilanciando attentamente queste considerazioni in base a casi d‚Äôuso e vincoli specifici. La chiave √® comprendere come queste decisioni influenzeranno il sistema durante tutto il suo ciclo di vita, dallo sviluppo iniziale al funzionamento continuo e all‚Äôevoluzione.\n\n1.7.1 Tendenze Emergenti\nSiamo solo all‚Äôinizio. Man mano che i sistemi di apprendimento automatico continuano a evolversi, diverse tendenze chiave stanno rimodellando il panorama della progettazione e dell‚Äôimplementazione dei sistemi ML.\nL‚Äôascesa dei ‚Äúagentic system‚Äù [sistemi agenti] segna una profonda evoluzione nei sistemi ML. I sistemi ML tradizionali erano principalmente reattivi: facevano previsioni o classificazioni basate sui dati di input. Al contrario, i ‚Äúagentic system‚Äù possono intraprendere azioni, imparare dai loro risultati e adattare il loro comportamento di conseguenza. Questi sistemi, esemplificati da agenti autonomi in grado di pianificare, ragionare ed eseguire attivit√† complesse, introducono nuove sfide architettoniche. Richiedono framework sofisticati per il processo decisionale, vincoli di sicurezza e interazione in tempo reale con l‚Äôambiente.\nL‚Äôevoluzione architettonica √® guidata da nuovi pattern di hardware e di distribuzione. Acceleratori AI specializzati stanno emergendo in tutto lo spettro, dai potenti chip dei data center agli efficienti processori edge alle minuscole unit√† di elaborazione neurale nei dispositivi mobili. Questo panorama informatico eterogeneo sta abilitando nuove possibilit√† architettoniche, come la distribuzione dinamica dei modelli tra livelli in base alle capacit√† di elaborazione e alle condizioni attuali. I confini tradizionali tra cloud, edge e sistemi embedded stanno diventando sempre pi√π fluidi.\nL‚Äôefficienza delle risorse sta acquisendo importanza man mano che i costi ambientali ed economici del ML su larga scala diventano pi√π evidenti. Ci√≤ ha innescato l‚Äôinnovazione nella compressione dei modelli, nelle tecniche di addestramento efficienti e nell‚Äôelaborazione consapevole dei consumi energetici. I sistemi futuri dovranno probabilmente bilanciare la spinta verso modelli pi√π potenti con le crescenti preoccupazioni per la sostenibilit√†. Questa enfasi sull‚Äôefficienza √® particolarmente rilevante data la nostra precedente discussione sul debito tecnico e sui costi operativi.\nL‚Äôintelligenza di sistema si sta muovendo verso un funzionamento pi√π autonomo. I futuri sistemi ML probabilmente incorporeranno un auto-monitoraggio pi√π sofisticato, una gestione automatizzata delle risorse e strategie di distribuzione adattive. Questa evoluzione si basa sul ciclo continuo discusso in precedenza, ma con una maggiore automazione nella gestione dei turni di distribuzione dei dati, degli aggiornamenti dei modelli e dell‚Äôottimizzazione del sistema.\nLe sfide dell‚Äôintegrazione stanno diventando pi√π complesse man mano che i sistemi ML interagiscono con ecosistemi tecnologici pi√π ampi. La necessit√† di integrarsi con i sistemi software esistenti, gestire diverse fonti di dati e operare oltre i confini organizzativi sta guidando nuovi approcci alla progettazione del sistema. Questa complessit√† di integrazione aggiunge nuove dimensioni alle considerazioni sul debito tecnico esplorato in precedenza.\nQueste tendenze suggeriscono che i futuri sistemi ML dovranno essere sempre pi√π adattabili ed efficienti, gestendo al contempo una crescente complessit√†. Comprendere queste direzioni √® importante per costruire sistemi che possano evolversi con il settore, evitando al contempo l‚Äôaccumulo di debito tecnico di cui abbiamo parlato in precedenza.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduzione</span>"
    ]
  },
  {
    "objectID": "contents/core/introduction/introduction.it.html#applicazioni-e-impatto-nel-mondo-reale",
    "href": "contents/core/introduction/introduction.it.html#applicazioni-e-impatto-nel-mondo-reale",
    "title": "1¬† Introduzione",
    "section": "1.8 Applicazioni e Impatto nel Mondo Reale",
    "text": "1.8 Applicazioni e Impatto nel Mondo Reale\nLa capacit√† di creare e rendere operativi sistemi ML su diverse scale e ambienti ha portato a cambiamenti trasformativi in numerosi settori. Questa sezione presenta alcuni esempi in cui i concetti teorici e le considerazioni pratiche di cui abbiamo discusso si manifestano in applicazioni tangibili e di impatto nel mondo reale.\nThis section showcases a few examples where theoretical concepts and practical considerations we have discussed manifest in tangible, impactful applications and real-world impact.\n\n1.8.1 Caso di Studio: FarmBeats: ML Edge ed Embedded per l‚ÄôAgricoltura\nFarmBeats, un progetto sviluppato da Microsoft Research, mostrato in Figura¬†1.6, √® un progresso significativo nell‚Äôapplicazione dell‚Äôapprendimento automatico all‚Äôagricoltura. Questo sistema mira ad aumentare la produttivit√† agricola e ridurre i costi sfruttando le tecnologie AI e IoT. FarmBeats esemplifica come i sistemi ML edge ed embedded possono essere distribuiti in ambienti reali e difficili per risolvere problemi pratici. Portando le capacit√† ML direttamente in azienda agricola, FarmBeats dimostra il potenziale dei sistemi AI distribuiti nella trasformazione delle industrie tradizionali.\n\n\n\n\n\n\nFigura¬†1.6: Microsoft Farmbeats: AI, Edge e IoT per l‚ÄôAgricoltura.\n\n\n\nAspetti dei Dati\nL‚Äôecosistema di dati in FarmBeats √® diversificato e distribuito. I sensori distribuiti nei campi raccolgono dati in tempo reale su umidit√† del suolo, temperatura e livelli di nutrienti. I droni dotati di telecamere multispettrali catturano immagini ad alta risoluzione delle colture, fornendo informazioni sulla salute delle piante e sui pattern di crescita. Le stazioni meteorologiche forniscono dati climatici locali, mentre le registrazioni agricole storiche offrono un contesto per le tendenze a lungo termine. La sfida non risiede solo nella raccolta di questi dati eterogenei, ma anche nella gestione del loro flusso da luoghi dispersi, spesso remoti, con connettivit√† limitata. FarmBeats impiega tecniche innovative di trasmissione dati, come l‚Äôutilizzo di spazi TV vuoti (frequenze di trasmissione inutilizzate) per estendere la connettivit√† Internet a sensori distanti. Questo approccio alla raccolta e alla trasmissione dei dati incarna i principi dell‚Äôedge computing discussi in precedenza, in cui l‚Äôelaborazione dei dati inizia alla fonte per ridurre i requisiti di larghezza di banda e consentire il processo decisionale in tempo reale.\nAspetti Algoritmo/Modello\nFarmBeats utilizza una variet√† di algoritmi ML su misura per applicazioni agricole. Per la previsione dell‚Äôumidit√† del suolo, utilizza reti neurali temporali in grado di catturare le complesse dinamiche del movimento dell‚Äôacqua nel suolo. Gli algoritmi di visione artificiale elaborano le immagini dei droni per rilevare lo stress delle colture, le infestazioni di parassiti e le stime della resa. Questi modelli devono essere robusti ai dati rumorosi e in grado di funzionare con risorse computazionali limitate. I metodi di apprendimento automatico come il ‚Äútransfer learning‚Äù consentono ai modelli di addestrarsi su aziende agricole ricche di dati per essere adattati all‚Äôuso in aree con dati storici limitati. Il sistema incorpora anche una combinazione di metodi che combinano gli output di pi√π algoritmi per migliorare l‚Äôaccuratezza e l‚Äôaffidabilit√† delle previsioni. Una sfida chiave che FarmBeats affronta √® la personalizzazione del modello, ovvero l‚Äôadattamento di modelli generali alle condizioni specifiche delle singole aziende agricole, che possono avere composizioni del suolo, microclimi e pratiche agricole uniche.\nAspetti dell‚ÄôInfrastruttura Informatica\nFarmBeats esemplifica il paradigma di edge computing esplorato nella discussione sullo spettro del sistema ML. Al livello pi√π basso, i modelli ML embedded vengono eseguiti direttamente su dispositivi e sensori IoT, eseguendo il filtraggio dei dati di base e il rilevamento delle anomalie. I dispositivi edge, come i ‚Äúruggedized field gateway‚Äù [gateway di campo rinforzati], aggregano i dati da pi√π sensori ed eseguono modelli pi√π complessi per il processo decisionale locale. Questi dispositivi edge operano in condizioni difficili, richiedendo progetti hardware robusti e una gestione efficiente dell‚Äôalimentazione per funzionare in modo affidabile in contesti agricoli remoti. Il sistema impiega un‚Äôarchitettura gerarchica, con attivit√† pi√π intensive dal punto di vista computazionale scaricate su server locali o sul cloud. Questo approccio a livelli consente a FarmBeats di bilanciare la necessit√† di elaborazione in tempo reale con i vantaggi dell‚Äôanalisi centralizzata dei dati e del training del modello. L‚Äôinfrastruttura include anche meccanismi per gli aggiornamenti dei modelli over-the-air, assicurando che i dispositivi edge possano ricevere modelli migliorati man mano che diventano disponibili pi√π dati e gli algoritmi vengono perfezionati.\nImpatto e Implicazioni Future\nFarmBeats mostra come i sistemi ML possono essere distribuiti in ambienti reali con risorse limitate per guidare miglioramenti significativi nei settori tradizionali. Fornendo agli agricoltori informazioni basate sull‚ÄôIA, il sistema ha dimostrato il potenziale per aumentare le rese delle colture, ridurre l‚Äôuso dell‚Äôacqua e ottimizzare l‚Äôallocazione delle risorse. Guardando al futuro, l‚Äôapproccio FarmBeats potrebbe essere esteso per affrontare le sfide globali in materia di sicurezza alimentare e agricoltura sostenibile. Il successo di questo sistema evidenzia anche la crescente importanza dell‚Äôedge e dell‚Äôembedded ML nelle applicazioni IoT, dove avvicinare l‚Äôintelligenza alla sorgente dei dati pu√≤ portare a soluzioni pi√π reattive, efficienti e scalabili. Man mano che le capacit√† di edge computing continuano a progredire, possiamo aspettarci di vedere simili architetture ML distribuite applicate ad altri domini, dalle smart city al monitoraggio ambientale.\n\n\n1.8.2 Caso di Studio: AlphaFold: ML Scientifico su Larga Scala\nAlphaFold, sviluppato da DeepMind, √® un traguardo storico nell‚Äôapplicazione dell‚Äôapprendimento automatico a problemi scientifici complessi. Questo sistema di IA √® progettato per prevedere la struttura tridimensionale delle proteine, come mostrato in Figura¬†1.7, dalle loro sequenze di amminoacidi, una sfida nota come ‚Äúproblema del ripiegamento delle proteine‚Äù che ha lasciato perplessi gli scienziati per decenni. Il successo di AlphaFold dimostra come i sistemi di ML su larga scala possano accelerare la scoperta scientifica e potenzialmente rivoluzionare campi come la biologia strutturale e la progettazione di farmaci. Questo studio di caso esemplifica l‚Äôuso di tecniche di ML avanzate e di enormi risorse computazionali per affrontare problemi alle frontiere della scienza.\n\n\n\n\n\n\nFigura¬†1.7: Esempi di target proteici nella categoria di modellazione libera. Fonte: Google DeepMind\n\n\n\nAspetti dei Dati\nI dati alla base del successo di AlphaFold sono vasti e sfaccettati. Il set di dati principale √® il Protein Data Bank (PDB), che contiene le strutture determinate sperimentalmente di oltre 180.000 proteine. Questo √® completato da database di sequenze proteiche, che assommano a centinaia di milioni. AlphaFold utilizza anche dati evolutivi sotto forma di allineamenti di sequenze multiple (MSA), che forniscono informazioni sui modelli di conservazione degli amminoacidi nelle proteine correlate. La sfida non risiede solo nel volume dei dati, ma anche nella loro qualit√† e rappresentazione. Le strutture proteiche sperimentali possono contenere errori o essere incomplete, il che richiede sofisticati processi di pulizia e convalida dei dati. Inoltre, la rappresentazione delle strutture e delle sequenze proteiche in una forma adatta all‚Äôapprendimento automatico √® una sfida significativa di per s√©. La pipeline di dati di AlphaFold prevede complessi passaggi di pre-elaborazione per convertire i dati grezzi di sequenza e strutturali in feature significative che catturano le propriet√† fisiche e chimiche rilevanti per il ripiegamento delle proteine.\nAspetti Algoritmo/Modello\nL‚Äôapproccio algoritmico di AlphaFold rappresenta un ‚Äútour de force‚Äù nell‚Äôapplicazione del deep learning ai problemi scientifici. Al centro, AlphaFold utilizza una nuova architettura di rete neurale che si combina con tecniche di biologia computazionale. Il modello impara a prevedere distanze inter-residuo e angoli di torsione, che vengono poi utilizzati per costruire una struttura proteica 3D completa. Un‚Äôinnovazione fondamentale √® l‚Äôuso di layer di ‚Äúattenzione equivariante‚Äù che rispettano le simmetrie intrinseche nelle strutture proteiche. Il processo di apprendimento coinvolge pi√π fasi, tra cui il ‚Äúpre-addestramento‚Äù iniziale su un ampio corpus di sequenze proteiche, seguito da una messa a punto su strutture note. AlphaFold incorpora anche la conoscenza del dominio sotto forma di vincoli basati sulla fisica e funzioni di punteggio, creando un sistema ibrido che sfrutta sia l‚Äôapprendimento basato sui dati sia la conoscenza scientifica pregressa. La capacit√† del modello di generare stime di confidenza accurate per le sue previsioni √® fondamentale, consentendo ai ricercatori di valutare l‚Äôaffidabilit√† delle strutture previste.\nAspetti dell‚ÄôInfrastruttura Informatica\nLe esigenze computazionali di AlphaFold esemplificano le sfide dei sistemi ML scientifici su larga scala. L‚Äôaddestramento del modello richiede enormi risorse di elaborazione parallela, sfruttando cluster di GPU o TPU (Tensor Processing Unit) in un ambiente di elaborazione distribuito. DeepMind ha utilizzato l‚Äôinfrastruttura cloud di Google, con la versione finale di AlphaFold addestrata su 128 core TPUv3 per diverse settimane. Il processo di inferenza, sebbene meno intensivo dal punto di vista computazionale rispetto all‚Äôaddestramento, richiede comunque risorse significative, soprattutto quando si prevedono strutture per proteine di grandi dimensioni o si elaborano molte proteine in parallelo. Per rendere AlphaFold pi√π accessibile alla comunit√† scientifica, DeepMind ha collaborato con l‚ÄôEuropean Bioinformatics Institute per creare un database pubblico di strutture proteiche previste, che di per s√© rappresenta una sfida sostanziale per l‚Äôelaborazione e la gestione dei dati. Questa infrastruttura consente ai ricercatori di tutto il mondo di accedere alle previsioni di AlphaFold senza dover eseguire il modello in proprio, dimostrando come le risorse di elaborazione centralizzate e ad alte prestazioni possano essere sfruttate per democratizzare l‚Äôaccesso alle funzionalit√† ML avanzate.\nImpatto e Implicazioni Future\nL‚Äôimpatto di AlphaFold sulla biologia strutturale √® stato profondo, con il potenziale di accelerare la ricerca in aree che vanno dalla biologia fondamentale alla scoperta di farmaci. Fornendo previsioni strutturali accurate per proteine che hanno resistito ai metodi sperimentali, AlphaFold apre nuove strade per comprendere i meccanismi delle malattie e progettare terapie mirate. Il successo di AlphaFold serve anche come una potente dimostrazione di come il ML pu√≤ essere applicato ad altri complessi problemi scientifici, portando potenzialmente a scoperte in campi come la scienza dei materiali o la modellazione climatica. Tuttavia, solleva anche importanti questioni sul ruolo dell‚ÄôIA nella scoperta scientifica e sulla natura mutevole dell‚Äôindagine scientifica nell‚Äôera dei sistemi ML su larga scala. Mentre guardiamo al futuro, l‚Äôapproccio AlphaFold suggerisce un nuovo paradigma per il ML scientifico, in cui enormi risorse computazionali vengono combinate con conoscenze specifiche del dominio per spingere i confini della comprensione umana.\n\n\n1.8.3 Caso di Studio: Veicoli Autonomi: Abbracciare lo Spettro ML\nWaymo, una sussidiaria di Alphabet Inc., √® all‚Äôavanguardia nella tecnologia dei veicoli autonomi, rappresentando una delle applicazioni pi√π ambiziose dei sistemi di apprendimento automatico fino ad oggi. Evoluzione del Google Self-Driving Car Project avviato nel 2009, l‚Äôapproccio di Waymo alla guida autonoma esemplifica come i sistemi ML possano abbracciare l‚Äôintero spettro, dai sistemi embedded all‚Äôinfrastruttura cloud. Questo caso di studio mostra l‚Äôimplementazione pratica di sistemi ML complessi in un ambiente reale critico per la sicurezza, integrando il processo decisionale in tempo reale con l‚Äôaddestramento e l‚Äôadattamento a lungo termine.\n\nAspetti dei Dati\nL‚Äôecosistema di dati alla base della tecnologia Waymo √® vasto e dinamico. Ogni veicolo funge da data center itinerante, la sua suite di sensori, composta da LiDAR, radar e telecamere ad alta risoluzione, genera circa un terabyte di dati per ora di guida. Questi dati del mondo reale sono integrati da un set di dati simulato ancora pi√π esteso, con i veicoli Waymo che hanno percorso oltre 20 miliardi di miglia in simulazione e pi√π di 20 milioni di miglia su strade pubbliche. La sfida non risiede solo nel volume di dati, ma nella loro eterogeneit√† e nella necessit√† di elaborazione in tempo reale. Waymo deve gestire contemporaneamente sia dati strutturati (ad esempio, coordinate GPS) che non strutturati (ad esempio, immagini della telecamera). La pipeline di dati si estende dall‚Äôelaborazione edge sul veicolo stesso a enormi sistemi di archiviazione ed elaborazione basati su cloud. Sono necessari sofisticati processi di validazione e pulizia dei dati, data la natura critica per la sicurezza dell‚Äôapplicazione. Inoltre, la rappresentazione dell‚Äôambiente del veicolo in una forma adatta all‚Äôapprendimento automatico presenta sfide significative, che richiedono una pre-elaborazione complessa per convertire i dati grezzi dei sensori in caratteristiche significative che catturino le dinamiche degli scenari di traffico.\nAspetti Algoritmo/Modello\nLo stack ML di Waymo rappresenta un sofisticato insieme di algoritmi su misura per la sfida multiforme della guida autonoma. Il sistema di percezione impiega tecniche di deep learning, tra cui reti neurali convoluzionali, per elaborare dati visivi per il rilevamento e il tracciamento di oggetti. I modelli di previsione, necessari per anticipare il comportamento di altri utenti della strada, sfruttano reti neurali ricorrenti per comprendere sequenze temporali. Waymo ha sviluppato modelli ML personalizzati come VectorNet per prevedere le traiettorie dei veicoli. I sistemi di pianificazione e decisionali possono incorporare tecniche di apprendimento per rinforzo o apprendimento per imitazione per navigare in scenari di traffico complessi. Un‚Äôinnovazione fondamentale nell‚Äôapproccio di Waymo √® l‚Äôintegrazione di questi diversi modelli in un sistema coerente in grado di funzionare in tempo reale. I modelli ML devono anche essere interpretabili in una certa misura, poich√© comprendere il ragionamento alla base delle decisioni di un veicolo √® fondamentale per la sicurezza e la conformit√† alle normative. Il processo di apprendimento di Waymo implica un continuo perfezionamento basato su esperienze di guida nel mondo reale e simulazioni approfondite, creando un ciclo di feedback che migliora costantemente le prestazioni del sistema.\nAspetti dell‚ÄôInfrastruttura Informatica\nL‚Äôinfrastruttura informatica che supporta i veicoli autonomi di Waymo incarna le sfide dell‚Äôimplementazione di sistemi ML nell‚Äôintero spettro, dall‚Äôedge al cloud. Ogni veicolo √® dotato di una piattaforma informatica personalizzata in grado di elaborare dati dei sensori e prendere decisioni in tempo reale, spesso sfruttando hardware specializzato come GPU o acceleratori AI personalizzati. Questo edge computing √® completato da un ampio utilizzo dell‚Äôinfrastruttura cloud, sfruttando la potenza dei data center di Google per la formazione di modelli, l‚Äôesecuzione di simulazioni su larga scala e l‚Äôesecuzione di apprendimento a livello di flotta. La connettivit√† tra questi livelli √® fondamentale, con veicoli che richiedono comunicazioni affidabili e ad alta larghezza di banda per aggiornamenti in tempo reale e caricamento dati. L‚Äôinfrastruttura di Waymo deve essere progettata per robustezza e tolleranza agli errori, garantendo un funzionamento sicuro anche in caso di guasti hardware o interruzioni di rete. La portata delle operazioni di Waymo presenta sfide significative nella gestione dei dati, nell‚Äôimplementazione dei modelli e nel monitoraggio del sistema in una flotta di veicoli distribuita geograficamente.\nImpatto e Implicazioni Future\nL‚Äôimpatto di Waymo si estende oltre il progresso tecnologico, rivoluzionando potenzialmente i trasporti, la pianificazione urbana e numerosi aspetti della vita quotidiana. Il lancio di Waymo One, un servizio di ‚Äúride-hailing‚Äù [a chiamata] commerciale che utilizza veicoli autonomi a Phoenix, in Arizona, rappresenta una pietra miliare significativa nell‚Äôimplementazione pratica dei sistemi di IA in applicazioni critiche per la sicurezza. I progressi di Waymo hanno implicazioni pi√π ampie per lo sviluppo di sistemi di IA solidi e reali, guidando innovazioni nella tecnologia dei sensori, nell‚Äôedge computing e nella sicurezza dell‚ÄôIA che hanno applicazioni ben oltre l‚Äôindustria automobilistica. Tuttavia, solleva anche importanti questioni su responsabilit√†, etica e interazione tra sistemi di IA e societ√† umana. Mentre Waymo continua ad espandere le sue operazioni ed esplorare applicazioni nei trasporti su camion e nelle consegne dell‚Äôultimo miglio, funge da importante banco di prova per sistemi di ML avanzati, guidando i progressi in aree come l‚Äôapprendimento continuo, la percezione robusta e l‚Äôinterazione uomo-IA. Il caso di studio di Waymo sottolinea sia l‚Äôenorme potenziale dei sistemi di ML per trasformare i settori sia le complesse sfide implicate nell‚Äôimplementazione dell‚ÄôIA nel mondo reale.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduzione</span>"
    ]
  },
  {
    "objectID": "contents/core/introduction/introduction.it.html#sfide-e-considerazioni",
    "href": "contents/core/introduction/introduction.it.html#sfide-e-considerazioni",
    "title": "1¬† Introduzione",
    "section": "1.9 Sfide e Considerazioni",
    "text": "1.9 Sfide e Considerazioni\nLa creazione e l‚Äôimplementazione di sistemi di apprendimento automatico presentano sfide uniche che vanno oltre lo sviluppo software tradizionale. Queste sfide aiutano a spiegare perch√© la creazione di sistemi ML efficaci non riguarda solo la scelta dell‚Äôalgoritmo giusto o la raccolta di dati sufficienti. Esploriamo le aree chiave in cui i professionisti del ML affrontano ostacoli significativi.\n\n1.9.1 Sfide dei Dati\nIl fondamento di qualsiasi sistema ML sono i suoi dati e la gestione di questi dati presenta diverse sfide fondamentali. Innanzitutto, c‚Äô√® la questione di base della qualit√† dei dati: i dati del mondo reale sono spesso disordinati e incoerenti. Si immagini un‚Äôapplicazione sanitaria che deve elaborare le cartelle cliniche dei pazienti di diversi ospedali. Ogni ospedale potrebbe registrare le informazioni in modo diverso, utilizzare unit√† di misura diverse o avere standard diversi per i dati da raccogliere. Alcune cartelle potrebbero avere informazioni mancanti, mentre altre potrebbero contenere errori o incongruenze che devono essere eliminate prima che i dati possano essere utili.\nMan mano che i sistemi ML crescono, spesso devono gestire quantit√† di dati sempre maggiori. Un servizio di streaming video come Netflix, ad esempio, deve elaborare miliardi di interazioni con gli spettatori per alimentare il suo sistema di raccomandazione. Questa scala introduce nuove sfide su come archiviare, elaborare e gestire in modo efficiente set di dati cos√¨ grandi.\nUn‚Äôaltra sfida critica √® il modo in cui i dati cambiano nel tempo. Questo fenomeno, noto come ‚Äúdata drift‚Äù, si verifica quando i modelli nei nuovi dati iniziano a differire dai modelli da cui il sistema ha appreso originariamente. Ad esempio, molti modelli predittivi hanno avuto difficolt√† durante la pandemia di COVID-19 perch√© il comportamento dei consumatori √® cambiato cos√¨ drasticamente che i modelli storici sono diventati meno rilevanti. I sistemi ML hanno bisogno di modi per rilevare quando ci√≤ accade e adattarsi di conseguenza.\n\n\n1.9.2 Sfide del Modello\nLa creazione e la manutenzione dei modelli ML stessi presentano un‚Äôaltra serie di sfide. I moderni modelli ML, in particolare nel deep learning, possono essere estremamente complessi. Si consideri un modello linguistico come GPT-3, che ha centinaia di miliardi di parametri (le singole impostazioni che il modello apprende durante l‚Äôaddestramento). Questa complessit√† crea sfide pratiche: questi modelli richiedono un‚Äôenorme potenza di calcolo per l‚Äôaddestramento e l‚Äôesecuzione, rendendo difficile la loro distribuzione in situazioni con risorse limitate, come su telefoni cellulari o dispositivi IoT.\nAddestrare questi modelli in modo efficace √® di per s√© una sfida significativa. A differenza della programmazione tradizionale in cui scriviamo istruzioni esplicite, i modelli ML imparano dagli esempi. Questo processo di apprendimento comporta molte scelte: come dovremmo strutturare il modello? Per quanto tempo dovremmo addestrarlo? Come possiamo sapere se sta imparando le cose giuste? Prendere queste decisioni spesso richiede sia competenza tecnica che notevoli tentativi ed errori.\nUna sfida particolarmente importante √® garantire che i modelli funzionino bene in condizioni reali. Un modello potrebbe funzionare in modo eccellente sui suoi dati di addestramento ma fallire quando si trova di fronte a situazioni leggermente diverse nel mondo reale. Questo divario tra le prestazioni di formazione e le prestazioni nel mondo reale √® una sfida centrale nell‚Äôapprendimento automatico, specialmente per applicazioni critiche come veicoli autonomi o sistemi di diagnosi medica.\n\n\n1.9.3 Sfide di Sistema\nFar funzionare i sistemi ML in modo affidabile nel mondo reale presenta una serie di sfide. A differenza dei software tradizionali che seguono regole fisse, i sistemi ML devono gestire l‚Äôincertezza e la variabilit√† nei loro input e output. In genere necessitano sia di sistemi di formazione (per apprendere dai dati) sia di sistemi di servizio (per fare previsioni), ognuno con requisiti e vincoli diversi.\nSi consideri un‚Äôazienda che costruisce un sistema di riconoscimento vocale. Hanno bisogno di infrastrutture per raccogliere e archiviare dati audio, sistemi per addestrare modelli su questi dati e quindi sistemi separati per elaborare effettivamente il parlato degli utenti in tempo reale. Ogni parte di questa pipeline deve funzionare in modo affidabile ed efficiente e tutte le parti devono funzionare insieme senza problemi.\nQuesti sistemi necessitano anche di monitoraggio e aggiornamento costanti. Come facciamo a sapere se il sistema funziona correttamente? Come aggiorniamo i modelli senza interrompere il servizio? Come gestiamo errori o input imprevisti? Queste sfide operative diventano particolarmente complesse quando i sistemi ML servono milioni di utenti.\n\n\n1.9.4 Considerazioni Etiche e Sociali\nMan mano che i sistemi ML diventano pi√π diffusi nella nostra vita quotidiana, il loro impatto pi√π ampio sulla societ√† diventa sempre pi√π importante da considerare. Una delle principali preoccupazioni √® l‚Äôequit√†: i sistemi ML a volte possono imparare a prendere decisioni che discriminano determinati gruppi di persone. Ci√≤ accade spesso in modo involontario, poich√© i sistemi rilevano pregiudizi presenti nei loro dati di formazione. Ad esempio, un sistema di screening delle domande di lavoro potrebbe imparare inavvertitamente a favorire determinati dati demografici se tali gruppi avevano storicamente maggiori probabilit√† di essere assunti.\nUn‚Äôaltra considerazione importante √® la trasparenza. Molti modelli ML moderni, in particolare i modelli di deep learning, funzionano come ‚Äúscatole nere‚Äù: sebbene possano fare previsioni, √® spesso difficile capire come sono arrivati alle loro decisioni. Ci√≤ diventa particolarmente problematico quando i sistemi ML prendono decisioni importanti sulla vita delle persone, come nell‚Äôassistenza sanitaria o nei servizi finanziari.\nAnche la privacy √® una delle principali preoccupazioni. I sistemi ML spesso necessitano di grandi quantit√† di dati per funzionare in modo efficace, ma questi dati potrebbero contenere informazioni personali sensibili. Come bilanciamo la necessit√† di dati con la necessit√† di proteggere la privacy individuale? Come possiamo garantire che i modelli non memorizzino e rivelino inavvertitamente informazioni private?\nQueste sfide non sono semplicemente problemi tecnici da risolvere, ma considerazioni in corso che modellano il nostro approccio alla progettazione e all‚Äôimplementazione del sistema ML. In questo libro, esploreremo queste sfide in dettaglio ed esamineremo le strategie per affrontarle in modo efficace.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduzione</span>"
    ]
  },
  {
    "objectID": "contents/core/introduction/introduction.it.html#direzioni-future",
    "href": "contents/core/introduction/introduction.it.html#direzioni-future",
    "title": "1¬† Introduzione",
    "section": "1.10 Direzioni Future",
    "text": "1.10 Direzioni Future\nGuardando al futuro dei sistemi di apprendimento automatico, diverse tendenze entusiasmanti stanno plasmando il campo. Questi sviluppi promettono sia di risolvere le sfide esistenti sia di aprire nuove possibilit√† per ci√≤ che i sistemi ML possono realizzare.\nUna delle tendenze pi√π significative √® la democratizzazione della tecnologia AI. Proprio come i personal computer hanno trasformato l‚Äôinformatica da mainframe specializzati a strumenti di uso quotidiano, i sistemi ML stanno diventando pi√π accessibili a sviluppatori e organizzazioni di tutte le dimensioni. I provider cloud ora offrono modelli pre-addestrati e piattaforme ML automatizzate che riducono le competenze necessarie per implementare soluzioni AI. Questa democratizzazione sta abilitando nuove applicazioni in tutti i settori, dalle piccole imprese che utilizzano l‚ÄôAI per il servizio clienti ai ricercatori che applicano l‚ÄôML a problemi precedentemente intrattabili.\nCon l‚Äôaumento delle preoccupazioni sui costi computazionali e sull‚Äôimpatto ambientale, c‚Äô√® una crescente attenzione nel rendere i sistemi ML pi√π efficienti. I ricercatori stanno sviluppando nuove tecniche per addestrare modelli con meno dati e potenza di calcolo. L‚Äôinnovazione nell‚Äôhardware specializzato, dalle GPU migliorate ai chip AI personalizzati, sta rendendo i sistemi ML pi√π veloci e pi√π efficienti dal punto di vista energetico. Questi progressi potrebbero rendere disponibili sofisticate capacit√† AI su pi√π dispositivi, dagli smartphone ai sensori IoT.\nForse la tendenza pi√π trasformativa √® lo sviluppo di sistemi ML pi√π autonomi in grado di adattarsi e migliorarsi. Questi sistemi stanno iniziando a gestire le proprie attivit√† di manutenzione, rilevando quando hanno bisogno di essere riqualificati, trovando e correggendo automaticamente gli errori e ottimizzando le proprie prestazioni. Questa automazione potrebbe ridurre drasticamente le spese generali operative dei sistemi ML in esecuzione, migliorandone al contempo l‚Äôaffidabilit√†.\nSebbene queste tendenze siano promettenti, √® importante riconoscere i limiti del campo. La creazione di un‚Äôintelligenza generale veramente artificiale rimane un obiettivo lontano. Gli attuali sistemi ML eccellono in attivit√† specifiche, ma mancano della flessibilit√† e della comprensione che gli esseri umani danno per scontate. Le sfide relative a pregiudizi, trasparenza e privacy continuano a richiedere un‚Äôattenta considerazione. Man mano che i sistemi ML diventano pi√π diffusi, sar√† fondamentale affrontare queste limitazioni sfruttando al contempo nuove capacit√†.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduzione</span>"
    ]
  },
  {
    "objectID": "contents/core/introduction/introduction.it.html#percorso-di-apprendimento-e-struttura-del-libro",
    "href": "contents/core/introduction/introduction.it.html#percorso-di-apprendimento-e-struttura-del-libro",
    "title": "1¬† Introduzione",
    "section": "1.11 Percorso di Apprendimento e Struttura del Libro",
    "text": "1.11 Percorso di Apprendimento e Struttura del Libro\nQuesto libro √® progettato per guidare dalla comprensione dei fondamenti dei sistemi ML alla progettazione e all‚Äôimplementazione efficaci. Per affrontare le complessit√† e le sfide dell‚Äôingegneria dei Sistemi di Machine Learning, abbiamo organizzato il contenuto attorno a cinque pilastri fondamentali che comprendono il ciclo di vita dei sistemi ML. Questi pilastri forniscono un framework per comprendere, sviluppare e mantenere sistemi ML robusti.\n\n\n\n\n\n\nFigura¬†1.8: Panoramica dei cinque pilastri fondamentali del sistema dell‚Äôingegneria dei Sistemi di Machine Learning.\n\n\n\nCome illustrato nella Figura Figura¬†1.8, i cinque pilastri centrali del framework sono:\n\nDati: Enfatizzazione dell‚Äôingegneria dei dati e dei principi fondamentali fondamentali per il funzionamento dell‚ÄôIA in relazione ai dati.\nAddestramento: Esplorazione delle metodologie per l‚Äôaddestramento dell‚ÄôIA, concentrandosi su efficienza, ottimizzazione e tecniche di accelerazione per migliorare le prestazioni del modello.\nDistribuzione: Inclusione di benchmark, strategie di addestramento su dispositivo e operazioni di apprendimento automatico per garantire un‚Äôapplicazione efficace del modello.\nOperazioni: Evidenziazione delle sfide di manutenzione uniche per i sistemi di apprendimento automatico, che richiedono approcci specializzati distinti dai sistemi di ingegneria tradizionali.\nEtica e Governance: Affrontare preoccupazioni quali sicurezza, privacy, pratiche di IA responsabili e le pi√π ampie implicazioni sociali delle tecnologie di IA.\n\nOgni pilastro rappresenta una fase critica nel ciclo di vita dei sistemi ML ed √® composto da elementi fondamentali che si basano l‚Äôuno sull‚Äôaltro. Questa struttura garantisce una comprensione completa di ‚ÄúMachine Learning in Science and Engineering‚Äù (MLSE), dai principi di base alle applicazioni avanzate e alle considerazioni etiche.\nPer informazioni pi√π dettagliate sulla panoramica del libro, sui contenuti, sui risultati di apprendimento, sul pubblico target, sui prerequisiti e sulla guida alla navigazione, fare riferimento alla sezione Informazioni sul libro. L√¨ si troveranno anche preziosi dettagli sulla nostra comunit√† di apprendimento e su come massimizzare l‚Äôesperienza con questa risorsa.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduzione</span>"
    ]
  },
  {
    "objectID": "contents/core/ml_systems/ml_systems.it.html",
    "href": "contents/core/ml_systems/ml_systems.it.html",
    "title": "2¬† Sistemi di ML",
    "section": "",
    "text": "2.1 Panoramica\nIl ML si sta evolvendo rapidamente, con nuovi paradigmi che plasmano il modo in cui i modelli vengono sviluppati, addestrati e implementati. Il settore sta vivendo un‚Äôinnovazione significativa guidata dai progressi in hardware, software e tecniche algoritmiche. Questi sviluppi stanno consentendo l‚Äôapplicazione dell‚Äôapprendimento automatico in diversi contesti, dalle infrastrutture cloud su larga scala ai dispositivi edge e persino ad ambienti minuscoli e con risorse limitate.\nI moderni sistemi di apprendimento automatico abbracciano uno spettro di opzioni di distribuzione, ciascuna con il proprio set di caratteristiche e casi d‚Äôuso. Da un lato, abbiamo l‚Äôapprendimento automatico basato su cloud, che sfrutta potenti risorse di elaborazione centralizzate per attivit√† complesse e ad alta intensit√† di dati. Muovendoci lungo lo spettro, incontriamo l‚Äôapprendimento automatico edge, che avvicina l‚Äôelaborazione alla fonte dei dati per una latenza ridotta e una migliore privacy. All‚Äôestremit√† opposta, troviamo TinyML, che consente l‚Äôapprendimento automatico su dispositivi a bassissimo consumo energetico con pesanti vincoli di memoria ed elaborazione.\nQuesto capitolo esplora il panorama dei sistemi di apprendimento automatico contemporanei, coprendo tre approcci chiave: Cloud ML, Edge ML e TinyML. Figura¬†2.1 illustra lo spettro dell‚Äôintelligenza distribuita attraverso questi approcci, fornendo un confronto visivo delle loro caratteristiche. Esamineremo le caratteristiche uniche, i vantaggi e le sfide di ogni approccio, come illustrato nella figura. Inoltre, discuteremo le tendenze e le tecnologie emergenti che stanno plasmando il futuro dell‚Äôimplementazione dell‚Äôapprendimento automatico, considerando come potrebbero influenzare l‚Äôequilibrio tra questi tre paradigmi.\nL‚Äôevoluzione dei sistemi di apprendimento automatico pu√≤ essere vista come una progressione dai paradigmi di elaborazione centralizzati a quelli distribuiti:\nOgnuno di questi paradigmi ha i suoi punti di forza ed √® adatto a diversi casi d‚Äôuso:\nLa progressione da Cloud a Edge a TinyML riflette una tendenza pi√π ampia nell‚Äôinformatica verso un‚Äôelaborazione pi√π distribuita e localizzata. Questa evoluzione √® guidata dalla necessit√† di tempi di risposta pi√π rapidi, migliore privacy, ridotto utilizzo della larghezza di banda e capacit√† di operare in ambienti con connettivit√† limitata o assente.\nFigura¬†2.2 illustra le principali differenze tra Cloud ML, Edge ML e TinyML in termini di hardware, latenza, connettivit√†, requisiti di alimentazione e complessit√† del modello. Passando da Cloud a Edge a TinyML, assistiamo a una drastica riduzione delle risorse disponibili, che presenta sfide significative per l‚Äôimplementazione di modelli di apprendimento automatico sofisticati. Questa disparit√† di risorse diventa particolarmente evidente quando si tenta di implementare modelli di deep learning su microcontrollori, la piattaforma hardware principale per TinyML. Questi piccoli dispositivi hanno capacit√† di memoria e archiviazione fortemente limitate, che sono spesso insufficienti per i modelli di deep learning convenzionali. Impareremo a mettere queste cose in prospettiva in questo capitolo.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Sistemi di ML</span>"
    ]
  },
  {
    "objectID": "contents/core/ml_systems/ml_systems.it.html#panoramica",
    "href": "contents/core/ml_systems/ml_systems.it.html#panoramica",
    "title": "2¬† Sistemi di ML",
    "section": "",
    "text": "Figura¬†2.1: Cloud vs.¬†Edge vs.¬†TinyML: Lo Spettro dell‚ÄôIntelligenza Distribuita. Fonte: ABI Research ‚Äì TinyML.\n\n\n\n\n\nCloud ML: Inizialmente, il ML era prevalentemente basato sul cloud. Per addestrare ed eseguire grandi modelli ML venivano utilizzati server potenti nei data center. Questo approccio sfrutta vaste risorse di elaborazione e capacit√† di archiviazione, consentendo lo sviluppo di modelli complessi addestrati su enormi set di dati. Cloud ML eccelle nelle attivit√† che richiedono un‚Äôampia potenza di elaborazione ed √® ideale per applicazioni in cui la reattivit√† in tempo reale non √® critica.\nEdge ML: Con l‚Äôaumento della necessit√† di elaborazione in tempo reale e a bassa latenza, √® emerso Edge ML. Questo paradigma avvicina le capacit√† di inferenza alla fonte dei dati, in genere su dispositivi edge come smartphone, telecamere intelligenti o gateway IoT. Edge ML riduce la latenza, migliora la privacy mantenendo i dati locali e pu√≤ funzionare con connettivit√† cloud intermittente. √à particolarmente utile per applicazioni che richiedono risposte rapide o che gestiscono dati sensibili.\nTinyML: L‚Äôultimo sviluppo in questa direzione √® TinyML, che consente l‚Äôesecuzione di modelli ML su microcontrollori con risorse estremamente limitate e piccoli sistemi embedded. TinyML consente l‚Äôinferenza sul dispositivo senza fare affidamento sulla connettivit√† al cloud o all‚Äôedge, aprendo nuove possibilit√† per dispositivi intelligenti alimentati a batteria. Questo approccio √® fondamentale per le applicazioni in cui dimensioni, consumo energetico e costi sono fattori critici.\n\n\n\nCloud ML rimane essenziale per le attivit√† che richiedono un‚Äôenorme potenza di calcolo o analisi di dati su larga scala.\nEdge ML √® ideale per le applicazioni che necessitano di risposte a bassa latenza o elaborazione di dati locali.\nTinyML abilita le funzionalit√† di intelligenza artificiale in dispositivi piccoli e a basso consumo energetico, espandendo la portata di ML a nuovi domini.\n\n\n\n\n\n\n\n\n\nFigura¬†2.2: Dalle GPU cloud ai microcontrollori: Navigazione nel panorama della memoria e dell‚Äôarchiviazione tra dispositivi di elaborazione. Fonte: (Lin et al. 2023)\n\n\nLin, Ji, Ligeng Zhu, Wei-Ming Chen, Wei-Chen Wang, e Song Han. 2023. ¬´Tiny Machine Learning: Progress and Futures Feature¬ª. IEEE Circuits Syst. Mag. 23 (3): 8‚Äì34. https://doi.org/10.1109/mcas.2023.3302182.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Sistemi di ML</span>"
    ]
  },
  {
    "objectID": "contents/core/ml_systems/ml_systems.it.html#cloud-ml",
    "href": "contents/core/ml_systems/ml_systems.it.html#cloud-ml",
    "title": "2¬† Sistemi di ML",
    "section": "2.2 Cloud ML",
    "text": "2.2 Cloud ML\nCloud ML sfrutta potenti server nel cloud per il training e l‚Äôesecuzione di modelli ML complessi e di grandi dimensioni e si basa sulla connettivit√† Internet. Figura¬†2.3 fornisce una panoramica delle capacit√† di Cloud ML che discuteremo pi√π in dettaglio in questa sezione.\n\n\n\n\n\n\nFigura¬†2.3: Panoramica della sezione per Cloud ML.\n\n\n\n\n2.2.1 Caratteristiche\nDefinizione di Cloud ML\nIl Cloud Machine Learning (Cloud ML) √® un sottocampo del machine learning che sfrutta la potenza e la scalabilit√† dell‚Äôinfrastruttura di cloud computing per sviluppare, addestrare e distribuire modelli di machine learning. Utilizzando le vaste risorse computazionali disponibili nel cloud, Cloud ML consente la gestione efficiente di set di dati su larga scala e algoritmi di machine learning complessi.\nInfrastruttura Centralizzata\nUna delle caratteristiche principali di Cloud ML √® la sua infrastruttura centralizzata. Figura¬†2.4 illustra questo concetto con un esempio dal data center Cloud TPU di Google. I provider di servizi cloud offrono una piattaforma virtuale composta da server ad alta capacit√†, soluzioni di storage espansive e architetture di rete robuste, tutte ospitate in data center distribuiti in tutto il mondo. Come mostrato nella figura, queste strutture centralizzate possono essere di grandi dimensioni, ospitando file e file di hardware specializzato. Questa configurazione centralizzata consente la messa in comune e la gestione efficiente delle risorse computazionali, semplificando la scalabilit√† dei progetti di machine learning in base alle esigenze.\n\n\n\n\n\n\nFigura¬†2.4: Data center Cloud TPU presso Google. Fonte: Google.\n\n\n\nElaborazione Dati Scalabile e Addestramento dei Modelli\nIl Cloud ML eccelle nella sua capacit√† di elaborare e analizzare enormi volumi di dati. L‚Äôinfrastruttura centralizzata √® progettata per gestire calcoli complessi e attivit√† di model training che richiedono una notevole potenza di calcolo. Sfruttando la scalabilit√† del cloud, i modelli di apprendimento automatico possono essere addestrati su grandi quantit√† di dati, con conseguente miglioramento delle capacit√† di apprendimento e delle prestazioni predittive.\nDeployment Flessibile e Accessibilit√†\nUn altro vantaggio di Cloud ML √® la flessibilit√† che offre in termini di deployment [distribuzione] e accessibilit√†. Una volta che un modello di machine learning √® stato addestrato e convalidato, pu√≤ essere facilmente distribuito e reso accessibile agli utenti tramite servizi basati su cloud. Ci√≤ consente un‚Äôintegrazione perfetta delle funzionalit√† di apprendimento automatico in varie applicazioni e servizi, indipendentemente dalla posizione o dal dispositivo dell‚Äôutente.\nCollaborazione e Condivisione delle Risorse\nIl Cloud ML promuove la collaborazione e la condivisione delle risorse tra team e organizzazioni. La natura centralizzata dell‚Äôinfrastruttura cloud consente a pi√π utenti di accedere e lavorare contemporaneamente sugli stessi progetti di apprendimento automatico. Questo approccio collaborativo facilita la condivisione delle conoscenze, accelera il processo di sviluppo e ottimizza l‚Äôutilizzo delle risorse.\nEfficacia dei Costi e Scalabilit√†\nSfruttando il modello di prezzo ‚Äúpay-as-you-go‚Äù offerto dai provider di servizi cloud, Cloud ML consente alle organizzazioni di evitare i costi iniziali associati alla creazione e alla manutenzione della propria infrastruttura di machine learning. La capacit√† di aumentare o diminuire le risorse in base alla domanda garantisce economicit√† e flessibilit√† nella gestione dei progetti di apprendimento automatico.\nIl Cloud ML ha rivoluzionato il modo in cui ci si approccia all‚Äôapprendimento automatico, rendendolo pi√π accessibile, scalabile ed efficiente. Ha aperto nuove possibilit√† per le organizzazioni di sfruttare la potenza dell‚Äôapprendimento automatico senza la necessit√† di investimenti significativi in hardware e infrastruttura.\n\n\n2.2.2 Vantaggi\nIl Cloud ML offre diversi vantaggi significativi che lo rendono una scelta potente per i progetti di apprendimento automatico:\nImmensa Potenza di Calcolo\nUno dei principali vantaggi del Cloud ML √® la sua capacit√† di fornire vaste risorse di calcolo. L‚Äôinfrastruttura cloud √® progettata per gestire algoritmi complessi ed elaborare grandi set di dati in modo efficiente. Ci√≤ √® particolarmente vantaggioso per i modelli di apprendimento automatico che richiedono una notevole potenza di calcolo, come reti di deep learning o modelli addestrati su enormi set di dati. Sfruttando le capacit√† di calcolo del cloud, le organizzazioni possono superare i limiti delle configurazioni hardware locali e ridimensionare i loro progetti di apprendimento automatico per soddisfare requisiti esigenti.\nScalabilit√† Dinamica\nIl Cloud ML offre scalabilit√† dinamica, consentendo alle organizzazioni di adattarsi facilmente alle mutevoli esigenze di calcolo. Man mano che il volume dei dati aumenta o la complessit√† dei modelli di apprendimento automatico aumenta, l‚Äôinfrastruttura cloud pu√≤ essere ridimensionata senza problemi verso l‚Äôalto o verso il basso per adattarsi a questi cambiamenti. Questa flessibilit√† garantisce prestazioni costanti e consente alle organizzazioni di gestire carichi di lavoro variabili senza la necessit√† di ingenti investimenti hardware. Col Cloud ML, le risorse possono essere allocate su richiesta, fornendo una soluzione conveniente ed efficiente per la gestione di progetti di machine learning.\nAccesso a Strumenti e Algoritmi Avanzati\nLe piattaforme Cloud ML forniscono accesso a un‚Äôampia gamma di strumenti e algoritmi avanzati specificamente progettati per l‚Äôapprendimento automatico. Questi strumenti spesso includono librerie, framework e API predefiniti che semplificano lo sviluppo e l‚Äôimplementazione di modelli di apprendimento automatico. Gli sviluppatori possono sfruttare queste risorse per accelerare la creazione, il training e l‚Äôottimizzazione di modelli sofisticati. Utilizzando gli ultimi progressi negli algoritmi e nelle tecniche di apprendimento automatico, le organizzazioni possono rimanere all‚Äôavanguardia dell‚Äôinnovazione e ottenere risultati migliori nei loro progetti di apprendimento automatico.\nAmbiente Collaborativo\nIl Cloud ML promuove un ambiente collaborativo che consente ai team di lavorare insieme senza problemi. La natura centralizzata dell‚Äôinfrastruttura cloud consente a pi√π utenti di accedere e contribuire agli stessi progetti di apprendimento automatico contemporaneamente. Questo approccio collaborativo facilita la condivisione delle conoscenze, promuove la collaborazione interfunzionale e accelera lo sviluppo e l‚Äôiterazione dei modelli di apprendimento automatico. I team possono condividere facilmente codice, set di dati e risultati, consentendo una collaborazione efficiente e guidando l‚Äôinnovazione in tutta l‚Äôorganizzazione.\nEfficacia in Termini di Costi\nL‚Äôadozione del Cloud ML pu√≤ essere una soluzione conveniente per le organizzazioni, soprattutto rispetto alla creazione e alla manutenzione di un‚Äôinfrastruttura di apprendimento automatico in sede. I provider di servizi cloud offrono modelli di prezzo flessibili, come piani pay-as-you-go o basati su abbonamento, consentendo alle organizzazioni di pagare solo per le risorse che consumano. Ci√≤ elimina la necessit√† di investimenti di capitale iniziali in hardware e infrastruttura, riducendo il costo complessivo dell‚Äôimplementazione di progetti di apprendimento automatico. Inoltre, la scalabilit√† di Cloud ML garantisce che le organizzazioni possano ottimizzare l‚Äôutilizzo delle risorse ed evitare l‚Äôeccesso di provisioning [fornitura], migliorando ulteriormente l‚Äôefficienza in termini di costi.\nI vantaggi di Cloud ML, tra cui l‚Äôimmensa potenza di calcolo, la scalabilit√† dinamica, l‚Äôaccesso a strumenti e algoritmi avanzati, l‚Äôambiente collaborativo e la convenienza, lo rendono una scelta interessante per le organizzazioni che desiderano sfruttare il potenziale del machine learning. Sfruttando le capacit√† del cloud, le organizzazioni possono accelerare le proprie iniziative di machine learning, guidare l‚Äôinnovazione e ottenere un vantaggio competitivo nell‚Äôattuale panorama basato sui dati.\n\n\n2.2.3 Sfide\nSebbene il Cloud ML offra numerosi vantaggi, presenta anche alcune sfide che le organizzazioni devono considerare:\nProblemi di Latenza\nUna delle principali sfide del Cloud ML √® il potenziale di problemi della latenza, in particolare nelle applicazioni che richiedono risposte in tempo reale. Poich√© i dati devono essere inviati dall‚Äôorigine dei dati ai server cloud centralizzati per l‚Äôelaborazione e quindi di nuovo all‚Äôapplicazione, potrebbero verificarsi ritardi dovuti alla trasmissione in rete. Questa latenza pu√≤ rappresentare un notevole svantaggio in scenari sensibili al fattore tempo, come veicoli autonomi, rilevamento delle frodi in tempo reale o sistemi di controllo industriale, in cui √® fondamentale prendere decisioni immediate. Gli sviluppatori devono progettare attentamente i propri sistemi per ridurre al minimo la latenza e garantire tempi di risposta accettabili.\nProblemi di Sicurezza e Privacy dei Dati\nLa centralizzazione dell‚Äôelaborazione e dell‚Äôarchiviazione dei dati nel cloud pu√≤ sollevare preoccupazioni sulla privacy e sulla sicurezza dei dati. Quando i dati sensibili vengono trasmessi e archiviati in data center remoti, diventano vulnerabili a potenziali attacchi informatici e accessi non autorizzati. I data center cloud possono diventare obiettivi interessanti per gli hacker che cercano di sfruttare le vulnerabilit√† e ottenere l‚Äôaccesso a informazioni preziose. Le organizzazioni devono investire in misure di sicurezza robuste, come crittografia, controlli di accesso e monitoraggio continuo, per proteggere i propri dati nel cloud. Anche la conformit√† alle normative sulla privacy dei dati, come GDPR o HIPAA, diventa una considerazione critica quando si gestiscono dati sensibili nel cloud.\nConsiderazioni sui Costi\nCon l‚Äôaumento delle esigenze di elaborazione dei dati, i costi associati all‚Äôutilizzo dei servizi cloud possono aumentare. Mentre il Cloud ML offre scalabilit√† e flessibilit√†, le organizzazioni che gestiscono grandi volumi di dati potrebbero dover affrontare costi crescenti man mano che consumano pi√π risorse cloud. Il modello di prezzo pay-as-you-go dei servizi cloud implica che i costi possono aumentare rapidamente, soprattutto per attivit√† ad alta intensit√† di elaborazione come l‚Äôaddestramento e l‚Äôinferenza dei modelli. Le organizzazioni devono monitorare e ottimizzare attentamente l‚Äôutilizzo del cloud per garantirne la convenienza. Potrebbero dover prendere in considerazione strategie come la compressione dei dati, la progettazione efficiente degli algoritmi e l‚Äôottimizzazione dell‚Äôallocazione delle risorse per ridurre al minimo i costi pur ottenendo le prestazioni desiderate.\nDipendenza dalla Connettivit√† Internet\nIl Cloud ML si basa su una connettivit√† Internet stabile e affidabile per funzionare in modo efficace. Poich√© i dati devono essere trasmessi da e verso il cloud, eventuali interruzioni o limitazioni nella connettivit√† di rete possono influire sulle prestazioni e sulla disponibilit√† del sistema di apprendimento automatico. Questa dipendenza dalla connettivit√† Internet pu√≤ rappresentare un problema in scenari in cui l‚Äôaccesso alla rete √® limitato, inaffidabile o costoso. Le organizzazioni devono garantire un‚Äôinfrastruttura di rete solida e considerare meccanismi di ‚Äúfailover‚Äù o capacit√† offline per mitigare l‚Äôimpatto dei problemi di connettivit√†.\nVendor Lock-In\nQuando si adotta il Cloud ML, le organizzazioni spesso diventano dipendenti dagli strumenti, dalle API e dai servizi specifici forniti dal fornitore cloud prescelto. Questo vendor lock-in [blocco da fornitore] pu√≤ rendere difficile cambiare fornitore o migrare verso piattaforme diverse in futuro. Le organizzazioni possono affrontare sfide in termini di portabilit√†, interoperabilit√† e costi quando prendono in considerazione un cambiamento nel loro fornitore di Cloud ML. √à importante valutare attentamente le offerte del fornitore, considerare obiettivi strategici a lungo termine e pianificare potenziali scenari di migrazione per ridurre al minimo i rischi associati al vendor lock-in.\nAffrontare queste sfide richiede un‚Äôattenta pianificazione, progettazione architettonica e strategie di mitigazione del rischio. Le organizzazioni devono soppesare i vantaggi del Cloud ML rispetto ai potenziali problemi e prendere decisioni informate in base ai loro requisiti specifici, alla sensibilit√† dei dati e agli obiettivi aziendali. Affrontando proattivamente queste sfide, le organizzazioni possono sfruttare efficacemente la potenza del Cloud ML garantendo al contempo la privacy dei dati, la sicurezza, l‚Äôeconomicit√† e l‚Äôaffidabilit√† complessiva del sistema.\n\n\n2.2.4 Casi d‚ÄôUso di Esempio\nIl Cloud ML ha trovato ampia adozione in vari domini, rivoluzionando il modo in cui le aziende operano e gli utenti interagiscono con la tecnologia. Esploriamo alcuni esempi notevoli del Cloud ML in azione:\nAssistenti Virtuali\nIl Cloud ML svolge un ruolo cruciale nel potenziamento di assistenti virtuali come Siri e Alexa. Questi sistemi sfruttano le immense capacit√† computazionali del cloud per elaborare e analizzare gli input vocali in tempo reale. Sfruttando la potenza dell‚Äôelaborazione del linguaggio naturale e degli algoritmi di apprendimento automatico, gli assistenti virtuali possono comprendere le domande degli utenti, estrarre informazioni rilevanti e generare risposte intelligenti e personalizzate. La scalabilit√† e la potenza di elaborazione del cloud consentono a questi assistenti di gestire un vasto numero di interazioni utente contemporaneamente, offrendo un‚Äôesperienza utente fluida e reattiva.\nSistemi di Raccomandazione Commerciali\nIl Cloud ML costituisce la spina dorsale dei sistemi di raccomandazione avanzati utilizzati da piattaforme come Netflix e Amazon. Questi sistemi sfruttano la capacit√† del cloud di elaborare e analizzare enormi set di dati per scoprire pattern, preferenze e comportamenti degli utenti. Sfruttando il filtraggio collaborativo e altre tecniche di apprendimento automatico, i sistemi di raccomandazione possono offrire contenuti personalizzati o suggerimenti di prodotti su misura per gli interessi di ciascun utente. La scalabilit√† del cloud consente a questi sistemi di aggiornare e perfezionare continuamente le proprie raccomandazioni in base alla quantit√† sempre crescente di dati utente, migliorandone il coinvolgimento e la soddisfazione.\nRilevamento delle Frodi\nNel settore finanziario, il Cloud ML ha rivoluzionato i sistemi di rilevamento delle frodi. Sfruttando la potenza di calcolo del cloud, questi sistemi possono analizzare grandi quantit√† di dati transazionali in tempo reale per identificare potenziali attivit√† fraudolente. Gli algoritmi di apprendimento automatico addestrati su pattern di frode storici possono rilevare anomalie e comportamenti sospetti, consentendo agli istituti finanziari di adottare misure proattive per prevenire le frodi e ridurre al minimo le perdite finanziarie. La capacit√† del cloud di elaborare e archiviare grandi volumi di dati lo rende una piattaforma ideale per implementare sistemi di rilevamento delle frodi robusti e scalabili.\nEsperienze Utente Personalizzate\nIl Cloud ML √® profondamente integrato nelle nostre esperienze online, plasmando il modo in cui interagiamo con le piattaforme digitali. Dagli annunci personalizzati sui feed dei social media alle funzionalit√† di testo predittivo nei servizi di posta elettronica, il Cloud ML alimenta algoritmi intelligenti che migliorano il coinvolgimento e la praticit√† dell‚Äôutente. Consente ai siti di e-commerce di consigliare prodotti in base alla cronologia di navigazione e acquisto di un utente, ottimizza i motori di ricerca per fornire risultati accurati e pertinenti e automatizza il tagging e la categorizzazione delle foto su piattaforme come Facebook. Sfruttando le risorse di calcolo del cloud, questi sistemi possono apprendere e adattarsi continuamente alle preferenze dell‚Äôutente, offrendo un‚Äôesperienza utente pi√π intuitiva e personalizzata.\nSicurezza e Rilevamento delle Anomalie\nIl Cloud ML svolge un ruolo nel rafforzare la sicurezza dell‚Äôutente alimentando i sistemi di rilevamento delle anomalie. Questi sistemi monitorano costantemente le attivit√† dell‚Äôutente e i log di sistema per identificare pattern insoliti o comportamenti sospetti. Analizzando grandi quantit√† di dati in tempo reale, gli algoritmi Cloud ML possono rilevare potenziali minacce informatiche, come tentativi di accesso non autorizzati, infezioni da malware o violazioni dei dati. La scalabilit√† e la potenza di elaborazione del cloud consentono a questi sistemi di gestire la crescente complessit√† e il volume dei dati di sicurezza, fornendo un approccio proattivo alla protezione di utenti e sistemi da potenziali minacce.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Sistemi di ML</span>"
    ]
  },
  {
    "objectID": "contents/core/ml_systems/ml_systems.it.html#edge-ml",
    "href": "contents/core/ml_systems/ml_systems.it.html#edge-ml",
    "title": "2¬† Sistemi di ML",
    "section": "2.3 Edge ML",
    "text": "2.3 Edge ML\n\n2.3.1 Caratteristiche\nDefinizione di Edge ML\nL‚ÄôEdge Machine Learning (Edge ML) esegue algoritmi di apprendimento automatico direttamente sui dispositivi endpoint o pi√π vicini al luogo in cui vengono generati i dati anzich√© affidarsi a server cloud centralizzati. Questo approccio avvicina l‚Äôelaborazione alla fonte dei dati, riducendo la necessit√† di inviarne grandi volumi sulle reti, con conseguente riduzione della latenza e miglioramento della privacy dei dati. Figura¬†2.5 fornisce una panoramica di questa sezione.\n\n\n\n\n\n\nFigura¬†2.5: Panoramica della sezione per Edge ML.\n\n\n\nElaborazione Dati Decentralizzata\nIn Edge ML, l‚Äôelaborazione dei dati avviene in modo decentralizzato, come illustrato in Figura¬†2.6. Invece di inviare dati a server remoti, i dati vengono elaborati localmente su dispositivi come smartphone, tablet o dispositivi Internet of Things (IoT). La figura mostra vari esempi di questi dispositivi edge, tra cui dispositivi indossabili, sensori industriali ed elettrodomestici intelligenti. Questa elaborazione locale consente ai dispositivi di prendere decisioni rapide in base ai dati che raccolgono senza affidarsi pesantemente alle risorse di un server centrale.\n\n\n\n\n\n\nFigura¬†2.6: Esempi di Edge ML. Fonte: Edge Impulse.\n\n\n\nArchiviazione e Calcolo dei Dati Locali\nL‚Äôarchiviazione e il calcolo dei dati locali sono caratteristiche chiave di Edge ML. Questa configurazione garantisce che i dati possano essere archiviati e analizzati direttamente sui dispositivi, mantenendo cos√¨ la privacy dei dati e riducendo la necessit√† di una connettivit√† Internet costante. Inoltre, questo spesso porta a un calcolo pi√π efficiente, poich√© i dati non devono percorrere lunghe distanze e i calcoli vengono eseguiti con una comprensione pi√π consapevole del contesto locale, che a volte pu√≤ portare ad analisi pi√π approfondite.\n\n\n2.3.2 Vantaggi\nLatenza Ridotta\nUno dei principali vantaggi di Edge ML √® la significativa riduzione della latenza rispetto al Cloud ML. Questa ridotta latenza pu√≤ essere un vantaggio fondamentale in situazioni in cui i millisecondi contano, come nei veicoli autonomi, dove un rapido processo decisionale pu√≤ fare la differenza tra sicurezza e incidente.\nPrivacy dei Dati Migliorata\nEdge ML offre anche una migliore privacy dei dati, poich√© i dati vengono principalmente archiviati ed elaborati localmente. Ci√≤ riduce al minimo il rischio di violazioni dei dati, pi√π comuni nelle soluzioni di archiviazione dati centralizzate. Le informazioni sensibili possono essere mantenute pi√π sicure, poich√© non vengono inviate su reti che potrebbero essere intercettate.\nMinore Utilizzo della Larghezza di Banda\nOperare pi√π vicino alla fonte dei dati significa che meno dati devono essere inviati sulle reti, riducendo l‚Äôutilizzo della larghezza di banda. Ci√≤ pu√≤ comportare risparmi sui costi e guadagni di efficienza, soprattutto in ambienti in cui la larghezza di banda √® limitata o costosa.\n\n\n2.3.3 Sfide\nRisorse di Calcolo Limitate Rispetto al Cloud ML\nTuttavia, Edge ML presenta le sue sfide. Una delle principali preoccupazioni sono le risorse di calcolo limitate rispetto alle soluzioni basate su cloud. I dispositivi endpoint possono avere una potenza di elaborazione o una capacit√† di archiviazione diverse rispetto ai server cloud, limitando la complessit√† dei modelli di apprendimento automatico che possono essere distribuiti.\nComplessit√† nella Gestione dei Nodi Edge\nLa gestione di una rete di nodi Edge pu√≤ introdurre complessit√†, soprattutto per quanto riguarda coordinamento, aggiornamenti e manutenzione. Garantire che tutti i nodi funzionino senza problemi e siano aggiornati con gli algoritmi e i protocolli di sicurezza pi√π recenti pu√≤ essere una sfida logistica.\nProblemi di Sicurezza nei Nodi Edge\nSebbene Edge ML offra una maggiore privacy dei dati, i nodi Edge possono talvolta essere pi√π vulnerabili ad attacchi fisici e informatici. Sviluppare protocolli di sicurezza affidabili che proteggano i dati su ogni nodo senza compromettere l‚Äôefficienza del sistema, resta una sfida significativa nell‚Äôimplementazione di soluzioni Edge ML.\n\n\n2.3.4 Casi d‚ÄôUso di Esempio\nEdge ML ha molte applicazioni, dai veicoli autonomi e dalle case intelligenti all‚ÄôIoT industriale. Questi esempi sono stati scelti per evidenziare scenari in cui l‚Äôelaborazione dei dati in tempo reale, la latenza ridotta e la privacy migliorata non sono solo vantaggiose, ma spesso fondamentali per il funzionamento e il successo di queste tecnologie. Dimostrano il ruolo che Edge ML pu√≤ svolgere nel guidare i progressi in vari settori, promuovendo l‚Äôinnovazione e aprendo la strada a sistemi pi√π intelligenti, reattivi e adattabili.\nVeicoli Autonomi\nI veicoli autonomi sono un esempio lampante del potenziale di Edge ML. Questi veicoli si affidano in larga misura all‚Äôelaborazione dei dati in tempo reale per navigare e prendere decisioni. I modelli di apprendimento automatico localizzati aiutano ad analizzare rapidamente i dati da vari sensori per prendere decisioni di guida immediate, garantendo sicurezza e funzionamento regolare.\nCase ed Edifici Intelligenti\nEdge ML svolge un ruolo cruciale nella gestione efficiente di vari sistemi in case ed edifici intelligenti, dall‚Äôilluminazione e dal riscaldamento alla sicurezza. Elaborando i dati localmente, questi sistemi possono funzionare in modo pi√π reattivo e armonioso con le abitudini e le preferenze degli occupanti, creando un ambiente di vita pi√π confortevole.\nIoT industriale\nL‚ÄôIoT industriale sfrutta Edge ML per monitorare e controllare processi industriali complessi. Qui, i modelli di apprendimento automatico possono analizzare i dati da numerosi sensori in tempo reale, consentendo la manutenzione predittiva, ottimizzando le operazioni e migliorando le misure di sicurezza. Questa rivoluzione nell‚Äôautomazione e nell‚Äôefficienza industriale sta trasformando la manifattura e la produzione in vari settori.\nL‚Äôapplicabilit√† di Edge ML √® vasta e non si limita a questi esempi. Vari altri settori, tra cui sanit√†, agricoltura e pianificazione urbana, stanno esplorando e integrando l‚ÄôEdge ML per sviluppare soluzioni innovative che rispondono alle esigenze e alle sfide del mondo reale, annunciando una nuova era di sistemi intelligenti e interconnessi.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Sistemi di ML</span>"
    ]
  },
  {
    "objectID": "contents/core/ml_systems/ml_systems.it.html#tiny-ml",
    "href": "contents/core/ml_systems/ml_systems.it.html#tiny-ml",
    "title": "2¬† Sistemi di ML",
    "section": "2.4 Tiny ML",
    "text": "2.4 Tiny ML\n\n2.4.1 Caratteristiche\nDefinizione di TinyML\nTinyML si colloca all‚Äôincrocio tra sistemi embedded e apprendimento automatico, rappresentando un campo in rapida crescita che porta algoritmi intelligenti direttamente a microcontrollori e sensori minuscoli. Questi microcontrollori operano con gravi limitazioni di risorse, in particolare per quanto riguarda memoria, archiviazione e potenza di calcolo. Figura¬†2.7 incapsula gli aspetti chiave di TinyML discussi in questa sezione.\n\n\n\n\n\n\nFigura¬†2.7: Panoramica della sezione per Tiny ML.\n\n\n\nMachine Learning On-Device\nIn TinyML, l‚Äôattenzione √® rivolta all‚Äôapprendimento automatico sul dispositivo. Ci√≤ significa che i modelli di apprendimento automatico vengono distribuiti e addestrati sul dispositivo, eliminando la necessit√† di server esterni o infrastrutture cloud. Ci√≤ consente a TinyML di abilitare un processo decisionale intelligente proprio dove vengono generati i dati, rendendo possibili approfondimenti e azioni in tempo reale, anche in contesti in cui la connettivit√† √® limitata o non disponibile.\nAmbienti a Basso Consumo Energetico e con Risorse Limitate\nTinyML eccelle in contesti a basso consumo energetico e con risorse limitate. Questi ambienti richiedono soluzioni altamente ottimizzate che funzionino entro le risorse disponibili. Figura¬†2.8 mostra un kit di dispositivi TinyML di esempio, che illustra la natura compatta di questi sistemi. Questi dispositivi possono solitamente stare nel palmo di una mano o, in alcuni casi, sono persino piccoli come un‚Äôunghia. TinyML soddisfa l‚Äôesigenza di efficienza tramite algoritmi e modelli specializzati progettati per offrire prestazioni decenti consumando al contempo un‚Äôenergia minima, garantendo cos√¨ periodi operativi prolungati, anche in dispositivi alimentati a batteria come quelli mostrati.\n\n\n\n\n\n\nFigura¬†2.8: Esempi di kit di dispositivi TinyML. Fonte: Widening Access to Applied Machine Learning with TinyML.\n\n\n\n\n\n\n\n\n\nEsercizio¬†2.1: TinyML con Arduino\n\n\n\n\n\nPrepararsi a portare l‚Äôapprendimento automatico sui dispositivi pi√π piccoli! Nel mondo dell‚Äôapprendimento automatico embedded, TinyML √® il luogo in cui i vincoli di risorse incontrano l‚Äôingegnosit√†. Questo notebook Colab guider√† nella creazione di un modello di riconoscimento dei gesti progettato su una scheda Arduino. Si imparer√† come addestrare una piccola ma efficace rete neurale, ottimizzarla per un utilizzo minimo di memoria e distribuirla al proprio microcontrollore. Se si √® entusiasti di rendere pi√π intelligenti gli oggetti di uso quotidiano, √® qui che si inizia!\n\n\n\n\n\n\n2.4.2 Vantaggi\nLatenza Estremamente Bassa\nUno dei vantaggi pi√π importanti di TinyML √® la sua capacit√† di offrire una latenza estremamente bassa. Poich√© il calcolo avviene direttamente sul dispositivo, il tempo necessario per inviare dati a server esterni e ricevere una risposta viene eliminato. Ci√≤ √® fondamentale nelle applicazioni che richiedono un processo decisionale immediato, consentendo risposte rapide a condizioni mutevoli.\nElevata Sicurezza dei Dati\nTinyML migliora intrinsecamente la sicurezza dei dati. Poich√© l‚Äôelaborazione e l‚Äôanalisi dei dati avvengono sul dispositivo, il rischio di intercettazione dei dati durante la trasmissione viene praticamente eliminato. Questo approccio localizzato alla gestione dei dati garantisce che le informazioni sensibili rimangano sul dispositivo, rafforzando la sicurezza dei dati dell‚Äôutente.\nEfficienza Energetica\nTinyML opera all‚Äôinterno di un framework efficiente dal punto di vista energetico, una necessit√† dati i suoi ambienti con risorse limitate. Utilizzando algoritmi snelli e metodi di calcolo ottimizzati, TinyML garantisce che i dispositivi possano eseguire attivit√† complesse senza esaurire rapidamente la durata della batteria, il che lo rende un‚Äôopzione sostenibile per le distribuzioni a lungo termine.\n\n\n2.4.3 Sfide\nCapacit√† di Calcolo Limitate\nTuttavia, il passaggio a TinyML comporta una serie di ostacoli. La limitazione principale sono le capacit√† di calcolo limitate dei dispositivi. La necessit√† di operare entro tali limiti implica che i modelli distribuiti debbano essere semplificati, il che potrebbe influire sull‚Äôaccuratezza e la complessit√† delle soluzioni.\nCiclo di Sviluppo Complesso\nTinyML introduce anche un ciclo di sviluppo complicato. La creazione di modelli leggeri ed efficaci richiede una profonda comprensione dei principi di apprendimento automatico e competenza nei sistemi embedded. Questa complessit√† richiede un approccio di sviluppo collaborativo, in cui la competenza multi-dominio √® essenziale per il successo.\nOttimizzazione e Compressione del Modello\nUna sfida centrale in TinyML √® l‚Äôottimizzazione e la compressione del modello. La creazione di modelli di machine learning in grado di operare efficacemente all‚Äôinterno della memoria limitata e della potenza di calcolo dei microcontrollori richiede approcci innovativi alla progettazione del modello. Gli sviluppatori si trovano spesso ad affrontare la sfida di trovare un delicato equilibrio e ottimizzare i modelli per mantenere l‚Äôefficacia, pur rispettando rigidi vincoli di risorse.\n\n\n2.4.4 Casi d‚ÄôUso di Esempio\nDispositivi Indossabili\nNei dispositivi indossabili, TinyML apre le porte a gadget pi√π intelligenti e reattivi. Dai fitness tracker che offrono feedback in tempo reale sugli allenamenti agli occhiali intelligenti che elaborano dati visivi al volo, TinyML trasforma il modo in cui interagiamo con la tecnologia indossabile, offrendo esperienze personalizzate direttamente dal dispositivo.\nManutenzione Predittiva\nNegli ambienti industriali, TinyML svolge un ruolo significativo nella manutenzione predittiva. Implementando algoritmi TinyML su sensori che monitorano lo stato di salute delle apparecchiature, le aziende possono identificare preventivamente potenziali problemi, riducendo i tempi di inattivit√† e prevenendo costosi guasti. L‚Äôanalisi dei dati in loco garantisce risposte rapide, impedendo potenzialmente a piccoli problemi di diventare problemi gravi.\nRilevamento delle Anomalie\nTinyML pu√≤ essere impiegato per creare modelli di rilevamento delle anomalie che identificano pattern di dati insoliti. Ad esempio, una fabbrica intelligente potrebbe usare TinyML per monitorare i processi industriali e individuare anomalie, aiutando a prevenire incidenti e migliorare la qualit√† del prodotto. Allo stesso modo, un‚Äôazienda di sicurezza potrebbe usare TinyML per monitorare il traffico di rete per pattern insoliti, aiutando a rilevare e prevenire attacchi informatici. TinyML potrebbe monitorare i dati dei pazienti per anomalie nell‚Äôassistenza sanitaria, aiutando a rilevare precocemente le malattie e a migliorare il trattamento dei pazienti.\nMonitoraggio Ambientale\nNel monitoraggio ambientale, TinyML consente l‚Äôanalisi dei dati in tempo reale da vari sensori distribuiti sul campo. Questi potrebbero spaziare dal monitoraggio della qualit√† dell‚Äôaria in citt√† al tracciamento della fauna selvatica nelle aree protette. Tramite TinyML, i dati possono essere elaborati localmente, consentendo risposte rapide alle mutevoli condizioni e fornendo una comprensione adeguata dei modelli pattern, cruciale per un processo decisionale informato.\nIn sintesi, TinyML funge da pioniere nell‚Äôevoluzione dell‚Äôapprendimento automatico, promuovendo l‚Äôinnovazione in vari campi portando l‚Äôintelligenza direttamente nell‚ÄôEdge. Il suo potenziale di trasformare la nostra interazione con la tecnologia e il mondo √® immenso, promettendo un futuro in cui i dispositivi sono connessi, intelligenti e capaci di prendere decisioni e rispondere in tempo reale.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Sistemi di ML</span>"
    ]
  },
  {
    "objectID": "contents/core/ml_systems/ml_systems.it.html#confronto",
    "href": "contents/core/ml_systems/ml_systems.it.html#confronto",
    "title": "2¬† Sistemi di ML",
    "section": "2.5 Confronto",
    "text": "2.5 Confronto\nMettiamo insieme le diverse varianti di ML che abbiamo esplorato individualmente per una visione completa. Figura¬†2.9 illustra le relazioni e le sovrapposizioni tra Cloud ML, Edge ML e TinyML utilizzando un diagramma di Venn. Questa rappresentazione visiva evidenzia efficacemente le caratteristiche uniche di ciascun approccio, mostrando anche le aree in comune. Ogni paradigma di ML ha le sue caratteristiche distinte, ma ci sono anche intersezioni in cui questi approcci condividono determinati attributi o capacit√†. Questo diagramma ci aiuta a capire come queste varianti si relazionano tra loro nel pi√π ampio panorama delle implementazioni di apprendimento automatico.\n\n\n\n\n\n\nFigura¬†2.9: Diagramma di Venn ML. Fonte: arXiv\n\n\n\nPer un confronto pi√π dettagliato di queste varianti di ML, possiamo fare riferimento a Tabella¬†2.1. Questa tabella offre un‚Äôanalisi completa di Cloud ML, Edge ML e TinyML in base a varie caratteristiche e aspetti. Esaminando queste diverse caratteristiche una accanto all‚Äôaltra, otteniamo una prospettiva pi√π chiara sui vantaggi unici e sui fattori distintivi di ogni approccio. Questo confronto dettagliato, combinato con la panoramica visiva fornita dal diagramma di Venn, aiuta a prendere decisioni informate in base alle esigenze e ai vincoli specifici di una determinata applicazione o progetto.\n\n\n\nTabella¬†2.1: Confronto degli aspetti delle funzionalit√† tra Cloud ML, Edge ML e TinyML.\n\n\n\n\n\n\n\n\n\n\n\nAspetto\nCloud ML\nEdge ML\nTinyML\n\n\n\n\nUbicazione della elaborazione\nServer centralizzati (Data Center)\nDispositivi locali (pi√π vicini alle fonti di dati)\nSul dispositivo (microcontrollori, sistemi embedded)\n\n\nLatenza\nAlta (dipende dalla connettivit√† Internet)\nModerata (latenza ridotta rispetto a Cloud ML)\nBassa (elaborazione immediata senza ritardo di rete)\n\n\nPrivacy dei dati\nModerata (dati trasmessi tramite reti)\nAlta (i dati rimangono sulle reti locali)\nMolto alta (dati elaborati sul dispositivo, non trasmessi)\n\n\nPotenza di calcolo\nAlta (usa una potente infrastruttura del data center)\nModerata (utilizza le capacit√† del dispositivo locale)\nBassa (limitata alla potenza del sistema embedded )\n\n\nConsumo energetico\nAlto (i data center consumano molta energia)\nModerato (meno dei data center, pi√π di TinyML)\nBasso (alta efficienza energetica, progettato per bassi consumi)\n\n\nScalabilit√†\nAlto (facile da scalare con risorse server aggiuntive)\nModerato (dipende dalle capacit√† del dispositivo locale)\nBasso (limitato dalle risorse hardware del dispositivo)\n\n\nCosto\nAlto (costi ricorrenti per l‚Äôuso del server, manutenzione)\nVariabile (dipende dalla complessit√† della configurazione locale)\nBasso (principalmente costi iniziali per i componenti hardware)\n\n\nConnettivit√†\nAlto (richiede una connettivit√† Internet stabile)\nBasso (pu√≤ funzionare con connettivit√† intermittente)\nMolto basso (pu√≤ funzionare senza alcuna connettivit√† di rete)\n\n\nElaborazione in tempo reale\nModerata (pu√≤ essere influenzata dalla latenza di rete)\nAlta (capace di elaborazione in tempo reale localmente)\nMolto alta (elaborazione immediata con latenza minima)\n\n\nEsempi di applicazione\nAnalisi di Big Data, Assistenti virtuali\nVeicoli autonomi, Case intelligenti\nDispositivi indossabili, Reti di sensori\n\n\nComplessit√†\nDa moderata ad alta (richiede conoscenza del cloud computing)\nModerata (richiede conoscenza della configurazione della rete locale)\nDa moderata ad alta (richiede competenza nei sistemi embedded)",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Sistemi di ML</span>"
    ]
  },
  {
    "objectID": "contents/core/ml_systems/ml_systems.it.html#conclusione",
    "href": "contents/core/ml_systems/ml_systems.it.html#conclusione",
    "title": "2¬† Sistemi di ML",
    "section": "2.6 Conclusione",
    "text": "2.6 Conclusione\nIn questo capitolo, abbiamo offerto una panoramica in evoluzione dell‚Äôapprendimento automatico, che copre i paradigmi cloud, edge e tiny ML. L‚Äôapprendimento automatico basato su cloud sfrutta le immense risorse computazionali delle piattaforme cloud per abilitare modelli potenti e accurati, ma presenta delle limitazioni, tra cui problemi di latenza e privacy. Edge ML mitiga queste limitazioni portando l‚Äôinferenza direttamente sui dispositivi edge, offrendo una latenza inferiore e ridotte esigenze di connettivit√†. TinyML va oltre, miniaturizzando i modelli ML per eseguirli direttamente su dispositivi con risorse altamente limitate, aprendo una nuova categoria di applicazioni intelligenti.\nOgni approccio ha i suoi compromessi, tra cui complessit√† del modello, latenza, privacy e costi dell‚Äôhardware. Nel tempo, prevediamo la convergenza di questi approcci ML embedded, col pre-training cloud che facilita implementazioni edge e tiny ML pi√π sofisticate. Progressi come l‚Äôapprendimento federato e l‚Äôapprendimento ‚Äúon-device‚Äù consentiranno ai dispositivi embedded di perfezionare i propri modelli imparando dai dati del mondo reale.\nIl panorama ML embedded si sta evolvendo rapidamente ed √® pronto a consentire applicazioni intelligenti su un ampio spettro di dispositivi e casi d‚Äôuso. Questo capitolo funge da ‚Äúistantanea‚Äù dello stato attuale del ML embedded. Man mano che algoritmi, hardware e connettivit√† continuano a migliorare, possiamo aspettarci che i dispositivi embedded di tutte le dimensioni diventino sempre pi√π capaci, sbloccando nuove applicazioni trasformative per l‚Äôintelligenza artificiale.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Sistemi di ML</span>"
    ]
  },
  {
    "objectID": "contents/core/ml_systems/ml_systems.it.html#sec-ml-systems-resource",
    "href": "contents/core/ml_systems/ml_systems.it.html#sec-ml-systems-resource",
    "title": "2¬† Sistemi di ML",
    "section": "2.7 Risorse",
    "text": "2.7 Risorse\nEcco un elenco curato di risorse per supportare studenti e insegnanti nei loro percorsi di apprendimento e insegnamento. Stiamo lavorando costantemente per espandere questa raccolta e presto aggiungeremo nuovi esercizi.\n\n\n\n\n\n\nSlide\n\n\n\n\n\nQueste slide sono uno strumento prezioso per gli insegnanti per tenere lezioni e per gli studenti per rivedere il materiale secondo il proprio ritmo. Incoraggiamo studenti e docenti a sfruttare queste slide per migliorare la loro comprensione e facilitare un trasferimento efficace delle conoscenze.\n\nEmbedded Systems Overview.\nEmbedded Computer Hardware.\nEmbedded I/O.\nEmbedded systems software.\nEmbedded ML software.\nEmbedded Inference.\nTinyML on Microcontrollers.\nTinyML as a Service (TinyMLaaS):\n\nTinyMLaaS: Introduction.\nTinyMLaaS: Design Overview.\n\n\n\n\n\n\n\n\n\n\n\nVideo\n\n\n\n\n\n\nProssimamente.\n\n\n\n\n\n\n\n\n\n\nEsercizi\n\n\n\n\n\nPer rafforzare i concetti trattati in questo capitolo, abbiamo curato una serie di esercizi che sfidano gli studenti ad applicare le proprie conoscenze e ad approfondire la propria comprensione.\n\nProssimamente.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Sistemi di ML</span>"
    ]
  },
  {
    "objectID": "contents/core/dl_primer/dl_primer.it.html",
    "href": "contents/core/dl_primer/dl_primer.it.html",
    "title": "3¬† Avvio al Deep Learning",
    "section": "",
    "text": "3.1 Panoramica",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Avvio al Deep Learning</span>"
    ]
  },
  {
    "objectID": "contents/core/dl_primer/dl_primer.it.html#panoramica",
    "href": "contents/core/dl_primer/dl_primer.it.html#panoramica",
    "title": "3¬† Avvio al Deep Learning",
    "section": "",
    "text": "3.1.1 Definizione e Importanza\nIl deep learning, un‚Äôarea specializzata nell‚Äôapprendimento automatico e nell‚Äôintelligenza artificiale (IA), utilizza algoritmi modellati sulla struttura e la funzione del cervello umano, noti come reti neurali artificiali. Questo campo √® un elemento fondamentale nell‚ÄôIA, che guida il progresso in diversi settori come la visione artificiale, l‚Äôelaborazione del linguaggio naturale e i veicoli a guida autonoma. La sua importanza nei sistemi di IA embedded √® evidenziata dalla sua capacit√† di gestire calcoli e previsioni intricati, ottimizzando le risorse limitate nelle impostazioni embedded.\nLa Figura¬†3.1 fornisce una rappresentazione visiva di come il deep learning si inserisce nel contesto pi√π ampio dell‚ÄôIA e del ‚Äúmachine learning‚Äù [apprendimento automatico]. Il diagramma illustra lo sviluppo cronologico e la relativa segmentazione di questi tre campi interconnessi, mostrando il deep learning come un sottoinsieme specializzato dell‚Äôapprendimento automatico, che a sua volta √® un sottoinsieme dell‚ÄôIA.\n\n\n\n\n\n\nFigura¬†3.1: Il diagramma illustra l‚Äôintelligenza artificiale come campo onnicomprensivo che comprende tutti i metodi computazionali che imitano le funzioni cognitive umane. Il Machine learning [apprendimento automatico] √® un sottoinsieme dell‚ÄôIA che include algoritmi in grado di apprendere dai dati. Il deep learning, un ulteriore sottoinsieme del ML, coinvolge specificamente reti neurali in grado di apprendere pattern [schemi] pi√π complessi in grandi volumi di dati. Fonte: NVIDIA.\n\n\n\nCome mostrato nella figura, l‚ÄôIA rappresenta il campo sovraordinato, che comprende tutti i metodi computazionali che imitano le funzioni cognitive umane. Il machine learning √® mostrato come un sottoinsieme dell‚ÄôIA che include algoritmi in grado di apprendere dai dati. Il deep learning, il sottoinsieme pi√π piccolo nel diagramma, coinvolge specificamente reti neurali in grado di apprendere modelli pi√π complessi da grandi volumi di dati.\n\n\n3.1.2 Breve Storia del Deep Learning\nL‚Äôidea del deep learning ha origine nelle prime reti neurali artificiali. Ha vissuto diversi cicli di interesse, a partire dall‚Äôintroduzione del Perceptron negli anni ‚Äô50 (Rosenblatt 1957), seguita dall‚Äôinvenzione degli algoritmi di backpropagation negli anni ‚Äô80 (Rumelhart, Hinton, e Williams 1986).\n\nRosenblatt, Frank. 1957. The perceptron, a perceiving and recognizing automaton Project Para. Cornell Aeronautical Laboratory.\n\nRumelhart, David E., Geoffrey E. Hinton, e Ronald J. Williams. 1986. ¬´Learning representations by back-propagating errors¬ª. Nature 323 (6088): 533‚Äì36. https://doi.org/10.1038/323533a0.\n\nKrizhevsky, Alex, Ilya Sutskever, e Geoffrey E. Hinton. 2012. ¬´ImageNet Classification with Deep Convolutional Neural Networks¬ª. In Advances in Neural Information Processing Systems 25: 26th Annual Conference on Neural Information Processing Systems 2012. Proceedings of a meeting held December 3-6, 2012, Lake Tahoe, Nevada, United States, a cura di Peter L. Bartlett, Fernando C. N. Pereira, Christopher J. C. Burges, L√©on Bottou, e Kilian Q. Weinberger, 1106‚Äì14. https://proceedings.neurips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html.\nIl termine ‚Äúdeep learning‚Äù √® diventato importante negli anni 2000, caratterizzato da progressi nella potenza di calcolo e nell‚Äôaccessibilit√† dei dati. Traguardi importanti includono l‚Äôaddestramento di successo di reti profonde come AlexNet (Krizhevsky, Sutskever, e Hinton 2012) da parte di Geoffrey Hinton, una figura di spicco nell‚Äôintelligenza artificiale, e il rinnovato focus sulle reti neurali come strumenti efficaci per l‚Äôanalisi e la modellazione dei dati.\nIl deep learning ha recentemente registrato una crescita esponenziale, trasformando vari settori. La Figura¬†3.2 illustra questa notevole progressione, evidenziando due tendenze chiave nel settore. Innanzitutto, il grafico mostra che la crescita computazionale ha seguito un modello di raddoppio di 18 mesi dal 1952 al 2010. Questa tendenza ha poi accelerato drasticamente fino a un ciclo di raddoppio di 6 mesi dal 2010 al 2022, indicando un balzo significativo nelle capacit√† computazionali.\nIn secondo luogo, la figura raffigura l‚Äôemergere di modelli su larga scala tra il 2015 e il 2022. Questi modelli sono apparsi da 2 a 3 ordini di grandezza pi√π veloci rispetto alla tendenza generale, seguendo un ciclo di raddoppio di 10 mesi ancora pi√π aggressivo. Questo rapido ridimensionamento delle dimensioni del modello rappresenta un cambiamento di paradigma nelle capacit√† di deep learning.\n\n\n\n\n\n\nFigura¬†3.2: Crescita dei modelli di deep learning.\n\n\n\nMolteplici fattori hanno contribuito a questa impennata, tra cui i progressi nella potenza computazionale, l‚Äôabbondanza di big data e i miglioramenti nei progetti algoritmici. In primo luogo, la crescita delle capacit√† computazionali, in particolare l‚Äôarrivo delle Graphics Processing Units (GPU) [unit√† di elaborazione grafica] e delle Tensor Processing Units (TPU) [unit√† di elaborazione tensoriale] (Jouppi et al. 2017), ha accelerato notevolmente i tempi di training e inferenza dei modelli di apprendimento profondo. Questi miglioramenti hardware hanno consentito la costruzione e il training di reti pi√π complesse e profonde di quanto fosse possibile negli anni precedenti.\n\nJouppi, Norman P., Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal, Raminder Bajwa, Sarah Bates, et al. 2017. ¬´In-Datacenter Performance Analysis of a Tensor Processing Unit¬ª. In Proceedings of the 44th Annual International Symposium on Computer Architecture, 1‚Äì12. ISCA ‚Äô17. New York, NY, USA: ACM. https://doi.org/10.1145/3079856.3080246.\nIn secondo luogo, la rivoluzione digitale ha prodotto una grande quantit√† di big data, offrendo materiale ricco da cui i modelli di deep learning possono imparare e distinguersi in attivit√† quali il riconoscimento di immagini e parlato, la traduzione linguistica e il gioco. I grandi set di dati etichettati sono stati fondamentali per perfezionare e distribuire con successo applicazioni di deep learning in contesti reali.\nInoltre, le collaborazioni e gli sforzi open source hanno alimentato una comunit√† dinamica di ricercatori e professionisti, accelerando i progressi nelle tecniche di deep learning. Innovazioni come il ‚Äúdeep reinforcement learning‚Äù, il ‚Äútransfer learning‚Äù e l‚Äôintelligenza artificiale generativa hanno ampliato la portata di ci√≤ che √® realizzabile col deep learning, aprendo nuove possibilit√† in vari settori, tra cui sanit√†, finanza, trasporti e intrattenimento.\nLe organizzazioni di tutto il mondo riconoscono il potenziale trasformativo del deep learning e investono molto in ricerca e sviluppo per sfruttare le sue capacit√† nel fornire soluzioni innovative, ottimizzare le operazioni e creare nuove opportunit√† di business. Mentre il deep learning continua la sua traiettoria ascendente, √® destinato a ridefinire il modo in cui interagiamo con la tecnologia, migliorando la praticit√†, la sicurezza e la connettivit√† nelle nostre vite.\n\n\n3.1.3 Applicazioni del Deep Learning\nIl deep learning √® ampiamente utilizzato in numerosi settori oggi, con il suo impatto trasformativo evidente in vari settori, come illustrato in Figura¬†3.3. Nella finanza, alimenta la previsione del mercato azionario, la valutazione del rischio e il rilevamento delle frodi, guidando le strategie di investimento e migliorando le decisioni finanziarie. Il marketing sfrutta il deep learning per la segmentazione e la personalizzazione dei clienti, consentendo pubblicit√† altamente mirate e l‚Äôottimizzazione dei contenuti in base all‚Äôanalisi del comportamento dei consumatori. Nella produzione, semplifica i processi e migliora il controllo di qualit√†, consentendo alle aziende di aumentare la produttivit√† e ridurre al minimo gli sprechi. L‚Äôassistenza sanitaria trae vantaggio dal deep learning nella diagnosi, nella pianificazione del trattamento e nel monitoraggio dei pazienti, salvando potenzialmente vite umane attraverso migliori previsioni mediche.\n\n\n\n\n\n\nFigura¬†3.3: Applicazioni, vantaggi e implementazioni del deep learning in vari settori, tra cui finanza, marketing, produzione e assistenza sanitaria. Fonte: Leeway Hertz\n\n\n\nOltre a questi settori principali, il deep learning migliora i prodotti e i servizi di tutti i giorni. Netflix lo utilizza per rafforzare i suoi sistemi di raccomandazione, fornendo agli utenti pi√π raccomandazioni personalizzate. Google ha migliorato notevolmente il suo servizio di traduzione, gestendo ora oltre 100 lingue con maggiore accuratezza, come evidenziato nei suoi recenti progressi. I veicoli autonomi di aziende come Waymo, Cruise e Motional sono diventati realt√† grazie al deep learning nel loro perception system. Inoltre, Amazon impiega il deep learning edge nei dispositivi Alexa per attivit√† come individuazione di parole chiave. Queste applicazioni dimostrano come il machine learning spesso predice ed elabora le informazioni con maggiore accuratezza e velocit√† rispetto agli esseri umani, rivoluzionando vari aspetti della nostra vita quotidiana.\n\n\n3.1.4 Rilevanza per l‚ÄôIA Embedded\nL‚ÄôIA embedded, l‚Äôintegrazione di algoritmi di intelligenza artificiale direttamente nei dispositivi hardware, trae naturalmente vantaggio dalle capacit√† del deep learning. La combinazione di algoritmi di deep learning e sistemi embedded ha gettato le basi per dispositivi intelligenti e autonomi in grado di analisi avanzate on-device [sul dispositivo]. Il deep learning aiuta a estrarre pattern e informazioni complesse dai dati di input, il che √® essenziale nello sviluppo di sistemi embedded intelligenti, dagli elettrodomestici ai macchinari industriali. Questa collaborazione inaugura una nuova era di dispositivi intelligenti e interconnessi, in grado di apprendere e adattarsi al comportamento dell‚Äôutente e alle condizioni ambientali, ottimizzando le prestazioni e offrendo praticit√† ed efficienza senza precedenti.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Avvio al Deep Learning</span>"
    ]
  },
  {
    "objectID": "contents/core/dl_primer/dl_primer.it.html#reti-neurali",
    "href": "contents/core/dl_primer/dl_primer.it.html#reti-neurali",
    "title": "3¬† Avvio al Deep Learning",
    "section": "3.2 Reti Neurali",
    "text": "3.2 Reti Neurali\nIl deep learning trae ispirazione dalle reti neurali del cervello umano per creare pattern decisionali. Questa sezione approfondisce i concetti fondamentali del deep learning, offrendo approfondimenti sugli argomenti pi√π complessi trattati pi√π avanti in questa introduzione.\nLe reti neurali fungono da fondamento del deep learning, ispirate alle reti neurali biologiche nel cervello umano per elaborare e analizzare i dati in modo gerarchico. Le reti neurali sono composte da unit√† di base chiamate perceptron, che sono solitamente organizzate in layer [strati]. Ogni layer √® costituito da diversi perceptron e pi√π layer sono impilati per formare l‚Äôintera rete. Le connessioni tra questi layer sono definite da insiemi di pesi o parametri che determinano come i dati vengono elaborati mentre fluiscono dall‚Äôinput all‚Äôoutput della rete.\nDi seguito, esaminiamo i componenti e le strutture primarie nelle reti neurali.\n\n3.2.1 Perceptron\nIl Perceptron √® l‚Äôunit√† di base o il nodo che costituisce la base per strutture pi√π complesse. Funziona prendendo pi√π input, ognuno dei quali rappresenta una feature dell‚Äôoggetto in analisi, come le caratteristiche di una casa per prevederne il prezzo o gli attributi di una canzone per prevederne la popolarit√† nei servizi di streaming musicale. Questi input sono indicati come \\(x_1, x_2, ..., x_n\\). Un perceptron pu√≤ essere configurato per eseguire attivit√† di regressione o classificazione. Per la regressione, viene utilizzato l‚Äôoutput numerico effettivo \\(\\hat{y}\\). Per la classificazione, l‚Äôoutput dipende dal fatto che \\(\\hat{y}\\) superi una determinata soglia. Se \\(\\hat{y}\\) supera questa soglia, il perceptron potrebbe restituire una classe (ad esempio, ‚Äòyes‚Äô) e, in caso contrario, un‚Äôaltra classe (ad esempio, ‚Äòno‚Äô).\nFigura¬†3.4 illustra gli elementi fondamentali di un perceptron, che funge da fondamento per reti neurali pi√π complesse. Un perceptron pu√≤ essere pensato come un decisore in miniatura, che utilizza i suoi pesi, il sui bias [polarizzazione] e la sua funzione di attivazione per elaborare input e generare output in base ai parametri appresi. Questo concetto costituisce la base per comprendere architetture di reti neurali pi√π complesse, come i perceptron multilayer [multistrato]. In queste strutture avanzate, i layer di perceptron lavorano di concerto, con l‚Äôoutput di ogni layer che funge da input per il layer successivo. Questa disposizione gerarchica crea un modello di deep learning in grado di comprendere e modellare pattern complessi e astratti all‚Äôinterno dei dati. Impilando queste semplici unit√†, le reti neurali acquisiscono la capacit√† di affrontare attivit√† sempre pi√π sofisticate, dal riconoscimento delle immagini all‚Äôelaborazione del linguaggio naturale.\n\n\n\n\n\n\nFigura¬†3.4: Perceptron. Concepiti negli anni ‚Äô50, i perceptron hanno aperto la strada allo sviluppo di reti neurali pi√π complesse e sono stati un elemento fondamentale nel deep learning. Fonte: Wikimedia - Chrislb.\n\n\n\nCiascun input \\(x_i\\) ha un peso corrispondente \\(w_{ij}\\) e il perceptron moltiplica semplicemente ogni input per il suo peso corrispondente. Questa operazione √® simile alla regressione lineare, dove l‚Äôoutput intermedio, \\(z\\), √® calcolato come la somma dei prodotti degli input e dei loro pesi:\n\\[\nz = \\sum (x_i \\cdot w_{ij})\n\\]\nA questo calcolo intermedio, viene aggiunto un termine di bias \\(b\\), che consente al modello di adattarsi meglio ai dati spostando la funzione di output lineare verso l‚Äôalto o verso il basso. Pertanto, la combinazione lineare intermedia calcolata dal perceptron, incluso il bias, diventa:\n\\[\nz = \\sum (x_i \\cdot w_{ij}) + b\n\\]\nQuesta forma base di un perceptron pu√≤ modellare solo relazioni lineari tra input e output. I pattern trovati in natura sono spesso complessi e si estendono oltre le relazioni lineari. Per consentire al perceptron di gestire relazioni non lineari, una funzione di attivazione viene applicata all‚Äôoutput lineare \\(z\\).\n\\[\n\\hat{y} = \\sigma(z)\n\\]\nFigura¬†3.5 illustra un esempio in cui i dati presentano un pattern non lineare che non potrebbe essere modellato adeguatamente con un approccio lineare. La funzione di attivazione, come la sigmoide, la tanh o la ReLU, trasforma la somma di input lineare in un output non lineare. L‚Äôobiettivo principale di questa funzione √® introdurre la non linearit√† nel modello, consentendogli di apprendere ed eseguire attivit√† pi√π sofisticate. Pertanto, l‚Äôoutput finale del perceptron, inclusa la funzione di attivazione, pu√≤ essere espresso come:\n\n\n\n\n\n\nFigura¬†3.5: Le funzioni di attivazione consentono la modellazione di relazioni non lineari complesse. Fonte: Medium - Sachin Kaushik.\n\n\n\n\n\n3.2.2 Perceptron Multilayer\nI ‚ÄúMultilayer perceptron‚Äù (MLP) sono un‚Äôevoluzione del modello del perceptron a singolo layer, caratterizzato da pi√π layer di nodi collegati in modo ‚Äúfeedforward‚Äù. Figura¬†3.6 fornisce una rappresentazione visiva di questa struttura. Come illustrato nella figura, le informazioni in una rete ‚Äúfeedforward‚Äù si muovono in una sola direzione: dal livello di input a sinistra, attraverso i livelli nascosti al centro, fino al livello di output a destra, senza cicli o loop.\n\n\n\n\n\n\nFigura¬†3.6: Perceptron Multilayer. Fonte: Wikimedia - Charlie.\n\n\n\nMentre un singolo perceptron √® limitato nella sua capacit√† di modellare pattern complessi, la vera forza delle reti neurali emerge dall‚Äôassemblaggio di pi√π layer. Ciascun layer √® costituito da numerosi perceptron che lavorano insieme, consentendo alla rete di catturare relazioni intricate e non lineari all‚Äôinterno dei dati. Con sufficiente profondit√† e ampiezza, queste reti possono approssimare praticamente qualsiasi funzione, indipendentemente da quanto sia complessa.\n\n\n3.2.3 Processo di Training\nUna rete neurale riceve un input, esegue un calcolo e produce una previsione. La previsione √® determinata dai calcoli eseguiti all‚Äôinterno dei set di perceptron trovati tra i layer di input e output. Questi calcoli dipendono principalmente dall‚Äôinput e dai pesi. Poich√© non si ha il controllo sull‚Äôinput, l‚Äôobiettivo durante il training [addestramento] √® quello di regolare i pesi in modo tale che l‚Äôoutput della rete fornisca la previsione pi√π accurata.\nIl processo di addestramento prevede diversi passaggi chiave, a partire dal passaggio in avanti (forward), in cui i pesi esistenti della rete vengono utilizzati per calcolare l‚Äôoutput per un dato input. Questo output viene poi confrontato con i veri valori target per calcolare un errore, che misura quanto bene la previsione della rete corrisponde al risultato previsto. In seguito, viene eseguito un passaggio all‚Äôindietro (backward). Ci√≤ comporta l‚Äôutilizzo dell‚Äôerrore per apportare modifiche ai pesi della rete tramite un processo chiamato ‚Äúbackpropagation‚Äù. Questa regolazione riduce l‚Äôerrore nelle previsioni successive. Il ciclo di passaggio forward [in avanti], calcolo dell‚Äôerrore e passaggio backward [all‚Äôindietro] viene ripetuto iterativamente. Questo processo continua finch√© le previsioni della rete non sono sufficientemente accurate o non viene raggiunto un numero predefinito di iterazioni, riducendo al minimo la ‚Äúfunzione di perdita‚Äù utilizzata per misurare l‚Äôerrore.\n\nForward Pass\nIl forward pass √® la fase iniziale in cui i dati si spostano attraverso la rete dal livello di input a quello di output, come illustrato in Figura¬†3.7. All‚Äôinizio dell‚Äôaddestramento, i pesi della rete vengono inizializzati in modo casuale, impostando le condizioni iniziali. Durante il ‚Äúforward pass‚Äù, ogni layer esegue calcoli specifici sui dati di input utilizzando questi pesi e il bias, e i risultati vengono poi passati al layer successivo. L‚Äôoutput finale di questa fase √® la previsione della rete. Questa ‚Äúprediction‚Äù viene confrontata con i valori target effettivi presenti nel set di dati per calcolare la ‚Äúloss‚Äù [perdita], che pu√≤ essere considerata come la differenza tra gli output previsti e i valori target. La perdita quantifica le prestazioni della rete in questa fase, fornendo una metrica cruciale per la successiva regolazione dei pesi durante il backward pass.\n\n\n\n\n\n\nFigura¬†3.7: Reti neurali: propagazione forward e backward. Fonte: Linkedin\n\n\n\n\n\nBackward Pass (Backpropagation)\nDopo aver completato il forward pass e calcolato la perdita, che misura quanto le previsioni del modello si discostano dai valori target effettivi, il passo successivo √® migliorare le prestazioni del modello regolando i pesi della rete. Poich√© non possiamo controllare gli input del modello, la regolazione dei pesi diventa il nostro metodo principale per perfezionare il modello.\nDeterminiamo come regolare i pesi del nostro modello tramite un algoritmo chiave chiamato ‚Äúbackpropagation‚Äù. La backpropagation utilizza la perdita calcolata per determinare il gradiente di ciascun peso. Questi gradienti descrivono la direzione e l‚Äôentit√† in cui i pesi devono essere regolati. Regolando i pesi in base a questi gradienti, il modello √® meglio posizionato per fare previsioni pi√π vicine ai valori target effettivi nel successivo ‚Äúforward pass‚Äù.\nComprendere questi concetti fondamentali apre la strada alla comprensione di architetture e tecniche di deep learning pi√π complesse, favorendo lo sviluppo di applicazioni pi√π sofisticate e produttive, in particolare all‚Äôinterno di sistemi di intelligenza artificiale embedded.\nVideo¬†3.1 and Video¬†3.2 build upon Video¬†3.3. Riguardano la ‚Äúgradient descent‚Äù [discesa del gradiente] e la backpropagation nelle reti neurali.\n\n\n\n\n\n\nVideo¬†3.1: Gradient descent\n\n\n\n\n\n\n\n\n\n\n\n\nVideo¬†3.2: Backpropagation\n\n\n\n\n\n\n\n\n\n3.2.4 Architetture dei Modelli\nLe architetture di deep learning si riferiscono ai vari approcci strutturati che stabiliscono come i neuroni e i layer sono organizzati e interagiscono nelle reti neurali. Queste architetture si sono evolute per affrontare efficacemente diversi problemi e diversi tipi di dati. Questa sezione fornisce una panoramica di alcune note architetture di deep learning e delle loro caratteristiche.\n\nMultilayer Perceptron (MLP)\nGli MLP sono architetture di deep learning di base che comprendono tre layer: uno di input, uno o pi√π layer nascosti e un layer di output. Questi layer sono completamente connessi, il che significa che ogni neurone in uno layer √® collegato a ogni neurone nei layer precedenti e successivi. Gli MLP possono modellare funzioni complesse e sono utilizzati in varie attivit√†, come regressione, classificazione e riconoscimento di pattern. La loro capacit√† di apprendere relazioni non lineari tramite backpropagation li rende uno strumento versatile nel toolkit di deep learning.\nNei sistemi di intelligenza artificiale embedded, gli MLP possono funzionare come modelli compatti per attivit√† pi√π semplici come l‚Äôanalisi dei dati dei sensori o il riconoscimento di pattern di base, in cui le risorse computazionali sono limitate. La loro capacit√† di apprendere relazioni non lineari con una complessit√† relativamente minore li rende una scelta adatta per i sistemi embedded.\n\n\n\n\n\n\nEsercizio¬†3.1: Multilayer Perceptron (MLP)\n\n\n\n\n\nAbbiamo appena scalfito la superficie delle reti neurali. Ora, proveremo ad applicare questi concetti in esempi pratici. Nei notebook Colab forniti, si esploreranno:\nPrevisione dei prezzi delle case: Scoprire come le reti neurali possono analizzare i dati sugli alloggi per stimare i valori delle propriet√†. \nClassificazione delle immagini: Scoprire come creare una rete per comprendere il famoso set di dati di cifre scritte a mano MNIST. \nDiagnosi medica nel mondo reale: Usare il deep learning per affrontare l‚Äôimportante compito della classificazione del cancro al seno. \n\n\n\n\n\nConvolutional Neural Networks (CNNs)\nLe CNN [reti neurali convoluzionali] sono utilizzate principalmente in attivit√† di riconoscimento di immagini e video. Questa architettura √® composta da due parti principali: la base convoluzionale e i layer completamente connessi. Nella base convoluzionale, i layer convoluzionali filtrano i dati di input per identificare feature come bordi, angoli e texture [trame]. Dopo ogni layer convoluzionale, √® possibile applicare un layer di pooling [raggruppamento] per ridurre le dimensioni spaziali dei dati, diminuendo cos√¨ il carico computazionale e concentrando le feature estratte. A differenza degli MLP, che trattano le feature di input come entit√† piatte e indipendenti, le CNN mantengono le relazioni spaziali tra i pixel, rendendole particolarmente efficaci per i dati di immagini e video. Le feature estratte dalla base convoluzionale vengono poi passate ai layer completamente connessi, simili a quelli utilizzati negli MLP, che eseguono la classificazione in base alle feature estratte dai layer di convoluzione. Le CNN si sono dimostrate altamente efficaci nel riconoscimento delle immagini, nel rilevamento di oggetti e in altre applicazioni di visione artificiale.\nVideo¬†3.3 spiega come funzionano le reti neurali usando il riconoscimento di cifre scritte a mano come applicazione di esempio. Affronta anche la matematica alla base delle reti neurali.\n\n\n\n\n\n\nVideo¬†3.3: Reti MLP & CNN\n\n\n\n\n\n\nLe CNN sono fondamentali per le attivit√† di riconoscimento di immagini e video, in cui spesso √® necessaria l‚Äôelaborazione in tempo reale. Possono essere ottimizzate per i sistemi embedded utilizzando tecniche come la quantizzazione e il ‚Äúpruning‚Äù [potatura] per ridurre al minimo l‚Äôutilizzo della memoria e le richieste computazionali, consentendo funzionalit√† efficienti di rilevamento di oggetti e riconoscimento facciale in dispositivi con risorse computazionali limitate.\n\n\n\n\n\n\nEsercizio¬†3.2: Convolutional Neural Networks (CNNs)\n\n\n\n\n\nAbbiamo discusso del fatto che le CNN [Reti neurali convoluzionali] sono eccellenti nell‚Äôidentificare le caratteristiche delle immagini, il che le rende ideali per attivit√† come la classificazione degli oggetti. Ora, si potr√† mettere in pratica questa conoscenza! Questo notebook Colab si concentra sulla creazione di una CNN per classificare le immagini dal set di dati CIFAR-10, che include oggetti come aeroplani, automobili e animali. Si impareranno le principali differenze tra CIFAR-10 e il set di dati MNIST che abbiamo esplorato in precedenza e come queste differenze influenzano la scelta del modello. Alla fine di questo notebook si avr√† una conoscenza delle CNN per il riconoscimento delle immagini.\n\n\n\n\n\n\nRecurrent Neural Networks (RNN)\nLe RNN [Reti Neurali Ricorrenti] sono adatte per l‚Äôanalisi di dati sequenziali, come la previsione di serie temporali e l‚Äôelaborazione del linguaggio naturale. In questa architettura, le connessioni tra i nodi formano un grafo diretto lungo una sequenza temporale, consentendo il trasporto delle informazioni attraverso le sequenze tramite vettori di stato nascosti. Le varianti delle RNN includono le Long Short-Term Memory (LSTM) e le Gated Recurrent Units (GRU), progettate per catturare dipendenze pi√π lunghe nei dati sequenziali.\nQueste reti possono essere utilizzate nei sistemi di riconoscimento vocale, nella manutenzione predittiva o nei dispositivi IoT in cui sono comuni i pattern di dati sequenziali. Le ottimizzazioni specifiche per le piattaforme embedded possono aiutare a gestirne i requisiti di elaborazione e memoria tipicamente elevati.\n\n\nGenerative Adversarial Network (GAN)\nLe GAN [Reti Generative Avversarie] sono costituite da due reti, un generatore e un discriminatore, addestrate simultaneamente tramite l‚Äôaddestramento adversarial [avversario] (Goodfellow et al. 2020). Il generatore produce dati che tentano di imitare la distribuzione di quelli reali, mentre il discriminatore distingue tra dati reali e dati generati. Le GAN sono ampiamente utilizzate nella generazione di immagini, nel trasferimento di stile e nell‚Äôaumento dei dati.\n\nGoodfellow, Ian, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, e Yoshua Bengio. 2020. ¬´Generative adversarial networks¬ª. Commun. ACM 63 (11): 139‚Äì44. https://doi.org/10.1145/3422622.\nIn contesti embedded, le reti GAN potrebbero essere utilizzate per l‚Äôaumento dei dati sul dispositivo per migliorare il training dei modelli direttamente sul dispositivo embedded, consentendo un apprendimento continuo e un adattamento ai nuovi dati senza la necessit√† di risorse di cloud computing.\n\n\nAutoencoder\nGli autoencoder sono reti neurali per la compressione dei dati e la riduzione del rumore (Bank, Koenigstein, e Giryes 2023). Sono strutturati per codificare i dati di input in una rappresentazione a dimensione inferiore e quindi decodificarli nella loro forma originale. Varianti come gli Variational Autoencoders (VAE) [Autoencoder Variazionali] introducono livelli probabilistici che consentono propriet√† generative, trovando applicazioni nella generazione di immagini e nel rilevamento di anomalie.\n\nBank, Dor, Noam Koenigstein, e Raja Giryes. 2023. ¬´Autoencoders¬ª. Machine Learning for Data Science Handbook: Data Mining and Knowledge Discovery Handbook, 353‚Äì74.\nL‚Äôuso degli autoencoder pu√≤ aiutare nella trasmissione e nell‚Äôarchiviazione efficiente dei dati, migliorando le prestazioni complessive dei sistemi embedded con risorse di calcolo e di memoria limitate.\n\n\nTransformer Network\nLe ‚ÄúTransformer network‚Äù [reti di trasformatori] sono emerse come un‚Äôarchitettura potente, specialmente nell‚Äôelaborazione del linguaggio naturale (Vaswani et al. 2017). Queste reti utilizzano meccanismi di auto-attenzione per soppesare l‚Äôinfluenza di diverse parole di input su ogni parola di output, consentendo il calcolo parallelo e catturando pattern intricati nei dati. Le reti di trasformatori hanno portato a risultati all‚Äôavanguardia in attivit√† come la traduzione linguistica, la sintesi e la generazione di testo.\n\nVaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz Kaiser, e Illia Polosukhin. 2017. ¬´Attention is all you need¬ª. Adv Neural Inf Process Syst 30.\nQueste reti possono essere ottimizzate per eseguire attivit√† correlate alla lingua direttamente sul dispositivo. Ad esempio, i trasformatori possono essere utilizzati nei sistemi embedded per servizi di traduzione in tempo reale o interfacce assistite dalla voce, dove latenza ed efficienza computazionale sono cruciali. Tecniche come la distillazione del modello possono essere impiegate per distribuire queste reti su dispositivi embedded con risorse limitate.\nQueste architetture servono a scopi specifici ed eccellono in diversi domini, offrendo un ricco toolkit per affrontare diversi problemi nei sistemi di intelligenza artificiale embedded. Comprendere le sfumature di queste architetture √® fondamentale nella progettazione di modelli di deep learning efficaci ed efficienti per varie applicazioni.\n\n\n\n3.2.5 ML Tradizionale vs Deep Learning\nIl deep learning estende il machine learning tradizionale utilizzando reti neurali per discernere i pattern nei dati. Al contrario, il machine learning tradizionale si basa su un set di algoritmi consolidati come alberi decisionali, k-nearest neighbor e macchine a vettori di supporto, ma non coinvolge le reti neurali. Figura¬†3.8 fornisce un confronto visivo tra Machine Learning e Deep Learning, evidenziandone le principali differenze di approccio e capacit√†.\n\n\n\n\n\n\nFigura¬†3.8: Confronto tra Machine Learning e Deep Learning. Fonte: Medium\n\n\n\nCome mostrato nella figura, i modelli di deep learning possono elaborare dati grezzi direttamente ed estrarre automaticamente le feature rilevanti, mentre il machine learning tradizionale richiede spesso l‚Äôingegneria manuale delle feature. La figura illustra anche come i modelli di deep learning possono gestire attivit√† pi√π complesse e set di dati pi√π grandi rispetto ai tradizionali approcci di machine learning.\nPer evidenziare ulteriormente le differenze, Tabella¬†3.1 fornisce un confronto pi√π dettagliato delle caratteristiche contrastanti tra ML tradizionale e deep learning. Questa tabella integra la rappresentazione visiva in Figura¬†3.8 offrendo punti di confronto specifici tra vari aspetti di questi due approcci.\n\n\n\nTabella¬†3.1: Confronto tra machine learning tradizionale e deep learning.\n\n\n\n\n\n\n\n\n\n\nAspetto\nML tradizionale\nDeep Learning\n\n\n\n\nRequisiti dei dati\nDa basso a moderato (efficiente con set di dati pi√π piccoli)\nAlto (richiede set di dati di grandi dimensioni per un apprendimento adeguato)\n\n\nComplessit√† del modello\nModerata (adatta a problemi ben definiti)\nAlta (rileva pattern intricati, adatta a compiti complessi)\n\n\nRisorse di calcolo\nDa basse a moderate (economiche, meno dispendiose in termini di risorse)\nAlta (richiede una potenza di calcolo e risorse sostanziali)\n\n\nVelocit√† di distribuzione\nVeloce (cicli di training e distribuzione pi√π rapidi)\nLento (tempi di training pi√π lunghi, in particolare con set di dati pi√π grandi)\n\n\nInterpretabilit√†\nAlta (chiare intuizioni sui percorsi decisionali)\nBassa (strutture complesse a layer, natura ‚Äúscatola nera‚Äù)\n\n\nManutenzione\nPi√π facile (semplice da aggiornare e mantenere)\nComplesso (richiede pi√π sforzi nella manutenzione e negli aggiornamenti)\n\n\n\n\n\n\n\n\n3.2.6 Scelta tra ML tradizionale e DL\n\nDisponibilit√† e Volume dei Dati\nQuantit√† di Dati: Gli algoritmi di machine learning tradizionali, come gli alberi decisionali o Naive Bayes, sono spesso pi√π adatti quando la disponibilit√† dei dati √® limitata. Offrono previsioni affidabili anche con set di dati pi√π piccoli. Ci√≤ √® particolarmente vero nella diagnostica medica per la previsione delle malattie e nella segmentazione dei clienti nel marketing.\nDiversit√† e Qualit√† dei Dati: Gli algoritmi di machine learning tradizionali spesso funzionano bene con dati strutturati (l‚Äôinput del modello √® un set di funzionalit√†, idealmente indipendenti l‚Äôuna dall‚Äôaltra) ma possono richiedere un notevole sforzo di pre-elaborazione (ad esempio, la ‚Äúfeature engineering‚Äù [progettazione delle funzionalit√†]). D‚Äôaltro canto, il deep learning adotta l‚Äôapproccio di eseguire automaticamente la progettazione delle funzionalit√† come parte dell‚Äôarchitettura del modello. Questo approccio consente la costruzione di modelli end-to-end in grado di mappare direttamente da dati di input non strutturati (come testo, audio e immagini) all‚Äôoutput desiderato senza fare affidamento su euristiche semplicistiche con efficacia limitata. Tuttavia, ci√≤ si traduce in modelli pi√π grandi che richiedono pi√π dati e risorse computazionali. Nei dati rumorosi, la necessit√† di set di dati pi√π grandi √® ulteriormente enfatizzata quando si utilizza il Deep Learning.\n\n\nComplessit√† del Problema\nGranularit√† del Problema: I problemi che sono semplici o moderatamente complessi, che possono coinvolgere relazioni lineari o polinomiali tra variabili, spesso trovano una migliore aderenza ai metodi tradizionali di apprendimento automatico.\nRappresentazione Gerarchica delle Feature: I modelli di deep learning sono eccellenti in attivit√† che richiedono una rappresentazione gerarchica delle feature [caratteristiche], come il riconoscimento di immagini e voce. Tuttavia, non tutti i problemi richiedono questa complessit√† e gli algoritmi tradizionali di apprendimento automatico possono talvolta offrire soluzioni pi√π semplici e ugualmente efficaci.\n\n\nRisorse Hardware e Computazionali\nVincoli di Risorse: La disponibilit√† di risorse computazionali spesso influenza la scelta tra ML tradizionale e deep learning. Il primo √® generalmente meno dispendioso in termini di risorse e quindi preferibile in ambienti con limitazioni hardware o vincoli di budget.\nScalabilit√† e Velocit√†: Gli algoritmi tradizionali di apprendimento automatico, come le Support Vector Machines (SVM) [macchine a vettori di supporto ], spesso consentono tempi di training pi√π rapidi e una scalabilit√† pi√π semplice, il che √® particolarmente vantaggioso nei progetti con tempistiche ristrette e volumi di dati in crescita.\n\n\nNormativa di Conformit√†\nLa conformit√† normativa √® fondamentale in vari settori, e richiede l‚Äôaderenza a linee guida e ‚Äúbest practice‚Äù come il General Data Protection Regulation (GDPR) [Regolamento generale sulla protezione dei dati] nell‚ÄôUE. I modelli ML tradizionali, grazie alla loro intrinseca interpretabilit√†, spesso si allineano meglio a queste normative, soprattutto in settori come la finanza e l‚Äôassistenza sanitaria.\n\n\nInterpretabilit√†\nComprendere il processo decisionale √® pi√π facile con le tecniche tradizionali di apprendimento automatico rispetto ai modelli di deep learning, che funzionano come ‚Äúscatole nere‚Äù, rendendo difficile tracciare i percorsi decisionali.\n\n\n\n3.2.7 Fare una Scelta Informata\nConsiderati i vincoli dei sistemi di intelligenza artificiale embedded, comprendere le differenze tra le tecniche di ML tradizionali e il deep learning diventa essenziale. Entrambe le strade offrono vantaggi unici e le loro caratteristiche distintive spesso determinano la scelta dell‚Äôuna rispetto all‚Äôaltra in diversi scenari.\nNonostante ci√≤, il deep learning ha costantemente superato i metodi tradizionali di apprendimento automatico in diverse aree chiave grazie all‚Äôabbondanza di dati, ai progressi computazionali e alla comprovata efficacia in attivit√† complesse. Ecco alcuni motivi specifici per cui ci concentriamo sul deep learning:\n\nPrestazioni Superiori in Attivit√† Complesse: I modelli di deep learning, in particolare le reti neurali profonde, eccellono in attivit√† in cui le relazioni tra i punti dati sono incredibilmente intricate. Attivit√† come il riconoscimento di immagini e parlato, la traduzione linguistica e la riproduzione di giochi complessi come Go e Scacchi hanno visto progressi significativi principalmente attraverso algoritmi di deep learning.\nGestione Efficiente dei Dati non Strutturati: A differenza dei metodi tradizionali di apprendimento automatico, il deep learning pu√≤ elaborare in modo pi√π efficace i dati non strutturati. Ci√≤ √® fondamentale nel panorama dei dati odierno, in cui la stragrande maggioranza dei dati, come testo, immagini e video, non √® strutturata.\nSfruttamento dei Big Data: Con la disponibilit√† dei Big Data, i modelli di deep learning possono apprendere e migliorare continuamente. Questi modelli eccellono nell‚Äôutilizzare grandi set di dati per migliorare la loro accuratezza predittiva, un limite degli approcci tradizionali di machine-learning.\nProgressi Hardware e Calcolo Parallelo: L‚Äôavvento di potenti GPU e la disponibilit√† di piattaforme di cloud computing hanno consentito il rapido training di modelli di deep learning. Questi progressi hanno affrontato una delle sfide significative del deep learning: la necessit√† di risorse computazionali sostanziali.\nAdattabilit√† Dinamica e Apprendimento Continuo: I modelli di deep learning possono adattarsi dinamicamente a nuove informazioni o dati. Possono essere addestrati per generalizzare il loro apprendimento a nuovi dati mai visti, cruciali in campi in rapida evoluzione come la guida autonoma o la traduzione linguistica in tempo reale.\n\nSebbene il deep learning abbia guadagnato una notevole popolarit√†, √® essenziale comprendere che il machine learning tradizionale √® ancora rilevante. Man mano che ci addentriamo nei meandri del deep learning, evidenzieremo anche le situazioni in cui i metodi tradizionali di machine learning potrebbero essere pi√π appropriati, grazie alla loro semplicit√†, efficienza e interpretabilit√†. Concentrandoci in questo testo sul deep learning, intendiamo fornire ai lettori le conoscenze e gli strumenti per affrontare problemi moderni e complessi in vari ambiti, fornendo al contempo approfondimenti sui vantaggi comparativi e sugli scenari applicativi appropriati per il deep learning e le tecniche tradizionali di machine learning.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Avvio al Deep Learning</span>"
    ]
  },
  {
    "objectID": "contents/core/dl_primer/dl_primer.it.html#conclusione",
    "href": "contents/core/dl_primer/dl_primer.it.html#conclusione",
    "title": "3¬† Avvio al Deep Learning",
    "section": "3.3 Conclusione",
    "text": "3.3 Conclusione\nIl deep learning √® diventato un potente set di tecniche per affrontare le complesse sfide del riconoscimento di pattern e della previsione. Iniziando con una panoramica, abbiamo delineato i concetti e i principi fondamentali che governano il deep learning, gettando le basi per studi pi√π avanzati.\nAl centro del deep learning, abbiamo esplorato le idee di base delle reti neurali, potenti modelli computazionali ispirati alla struttura neuronale interconnessa del cervello umano. Questa esplorazione ci ha permesso di apprezzare le capacit√† e il potenziale delle reti neurali nella creazione di algoritmi sofisticati in grado di apprendere e adattarsi dai dati.\nComprendere il ruolo delle librerie e dei framework √® stata una parte fondamentale della nostra discussione. Abbiamo offerto approfondimenti sugli strumenti che possono facilitare lo sviluppo e l‚Äôimplementazione di modelli di deep learning. Queste risorse semplificano l‚Äôimplementazione delle reti neurali e aprono strade all‚Äôinnovazione e all‚Äôottimizzazione.\nSuccessivamente, abbiamo affrontato le sfide che si potrebbero incontrare quando si racchiudono algoritmi di deep learning nei sistemi embedded, fornendo una prospettiva critica sulle complessit√† e sulle considerazioni relative all‚Äôintroduzione dell‚Äôintelligenza artificiale nei dispositivi edge.\nInoltre, abbiamo esaminato i limiti del deep learning. Attraverso le discussioni, abbiamo svelato le sfide affrontate nelle applicazioni del deep learning e delineato scenari in cui l‚Äôapprendimento automatico tradizionale potrebbe superare il deep learning. Queste sezioni sono fondamentali per promuovere una visione equilibrata delle capacit√† e dei limiti del deep learning.\nIn questo ‚ÄúAvviamento‚Äù, abbiamo fornito le conoscenze per fare scelte informate tra l‚Äôimplementazione dell‚Äôapprendimento automatico tradizionale o delle tecniche di deep learning, a seconda delle esigenze e dei vincoli unici di un problema specifico.\nConcludendo questo capitolo, ci auguriamo che sia stato acquisito il ‚Äúlinguaggio‚Äù di base del deep learning e si sia pronti ad approfondire i capitoli successivi con una solida comprensione e una prospettiva critica. Il viaggio che √® pieno di entusiasmanti opportunit√† e sfide nel racchiudere l‚Äôintelligenza artificiale nei sistemi.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Avvio al Deep Learning</span>"
    ]
  },
  {
    "objectID": "contents/core/dl_primer/dl_primer.it.html#sec-deep-learning-primer-resource",
    "href": "contents/core/dl_primer/dl_primer.it.html#sec-deep-learning-primer-resource",
    "title": "3¬† Avvio al Deep Learning",
    "section": "3.4 Risorse",
    "text": "3.4 Risorse\nEcco un elenco curato di risorse per supportare studenti e insegnanti nei loro percorsi di apprendimento e insegnamento. Stiamo lavorando costantemente per espandere questa raccolta e presto aggiungeremo nuovi esercizi.\n\n\n\n\n\n\nSlide\n\n\n\n\n\nQueste slide sono uno strumento prezioso per gli insegnanti per tenere lezioni e per gli studenti per rivedere il materiale secondo il proprio ritmo. Incoraggiamo studenti e docenti a sfruttare queste slide per migliorare la loro comprensione e facilitare un trasferimento efficace delle conoscenze.\n\nPast, Present, and Future of ML.\nThinking About Loss.\nMinimizing Loss.\nFirst Neural Network.\nUnderstanding Neurons.\nIntro to CLassification.\nTraining, Validation, and Test Data.\nIntro to Convolutions.\n\n\n\n\n\n\n\n\n\n\nVideo\n\n\n\n\n\n\nVideo¬†3.3\nVideo¬†3.1\nVideo¬†3.2\n\n\n\n\n\n\n\n\n\n\nEsercizi\n\n\n\n\n\nPer rafforzare i concetti trattati in questo capitolo, abbiamo curato una serie di esercizi che sfidano gli studenti ad applicare le proprie conoscenze e ad approfondire la propria comprensione.\n\nEsercizio¬†3.1\nEsercizio¬†3.2",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Avvio al Deep Learning</span>"
    ]
  },
  {
    "objectID": "contents/core/workflow/workflow.it.html",
    "href": "contents/core/workflow/workflow.it.html",
    "title": "4¬† Workflow dell‚ÄôIA",
    "section": "",
    "text": "4.1 Panoramica\nFigura¬†4.1 illustra il flusso di lavoro sistematico necessario per sviluppare un modello di machine learning di successo. Questo processo end-to-end, comunemente denominato ciclo di vita del machine learning, consente di creare, distribuire e gestire i modelli in modo efficace. Solitamente comporta i seguenti passaggi chiave:\nSeguire questo flusso di lavoro ML strutturato ci guida attraverso le fasi chiave dello sviluppo. Garantisce di creare modelli efficaci e robusti pronti per la distribuzione nel mondo reale, con conseguenti modelli di qualit√† superiore che risolvono le varie esigenze.\nIl flusso di lavoro ML √® iterativo, richiede un monitoraggio continuo e potenziali aggiustamenti. Ulteriori considerazioni includono:",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Workflow dell'IA</span>"
    ]
  },
  {
    "objectID": "contents/core/workflow/workflow.it.html#panoramica",
    "href": "contents/core/workflow/workflow.it.html#panoramica",
    "title": "4¬† Workflow dell‚ÄôIA",
    "section": "",
    "text": "Figura¬†4.1: Metodologia di progettazione multi-step per lo sviluppo di un modello di machine learning. Comunemente denominato ciclo di vita del machine learning\n\n\n\n\n\nDefinizione del Problema - Si inizia articolando chiaramente il problema specifico da risolvere. Questo si concentra sui problemi durante la raccolta dati e la creazione del modello.\nRaccolta e Preparazione dei Dati: Raccogliere dati di training pertinenti e di alta qualit√† che catturino tutti gli aspetti del problema. Pulire e pre-elaborare i dati per prepararli alla modellazione.\nSelezione e Training del Modello: Scegliere un algoritmo di apprendimento automatico adatto al tipo di problema e ai dati. Considerare i pro e i contro dei diversi approcci. Inserire i dati preparati nel modello per addestrarlo. Il tempo di addestramento varia in base alle dimensioni dei dati e alla complessit√† del modello.\nValutazione del Modello: Testare il modello addestrato su nuovi dati non ancora esaminati per misurarne l‚Äôaccuratezza predittiva. Identificare eventuali limitazioni.\nDistribuzione del Modello: Integrare il modello convalidato in applicazioni o sistemi per avviarne l‚Äôoperativit√†.\nMonitoraggio e Manutenzione: Tenere traccia delle prestazioni del modello in produzione. Ri-addestrare periodicamente su nuovi dati per mantenerli aggiornati.\n\n\n\n\nControllo della Versione: Tenere traccia delle modifiche al codice e ai dati per riprodurre i risultati e ripristinare le versioni precedenti se necessario.\nDocumentazione: Mantenere una documentazione dettagliata per la comprensione e la riproduzione del flusso di lavoro.\nTest: Testare rigorosamente il flusso di lavoro per garantirne la funzionalit√†.\nSicurezza: Proteggere il flusso di lavoro e i dati quando si distribuiscono modelli in contesti di produzione.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Workflow dell'IA</span>"
    ]
  },
  {
    "objectID": "contents/core/workflow/workflow.it.html#ia-tradizionale-o-embedded",
    "href": "contents/core/workflow/workflow.it.html#ia-tradizionale-o-embedded",
    "title": "4¬† Workflow dell‚ÄôIA",
    "section": "4.2 IA Tradizionale o Embedded",
    "text": "4.2 IA Tradizionale o Embedded\nIl flusso di lavoro ML √® una guida universale applicabile su diverse piattaforme, tra cui soluzioni basate su cloud, edge computing e TinyML. Tuttavia, il flusso di lavoro per l‚ÄôIA Embedded introduce complessit√† e sfide uniche, rendendolo un dominio accattivante e aprendo la strada a innovazioni straordinarie. Figura¬†4.2 illustra le differenze tra Machine Learning e Deep Learning.\n\n\n\n\n\n\nFigura¬†4.2: Confronto tra Machine Learning tradizionale e Deep Learning. Fonte: BBN Times\n\n\n\nFigura¬†4.3 illustra gli utilizzi dell‚Äôintelligenza artificiale embedded in vari settori.\n\n\n\n\n\n\nFigura¬†4.3: Applicazioni di IA Embedded. Fonte: Rinf.tech\n\n\n\n\n4.2.1 Ottimizzazione delle Risorse\n\nFlusso di Lavoro ML Tradizionale: Questo workflow d√† priorit√† all‚Äôaccuratezza e alle prestazioni del modello, spesso sfruttando abbondanti risorse di calcolo in ambienti cloud o data center.\nFlusso di Lavoro IA Embedded: Dati i vincoli di risorse dei sistemi embedded, questo flusso di lavoro richiede un‚Äôattenta pianificazione per ottimizzare le dimensioni del modello e le richieste di calcolo. Tecniche come la quantizzazione e il pruning [potatura] del modello sono fondamentali.\n\n\n\n4.2.2 Elaborazione in Real-time\n\nFlusso di Lavoro ML Tradizionale: Meno enfasi sull‚Äôelaborazione in tempo reale, spesso basata sull‚Äôelaborazione di dati in batch.\nFlusso di Lavoro IA Embedded: D√† priorit√† all‚Äôelaborazione dei dati in tempo reale, rendendo essenziali bassa latenza ed esecuzione rapida, soprattutto in applicazioni come veicoli autonomi e automazione industriale.\n\n\n\n4.2.3 Gestione dei Dati e Privacy\n\nFlusso di Lavoro ML Tradizionale: Elabora i dati in posizioni centralizzate, spesso richiedendo un ampio trasferimento di dati e concentrandosi sulla sicurezza dei dati durante il transito e l‚Äôarchiviazione.\nFlusso di Lavoro IA Embedded: Questo workflow sfrutta l‚Äôedge computing per elaborare i dati pi√π vicino alla fonte, riducendo la trasmissione dei dati e migliorando la privacy tramite la localizzazione dei dati.\n\n\n\n4.2.4 Integrazione Hardware-Software\n\nFlusso di Lavoro ML Tradizionale: In genere funziona su hardware generico, con sviluppo di software indipendente.\nFlusso di Lavoro IA Embedded: Questo flusso di lavoro prevede un approccio pi√π integrato allo sviluppo hardware e software, spesso incorporando chip personalizzati o acceleratori hardware per ottenere prestazioni ottimali.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Workflow dell'IA</span>"
    ]
  },
  {
    "objectID": "contents/core/workflow/workflow.it.html#ruoli-e-responsabilit√†",
    "href": "contents/core/workflow/workflow.it.html#ruoli-e-responsabilit√†",
    "title": "4¬† Workflow dell‚ÄôIA",
    "section": "4.3 Ruoli e responsabilit√†",
    "text": "4.3 Ruoli e responsabilit√†\nLa creazione di una soluzione ML, in particolare per l‚Äôintelligenza artificiale embedded, √® uno sforzo multidisciplinare che coinvolge vari specialisti. A differenza dello sviluppo software tradizionale, la creazione di una soluzione ML richiede un approccio multidisciplinare a causa della natura sperimentale dello sviluppo del modello e dei requisiti ad alta intensit√† di risorse per il training e l‚Äôimplementazione di questi modelli.\nC‚Äô√® una forte necessit√† di ruoli incentrati sui dati per il successo delle pipeline di apprendimento automatico. Gli scienziati dei dati e gli ingegneri dei dati gestiscono la raccolta dei dati, creano pipeline di dati e ne garantiscono la qualit√†. Poich√© la natura dei modelli di apprendimento automatico dipende dai dati che consumano, i modelli sono unici e variano a seconda delle diverse applicazioni, il che richiede un‚Äôampia sperimentazione. I ricercatori e gli ingegneri di apprendimento automatico guidano questa fase sperimentale attraverso test continui, convalida e iterazione per ottenere prestazioni ottimali.\nLa fase di implementazione richiede spesso hardware e infrastrutture specializzati, poich√© i modelli di machine learning possono essere ad alta intensit√† di risorse, richiedendo un‚Äôelevata potenza di calcolo e una gestione efficiente delle risorse. Ci√≤ richiede la collaborazione con gli ingegneri hardware per garantire che l‚Äôinfrastruttura possa supportare le esigenze computazionali di training e inferenza del modello.\nPoich√© i modelli prendono decisioni che possono avere un impatto sugli individui e sulla societ√†, gli aspetti etici e legali dell‚Äôapprendimento automatico stanno diventando sempre pi√π importanti. Sono necessari esperti di etica e consulenti legali per garantire la conformit√† agli standard etici e alle normative legali.\nComprendere i vari ruoli coinvolti in un progetto ML √® fondamentale per il suo completamento con successo. Tabella¬†4.1 fornisce una panoramica generale di questi ruoli tipici, anche se √® importante notare che i confini tra loro a volte possono essere confusi. Esaminiamo questa ripartizione:\n\n\n\nTabella¬†4.1: Ruoli e responsabilit√† delle persone coinvolte in Operazioni di ML.\n\n\n\n\n\n\n\n\n\nRuolo\nResponsabilit√†\n\n\n\n\nProject Manager\nSupervisiona il progetto, assicurando che le tempistiche e le milestone siano rispettate.\n\n\nEsperti di Dominio\nOffrono approfondimenti specifici del dominio per definire i requisiti del progetto.\n\n\nData Scientist\nSpecializzati nell‚Äôanalisi dei dati e nello sviluppo di modelli.\n\n\nIngegneri di Apprendimento Automatico\nConcentrati sullo sviluppo e l‚Äôimplementazione del modello.\n\n\nData Scientist\nSpecializzati nell‚Äôanalisi dei dati e nello sviluppo di modelli.\n\n\nEmbedded Systems Engineer\nIntegra modelli ML in sistemi embedded.\n\n\nSoftware Developer\nSviluppa componenti software per l‚Äôintegrazione del sistema IA.\n\n\nHardware Engineer\nProgetta e ottimizza l‚Äôhardware per il sistema AI embedded.\n\n\nUI/UX Designer\nConcentrato sulla progettazione incentrata sull‚Äôutente.\n\n\nQA Engineer\nAssicura che il sistema soddisfi gli standard di qualit√†.\n\n\nEticisti e Consulenti Legali\nConsulenti sulla conformit√† etica e legale.\n\n\nPersonale Operativo e di Manutenzione\nMonitora e mantiene il sistema distribuito.\n\n\nSpecialisti della sicurezza\nGarantiscono la sicurezza del sistema.\n\n\n\n\n\n\nQuesta visione olistica facilita una collaborazione senza soluzione di continuit√† e alimenta un ambiente maturo per innovazione e scoperte. Man mano che procederemo nei prossimi capitoli, esploreremo l‚Äôessenza e le competenze di ciascun ruolo e favoriremo una comprensione pi√π profonda delle complessit√† coinvolte nei progetti di intelligenza artificiale. Per una discussione pi√π dettagliata degli strumenti e delle tecniche specifici utilizzati da questi ruoli, nonch√© per un‚Äôanalisi approfondita delle loro responsabilit√†, fare riferimento a Sezione 13.5.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Workflow dell'IA</span>"
    ]
  },
  {
    "objectID": "contents/core/workflow/workflow.it.html#conclusione",
    "href": "contents/core/workflow/workflow.it.html#conclusione",
    "title": "4¬† Workflow dell‚ÄôIA",
    "section": "4.4 Conclusione",
    "text": "4.4 Conclusione\nQuesto capitolo ha gettato le basi per comprendere il flusso di lavoro dell‚Äôapprendimento automatico, un approccio strutturato fondamentale per lo sviluppo, l‚Äôimplementazione e la manutenzione dei modelli ML. Abbiamo esplorato le sfide uniche affrontate nei flussi di lavoro ML, dove l‚Äôottimizzazione delle risorse, l‚Äôelaborazione in tempo reale, la gestione dei dati e l‚Äôintegrazione hardware-software sono fondamentali. Queste distinzioni sottolineano l‚Äôimportanza di adattare i flussi di lavoro per soddisfare le esigenze specifiche dell‚Äôambiente applicativo.\nInoltre, abbiamo sottolineato l‚Äôimportanza della collaborazione multidisciplinare nei progetti ML. Esaminando i diversi ruoli coinvolti, dai data scientist agli ingegneri del software, abbiamo ottenuto una panoramica del lavoro di squadra necessario per affrontare la natura sperimentale e ad alta intensit√† di risorse dello sviluppo ML. Questa comprensione √® fondamentale per promuovere una comunicazione e una collaborazione efficaci tra diversi settori di competenza.\nMentre passiamo a discussioni pi√π dettagliate nei capitoli successivi, questa panoramica di alto livello ci fornisce una prospettiva olistica sul flusso di lavoro ML e sui vari ruoli coinvolti. Queste basi si riveleranno importanti quando approfondiremo aspetti specifici dell‚Äôapprendimento automatico, che ci consentiranno di contestualizzare concetti avanzati nel quadro pi√π ampio dello sviluppo e dell‚Äôimplementazione del machine learning.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Workflow dell'IA</span>"
    ]
  },
  {
    "objectID": "contents/core/workflow/workflow.it.html#sec-ai-workflow-resource",
    "href": "contents/core/workflow/workflow.it.html#sec-ai-workflow-resource",
    "title": "4¬† Workflow dell‚ÄôIA",
    "section": "4.5 Risorse",
    "text": "4.5 Risorse\nEcco un elenco curato di risorse per supportare studenti e insegnanti nei loro percorsi di apprendimento e insegnamento. Lavoriamo continuamente per espandere questa raccolta e presto aggiungeremo nuovi esercizi.\n\n\n\n\n\n\nSlide\n\n\n\n\n\nQueste slide sono uno strumento prezioso per gli insegnanti per tenere lezioni e per gli studenti per rivedere il materiale secondo il proprio ritmo. Incoraggiamo studenti e docenti a sfruttare queste slide per migliorare la loro comprensione e facilitare un trasferimento efficace delle conoscenze.\n\nML Workflow.\nML Lifecycle.\n\n\n\n\n\n\n\n\n\n\nVideo\n\n\n\n\n\n\nProssimamente.\n\n\n\n\n\n\n\n\n\n\nEsercizi\n\n\n\n\n\nPer rafforzare i concetti trattati in questo capitolo, abbiamo curato una serie di esercizi che sfidano gli studenti ad applicare le proprie conoscenze e ad approfondire la propria comprensione.\n\nProssimamente.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Workflow dell'IA</span>"
    ]
  },
  {
    "objectID": "contents/core/data_engineering/data_engineering.it.html",
    "href": "contents/core/data_engineering/data_engineering.it.html",
    "title": "5¬† Data Engineering",
    "section": "",
    "text": "5.1 Panoramica\nSi immagini un mondo in cui l‚Äôintelligenza artificiale pu√≤ diagnosticare malattie con una precisione senza precedenti, ma solo se i dati utilizzati per addestrarla sono imparziali e affidabili. √à qui che entra in gioco il ‚Äúdata engineering‚Äù [ingegneria dei dati]. Sebbene oltre il 90% dei dati mondiali sia stato creato negli ultimi due decenni, questa enorme quantit√† di informazioni √® utile solo per creare modelli di intelligenza artificiale efficaci con un‚Äôelaborazione e una preparazione adeguate. L‚Äôingegneria dei dati colma questa lacuna trasformando i dati grezzi in un formato di alta qualit√† che alimenta l‚Äôinnovazione dell‚Äôintelligenza artificiale. Nel mondo odierno basato sui dati, proteggere la privacy degli utenti √® fondamentale. Che siano obbligatorie per legge o guidate dalle preoccupazioni degli utenti, le tecniche di anonimizzazione come la privacy differenziale e l‚Äôaggregazione sono fondamentali per mitigare i rischi per la privacy. Tuttavia, un‚Äôimplementazione attenta √® fondamentale per garantire che questi metodi non compromettano l‚Äôutilit√† dei dati. I creatori di set di dati affrontano complesse sfide di privacy e rappresentazione quando creano dati di addestramento di alta qualit√†, in particolare per domini sensibili come l‚Äôassistenza sanitaria. Dal punto di vista legale, i creatori potrebbero dover rimuovere identificatori diretti come nomi ed et√†. Anche senza obblighi legali, la rimozione di tali informazioni pu√≤ aiutare a creare fiducia negli utenti. Tuttavia, un‚Äôeccessiva anonimizzazione pu√≤ compromettere l‚Äôutilit√† del set di dati. Tecniche come la privacy differenziale\\(^{1}\\), l‚Äôaggregazione e la riduzione dei dettagli forniscono alternative per bilanciare privacy e utilit√†, ma hanno degli svantaggi. I creatori devono trovare un equilibrio ponderato in base al caso d‚Äôuso.\nSebbene la privacy sia fondamentale, garantire modelli di intelligenza artificiale equi e solidi richiede di affrontare le lacune (gap) della rappresentazione nei dati. √à fondamentale ma non sufficiente garantire la diversit√† tra variabili individuali come genere, razza e accento. Queste combinazioni, a volte chiamate lacune (gap) di ordine superiore, possono influire in modo significativo sulle prestazioni del modello. Ad esempio, un set di dati medico potrebbe avere dati bilanciati su genere, et√† e diagnosi individualmente, ma non ha abbastanza casi per catturare donne anziane con una condizione specifica. Tali higher-order gaps [lacune di ordine superiore] non sono immediatamente evidenti, ma possono influire in modo critico sulle prestazioni del modello.\nLa creazione di dati di training utili ed etici richiede una considerazione globale dei rischi per la privacy e delle lacune di rappresentazione. Le soluzioni perfette elusive necessitano di pratiche di ingegneria dei dati coscienziose come l‚Äôanonimizzazione, l‚Äôaggregazione, il sotto-campionamento di gruppi sovrarappresentati e la generazione di dati sintetizzati per bilanciare esigenze contrastanti. Ci√≤ facilita modelli che sono sia accurati che socialmente responsabili. La collaborazione interfunzionale e i controlli esterni possono anche rafforzare i dati di training. Le sfide sono molteplici ma superabili con uno sforzo ponderato.\nIniziamo discutendo della raccolta dati: Dove reperiamo i dati e come li raccogliamo? Le opzioni spaziano dall‚Äôestrazione di dati dal web, all‚Äôaccesso alle API e all‚Äôutilizzo di sensori e dispositivi IoT, fino alla conduzione di sondaggi e alla raccolta di input dagli utenti. Questi metodi riflettono pratiche del mondo reale. Successivamente, approfondiremo l‚Äôetichettatura dei dati, tenendo conto anche del coinvolgimento umano. Discuteremo i compromessi e le limitazioni dell‚Äôetichettatura umana ed esploreremo i metodi emergenti per l‚Äôetichettatura automatizzata. Successivamente, affronteremo la pulizia e la preelaborazione dei dati, un passaggio cruciale ma spesso sottovalutato nella preparazione dei dati grezzi per l‚Äôaddestramento del modello di intelligenza artificiale. Segue l‚Äôaumento dei dati, una strategia per migliorare set di dati limitati generando campioni sintetici. Ci√≤ √® particolarmente pertinente per i sistemi embedded, poich√© molti casi d‚Äôuso necessitano di ampi repository di dati prontamente disponibili per la cura [https://it.wikipedia.org/wiki/Data_curation]. La generazione di dati sintetici emerge come un‚Äôalternativa praticabile con vantaggi e svantaggi. Parleremo anche del versioning del dataset, sottolineando l‚Äôimportanza di tracciare le modifiche dei dati nel tempo. I dati sono in continua evoluzione; quindi, √® fondamentale ideare strategie per gestire e archiviare dataset espansivi. Alla fine di questa sezione, si avr√† una comprensione completa dell‚Äôintera pipeline di dati, dalla raccolta all‚Äôarchiviazione, essenziale per rendere operativi i sistemi di intelligenza artificiale. Intraprendiamo questo viaggio!",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Data Engineering</span>"
    ]
  },
  {
    "objectID": "contents/core/data_engineering/data_engineering.it.html#definizione-del-problema",
    "href": "contents/core/data_engineering/data_engineering.it.html#definizione-del-problema",
    "title": "5¬† Data Engineering",
    "section": "5.2 Definizione del Problema",
    "text": "5.2 Definizione del Problema\nIn molti domini di machine learning, algoritmi sofisticati sono al centro dell‚Äôattenzione, mentre l‚Äôimportanza fondamentale della qualit√† dei dati viene spesso trascurata. Questa negligenza d√† origine a ‚ÄúData Cascades‚Äù di Sambasivan et al. (2021), eventi in cui le lacune nella qualit√† dei dati si sommano, portando a conseguenze negative a valle come previsioni errate, cessazioni di progetti e persino potenziali danni alle comunit√†.\nFigura¬†5.1 illustra queste potenziali insidie nei dati in ogni fase e come influenzano l‚Äôintero processo lungo la linea. L‚Äôinfluenza degli errori nella raccolta dei dati √® particolarmente pronunciata. Come illustrato nella figura, qualsiasi lacuna in questa fase iniziale diventer√† evidente nelle fasi successive (nella valutazione e nell‚Äôimplementazione del modello) e potrebbe portare a conseguenze costose, come l‚Äôabbandono dell‚Äôintero modello e il riavvio da zero. Pertanto, investire in tecniche di ingegneria dei dati fin dall‚Äôinizio ci aiuter√† a rilevare gli errori in anticipo, mitigando gli effetti a cascata illustrati nella figura.\n\n\n\n\n\n\nFigura¬†5.1: Data cascades: costi composti. Fonte: Sambasivan et al. (2021).\n\n\nSambasivan, Nithya, Shivani Kapania, Hannah Highfill, Diana Akrong, Praveen Paritosh, e Lora M Aroyo. 2021. ¬´‚ÄúEveryone wants to do the model work, not the data work‚Äù: Data Cascades in High-Stakes AI¬ª. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems, 1‚Äì15.\n\n\nNonostante molti professionisti del ML riconoscano l‚Äôimportanza dei dati, altri segnalano di dover affrontare queste ‚Äúcascate‚Äù. Ci√≤ evidenzia un problema sistemico: mentre il fascino dello sviluppo di modelli avanzati rimane, i dati spesso devono essere maggiormente apprezzati.\nKeyword Spotting (KWS) fornisce un esempio eccellente di TinyML in azione, come illustrato in Figura¬†5.2. Questa tecnologia √® fondamentale per le interfacce abilitate alla voce su dispositivi endpoint come gli smartphone. In genere funzionando come motori di wake-word leggeri, i sistemi KWS sono costantemente attivi, in ascolto di una frase specifica per attivare ulteriori azioni. Come illustrato nella figura, quando diciamo ‚ÄúOK, Google‚Äù o ‚ÄúAlexa‚Äù, questo avvia un processo su un microcontrollore embedded nel dispositivo. Nonostante le loro risorse limitate, questi microcontrollori svolgono un ruolo importante nel consentire interazioni vocali senza interruzioni con i dispositivi, spesso operando in ambienti con elevato rumore ambientale. L‚Äôunicit√† della wake-word, come mostrato nella figura, aiuta a ridurre al minimo i falsi positivi, assicurando che il sistema non venga attivato inavvertitamente.\n\n\n\n\n\n\nFigura¬†5.2: Esempio di individuazione delle ‚ÄúKeyword Spotting‚Äù: interazione con Alexa. Fonte: Amazon.\n\n\n\n√à importante comprendere che queste tecnologie di individuazione delle ‚Äúparole chiave‚Äù non sono isolate; si integrano perfettamente in sistemi pi√π grandi, elaborando segnali in modo continuo e gestendo al contempo un basso consumo energetico. Questi sistemi vanno oltre il semplice riconoscimento delle parole chiave, evolvendosi per facilitare diversi rilevamenti di suoni, come la rottura di un vetro. Questa evoluzione √® orientata alla creazione di dispositivi intelligenti in grado di comprendere e rispondere ai comandi vocali, annunciando un futuro in cui anche gli elettrodomestici possono essere controllati tramite interazioni vocali.\nCreare un modello KWS affidabile √® un compito complesso. Richiede una profonda comprensione dello scenario di distribuzione, che comprenda dove e come funzioneranno questi dispositivi. Ad esempio, l‚Äôefficacia di un modello KWS non riguarda solo il riconoscimento di una parola; riguarda la sua distinzione tra vari accenti e rumori di sottofondo, che si tratti di un bar affollato o del suono stridulo di una televisione in un soggiorno o in una cucina dove questi dispositivi sono comunemente presenti. Riguarda la garanzia che un sussurrato ‚ÄúAlexa‚Äù nel cuore della notte o un urlato ‚ÄúOK Google‚Äù in un mercato rumoroso vengano riconosciuti con la stessa precisione.\nInoltre, molti degli attuali assistenti vocali KWS supportano un numero limitato di lingue, lasciando una parte sostanziale della diversit√† linguistica mondiale non rappresentata. Questa limitazione √® in parte dovuta alla difficolt√† di raccogliere e monetizzare i dati per le lingue parlate da popolazioni pi√π piccole. La distribuzione ‚Äúlong-tail‚Äù [https://it.wikipedia.org/wiki/Coda_lunga] delle lingue implica che molte lingue hanno dati limitati, rendendo difficile lo sviluppo di tecnologie di supporto.\nQuesto livello di accuratezza e robustezza dipende dalla disponibilit√† e dalla qualit√† dei dati, dalla capacit√† di etichettare correttamente i dati e dalla trasparenza dei dati per l‚Äôutente finale prima che vengano utilizzati per addestrare il modello. Tuttavia, tutto inizia con una chiara comprensione della dichiarazione o definizione del problema.\nIn genere, in ML, la definizione del problema ha alcuni passaggi chiave:\n\nIdentificare chiaramente la definizione del problema\nDefinire obiettivi chiari\nStabilire un benchmark [riferimento] di successo\nComprendere l‚Äôimpegno/l‚Äôuso dell‚Äôutente finale\nComprendere i vincoli e le limitazioni dell‚Äôimplementazione\nSeguito infine dalla raccolta dati.\n\nUna solida base di progetto √® essenziale per la sua traiettoria e il suo successo finale. Al centro di questa base c‚Äô√® innanzitutto l‚Äôidentificazione di un problema chiaro, come garantire che i comandi vocali nei sistemi di assistenza vocale siano riconosciuti in modo coerente in diversi ambienti. Obiettivi chiari, come la creazione di set di dati rappresentativi per scenari diversi, forniscono una direzione unificata. I benchmark, come l‚Äôaccuratezza del sistema nel rilevamento delle parole chiave, offrono risultati misurabili per valutare i progressi. Il coinvolgimento delle parti interessate, dagli utenti finali agli investitori, fornisce informazioni preziose e garantisce l‚Äôallineamento con le esigenze del mercato. Inoltre, quando si esplorano ambiti come l‚Äôassistenza vocale, √® importante comprendere i limiti della piattaforma. I sistemi embedded, come i microcontrollori, sono dotati di limitazioni intrinseche di potenza di elaborazione, memoria ed efficienza energetica. Riconoscere queste limitazioni garantisce che le funzionalit√†, come il rilevamento delle parole chiave, siano personalizzate per funzionare in modo ottimale, bilanciando le prestazioni col risparmio delle risorse.\nIn questo contesto, usando KWS come esempio, possiamo suddividere ciascuno dei passaggi come segue:\n\nIdentificazione del Problema: In sostanza, KWS rileva parole chiave specifiche tra suoni ambientali e altre parole pronunciate. Il problema principale √® progettare un sistema in grado di riconoscere queste parole chiave con elevata accuratezza, bassa latenza e minimi falsi positivi o negativi, soprattutto se distribuito su dispositivi con risorse di elaborazione limitate.\nImpostazione di Obiettivi Chiari: Gli obiettivi per un sistema KWS potrebbero includere:\n\nRaggiungimento di un tasso di accuratezza specifico (ad esempio, accuratezza del 98% nel rilevamento delle parole chiave).\nGaranzia di bassa latenza (ad esempio, rilevamento delle parole chiave e risposta entro 200 millisecondi).\nRiduzione al minimo del consumo di energia per estendere la durata della batteria sui dispositivi embedded.\nGaranzia che le dimensioni del modello siano ottimizzate per la memoria disponibile sul dispositivo.\n\nBenchmark per il successo: Stabilire metriche chiare per misurare il successo del sistema KWS. Questo potrebbe includere:\n\nTasso di Veri Positivi: La percentuale di parole chiave identificate correttamente.\nTasso di Falsi Positivi: La percentuale di parole chiave non identificate erroneamente come parole chiave.\nTempo di Risposta: Il tempo impiegato dall‚Äôenunciazione della parola chiave alla risposta del sistema.\nConsumo Energetico: Potenza media utilizzata durante il rilevamento della parola chiave.\n\nCoinvolgimento e Comprensione delle Parti Interessate:: Coinvolgere le parti interessate, tra cui produttori di dispositivi, sviluppatori di hardware e software e utenti finali. Comprendere le loro esigenze, capacit√† e vincoli. Ad esempio:\n\nI produttori di dispositivi potrebbero dare priorit√† al basso consumo energetico.\nGli sviluppatori di software potrebbero enfatizzare la facilit√† di integrazione.\nGli utenti finali darebbero priorit√† all‚Äôaccuratezza e alla reattivit√†.\n\nComprensione dei Vincoli e delle Limitazioni dei Sistemi Embedded: I dispositivi embedded presentano una serie di problematiche:\n\nLimiti della Memoria: I modelli KWS devono essere leggeri per adattarsi ai vincoli di memoria dei dispositivi embedded. In genere, i modelli KWS devono essere piccoli quanto 16 KB per adattarsi alla ‚Äúisola always-on‚Äù [porzione sempre attiva] del SoC. Inoltre, questa √® solo la dimensione del modello. Anche il codice applicativo aggiuntivo per la pre-elaborazione potrebbe dover rientrare nei vincoli di memoria.\nPotenza di Elaborazione: Le capacit√† di calcolo dei dispositivi embedded sono limitate (alcune centinaia di MHz di velocit√† di clock), quindi il modello KWS deve essere ottimizzato per l‚Äôefficienza.\nConsumo Energetico: Poich√© molti dispositivi embedded sono alimentati a batteria, il sistema KWS deve essere efficiente dal punto di vista energetico.\nVincoli Ambientali: I dispositivi potrebbero essere distribuiti in vari ambienti, dalle silenziose camere da letto agli ambienti industriali rumorosi. Il sistema KWS deve essere sufficientemente robusto per funzionare efficacemente in questi scenari.\n\nRaccolta e Analisi dei Dati: Per un sistema KWS, la qualit√† e la diversit√† dei dati sono fondamentali. Le considerazioni potrebbero includere:\n\nVariet√† di Accenti: Raccogliere dati da parlanti con accenti diversi per garantire un riconoscimento ad ampio raggio.\nRumori di Sottofondo: Includere campioni di dati con diversi rumori ambientali per addestrare il modello per scenari del mondo reale.\nVariazioni delle Parole Chiave: Le persone potrebbero pronunciare le parole chiave in modo diverso o avere leggere variazioni nella parola di attivazione stessa. Assicurarsi che il set di dati catturi queste sfumature.\n\nFeedback e Perfezionamento Iterativo: Una volta sviluppato un prototipo di sistema KWS, √® fondamentale testarlo in scenari del mondo reale, raccogliere feedback e perfezionare iterativamente il modello. Ci√≤ garantisce che il sistema rimanga allineato con il problema e gli obiettivi definiti. Ci√≤ √® importante perch√© gli scenari di distribuzione cambiano nel tempo man mano che le cose si evolvono.\n\n\n\n\n\n\n\nEsercizio¬†5.1: Keyword Spotting con TensorFlow Lite Micro\n\n\n\n\n\nEsplorare una guida pratica per la creazione e l‚Äôimplementazione di sistemi Keyword Spotting utilizzando TensorFlow Lite Micro. Seguire i passaggi dalla raccolta dati all‚Äôaddestramento del modello e all‚Äôimplementazione nei microcontrollori. Imparare a creare modelli KWS efficienti che riconoscono parole chiave specifiche in mezzo al rumore di fondo. Perfetto per chi √® interessato all‚Äôapprendimento automatico sui sistemi embedded. Sbloccare il potenziale dei dispositivi ‚Äúvoice-enabled‚Äù con TensorFlow Lite Micro!\n\n\n\n\nIl capitolo corrente sottolinea il ruolo essenziale della qualit√† dei dati nell‚Äôapprendimento automatico, utilizzando come esempio i sistemi Keyword Spotting. Descrive i passaggi chiave, dalla definizione del problema al coinvolgimento delle parti interessate, sottolineando il feedback iterativo. Il prossimo capitolo approfondir√† la gestione della qualit√† dei dati, discutendone le conseguenze e le tendenze future, concentrandosi sull‚Äôimportanza di dati diversificati e di alta qualit√† nello sviluppo di sistemi di intelligenza artificiale, affrontando considerazioni etiche e metodi di reperimento dei dati.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Data Engineering</span>"
    ]
  },
  {
    "objectID": "contents/core/data_engineering/data_engineering.it.html#ricerca-dei-dati.",
    "href": "contents/core/data_engineering/data_engineering.it.html#ricerca-dei-dati.",
    "title": "5¬† Data Engineering",
    "section": "5.3 Ricerca dei Dati.",
    "text": "5.3 Ricerca dei Dati.\nLa qualit√† e la diversit√† dei dati raccolti sono importanti per sviluppare sistemi di intelligenza artificiale accurati e robusti. Il reperimento di dati di training di alta qualit√† richiede un‚Äôattenta considerazione degli obiettivi, delle risorse e delle implicazioni etiche. I dati possono essere ottenuti da varie fonti a seconda delle esigenze del progetto:\n\n5.3.1 Dataset preesistenti\nPiattaforme come Kaggle e UCI Machine Learning Repository forniscono un comodo punto di partenza. I dataset preesistenti sono preziosi per ricercatori, sviluppatori e aziende. Uno dei loro principali vantaggi √® l‚Äôefficienza dei costi. Creare un set di dati da zero pu√≤ richiedere molto tempo ed essere costoso, quindi accedere a dati gi√† pronti pu√≤ far risparmiare risorse significative. Inoltre, molti set di dati, come ImageNet, sono diventati parametri di riferimento standard nella comunit√† di apprendimento automatico, consentendo confronti di prestazioni coerenti tra diversi modelli e algoritmi. Questa disponibilit√† di dati significa che gli esperimenti possono essere avviati immediatamente senza ritardi nella raccolta e nella preelaborazione dei dati. In un campo in rapida evoluzione come il ML, questa praticit√† √® importante.\nLa garanzia di qualit√† che deriva dai dataset preesistenti pi√π diffusi √® importante da considerare perch√© diversi set di dati contengono errori. Ad esempio, nel set di dati ImageNet √® stato riscontrato oltre il 6,4% di errori. Dato il loro uso diffuso, la comunit√† spesso identifica e corregge eventuali errori o distorsioni in questi set di dati. Questa garanzia √® particolarmente utile per studenti e nuovi arrivati nel campo, in quanto possono concentrarsi sull‚Äôapprendimento e sulla sperimentazione senza preoccuparsi dell‚Äôintegrit√† dei dati. La documentazione di supporto che spesso accompagna i set di dati esistenti √® inestimabile, sebbene ci√≤ si applichi generalmente solo a quelli ampiamente utilizzati. Una buona documentazione fornisce approfondimenti sul processo di raccolta dati e sulle definizioni delle variabili e talvolta offre persino prestazioni del modello di base. Queste informazioni non solo aiutano la comprensione, ma promuovono anche la riproducibilit√† nella ricerca, un pilastro dell‚Äôintegrit√† scientifica; attualmente, c‚Äô√® una crisi attorno al miglioramento della riproducibilit√† nei sistemi di apprendimento automatico. Quando altri ricercatori hanno accesso agli stessi dati, possono convalidare i risultati, testare nuove ipotesi o applicare metodologie diverse, consentendoci cos√¨ di basarci pi√π rapidamente sul lavoro reciproco.\nSebbene piattaforme come Kaggle e UCI Machine Learning Repository siano risorse inestimabili, √® essenziale comprendere il contesto in cui sono stati raccolti i dati. I ricercatori dovrebbero fare attenzione al potenziale ‚Äúoverfitting‚Äù quando utilizzano set di dati popolari, poich√© potrebbero essere stati addestrati pi√π modelli su di essi, portando a metriche di prestazioni gonfiate. A volte, questi set di dati non riflettono i dati del mondo reale.\nNegli ultimi anni, si √® sviluppata una crescente consapevolezza dei problemi di ‚Äúbias‚Äù [distorsione], validit√† e riproducibilit√† che possono esistere nei set di dati di apprendimento automatico. Figura¬†5.3 illustra un‚Äôaltra preoccupazione critica: il potenziale di disallineamento quando si utilizza lo stesso set di dati per addestrare modelli diversi.\n\n\n\n\n\n\nFigura¬†5.3: Addestrare modelli diversi con lo stesso set di dati. Fonte: (icons from left to right: Becris; Freepik; Freepik; Paul J; SBTS2018).\n\n\n\nCome mostrato in Figura¬†5.3, addestrare pi√π modelli utilizzando lo stesso set di dati pu√≤ comportare un ‚Äúdisallineamento‚Äù tra i modelli e il mondo. Questo disallineamento crea un intero ecosistema di modelli che riflette solo un sottoinsieme ristretto di dati del mondo reale. Un tale scenario pu√≤ portare a una generalizzazione limitata e a risultati potenzialmente distorti in varie applicazioni che utilizzano questi modelli.\n\n\n5.3.2 Web Scraping\nIl ‚Äúweb scraping‚Äù si riferisce a tecniche automatizzate per l‚Äôestrazione di dati dai siti Web. In genere comporta l‚Äôinvio di richieste HTTP ai server Web, il recupero di contenuti HTML e l‚Äôanalisi di tali contenuti per estrarre informazioni rilevanti. Gli strumenti e i framework pi√π diffusi per il web scraping includono Beautiful Soup, Scrapy e Selenium. Questi strumenti offrono diverse funzionalit√†, dall‚Äôanalisi dei contenuti HTML all‚Äôautomazione delle interazioni con i browser Web, in particolare per i siti Web che caricano i contenuti in modo dinamico tramite JavaScript.\nIl web scraping pu√≤ raccogliere efficacemente grandi set di dati per l‚Äôaddestramento di modelli di apprendimento automatico, in particolare quando i dati etichettati da esseri umani sono scarsi. Per la ricerca sulla visione artificiale, il web scraping consente la raccolta di enormi volumi di immagini e video. I ricercatori hanno utilizzato questa tecnica per creare set di dati influenti come ImageNet e OpenImages. Ad esempio, si potrebbero effettuare scraping di siti di e-commerce per accumulare foto di prodotti per il riconoscimento di oggetti o piattaforme di social media per raccogliere caricamenti di utenti per l‚Äôanalisi facciale. Anche prima di ImageNet, il progetto LabelMe di Stanford ha raschiato (scraped) Flickr per oltre 63.000 immagini annotate che coprono centinaia di categorie di oggetti.\nOltre alla visione artificiale, lo scraping web supporta la raccolta di dati testuali per il linguaggio naturale. I ricercatori possono ‚Äúraschiare‚Äù siti di notizie per dati di analisi del ‚Äúsentiment‚Äù, forum e siti di recensioni per la ricerca sui sistemi di dialogo o social media per la modellazione di argomenti. Ad esempio, i dati di training per il chatbot ChatGPT sono stati ottenuti tramite scraping di gran parte dell‚ÄôInternet pubblico. I repository GitHub sono stati sottoposti a scraping per addestrare l‚Äôassistente di codifica Copilot AI di GitHub.\nIl web scraping pu√≤ anche raccogliere dati strutturati, come prezzi delle azioni, dati meteorologici o informazioni sui prodotti, per applicazioni analitiche. Una volta che i dati sono stati ‚Äúraschiati‚Äù, √® essenziale archiviarli in modo strutturato, spesso utilizzando database o data warehouse. Una corretta gestione dei dati garantisce l‚Äôusabilit√† dei dati raccolti per analisi e applicazioni future.\nTuttavia, mentre il web scraping offre numerosi vantaggi, ci sono limitazioni significative e considerazioni etiche da sostenere. Non tutti i siti Web consentono lo scraping e la violazione di queste restrizioni pu√≤ portare a ripercussioni legali. Anche lo scraping di materiale protetto da copyright o comunicazioni private √® immorale e potenzialmente illegale. Il web scraping etico impone l‚Äôaderenza al file ‚Äòrobots.txt‚Äô di un sito web, che delinea le sezioni del sito a cui √® possibile accedere e che possono essere scansionate dai bot automatizzati.\nPer scoraggiare lo scraping automatizzato, molti siti web implementano limiti di velocit√†. Se un bot invia troppe richieste in un breve periodo, potrebbe essere temporaneamente bloccato, limitando la velocit√† di accesso ai dati. Inoltre, la natura dinamica dei contenuti web implica che i dati estratti a intervalli diversi potrebbero richiedere maggiore coerenza, ponendo sfide per gli studi a lungo termine. Tuttavia, ci sono tendenze emergenti come la Web Navigation in cui gli algoritmi di apprendimento automatico possono navigare automaticamente nel sito web per accedere ai contenuti dinamici.\nIl volume di dati pertinenti disponibili per lo scraping potrebbe essere limitato per argomenti di nicchia. Ad esempio, mentre lo scraping per argomenti comuni come immagini di gatti e cani potrebbe produrre dati abbondanti, la ricerca di condizioni mediche rare potrebbe essere meno fruttuosa. Inoltre, i dati ottenuti tramite scraping sono spesso non strutturati e rumorosi, il che richiede un‚Äôaccurata pre-elaborazione e pulizia. √à fondamentale comprendere che non tutti i dati raccolti saranno di alta qualit√† o accuratezza. L‚Äôimpiego di metodi di verifica, come il riferimento incrociato con fonti alternative di dati, pu√≤ migliorare l‚Äôaffidabilit√† dei dati.\nQuando si esegue lo scraping di dati personali, sorgono problemi di privacy, sottolineando la necessit√† di anonimizzazione. Pertanto, √® fondamentale aderire ai ‚ÄúTermini del Servizio‚Äù di un sito Web, limitare la raccolta di dati a quelli di dominio pubblico e garantire l‚Äôanonimato di tutti i dati personali acquisiti.\nMentre il web scraping pu√≤ essere un metodo scalabile per accumulare grandi set di dati di training per sistemi di intelligenza artificiale, la sua applicabilit√† √® limitata a tipi di dati specifici. Ad esempio, il web scraping rende pi√π complessa la ricerca di dati per unit√† di misura inerziali (IMU) per il riconoscimento dei gesti. Al massimo, si pu√≤ effettuare lo scraping di un set di dati esistente.\nLa raccolta dal Web pu√≤ produrre dati incoerenti o imprecisi. Ad esempio, la foto in Figura¬†5.4 viene visualizzata quando si cerca ‚Äúsemaforo‚Äù su Google Images. √à un‚Äôimmagine del 1914 che mostra semafori obsoleti, che sono anche appena distinguibili a causa della scarsa qualit√† dell‚Äôimmagine. Questo pu√≤ essere problematico per i set di dati estratti dal Web, poich√© lo inquina con campioni di dati non applicabili (vecchi).\n\n\n\n\n\n\nFigura¬†5.4: Un‚Äôimmagine di vecchi semafori (1914). Fonte: Vox.\n\n\n\n\n\n\n\n\n\nEsercizio¬†5.2: Web Scraping\n\n\n\n\n\nScoprire la potenza del web scraping con Python usando librerie come Beautiful Soup e Pandas. Questo esercizio estrarr√† la documentazione Python per i nomi e le descrizioni delle funzioni ed esplorer√† le statistiche dei giocatori NBA. Alla fine, si avranno le competenze per estrarre e analizzare dati da siti Web reali. Pronti all‚Äôimmersione? Accedere al notebook Google Colab qui sotto e iniziare a fare pratica!\n\n\n\n\n\n\n5.3.3 Crowdsourcing\nIl crowdsourcing per i dataset √® la pratica di ottenere dati utilizzando i servizi di molte persone, sia da una comunit√† specifica che dal pubblico in generale, in genere tramite Internet. Invece di affidarsi a un piccolo team o a un‚Äôorganizzazione specifica per raccogliere o etichettare i dati, il crowdsourcing sfrutta lo sforzo collettivo di un vasto gruppo distribuito di partecipanti. Servizi come Amazon Mechanical Turk consentono la distribuzione di attivit√† di annotazione a una forza lavoro ampia e diversificata. Questo facilita la raccolta di etichette per attivit√† complesse come l‚Äôanalisi del ‚Äúsentiment‚Äù o il riconoscimento delle immagini che richiedono il giudizio umano.\nIl crowdsourcing √® emerso come un approccio efficace per la raccolta di dati e la risoluzione dei problemi. Uno dei principali vantaggi del crowdsourcing √® la scalabilit√†: distribuendo le attivit√† a un ampio pool globale di collaboratori su piattaforme digitali, i progetti possono elaborare rapidamente enormi volumi di dati. Ci√≤ rende il crowdsourcing ideale per l‚Äôetichettatura, la raccolta e l‚Äôanalisi di dati su larga scala.\nInoltre, il crowdsourcing attinge a un gruppo eterogeneo di partecipanti, apportando un‚Äôampia gamma di prospettive, intuizioni culturali e capacit√† linguistiche che possono arricchire i dati e migliorare la risoluzione creativa dei problemi in modi che un gruppo pi√π omogeneo potrebbe non fare. Poich√© il crowdsourcing attinge da un vasto pubblico oltre i canali tradizionali, √® pi√π conveniente rispetto ai metodi convenzionali, soprattutto per microattivit√† pi√π semplici.\nLe piattaforme di crowdsourcing consentono anche una grande flessibilit√†, poich√© i parametri delle attivit√† possono essere modificati in tempo reale in base ai risultati iniziali. Ci√≤ crea un ciclo di feedback per miglioramenti iterativi al processo di raccolta dati. I lavori complessi possono essere suddivisi in microattivit√† e distribuiti a pi√π persone, con risultati convalidati in modo incrociato assegnando versioni ridondanti della stessa attivit√†. Se gestito in modo ponderato, il crowdsourcing consente il coinvolgimento della comunit√† attorno a un progetto collaborativo, in cui i partecipanti trovano una ricompensa nel contribuire.\nTuttavia, mentre il crowdsourcing offre numerosi vantaggi, √® essenziale affrontarlo con una strategia chiara. Mentre fornisce l‚Äôaccesso a un set diversificato di annotatori, introduce anche variabilit√† nella qualit√† delle annotazioni. Inoltre, piattaforme come Mechanical Turk potrebbero non sempre catturano uno spettro demografico completo; spesso, gli individui esperti di tecnologia sono sovra-rappresentati, mentre i bambini e gli anziani potrebbero essere sotto-rappresentati. Fornire istruzioni chiare e formazione per gli annotatori √® fondamentale. Controlli periodici e convalide dei dati etichettati aiutano a mantenere la qualit√†. Ci√≤ si ricollega all‚Äôargomento della chiara definizione del problema di cui abbiamo discusso in precedenza. Il crowdsourcing per i set di dati richiede anche una particolare attenzione alle considerazioni etiche. √à fondamentale assicurarsi che i partecipanti siano informati su come verranno utilizzati i loro dati e che la loro privacy sia protetta. Il controllo di qualit√† tramite protocolli dettagliati, trasparenza nell‚Äôapprovvigionamento e verifica √® essenziale per garantire risultati affidabili.\nPer TinyML, il crowdsourcing pu√≤ presentare alcune sfide uniche. I dispositivi TinyML sono altamente specializzati per attivit√† particolari entro vincoli rigorosi. Di conseguenza, i dati di cui hanno bisogno tendono a essere molto specifici. Ottenere tali dati specializzati da un pubblico generico pu√≤ essere difficile tramite crowdsourcing. Ad esempio, le applicazioni TinyML spesso si basano su dati raccolti da determinati sensori o hardware. Il crowdsourcing richiederebbe ai partecipanti di avere accesso a dispositivi molto specifici e coerenti, come i microfoni, con le stesse frequenze di campionamento. Queste sfumature hardware presentano ostacoli anche per semplici attivit√† audio come l‚Äôindividuazione di parole chiave.\nOltre all‚Äôhardware, i dati stessi necessitano di elevata granularit√† e qualit√†, dati i limiti di TinyML. Pu√≤ essere difficile garantire ci√≤ quando si fa crowdsourcing da chi non ha familiarit√† con il contesto e i requisiti dell‚Äôapplicazione. Ci sono anche potenziali problemi relativi alla privacy, alla raccolta in tempo reale, alla standardizzazione e alle competenze tecniche. Inoltre, la natura ristretta di molte attivit√† TinyML semplifica l‚Äôetichettatura accurata dei dati con la giusta comprensione. I partecipanti potrebbero aver bisogno di un contesto completo per fornire annotazioni affidabili.\nPertanto, mentre il crowdsourcing pu√≤ funzionare bene in molti casi, le esigenze specializzate di TinyML introducono sfide uniche per i dati. √à richiesta un‚Äôattenta pianificazione per linee guida, targeting e controllo di qualit√†. Per alcune applicazioni, il crowdsourcing potrebbe essere fattibile, ma altre potrebbero richiedere pi√π lavoro per la raccolta dati pi√π mirati per ottenere dati di training pertinenti e di alta qualit√†.\n\n\n5.3.4 Dati Sintetici\nLa generazione di dati sintetici pu√≤ essere una soluzione preziosa per affrontare le limitazioni della raccolta dati. Figura¬†5.5 illustra come funziona questo processo: i dati sintetici vengono uniti ai dati storici per creare un set di dati pi√π ampio e diversificato per l‚Äôaddestramento del modello.\n\n\n\n\n\n\nFigura¬†5.5: Aumento delle dimensioni dei dati di training con la generazione di dati sintetici. Fonte: AnyLogic.\n\n\n\nCome mostrato nella figura, i dati sintetici implicano la creazione di informazioni che non sono state originariamente catturate o osservate, ma vengono generate utilizzando algoritmi, simulazioni o altre tecniche per assomigliare ai dati del mondo reale. Questo approccio √® diventato particolarmente prezioso in campi in cui i dati del mondo reale sono scarsi, costosi o eticamente difficili da ottenere, come nelle applicazioni TinyML. Varie tecniche, tra cui le ‚ÄúGenerative Adversarial Networks (GAN)‚Äù, possono produrre dati sintetici di alta qualit√† quasi indistinguibili dai dati reali. Questi metodi hanno fatto notevoli progressi, rendendo la generazione di dati sintetici sempre pi√π realistica e affidabile.\nPotrebbe essere necessario disporre di pi√π dati del mondo reale per l‚Äôanalisi o l‚Äôaddestramento di modelli di apprendimento automatico in molti domini, in particolare quelli emergenti. I dati sintetici possono colmare questa lacuna producendo grandi volumi di dati che imitano scenari del mondo reale. Ad esempio, rilevare il suono di un vetro che si rompe potrebbe essere difficile nelle applicazioni di sicurezza in cui un dispositivo TinyML sta cercando di identificare le effrazioni. La raccolta di dati del mondo reale richiederebbe la rottura di numerose finestre, il che √® poco pratico e costoso.\nInoltre, avere un set di dati diversificato √® fondamentale nell‚Äôapprendimento automatico, in particolare nel deep learning. I dati sintetici possono aumentare i set di dati esistenti introducendo varianti, migliorando cos√¨ la robustezza dei modelli. Ad esempio, SpecAugment √® un‚Äôeccellente tecnica di aumento dei dati per i sistemi di ‚ÄúAutomatic Speech Recognition‚Äù (ASR).\nAnche la privacy e la riservatezza sono grandi problemi. I set di dati contenenti informazioni sensibili o personali sollevano problemi di privacy quando vengono condivisi o utilizzati. I dati sintetici, essendo generati artificialmente, non hanno questi legami diretti con individui reali, consentendo un utilizzo pi√π sicuro preservando al contempo le propriet√† statistiche essenziali.\nLa generazione di dati sintetici, in particolare una volta stabiliti i meccanismi di generazione, pu√≤ essere un‚Äôalternativa pi√π conveniente. I dati sintetici eliminano la necessit√† di rompere pi√π finestre per raccogliere dati rilevanti nello scenario applicativo di sicurezza di cui sopra.\nMolti casi d‚Äôuso embedded riguardano situazioni uniche, come gli impianti di produzione, che sono difficili da simulare. I dati sintetici consentono ai ricercatori il controllo completo sul processo di generazione dei dati, consentendo la creazione di scenari o condizioni specifici che sono difficili da catturare nella vita reale.\nSebbene i dati sintetici offrano numerosi vantaggi, √® essenziale utilizzarli giudiziosamente. Bisogna fare attenzione a garantire che i dati generati rappresentino accuratamente le distribuzioni sottostanti del mondo reale e non introducano distorsioni indesiderate.\n\n\n\n\n\n\nEsercizio¬†5.3: Dati Sintetici\n\n\n\n\n\nScopriamo la generazione di dati sintetici utilizzando le Generative Adversarial Network (GAN) su dati tabellari. Adotteremo un approccio pratico, immergendoci nel funzionamento del modello CTGAN e applicandolo al set di dati Synthea dal dominio sanitario. Dalla pre-elaborazione dei dati al training e valutazione del modello, procederemo passo dopo passo, imparando come creare dati sintetici, valutarne la qualit√† e sbloccare il potenziale delle GAN per l‚Äôaumento dei dati e le applicazioni del mondo reale.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Data Engineering</span>"
    ]
  },
  {
    "objectID": "contents/core/data_engineering/data_engineering.it.html#archiviazione-dati",
    "href": "contents/core/data_engineering/data_engineering.it.html#archiviazione-dati",
    "title": "5¬† Data Engineering",
    "section": "5.4 Archiviazione Dati",
    "text": "5.4 Archiviazione Dati\nL‚Äôapprovvigionamento e l‚Äôarchiviazione dei dati vanno di pari passo e i dati devono essere archiviati in un formato che faciliti l‚Äôaccesso e l‚Äôelaborazione. A seconda del caso d‚Äôuso, possono essere utilizzati vari tipi di sistemi di archiviazione dati per archiviare i set di dati. Alcuni esempi sono mostrati in Tabella¬†5.1.\n\n\n\nTabella¬†5.1: Panoramica comparativa del database, del data warehouse e del data lake.\n\n\n\n\n\n\n\n\n\n\nDatabase\nData Warehouse\nData Lake\n\n\n\n\nScopo\nOperativo e transazionale\nAnalitico\n\n\nTipo di dati\nStrutturato\nStrutturato, semi-strutturato e/o non strutturato\n\n\nScala\nDa piccoli a grandi volumi di dati\nGrandi volumi di dati integrati Grandi volumi di dati diversi\n\n\nEsempi\nMySQL\nGoogle BigQuery, Amazon Redshift, Microsoft Azure Synapse, Google Cloud Storage, AWS S3, Azure Data Lake Storage\n\n\n\n\n\n\nI dati archiviati sono spesso accompagnati da metadati, definiti come ‚Äúdati sui dati‚Äù. Forniscono informazioni contestuali dettagliate sui dati, come mezzi di creazione dei dati, ora di creazione, licenza di utilizzo dei dati allegata, ecc. Figura¬†5.6 illustra i pilastri chiave della raccolta dati e i metodi associati, evidenziando l‚Äôimportanza della gestione dei dati strutturati. Ad esempio, Hugging Face ha implementato le Dataset Cards per promuovere un utilizzo responsabile dei dati. Queste ‚Äúcard‚Äù [schede], che si allineano al pilastro della documentazione mostrato in Figura¬†5.6, consentono ai creatori di dataset di rivelare potenziali bias e di istruire gli utenti sui contenuti e le limitazioni di un dataset.\nLe ‚Äúdataset card‚Äù forniscono un contesto importante sull‚Äôutilizzo appropriato del dataset evidenziando ‚Äúbia‚Äù [pregiudizi] e altri dettagli importanti. Avere questo tipo di metadati strutturati pu√≤ anche consentire un rapido recupero, allineandosi ai principi di gestione efficiente dei dati illustrati nella figura. Una volta sviluppato e distribuito il modello sui dispositivi edge, i sistemi di storage possono continuare a memorizzare dati in arrivo, aggiornamenti del modello o risultati analitici, utilizzando potenzialmente metodi da pi√π pilastri mostrati in Figura¬†5.6. Questo processo continuo di raccolta e gestione dei dati garantisce che il modello rimanga aggiornato e pertinente nel suo ambiente operativo.\n\n\n\n\n\n\nFigura¬†5.6: Pilastri della raccolta dati. Fonte: Alexsoft\n\n\n\nData Governance: Con una grande quantit√† di archiviazione dati, √® anche fondamentale disporre di policy e pratiche (ad esempio, ‚Äúgovernance‚Äù [gestione] dei dati) che aiutino a gestire i dati durante il loro ciclo di vita, dall‚Äôacquisizione allo smaltimento. La governance dei dati descrive il modo in cui i dati vengono gestiti e include l‚Äôadozione di decisioni chiave in merito al loro accesso e controllo. Figura¬†5.7 illustra i diversi domini coinvolti nella governance dei dati. Implica l‚Äôesercizio dell‚Äôautorit√† e l‚Äôassunzione di decisioni sui dati per mantenerne la qualit√†, garantire la conformit√†, mantenere la sicurezza e ricavarne valore. La governance dei dati √® resa operativa sviluppando politiche, incentivi e sanzioni, coltivando una cultura che percepisce i dati come un bene prezioso. Procedure specifiche e autorit√† assegnate vengono implementate per salvaguardare la qualit√† dei dati e monitorarne l‚Äôutilizzo e i rischi correlati.\n\n\n\n\n\n\nFigura¬†5.7: Una panoramica del framework di governance dei dati. Fonte: StarCIO..\n\n\n\nLa governance dei dati utilizza tre approcci integrativi: pianificazione e controllo, organizzativo e basato sul rischio.\n\nL‚Äôapproccio di pianificazione e controllo, comune nell‚ÄôIT, allinea business e tecnologia attraverso cicli annuali e continui aggiustamenti, concentrandosi su una governance verificabile e basata su policy.\nL‚Äôapproccio organizzativo enfatizza la struttura, stabilendo ruoli autorevoli come Chief Data Officer e garantendo responsabilit√† e rendicontazione nella governance.\nL‚Äôapproccio basato sul rischio, intensificato dai progressi dell‚ÄôIA, si concentra sull‚Äôidentificazione e la gestione dei rischi intrinseci nei dati e negli algoritmi. Affronta in particolare i problemi specifici dell‚ÄôIA attraverso valutazioni regolari e strategie di gestione proattiva del rischio, consentendo azioni incidentali e preventive per mitigare gli impatti indesiderati degli algoritmi.\n\nEcco alcuni esempi di governance dei dati in diversi settori:\n\nMedicina: Gli Health Information Exchanges (HIE) [scambi di informazioni sanitarie] consentono la condivisione di informazioni sanitarie tra diversi operatori sanitari per migliorare l‚Äôassistenza ai pazienti. Implementano rigorose pratiche di governance dei dati per mantenere l‚Äôaccuratezza, l‚Äôintegrit√†, la privacy e la sicurezza dei dati, rispettando normative come l‚ÄôHealth Insurance Portability and Accountability Act (HIPAA). Le policy di governance assicurano che i dati dei pazienti siano condivisi solo con entit√† autorizzate e che i pazienti possano controllare l‚Äôaccesso alle proprie informazioni.\nFinanza: Basilea III Framework √® un quadro normativo internazionale per le banche. Garantisce che le banche stabiliscano policy, pratiche e responsabilit√† chiare per la gestione dei dati, assicurandone accuratezza, completezza e tempestivit√†. Non solo consente alle banche di soddisfare la conformit√† normativa, ma previene anche le crisi finanziarie gestendo i rischi in modo pi√π efficace.\nGoverno: Le agenzie governative che gestiscono i dati dei cittadini, i registri pubblici e le informazioni amministrative implementano la governance dei dati per gestire i dati in modo trasparente e sicuro. Il sistema di previdenza sociale negli Stati Uniti e il sistema Aadhar in India sono buoni esempi di tali sistemi di governance.\n\nConsiderazioni speciali sull‚Äôarchiviazione dei dati per TinyML\nFormati di Archiviazione Audio Efficienti: I sistemi di individuazione delle parole chiave necessitano di formati di archiviazione audio specializzati per consentire una rapida ricerca delle parole chiave nei dati audio. I formati tradizionali come WAV e MP3 archiviano forme d‚Äôonda audio complete, che richiedono un‚Äôelaborazione estesa per la ricerca. L‚Äôindividuazione delle parole chiave utilizza un archivio compresso ottimizzato per la ricerca basata su frammenti. Un approccio consiste nell‚Äôarchiviazione di feature acustiche compatte anzich√© audio grezzo. Tale flusso di lavoro implicherebbe:\n\nEstrazione di Feature Acustiche: I coefficienti Mel-frequency cepstral (MFCC) rappresentano comunemente importanti caratteristiche audio.\nCreazione di Embedding: Gli ‚Äúembedding‚Äù trasformano le feature acustiche estratte in spazi vettoriali continui, consentendo un‚Äôarchiviazione dei dati pi√π compatta e rappresentativa. Questa rappresentazione √® essenziale per convertire dati ad alta dimensionalit√†, come l‚Äôaudio, in un formato pi√π gestibile ed efficiente per l‚Äôelaborazione e l‚Äôarchiviazione.\nQuantizzazione vettoriale: Questa tecnica rappresenta dati ad alta dimensionalit√†, come gli embedding, con vettori a bassa dimensionalit√†, riducendo le esigenze di archiviazione. Inizialmente, un codebook viene generato dai dati di training per definire un set di vettori di codice che rappresentano i vettori di dati originali. Successivamente, ogni vettore di dati viene abbinato alla ‚Äúcodeword‚Äù pi√π vicina in base al codebook, garantendo una perdita minima di informazioni.\nArchiviazione sequenziale: L‚Äôaudio viene frammentato in frame brevi e le feature [caratteristiche] quantizzate (o embedded) per ogni frame vengono archiviate in sequenza per mantenere l‚Äôordine temporale, preservando la coerenza e il contesto dei dati audio.\n\nQuesto formato consente di decodificare le feature frame per frame per la corrispondenza delle parole chiave. La ricerca delle feature √® pi√π rapida della decompressione dell‚Äôaudio completo.\nSelective Network Output Storage: [Archiviazione selettiva dell‚Äôoutput di rete] Un‚Äôaltra tecnica per ridurre l‚Äôarchiviazione consiste nell‚Äôeliminare le caratteristiche audio intermedie archiviate durante l‚Äôaddestramento ma non richieste durante l‚Äôinferenza. La rete viene eseguita su audio completo durante l‚Äôaddestramento. Tuttavia, solo gli output finali vengono archiviati durante l‚Äôinferenza.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Data Engineering</span>"
    ]
  },
  {
    "objectID": "contents/core/data_engineering/data_engineering.it.html#elaborazione-dei-dati",
    "href": "contents/core/data_engineering/data_engineering.it.html#elaborazione-dei-dati",
    "title": "5¬† Data Engineering",
    "section": "5.5 Elaborazione dei Dati",
    "text": "5.5 Elaborazione dei Dati\nIl ‚ÄúData processing‚Äù elaborazione dei dati si riferisce ai passaggi necessari per trasformare i dati grezzi in un formato adatto per l‚Äôinserimento negli algoritmi di apprendimento automatico. √à una fase cruciale in qualsiasi flusso di lavoro ML, ma spesso trascurata. Con un‚Äôelaborazione dei dati adeguata, √® probabile che i modelli ML raggiungano prestazioni ottimali. Figura¬†5.8 mostra una ripartizione dell‚Äôallocazione del tempo di uno scienziato dei dati, evidenziando la parte significativa spesa per la pulizia e l‚Äôorganizzazione dei dati (%60).\n\n\n\n\n\n\nFigura¬†5.8: Ripartizione delle attivit√† dei ‚ÄúData scientist‚Äù in base al tempo impiegato. Fonte: Forbes.\n\n\n\nUna corretta pulizia dei dati √® un passaggio cruciale che influisce direttamente sulle prestazioni del modello. I dati del mondo reale sono spesso sporchi, contengono errori, valori mancanti, rumore, anomalie e incongruenze. La pulizia dei dati comporta il rilevamento e la correzione di questi problemi per preparare dati di alta qualit√† per la modellazione. Selezionando attentamente le tecniche appropriate, i data scientist possono migliorare l‚Äôaccuratezza del modello, ridurre l‚Äôoverfitting e addestrare gli algoritmi per apprendere pattern pi√π solidi. Nel complesso, un‚Äôelaborazione dei dati ponderata consente ai sistemi di apprendimento automatico di scoprire meglio le informazioni e di fare previsioni dai dati del mondo reale.\nI dati spesso provengono da fonti diverse e possono essere non strutturati o semi-strutturati. Pertanto, elaborarli e standardizzarli √® essenziale, assicurando che aderiscano a un formato uniforme. Tali trasformazioni possono includere:\n\nNormalizzazione di variabili numeriche\nCodifica di variabili categoriali\nUtilizzo di tecniche come la riduzione della dimensionalit√†\n\nLa convalida dei dati svolge un ruolo pi√π ampio rispetto alla garanzia di aderenza a determinati standard, come impedire che i valori di temperatura scendano sotto lo zero assoluto. Questi problemi si verificano in TinyML perch√© i sensori potrebbero funzionare male o produrre temporaneamente letture errate; tali transienti non sono rari. Pertanto, √® fondamentale rilevare gli errori nei dati in anticipo prima che si propaghino attraverso la pipeline dei dati. Rigorosi processi di convalida, tra cui la verifica delle pratiche di annotazione iniziali, il rilevamento di valori anomali e la gestione dei valori mancanti tramite tecniche come l‚Äôimputazione della media, contribuiscono direttamente alla qualit√† dei set di dati. Ci√≤, a sua volta, influisce sulle prestazioni, la correttezza e la sicurezza dei modelli addestrati su di essi.\nDiamo un‚Äôocchiata a Figura¬†5.9 per un esempio di pipeline di elaborazione dei dati. Nel contesto di TinyML, il Multilingual Spoken Words Corpus (MSWC) √® un esempio di pipeline di elaborazione dei dati, flussi di lavoro sistematici e automatizzati per la trasformazione, l‚Äôarchiviazione e l‚Äôelaborazione dei dati. I dati di input (che sono una raccolta di brevi registrazioni) attraversano diverse fasi di elaborazione, come l‚Äôallineamento audio-parola e l‚Äôestrazione di parole chiave.\nMSWC semplifica il flusso di dati, dai dati grezzi ai set di dati utilizzabili, le pipeline di dati migliorano la produttivit√† e facilitano lo sviluppo rapido di modelli di apprendimento automatico. MSWC √® una raccolta ampia e in continua espansione di registrazioni audio di parole pronunciate in 50 lingue diverse, utilizzate collettivamente da oltre 5 miliardi di persone. Questo set di dati √® destinato allo studio accademico e all‚Äôuso aziendale in aree come l‚Äôidentificazione di parole chiave e la ricerca basata sul parlato. √à concesso in licenza aperta con Creative Commons Attribution 4.0 per un ampio utilizzo.\n\n\n\n\n\n\nFigura¬†5.9: Una panoramica della pipeline di elaborazione dati del Multilingual Spoken Words Corpus (MSWC). Fonte: Mazumder et al. (2021).\n\n\nMazumder, Mark, Sharad Chitlangia, Colby Banbury, Yiping Kang, Juan Manuel Ciro, Keith Achorn, Daniel Galvez, et al. 2021. ¬´Multilingual spoken words corpus¬ª. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2).\n\n\nIl MSWC ha utilizzato un metodo di allineamento forzato per estrarre automaticamente singole registrazioni di parole per addestrare modelli di individuazione delle parole chiave dal progetto Common Voice, che presenta registrazioni a livello di frase in crowdsourcing. L‚Äôallineamento forzato si riferisce a metodi di lunga data nell‚Äôelaborazione del parlato che prevedono quando fenomeni del parlato come sillabe, parole o frasi iniziano e finiscono all‚Äôinterno di una registrazione audio. Nei dati MSWC, le registrazioni in crowdsourcing spesso presentano rumori di sottofondo, come elettricit√† statica e vento. A seconda dei requisiti del modello, questi rumori possono essere rimossi o mantenuti intenzionalmente.\nMantenere l‚Äôintegrit√† dell‚Äôinfrastruttura dati √® uno lavoro continuo. Ci√≤ comprende archiviazione dei dati, sicurezza, gestione degli errori e rigoroso controllo delle versioni. Gli aggiornamenti periodici sono fondamentali, soprattutto in ambiti dinamici come l‚Äôindividuazione delle parole chiave, per adattarsi alle tendenze linguistiche in evoluzione e alle integrazioni dei dispositivi.\nC‚Äô√® un boom nelle pipeline di elaborazione dati, comunemente presenti nelle toolchain delle operazioni ML, di cui parleremo nel capitolo MLOps. In breve, questi includono framework come MLOps di Google Cloud. Fornisce metodi per l‚Äôautomazione e il monitoraggio in tutte le fasi della costruzione del sistema ML, tra cui integrazione, test, rilascio, distribuzione e gestione dell‚Äôinfrastruttura. Diversi meccanismi si concentrano sull‚Äôelaborazione dati, parte integrante di questi sistemi.\n\n\n\n\n\n\nEsercizio¬†5.4: Elaborazione dei Dati\n\n\n\n\n\nEsploriamo due progetti significativi nell‚Äôelaborazione dei dati vocali e nell‚Äôapprendimento automatico. MSWC √® un vasto set di dati audio con oltre 340.000 parole chiave e 23,4 milioni di esempi parlati di 1 secondo. Viene utilizzato in varie applicazioni come dispositivi voice-enabled e automazione dei call center. Il progetto Few-Shot Keyword Spotting introduce un nuovo approccio per l‚Äôindividuazione delle parole chiave in diverse lingue, ottenendo risultati impressionanti con dati di addestramento minimi. Esamineremo il set di dati MSWC, impareremo come strutturarlo in modo efficace e poi addestreremo un modello di individuazione di parole chiave con la tecnica ‚Äúfew-shot‚Äù [https://www.ibm.com/it-it/topics/few-shot-learning]. Cominciamo!",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Data Engineering</span>"
    ]
  },
  {
    "objectID": "contents/core/data_engineering/data_engineering.it.html#etichettatura-dei-dati",
    "href": "contents/core/data_engineering/data_engineering.it.html#etichettatura-dei-dati",
    "title": "5¬† Data Engineering",
    "section": "5.6 Etichettatura dei Dati",
    "text": "5.6 Etichettatura dei Dati\nIl ‚ÄúData labeling‚Äù etichettatura dei dati √® importante per creare set di dati di training di alta qualit√† per modelli di apprendimento automatico. Le etichette forniscono informazioni di base, consentendo ai modelli di apprendere relazioni tra input e output desiderati. Questa sezione copre considerazioni chiave per la selezione di tipi di etichette, formati e contenuti per acquisire le informazioni necessarie per le attivit√†. Discute approcci di annotazione comuni, dall‚Äôetichettatura manuale al crowdsourcing ai metodi assistiti dall‚Äôintelligenza artificiale, e le ‚Äúbest practice‚Äù per garantire la qualit√† delle etichette tramite formazione, linee guida e controlli di qualit√†. Sottolineiamo anche il trattamento etico degli annotatori umani. Viene anche esplorata l‚Äôintegrazione dell‚Äôintelligenza artificiale per accelerare e aumentare l‚Äôannotazione umana. Comprendere le esigenze, le sfide e le strategie di etichettatura √® essenziale per costruire dataset affidabili e utili per addestrare sistemi di apprendimento automatico performanti e affidabili.\n\n5.6.1 Tipi di Etichette\nLe etichette contengono informazioni su attivit√† o concetti chiave. Figura¬†5.10 include alcuni tipi di etichette comuni: una ‚Äúclassification label‚Äù [etichetta di classificazione] viene utilizzata per categorizzare le immagini con etichette (etichettando un‚Äôimmagine con ‚Äúdog‚Äù [cane] e presenta un cane); un ‚Äúbounding box‚Äù [riquadro delimitatore] identifica la posizione dell‚Äôoggetto (disegnando un riquadro attorno al cane); una ‚Äúsegmentation map‚Äù [mappa di segmentazione] classifica gli oggetti a livello di pixel (evidenziando il cane con un colore distinto); una ‚Äúcaption‚Äù [didascalia] fornisce annotazioni descrittive (descrivendo le azioni, la posizione, il colore, ecc. del cane); e una ‚Äútranscript‚Äù [trascrizione] denota il contenuto audio. La scelta del formato dell‚Äôetichetta dipende dal caso d‚Äôuso e dai vincoli di risorse, poich√© etichette pi√π dettagliate richiedono un lavoro maggiore per la raccolta (Johnson-Roberson et al. 2017).\n\nJohnson-Roberson, Matthew, Charles Barto, Rounak Mehta, Sharath Nittur Sridhar, Karl Rosaen, e Ram Vasudevan. 2017. ¬´Driving in the Matrix: Can virtual worlds replace human-generated annotations for real world tasks?¬ª In 2017 IEEE International Conference on Robotics and Automation (ICRA), 746‚Äì53. Singapore, Singapore: IEEE. https://doi.org/10.1109/icra.2017.7989092.\n\n\n\n\n\n\nFigura¬†5.10: Una panoramica dei tipi di etichette comuni.\n\n\n\nA meno che non si concentri sull‚Äôapprendimento auto-supervisionato, un set di dati fornir√† probabilmente etichette che affrontano una o pi√π attivit√† di interesse. Date le loro limitazioni di risorse uniche, i creatori di set di dati devono considerare quali informazioni le etichette dovrebbero catturare e come possono ottenere praticamente le etichette necessarie. I creatori devono prima decidere quali tipi di etichette di contenuto dovrebbero catturare. Ad esempio, un creatore interessato al rilevamento delle auto vorrebbe etichettare le auto nel suo dataset. Tuttavia, potrebbe anche considerare se raccogliere simultaneamente etichette per altre attivit√† per cui il set di dati potrebbe essere potenzialmente utilizzato, come il rilevamento dei pedoni.\nInoltre, gli annotatori possono fornire metadati per le informazioni su come il set di dati rappresenta diverse caratteristiche di interesse (cfr. Sezione 5.9). Il dataset Common Voice, ad esempio, include vari tipi di metadati che forniscono informazioni sugli oratori, sulle registrazioni e sulla qualit√† del set di dati per ciascuna lingua rappresentata (Ardila et al. 2020). Includono suddivisioni demografiche che mostrano il numero di registrazioni per fascia di et√† e genere del parlante. Questo ci consente di vedere chi ha contribuito alle registrazioni per ogni lingua. Includono anche statistiche come la durata media delle registrazioni e il numero totale di ore di registrazioni convalidate. Queste forniscono informazioni sulla natura e le dimensioni dei set di dati per ogni lingua.\n\nArdila, Rosana, Megan Branson, Kelly Davis, Michael Kohler, Josh Meyer, Michael Henretty, Reuben Morais, Lindsay Saunders, Francis Tyers, e Gregor Weber. 2020. ¬´Common Voice: A Massively-Multilingual Speech Corpus¬ª. In Proceedings of the Twelfth Language Resources and Evaluation Conference, 4218‚Äì22. Marseille, France: European Language Resources Association. https://aclanthology.org/2020.lrec-1.520.\nInoltre, le metriche di controllo qualit√† come la percentuale di registrazioni convalidate sono utili per sapere quanto siano completi e puliti i set di dati. I metadati includono anche suddivisioni demografiche normalizzate scalate al 100% per il confronto tra le lingue. Questo evidenzia le differenze di rappresentazione tra lingue con risorse pi√π elevate e pi√π basse.\nSuccessivamente, i creatori devono determinare il formato di tali etichette. Ad esempio, un creatore interessato al rilevamento delle auto potrebbe scegliere tra etichette di classificazione binaria che indicano se √® presente un‚Äôauto, riquadri di delimitazione che mostrano le posizioni generali di tutte le auto o etichette di segmentazione pixel per pixel che mostrano la posizione esatta di ogni auto. La scelta del formato dell‚Äôetichetta pu√≤ dipendere dal caso d‚Äôuso e dai vincoli di risorse, poich√© le etichette pi√π dettagliate sono in genere pi√π costose e richiedono pi√π tempo per essere acquisite.\n\n\n5.6.2 Metodi di Annotazione\nGli approcci comuni all‚Äôannotazione includono etichettatura manuale, crowdsourcing e tecniche semi-automatiche. L‚Äôetichettatura manuale da parte di esperti produce alta qualit√† ma necessita di maggiore scalabilit√†. Il crowdsourcing consente ai non esperti di distribuire annotazioni, spesso tramite piattaforme dedicate (Sheng e Zhang 2019). Metodi debolmente supervisionati e programmatici possono ridurre il lavoro manuale generando etichette in modo euristico o automatico (Ratner et al. 2018).\n\nSheng, Victor S., e Jing Zhang. 2019. ¬´Machine Learning with Crowdsourcing: A Brief Summary of the Past Research and Future Directions¬ª. Proceedings of the AAAI Conference on Artificial Intelligence 33 (01): 9837‚Äì43. https://doi.org/10.1609/aaai.v33i01.33019837.\n\nRatner, Alex, Braden Hancock, Jared Dunnmon, Roger Goldman, e Christopher R√©. 2018. ¬´Snorkel MeTaL: Weak Supervision for Multi-Task Learning¬ª. In Proceedings of the Second Workshop on Data Management for End-To-End Machine Learning. ACM. https://doi.org/10.1145/3209889.3209898.\nDopo aver deciso il contenuto e il formato desiderati per le etichette, i creatori iniziano il processo di annotazione. Per raccogliere un gran numero di etichette da annotatori umani, i creatori si affidano spesso a piattaforme di annotazione dedicate, che possono metterli in contatto con team di annotatori umani. Quando utilizzano queste piattaforme, i creatori potrebbero aver bisogno di maggiori informazioni sui background degli annotatori e sui livelli di esperienza con argomenti di interesse. Tuttavia, alcune piattaforme offrono l‚Äôaccesso ad annotatori con competenze specifiche (ad esempio, medici).\n\n\n\n\n\n\nEsercizio¬†5.5: Etichette Autoprodotte\n\n\n\n\n\nEsploriamo Wake Vision, un set di dati completo progettato per il rilevamento di persone con TinyML. Questo set di dati deriva da un set di dati pi√π ampio e generico, Open Images (Kuznetsova et al. 2020), e specificamente adattato per il rilevamento binario di persone.\nIl processo di trasformazione comporta il filtraggio e la rietichettatura delle etichette e dei riquadri di delimitazione esistenti in Open Images utilizzando una pipeline automatizzata. Questo metodo non solo consente di risparmiare tempo e risorse, ma garantisce anche che il set di dati soddisfi i requisiti specifici delle applicazioni TinyML.\nInoltre, generiamo metadati per confrontare la correttezza e la robustezza dei modelli in scenari difficili.\nCominciamo!\n\n\n\n\n\nKuznetsova, Alina, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, et al. 2020. ¬´The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale¬ª. International journal of computer vision 128 (7): 1956‚Äì81.\n\n\n5.6.3 Garantire la Qualit√† dell‚ÄôEtichetta\nNon vi √® alcuna garanzia che le etichette dei dati siano effettivamente corrette. Figura¬†5.11 mostra alcuni esempi di casi di etichettatura rigida: alcuni errori derivano da immagini sfocate che le rendono difficili da identificare (l‚Äôimmagine della rana), e altri derivano da una mancanza di conoscenza del dominio (il caso della cicogna nera). √à possibile che nonostante le migliori istruzioni fornite agli etichettatori, etichettino ancora in modo errato alcune immagini (Northcutt, Athalye, e Mueller 2021). Strategie come controlli di qualit√†, formazione degli annotatori e raccolta di pi√π etichette per ciascun elemento possono aiutare a garantire la qualit√† delle etichette. Per attivit√† ambigue, pi√π annotatori possono aiutare a identificare i punti dati controversi e quantificare i livelli di disaccordo.\n\n\n\n\n\n\nFigura¬†5.11: Alcuni esempi di casi di etichettatura rigida. Fonte: Northcutt, Athalye, e Mueller (2021).\n\n\nNorthcutt, Curtis G, Anish Athalye, e Jonas Mueller. 2021. ¬´Pervasive Label Errors in Test Sets Destabilize Machine Learning Benchmarks¬ª. arXiv. https://doi.org/https://doi.org/10.48550/arXiv.2103.14749 arXiv-issued DOI via DataCite.\n\n\nQuando si lavora con annotatori umani, √® importante offrire un compenso equo e dare priorit√† al trattamento etico, poich√© gli annotatori possono essere sfruttati o danneggiati durante il processo di etichettatura (Perrigo, 2023). Ad esempio, se √® probabile che un set di dati contenga contenuti inquietanti, gli annotatori potrebbero trarre vantaggio dall‚Äôavere la possibilit√† di visualizzare le immagini in scala di grigi (Google, s.d.).\n\nGoogle. s.d. ¬´Information quality content moderation¬ª. https://blog.google/documents/83/.\n\n\n5.6.4 Annotazione assistita dall‚Äôintelligenza artificiale\nIl ML ha una domanda insaziabile di dati. Pertanto, sono necessari pi√π dati. Ci√≤ solleva la questione di come possiamo ottenere pi√π dati etichettati. Invece di generare e curare sempre i dati manualmente, possiamo fare affidamento sui modelli di intelligenza artificiale esistenti per etichettare i set di dati in modo pi√π rapido ed economico, anche se spesso con una qualit√† inferiore rispetto all‚Äôannotazione umana. Questo pu√≤ essere fatto in vari modi come mostrato in Figura¬†5.12, tra cui i seguenti:\n\nPre-annotazione: I modelli di intelligenza artificiale possono generare etichette preliminari per un set di dati utilizzando metodi come l‚Äôapprendimento semi-supervisionato (Chapelle, Scholkopf, e Zien 2009), che gli esseri umani possono poi esaminare e correggere. Questo pu√≤ far risparmiare una notevole quantit√† di tempo, soprattutto per set di dati di grandi dimensioni.\nApprendimento attivo: I modelli di intelligenza artificiale possono identificare i dati pi√π informativi in un dataset, che possono quindi essere riordinati per priorit√† per l‚Äôannotazione umana. Questo pu√≤ aiutare a migliorare la qualit√† del set di dati etichettato riducendo al contempo il tempo di annotazione complessivo.\nControllo qualit√†: I modelli di intelligenza artificiale possono identificare e segnalare potenziali errori nelle annotazioni umane, contribuendo a garantire l‚Äôaccuratezza e la coerenza del set di dati etichettato.\n\n\nChapelle, O., B. Scholkopf, e A. Zien Eds. 2009. ¬´Semi-Supervised Learning (Chapelle, O. et al., Eds.; 2006) [Book reviews]¬ª. IEEE Trans. Neural Networks 20 (3): 542‚Äì42. https://doi.org/10.1109/tnn.2009.2015974.\n\n\n\n\n\n\nFigura¬†5.12: Strategie per acquisire ulteriori dati di addestramento etichettati. Fonte: Standford AI Lab.\n\n\n\nEcco alcuni esempi di come l‚Äôannotazione assistita dall‚Äôintelligenza artificiale √® stata proposta come utile:\n\nImmagini mediche: L‚Äôannotazione assistita dall‚Äôintelligenza artificiale etichetta le immagini mediche, come scansioni MRI (Magnetic Resonance Imaging) e raggi X (Krishnan, Rajpurkar, e Topol 2022). Annotare attentamente i set di dati medici √® estremamente impegnativo, soprattutto su larga scala, poich√© gli esperti del settore sono scarsi e diventano costosi. Ci√≤ pu√≤ aiutare ad addestrare i modelli di intelligenza artificiale per diagnosticare malattie e altre condizioni mediche in modo pi√π accurato ed efficiente.\nAuto a guida autonoma: L‚Äôannotazione assistita dall‚Äôintelligenza artificiale viene utilizzata per etichettare immagini e video di auto a guida autonoma. Ci√≤ pu√≤ aiutare ad addestrare i modelli di intelligenza artificiale per identificare oggetti sulla strada, come altri veicoli, pedoni e segnali stradali.\nSocial media: L‚Äôannotazione assistita dall‚Äôintelligenza artificiale etichetta i post sui social media come immagini e video. Ci√≤ pu√≤ aiutare ad addestrare i modelli di intelligenza artificiale a identificare e classificare diversi tipi di contenuti, come notizie, pubblicit√† e post personali.\n\n\nKrishnan, Rayan, Pranav Rajpurkar, e Eric J. Topol. 2022. ¬´Self-supervised learning in medicine and healthcare¬ª. Nat. Biomed. Eng. 6 (12): 1346‚Äì52. https://doi.org/10.1038/s41551-022-00914-1.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Data Engineering</span>"
    ]
  },
  {
    "objectID": "contents/core/data_engineering/data_engineering.it.html#controllo-della-versione-dei-dati",
    "href": "contents/core/data_engineering/data_engineering.it.html#controllo-della-versione-dei-dati",
    "title": "5¬† Data Engineering",
    "section": "5.7 Controllo della Versione dei Dati",
    "text": "5.7 Controllo della Versione dei Dati\nI sistemi di produzione sono costantemente inondati da volumi di dati fluttuanti e in aumento, che determinano la rapida comparsa di numerose repliche di dati. Questi dati in aumento servono come base per l‚Äôaddestramento di modelli di apprendimento automatico. Ad esempio, un‚Äôazienda di vendita globale impegnata nella previsione delle vendite riceve continuamente dati sul comportamento dei consumatori. Allo stesso modo, i sistemi sanitari che formulano modelli predittivi per la diagnosi delle malattie acquisiscono costantemente nuovi dati sui pazienti. Le applicazioni TinyML, come l‚Äôindividuazione delle parole chiave, sono molto affamate di dati per quanto riguarda la quantit√† di dati generati. Di conseguenza, √® fondamentale un monitoraggio meticoloso delle versioni dei dati e delle prestazioni del modello corrispondente.\nIl ‚ÄúData Version Control‚Äù [controllo delle versioni dei dati] offre una metodologia strutturata per gestire in modo efficiente alterazioni e versioni di set di dati. Facilita il monitoraggio delle modifiche, conserva pi√π versioni e garantisce riproducibilit√† e tracciabilit√† nei progetti incentrati sui dati. Inoltre, il controllo delle versioni dei dati offre la versatilit√† di rivedere e utilizzare versioni specifiche in base alle necessit√†, garantendo che ogni fase dell‚Äôelaborazione dei dati e dello sviluppo del modello possa essere riesaminata e verificata in modo preciso e semplice. Ha una variet√† di usi pratici -\nGestione del Rischio: Il controllo della versione dei dati consente trasparenza e responsabilit√† monitorando le versioni del set di dati.\nCollaborazione ed Efficienza: Un facile accesso a diverse versioni del set di dati in un unico posto pu√≤ migliorare la condivisione dei dati di controllo specifici e consentire una collaborazione efficiente.\nRiproducibilit√†: Il controllo della versione dei dati consente di monitorare le prestazioni dei modelli riguardanti diverse versioni dei dati, e quindi di abilitare la riproducibilit√†.\nConcetti Chiave\n\nCommit: √à un‚Äôistantanea immutabile dei dati in un momento specifico, che rappresenta una versione univoca. Ogni commit √® associato a un identificatore univoco per consentire\nBranch: I ‚Äúbranch‚Äù [rami] consentono a sviluppatori e specialisti dei data di discostarsi dalla linea di sviluppo principale e continuare a lavorare in modo indipendente senza influenzare altri rami. Ci√≤ √® particolarmente utile quando si sperimentano nuove funzionalit√† o modelli, consentendo sviluppo e sperimentazione paralleli senza il rischio di danneggiare il ramo principale stabile.\nMerge: I ‚ÄúMerge‚Äù [unioni] aiutano a integrare le modifiche da rami diversi mantenendo l‚Äôintegrit√† dei dati.\n\nCon il controllo della versione dei dati in atto, possiamo tracciare le modifiche mostrate in Figura¬†5.13, riprodurre i risultati precedenti ripristinando le versioni precedenti e collaborare in modo sicuro ramificando e isolando le modifiche.\n\n\n\n\n\n\nFigura¬†5.13: Data versioning.\n\n\n\nSistemi di Data Version Control pi√π Diffusi\n[DVC]: √à l‚Äôacronimo di Data Version Control ed √® uno strumento open source e leggero che funziona su Git Hub e supporta tutti i tipi di formati di dati. Pu√≤ integrarsi perfettamente nel flusso di lavoro se Git viene utilizzato per gestire il codice. Cattura le versioni dei dati e dei modelli nei commit Git mentre li archivia in locale o sul cloud (ad esempio, AWS, Google Cloud, Azure). Questi dati e modelli (ad esempio, artefatti di ML) sono definiti nei file di metadati, che vengono aggiornati a ogni commit. Pu√≤ consentire il monitoraggio delle metriche dei modelli su diverse versioni dei dati.\nlakeFS: √à uno strumento open source che supporta il controllo della versione dei dati sui ‚Äúdata lake‚Äù. Supporta molte operazioni simili a git, come i ‚Äúbranch‚Äù e il ‚Äúmerge‚Äù dei dati, nonch√© il ripristino delle versioni precedenti dei dati. Ha anche una funzionalit√† UI unica, che semplifica notevolmente l‚Äôesplorazione e la gestione dei dati.\nGit LFS: √à utile per il controllo della versione dei dataset di dimensioni ridotte. Utilizza le funzionalit√† di ‚Äúbranch‚Äù e ‚Äúmerge‚Äù native di Git, ma √® limitato nel tracciamento delle metriche, nel ripristino delle versioni precedenti o nell‚Äôintegrazione con i ‚Äúdata lake‚Äù.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Data Engineering</span>"
    ]
  },
  {
    "objectID": "contents/core/data_engineering/data_engineering.it.html#ottimizzazione-dei-dati-per-lia-embedded",
    "href": "contents/core/data_engineering/data_engineering.it.html#ottimizzazione-dei-dati-per-lia-embedded",
    "title": "5¬† Data Engineering",
    "section": "5.8 Ottimizzazione dei Dati per l‚ÄôIA Embedded",
    "text": "5.8 Ottimizzazione dei Dati per l‚ÄôIA Embedded\nI creatori che lavorano su sistemi embedded potrebbero avere priorit√† insolite quando puliscono i loro dataset. Da un lato, i modelli potrebbero essere sviluppati per casi d‚Äôuso insolitamente specifici, che richiedono un filtraggio intensivo dei dataset. Mentre altri modelli di linguaggio naturale possono essere in grado di trasformare qualsiasi discorso in testo, un modello per un sistema embedded pu√≤ essere incentrato su un singolo compito limitato, come il rilevamento di una parola chiave. Di conseguenza, i creatori possono filtrare in modo aggressivo grandi quantit√† di dati perch√© devono affrontare un determinato compito. Un sistema di intelligenza artificiale embedded pu√≤ anche essere legato a specifici dispositivi hardware o ambienti. Ad esempio, un modello video potrebbe dover elaborare immagini da un singolo tipo di telecamera, che verr√† montata solo sui campanelli nei quartieri residenziali. In questo scenario, i creatori possono scartare le immagini se provengono da un diverso tipo di telecamera, mostrano il tipo sbagliato di scenario o sono state scattate dall‚Äôaltezza o dall‚Äôangolazione sbagliate.\nD‚Äôaltra parte, ci si aspetta spesso che i sistemi di IA embedded forniscano prestazioni particolarmente accurate in contesti imprevedibili del mondo reale. Ci√≤ pu√≤ portare i creatori a progettare set di dati per rappresentare variazioni nei potenziali input e promuovere la robustezza del modello. Di conseguenza, possono definire un ambito ristretto per il loro progetto ma poi puntare a una copertura approfondita entro quei limiti. Ad esempio, i creatori del modello del campanello menzionato sopra potrebbero provare a coprire le variazioni nei dati derivanti da:\n\nQuartieri geograficamente, socialmente e architettonicamente diversi\nDiversi tipi di illuminazione artificiale e naturale\nDiverse stagioni e condizioni meteorologiche\nOstruzioni (ad esempio gocce di pioggia o scatole di consegna che oscurano la visuale della telecamera)\n\nCome descritto sopra, i creatori possono prendere in considerazione il crowdsourcing o la generazione sintetica di dati per includere queste varianti.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Data Engineering</span>"
    ]
  },
  {
    "objectID": "contents/core/data_engineering/data_engineering.it.html#sec-data-transparency",
    "href": "contents/core/data_engineering/data_engineering.it.html#sec-data-transparency",
    "title": "5¬† Data Engineering",
    "section": "5.9 Trasparenza dei Dati",
    "text": "5.9 Trasparenza dei Dati\nFornendo una documentazione chiara e dettagliata, i creatori possono aiutare gli sviluppatori a capire come utilizzare al meglio i loro set di dati. Diversi gruppi hanno suggerito formati di documentazione standardizzati per i set di dati, come Data Cards (Pushkarna, Zaldivar, e Kjartansson 2022), datasheet (Gebru et al. 2021), data statement (Bender e Friedman 2018), o Data Nutrition Labels (Holland et al. 2020). Quando rilasciano un dataset, i creatori possono descrivere quali tipi di dati hanno raccolto, come li hanno raccolti ed etichettati e quali tipi di casi d‚Äôuso potrebbero essere adatti o meno al set di dati. Quantitativamente, potrebbe essere opportuno mostrare quanto bene il set di dati rappresenti gruppi diversi (ad esempio, gruppi di genere diversi, telecamere diverse).\n\nGebru, Timnit, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daum√© III, e Kate Crawford. 2021. ¬´Datasheets for datasets¬ª. Commun. ACM 64 (12): 86‚Äì92. https://doi.org/10.1145/3458723.\n\nBender, Emily M., e Batya Friedman. 2018. ¬´Data Statements for Natural Language Processing: Toward Mitigating System Bias and Enabling Better Science¬ª. Transactions of the Association for Computational Linguistics 6 (dicembre): 587‚Äì604. https://doi.org/10.1162/tacl_a_00041.\n\nHolland, Sarah, Ahmed Hosny, Sarah Newman, Joshua Joseph, e Kasia Chmielinski. 2020. ¬´The Dataset Nutrition Label: A Framework to Drive Higher Data Quality Standards¬ª. In Data Protection and Privacy. Hart Publishing. https://doi.org/10.5040/9781509932771.ch-001.\nFigura¬†5.14 mostra un esempio di una scheda dati per un set di dati di computer vision (CV). Include alcune informazioni di base sul set di dati e istruzioni su come utilizzarlo, inclusi i ‚Äúbias‚Äù noti.\n\n\n\n\n\n\nFigura¬†5.14: Data card che descrive un dataset CV. Fonte: Pushkarna, Zaldivar, e Kjartansson (2022).\n\n\nPushkarna, Mahima, Andrew Zaldivar, e Oddur Kjartansson. 2022. ¬´Data Cards: Purposeful and Transparent Dataset Documentation for Responsible AI¬ª. In 2022 ACM Conference on Fairness, Accountability, and Transparency. ACM. https://doi.org/10.1145/3531146.3533231.\n\n\nTenere traccia della provenienza dei dati, essenzialmente le origini e il viaggio di ogni dato attraverso la pipeline dei dati, non √® solo una buona pratica, ma un requisito essenziale per la qualit√†. La provenienza dei dati contribuisce in modo significativo alla trasparenza dei sistemi di machine learning. I sistemi trasparenti semplificano l‚Äôanalisi dei dati, consentendo una migliore identificazione e rettifica di errori, bias o incongruenze. Ad esempio, se un modello di ML addestrato su dati medici non √® performante in aree specifiche, tracciare la provenienza pu√≤ aiutare a identificare se il problema riguarda i metodi di raccolta dati, i gruppi demografici rappresentati nei dati o altri fattori. Questo livello di trasparenza non aiuta solo a eseguire il debug del sistema, ma svolge anche un ruolo cruciale nel migliorare la qualit√† complessiva dei dati. Migliorando l‚Äôaffidabilit√† e la credibilit√† del set di dati, la provenienza dei dati migliora anche le prestazioni del modello e la sua accettabilit√† tra gli utenti finali.\nQuando si produce la documentazione, i creatori devono anche specificare come gli utenti possono accedere al dataset e come questo verr√† mantenuto nel tempo. Ad esempio, gli utenti potrebbero dover sottoporsi a una formazione o ricevere un‚Äôautorizzazione speciale dai creatori prima di accedere a un set di dati di informazioni protette, come con molti dataset medici. In alcuni casi, gli utenti potrebbero non accedere direttamente ai dati. Devono invece inviare il loro modello per essere addestrato sull‚Äôhardware dei creatori del set di dati, seguendo una configurazione di apprendimento ‚Äúfederato‚Äù (Aledhari et al. 2020). I creatori possono anche descrivere per quanto tempo il dataset rimarr√† accessibile, come gli utenti possono inviare feedback su eventuali errori che scoprono e se ci sono piani per aggiornare il set di dati.\n\nAledhari, Mohammed, Rehma Razzak, Reza M. Parizi, e Fahad Saeed. 2020. ¬´Federated Learning: A Survey on Enabling Technologies, Protocols, and Applications¬ª. #IEEE_O_ACC# 8: 140699‚Äì725. https://doi.org/10.1109/access.2020.3013541.\nAlcune leggi e normative promuovono anche la trasparenza dei dati attraverso nuovi requisiti per le organizzazioni:\n\nIl ‚ÄúGeneral Data Protection Regulation (GDPR)‚Äù nell‚ÄôUnione Europea: Stabilisce requisiti rigorosi per l‚Äôelaborazione e la protezione dei dati personali dei cittadini dell‚ÄôUE. Impone policy sulla privacy in linguaggio semplice che spiegano chiaramente quali dati vengono raccolti, perch√© vengono utilizzati, per quanto tempo vengono archiviati e con chi vengono condivisi. Il GDPR impone inoltre che le informative sulla privacy debbano includere dettagli sulla base giuridica per l‚Äôelaborazione, i trasferimenti di dati, i periodi di conservazione, i diritti di accesso e cancellazione e le informazioni di contatto per i responsabili del trattamento dei dati.\nIl ‚ÄúCalifornia‚Äôs Consumer Privacy Act‚Äù (CCPA): Il CCPA richiede policy sulla privacy chiare e diritti di esclusione per vendere dati personali. In modo significativo, stabilisce anche i diritti dei consumatori di essere interpellati per la divulgazione dei propri dati specifici. Le aziende devono fornire copie delle informazioni personali raccolte e dettagli su come vengono utilizzate, quali categorie vengono raccolte e cosa ricevono le terze parti. I consumatori possono identificare dati che ritengono debbano essere pi√π accurati. La legge rappresenta un importante passo avanti nel potenziamento dell‚Äôaccesso ai dati personali.\n\nGarantire la trasparenza dei dati presenta diverse sfide, soprattutto perch√© richiede molto tempo e risorse finanziarie. I sistemi di dati sono anche piuttosto complessi e la trasparenza completa pu√≤ richiedere tempo. La trasparenza completa pu√≤ anche sopraffare i consumatori con troppi dettagli. Infine, √® anche importante bilanciare il compromesso tra trasparenza e privacy.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Data Engineering</span>"
    ]
  },
  {
    "objectID": "contents/core/data_engineering/data_engineering.it.html#licenze",
    "href": "contents/core/data_engineering/data_engineering.it.html#licenze",
    "title": "5¬† Data Engineering",
    "section": "5.10 Licenze",
    "text": "5.10 Licenze\nMolti dataset di alta qualit√† provengono da fonti proprietarie o contengono informazioni protette da copyright. Ci√≤ introduce le licenze come una competenza legale impegnativa. Le aziende desiderose di addestrare sistemi di ML devono impegnarsi in trattative per ottenere licenze che garantiscano l‚Äôaccesso legale a questi dataset. Inoltre, i termini delle licenze possono imporre restrizioni sulle applicazioni dei dati e sui metodi di condivisione. Il mancato rispetto di queste licenze pu√≤ avere gravi conseguenze.\nAd esempio, ImageNet, uno dei dataset pi√π ampiamente utilizzati per la ricerca sulla visione artificiale, √® un caso emblematico. La maggior parte delle sue immagini √® stata ottenuta da fonti online pubbliche senza esplicita autorizzazione, suscitando preoccupazioni etiche (Prabhu e Birhane, 2020). L‚Äôaccesso al set di dati ImageNet per le aziende richiede la registrazione e l‚Äôadesione ai suoi termini di utilizzo, che limitano l‚Äôuso commerciale (ImageNet, 2021). I principali attori come Google e Microsoft investono in modo significativo nella concessione di licenze per i set di dati per migliorare i loro sistemi di visione di ML. Tuttavia, il fattore costo limita l‚Äôaccessibilit√† per i ricercatori di aziende pi√π piccole con budget limitati.\nIl dominio legale della concessione di licenze per i dati ha visto casi importanti che aiutano a definire i parametri di utilizzo corretto. Un esempio importante √® Authors Guild, Inc.¬†contro Google, Inc. Questa causa del 2005 sosteneva che il progetto di scansione di libri di Google violava i diritti d‚Äôautore visualizzando frammenti senza autorizzazione. Tuttavia, i tribunali alla fine si sono pronunciati a favore di Google, sostenendo il ‚Äúfair use‚Äù [correttezza] in base alla natura trasformativa della creazione di un indice ricercabile e della visualizzazione di estratti limitati di testo. Questo precedente fornisce alcune basi legali per sostenere che le protezioni del ‚Äúfair use‚Äù si applicano all‚Äôindicizzazione di set di dati e alla generazione di campioni rappresentativi per l‚Äôapprendimento automatico. Tuttavia, le restrizioni di licenza rimangono vincolanti, quindi un‚Äôanalisi completa dei termini di licenza √® fondamentale. Il caso dimostra perch√© le negoziazioni con i fornitori di dati sono importanti per consentire un utilizzo legale entro limiti accettabili.\nNuove Normative sui Dati e le Loro Implicazioni\nAnche le nuove normative sui dati hanno un impatto sulle pratiche di licenza. Il panorama legislativo si sta evolvendo con normative come l‚ÄôArtificial Intelligence Act dell‚ÄôUE, che √® pronto a regolamentare lo sviluppo e l‚Äôuso dei sistemi di intelligenza artificiale all‚Äôinterno dell‚ÄôUnione Europea (UE). Questa legislazione:\n\nClassifica i sistemi di IA in base al rischio.\nImpone prerequisiti di sviluppo e utilizzo.\nSottolinea la qualit√† dei dati, la trasparenza, la supervisione umana e la responsabilit√†.\n\nInoltre, l‚ÄôEU Act affronta le dimensioni etiche e le sfide operative in settori quali sanit√† e finanza. Gli elementi chiave includono il divieto di sistemi di intelligenza artificiale che presentano rischi ‚Äúinaccettabili‚Äù, condizioni rigorose per sistemi ad alto rischio e obblighi minimi per sistemi di intelligenza artificiale a ‚Äúrischio limitato‚Äù. Il proposto ‚ÄúEuropean AI Board‚Äù supervisioner√† e garantir√† l‚Äôimplementazione di una regolamentazione efficiente.\nProblemi nell‚ÄôAssemblaggio di Dataset di Training ML\nProblemi complessi di licenza relativi a dati proprietari, leggi sul copyright e normative sulla privacy limitano le opzioni per l‚Äôassemblaggio dei set di dati di training ML. Tuttavia, espandere l‚Äôaccessibilit√† tramite licenze pi√π aperte o collaborazioni di dati pubblico-private potrebbe accelerare notevolmente il progresso del settore e gli standard etici.\nA volte, alcune parti di un dataset potrebbero dover essere rimosse o oscurate per rispettare gli accordi di utilizzo dei dati o proteggere informazioni sensibili. Ad esempio, un set di dati di informazioni utente potrebbe contenere nomi, dettagli di contatto e altri dati identificativi che potrebbero dover essere rimossi dal set di dati; questo avviene molto tempo dopo che il set di dati √® gi√† stato attivamente reperito e utilizzato per l‚Äôaddestramento dei modelli. Analogamente, un dataset che include contenuti protetti da copyright o segreti commerciali potrebbe dover filtrare tali parti prima di essere distribuito. Leggi come il General Data Protection Regulation (GDPR), il California Consumer Privacy Act (CCPA) e L‚ÄôAmended Act on the Protection of Personal Information (APPI) sono state approvate per garantire il diritto all‚Äôoblio. Queste normative impongono legalmente ai fornitori di modelli di cancellare i dati degli utenti su richiesta.\nI raccoglitori e i fornitori di dati devono essere in grado di adottare misure appropriate per de-identificare o filtrare qualsiasi informazione proprietaria, concessa in licenza, riservata o regolamentata, se necessario. A volte, gli utenti possono richiedere esplicitamente che i loro dati vengano rimossi.\nLa possibilit√† di aggiornare il set di dati rimuovendo i dati consentir√† ai creatori di rispettare gli obblighi legali ed etici relativi al loro utilizzo e alla privacy. Tuttavia, la capacit√† di rimuovere i dati presenta alcune limitazioni importanti. Dobbiamo considerare che alcuni modelli potrebbero essere gi√† stati addestrati sul dataset e non esiste un modo chiaro o noto per eliminare l‚Äôeffetto di un particolare campione di dati dalla rete addestrata. Non esiste un meccanismo di cancellazione. Quindi, ci√≤ solleva la questione: il modello dovrebbe essere riaddestrato da zero ogni volta che viene rimosso un campione? Questa √® un‚Äôopzione costosa. Una volta che i dati sono stati utilizzati per addestrare un modello, la semplice rimozione dal set di dati originale potrebbe non eliminare completamente il suo impatto sul comportamento del modello. Sono necessarie nuove ricerche sugli effetti della rimozione dei dati sui modelli gi√† addestrati e se sia necessario un ri-addestramento completo per evitare di conservare artefatti di dati eliminati. Ci√≤ presenta una considerazione importante quando si bilanciano gli obblighi di licenza dei dati con l‚Äôefficienza e la praticit√† in un sistema di ML in evoluzione e distribuito.\nLa licenza del dataset √® un dominio poliedrico che interseca tecnologia, etica e legge. Comprendere queste complessit√† diventa fondamentale per chiunque crei set di dati durante l‚Äôingegneria dei dati, man mano che il mondo si evolve.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Data Engineering</span>"
    ]
  },
  {
    "objectID": "contents/core/data_engineering/data_engineering.it.html#conclusione",
    "href": "contents/core/data_engineering/data_engineering.it.html#conclusione",
    "title": "5¬† Data Engineering",
    "section": "5.11 Conclusione",
    "text": "5.11 Conclusione\nI dati sono il componente fondamentale dei sistemi di intelligenza artificiale. Senza dati di qualit√†, anche gli algoritmi di apprendimento automatico pi√π avanzati falliranno. L‚Äôingegneria dei dati comprende il processo end-to-end di raccolta, archiviazione, elaborazione e gestione dei dati per alimentare lo sviluppo di modelli di apprendimento automatico. Si inizia con la definizione chiara del problema principale e degli obiettivi, che guidano una raccolta dati efficace. I dati possono essere reperiti da diversi mezzi, tra cui dataset esistenti, web scraping, crowdsourcing e generazione di dati sintetici. Ogni approccio comporta compromessi tra costi, velocit√†, privacy e specificit√†. Una volta raccolti i dati, un‚Äôetichettatura ponderata tramite annotazione manuale o assistita dall‚Äôintelligenza artificiale consente la creazione di set di dati di training di alta qualit√†. Un‚Äôarchiviazione adeguata in database, ‚Äúwarehouse‚Äù o ‚Äúlake‚Äù facilita l‚Äôaccesso e l‚Äôanalisi. I metadati forniscono dettagli contestuali sui dati. L‚Äôelaborazione dei dati trasforma i dati grezzi in un formato pulito e coerente per lo sviluppo di modelli di apprendimento automatico. In tutta questa pipeline, la trasparenza attraverso la documentazione e il tracciamento della provenienza √® fondamentale per l‚Äôetica, la verificabilit√† e la riproducibilit√†. I protocolli di licenza dei dati regolano anche l‚Äôaccesso e l‚Äôuso legale dei dati. Le principali sfide nell‚Äôingegneria dei dati includono rischi per la privacy, lacune di rappresentazione, restrizioni legali sui dati proprietari e la necessit√† di bilanciare vincoli concorrenti come velocit√† e qualit√†. Progettando attentamente dati di training di alta qualit√†, i professionisti dell‚Äôapprendimento automatico possono sviluppare sistemi di intelligenza artificiale accurati, robusti e responsabili, tra cui applicazioni embedded e TinyML.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Data Engineering</span>"
    ]
  },
  {
    "objectID": "contents/core/data_engineering/data_engineering.it.html#sec-data-engineering-resource",
    "href": "contents/core/data_engineering/data_engineering.it.html#sec-data-engineering-resource",
    "title": "5¬† Data Engineering",
    "section": "5.12 Risorse",
    "text": "5.12 Risorse\nEcco un elenco curato di risorse per supportare studenti e insegnanti nei loro percorsi di apprendimento e insegnamento. Lavoriamo continuamente per espandere questa raccolta e presto aggiungeremo nuovi esercizi.\n\n\n\n\n\n\nSlide\n\n\n\n\n\nQueste slide sono uno strumento prezioso per gli insegnanti per tenere lezioni e per gli studenti per rivedere il materiale secondo il proprio ritmo. Incoraggiamo studenti e docenti a sfruttare queste slide per migliorare la loro comprensione e facilitare un trasferimento efficace delle conoscenze.\n\nData Engineering: Overview.\nFeature engineering.\nData Standards: Speech Commands.\nCrowdsourcing Data for the Long Tail.\nReusing and Adapting Existing Datasets.\nResponsible Data Collection.\nRilevamento Dati Anomali:\n\nAnomaly Detection: Overview.\nAnomaly Detection: Challenges.\nAnomaly Detection: Datasets.\nAnomaly Detection: Using Autoencoders.\n\n\n\n\n\n\n\n\n\n\n\nVideo\n\n\n\n\n\n\nProssimamente.\n\n\n\n\n\n\n\n\n\n\nEsercizi\n\n\n\n\n\nPer rafforzare i concetti trattati in questo capitolo, abbiamo curato una serie di esercizi che sfidano gli studenti ad applicare le proprie conoscenze e ad approfondire la propria comprensione.\n\nEsercizio¬†5.1\nEsercizio¬†5.2\nEsercizio¬†5.3\nEsercizio¬†5.4\nEsercizio¬†5.5",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Data Engineering</span>"
    ]
  },
  {
    "objectID": "contents/core/frameworks/frameworks.it.html",
    "href": "contents/core/frameworks/frameworks.it.html",
    "title": "6¬† Framework di IA",
    "section": "",
    "text": "6.1 Panoramica\nI framework di machine learning [apprendimento automatico] forniscono gli strumenti e l‚Äôinfrastruttura per creare, addestrare e distribuire in modo efficiente modelli di apprendimento automatico. In questo capitolo esploreremo l‚Äôevoluzione e le capacit√† chiave dei principali framework come TensorFlow (TF), PyTorch e framework specializzati per dispositivi embedded. Ci immergeremo nei componenti come grafi computazionali, algoritmi di ottimizzazione, accelerazione hardware e altro che consentono agli sviluppatori di creare rapidamente modelli performanti. Comprendere questi framework √® essenziale per sfruttare la potenza del deep learning in tutto lo spettro, dal cloud ai dispositivi edge [periferici].\nI framework di apprendimento automatico gestiscono gran parte della complessit√† dello sviluppo di modelli tramite API di alto livello e linguaggi specifici per dominio che consentono ai professionisti di creare rapidamente modelli combinando componenti e astrazioni predefiniti. Ad esempio, framework come TensorFlow e PyTorch forniscono API Python per definire architetture di reti neurali utilizzando livelli, ottimizzatori, set di dati e altro. Ci√≤ consente un‚Äôiterazione rapida rispetto alla codifica di ogni dettaglio del modello partendo da zero.\nUna capacit√† chiave offerta da questi framework √® rappresentata dai motori di training distribuiti che possono scalare l‚Äôaddestramento del modello su cluster di GPU e TPU. Ci√≤ rende possibile il training di modelli all‚Äôavanguardia con miliardi o trilioni di parametri su vasti set di dati. I framework si integrano anche con hardware specializzato come le GPU NVIDIA per accelerare ulteriormente il training tramite ottimizzazioni come la parallelizzazione ed efficienti operazioni matriciali.\nInoltre, i framework semplificano il deploy [distribuzione] di modelli finiti in produzione tramite strumenti come TensorFlow Serving per il model serving scalabile e TensorFlow Lite per l‚Äôottimizzazione su dispositivi mobili ed edge. Altre capacit√† preziose includono visualizzazione, tecniche di ottimizzazione del modello come quantizzazione e potatura e monitoraggio delle metriche durante il training.\nI principali framework open source come TensorFlow, PyTorch e MXNet alimentano gran parte della ricerca e dello sviluppo dell‚ÄôIA oggi. Offerte commerciali come Amazon SageMaker e Microsoft Azure Machine Learning integrano questi framework open source con funzionalit√† proprietarie e strumenti aziendali.\nGli ingegneri e i professionisti del machine learning sfruttano questi framework robusti per concentrarsi su attivit√† di alto valore come architettura del modello, progettazione delle feature e ottimizzazione degli iperparametri anzich√© sull‚Äôinfrastruttura. L‚Äôobiettivo √® creare e distribuire modelli performanti che risolvano in modo efficiente i problemi del mondo reale.\nIn questo capitolo, esploreremo i principali framework cloud odierni e il modo in cui hanno adattato modelli e strumenti specificamente per la distribuzione embedded ed edge. Confronteremo modelli di programmazione, hardware supportato, capacit√† di ottimizzazione e altro ancora per comprendere appieno in che modo i framework consentono un apprendimento automatico scalabile dal cloud all‚Äôedge.",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Framework di IA</span>"
    ]
  },
  {
    "objectID": "contents/core/frameworks/frameworks.it.html#evoluzione-dei-framework",
    "href": "contents/core/frameworks/frameworks.it.html#evoluzione-dei-framework",
    "title": "6¬† Framework di IA",
    "section": "6.2 Evoluzione dei Framework",
    "text": "6.2 Evoluzione dei Framework\nI framework di apprendimento automatico si sono evoluti in modo significativo per soddisfare le diverse esigenze dei professionisti del machine learning e i progressi nelle tecniche di intelligenza artificiale. Qualche decennio fa, la creazione e l‚Äôaddestramento di modelli di apprendimento automatico richiedevano un‚Äôampia codifica e infrastruttura di basso livello. Oltre alla necessit√† di una codifica di basso livello, la prima ricerca sulle reti neurali era limitata da dati e potenza di calcolo insufficienti. Tuttavia, i framework di apprendimento automatico si sono evoluti notevolmente nell‚Äôultimo decennio per soddisfare le crescenti esigenze dei professionisti e i rapidi progressi nelle tecniche di deep learning [apprendimento profondo]. Il rilascio di grandi set di dati come ImageNet (Deng et al. 2009) e i progressi nel calcolo parallelo con GPU hanno sbloccato il potenziale per reti neurali molto pi√π profonde.\n\nDeng, Jia, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, e Fei-Fei Li. 2009. ¬´ImageNet: A large-scale hierarchical image database¬ª. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, 248‚Äì55. IEEE. https://doi.org/10.1109/cvpr.2009.5206848.\n\nTeam, The Theano Development, Rami Al-Rfou, Guillaume Alain, Amjad Almahairi, Christof Angermueller, Dzmitry Bahdanau, Nicolas Ballas, et al. 2016. ¬´Theano: A Python framework for fast computation of mathematical expressions¬ª. https://arxiv.org/abs/1605.02688.\n\nJia, Yangqing, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross Girshick, Sergio Guadarrama, e Trevor Darrell. 2014. ¬´Caffe: Convolutional Architecture for Fast Feature Embedding¬ª. In Proceedings of the 22nd ACM international conference on Multimedia, 675‚Äì78. ACM. https://doi.org/10.1145/2647868.2654889.\n\nKrizhevsky, Alex, Ilya Sutskever, e Geoffrey E. Hinton. 2012. ¬´ImageNet Classification with Deep Convolutional Neural Networks¬ª. In Advances in Neural Information Processing Systems 25: 26th Annual Conference on Neural Information Processing Systems 2012. Proceedings of a meeting held December 3-6, 2012, Lake Tahoe, Nevada, United States, a cura di Peter L. Bartlett, Fernando C. N. Pereira, Christopher J. C. Burges, L√©on Bottou, e Kilian Q. Weinberger, 1106‚Äì14. https://proceedings.neurips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html.\n\nChollet, Fran√ßois. 2018. ¬´Introduction to keras¬ª. March 9th.\n\nTokui, Seiya, Ryosuke Okuta, Takuya Akiba, Yusuke Niitani, Toru Ogawa, Shunta Saito, Shuji Suzuki, Kota Uenishi, Brian Vogel, e Hiroyuki Yamazaki Vincent. 2019. ¬´Chainer: A Deep Learning Framework for Accelerating the Research Cycle¬ª. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, 5:1‚Äì6. ACM. https://doi.org/10.1145/3292500.3330756.\n\nSeide, Frank, e Amit Agarwal. 2016. ¬´Cntk: Microsoft‚Äôs Open-Source Deep-Learning Toolkit¬ª. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2135‚Äì35. ACM. https://doi.org/10.1145/2939672.2945397.\n\nAnsel, Jason, Edward Yang, Horace He, Natalia Gimelshein, Animesh Jain, Michael Voznesensky, Bin Bao, et al. 2024. ¬´PyTorch 2: Faster Machine Learning Through Dynamic Python Bytecode Transformation and Graph Compilation¬ª. In Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2, a cura di Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d‚ÄôAlch√©-Buc, Emily B. Fox, e Roman Garnett, 8024‚Äì35. ACM. https://doi.org/10.1145/3620665.3640366.\nI primi framework di apprendimento automatico, Theano di Team et al. (2016) e Caffe di Jia et al. (2014), sono stati sviluppati da istituzioni accademiche. Theano √® stato creato dal Montreal Institute for Learning Algorithms, mentre Caffe √® stato sviluppato dal Berkeley Vision and Learning Center. Nel crescente interesse per il deep learning dovuto alle prestazioni all‚Äôavanguardia di AlexNet Krizhevsky, Sutskever, e Hinton (2012) sul dataset ImageNet, aziende private e singole persone hanno iniziato a sviluppare framework di ML, dando vita a Keras di Chollet (2018), Chainer di Tokui et al. (2019), TensorFlow di Google (Yu et al. 2018), CNTK di Microsoft (Seide e Agarwal 2016) e PyTorch di Facebook (Ansel et al. 2024).\nMolti di questi framework ML possono essere suddivisi in framework di alto livello, di basso livello e di grafi computazionali statici e dinamici. I framework di alto livello forniscono un livello di astrazione pi√π elevato rispetto a quelli di basso livello. I framework di alto livello hanno funzioni e moduli predefiniti per attivit√† ML comuni, come la creazione, l‚Äôaddestramento e la valutazione di modelli ML comuni, la preelaborazione dei dati, le funzionalit√† di progettazione e la visualizzazione dei dati, che i framework di basso livello non hanno. Pertanto, i framework di alto livello possono risultare pi√π facili da usare ma sono meno personalizzabili rispetto a quelli di basso livello (ad esempio, gli utenti di framework di basso livello possono definire livelli personalizzati, funzioni ‚Äúloss‚Äù [di perdita], algoritmi di ottimizzazione, ecc.). Esempi di framework di alto livello sono TensorFlow/Keras e PyTorch. Esempi di framework ML di basso livello includono TensorFlow con API di basso livello, Theano, Caffe, Chainer e CNTK.\nFramework come Theano e Caffe utilizzavano grafi computazionali statici, che richiedevano la definizione anticipata dell‚Äôarchitettura completa del modello, limitandone cos√¨ la flessibilit√†. Al contrario, i grafici dinamici vengono costruiti al volo per uno sviluppo pi√π iterativo. Intorno al 2016, framework come PyTorch e TensorFlow 2.0 hanno iniziato ad adottare grafici dinamici, offrendo maggiore flessibilit√† per lo sviluppo del modello. Discuteremo di questi concetti e dettagli pi√π avanti nella sezione Training dell‚ÄôIA.\nLo sviluppo di questi framework ha suscitato un‚Äôesplosione di dimensioni e complessit√† del modello nel tempo, dai primi perceptron multilayer e reti convoluzionali ai moderni trasformatori con miliardi o trilioni di parametri. Nel 2016, i modelli ResNet di He et al. (2016) hanno raggiunto un‚Äôaccuratezza ImageNet record con oltre 150 livelli e 25 milioni di parametri. Poi, nel 2020, il modello linguistico GPT-3 di OpenAI (Brown et al. 2020) ha spinto i parametri a un sorprendente numero di 175 miliardi utilizzando il parallelismo del modello nei framework per addestrare migliaia di GPU e TPU.\n\nHe, Kaiming, Xiangyu Zhang, Shaoqing Ren, e Jian Sun. 2016. ¬´Deep Residual Learning for Image Recognition¬ª. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 770‚Äì78. IEEE. https://doi.org/10.1109/cvpr.2016.90.\n\nBrown, Tom B., Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, et al. 2020. ¬´Language Models are Few-Shot Learners¬ª. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, a cura di Hugo Larochelle, Marc‚ÄôAurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, e Hsuan-Tien Lin. https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html.\nOgni generazione di framework ha sbloccato nuove capacit√† che hanno alimentato il progresso:\n\nTheano e TensorFlow (2015) hanno introdotto grafi computazionali e differenziazione automatica per semplificare la creazione di modelli.\nCNTK (2016) ha aperto la strada a un addestramento distribuito efficiente combinando parallelismo di modelli e dati.\nPyTorch (2016) ha fornito programmazione imperativa e grafici dinamici per una sperimentazione flessibile.\nTensorFlow 2.0 (2019) ha impostato di default l‚Äôesecuzione Eager per intuitivit√† e debug.\nTensorFlow Graphics (2020) ha aggiunto strutture dati 3D per gestire nuvole di punti e mesh.\n\nNegli ultimi anni, il panorama dei framework di apprendimento automatico si √® notevolmente consolidato. Figura¬†6.2 illustra questa convergenza, mostrando che TensorFlow e PyTorch sono diventati i framework ML prevalentemente dominanti, rappresentando collettivamente oltre il 95% dei framework ML utilizzati nella ricerca e nella produzione. Sebbene entrambi i framework siano diventati importanti, presentano caratteristiche distinte. Figura¬†6.1 traccia un contrasto tra gli attributi di TensorFlow e PyTorch, contribuendo a spiegare il loro predominio complementare nel settore.\n\n\n\n\n\n\nFigura¬†6.1: PyTorch e TensorFlow: Caratteristiche e Funzioni. Fonte: K&C\n\n\n\n\n\n\n\n\n\nFigura¬†6.2: Popolarit√† dei framework ML negli Stati Uniti misurata dalle ricerche web di Google. Fonte: Google.\n\n\n\nUn approccio unico non funziona bene in tutto lo spettro, dal cloud ai piccoli dispositivi edge. Diversi framework rappresentano varie filosofie sull‚Äôesecuzione di grafici, API dichiarative rispetto a quelle imperative e altro ancora. Le dichiarative definiscono cosa dovrebbe fare il programma, mentre le imperative si concentrano su come dovrebbe essere fatto passo dopo passo. Ad esempio, TensorFlow utilizza l‚Äôesecuzione di grafici e la modellazione in stile dichiarativo, mentre PyTorch adotta l‚Äôesecuzione rapida e la modellazione imperativa per una maggiore flessibilit√† con Python. Ogni approccio comporta dei compromessi che discuteremo in Sezione 6.3.7.\nGli attuali framework avanzati consentono ai professionisti di sviluppare e distribuire modelli sempre pi√π complessi, un fattore chiave dell‚Äôinnovazione nel campo dell‚Äôintelligenza artificiale. Questi framework continuano a evolversi ed espandere le loro capacit√† per la prossima generazione di machine learning. Per capire come questi sistemi continuano a evolversi, approfondiremo TensorFlow come esempio di come il framework sia cresciuto in complessit√† nel tempo.",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Framework di IA</span>"
    ]
  },
  {
    "objectID": "contents/core/frameworks/frameworks.it.html#sec-deep_dive_into_tensorflow",
    "href": "contents/core/frameworks/frameworks.it.html#sec-deep_dive_into_tensorflow",
    "title": "6¬† Framework di IA",
    "section": "6.3 Approfondimento su TensorFlow",
    "text": "6.3 Approfondimento su TensorFlow\nTensorFlow √® stato sviluppato dal team di Google Brain ed √® stato rilasciato come libreria software open source il 9 novembre 2015. √à stato progettato per il calcolo numerico utilizzando grafici di flusso di dati e da allora √® diventato popolare per un‚Äôampia gamma di applicazioni di apprendimento automatico e deep learning.\nTensorFlow √® un framework di training e inferenza che fornisce funzionalit√† integrate per gestire tutto, dalla creazione e training del modello alla distribuzione, come mostrato in Figura¬†6.3. Sin dal suo sviluppo iniziale, l‚Äôecosistema TensorFlow √® cresciuto fino a includere molte diverse ‚Äúvariet√†‚Äù di TensorFlow, ciascuna pensata per consentire agli utenti di supportare ML su diverse piattaforme. In questa sezione, discuteremo principalmente solo del pacchetto core.\n\n6.3.1 Ecosistema TF\n\nTensorFlow Core: pacchetto principale con cui interagiscono la maggior parte degli sviluppatori. Fornisce una piattaforma completa e flessibile per definire, addestrare e distribuire modelli di apprendimento automatico. Include tf.keras come API di alto livello.\nTensorFlow Lite: progettato per distribuire modelli leggeri su dispositivi mobili, embedded ed edge. Offre strumenti per convertire i modelli TensorFlow in un formato pi√π compatto adatto a dispositivi con risorse limitate e fornisce modelli pre-addestrati ottimizzati per dispositivi mobili.\nTensorFlow Lite Micro: progettato per eseguire modelli di apprendimento automatico su microcontrollori con risorse minime. Funziona senza la necessit√† di supporto del sistema operativo, librerie C o C++ standard o allocazione dinamica della memoria, utilizzando solo pochi kilobyte di memoria.\nTensorFlow.js: libreria JavaScript che consente l‚Äôaddestramento e la distribuzione di modelli di apprendimento automatico direttamente nel browser o su Node.js. Fornisce inoltre strumenti per il porting di modelli TensorFlow pre-addestrati nel formato browser-friendly.\nTensorFlow su dispositivi edge (Coral): piattaforma di componenti hardware e strumenti software di Google che consente l‚Äôesecuzione di modelli TensorFlow su dispositivi edge, sfruttando Edge TPU per l‚Äôaccelerazione.\nTensorFlow Federated (TFF): framework per l‚Äôapprendimento automatico e altri calcoli su dati decentralizzati. TFF facilita l‚Äôapprendimento ‚Äúfederato‚Äù, consentendo l‚Äôaddestramento del modello su molti dispositivi senza centralizzare i dati.\nTensorFlow Graphics: libreria per l‚Äôutilizzo di TensorFlow per svolgere attivit√† correlate alla grafica, tra cui l‚Äôelaborazione di forme 3D e nuvole di punti, utilizzando il deep learning.\nTensorFlow Hub: repository di componenti di modelli di apprendimento automatico riutilizzabili che consente agli sviluppatori di riutilizzare componenti di modelli pre-addestrati, facilitando l‚Äôapprendimento per trasferimento e la composizione del modello.\nTensorFlow Serving: framework progettato per servire e distribuire modelli di apprendimento automatico per l‚Äôinferenza in ambienti di produzione. Fornisce strumenti per il versioning e l‚Äôaggiornamento dinamico dei modelli distribuiti senza interruzione del servizio.\nTensorFlow Extended (TFX): piattaforma end-to-end progettata per distribuire e gestire pipeline di apprendimento automatico in ambienti di produzione. TFX comprende validazione dei dati, pre-elaborazione, addestramento del modello, convalida e componenti di servizio.\n\n\n\n\n\n\n\nFigura¬†6.3: Panoramica dell‚Äôarchitettura di TensorFlow 2.0. Fonte: Tensorflow.\n\n\n\nTensorFlow √® stato sviluppato per affrontare le limitazioni di DistBelief (Yu et al. 2018)‚Äîil framework in uso presso Google dal 2011 al 2015‚Äîoffrendo flessibilit√† lungo tre direttrici: 1) definizione di nuovi livelli [livelli], 2) perfezionamento degli algoritmi di training e 3) definizione di nuovi algoritmi di training. Per comprendere quali limitazioni di DistBelief hanno portato allo sviluppo di TensorFlow, faremo prima una breve panoramica dell‚Äôarchitettura del server dei parametri utilizzata da DistBelief (Dean et al. 2012).\n\nYu, Yuan, Martƒ±ÃÅn Abadi, Paul Barham, Eugene Brevdo, Mike Burrows, Andy Davis, Jeff Dean, et al. 2018. ¬´Dynamic control flow in large-scale machine learning¬ª. In Proceedings of the Thirteenth EuroSys Conference, 265‚Äì83. ACM. https://doi.org/10.1145/3190508.3190551.\n\nDean, Jeffrey, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Quoc V. Le, Mark Z. Mao, et al. 2012. ¬´Large Scale Distributed Deep Networks¬ª. In Advances in Neural Information Processing Systems 25: 26th Annual Conference on Neural Information Processing Systems 2012. Proceedings of a meeting held December 3-6, 2012, Lake Tahoe, Nevada, United States, a cura di Peter L. Bartlett, Fernando C. N. Pereira, Christopher J. C. Burges, L√©on Bottou, e Kilian Q. Weinberger, 1232‚Äì40. https://proceedings.neurips.cc/paper/2012/hash/6aca97005c68f1206823815f66102863-Abstract.html.\nL‚Äôarchitettura Parameter Server (PS) √® un design popolare per distribuire il training di modelli di apprendimento automatico, in particolare reti neurali profonde, su pi√π macchine. L‚Äôidea fondamentale √® di separare l‚Äôarchiviazione e la gestione dei parametri del modello dal calcolo utilizzato per aggiornare tali parametri. In genere, i server dei parametri gestiscono l‚Äôarchiviazione e la gestione dei parametri del modello, suddividendoli su pi√π server. I processi worker eseguono le attivit√† di calcolo, tra cui l‚Äôelaborazione dei dati e il calcolo dei gradienti, che vengono poi inviati ai server dei parametri per l‚Äôaggiornamento.\nStorage: I processi del server dei parametri stateful [con stato] gestivano l‚Äôarchiviazione e la gestione dei parametri del modello. Data l‚Äôampia scala dei modelli e la natura distribuita del sistema, questi parametri erano condivisi tra pi√π server dei parametri. Ogni server manteneva una parte dei parametri del modello, rendendolo \"stateful\" poich√© doveva mantenere e gestire questo stato durante il processo di training.\nComputation: I processi worker, che potevano essere eseguiti in parallelo, erano senza stato e puramente computazionali. Elaboravano dati e calcolavano gradienti senza mantenere alcuno stato o memoria a lungo termine (M. Li et al. 2014). I worker non conservavano informazioni tra le diverse attivit√†. Invece, comunicavano periodicamente con i server dei parametri per recuperare i parametri pi√π recenti e restituire i gradienti calcolati.\n\nLi, Mu, David G. Andersen, Alexander J. Smola, e Kai Yu. 2014. ¬´Communication Efficient Distributed Machine Learning with the Parameter Server¬ª. In Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014, December 8-13 2014, Montreal, Quebec, Canada, a cura di Zoubin Ghahramani, Max Welling, Corinna Cortes, Neil D. Lawrence, e Kilian Q. Weinberger, 19‚Äì27. https://proceedings.neurips.cc/paper/2014/hash/1ff1de774005f8da13f42943881c655f-Abstract.html.\n\n\n\n\n\n\nEsercizio¬†6.1: TensorFlow Core\n\n\n\n\n\nAndiamo a comprendere in modo completo gli algoritmi di apprendimento automatico di base utilizzando TensorFlow e le loro applicazioni pratiche nell‚Äôanalisi dei dati e nella modellazione predittiva. Inizieremo con la regressione lineare per prevedere i tassi di sopravvivenza dal set di dati del Titanic. Poi, utilizzando TensorFlow, costruiremo classificatori per identificare diverse specie di fiori in base ai loro attributi. Successivamente, utilizzeremo l‚Äôalgoritmo K-Means e la sua applicazione nella segmentazione dei set di dati in cluster coesi. Infine, applicheremo modelli hidden [nascosti] di Markov (HMM) per prevedere i pattern meteorologici.\n\n\n\n\n\n\n\n\n\n\nEsercizio¬†6.2: TensorFlow Lite\n\n\n\n\n\nQui vedremo come costruire un modello di apprendimento automatico in miniatura per microcontrollori. Costruiremo una mini rete neurale semplificata per apprendere dai dati anche con risorse limitate e ottimizzata per l‚Äôimplementazione riducendo il nostro modello per un uso efficiente sui microcontrollori. TensorFlow Lite, una potente tecnologia derivata da TensorFlow, riduce i modelli per dispositivi minuscoli e aiuta ad abilitare funzionalit√† sul dispositivo come il riconoscimento delle immagini nei dispositivi smart [intelligenti]. Viene utilizzato nell‚Äôedge computing per consentire analisi e decisioni pi√π rapide nei dispositivi che elaborano i dati localmente.\n\n\n\n\nDistBelief e la sua architettura definita sopra sono stati fondamentali per abilitare il deep learning distribuito in Google, ma hanno anche introdotto delle limitazioni che hanno motivato lo sviluppo di TensorFlow:\n\n\n6.3.2 Grafico di Calcolo Statico\nI parametri del modello sono distribuiti su vari server di parametri nell‚Äôarchitettura del server di parametri. Poich√© DistBelief √® stato progettato principalmente per il paradigma della rete neurale, i parametri corrispondevano a una struttura di rete neurale fissa. Se il computation graph [grafico di calcolo] fosse dinamico, la distribuzione e il coordinamento dei parametri diventerebbero significativamente pi√π complicati. Ad esempio, una modifica nel grafico potrebbe richiedere l‚Äôinizializzazione di nuovi parametri o la rimozione di quelli esistenti, complicando le attivit√† di gestione e sincronizzazione dei server di parametri. Ci√≤ ha reso pi√π difficile implementare modelli al di fuori del framework neurale o modelli che richiedevano grafici di calcolo dinamici.\nTensorFlow √® stato progettato come un framework di calcolo pi√π generale che esprime il calcolo come un grafico del flusso di dati. Ci√≤ consente una pi√π ampia variet√† di modelli e algoritmi di apprendimento automatico al di fuori delle reti neurali e fornisce flessibilit√† nel perfezionamento dei modelli.\n\n\n6.3.3 Usabilit√† & Distribuzione\nIl modello del server dei parametri delinea i ruoli (nodi worker e server dei parametri) ed √® ottimizzato per i deployment [distribuzioni] dei data center, che potrebbero essere ottimali solo per alcuni casi d‚Äôuso. Ad esempio, questa divisione introduce overhead o complessit√† sui dispositivi edge o in altri ambienti non data center.\nTensorFlow √® stato creato per funzionare su pi√π piattaforme, dai dispositivi mobili e edge all‚Äôinfrastruttura cloud. Mirava anche a essere pi√π leggero e intuitivo per gli sviluppatori e a fornire facilit√† d‚Äôuso tra il training locale e quello distribuito.\n\n\n6.3.4 Progettazione dell‚ÄôArchitettura\nInvece di utilizzare l‚Äôarchitettura del server dei parametri, TensorFlow distribuisce i task [attivit√†] su un cluster. Queste attivit√† sono processi denominati che possono comunicare su una rete e ciascuna pu√≤ eseguire la struttura principale di TensorFlow, il grafico del flusso di dati e l‚Äôinterfaccia con vari dispositivi di elaborazione (come CPU o GPU). Questo grafico [grafo] √® una rappresentazione diretta in cui i nodi simboleggiano le operazioni di elaborazione e gli edge rappresentano i tensori (dati) che scorrono tra queste operazioni.\nNonostante l‚Äôassenza di server di parametri tradizionali, alcuni ‚Äútask PS‚Äù memorizzano e gestiscono parametri che ricordano i server di parametri di altri sistemi. I task rimanenti, che di solito gestiscono calcoli, elaborazione dati e gradienti, sono denominati ‚Äútask worker‚Äù. I task PS di TensorFlow possono eseguire qualsiasi calcolo rappresentabile dal grafico del flusso di dati, il che significa che non sono limitati solo all‚Äôarchiviazione dei parametri e il calcolo pu√≤ essere distribuito. Questa capacit√† li rende significativamente pi√π versatili e offre agli utenti il potere di programmare i task PS utilizzando l‚Äôinterfaccia TensorFlow standard, la stessa che userebbero per definire i loro modelli. Come accennato in precedenza, la struttura dei grafici del flusso di dati li rende anche intrinsecamente buoni per il parallelismo, consentendo l‚Äôelaborazione di grandi set di dati.\n\n\n6.3.5 Funzionalit√† Native & Keras\nTensorFlow include librerie per aiutare gli utenti a sviluppare e distribuire pi√π modelli specifici per i casi d‚Äôuso e, poich√© questo framework √® open source, questo elenco continua a crescere. Queste librerie affrontano l‚Äôintero ciclo di vita dello sviluppo ML: preparazione dei dati, creazione di modelli, distribuzione e IA responsabile.\nUno dei maggiori vantaggi di TensorFlow √® la sua integrazione con Keras, anche se, come vedremo nella prossima sezione, Pytorch ha recentemente aggiunto un‚Äôintegrazione Keras. Keras √® un altro framework ML creato per essere estremamente intuitivo e, di conseguenza, ha un alto livello di astrazione. Parleremo di Keras pi√π approfonditamente pi√π avanti in questo capitolo. Tuttavia, quando si discute della sua integrazione con TensorFlow, √® importante notare che era stato originariamente creato per essere indipendente dal backend. Ci√≤ significa che gli utenti potrebbero astrarre queste complessit√†, offrendo un modo pi√π pulito e intuitivo per definire e addestrare modelli senza preoccuparsi di problemi di compatibilit√† con diversi backend. Gli utenti di TensorFlow hanno evidenziato alcuni problemi sull‚Äôusabilit√† e la leggibilit√† dell‚ÄôAPI di TensorFlow, quindi, man mano che TF acquisiva importanza, ha integrato Keras come API di alto livello. Questa integrazione ha offerto grandi vantaggi agli utenti di TensorFlow poich√© ha introdotto una leggibilit√† e una portabilit√† pi√π intuitive dei modelli, sfruttando comunque le potenti funzionalit√† di backend, il supporto di Google e l‚Äôinfrastruttura per distribuire i modelli su varie piattaforme.\n\n\n\n\n\n\nEsercizio¬†6.3: Esplorazione di Keras: Creazione, Addestramento e Valutazione di Reti Neurali\n\n\n\n\n\nQui, impareremo come utilizzare Keras, un‚ÄôAPI di reti neurali di alto livello, per lo sviluppo e l‚Äôaddestramento (training) di modelli. Esploreremo l‚ÄôAPI funzionale per la creazione di modelli concisi, comprenderemo le classi ‚Äúloss‚Äù e metriche per la valutazione dei modelli e utilizzeremo gli ottimizzatori nativi per aggiornare i parametri del modello durante l‚Äôaddestramento. Inoltre, scopriremo come definire layer e metriche personalizzati su misura per le nostre esigenze. Infine, esamineremo i cicli di addestramento di Keras per semplificare il processo di addestramento delle reti neurali su grandi set di dati. Questa conoscenza ci consentir√† di costruire e ottimizzare modelli di reti neurali in varie applicazioni di machine learning e intelligenza artificiale.\n\n\n\n\n\n\n6.3.6 Limitazioni e Sfide\nTensorFlow √® uno dei framework di deep learning pi√π popolari, ma ha dovuto affrontare critiche e debolezze, principalmente legate all‚Äôusabilit√† e all‚Äôutilizzo delle risorse. Sebbene vantaggioso, il ritmo rapido degli aggiornamenti tramite il supporto di Google ha talvolta portato a problemi di retrocompatibilit√†, funzioni deprecate e documentazione instabile. Inoltre, anche con l‚Äôimplementazione di Keras, la sintassi e la curva di apprendimento di TensorFlow possono risultare difficili per i nuovi utenti. Un‚Äôaltra critica importante di TensorFlow √® il suo elevato overhead e consumo di memoria dovuto alla gamma di librerie integrate e al supporto. Sebbene le versioni ridotte possano risolvere alcuni di questi problemi, potrebbero comunque essere limitate in ambienti con risorse limitate.\n\n\n6.3.7 PyTorch & TensorFlow\nPyTorch e TensorFlow si sono affermati come leader nel settore. Entrambi i framework offrono funzionalit√† robuste ma differiscono per filosofie di progettazione, facilit√† d‚Äôuso, ecosistema e capacit√† di distribuzione.\nFilosofia di Progettazione e Paradigma di Programmazione: PyTorch utilizza un grafo computazionale dinamico denominato eager execution [esecuzione rapida]. Ci√≤ lo rende intuitivo e facilita il debug poich√© le operazioni vengono eseguite immediatamente e possono essere ispezionate al volo. Al contrario, le versioni precedenti di TensorFlow erano incentrate su un grafo computazionale statico, che richiedeva la definizione completa del grafico prima dell‚Äôesecuzione. Tuttavia, TensorFlow 2.0 ha introdotto la ‚Äúeager execution‚Äù per default, rendendolo pi√π allineato con PyTorch. La natura dinamica di PyTorch e l‚Äôapproccio basato su Python hanno consentito la sua semplicit√† e flessibilit√†, in particolare per la prototipazione rapida. L‚Äôapproccio grafico statico di TensorFlow nelle sue versioni precedenti aveva una curva di apprendimento pi√π ripida; l‚Äôintroduzione di TensorFlow 2.0, con la sua integrazione Keras come API di alto livello, ha semplificato notevolmente il processo di sviluppo.\nDeployment: PyTorch √® fortemente favorito negli ambienti di ricerca, ma la distribuzione dei modelli PyTorch in contesti di produzione √® sempre stata un problema. Tuttavia, la distribuzione √® diventata pi√π fattibile con l‚Äôintroduzione di TorchScript, lo strumento TorchServe e PyTorch Mobile. TensorFlow si distingue per la sua forte scalabilit√† e capacit√† di distribuzione, in particolare su piattaforme embedded e mobili con TensorFlow Lite. TensorFlow Serving e TensorFlow.js facilitano ulteriormente la distribuzione in vari ambienti, conferendogli cos√¨ una portata pi√π ampia nell‚Äôecosistema.\nPrestazioni: Entrambi i framework offrono un‚Äôaccelerazione hardware efficiente per le loro operazioni. Tuttavia, TensorFlow ha un flusso di lavoro di ottimizzazione leggermente pi√π robusto, come il compilatore XLA (Accelerated Linear Algebra), che pu√≤ aumentare ulteriormente le prestazioni. Il suo grafo computazionale statico era anche vantaggioso per alcune ottimizzazioni nelle prime versioni.\nEcosistema: PyTorch ha un ecosistema in crescita con strumenti come TorchServe per servire modelli e librerie come TorchVision, TorchText e TorchAudio per domini specifici. Come abbiamo detto prima, TensorFlow ha un ecosistema ampio e maturo. TensorFlow Extended (TFX) fornisce una piattaforma end-to-end per distribuire pipeline di apprendimento automatico di produzione. Altri strumenti e librerie includono TensorFlow Lite, TensorFlow Lite Micro, TensorFlow.js, TensorFlow Hub e TensorFlow Serving. Tabella¬†6.1 fornisce un‚Äôanalisi comparativa:\n\n\n\nTabella¬†6.1: Confronto tra PyTorch e TensorFlow.\n\n\n\n\n\n\n\n\n\n\nAspetto\nPytorch\nTensorFlow\n\n\n\n\nFilosofia di Progettazione\nGrafo computazionale dinamico (eager execution)\nGrafo computazionale statico (prime versioni); Esecuzione rapida in TensorFlow 2.0\n\n\nDeployment\nTradizionalmente impegnativa; Migliorata con TorchScript e TorchServe\nScalabile, specialmente su piattaforme embedded con TensorFlow Lite\n\n\nPrestazioni e Ottimizzazione\nAccelerazione GPU efficiente\nOttimizzazione robusta con compilatore XLA\n\n\nEcosistema\nTorchServe, TorchVision, TorchText, TorchAudio, PyTorch Mobile\nTensorFlow Extended (TFX), TensorFlow Lite, TensorFlow Lite Micro TensorFlow.js, TensorFlow Hub, TensorFlow Serving\n\n\nFacilit√† d‚Äôuso\nPreferito per il suo approccio Pythonic e la prototipazione rapida\nCurva di apprendimento inizialmente ripida; Semplificato con Keras in TensorFlow 2.0",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Framework di IA</span>"
    ]
  },
  {
    "objectID": "contents/core/frameworks/frameworks.it.html#componenti-di-base-del-framework",
    "href": "contents/core/frameworks/frameworks.it.html#componenti-di-base-del-framework",
    "title": "6¬† Framework di IA",
    "section": "6.4 Componenti di Base del Framework",
    "text": "6.4 Componenti di Base del Framework\nDopo aver introdotto i popolari framework di machine learning e aver fornito un confronto di alto livello, questa sezione presenter√† le funzionalit√† principali che formano la struttura di questi framework. Tratter√† la struttura speciale chiamata tensori, che questi framework utilizzano per gestire pi√π facilmente dati multidimensionali complessi. Si vedr√† anche come questi framework rappresentano diversi tipi di architetture di reti neurali e le loro operazioni richieste tramite grafi computazionali. Inoltre, si vedr√† come offrono strumenti che rendono lo sviluppo di modelli di machine learning pi√π astratto ed efficiente, come caricatori di dati, algoritmi di ottimizzazione delle perdite implementate, tecniche di differenziazione efficienti e la capacit√† di accelerare il processo di training su acceleratori hardware.\n\n6.4.1 Strutture Dati Tensoriali\nCome mostrato nella figura, i vettori possono essere rappresentati come una pila di numeri in un array unidimensionale. Le matrici seguono la stessa idea e si possono pensare a loro come a molti vettori impilati l‚Äôuno sull‚Äôaltro, rendendoli bidimensionali. I tensori di dimensioni superiori funzionano allo stesso modo. Un tensore tridimensionale, come illustrato in Figura¬†6.4, √® semplicemente un set di matrici impilate l‚Äôuna sull‚Äôaltra in un‚Äôaltra direzione. Pertanto, vettori e matrici possono essere considerati casi speciali di tensori con dimensioni 1D e 2D, rispettivamente.\n\n\n\n\n\n\nFigura¬†6.4: Visualizzazione della Struttura Dati Tensore.\n\n\n\nI tensori offrono una struttura flessibile che pu√≤ rappresentare dati in dimensioni superiori. Figura¬†6.5 illustra come questo concetto si applica ai dati immagine. Come mostrato nella figura, le immagini non sono rappresentate da una sola matrice di valori pixel. Invece, hanno in genere tre canali, dove ogni canale √® una matrice contenente valori pixel che rappresentano l‚Äôintensit√† di rosso, verde o blu. Insieme, questi canali creano un‚Äôimmagine colorata. Senza i tensori, archiviare tutte queste informazioni da pi√π matrici pu√≤ risultare complesso. Tuttavia, come illustra Figura¬†6.5, i tensori facilitano il contenimento dei dati dell‚Äôimmagine in un‚Äôunica struttura tridimensionale, in cui ciascun numero rappresenta un determinato valore di colore in una posizione specifica nell‚Äôimmagine.\n\n\n\n\n\n\nFigura¬†6.5: Visualizzazione della struttura dell‚Äôimmagine colorata che pu√≤ essere facilmente memorizzata come un Tensore 3D. Credito: Niklas Lang\n\n\n\nNon finisce qui. Se volessimo archiviare una serie di immagini, potremmo usare un tensore quadridimensionale, in cui la nuova dimensione rappresenta immagini diverse. Ci√≤ significa che si stanno archiviando pi√π immagini, ciascuna con tre matrici che rappresentano i tre canali del colore. Questo d√† un‚Äôidea dell‚Äôutilit√† dei tensori quando si gestiscono dati multidimensionali in modo efficiente.\nI tensori hanno anche un attributo unico che consente ai framework di calcolare automaticamente i gradienti, semplificando l‚Äôimplementazione di modelli complessi e algoritmi di ottimizzazione. Nel machine learning, come discusso nel Capitolo 3, la backpropagation richiede di prendere la derivata delle equazioni. Una delle caratteristiche principali dei tensori in PyTorch e TensorFlow √® la loro capacit√† di tracciare i calcoli e calcolare i gradienti. Ci√≤ √® fondamentale per la backpropagation nelle reti neurali. Ad esempio, in PyTorch, si pu√≤ usare l‚Äôattributo requires_grad, che consente di calcolare e memorizzare automaticamente i gradienti durante il ‚Äúbackward pass‚Äù, facilitando il processo di ottimizzazione. Analogamente, in TensorFlow, tf.GradientTape registra le operazioni per la differenziazione automatica.\nSi consideri questa semplice equazione matematica che si vuole differenziare. Matematicamente, il calcolo del gradiente si effettua nel modo seguente:\nDato: \\[\ny = x^2\n\\]\nLa derivata di \\(y\\) rispetto a \\(x\\) √®: \\[\n\\frac{dy}{dx} = 2x\n\\]\nQuando \\(x = 2\\): \\[\n\\frac{dy}{dx} = 2*2 = 4\n\\]\nIl gradiente di \\(y\\) rispetto a \\(x\\), con \\(x = 2\\), √® 4.\nUna potente caratteristica dei tensori in PyTorch e TensorFlow √® la loro capacit√† di calcolare facilmente le derivate (gradienti). Ecco gli esempi di codice corrispondenti in PyTorch e TensorFlow:\n\nPyTorchTensorFlow\n\n\nimport torch\n\n# Create a tensor with gradient tracking\nx = torch.tensor(2.0, requires_grad=True)\n\n# Define a simple function\ny = x ** 2\n\n# Compute the gradient\ny.backward()\n\n# Print the gradient\nprint(x.grad)\n\n# Output\ntensor(4.0)\n\n\nimport tensorflow as tf\n\n# Create a tensor with gradient tracking\nx = tf.Variable(2.0)\n\n# Define a simple function\nwith tf.GradientTape() as tape:\n    y = x ** 2\n\n# Compute the gradient\ngrad = tape.gradient(y, x)\n\n# Print the gradient\nprint(grad)\n\n# Output\ntf.Tensor(4.0, shape=(), dtype=float32)\n\n\n\nQuesta differenziazione automatica √® una potente funzionalit√† dei tensori in framework come PyTorch e TensorFlow, che semplifica l‚Äôimplementazione e l‚Äôottimizzazione di modelli complessi di apprendimento automatico.\n\n\n6.4.2 Grafi computazionali\n\nDefinizione di Grafico\nI grafi computazionali sono una componente chiave di framework di deep learning come TensorFlow e PyTorch. Ci consentono di esprimere architetture di reti neurali complesse in modo efficiente e differenziato. Un grafo computazionale √® costituito da un grafo aciclico diretto (directed acyclic graph, DAG) in cui ogni nodo rappresenta un‚Äôoperazione o una variabile e gli spigoli rappresentano le dipendenze dei dati tra di essi.\n√à importante distinguere i grafici computazionali dai diagrammi delle reti neurali, come quelli dei ‚Äúmultilayer perceptrons (MLP)‚Äù, che rappresentano nodi e layer. I diagrammi di reti neurali, come illustrato nel Capitolo 3, visualizzano l‚Äôarchitettura e il flusso di dati attraverso nodi e layer, fornendo una comprensione intuitiva della struttura del modello. Al contrario, i grafi computazionali forniscono una rappresentazione di basso livello delle operazioni matematiche sottostanti e delle dipendenze dei dati necessarie per implementare e addestrare queste reti.\nAd esempio, un nodo potrebbe rappresentare un‚Äôoperazione di moltiplicazione di matrici, prendendo due matrici di input (o tensori) e producendo una matrice di output (o tensore). Per visualizzarlo, si consideri il semplice esempio in Figura¬†6.6. Il grafo aciclico orientato calcola \\(z = x \\times y\\), dove ogni variabile √® solo un numero.\n\n\n\n\n\n\nFigura¬†6.6: Esempio di base di un grafo computazionale.\n\n\n\nFramework come TensorFlow e PyTorch creano grafi computazionali per implementare le architetture delle reti neurali che in genere rappresentiamo con diagrammi. Quando si definisce un layer di rete neurale nel codice (ad esempio, un ‚Äúlayer denso‚Äù in TensorFlow), il framework costruisce un grafo computazionale che include tutte le operazioni necessarie (come moltiplicazione di matrici, addizione e funzioni di attivazione) e le relative dipendenze dai dati. Questo grafo consente al framework di gestire in modo efficiente il flusso di dati, ottimizzare l‚Äôesecuzione delle operazioni e calcolare automaticamente i gradienti per l‚Äôaddestramento. Internamente, i grafi computazionali rappresentano astrazioni per layer comuni come quelli convoluzionali, di pooling, ricorrenti e densi, con dati che includono attivazioni, pesi e bias rappresentati in tensori. Questa rappresentazione consente un calcolo efficiente, sfruttando la struttura del grafico per parallelizzare le operazioni e applicare ottimizzazioni.\nAlcuni livelli comuni che i grafi computazionali potrebbero implementare includono layer convoluzionali, di attenzione, ricorrenti e densi. I layer fungono da astrazioni di livello superiore che definiscono calcoli specifici in cima alle operazioni di base rappresentate nel grafo. Ad esempio, un layer Denso esegue la moltiplicazione e l‚Äôaddizione di matrici tra tensori di input, peso e bias. √à importante notare che un layer opera su tensori come input e output; il layer stesso non √® un tensore. Alcune differenze chiave tra layer e tensori sono:\n\nI layer contengono stati come pesi e bias. I tensori sono senza stato, contengono solo dati.\nI layer possono modificare lo stato interno durante l‚Äôaddestramento. I tensori sono immutabili/di sola lettura.\nI layer sono astrazioni di livello superiore. I tensori sono a un livello inferiore e rappresentano direttamente dati e operazioni matematiche.\nI layer definiscono pattern di calcolo fissi. I tensori scorrono tra i livelli durante l‚Äôesecuzione.\nI layer vengono utilizzati indirettamente durante la creazione di modelli. I tensori scorrono tra i livelli durante l‚Äôesecuzione.\n\nQuindi, mentre i tensori sono una struttura dati fondamentale che i layer consumano e producono, i layer hanno funzionalit√† aggiuntive per definire operazioni parametrizzate e addestramento. Mentre un layer configura le operazioni tensoriali in background, il layer rimane distinto dagli oggetti tensoriali. L‚Äôastrazione del layer rende la creazione e l‚Äôaddestramento di reti neurali molto pi√π intuitive. Questa astrazione consente agli sviluppatori di creare modelli impilando insieme questi layer senza implementare la logica del layer. Ad esempio, la chiamata di tf.keras.layers.Conv2D in TensorFlow crea un layer convoluzionale. Il framework gestisce il calcolo delle convoluzioni, la gestione dei parametri, ecc. Ci√≤ semplifica lo sviluppo del modello, consentendo agli sviluppatori di concentrarsi sull‚Äôarchitettura anzich√© sulle implementazioni di basso livello. Le astrazioni dei layer utilizzano implementazioni altamente ottimizzate per le prestazioni. Consentono inoltre la portabilit√†, poich√© la stessa architettura pu√≤ essere eseguita su backend hardware diversi come GPU e TPU.\nInoltre, i grafi computazionali includono funzioni di attivazione come ReLU, sigmoide e tanh che sono essenziali per le reti neurali e molti framework le forniscono come astrazioni standard. Queste funzioni introducono non linearit√† che consentono ai modelli di approssimare funzioni complesse. I framework le forniscono come operazioni semplici e predefinite che possono essere utilizzate durante la costruzione di modelli, ad esempio if.nn.relu in TensorFlow. Questa astrazione consente flessibilit√†, poich√© gli sviluppatori possono facilmente scambiare le funzioni di attivazione per ottimizzare le prestazioni. Le attivazioni predefinite sono inoltre ottimizzate dal framework per un‚Äôesecuzione pi√π rapida.\nNegli ultimi anni, modelli come ResNets e MobileNets sono emersi come architetture popolari, con i framework attuali che li pre-confezionano come grafi computazionali. Invece di preoccuparsi dei dettagli, gli sviluppatori possono utilizzarli come punto di partenza, personalizzandoli secondo necessit√† sostituendo i layer. Ci√≤ semplifica e velocizza lo sviluppo del modello, evitando di reinventare le architetture da zero. I modelli predefiniti includono implementazioni ben collaudate e ottimizzate che garantiscono buone prestazioni. Il loro design modulare consente inoltre di trasferire le funzionalit√† apprese a nuove attivit√† tramite apprendimento tramite trasferimento. Queste architetture predefinite forniscono i mattoni ad alte prestazioni per creare rapidamente modelli robusti.\nQueste astrazioni di layer, funzioni di attivazione e architetture predefinite fornite dai framework costituiscono un grafo computazionale. Quando un utente definisce un layer in un framework (ad esempio, tf.keras.layers.Dense()), il framework configura nodi e bordi del grafo computazionale per rappresentare tale layer. I parametri del layer come pesi e bias diventano variabili nel grafo. I calcoli del layer diventano nodi operativi (come x e y nella figura sopra). Quando si chiama una funzione di attivazione come tf.nn.relu(), il framework aggiunge un nodo operativo ReLU al grafo. Le architetture predefinite sono solo sottografi preconfigurati che possono essere inseriti nel grafo del modello. Quindi, la definizione del modello tramite astrazioni di alto livello crea un grafo computazionale: i livelli, le attivazioni e le architetture che utilizziamo diventano nodi e rami del grafo.\nCostruiamo implicitamente un grafo computazionale quando definiamo un‚Äôarchitettura di rete neurale in un framework. Il framework utilizza questo grafo per determinare le operazioni da eseguire durante l‚Äôaddestramento e l‚Äôinferenza. I grafi computazionali offrono diversi vantaggi rispetto al codice grezzo e questa √® una delle funzionalit√† principali offerte da un buon framework di ML:\n\nRappresentazione esplicita del flusso di dati e delle operazioni\nCapacit√† di ottimizzare il grafo prima dell‚Äôesecuzione\nDifferenziazione automatica per il training\nAgnosticismo linguistico: il grafo pu√≤ essere tradotto per essere eseguito su GPU, TPU, ecc.\nPortabilit√†: il grafo pu√≤ essere serializzato, salvato e ripristinato in seguito\n\nI grafi computazionali sono i componenti fondamentali dei framework di ML. La definizione del modello tramite astrazioni di alto livello crea un grafo computazionale: i layer, le attivazioni e le architetture che utilizziamo diventano nodi e rami del grafico. I compilatori e gli ottimizzatori del framework operano su questo grafo per generare codice eseguibile. Le astrazioni forniscono un‚ÄôAPI intuitiva per gli sviluppatori per la creazione di grafi computazionali. Sotto, ci sono ancora grafi! Quindi, anche se non si possono manipolare direttamente i grafi come utente del framework, consentono di eseguire ad alto livello e in modo efficiente le specifiche del modello. Le astrazioni semplificano la creazione del modello, mentre i grafi computazionali la rendono possibile.\n\n\nGrafi Statici vs.¬†Dinamici\nI framework di deep learning hanno tradizionalmente seguito uno dei due approcci per esprimere grafi computazionali.\nGrafi statici (declare-then-execute): Con questo modello, l‚Äôintero grafo computazionale deve essere definito in anticipo prima di eseguirlo. Tutte le operazioni e le dipendenze dei dati devono essere specificate durante la fase di dichiarazione. TensorFlow originariamente seguiva questo approccio statico: i modelli venivano definiti in un contesto separato e poi veniva creata una sessione per eseguirli. Il vantaggio dei grafi statici √® che consentono un‚Äôottimizzazione pi√π aggressiva poich√© il framework pu√≤ vedere il grafo completo. Tuttavia, tende anche a essere meno flessibile per la ricerca e l‚Äôinterattivit√†. Le modifiche al grafo richiedono la nuova dichiarazione del modello completo.\nPer esempio:\nx = tf.placeholder(tf.float32)\ny = tf.matmul(x, weights) + biases\nIn questo esempio, x √® un segnaposto per i dati di input e y √® il risultato di un‚Äôoperazione di moltiplicazione di matrici seguita da un‚Äôaddizione. Il modello √® definito in questa fase di dichiarazione, in cui tutte le operazioni e le variabili devono essere specificate in anticipo.\nUna volta definito l‚Äôintero grafo, il framework lo compila e lo ottimizza. Ci√≤ significa che i passaggi computazionali sono definitivamente ‚Äúscolpiti‚Äù e il framework pu√≤ applicare varie ottimizzazioni per migliorare l‚Äôefficienza e le prestazioni. Quando in seguito si esegue il grafo, si forniscono i tensori di input effettivi e le operazioni predefinite vengono eseguite nella sequenza ottimizzata.\nQuesto approccio √® simile alla creazione di un progetto in cui ogni dettaglio √® pianificato prima dell‚Äôinizio della costruzione. Sebbene ci√≤ consenta potenti ottimizzazioni, significa anche che qualsiasi modifica al modello richiede la ridefinizione dell‚Äôintero grafo da zero.\nGrafi dinamici (define-by-run): A differenza della dichiarazione (di tutto) prima e dell‚Äôesecuzione poi, il grafo viene creato dinamicamente durante l‚Äôesecuzione. Non esiste una fase di dichiarazione separata: le operazioni vengono eseguite immediatamente come definite. Questo stile √® imperativo e flessibile, facilitando la sperimentazione.\nPyTorch utilizza grafi dinamici, creandoli al volo mentre avviene l‚Äôesecuzione. Ad esempio, si consideri il seguente frammento di codice, in cui il grafo viene creato durante l‚Äôesecuzione:\nx = torch.randn(4,784)\ny = torch.matmul(x, weights) + biases\nL‚Äôesempio sopra non ha fasi separate di compilazione/build/esecuzione. Le operazioni definiscono ed eseguono immediatamente. Con i grafi dinamici, la definizione √® intrecciata con l‚Äôesecuzione, fornendo un flusso di lavoro pi√π intuitivo e interattivo. Tuttavia, lo svantaggio √® che c‚Äô√® meno potenziale di ottimizzazione poich√© il framework vede solo il grafo mentre viene creato. Figura¬†6.7 mostra le differenze tra un grafo di calcolo statico e uno dinamico.\n\n\n\n\n\n\nFigura¬†6.7: Confronto tra grafi statici e dinamici. Fonte: Dev\n\n\n\nDi recente, la distinzione si √® offuscata poich√© i framework adottano entrambe le modalit√†. TensorFlow 2.0 passa automaticamente alla modalit√† di grafo dinamico, consentendo agli utenti di lavorare con quelli statici quando necessario. La dichiarazione dinamica offre flessibilit√† e facilit√† d‚Äôuso, rendendo i framework pi√π intuitivi, mentre i grafi statici forniscono vantaggi di ottimizzazione a scapito dell‚Äôinterattivit√†. Il framework ideale bilancia questi approcci. Tabella¬†6.2 confronta i pro e i contro dei grafi di esecuzione statici e dinamici:\n\n\n\nTabella¬†6.2: Confronto tra Grafi di Esecuzione Statici (Declare-then-execute) e Dinamici (Define-by-run), evidenziandone i rispettivi pro e contro.\n\n\n\n\n\n\n\n\n\n\nGrafo di esecuzione\nPro\nContro\n\n\n\n\nStatico (Declare-then-execute)\n\nAbilita le ottimizzazioni del grafo visualizzando il modello completo in anticipo\nPu√≤ esportare e distribuire grafici congelati\nIl grafo √® impacchettato indipendentemente dal codice\n\n\nMeno flessibile per la ricerca e l‚Äôiterazione\nLe modifiche richiedono la ricostruzione del grafo\nL‚Äôesecuzione ha fasi di compilazione ed esecuzione separate\n\n\n\nDinamico (Define-by-run)\n\nStile imperativo intuitivo come il codice Python\nAlterna la creazione del grafo con l‚Äôesecuzione\nFacile da modificare i grafi\nIl debug si adatta perfettamente al flusso di lavoro\n\n\nPi√π difficile da ottimizzare senza un grafo completo\nPossibili rallentamenti dalla creazione del grafo durante l‚Äôesecuzione\nPu√≤ richiedere pi√π memoria\n\n\n\n\n\n\n\n\n\n\n6.4.3 Tool della Pipeline dei Dati\nI grafi computazionali possono essere validi solo quanto i dati da cui apprendono e su cui lavorano. Pertanto, alimentare i dati di training in modo efficiente √® fondamentale per ottimizzare le prestazioni della ‚Äúdeep neural network‚Äù [rete neurale profonda], sebbene spesso venga trascurata come una delle funzionalit√† principali. Molti framework di IA moderni forniscono pipeline specializzate per acquisire, elaborare e aumentare i set di dati per il training del modello.\n\nData Loader\nAl centro di queste pipeline ci sono i ‚Äúdata loader‚Äù, che gestiscono esempi di training di lettura da fonti come file, database e storage di oggetti. I data loader facilitano il caricamento e la pre-elaborazione efficienti dei dati, cruciali per i modelli di deep learning. Ad esempio, la pipeline di caricamento dati tf.data di TensorFlow √® progettata per gestire questo processo. A seconda dell‚Äôapplicazione, i modelli di deep learning richiedono diversi formati di dati come file CSV o cartelle di immagini. Alcuni formati popolari includono:\n\nCSV: Un formato versatile e semplice spesso utilizzato per dati tabulari.\nTFRecord: Formato proprietario di TensorFlow, ottimizzato per le prestazioni.\nParquet: Archiviazione a colonne, che offre compressione e recupero dati efficienti.\nJPEG/PNG: Comunemente utilizzato per dati immagine.\nWAV/MP3: Formati prevalenti per dati audio.\n\nEsempi di batch di data loader per sfruttare il supporto di vettorizzazione nell‚Äôhardware. Il ‚Äúbatching‚Äù si riferisce al raggruppamento di pi√π dati per l‚Äôelaborazione simultanea, sfruttando le capacit√† di calcolo vettorizzate di hardware come le GPU. Sebbene le dimensioni tipiche dei batch siano comprese tra 32 e 512 esempi, la dimensione ottimale spesso dipende dall‚Äôingombro di memoria dei dati e dai vincoli hardware specifici. I loader avanzati possono trasmettere in streaming set di dati virtualmente illimitati da dischi e archivi cloud. Trasmettono in streaming grandi dataset da dischi o reti anzich√© caricarli completamente in memoria, consentendo dimensioni illimitate.\nI data loader possono anche mescolare i dati tra ‚Äúepoche‚Äù per la randomizzazione e le funzionalit√† di preelaborazione in parallelo con l‚Äôaddestramento del modello per accelerarne il processo. Mescolare casualmente l‚Äôordine degli esempi tra epoche di training riduce il bias e migliora la generalizzazione.\nI data loader supportano anche strategie di ‚Äúcaching‚Äù e ‚Äúprefetching‚Äù per ottimizzare la distribuzione dei dati per un addestramento del modello rapido e fluido. Il caching [memorizzazione nella cache] dei batch preelaborati consente di riutilizzarli in modo efficiente durante pi√π fasi di addestramento ed elimina l‚Äôelaborazione ridondante. Il prefetching, al contrario, comporta il precaricamento dei batch successivi, assicurando che il modello non resti mai inattivo in attesa di dati.\n\n\n\n6.4.4 Data Augmentation\nFramework di apprendimento automatico come TensorFlow e PyTorch forniscono strumenti per semplificare e snellire il processo di ‚Äúdata augmentation‚Äù [aumento dei dati], migliorando l‚Äôefficienza dell‚Äôespansione sintetica dei set di dati. Questi framework offrono funzionalit√† integrate per applicare trasformazioni casuali, come capovolgimento, ritaglio, rotazione, modifica del colore e aggiunta di rumore per le immagini. Per i dati audio, gli aumenti comuni comportano la miscelazione di clip con rumore di fondo o la modulazione di velocit√†, tono e volume.\nIntegrando gli strumenti di ‚Äúaugmentation‚Äù nella pipeline di dati, i framework consentono di applicare queste trasformazioni al volo durante ogni epoca di addestramento. Questo approccio incrementa la variazione nella distribuzione dei dati di addestramento, riducendo cos√¨ l‚Äôoverfitting e migliorando la generalizzazione del modello. Figura¬†6.8 mostra i casi di overfitting e underfitting. L‚Äôuso di ‚Äúdata loader‚Äù performanti in combinazione con ampie capacit√† di ‚Äúaugmentation‚Äù consente ai professionisti di alimentare in modo efficiente set di dati massicci e vari alle reti neurali.\n\n\n\n\n\n\nFigura¬†6.8: Overfitting e underfitting. Fonte: Aquarium Learning\n\n\n\nQueste pipeline di dati ‚Äúhands-off‚Äù rappresentano un miglioramento significativo in termini di usabilit√† e produttivit√†. Consentono agli sviluppatori di concentrarsi maggiormente sull‚Äôarchitettura del modello e meno sulla manipolazione dei dati durante l‚Äôaddestramento di modelli di deep learning.\n\n\n6.4.5 Funzioni Loss e Algoritmi di Ottimizzazione\nL‚Äôaddestramento di una rete neurale √® fondamentalmente un processo iterativo che cerca di minimizzare una funzione di loss [perdita]. L‚Äôobiettivo √® di mettere a punto i pesi e i parametri del modello per produrre previsioni vicine alle vere etichette target. I framework di apprendimento automatico hanno notevolmente semplificato questo processo offrendo funzioni di loss [perdita] e algoritmi di ottimizzazione.\nI framework di apprendimento automatico forniscono funzioni di perdita implementate che sono necessarie per quantificare la differenza tra le previsioni del modello e i valori reali. Diversi set di dati richiedono una diversa funzione di perdita per funzionare correttamente, poich√© tale funzione indica al computer l‚Äô‚Äúobiettivo‚Äù a cui mirare. Le funzioni di perdita comunemente utilizzate includono il ‚ÄúMean Squared Error (MSE)‚Äù [errore quadratico medio] per le attivit√† di regressione, la ‚ÄúCross-Entropy Loss‚Äù per le attivit√† di classificazione, e la Kullback-Leibler (KL) per i modelli probabilistici. Ad esempio, tf.keras.losses di TensorFlow contiene una serie di queste funzioni di perdita comunemente utilizzate.\nGli algoritmi di ottimizzazione vengono utilizzati per trovare in modo efficiente il set di parametri del modello che minimizzano la funzione di perdita, assicurando che il modello funzioni bene sui dati di training e si generalizzi a nuovi dati. I framework moderni sono dotati di implementazioni efficienti di diversi algoritmi di ottimizzazione, molti dei quali sono varianti della ‚Äúdiscesa del gradiente‚Äù con metodi stocastici e tassi di apprendimento adattivo. Alcuni esempi di queste varianti sono Stochastic Gradient Descent, Adagrad, Adadelta e Adam. L‚Äôimplementazione di tali varianti √® fornita in tf.keras.optimizers. Ulteriori informazioni con esempi chiari sono disponibili nella sezione Training dell‚ÄôIA.\n\n\n6.4.6 Supporto al Training del Modello\n√à richiesta una fase di compilazione prima di addestrare un modello di rete neurale definito. Durante questa fase, l‚Äôarchitettura di alto livello della rete neurale viene trasformata in un formato eseguibile ottimizzato. Questo processo comprende diverse fasi. La prima fase consiste nel costruire il grafo computazionale, che rappresenta tutte le operazioni matematiche e il flusso di dati all‚Äôinterno del modello. Ne abbiamo discusso in precedenza.\nDurante l‚Äôaddestramento, l‚Äôattenzione √® rivolta all‚Äôesecuzione del grafo computazionale. A ogni parametro all‚Äôinterno del grafo, come pesi e bias, viene assegnato un valore iniziale. A seconda del metodo di inizializzazione scelto, questo valore potrebbe essere casuale o basato su una logica predefinita.\nIl passaggio critico successivo √® l‚Äôallocazione della memoria. La memoria essenziale √® riservata alle operazioni del modello sia su CPU che su GPU, garantendo un‚Äôelaborazione efficiente dei dati. Le operazioni del modello vengono poi mappate sulle risorse hardware disponibili, in particolare GPU o TPU, per accelerare l‚Äôelaborazione. Una volta completata la compilazione, il modello viene preparato per l‚Äôaddestramento.\nIl processo di addestramento impiega vari strumenti per migliorare l‚Äôefficienza. L‚Äôelaborazione batch √® comunemente utilizzata per massimizzare la produttivit√† computazionale. Tecniche come la vettorizzazione consentono operazioni su interi array di dati anzich√© procedere elemento per elemento, il che aumenta la velocit√†. Ottimizzazioni come la ‚Äúkernel fusion‚Äù (fare riferimento al capitolo Ottimizzazioni) amalgamano pi√π operazioni in un‚Äôunica azione, riducendo al minimo il sovraccarico computazionale. Le operazioni possono anche essere segmentate in fasi, facilitando l‚Äôelaborazione simultanea di diversi mini-batch in varie parti.\nI framework eseguono costantemente il checkpoint dello stato, preservando le versioni intermedie del modello durante l‚Äôaddestramento. Ci√≤ garantisce che i progressi vengano recuperati in caso di interruzione e che l‚Äôaddestramento possa essere ripreso dall‚Äôultimo checkpoint. Inoltre, il sistema monitora attentamente le prestazioni del modello rispetto a un set di dati di convalida. Se il modello inizia a sovradimensionarsi (se le sue prestazioni sul set di convalida diminuiscono), l‚Äôaddestramento viene automaticamente interrotto, conservando risorse computazionali e tempo.\nI framework di ML incorporano una combinazione di compilazione del modello, metodi di elaborazione batch avanzati e utilit√† come il checkpoint e l‚Äôarresto anticipato. Queste risorse gestiscono gli aspetti complessi delle prestazioni, consentendo ai professionisti di concentrarsi sullo sviluppo e l‚Äôaddestramento del modello. Di conseguenza, gli sviluppatori sperimentano sia velocit√† che facilit√† quando utilizzano le capacit√† delle reti neurali.\n\n\n6.4.7 Validazione e Analisi\nDopo aver addestrato i modelli di deep learning, i framework forniscono utilit√† per valutare le prestazioni e ottenere informazioni sul funzionamento dei modelli. Questi strumenti consentono una sperimentazione e un debug disciplinati.\n\nMetriche di Valutazione\nI framework includono implementazioni di comuni metriche di valutazione per la convalida:\n\nAccuratezza: Frazione di previsioni corrette complessive. Sono ampiamente utilizzate per la classificazione.\nPrecisione: Delle previsioni positive, quante erano positive. Utile per set di dati sbilanciati.\nRichiamo: Dei positivi effettivi, quanti ne abbiamo previsti correttamente? Misura della Completezza.\nPunteggio F1: Media armonica di precisione e richiamo. Combina entrambe le metriche.\nAUC-ROC - Area sotto la curva ROC. Sono utilizzate per l‚Äôanalisi della soglia di classificazione.\nMAP - Mean Average Precision. Valuta le previsioni classificate nel recupero/rilevamento.\nMatrice di Confusione: Matrice che mostra i veri positivi, i veri negativi, i falsi positivi e i falsi negativi. Fornisce una visione pi√π dettagliata delle prestazioni di classificazione.\n\nQueste metriche quantificano le prestazioni del modello sui dati di convalida per il confronto.\n\n\nVisualizzazione\nGli strumenti di visualizzazione forniscono informazioni sui modelli:\n\nCurve di perdita: Tracciano la perdita di training e validazione nel tempo per individuare l‚Äôoverfitting.\nLoss curves [Griglie di attivazione]: Illustrano le funzionalit√† apprese dai filtri convoluzionali.\nProjection [Proiezione]: Riduce la dimensionalit√† per una visualizzazione intuitiva.\nPrecision-recall curves [Curve di richiamo della precisione]: Valutano i compromessi di classificazione. Figura¬†6.9 mostra un esempio di una curva di precisione-richiamo.\n\n\n\n\n\n\n\nFigura¬†6.9: Lettura di una curva ‚Äúprecision-recall‚Äù. Fonte: AIM\n\n\n\nStrumenti come TensorBoard per TensorFlow e TensorWatch per PyTorch consentono metriche e visualizzazioni in tempo reale durante il training.\n\n\n\n6.4.8 Programmazione differenziabile\nI metodi di addestramento per il machine learning come la backpropagation si basano sulla modifica della funzione di perdita rispetto alla modifica dei pesi (che essenzialmente √® la definizione di derivata). Pertanto, la capacit√† di addestrare rapidamente ed efficientemente grandi modelli di machine learning si basa sulla capacit√† del computer di prendere derivate. Ci√≤ rende la programmazione differenziabile uno degli elementi pi√π importanti di un framework di apprendimento automatico.\nPossiamo utilizzare quattro metodi principali per far s√¨ che i computer prendano derivate. Innanzitutto, possiamo calcolare manualmente le derivate a mano e inserirle nel computer. Questo diventerebbe rapidamente un incubo con molti layer di reti neurali se dovessimo calcolare manualmente tutte le derivate nei passaggi di backpropagation. Un altro metodo √® la differenziazione simbolica utilizzando sistemi di computer algebrici come Mathematica, che pu√≤ introdurre un layer di inefficienza, poich√© √® necessario un livello di astrazione per prendere le derivate. Le derivate numeriche, la pratica di approssimare i gradienti utilizzando metodi di differenze finite, soffrono di molti problemi, tra cui elevati costi computazionali e dimensioni della griglia pi√π grandi, che portano a molti errori. Ci√≤ porta alla differenziazione automatica, che sfrutta le funzioni primitive che i computer utilizzano per rappresentare le operazioni per ottenere una derivata esatta. Con la differenziazione automatica, la complessit√† computazionale del calcolo del gradiente √® proporzionale al calcolo della funzione stessa. Le complessit√† della differenziazione automatica non sono gestite dagli utenti finali al momento, ma le risorse per saperne di pi√π possono essere trovate ampiamente, ad esempio qui. La differenziazione automatica e la programmazione differenziabile di oggi sono onnipresenti e vengono eseguite in modo efficiente e automatico dai moderni framework di machine learning.\n\n\n6.4.9 Accelerazione Hardware\nLa tendenza a formare e distribuire continuamente modelli di apprendimento automatico pi√π grandi ha reso necessario il supporto dell‚Äôaccelerazione hardware per le piattaforme di machine-learning. Figura¬†6.10 mostra il gran numero di aziende che offrono acceleratori hardware in diversi domini, come il machine learning ‚ÄúVery Low Power‚Äù e quello ‚ÄúEmbedded‚Äù. I ‚Äúdeep layer‚Äù delle reti neurali richiedono molte moltiplicazioni di matrici, che attraggono hardware in grado di calcolare rapidamente e in parallelo tali operazioni. In questo panorama, due architetture hardware, GPU e TPU, sono emerse come scelte principali per l‚Äôaddestramento di modelli di apprendimento automatico.\nL‚Äôuso di acceleratori hardware √® iniziato con AlexNet, che ha aperto la strada a lavori futuri per utilizzare le GPU come acceleratori hardware per l‚Äôaddestramento di modelli di visione artificiale. Le GPU, o ‚ÄúGraphics Processing Units‚Äù [unit√† di elaborazione grafica], eccellono nella gestione di molti calcoli contemporaneamente, il che le rende ideali per le operazioni matriciali centrali per l‚Äôaddestramento delle reti neurali. La loro architettura, progettata per il rendering della grafica, √® perfetta per le operazioni matematiche richieste nell‚Äôapprendimento automatico. Sebbene siano molto utili per le attivit√† di apprendimento automatico e siano state implementate in molte piattaforme hardware, le GPU sono comunque di uso generale in quanto possono essere utilizzate per altre applicazioni.\nD‚Äôaltro canto, le Tensor Processing Units (TPU) sono unit√† hardware progettate specificamente per le reti neurali. Si concentrano sull‚Äôoperazione di ‚Äúmoltiplicazione e accumulazione‚Äù (MAC) e il loro hardware √® costituito da una grande matrice hardware che contiene elementi che calcolano in modo efficiente l‚Äôoperazione MAC. Questo concetto, chiamato systolic array architecture, √® stato ideato da Kung e Leiserson (1979), ma ha dimostrato di essere una struttura utile per calcolare in modo efficiente i prodotti matriciali e altre operazioni all‚Äôinterno delle reti neurali (come le convoluzioni).\n\nKung, Hsiang Tsung, e Charles E Leiserson. 1979. ¬´Systolic arrays (for VLSI)¬ª. In Sparse Matrix Proceedings 1978, 1:256‚Äì82. Society for industrial; applied mathematics Philadelphia, PA, USA.\nSebbene le TPU possano ridurre drasticamente i tempi di addestramento, presentano anche degli svantaggi. Ad esempio, molte operazioni all‚Äôinterno dei framework di apprendimento automatico (principalmente TensorFlow in questo caso, poich√© la TPU si integra direttamente con esso) non sono supportate dalle TPU. Non possono inoltre supportare operazioni personalizzate dai framework di apprendimento automatico e la progettazione della rete deve essere strettamente allineata alle capacit√† hardware.\nOggi, le GPU NVIDIA dominano il training, supportate da librerie software come CUDA, cuDNN e TensorRT. I framework includono anche ottimizzazioni per massimizzare le prestazioni su questi tipi di hardware, come l‚Äôeliminazione di connessioni non importanti e la fusione di layer. La combinazione di queste tecniche con l‚Äôaccelerazione hardware fornisce una maggiore efficienza. Per l‚Äôinferenza, l‚Äôhardware si sta spostando sempre di pi√π verso ASIC e SoC ottimizzati. Le TPU di Google accelerano i modelli nei data center, mentre Apple, Qualcomm, la famiglia NVIDIA Jetson e altri ora producono chip ‚Äúmobili‚Äù incentrati sull‚Äôintelligenza artificiale.\n\n\n\n\n\n\nFigura¬†6.10: Aziende che offrono acceleratori hardware di ML. Fonte: Gradient Flow.",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Framework di IA</span>"
    ]
  },
  {
    "objectID": "contents/core/frameworks/frameworks.it.html#sec-ai_frameworks-advanced",
    "href": "contents/core/frameworks/frameworks.it.html#sec-ai_frameworks-advanced",
    "title": "6¬† Framework di IA",
    "section": "6.5 Funzionalit√† Avanzate",
    "text": "6.5 Funzionalit√† Avanzate\nOltre a fornire gli strumenti essenziali per il training di modelli di apprendimento automatico, i framework offrono anche funzionalit√† avanzate. Queste funzionalit√† includono la distribuzione del training su diverse piattaforme hardware, la facile messa a punto di grandi modelli pre-addestrati e l‚Äôesemplificazione del ‚Äúfederated learning‚Äù. L‚Äôimplementazione di queste funzionalit√† in modo indipendente sarebbe altamente complessa e richiederebbe molte risorse, ma i framework semplificano questi processi, rendendo le tecniche avanzate di apprendimento automatico pi√π accessibili.\n\n6.5.1 Training distribuito\nPoich√© i modelli di apprendimento automatico sono diventati pi√π grandi nel corso degli anni, √® diventato essenziale per i modelli di grandi dimensioni utilizzare pi√π nodi di elaborazione nel processo di training. Questo processo, l‚Äôapprendimento distribuito, ha consentito maggiori capacit√† di training, ma ha anche imposto problemi nell‚Äôimplementazione.\nPossiamo considerare tre diversi modi per distribuire il lavoro di training dei modelli di apprendimento automatico su pi√π nodi di elaborazione. Il partizionamento dei dati di input (o parallelismo dei dati) si riferisce a pi√π processori che eseguono lo stesso modello su diverse partizioni di input. Questa √® l‚Äôimplementazione pi√π semplice ed √® disponibile per molti framework di machine learning. La distribuzione pi√π impegnativa del lavoro √® rappresentata dal parallelismo del modello, che si riferisce a pi√π nodi di elaborazione che lavorano su parti diverse del modello, e dal parallelismo del modello pipelined, che si riferisce a pi√π nodi di elaborazione che lavorano su diversi layer del modello sullo stesso input. Gli ultimi due menzionati qui sono aree di ricerca attive.\nI framework di ML che supportano l‚Äôapprendimento distribuito includono TensorFlow (tramite il suo modulo tf.distribute), PyTorch (tramite i suoi moduli torch.nn.DataParallel e torch.nn.DistributedDataParallel) e MXNet (tramite la sua API gluon).\n\n\n6.5.2 Conversione del Modello\nI modelli di machine learning hanno vari metodi per essere rappresentati e utilizzati in diversi framework e per diversi tipi di dispositivi. Ad esempio, un modello pu√≤ essere convertito per essere compatibile con i framework di inferenza all‚Äôinterno del dispositivo mobile. Il formato di default per i modelli TensorFlow sono i file di checkpoint contenenti pesi e architetture, necessari per riaddestrare i modelli. Tuttavia, i modelli vengono in genere convertiti nel formato TensorFlow Lite per la distribuzione mobile. TensorFlow Lite utilizza una rappresentazione compatta del ‚Äúflat buffer‚Äù e ottimizzazioni per un‚Äôinferenza rapida su hardware mobile, eliminando tutto il bagaglio non necessario associato ai metadati di addestramento, come le strutture dei file di checkpoint.\nLe ottimizzazioni del modello come la quantizzazione (vedere il capitolo Ottimizzazioni) possono ottimizzare ulteriormente i modelli per architetture target come i dispositivi mobili. Ci√≤ riduce la precisione di pesi e attivazioni a uint8 o a int8 per un ingombro ridotto e un‚Äôesecuzione pi√π rapida con acceleratori hardware supportati. Per la quantizzazione post-training, il convertitore di TensorFlow gestisce automaticamente analisi e conversione.\nFramework come TensorFlow semplificano la distribuzione di modelli addestrati su dispositivi IoT mobili ed embedded tramite API di conversione semplici per il formato TFLite e la quantizzazione. La conversione pronta all‚Äôuso consente un‚Äôinferenza ad alte prestazioni su dispositivi mobili senza l‚Äôonere dell‚Äôottimizzazione manuale. Oltre a TFLite, altri target comuni includono TensorFlow.js per la distribuzione Web, TensorFlow Serving per i servizi cloud e TensorFlow Hub per l‚Äôapprendimento tramite trasferimento. Le utility di conversione di TensorFlow gestiscono questi scenari per semplificare i flussi di lavoro end-to-end.\nUlteriori informazioni sulla conversione dei modelli in TensorFlow sono linkate qui.\n\n\n6.5.3 AutoML, No-Code/Low-Code ML\nIn molti casi, l‚Äôapprendimento automatico pu√≤ avere una barriera d‚Äôingresso relativamente alta rispetto ad altri campi. Per addestrare e distribuire con successo modelli, √® necessario avere una comprensione critica di una variet√† di discipline, dalla scienza dei dati (elaborazione dei dati, pulizia dei dati), strutture di modelli (ottimizzazione degli iperparametri, architettura delle reti neurali), hardware (accelerazione, elaborazione parallela) e altro a seconda del problema in questione. La complessit√† di questi problemi ha portato all‚Äôintroduzione di framework come AutoML, che cerca di rendere ‚Äúl‚Äôapprendimento automatico disponibile anche a chi non √® esperto di apprendimento automatico‚Äù e di ‚Äúautomatizzare la ricerca nell‚Äôapprendimento automatico‚Äù. Hanno creato AutoWEKA, che aiuta nel complesso processo di selezione degli iperparametri, e Auto-sklearn e Auto-pytorch, un‚Äôestensione di AutoWEKA nelle popolari librerie sklearn e PyTorch.\nMentre questi sforzi per automatizzare parti delle attivit√† di apprendimento automatico sono in corso, altri si sono concentrati sulla semplificazione dei modelli tramite l‚Äôimplementazione di apprendimento automatico ‚Äúno-code‚Äù [senza codice]/low-code [a basso codice], utilizzando un‚Äôinterfaccia drag-and-drop con un‚Äôinterfaccia utente di facile navigazione. Aziende come Apple, Google e Amazon hanno gi√† creato queste piattaforme di facile utilizzo per consentire agli utenti di costruire modelli di apprendimento automatico che possono essere integrati nel loro ecosistema.\nQuesti passaggi per rimuovere le barriere all‚Äôingresso continuano a democratizzare il machine learning, semplificano l‚Äôaccesso per i principianti e semplificano il flusso di lavoro per gli esperti.\n\n\n6.5.4 Metodi di Apprendimento Avanzati\n\nIl Transfer Learning\nIl ‚Äútransfer learning‚Äù √® la pratica di utilizzare le conoscenze acquisite da un modello pre-addestrato per addestrare e migliorare le prestazioni di un modello per un‚Äôattivit√† diversa. Ad esempio, modelli come MobileNet e ResNet vengono addestrati sul set di dati ImageNet. Per fare ci√≤, si pu√≤ congelare il modello pre-addestrato, utilizzandolo come un estrattore di feature per addestrare un modello molto pi√π piccolo costruito sopra l‚Äôestrazione di feature. Si pu√≤ anche mettere a punto l‚Äôintero modello per adattarlo al nuovo compito. I framework di apprendimento automatico semplificano il caricamento di modelli pre-addestrati, il congelamento di layer specifici e l‚Äôaddestramento di layer personalizzati in cima. Semplificano questo processo fornendo API intuitive e un facile accesso a grandi repository di modelli pre-addestrati.\nIl transfer learning, pur essendo potente, presenta delle sfide. Un problema significativo √® la potenziale incapacit√† del modello modificato di svolgere le sue attivit√† originali dopo il transfer learning. Per affrontare queste sfide, i ricercatori hanno proposto varie soluzioni. Ad esempio, Z. Li e Hoiem (2018) ha introdotto il concetto di ‚ÄúImparare senza Dimenticare.‚Äù nel suo articolo ‚ÄúLearning without Forgetting‚Äù, che da allora √® stato implementato nelle moderne piattaforme di machine learning. Figura¬†6.11 fornisce un‚Äôillustrazione semplificata del concetto di ‚Äútransfer learning‚Äù [apprendimento per trasferimento]:\n\nLi, Zhizhong, e Derek Hoiem. 2018. ¬´Learning without Forgetting¬ª. IEEE Trans. Pattern Anal. Mach. Intell. 40 (12): 2935‚Äì47. https://doi.org/10.1109/tpami.2017.2773081.\n\n\n\n\n\n\nFigura¬†6.11: Trasferimento dell‚Äôapprendimento. Fonte: Tech Target\n\n\n\nCome mostrato in Figura¬†6.11, l‚Äôapprendimento per trasferimento implica l‚Äôassunzione di un modello addestrato su un‚Äôattivit√† (l‚Äôattivit√† di origine) e l‚Äôadattamento per eseguire un‚Äôattivit√† nuova e correlata (l‚Äôattivit√† di target). Questo processo consente al modello di sfruttare le conoscenze acquisite dall‚Äôattivit√† di origine, migliorando potenzialmente le prestazioni e riducendo i tempi di addestramento sull‚Äôattivit√† di destinazione. Tuttavia, come accennato in precedenza, √® necessario prestare attenzione per garantire che il modello non ‚Äúdimentichi‚Äù la sua capacit√† di eseguire l‚Äôattivit√† originale durante questo processo.\n\n\nIl Federated Learning\nIl ‚ÄúFederated learning‚Äù di McMahan et al. (2017) √® una forma di elaborazione distribuita che prevede l‚Äôaddestramento di modelli su dispositivi personali anzich√© la centralizzazione dei dati su un singolo server (Figura¬†12.6). Inizialmente, un modello globale di base viene addestrato su un server centrale per essere distribuito a tutti i dispositivi. Utilizzando questo modello di base, i dispositivi calcolano individualmente i gradienti e li inviano all‚Äôhub centrale. Intuitivamente, questo trasferisce i parametri del modello anzich√© i dati stessi. L‚Äôapprendimento federato migliora la privacy mantenendo i dati sensibili sui dispositivi locali e condividendo gli aggiornamenti del modello solo con un server centrale. Questo metodo √® particolarmente utile quando si gestiscono dati sensibili o quando un‚Äôinfrastruttura su larga scala non √® praticabile.\n\nMcMahan, Brendan, Eider Moore, Daniel Ramage, Seth Hampson, e Blaise Ag√ºera y Arcas. 2017. ¬´Communication-Efficient Learning of Deep Networks from Decentralized Data¬ª. In Proceedings of the 20th International Conference on Artificial Intelligence and Statistics, AISTATS 2017, 20-22 April 2017, Fort Lauderdale, FL, USA, a cura di Aarti Singh e Xiaojin (Jerry) Zhu, 54:1273‚Äì82. Proceedings of Machine Learning Research. PMLR. http://proceedings.mlr.press/v54/mcmahan17a.html.\n\n\n\n\n\n\nFigura¬†6.12: Un approccio con server centralizzato al ‚Äúfederated learning‚Äù. Fonte: NVIDIA.\n\n\n\nTuttavia, il federated learning deve affrontare sfide come garantire l‚Äôaccuratezza dei dati, gestire dati non-IID (independent and identically distributed) [indipendenti e distribuiti in modo identico], gestire la produzione di dati non bilanciata e superare il sovraccarico della comunicazione e l‚Äôeterogeneit√† dei dispositivi. Anche i problemi di privacy e sicurezza, come gli attacchi di inversione del gradiente, pongono sfide significative.\nI framework di apprendimento automatico semplificano l‚Äôimplementazione dell‚Äôapprendimento federato fornendo gli strumenti e le librerie necessarie. Ad esempio, TensorFlow Federated (TFF) offre un framework open source per supportare l‚Äôapprendimento federato. TFF consente agli sviluppatori di simulare e implementare algoritmi di apprendimento federato, offrendo un core federato per operazioni di basso livello e API di alto livello per attivit√† federate comuni. Si integra perfettamente con TensorFlow, consentendo l‚Äôuso di modelli e ottimizzatori TensorFlow in un ambiente federato. TFF supporta tecniche di aggregazione sicure per migliorare la privacy e consente la personalizzazione degli algoritmi di apprendimento federato. Sfruttando questi strumenti, gli sviluppatori possono distribuire in modo efficiente il training, perfezionare i modelli pre-addestrati e gestire le complessit√† intrinseche dell‚Äôapprendimento federato.\nSono stati sviluppati anche altri programmi open source come Flower per semplificare l‚Äôimplementazione dell‚Äôapprendimento federato con vari framework di machine learning.",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Framework di IA</span>"
    ]
  },
  {
    "objectID": "contents/core/frameworks/frameworks.it.html#specializzazione-del-framework",
    "href": "contents/core/frameworks/frameworks.it.html#specializzazione-del-framework",
    "title": "6¬† Framework di IA",
    "section": "6.6 Specializzazione del Framework",
    "text": "6.6 Specializzazione del Framework\nFinora abbiamo parlato in generale dei framework di ML. Tuttavia, in genere, i framework sono ottimizzati in base alle capacit√† computazionali e ai requisiti applicativi dell‚Äôambiente target, che vanno dal cloud all‚Äôedge ai dispositivi minuscoli. La scelta del framework giusto √® fondamentale in base all‚Äôambiente target per la distribuzione. Questa sezione fornisce una panoramica dei principali tipi di framework di IA su misura per ambienti cloud, edge e TinyML per aiutare a comprendere le somiglianze e le differenze tra questi ecosistemi.\n\n6.6.1 Cloud\nI framework di IA basati su cloud presuppongono l‚Äôaccesso a un‚Äôampia potenza di calcolo, memoria e risorse di archiviazione nel cloud. In genere supportano sia il training che l‚Äôinferenza. I framework di IA basati su cloud sono adatti per applicazioni in cui i dati possono essere inviati al cloud per l‚Äôelaborazione, come servizi di IA basati su cloud, analisi di dati su larga scala e applicazioni Web. I framework di IA cloud pi√π diffusi includono quelli che abbiamo menzionato in precedenza, come TensorFlow, PyTorch, MXNet, Keras, ecc. Questi framework utilizzano GPU, TPU, training distribuito e AutoML per fornire IA scalabile. Concetti come model serving, MLOps e AIOps sono correlati all‚Äôoperativit√† dell‚ÄôIA nel cloud. L‚ÄôIA cloud alimenta servizi come Google Cloud AI e consente il ‚Äútransfer learning‚Äù tramite modelli pre-addestrati.\n\n\n6.6.2 Edge\nI framework Edge AI sono pensati per distribuire modelli di IA su dispositivi IoT, smartphone e server edge. I framework Edge AI sono ottimizzati per dispositivi con risorse di calcolo moderate, bilanciando potenza e prestazioni. I framework Edge AI sono ideali per applicazioni che richiedono elaborazione in tempo reale o quasi reale, tra cui robotica, veicoli autonomi e dispositivi intelligenti. I principali framework Edge AI includono TensorFlow Lite, PyTorch Mobile, CoreML e altri. Impiegano ottimizzazioni come compressione del modello, quantizzazione ed architetture di reti neurali efficienti. Il supporto hardware include CPU, GPU, NPU e acceleratori come Edge TPU. Edge AI consente casi d‚Äôuso come visione mobile, riconoscimento vocale e rilevamento di anomalie in tempo reale.\n\n\n6.6.3 Embedded\nI framework TinyML sono specializzati per distribuire modelli AI su dispositivi con risorse estremamente limitate, in particolare microcontrollori e sensori all‚Äôinterno dell‚Äôecosistema IoT. I framework TinyML sono progettati per dispositivi con risorse limitate, enfatizzando memoria minima e consumo energetico. I framework TinyML sono specializzati per casi d‚Äôuso su dispositivi IoT con risorse limitate per applicazioni di manutenzione predittiva, riconoscimento dei gesti e monitoraggio ambientale. I principali framework TinyML includono TensorFlow Lite Micro, uTensor e ARM NN. Ottimizzano modelli complessi per adattarli a kilobyte di memoria tramite tecniche come l‚Äôaddestramento consapevole della quantizzazione e la precisione ridotta. TinyML consente il rilevamento intelligente su dispositivi alimentati a batteria, consentendo l‚Äôapprendimento collaborativo tramite apprendimento federato. La scelta del framework implica il bilanciamento delle prestazioni del modello e dei vincoli computazionali della piattaforma target, che sia cloud, edge o TinyML. Tabella¬†6.3 confronta i principali framework di IA negli ambienti cloud, edge e TinyML:\n\n\n\nTabella¬†6.3: Confronto dei tipi di framework per Cloud AI, Edge AI e TinyML.\n\n\n\n\n\n\n\n\n\n\n\nTipo di framework\nEsempi\nTecnologie chiave\nCasi d‚Äôuso\n\n\n\n\nCloud AI\nTensorFlow, PyTorch, MXNet, Keras\nGPU, TPU, addestramento distribuito, AutoML, MLOps\nServizi cloud, app Web, analisi di big data\n\n\nEdge AI\nTensorFlow Lite, PyTorch Mobile, Core ML\nOttimizzazione del modello, compressione, quantizzazione, architetture NN efficienti\nApp mobili, sistemi autonomi, elaborazione in tempo reale\n\n\nTinyML\nTensorFlow Lite Micro, uTensor, ARM NN\nTraining consapevole della quantizzazione, precisione ridotta, ricerca di architettura neurale\nSensori IoT, dispositivi indossabili, manutenzione predittiva, riconoscimento dei gesti\n\n\n\n\n\n\nDifferenze principali:\n\nCloud AI sfrutta un‚Äôenorme potenza di calcolo per modelli complessi utilizzando GPU/TPU e training distribuito.\nEdge AI ottimizza i modelli per l‚Äôesecuzione locale su dispositivi edge con risorse limitate.\nTinyML adatta i modelli a una memoria estremamente bassa e calcola ambienti come i microcontrollori.",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Framework di IA</span>"
    ]
  },
  {
    "objectID": "contents/core/frameworks/frameworks.it.html#sec-ai_frameworks_embedded",
    "href": "contents/core/frameworks/frameworks.it.html#sec-ai_frameworks_embedded",
    "title": "6¬† Framework di IA",
    "section": "6.7 Framework di IA Embedded",
    "text": "6.7 Framework di IA Embedded\n\n6.7.1 Vincoli di Risorse\nI sistemi embedded affrontano gravi limitazioni di risorse che pongono sfide uniche quando si distribuiscono modelli di machine learning rispetto alle piattaforme di elaborazione tradizionali. Ad esempio, le unit√† microcontrollore (MCU) comunemente utilizzate nei dispositivi IoT hanno spesso:\n\nRAM varia da decine di kilobyte a pochi megabyte. Il popolare MCU ESP8266 ha circa 80 KB di RAM a disposizione degli sviluppatori. Ci√≤ contrasta con 8 GB o pi√π su laptop e desktop tipici odierni.\nMemoria Flash varia da centinaia di kilobyte a pochi megabyte. Il microcontrollore Arduino Uno fornisce solo 32 KB di archiviazione del codice. I computer standard odierni hanno un‚Äôarchiviazione su disco nell‚Äôordine dei terabyte.\nPotenza di elaborazione da pochi MHz a circa 200 MHz. L‚ÄôESP8266 funziona a 80 MHz. Questo √® di diversi ordini di grandezza pi√π lento delle CPU multi-core multi-GHz nei server e nei laptop di fascia alta.\n\nQuesti vincoli rigorosi spesso rendono impossibile l‚Äôaddestramento di modelli di apprendimento automatico direttamente sui microcontrollori. La RAM limitata impedisce la gestione di grandi set di dati per il training. L‚Äôuso di energia per l‚Äôaddestramento esaurirebbe rapidamente anche i dispositivi alimentati a batteria. Al contrario, i modelli vengono addestrati su sistemi ricchi di risorse e distribuiti su microcontrollori per un‚Äôinferenza ottimizzata. Ma anche l‚Äôinferenza pone delle sfide:\n\nDimensioni del Modello: I modelli di intelligenza artificiale sono troppo grandi per adattarsi a dispositivi IoT ed embedded. Ci√≤ richiede tecniche di compressione del modello, come quantizzazione, potatura e ‚Äúknowledge distillation‚Äù [distillazione della conoscenza]. Inoltre, come vedremo, molti dei framework utilizzati dagli sviluppatori di intelligenza artificiale hanno grandi quantit√† di overhead e librerie integrate che i sistemi embedded non possono supportare.\nComplessit√† delle Attivit√†: Con solo decine di KB o pochi MB di RAM, i dispositivi IoT e i sistemi embedded sono limitati nella complessit√† delle attivit√† che possono gestire. Le attivit√† che richiedono grandi set di dati o algoritmi sofisticati, ad esempio LLM, che verrebbero eseguiti senza problemi su piattaforme di elaborazione tradizionali potrebbero non essere fattibili su sistemi embedded senza compressione o altre tecniche di ottimizzazione a causa delle limitazioni di memoria.\nArchiviazione ed Elaborazione dei Dati: I sistemi embedded spesso elaborano i dati in tempo reale e potrebbero archiviarne solo piccole quantit√† localmente. Al contrario, i sistemi di elaborazione tradizionali possono contenere ed elaborare grandi set di dati in memoria, consentendo un‚Äôanalisi pi√π rapida delle operazioni sui dati e aggiornamenti in tempo reale.\nSicurezza e Privacy: La poca memoria limita anche la complessit√† degli algoritmi e dei protocolli di sicurezza, la crittografia dei dati, le protezioni da reverse engineering e altro che pu√≤ essere implementato sul dispositivo. Ci√≤ potrebbe rendere alcuni dispositivi IoT pi√π vulnerabili agli attacchi.\n\nDi conseguenza, le ottimizzazioni software specializzate e i framework ML su misura per i microcontrollori devono funzionare entro questi stretti limiti delle risorse. Tecniche di ottimizzazione intelligenti come quantizzazione, potatura e distillazione della conoscenza comprimono i modelli per adattarli alla memoria limitata (vedere la sezione Ottimizzazioni). Gli insegnamenti tratti dalla ricerca di architettura neurale aiutano a guidare la progettazione dei modelli.\nI miglioramenti hardware come gli acceleratori ML dedicati sui microcontrollori aiutano anche ad alleviare i vincoli. Ad esempio, Hexagon DSP di Qualcomm accelera i modelli TensorFlow Lite sui chip mobili Snapdragon. Google Edge TPU racchiude le prestazioni ML in un piccolo ASIC per dispositivi edge. ARM Ethos-U55 offre un‚Äôinferenza efficiente sui microcontrollori di classe Cortex-M. Questi chip ML personalizzati sbloccano funzionalit√† avanzate per applicazioni con risorse limitate.\nA causa della potenza di elaborazione limitata, √® quasi sempre impossibile addestrare modelli di intelligenza artificiale su IoT o sistemi embedded. Invece, i modelli vengono addestrati su potenti computer tradizionali (spesso con GPU) e poi distribuiti sul dispositivo embedded per l‚Äôinferenza. TinyML si occupa specificamente di questo, assicurando che i modelli siano sufficientemente leggeri per l‚Äôinferenza in tempo reale su questi dispositivi limitati.\n\n\n6.7.2 Framework e Librerie\nI framework di intelligenza artificiale embedded sono strumenti software e librerie progettati per abilitare funzionalit√† di intelligenza artificiale e ML su sistemi embedded. Questi framework sono essenziali per portare l‚Äôintelligenza artificiale su dispositivi IoT, robotica e altre piattaforme di edge computing e sono progettati per funzionare dove risorse di elaborazione, memoria e consumo energetico sono limitati.\n\n\n6.7.3 Sfide\nSebbene i sistemi embedded rappresentino un‚Äôenorme opportunit√† per l‚Äôimplementazione dell‚Äôapprendimento automatico per abilitare capacit√† intelligenti in edge, questi ambienti con risorse limitate pongono sfide significative. A differenza dei tipici ambienti cloud o desktop ricchi di risorse computazionali, i dispositivi embedded introducono gravi limitazioni in termini di memoria, potenza di elaborazione, efficienza energetica e hardware specializzato. Di conseguenza, le tecniche e i framework di apprendimento automatico esistenti progettati per cluster di server con risorse abbondanti non si traducono direttamente nei sistemi embedded. Questa sezione svela alcune delle sfide e delle opportunit√† per i sistemi embedded e i framework ML.\n\nEcosistema Frammentato\nLa mancanza di un framework ML unificato ha portato a un ecosistema altamente frammentato. Gli ingegneri di aziende come STMicroelectronics, NXP Semiconductors e Renesas hanno dovuto sviluppare soluzioni personalizzate su misura per le loro specifiche architetture di microcontrollori e DSP. Questi framework ad hoc richiedevano un‚Äôampia ottimizzazione manuale per ogni piattaforma hardware di basso livello. Ci√≤ ha reso estremamente difficile il porting dei modelli, richiedendo la riqualificazione per nuove architetture Arm, RISC-V o proprietarie.\n\n\nEsigenze Hardware Disparate\nSenza un framework condiviso, non esisteva un modo standard per valutare le capacit√† dell‚Äôhardware. Fornitori come Intel, Qualcomm e NVIDIA crearono soluzioni integrate, combinando modelli e migliorando software e hardware. Ci√≤ rese difficile discernere i motivi del guadagni di prestazioni, se fosse merito dei nuovi progetti di chip come i core x86 a basso consumo di Intel o le ottimizzazioni software. Era necessario un framework standard affinch√© i fornitori potessero valutare le capacit√† del loro hardware in modo equo e riproducibile.\n\n\nMancanza di Portabilit√†\nCon strumenti standardizzati, adattare modelli addestrati in framework comuni come TensorFlow o PyTorch per funzionare in modo efficiente sui microcontrollori era pi√π facile. Richiedeva una traduzione manuale dispendiosa, in termini di tempo, dei modelli per l‚Äôesecuzione su DSP specializzati di aziende come CEVA o core Arm M-series a basso consumo. Nessuno strumento immediato consentiva l‚Äôimplementazione portatile su diverse architetture.\n\n\nInfrastruttura Incompleta\nL‚Äôinfrastruttura per supportare i flussi di lavoro di sviluppo dei modelli chiave doveva essere migliorata. √à necessario un maggiore supporto per le tecniche di compressione per adattare modelli di grandi dimensioni a budget di memoria limitati. Mancavano strumenti per la quantizzazione per ridurre la precisione per un‚Äôinferenza pi√π rapida. Le API standardizzate per l‚Äôintegrazione nelle applicazioni erano incomplete. Mancavano funzionalit√† essenziali come il debugging sul dispositivo, le metriche e la profilazione delle prestazioni. Queste lacune hanno aumentato i costi e la difficolt√† dello sviluppo ML embedded.\n\n\nNessun Benchmark Standard\nSenza benchmark unificati, non esisteva un modo standard per valutare e confrontare le capacit√† di diverse piattaforme hardware di fornitori come NVIDIA, Arm e Ambiq Micro. Le valutazioni esistenti si basavano su benchmark proprietari pensati per mostrare i punti di forza di specifici chip. Ci√≤ rendeva impossibile misurare i miglioramenti hardware in modo oggettivo, imparziale e imparziale. Il capitolo Benchmarking dell‚ÄôIA affronta questo argomento in modo pi√π dettagliato.\n\n\nTest Minimi del Mondo Reale\nGran parte dei benchmark si basava su dati sintetici. Testare rigorosamente i modelli su applicazioni embedded nel mondo reale era difficile senza set di dati e benchmark standardizzati, sollevando dubbi su come le dichiarazioni sulle prestazioni si sarebbero tradotte in un utilizzo nel mondo reale. Erano necessari test pi√π approfonditi per convalidare i chip in casi di utilizzo reali.\nLa mancanza di framework e infrastrutture condivisi ha rallentato l‚Äôadozione di TinyML, ostacolandone l‚Äôintegrazione nei prodotti embedded. I recenti framework standard hanno iniziato ad affrontare questi problemi attraverso una migliore portabilit√†, profilazione delle prestazioni e supporto per il benchmarking. Tuttavia, √® ancora necessaria un‚Äôinnovazione continua per consentire un‚Äôimplementazione fluida e conveniente dell‚ÄôIA nei dispositivi edge.\n\n\nRiepilogo\nL‚Äôassenza di framework, benchmark e infrastrutture standardizzati per ML embedded ne ha tradizionalmente ostacolato l‚Äôadozione. Tuttavia, sono stati compiuti recenti progressi nello sviluppo di framework condivisi come TensorFlow Lite Micro e suite di benchmark come MLPerf Tiny che mirano ad accelerare la proliferazione di soluzioni TinyML. Tuttavia, superare la frammentazione e la difficolt√† dell‚Äôimplementazione embedded rimane un processo in corso.",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Framework di IA</span>"
    ]
  },
  {
    "objectID": "contents/core/frameworks/frameworks.it.html#esempi",
    "href": "contents/core/frameworks/frameworks.it.html#esempi",
    "title": "6¬† Framework di IA",
    "section": "6.8 Esempi",
    "text": "6.8 Esempi\nIl deployment [distribuzione] di machine learning su microcontrollori e altri dispositivi embedded richiede spesso librerie software e framework appositamente ottimizzati per funzionare entro vincoli rigorosi di memoria, elaborazione e potenza. Esistono diverse opzioni per eseguire l‚Äôinferenza su hardware con risorse limitate, ciascuna con il proprio approccio all‚Äôottimizzazione dell‚Äôesecuzione del modello. Questa sezione esplorer√† le caratteristiche chiave e i principi di progettazione alla base di TFLite Micro, TinyEngine e CMSIS-NN, fornendo informazioni su come ogni framework affronta il complesso problema dell‚Äôesecuzione di reti neurali molto accurata ma efficiente sui microcontrollori. Mostrer√† inoltre diversi approcci per l‚Äôimplementazione di framework TinyML efficienti.\nTabella¬†6.4 riassume le principali differenze e somiglianze tra questi tre framework di inferenza di apprendimento automatico specializzati per sistemi embedded e microcontrollori.\n\n\n\nTabella¬†6.4: Confronto dei framework: TensorFlow Lite Micro, TinyEngine e CMSIS-NN\n\n\n\n\n\n\n\n\n\n\n\nFramework\nTensorFlow Lite Micro\nTinyEngine\nCMSIS-NN\n\n\n\n\nApproccio\nBasato su interprete\nCompilazione statica\nKernel di reti neurali ottimizzati\n\n\nFocus sull‚Äôhardware\nDispositivi embedded generali\nMicrocontrollori\nProcessori ARM Cortex-M\n\n\nSupporto aritmetico\nVirgola mobile\nVirgola mobile, virgola fissa\nVirgola mobile, virgola fissa\n\n\nSupporto del modello\nModelli di rete neurale generale\nModelli co-progettati con TinyNAS\nTipi di livelli di rete neurale comuni\n\n\nImpronta del codice\nPi√π grande grazie all‚Äôinclusione di interprete e operazioni\nPiccola, include solo le operazioni necessarie per il modello\nNativamente leggera\n\n\nLatenza\nPi√π alta grazie a overhead di interpretazione\nMolto bassa grazie al modello compilato\nfocalizzato sulla bassa latenza\n\n\nGestione della memoria\nGestita dinamicamente da interprete\nOttimizzazione a livello di modello\nStrumenti per un‚Äôallocazione efficiente\n\n\nApproccio di ottimizzazione\nAlcune funzionalit√† di e generazione del codice\nKernel specializzati, fusione di operatori\nOttimizzazioni di assemblaggio specifiche dell‚Äôarchitettura\n\n\nPrincipali vantaggi\nFlessibilit√†, portabilit√†, facile aggiornamento dei modelli\nMassimizza le prestazioni, ottimizza l‚Äôutilizzo della memoria\nAccelerazione hardware, API standardizzata, portabilit√†\n\n\n\n\n\n\nNe comprenderemo ciascuno in modo pi√π dettagliato nelle sezioni seguenti.\n\n6.8.1 Interprete\nTensorFlow Lite Micro (TFLM) √® un framework di inferenza di apprendimento automatico progettato per dispositivi embedded con risorse limitate. Utilizza un interprete per caricare ed eseguire modelli di apprendimento automatico, il che fornisce flessibilit√† e facilit√† di aggiornamento dei modelli sul campo (David et al. 2021).\n\nDavid, Robert, Jared Duke, Advait Jain, Vijay Janapa Reddi, Nat Jeffries, Jian Li, Nick Kreeger, et al. 2021. ¬´Tensorflow lite micro: Embedded machine learning for tinyml systems¬ª. Proceedings of Machine Learning and Systems 3: 800‚Äì811.\nGli interpreti tradizionali spesso hanno un overhead di branching [diramazione] significativo, che pu√≤ ridurre le prestazioni. Tuttavia, l‚Äôinterpretazione del modello di machine learning trae vantaggio dall‚Äôefficienza dei kernel di lunga durata, in cui ogni runtime del kernel √® relativamente grande e aiuta a mitigare l‚Äôoverhead dell‚Äôinterprete.\nUn‚Äôalternativa a un motore di inferenza basato su interprete √® quella di generare codice nativo da un modello durante l‚Äôesportazione. Ci√≤ pu√≤ migliorare le prestazioni, ma sacrifica portabilit√† e flessibilit√†, poich√© il codice generato deve essere ricompilato per ogni piattaforma target e deve essere sostituito completamente per modificare un modello.\nTFLM bilancia la semplicit√† della compilazione del codice e la flessibilit√† di un approccio basato su interprete includendo alcune funzionalit√† di generazione del codice. Ad esempio, la libreria pu√≤ essere costruita esclusivamente da file sorgenti, offrendo gran parte della semplicit√† della compilazione associata alla generazione di codice, pur mantenendo i vantaggi di un framework che esegue il modello interpretandolo.\nUn approccio basato su interprete offre diversi vantaggi rispetto alla generazione di codice per l‚Äôinferenza di apprendimento automatico su dispositivi embedded:\n\nFlessibilit√†: I modelli possono essere aggiornati sul campo senza ricompilare l‚Äôintera applicazione.\nPortabilit√†: L‚Äôinterprete pu√≤ essere utilizzato per eseguire modelli su diverse piattaforme target senza dover effettuare il porting del codice.\nEfficienza della Memoria: L‚Äôinterprete pu√≤ condividere il codice su pi√π modelli, riducendo l‚Äôutilizzo della memoria.\nFacilit√† di sviluppo: Gli interpreti sono pi√π facili da sviluppare e gestire rispetto ai generatori di codice.\n\nTensorFlow Lite Micro √® un framework potente e flessibile per l‚Äôinferenza di apprendimento automatico su dispositivi embedded. Il suo approccio basato su interprete offre diversi vantaggi rispetto alla generazione di codice, tra cui flessibilit√†, portabilit√†, efficienza della memoria e facilit√† di sviluppo.\n\n\n6.8.2 Basati su Compilatore\nTinyEngine √® un framework di inferenza ML progettato specificamente per microcontrollori con risorse limitate. Utilizza diverse ottimizzazioni per consentire l‚Äôesecuzione di reti neurali molto accurate entro i vincoli rigorosi di memoria, elaborazione e archiviazione sui microcontrollori (Lin et al. 2020).\n\nLin, Ji, Wei-Ming Chen, Yujun Lin, John Cohn, Chuang Gan, e Song Han. 2020. ¬´MCUNet: Tiny Deep Learning on IoT Devices¬ª. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, a cura di Hugo Larochelle, Marc‚ÄôAurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, e Hsuan-Tien Lin. https://proceedings.neurips.cc/paper/2020/hash/86c51678350f656dcc7f490a43946ee5-Abstract.html.\nMentre framework di inferenza come TFLite Micro utilizzano interpreti per eseguire il grafo della rete neurale in modo dinamico in fase di esecuzione, ci√≤ aggiunge un overhead significativo per quanto riguarda l‚Äôutilizzo della memoria per archiviare metadati, latenza di interpretazione e mancanza di ottimizzazioni. Tuttavia, TFLite sostiene che l‚Äôoverhead √® piccolo. TinyEngine elimina questo overhead utilizzando un approccio di generazione del codice. Analizza il grafo di rete durante la compilazione e genera codice specializzato per eseguire solo quel modello. Questo codice viene compilato in modo nativo nel binario dell‚Äôapplicazione, evitando i costi di interpretazione in fase di esecuzione.\nI framework ML convenzionali pianificano la memoria per layer, cercando di ridurre al minimo l‚Äôutilizzo per ogni layer separatamente. TinyEngine esegue la pianificazione a livello di modello anzich√© analizzare l‚Äôutilizzo della memoria tra i layer. Assegna una dimensione di buffer comune in base alle esigenze massime di memoria di tutti i layer. Questo buffer viene quindi condiviso in modo efficiente tra i layer per aumentare il riutilizzo dei dati.\nTinyEngine √® inoltre specializzato nei kernel per ogni layer tramite tecniche come operatori di tiling, unrolling e fusing. Ad esempio, generer√† kernel di calcolo unrolled [srotolato] con il numero di loop necessari per una convoluzione 3x3 o 5x5. Questi kernel specializzati estraggono le massime prestazioni dall‚Äôhardware del microcontrollore. Utilizza convoluzioni depthwise [in profondit√†] ottimizzate per ridurre al minimo le allocazioni di memoria calcolando l‚Äôoutput di ogni canale posizionato sui dati del canale di input. Questa tecnica sfrutta la natura separabile dei canali delle convoluzioni depthwise per ridurre le dimensioni di picco della memoria.\nCome TFLite Micro, il binario TinyEngine compilato include solo le operazioni necessarie per un modello specifico anzich√© tutte le operazioni possibili. Ci√≤ si traduce in un footprint binario molto piccolo, mantenendo basse le dimensioni del codice per i dispositivi con limiti di memoria.\nUna differenza tra TFLite Micro e TinyEngine √® che quest‚Äôultimo √® co-progettato con ‚ÄúTinyNAS‚Äù, un metodo di ricerca di architettura per modelli di microcontrollori simile al NAS differenziale per microcontrollori. L‚Äôefficienza di TinyEngine consente di esplorare modelli pi√π grandi e accurati tramite NAS. Fornisce inoltre feedback a TinyNAS su quali modelli possono rientrare nei vincoli hardware.\nAttraverso varie tecniche personalizzate, come la compilazione statica, la pianificazione basata sul modello, kernel specializzati e la co-progettazione con NAS, TinyEngine consente un‚Äôinferenza di deep learning ad alta precisione entro i vincoli di risorse rigorosi dei microcontrollori.\n\n\n6.8.3 Libreria\nCMSIS-NN, acronimo di Cortex Microcontroller Software Interface Standard for Neural Networks, √® una libreria software ideata da ARM. Offre un‚Äôinterfaccia standardizzata per distribuire l‚Äôinferenza di reti neurali su microcontrollori e sistemi embedded, concentrandosi sull‚Äôottimizzazione per i processori ARM Cortex-M (Lai, Suda, e Chandra 2018).\n\nLai, Liangzhen, Naveen Suda, e Vikas Chandra. 2018. ¬´Cmsis-nn: Efficient neural network kernels for arm cortex-m cpus¬ª. ArXiv preprint abs/1801.06601. https://arxiv.org/abs/1801.06601.\nKernel di Reti Neurali: CMSIS-NN ha kernel altamente efficienti che gestiscono operazioni fondamentali di reti neurali come convoluzione, pooling, layer completamente connessi e funzioni di attivazione. Si rivolge a un‚Äôampia gamma di modelli di reti neurali supportando l‚Äôaritmetica a virgola mobile e fissa. Quest‚Äôultima √® particolarmente utile per i dispositivi con risorse limitate in quanto riduce i requisiti di memoria e di calcolo (Quantization).\nAccelerazione Hardware: CMSIS-NN sfrutta la potenza delle istruzioni SIMD (Single Instruction, Multiple Data) disponibili su molti processori Cortex-M. Ci√≤ consente l‚Äôelaborazione parallela di pi√π elementi di dati all‚Äôinterno di una singola istruzione, aumentando cos√¨ l‚Äôefficienza computazionale. Alcuni processori Cortex-M dispongono di estensioni di Digital Signal Processing (DSP) che CMSIS-NN pu√≤ sfruttare per l‚Äôesecuzione accelerata della rete neurale. La libreria include anche ottimizzazioni a livello di assembly su misura per specifiche architetture di microcontrollori per migliorare ulteriormente le prestazioni.\nAPI standardizzata: CMSIS-NN offre un‚ÄôAPI coerente e astratta che protegge gli sviluppatori dalle complessit√† dei dettagli hardware di basso livello. Ci√≤ semplifica l‚Äôintegrazione dei modelli di rete neurale nelle applicazioni. Pu√≤ anche comprendere strumenti o utilit√† per convertire i formati di modelli di rete neurale pi√π diffusi in un formato compatibile con CMSIS-NN.\nGestione della Memoria: CMSIS-NN fornisce funzioni per un‚Äôallocazione e una gestione efficienti della memoria, il che √® fondamentale nei sistemi embedded in cui le risorse di memoria sono scarse. Garantisce un utilizzo ottimale della memoria durante l‚Äôinferenza e, in alcuni casi, consente operazioni in loco per ridurre il sovraccarico di memoria.\nPortabilit√†: CMSIS-NN √® progettato per la portabilit√† su vari processori Cortex-M. Questo consente agli sviluppatori di scrivere codice che possa funzionare su diversi microcontrollori senza modifiche significative.\nBassa Latenza: CMSIS-NN riduce al minimo la latenza di inferenza, rendendolo una scelta ideale per applicazioni in tempo reale in cui √® fondamentale prendere decisioni rapide.\nEfficienza Energetica: La libreria √® progettata con un focus sull‚Äôefficienza energetica, rendendola adatta per dispositivi alimentati a batteria e con vincoli energetici.",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Framework di IA</span>"
    ]
  },
  {
    "objectID": "contents/core/frameworks/frameworks.it.html#scelta-del-framework-giusto",
    "href": "contents/core/frameworks/frameworks.it.html#scelta-del-framework-giusto",
    "title": "6¬† Framework di IA",
    "section": "6.9 Scelta del Framework Giusto",
    "text": "6.9 Scelta del Framework Giusto\nLa scelta del framework di machine learning giusto per una determinata applicazione richiede un‚Äôattenta valutazione di modelli, hardware e considerazioni software. Figura¬†6.13 fornisce un confronto tra diversi framework TensorFlow, che discuteremo pi√π in dettaglio:\n\n\n\n\n\n\nFigura¬†6.13: Confronto tra Framework TensorFlow - Generale. Fonte: TensorFlow.\n\n\n\nAnalizzando questi tre aspetti, modelli, hardware e software, come illustrato in Figura¬†6.13, gli ingegneri ML possono selezionare il framework ottimale e personalizzarlo in base alle esigenze per applicazioni ML su dispositivo efficienti e performanti. L‚Äôobiettivo √® bilanciare complessit√† del modello, limitazioni hardware e integrazione software per progettare una pipeline ML su misura per dispositivi embedded e edge. Mentre esaminiamo le differenze mostrate in Figura¬†6.13, acquisiremo informazioni su come scegliere il framework giusto e comprenderemo cosa causa le variazioni tra i framework.\n\n6.9.1 Modello\nFigura¬†6.13 illustra le principali differenze tra le varianti di TensorFlow, in particolare in termini di operazioni supportate (op) e funzionalit√†. TensorFlow supporta molte pi√π operazioni rispetto a TensorFlow Lite e TensorFlow Lite Micro, poich√© viene in genere utilizzato per la ricerca o l‚Äôimplementazione cloud, che richiedono un numero elevato di operatori e una maggiore flessibilit√†.\nLa figura dimostra chiaramente questa differenza nel supporto op tra i framework. TensorFlow Lite supporta operazioni selezionate per il training sul dispositivo, mentre TensorFlow Micro no. Inoltre, la figura mostra che TensorFlow Lite supporta forme dinamiche e training consapevole della quantizzazione, funzionalit√† assenti in TensorFlow Micro. Al contrario, sia TensorFlow Lite che TensorFlow Micro offrono strumenti e supporto di quantizzazione nativi. Qui, la quantizzazione si riferisce alla trasformazione di un programma ML in una rappresentazione approssimata con operazioni di precisione inferiore disponibili, una funzionalit√† cruciale per dispositivi embedded e edge con risorse computazionali limitate.\n\n\n6.9.2 Software\nCome mostrato in Figura¬†6.14, TensorFlow Lite Micro non supporta il sistema operativo, mentre TensorFlow e TensorFlow Lite s√¨. Questa scelta di progettazione per TensorFlow Lite Micro aiuta a ridurre il sovraccarico di memoria, a rendere i tempi di avvio pi√π rapidi e a consumare meno energia. Invece, TensorFlow Lite Micro pu√≤ essere utilizzato insieme a sistemi operativi in tempo reale (RTOS) come FreeRTOS, Zephyr e Mbed OS.\nLa figura evidenzia anche un‚Äôimportante funzionalit√† di gestione della memoria: TensorFlow Lite e TensorFlow Lite Micro supportano la mappatura della memoria del modello, consentendo l‚Äôaccesso diretto ai modelli dall‚Äôarchiviazione flash anzich√© caricarli nella RAM. Al contrario, TensorFlow non offre questa capacit√†.\n\n\n\n\n\n\nFigura¬†6.14: Confronto tra Framework TensorFlow - Software. Fonte: TensorFlow.\n\n\n\nUn‚Äôaltra differenza fondamentale √® la ‚Äúaccelerator delegation‚Äù [delega dell‚Äôacceleratore]. TensorFlow e TensorFlow Lite supportano questa funzionalit√†, consentendo loro di pianificare il codice su acceleratori diversi. Tuttavia, TensorFlow Lite Micro non offre la delega dell‚Äôacceleratore, poich√© i sistemi embedded tendono ad avere una gamma limitata di acceleratori specializzati.\nQueste differenze dimostrano come ogni variante di TensorFlow sia ottimizzata per il suo ambiente di distribuzione di target, dai potenti server cloud ai dispositivi embedded con risorse limitate.\n\n\n6.9.3 Hardware\nTensorFlow Lite e TensorFlow Lite Micro hanno dimensioni binarie di base e footprint di memoria significativamente pi√π piccoli rispetto a TensorFlow (vedere Figura¬†6.15). Ad esempio, un tipico binario TensorFlow Lite Micro √® inferiore a 200 KB, mentre TensorFlow √® molto pi√π grande. Ci√≤ √® dovuto agli ambienti con risorse limitate dei sistemi embedded. TensorFlow supporta x86, TPU e GPU come NVIDIA, AMD e Intel.\n\n\n\n\n\n\nFigura¬†6.15: Confronto tra Framework TensorFlow - Hardware. Fonte: TensorFlow.\n\n\n\nTensorFlow Lite supporta i processori Arm Cortex-A e x86 comunemente utilizzati su telefoni cellulari e tablet. Quest‚Äôultimo √® privo di tutta la logica di training non necessaria per l‚Äôimplementazione sul dispositivo. TensorFlow Lite Micro fornisce supporto per core Arm Cortex M focalizzati sui microcontrollori come M0, M3, M4 e M7, nonch√© DSP come Hexagon e SHARC e MCU come STM32, NXP Kinetis, Microchip AVR.\n\n\n6.9.4 Altri Fattori\nLa selezione del framework di IA appropriato √® essenziale per garantire che i sistemi embedded possano eseguire in modo efficiente i modelli di IA. Diversi fattori chiave oltre a modelli, hardware e software dovrebbero essere presi in considerazione quando si valutano i framework IA per i sistemi embedded.\nAltri fattori chiave da considerare quando si sceglie un framework di apprendimento automatico sono prestazioni, scalabilit√†, facilit√† d‚Äôuso, integrazione con strumenti di ingegneria dei dati, integrazione con strumenti di ottimizzazione dei modelli e supporto della community. Gli sviluppatori possono prendere decisioni informate e massimizzare il potenziale delle iniziative di apprendimento automatico comprendendo questi diversi fattori.\n\nPrestazioni\nLe prestazioni sono fondamentali nei sistemi embedded in cui le risorse di calcolo sono limitate. Valutare la capacit√† del framework di ottimizzare l‚Äôinferenza del modello per l‚Äôhardware embedded. La quantizzazione del modello e il supporto dell‚Äôaccelerazione hardware sono cruciali per ottenere un‚Äôinferenza efficiente.\n\n\nScalabilit√†\nLa scalabilit√† √® essenziale quando si considera la potenziale crescita di un progetto di IA embedded. Il framework dovrebbe supportare l‚Äôimplementazione di modelli su vari dispositivi embedded, dai microcontrollori ai processori pi√π potenti. Dovrebbe inoltre gestire senza problemi sia le distribuzioni su piccola che su larga scala.\n\n\nIntegrazione con Strumenti di Data Engineering\nGli strumenti di ingegneria dei dati sono essenziali per la pre-elaborazione dei dati e la gestione della pipeline. Un framework di intelligenza artificiale ideale per sistemi embedded dovrebbe integrarsi perfettamente con questi strumenti, consentendo un‚Äôefficiente acquisizione dei dati, trasformazione e addestramento del modello.\n\n\nIntegrazione con Strumenti di Ottimizzazione del Modello\nL‚Äôottimizzazione del modello garantisce che i modelli di intelligenza artificiale siano adatti per la distribuzione embedded. Valutare se il framework si integra con strumenti di ottimizzazione del modello come TensorFlow Lite Converter o ONNX Runtime per facilitare la quantizzazione del modello e la riduzione delle dimensioni.\n\n\nFacilit√† d‚ÄôUso\nLa facilit√† d‚Äôuso di un framework di IA ha un impatto significativo sull‚Äôefficienza dello sviluppo. Un framework con un‚Äôinterfaccia intuitiva e una documentazione chiara riduce la curva di apprendimento degli sviluppatori. Si dovrebbe considerare se il framework supporta API di alto livello, consentendo agli sviluppatori di concentrarsi sulla progettazione del modello piuttosto che sui dettagli di implementazione di basso livello. Questo fattore √® incredibilmente importante per i sistemi embedded, che hanno meno funzionalit√† di quelle a cui gli sviluppatori tipici potrebbero essere abituati.\n\n\nSupporto della Community\nIl supporto della community gioca un altro fattore essenziale. I framework con community attive e coinvolte spesso hanno basi di codice ben mantenute, ricevono aggiornamenti regolari e forniscono forum preziosi per la risoluzione dei problemi. Di conseguenza, anche il supporto della community gioca un ruolo nella facilit√† d‚Äôuso perch√© garantisce che gli sviluppatori abbiano accesso a una vasta gamma di risorse, tra cui tutorial e progetti di esempio. Il supporto della community fornisce una certa garanzia che il framework continuer√† a essere supportato per futuri aggiornamenti. Ci sono solo pochi framework che soddisfano le esigenze di TinyML. TensorFlow Lite Micro √® il pi√π popolare e ha il maggior supporto della comunit√†.",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Framework di IA</span>"
    ]
  },
  {
    "objectID": "contents/core/frameworks/frameworks.it.html#tendenze-future-nei-framework-ml",
    "href": "contents/core/frameworks/frameworks.it.html#tendenze-future-nei-framework-ml",
    "title": "6¬† Framework di IA",
    "section": "6.10 Tendenze Future nei Framework ML",
    "text": "6.10 Tendenze Future nei Framework ML\n\n6.10.1 Decomposizione\nAttualmente, lo stack del sistema ML √® costituito da quattro astrazioni come mostrato in Figura¬†6.16, vale a dire (1) grafi computazionali, (2) programmi tensoriali, (3) librerie e runtime e (4) primitive hardware.\n\n\n\n\n\n\nFigura¬†6.16: Quattro astrazioni negli attuali stack dei sistemi ML. Fonte: TVM.\n\n\n\nCi√≤ ha portato a confini verticali (ad esempio, tra i livelli di astrazione) e orizzontali (ad esempio, approcci basati sulla libreria rispetto a quelli basati sulla compilazione per il calcolo dei tensori), che ostacolano l‚Äôinnovazione per il ML. Il lavoro futuro nei framework ML pu√≤ guardare alla rottura di questi confini. A dicembre 2021 √® stato proposto Apache TVM Unity, che mirava a facilitare le interazioni tra i diversi livelli di astrazione (nonch√© le persone dietro di essi, come scienziati ML, ingegneri ML e ingegneri hardware) e a co-ottimizzare le decisioni in tutti e quattro i livelli di astrazione.\n\n\n6.10.2 Compilatori e Librerie ad Alte Prestazioni\nCon l‚Äôulteriore sviluppo dei framework ML, continueranno a emergere compilatori e librerie ad alte prestazioni. Alcuni esempi attuali includono TensorFlow XLA e CUTLASS di Nvidia, che accelerano le operazioni di algebra lineare nei grafi computazionali, e TensorRT di Nvidia, che accelera e ottimizza l‚Äôinferenza.\n\n\n6.10.3 ML per Framework ML\nPossiamo anche usare il ML per migliorare i framework di ML in futuro. Alcuni usi correnti di ML per framework ML includono:\n\nOttimizzazione degli iperparametri tramite tecniche quali ottimizzazione bayesiana, ricerca casuale e ricerca a griglia\nNeural Architecture Search (NAS) per cercare automaticamente architetture di rete ottimali\nAutoML, che come descritto in Sezione 6.5, automatizza la pipeline ML.",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Framework di IA</span>"
    ]
  },
  {
    "objectID": "contents/core/frameworks/frameworks.it.html#conclusione",
    "href": "contents/core/frameworks/frameworks.it.html#conclusione",
    "title": "6¬† Framework di IA",
    "section": "6.11 Conclusione",
    "text": "6.11 Conclusione\nIn sintesi, la selezione del framework di machine learning ottimale richiede una valutazione approfondita di varie opzioni in base a criteri quali usabilit√†, supporto della community, prestazioni, compatibilit√† hardware e capacit√† di conversione del modello. Non esiste una soluzione adatta a tutti, poich√© il framework giusto dipende da vincoli e casi d‚Äôuso specifici.\nAbbiamo prima introdotto la necessit√† di framework di apprendimento automatico come TensorFlow e PyTorch. Questi framework offrono funzionalit√† quali tensori per la gestione di dati multidimensionali, grafi computazionali per la definizione e l‚Äôottimizzazione delle operazioni del modello e una suite di strumenti tra cui funzioni di perdita, ottimizzatori e caricatori di dati che semplificano lo sviluppo del modello.\nLe funzionalit√† avanzate migliorano ulteriormente l‚Äôusabilit√† di questi framework, consentendo attivit√† come la messa a punto di grandi modelli pre-addestrati e la facilitazione del ‚Äúfederated learning‚Äù. Queste funzionalit√† sono fondamentali per sviluppare modelli di apprendimento automatico sofisticati in modo efficiente.\nI framework di intelligenza artificiale embedded o TinyML, come TensorFlow Lite Micro, forniscono strumenti specializzati per distribuire modelli su piattaforme con risorse limitate. TensorFlow Lite Micro, ad esempio, offre strumenti di ottimizzazione completi, tra cui la mappatura della quantizzazione e le ottimizzazioni del kernel, per garantire prestazioni elevate su piattaforme basate su microcontrollori come i processori Arm Cortex-M e RISC-V. I framework creati appositamente per hardware specializzato come CMSIS-NN su processori Cortex-M possono massimizzare ulteriormente le prestazioni ma sacrificare la portabilit√†. I framework integrati dei fornitori di processori adattano lo stack alle loro architetture, liberando il pieno potenziale dei loro chip ma ci si blocca nel loro ecosistema.\nIn definitiva, la scelta del framework giusto implica la ricerca della migliore corrispondenza tra le sue capacit√† e i requisiti della piattaforma target. Ci√≤ richiede un bilanciamento tra esigenze di prestazioni, vincoli hardware, complessit√† del modello e altri fattori. Una valutazione approfondita dei modelli e dei casi d‚Äôuso previsti e la valutazione delle opzioni rispetto alle metriche chiave guideranno gli sviluppatori nella selezione del framework ideale per le loro applicazioni di machine learning.",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Framework di IA</span>"
    ]
  },
  {
    "objectID": "contents/core/frameworks/frameworks.it.html#sec-ai-frameworks-resource",
    "href": "contents/core/frameworks/frameworks.it.html#sec-ai-frameworks-resource",
    "title": "6¬† Framework di IA",
    "section": "6.12 Risorse",
    "text": "6.12 Risorse\nEcco un elenco curato di risorse per supportare studenti e insegnanti nei loro percorsi di apprendimento e insegnamento. Lavoriamo continuamente per espandere questa raccolta e presto aggiungeremo nuovi esercizi.\n\n\n\n\n\n\nSlide\n\n\n\n\n\nQueste slide sono uno strumento prezioso per gli insegnanti per tenere lezioni e per gli studenti per rivedere il materiale secondo il proprio ritmo. Incoraggiamo studenti e docenti a sfruttare queste slide per migliorare la loro comprensione e facilitare un trasferimento efficace delle conoscenze.\n\nFrameworks overview.\nEmbedded systems software.\nInference engines: TF vs.¬†TFLite.\nTF flavors: TF vs.¬†TFLite vs.¬†TFLite Micro.\nTFLite Micro:\n\nTFLite Micro Big Picture.\nTFLite Micro Interpreter.\nTFLite Micro Model Format.\nTFLite Micro Memory Allocation.\nTFLite Micro NN Operations.\n\n\n\n\n\n\n\n\n\n\n\nVideo\n\n\n\n\n\n\nProssimamente.\n\n\n\n\n\n\n\n\n\n\nEsercizi\n\n\n\n\n\nPer rafforzare i concetti trattati in questo capitolo, abbiamo curato una serie di esercizi che sfidano gli studenti ad applicare le proprie conoscenze e ad approfondire la propria comprensione.\n\nEsercizio¬†6.1\nEsercizio¬†6.2\nEsercizio¬†6.3",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Framework di IA</span>"
    ]
  },
  {
    "objectID": "contents/core/training/training.it.html",
    "href": "contents/core/training/training.it.html",
    "title": "7¬† Addestramento dell‚ÄôIA",
    "section": "",
    "text": "7.1 Panoramica\nIl training √® fondamentale per sviluppare sistemi di intelligenza artificiale accurati e utili tramite il machine learning. Il training crea un modello di apprendimento automatico che pu√≤ essere generalizzato a dati nuovi e inediti anzich√© memorizzare gli esempi dell‚Äôaddestramento. Ci√≤ avviene inserendo dati di training in algoritmi che apprendono pattern da questi esempi regolando i parametri interni.\nGli algoritmi riducono al minimo una ‚Äúfunzione loss‚Äù [perdita], che confronta le loro previsioni sui dati di training con le etichette o le soluzioni note, guidando l‚Äôapprendimento. Un training efficace richiede spesso set di dati rappresentativi di alta qualit√† sufficientemente grandi da catturare la variabilit√† nei casi d‚Äôuso del mondo reale.\nRichiede inoltre la scelta di un algoritmo adatto all‚Äôattivit√†, che si tratti di una rete neurale per la visione artificiale, un algoritmo di apprendimento di rinforzo per il controllo robotico o un metodo basato su alberi per la previsione categoriale. √à necessaria un‚Äôattenta messa a punto per la struttura del modello, come la profondit√† e la larghezza della rete neurale e i parametri di apprendimento come la dimensione del passo e la forza della regolarizzazione.\nSono importanti anche le tecniche per prevenire l‚Äôoverfitting, come le penalit√† di regolarizzazione nonch√© la convalida con dati trattenuti. L‚Äôoverfitting pu√≤ verificarsi quando un modello si adatta troppo ai dati di training, non riuscendo a generalizzare con i nuovi dati. Ci√≤ pu√≤ accadere se il modello √® troppo complesso o √® stato addestrato troppo a lungo.\nPer evitare l‚Äôoverfitting, le tecniche di regolarizzazione possono aiutare a vincolare il modello. Un metodo di regolarizzazione consiste nell‚Äôaggiungere un termine di penalit√† alla funzione di perdita che scoraggia la complessit√†, come la norma L2 dei pesi. Questo penalizza i valori dei parametri elevati. Un‚Äôaltra tecnica √® il dropout, in cui una percentuale di neuroni viene impostata casualmente a zero durante l‚Äôaddestramento. Ci√≤ riduce il co-adattamento dei neuroni.\nI metodi di validazione aiutano anche a rilevare ed evitare l‚Äôoverfitting. Una parte dei dati di training viene tenuta fuori dal ciclo di training come un set di validazione. Il modello viene valutato su questi dati. Se l‚Äôerrore di convalida aumenta mentre l‚Äôerrore di training diminuisce, si verifica un overfitting. Il training pu√≤ quindi essere interrotto in anticipo o regolarizzato in modo pi√π forte. La regolarizzazione e la convalida consentono ai modelli di addestrarsi alla massima capacit√† senza overfitting [sovra-adattare] i dati di training.\nIl training richiede notevoli risorse di elaborazione, in particolare per le reti neurali profonde (deep) utilizzate nella visione artificiale, nell‚Äôelaborazione del linguaggio naturale e in altre aree. Queste reti hanno milioni di pesi regolabili che devono essere regolati tramite un training esteso. I miglioramenti hardware e le tecniche di training distribuite hanno consentito di addestrare reti neurali sempre pi√π grandi che possono raggiungere prestazioni di livello umano in alcune attivit√†.\nIn sintesi, alcuni punti chiave sul training:\nGuideremo attraverso questi dettagli nelle restanti sezioni. Comprendere come sfruttare in modo efficace dati, algoritmi, ottimizzazione dei parametri e generalizzazione attraverso il training √® essenziale per sviluppare sistemi di intelligenza artificiale capaci e distribuibili che funzionino in modo robusto nel mondo reale.",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Addestramento dell'IA</span>"
    ]
  },
  {
    "objectID": "contents/core/training/training.it.html#panoramica",
    "href": "contents/core/training/training.it.html#panoramica",
    "title": "7¬† Addestramento dell‚ÄôIA",
    "section": "",
    "text": "I Dati sono cruciali: I modelli di machine learning apprendono dagli esempi nei dati di training. Dati pi√π rappresentativi e di qualit√† elevata portano a migliori prestazioni del modello. I dati devono essere elaborati e formattati per il training.\nGli algoritmi imparano dai dati: Diversi algoritmi (reti neurali, alberi decisionali, ecc.) hanno approcci diversi per trovare dei pattern nei dati. √à importante scegliere l‚Äôalgoritmo giusto per l‚Äôattivit√†.\nL‚Äôaddestramento affina i parametri del modello: L‚Äôaddestramento del modello regola i parametri interni per trovare pattern nei dati. I modelli avanzati come le reti neurali hanno molti pesi regolabili. L‚Äôaddestramento regola iterativamente i pesi per ridurre al minimo una funzione di perdita.\nLa generalizzazione √® l‚Äôobiettivo: Un modello che sovra-adatta i dati di addestramento non generalizzer√† bene. Le tecniche di regolarizzazione (dropout, early stopping arresto anticipato, ecc.) riducono il sovra-adattamento. I dati di validazione vengono utilizzati per valutare la generalizzazione.\nL‚Äôaddestramento richiede risorse di elaborazione: L‚Äôaddestramento di modelli complessi richiede una notevole potenza di elaborazione e tempo. I miglioramenti hardware e il training distribuito su GPU/TPU hanno consentito dei progressi.",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Addestramento dell'IA</span>"
    ]
  },
  {
    "objectID": "contents/core/training/training.it.html#matematica-delle-reti-neurali",
    "href": "contents/core/training/training.it.html#matematica-delle-reti-neurali",
    "title": "7¬† Addestramento dell‚ÄôIA",
    "section": "7.2 Matematica delle Reti Neurali",
    "text": "7.2 Matematica delle Reti Neurali\nIl deep learning ha rivoluzionato l‚Äôapprendimento automatico e l‚Äôintelligenza artificiale, consentendo ai computer di apprendere pattern complessi e prendere decisioni intelligenti. La rete neurale √® al centro della rivoluzione del deep learning e, come discusso nella sezione 3, ‚ÄúAvvio al Deep Learning‚Äù, √® un pilastro in alcuni di questi progressi.\nLe reti neurali sono costituite da semplici funzioni stratificate l‚Äôuna sull‚Äôaltra. Ogni layer acquisisce alcuni dati, esegue alcuni calcoli e li passa al layer successivo. Questi layer apprendono progressivamente funzionalit√† di alto livello utili per le attivit√† che la rete √® addestrata a svolgere. Ad esempio, in una rete addestrata per il riconoscimento delle immagini, il layer di input pu√≤ acquisire valori di pixel, mentre i layer successivi possono rilevare forme semplici come i bordi. I layer successivi possono rilevare forme pi√π complesse come nasi, occhi, ecc. Il layer di output finale classifica l‚Äôimmagine nel suo complesso.\nLa rete, in una rete neurale, si riferisce al modo in cui questi layer sono connessi. L‚Äôoutput di ogni layer √® considerato un set di neuroni, che sono collegati ai neuroni nei layer successivi, formando una ‚Äúrete‚Äù. Il modo in cui questi neuroni interagiscono √® determinato dai pesi tra di loro, che modellano le forze sinaptiche simili a quelle di un neurone del cervello. La rete neurale viene addestrata regolando questi pesi. Concretamente, i pesi vengono inizialmente impostati in modo casuale, quindi viene immesso l‚Äôinput, l‚Äôoutput viene confrontato con il risultato desiderato e, infine, i pesi vengono modificati per migliorare la rete. Questo processo viene ripetuto finch√© la rete non riduce al minimo in modo affidabile la perdita (loss), indicando di aver appreso i pattern nei dati.\nCome viene definito matematicamente questo processo? Formalmente, le reti neurali sono modelli matematici costituiti da operazioni lineari e non lineari alternate, parametrizzate da un set di pesi apprendibili che vengono addestrati per minimizzare una qualche funzione di perdita (loss). Questa funzione di perdita misura quanto √® buono il nostro modello per quanto riguarda l‚Äôadattamento dei nostri dati di addestramento e produce un valore numerico quando viene valutato sul nostro modello rispetto ai dati di addestramento. L‚Äôaddestramento delle reti neurali comporta la valutazione ripetuta della funzione di perdita su molti dati diversi per misurare quanto √® buono il nostro modello, quindi la modifica continua dei pesi del nostro modello utilizzando la backpropagation in modo che la perdita diminuisca, ottimizzando infine il modello per adattarlo ai nostri dati.\n\n7.2.1 Notazione delle Reti Neurali\nIl nucleo di una rete neurale pu√≤ essere visto come una sequenza di operazioni lineari e non lineari alternate, come mostrato in Figura¬†7.1.\n\n\n\n\n\n\nFigura¬†7.1: Diagramma della rete neurale. Fonte: astroML.\n\n\n\nLe reti neurali sono strutturate con layer [strati] di neuroni collegati da pesi (che rappresentano operazioni lineari) e funzioni di attivazione (che rappresentano operazioni non lineari). Esaminando la figura, vediamo come le informazioni fluiscono attraverso la rete, partendo dal layer di input, passando attraverso uno o pi√π layer nascosti, e infine raggiungendo il layer di output. Ogni connessione tra neuroni rappresenta un peso, mentre ciascun neurone applica tipicamente una funzione di attivazione non lineare ai suoi input.\nLa rete neurale funziona prendendo un vettore di input \\(x_i\\) e passandolo attraverso una serie di layer, ognuno dei quali esegue operazioni lineari e non lineari. L‚Äôoutput della rete a ogni layer \\(A_j\\) pu√≤ essere rappresentato come:\n\\[\nA_j = f\\left(\\sum_{i=1}^{N} w_{ij} x_i\\right)\n\\]\nDove:\n\n\\(N\\) - Il numero totale di feature di input.\n\\(x_{i}\\) - La singola feature di input, dove \\(i\\) varia da \\(1\\) a \\(N\\).\n\\(w_{ij}\\) - I pesi che collegano il neurone \\(i\\) in uno layer al neurone \\(j\\) nel layer successivo, che vengono aggiustati durante l‚Äôaddestramento.\n\\(f(\\theta)\\) - La funzione di attivazione non lineare applicata a ogni layer (ad esempio, ReLU, softmax, ecc.).\n\\(A_{j}\\) - L‚Äôoutput della rete neurale a ogni layer \\(j\\), dove \\(j\\) indica il numero del layer.\n\nNel contesto di Figura¬†7.1, \\(x_1, x_2, x_3, x_4,\\) e \\(x_5\\) rappresentano le caratteristiche di input. Ogni neurone di input \\(x_i\\) corrisponde a una feature dei dati di input. Le frecce dal layer di input al layer nascosto indicano le connessioni tra i neuroni di input e i neuroni nascosti, con ogni connessione associata a un peso \\(w_{ij}\\).\nIl layer nascosto √® costituito dai neuroni \\(a_1, a_2, a_3,\\) e \\(a_4\\), ognuno dei quali riceve input da tutti i neuroni nello layer di input. I pesi \\(w_{ij}\\) collegano i neuroni di input ai neuroni nascosti. Ad esempio, \\(w_{11}\\) √® il peso che collega l‚Äôinput \\(x_1\\) al neurone nascosto \\(a_1\\).\nIl numero di nodi in ogni layer e il numero totale di layer insieme definiscono l‚Äôarchitettura della rete neurale. Nel primo layer (layer di input), il numero di nodi corrisponde alla dimensionalit√† dei dati di input, mentre nell‚Äôultimo layer (layer di output), il numero di nodi corrisponde alla dimensionalit√† dell‚Äôoutput. Il numero di nodi nei layer intermedi pu√≤ essere impostato arbitrariamente, consentendo flessibilit√† nella progettazione dell‚Äôarchitettura di rete.\nI pesi, che determinano il modo in cui ogni layer della rete neurale interagisce con gli altri, sono matrici di numeri reali. Inoltre, ogni layer in genere include un vettore di bias [polarizzazione], ma qui lo ignoriamo per semplicit√†. La matrice dei pesi \\(W_j\\) che collega il layer \\(j-1\\) al layer \\(j\\) ha le dimensioni:\n\\[\nW_j \\in \\mathbb{R}^{d_j \\times d_{j-1}}\n\\]\ndove \\(d_j\\) √® il numero di nodi nel layer \\(j\\) e \\(d_{j-1}\\) √® il numero di nodi nel layer precedente \\(j-1\\).\nL‚Äôoutput finale \\(y_k\\) della rete si ottiene applicando un‚Äôaltra funzione di attivazione \\(g(\\theta)\\) alla somma ponderata degli output del layer nascosto:\n\\[\ny = g\\left(\\sum_{j=1}^{M} w_{jk} A_j\\right)\n\\]\nDove:\n\n\\(M\\) - Il numero di neuroni nascosti nel layer finale prima dell‚Äôoutput.\n\\(w_{jk}\\) - Il peso tra il neurone nascosto \\(a_j\\) e il neurone di output \\(y_k\\).\n\\(g(\\theta)\\) - La funzione di attivazione applicata alla somma ponderata degli output del layer nascosto.\n\nLa nostra rete neurale, come definita, esegue una sequenza di operazioni lineari e non lineari sui dati di input (\\(x_{i}\\)) per ottenere previsioni (\\(y_{i}\\)), che si spera siano una buona risposta a ci√≤ che vogliamo che la rete neurale faccia sull‚Äôinput (ad esempio, classificare se l‚Äôimmagine di input √® un gatto o meno). La nostra rete neurale pu√≤ quindi essere rappresentata succintamente come una funzione \\(N\\) che accetta un input \\(x \\in \\mathbb{R}^{d_0}\\) parametrizzato da \\(W_1, ..., W_n\\) e produce l‚Äôoutput finale \\(y\\):\n\\[\ny = N(x; W_1, ..., W_n) \\quad \\text{where } A_0 = x\n\\]\nQuesta equazione indica che la rete inizia con l‚Äôinput \\(A_0 = x\\) e calcola iterativamente \\(A_j\\) a ogni layer utilizzando i parametri \\(W_j\\) fino a produrre l‚Äôoutput finale \\(y\\) al layer di output.\nSuccessivamente vedremo come valutare questa rete neurale rispetto ai dati di addestramento introducendo una funzione di perdita.\n\n\n\n\n\n\nNota\n\n\n\nPerch√© sono necessarie le operazioni non lineari? Se avessimo solo layer lineari, l‚Äôintera rete sarebbe equivalente a un singolo layer lineare costituito dal prodotto degli operatori lineari. Quindi, le funzioni non lineari svolgono un ruolo chiave nella potenza delle reti neurali poich√© migliorano la capacit√† della rete neurale di adattare le funzioni.\n\n\n\n\n\n\n\n\nNota\n\n\n\nAnche le convoluzioni sono operatori lineari e possono essere convertite in una moltiplicazione di matrici.\n\n\n\n\n7.2.2 Funzione Loss come Misura della Bont√† di Adattamento Rispetto ai Dati di Addestramento\nDopo aver definito la nostra rete neurale, ci vengono forniti alcuni dati di addestramento, ovvero un set di punti \\({(x_j, y_j)}\\) per \\(j=1 \\rightarrow M\\), dove \\(M\\) √® il numero totale di campioni nel set di dati e \\(j\\) indicizza ogni campione. Vogliamo valutare quanto √® buona la nostra rete neurale nell‚Äôadattare questi dati. Per fare ci√≤, introduciamo una funzione di perdita, ovvero una funzione che prende l‚Äôoutput della rete neurale su un particolare punto dati \\(\\hat{y_j} = N(x_j; W_1, ..., W_n)\\) e lo confronta con la ‚Äúetichetta‚Äù di quel particolare dato (il corrispondente \\(y_j\\)) e restituisce un singolo scalare numerico (ovvero un numero reale) che rappresenta quanto √® ‚Äúbene‚Äù la rete neurale adatta quel particolare dato; la misura finale di quanto √® buona la rete neurale sull‚Äôintero set di dati √® quindi solo la media delle perdite su tutti i dati.\nEsistono molti tipi diversi di funzioni di perdita; ad esempio, nel caso della classificazione delle immagini, potremmo usare la funzione di ‚Äúcross-entropy loss‚Äù [perdita di entropia incrociata], che ci dice quanto bene si confrontano due vettori che rappresentano le previsioni di classificazione (ad esempio, se la nostra previsione prevede che un‚Äôimmagine sia pi√π probabilmente un cane, ma l‚Äôetichetta dice che √® un gatto, restituir√† una ‚Äúperdita‚Äù elevata, che indica un cattivo adattamento).\nMatematicamente, una funzione di perdita √® una funzione che prende due vettori con valori reali, uno che rappresenta gli output previsti della rete neurale e l‚Äôaltro che rappresenta le etichette vere, e restituisce un singolo scalare numerico che rappresenta l‚Äôerrore o la ‚Äúperdita‚Äù.\n\\[\nL: \\mathbb{R}^{d_{n}} \\times \\mathbb{R}^{d_{n}} \\longrightarrow \\mathbb{R}\n\\]\nPer un singolo esempio di training, la perdita √® data da:\n\\[\nL(N(x_j; W_1, ..., W_n), y_j)\n\\]\ndove \\(\\hat{y}_j = N(x_j; W_1, ..., W_n)\\) √® l‚Äôoutput previsto della rete neurale per l‚Äôinput \\(x_j\\), and \\(y_j\\) √® la vera etichetta.\nLa perdita totale nell‚Äôintero set di dati, \\(L_{full}\\), viene quindi calcolata come la perdita media in tutti i dati di training:\n\nFunzione di Perdita per l‚ÄôOttimizzazione del Modello di Rete Neurale su Dataset \\[\nL_{full} = \\frac{1}{M} \\sum_{j=1}^{M} L(N(x_j; W_1,...W_n), y_j)\n\\]\n\n\n\n7.2.3 Addestramento di Reti Neurali con Discesa del Gradiente\nOra che possiamo misurare quanto bene la nostra rete si adatta ai dati di training, possiamo ottimizzare i pesi della rete neurale per ridurre al minimo questa perdita. In questo contesto, stiamo denotando \\(W_i\\) come pesi per ogni layer \\(i\\) nella rete. Ad alto livello, modifichiamo i parametri delle matrici a valori reali \\(W_i\\) per ridurre al minimo la funzione di perdita \\(L_{full}\\). Nel complesso, il nostro obiettivo matematico √®\n\nObiettivo dell‚ÄôAddestramento della Rete Neurale \\[\nmin_{W_1, ..., W_n} L_{full}\n\\] \\[\n= min_{W_1, ..., W_n} \\frac{1}{M} \\sum_{j=1}^{M} L(N(x_j; W_1,...W_n), y_j)\n\\]\n\nQuindi, come ottimizziamo questo obiettivo? Ricordiamo dal calcolo che la minimizzazione di una funzione pu√≤ essere eseguita prendendo la derivata della funzione relativa ai parametri di input e modificando i parametri nella direzione del gradiente. Questa tecnica √® chiamata a ‚Äúdiscesa del gradiente‚Äù e concretamente comporta il calcolo della derivata della funzione di perdita \\(L_{full}\\) relativa a \\(W_1, ..., W_n\\) per ottenere un gradiente per questi parametri per fare un passo avanti, poi aggiornare questi parametri nella direzione del gradiente. Quindi, possiamo addestrare la nostra rete neurale utilizzando la discesa del gradiente, che applica ripetutamente la regola di aggiornamento.\n\nRegola di Aggiornamento della Discesa del Gradiente \\[\nW_i := W_i - \\lambda \\frac{\\partial L_{full}}{\\partial W_i} \\mbox{ for } i=1..n\n\\]\n\n\n\n\n\n\n\nNota\n\n\n\nIn pratica, il gradiente viene calcolato su un mini-batch di punti dati per migliorare l‚Äôefficienza computazionale. Questo processo √® chiamato ‚Äúdiscesa del gradiente stocastico‚Äù o ‚Äúdiscesa del gradiente batch‚Äù.\n\n\nDove \\(\\lambda\\) √® la dimensione del passo o il tasso di apprendimento delle nostre modifiche, nell‚Äôaddestramento della nostra rete neurale, eseguiamo ripetutamente il passaggio precedente fino alla convergenza, o quando la perdita non diminuisce pi√π. Figura¬†7.2 illustra questo processo: vogliamo raggiungere il punto minimo, il che si ottiene seguendo il gradiente (come illustrato con le frecce blu nella figura). Questo precedente approccio √® noto come discesa del gradiente completa poich√© stiamo calcolando la derivata relativa a tutti i dati di addestramento e solo dopo eseguiamo un singolo passaggio del gradiente; un approccio pi√π efficiente √® quello di calcolare il gradiente relativo solo a un batch casuale di dati e poi eseguire un passaggio, un processo noto come discesa del gradiente batch o discesa del gradiente stocastica (Robbins e Monro 1951), che √® pi√π efficiente poich√© ora eseguiamo molti pi√π passi per passaggio di tutti i dati di addestramento. Successivamente, tratteremo la matematica alla base del calcolo del gradiente della funzione di perdita relativa a \\(W_i\\), un processo noto come backpropagation.\n\nRobbins, Herbert, e Sutton Monro. 1951. ¬´A Stochastic Approximation Method¬ª. The Annals of Mathematical Statistics 22 (3): 400‚Äì407. https://doi.org/10.1214/aoms/1177729586.\n\n\n\n\n\n\nFigura¬†7.2: Discesa del gradiente. Fonte: Towards Data Science.\n\n\n\n\n\n7.2.4 Backpropagation\nL‚Äôaddestramento delle reti neurali comporta ripetute applicazioni dell‚Äôalgoritmo di discesa del gradiente, che prevede il calcolo della derivata della funzione di perdita rispetto alle \\(W_i\\). Come calcoliamo la derivata della perdita relativa alle \\(W_i\\), dato che le \\(W_i\\) sono funzioni annidate l‚Äôuna dell‚Äôaltra in una rete neurale profonda? Il trucco √® sfruttare la regola della catena: possiamo calcolare la derivata della perdita relativa alle \\(W_i\\) applicando ripetutamente la regola della catena in un processo completo noto come backpropagation. In particolare, possiamo calcolare i gradienti calcolando la derivata della perdita relativa agli output dell‚Äôultimo layer, poi usarla progressivamente per calcolare la derivata della perdita relativa a ciascun layer precedente a quello di input. Questo processo inizia dalla fine della rete (il layer pi√π vicino all‚Äôoutput) e procede all‚Äôindietro, e quindi prende il nome di backpropagation.\nAnalizziamolo. Possiamo calcolare la derivata della perdita relativa agli output di ciascun layer della rete neurale utilizzando applicazioni ripetute della regola della catena.\n\\[\n\\frac{\\partial L_{full}}{\\partial L_{n}} = \\frac{\\partial A_{n}}{\\partial L_{n}} \\frac{\\partial L_{full}}{\\partial A_{n}}\n\\]\n\\[\n\\frac{\\partial L_{full}}{\\partial L_{n-1}} = \\frac{\\partial A_{n-1}}{\\partial L_{n-1}} \\frac{\\partial L_{n}}{\\partial A_{n-1}} \\frac{\\partial A_{n}}{\\partial L_{n}} \\frac{\\partial L_{full}}{\\partial A_{n}}  \n\\]\no pi√π in generale\n\\[\n\\frac{\\partial L_{full}}{\\partial L_{i}} = \\frac{\\partial A_{i}}{\\partial L_{i}} \\frac{\\partial L_{i+1}}{\\partial A_{i}} ... \\frac{\\partial A_{n}}{\\partial L_{n}} \\frac{\\partial L_{full}}{\\partial A_{n}}  \n\\]\n\n\n\n\n\n\nNota\n\n\n\nIn quale ordine dovremmo eseguire questo calcolo? Da una prospettiva computazionale, √® preferibile eseguire i calcoli dalla fine alla parte frontale. (ad esempio: prima si calcola \\(\\frac{\\partial L_{full}}{\\partial A_{n}}\\), poi i termini precedenti, anzich√© iniziare dal centro) poich√© ci√≤ evita di materializzare e calcolare grandi jacobiani. Questo perch√© \\(\\ \\frac {\\partial L_{full}}{\\partial A_{n}}\\) √® un vettore; quindi, qualsiasi operazione di matrice che include questo termine ha un output che √® compresso per essere un vettore. Quindi, eseguire il calcolo dalla fine evita grandi moltiplicazioni matrice-matrice assicurando che i prodotti intermedi siano vettori.\n\n\n\n\n\n\n\n\nNota\n\n\n\nNella nostra notazione, assumiamo che le attivazioni intermedie \\(A_{i}\\) siano vettori colonna, anzich√© vettori riga, quindi la regola della catena √® \\(\\frac{\\partial L}{\\partial L_{i}} = \\frac{\\partial L_{i+1}}{\\partial L_{i}} ... \\frac{\\partial L}{\\partial L_{n}}\\) piuttosto che \\(\\frac{\\partial L}{\\partial L_{i}} = \\frac{\\partial L}{\\partial L_{n}} ... \\frac{\\partial L_{i+1}}{\\partial L_{i}}\\)\n\n\nDopo aver calcolato la derivata della perdita relativa all‚Äôoutput di ogni layer, possiamo facilmente ottenere la derivata della perdita relativa ai parametri, utilizzando di nuovo la regola della catena:\n\\[\n\\frac{\\partial L_{full}}{W_{i}} = \\frac{\\partial L_{i}}{\\partial W_{i}} \\frac{\\partial L_{full}}{\\partial L_{i}}\n\\]\nEd √® in definitiva cos√¨ che le derivate dei pesi dei layer vengono calcolate usando la backpropagation! Come appare concretamente in un esempio specifico? Di seguito, esaminiamo un esempio specifico di una semplice rete neurale a 2 layer su un‚Äôattivit√† di regressione usando una funzione di perdita MSE con input a 100 dimensioni e uno layer nascosto a 30 dimensioni:\n\nEsempio di backpropagation\nSupponiamo di avere una rete neurale a due layer \\[\nL_1 = W_1 A_{0}\n\\] \\[\nA_1 = ReLU(L_1)\n\\] \\[\nL_2 = W_2 A_{1}\n\\] \\[\nA_2 = ReLU(L_2)\n\\] \\[\nNN(x) = \\mbox{Let } A_{0} = x \\mbox{ then output } A_2\n\\] dove \\(W_1 \\in \\mathbb{R}^{30 \\times 100}\\) e \\(W_2 \\in \\mathbb{R}^{1 \\times 30}\\). Inoltre, supponiamo di utilizzare la funzione di perdita MSE: \\[\nL(x, y) = (x-y)^2\n\\] Vogliamo calcolare \\[\n\\frac{\\partial L(NN(x), y)}{\\partial W_i} \\mbox{ for } i=1,2\n\\] Notare quanto segue: \\[\n\\frac{\\partial L(x, y)}{\\partial x} = 2 \\times (x-y)\n\\] \\[\n\\frac{\\partial ReLU(x)}{\\partial x} \\delta  = \\left\\{\\begin{array}{lr}\n0 & \\text{for } x \\leq 0 \\\\\n1 & \\text{for } x \\geq 0 \\\\\n\\end{array}\\right\\} \\odot \\delta\n\\] \\[\n\\frac{\\partial WA}{\\partial A} \\delta = W^T \\delta\n\\] \\[\n\\frac{\\partial WA}{\\partial W} \\delta = \\delta A^T\n\\] Quindi abbiamo \\[\n\\frac{\\partial L(NN(x), y)}{\\partial W_2} = \\frac{\\partial L_2}{\\partial W_2} \\frac{\\partial A_2}{\\partial L_2} \\frac{\\partial L(NN(x), y)}{\\partial A_2}\n\\] \\[\n= (2L(NN(x) - y) \\odot ReLU'(L_2)) A_1^T\n\\] e \\[\n\\frac{\\partial L(NN(x), y)}{\\partial W_1} = \\frac{\\partial L_1}{\\partial W_1} \\frac{\\partial A_1}{\\partial L_1} \\frac{\\partial L_2}{\\partial A_1} \\frac{\\partial A_2}{\\partial L_2} \\frac{\\partial L(NN(x), y)}{\\partial A_2}\n\\] \\[\n= [ReLU'(L_1) \\odot (W_2^T [2L(NN(x) - y) \\odot ReLU'(L_2)])] A_0^T\n\\]\n\n\n\n\n\n\n\nConsiglio\n\n\n\nRicontrollare il lavoro assicurandosi che le forme siano corrette!\n\nTutti i prodotti di Hadamard (\\(\\odot\\)) dovrebbero operare su tensori della stessa forma\nTutte le moltiplicazioni di matrici dovrebbero operare su matrici che condividono una dimensione comune (ad esempio, m per n, n per k)\nTutti i gradienti relativi ai pesi dovrebbero avere la stessa forma delle stesse matrici dei pesi\n\n\n\nL‚Äôintero processo di backpropagation pu√≤ essere complesso, specialmente per reti molto profonde. Fortunatamente, framework di machine learning come PyTorch supportano la differenziazione automatica, che esegue la backpropagation. In questi framework, dobbiamo semplicemente specificare il passaggio in avanti e le derivate ci verranno calcolate automaticamente. Tuttavia, √® utile comprendere il processo teorico che avviene internamente in questi framework di apprendimento automatico.\n\n\n\n\n\n\nNota\n\n\n\nCome visto sopra, le attivazioni intermedie \\(A_i\\) vengono riutilizzate nella backpropagation. Per migliorare le prestazioni, queste attivazioni vengono memorizzate nella cache dal passaggio in avanti per evitare di essere ricalcolate. Tuttavia, le attivazioni devono essere mantenute in memoria tra i passaggi in avanti e indietro, il che comporta un maggiore utilizzo della memoria. Se la rete e le dimensioni del batch sono grandi, ci√≤ potrebbe causare problemi di memoria. Analogamente, le derivate rispetto agli output di ogni layer vengono memorizzate nella cache per evitare il ricalcolo.\n\n\n\n\n\n\n\n\nEsercizio¬†7.1: Reti Neurali con Backpropagation e Discesa del Gradiente\n\n\n\n\n\nScoprire la matematica dietro le potenti reti neurali! Il deep learning potrebbe sembrare magico, ma √® radicato nei principi matematici. In questo capitolo, abbiamo scomposto la notazione delle reti neurali, le funzioni di perdita e la potente tecnica della backpropagation. Ora, prepariamoci a implementare questa teoria con questi notebook Colab. Immergersi nel cuore di come le reti neurali apprendono. Si vedr√† la matematica dietro la backpropagation e la discesa del gradiente, aggiornando quei pesi passo dopo passo.",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Addestramento dell'IA</span>"
    ]
  },
  {
    "objectID": "contents/core/training/training.it.html#grafi-del-calcolo-differenziabili",
    "href": "contents/core/training/training.it.html#grafi-del-calcolo-differenziabili",
    "title": "7¬† Addestramento dell‚ÄôIA",
    "section": "7.3 Grafi del Calcolo Differenziabili",
    "text": "7.3 Grafi del Calcolo Differenziabili\nIn generale, la discesa del gradiente stocastico mediante backpropagation pu√≤ essere eseguita su qualsiasi grafo computazionale che un utente pu√≤ definire, a condizione che le operazioni del calcolo siano differenziabili. Pertanto, le librerie generiche di deep learning come PyTorch e Tensorflow consentono agli utenti di specificare il loro processo computazionale (ad esempio, reti neurali) come grafo computazionale. La backpropagation viene eseguita automaticamente tramite differenziazione automatica quando la discesa del gradiente stocastico viene eseguita su questi grafi computazionali. Inquadrare l‚Äôaddestramento dell‚ÄôIA come un problema di ottimizzazione su grafi di calcolo differenziabili √® un modo generale per comprendere cosa sta accadendo internamente con i sistemi di deep learning.\nLa struttura raffigurata in Figura¬†7.3 mostra un segmento di un grafo computazionale differenziabile. In questo grafo, l‚Äôinput ‚Äòx‚Äô viene elaborato tramite una serie di operazioni: viene prima moltiplicato per una matrice di pesi ‚ÄòW‚Äô (MatMul), poi aggiunto a un bias ‚Äòb‚Äô (Add) e infine passato a una funzione di attivazione, Rectified Linear Unit (ReLU). Questa sequenza di operazioni ci fornisce l‚Äôoutput C. La natura differenziabile del grafo significa che ogni operazione ha un gradiente ben definito. La differenziazione automatica, come implementata nei framework ML, sfrutta questa propriet√† per calcolare in modo efficiente i gradienti della perdita rispetto a ciascun parametro nella rete (ad esempio, ‚ÄòW‚Äô e ‚Äòb‚Äô).\n\n\n\n\n\n\nFigura¬†7.3: Grafo Computazionale. Fonte: TensorFlow.",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Addestramento dell'IA</span>"
    ]
  },
  {
    "objectID": "contents/core/training/training.it.html#dati-di-training",
    "href": "contents/core/training/training.it.html#dati-di-training",
    "title": "7¬† Addestramento dell‚ÄôIA",
    "section": "7.4 Dati di Training",
    "text": "7.4 Dati di Training\nPer consentire un training efficace della rete neurale, i dati disponibili devono essere suddivisi in set di training, di validazione e di test. Il set di training viene utilizzato per addestrare i parametri del modello. Il set di validazione valuta il modello durante il training per ottimizzare gli iperparametri e prevenire l‚Äôoverfitting. Il set di test fornisce una valutazione finale imparziale delle prestazioni del modello addestrato.\nMantenere chiare suddivisioni tra training, validation e test con dati rappresentativi √® fondamentale per addestrare, ottimizzare e valutare correttamente i modelli per ottenere le migliori prestazioni nel mondo reale. A tal fine, scopriremo le insidie o gli errori comuni che le persone commettono quando creano queste suddivisioni dei dati.\nTabella¬†7.1 confronta le differenze tra le suddivisioni dei dati di training, validazione e test:\n\n\n\nTabella¬†7.1: Confronto tra suddivisioni di dati di training, validazione e test.\n\n\n\n\n\n\n\n\n\n\nSuddivisione\nScopo\nDimensioni tipiche\n\n\n\n\nSet di addestramento\nAddestrare i parametri del modello\n60-80% dei dati totali\n\n\nSet di validazione\nValutare il modello durante l‚Äôaddestramento per ottimizzare gli iperparametri e prevenire l‚Äôoverfitting\n‚àº20% dei dati totali\n\n\nSet di test\nFornire una valutazione imparziale del modello finale addestrato\n‚àº20% dei dati totali\n\n\n\n\n\n\n\n7.4.1 Suddivisioni di Dataset\n\nSet di Training\nIl set di training viene utilizzato per addestrare il modello. √à il sottoinsieme pi√π grande, in genere il 60-80% dei dati totali. Il modello vede e impara dai dati di addestramento per fare previsioni. √à necessario un set di training sufficientemente grande e rappresentativo affinch√© il modello apprenda efficacemente i pattern sottostanti.\n\n\nSet di Validazione\nIl set di validazione valuta il modello durante l‚Äôaddestramento, in genere dopo ogni epoca. Solitamente, il 20% dei dati viene assegnato a questo set. Il modello non impara n√© aggiorna i suoi parametri in base ai dati di validazione. Vengono usati per ottimizzare gli iperparametri e apportare altre modifiche per migliorare l‚Äôaddestramento. Il monitoraggio di metriche come perdita e accuratezza sul set di validazione impedisce l‚Äôoverfitting solo sui dati di addestramento.\n\n\nSet di Test\nIl set di test agisce come un dataset che il modello non ha visto durante l‚Äôaddestramento. Viene utilizzato per fornire una valutazione imparziale del modello addestrato finale. In genere, il 20% dei dati √® riservato ai test. Mantenere un set di test ‚Äúhold-out‚Äù [esterno] √® fondamentale per ottenere una stima accurata di come il modello addestrato si comporterebbe su dati non ancora visti del mondo reale. La mancanza di dati dal set di test deve essere evitata a tutti i costi.\nLe proporzioni relative dei set di training, validazione e test possono variare in base alle dimensioni dei dati e all‚Äôapplicazione. Tuttavia, seguire le linee guida generali per una suddivisione 60/20/20 √® un buon punto di partenza. Un‚Äôattenta suddivisione dei dati garantisce che i modelli siano adeguatamente addestrati, ottimizzati e valutati per ottenere le prestazioni migliori.\nVideo¬†7.1 spiega come suddividere correttamente il dataset in set di training, validazione e test, assicurando un processo di training ottimale.\n\n\n\n\n\n\nVideo¬†7.1: Train/Dev/Test Sets\n\n\n\n\n\n\n\n\n\n7.4.2 Errori e Insidie Comuni\n\nDati di Training Insufficienti\nAssegnare troppo pochi dati al set di training √® un errore comune quando si suddividono i dati, il che pu√≤ avere un impatto significativo sulle prestazioni del modello. Se il set di training √® troppo piccolo, il modello non avr√† campioni sufficienti per apprendere in modo efficace i veri pattern nei dati. Ci√≤ comporta un‚Äôelevata varianza e impedisce al modello di generalizzare bene ai nuovi dati.\nAd esempio, se si addestra un modello di classificazione delle immagini per riconoscere cifre scritte a mano, fornire solo 10 o 20 immagini per classe di cifre sarebbe del tutto inadeguato. Il modello avrebbe bisogno di pi√π esempi per catturare le ampie varianze negli stili di scrittura, rotazioni, larghezze dei tratti e altre varianti.\nCome regola generale, la dimensione del training set dovrebbe essere di almeno centinaia o migliaia di esempi affinch√© la maggior parte degli algoritmi di apprendimento automatico funzioni in modo efficace. A causa dell‚Äôelevato numero di parametri, il set di training spesso deve essere di decine o centinaia di migliaia per le reti neurali profonde, in particolare quelle che utilizzano layer convoluzionali.\nDati di training insufficienti si manifestano in genere in sintomi quali alti tassi di errore su set di validazione/test, bassa accuratezza del modello, alta varianza e overfitting su campioni di set di training di piccole dimensioni. La soluzione √® raccogliere pi√π dati di training di qualit√†. Le tecniche di data augmentation possono anche aiutare ad aumentare virtualmente le dimensioni dei dati di training per immagini, audio, ecc.\n√à importante considerare attentamente la complessit√† del modello e la difficolt√† del problema quando si assegnano i campioni di training per garantire che siano disponibili dati sufficienti affinch√© il modello possa apprendere correttamente. Si consiglia inoltre di seguire le linee guida sulle dimensioni minime dei set di training per diversi algoritmi. Sono necessari pi√π dati di training per mantenere il successo complessivo di qualsiasi applicazione di machine learning.\nSi consideri Figura¬†7.4 dove proviamo a classificare/suddividere i dati in due categorie (qui, per colore): a sinistra, l‚Äôoverfitting √® rappresentato da un modello che ha appreso troppo bene le sfumature nei dati di training (o il set di dati era troppo piccolo o abbiamo eseguito il modello per troppo tempo), facendo s√¨ che segua il rumore insieme al segnale, come indicato dalle eccessive curve della linea. Il lato destro mostra l‚Äôunderfitting, dove la semplicit√† del modello gli impedisce di catturare la struttura sottostante del dataset, con conseguente linea che non si adatta bene ai dati. Il grafico centrale rappresenta un adattamento ideale, dove il modello bilancia bene tra generalizzazione e adattamento, catturando la tendenza principale dei dati senza essere influenzato da valori anomali. Sebbene il modello non sia un adattamento perfetto (manca di alcuni punti), ci interessa di pi√π la sua capacit√† di riconoscere pattern generali piuttosto che valori anomali idiosincratici.\n\n\n\n\n\n\nFigura¬†7.4: Adattamento dei dati: overfitting, right fit e underfitting. Fonte: MathWorks.\n\n\n\nFigura¬†7.5 il processo di adattamento dei dati nel tempo. Durante l‚Äôaddestramento, cerchiamo il ‚Äúpunto ottimale‚Äù tra underfitting e overfitting. Inizialmente, quando il modello non ha avuto abbastanza tempo per apprendere i pattern nei dati, ci troviamo nella zona di underfitting, indicata da alti tassi di errore sul set di convalida (da ricordare che il modello √® addestrato sul set di addestramento e testiamo la sua generalizzabilit√† sul set di convalida o sui dati che non ha mai visto prima). A un certo punto, raggiungiamo un minimo globale per i tassi di errore e idealmente vogliamo interrompere l‚Äôaddestramento l√¨. Se continuiamo l‚Äôaddestramento, il modello inizier√† a ‚Äúmemorizzare‚Äù o a conoscere i dati troppo bene, tanto che il tasso di errore inizier√† a risalire, poich√© il modello non riuscir√† a generalizzare a dati che non ha mai visto prima.\n\n\n\n\n\n\nFigura¬†7.5: Adattamento dei dati nel tempo. Fonte: IBM.\n\n\n\nIl Video¬†7.2 fornisce una panoramica di bias e varianza e la relazione tra i due concetti e l‚Äôaccuratezza del modello.\n\n\n\n\n\n\nVideo¬†7.2: Bias/Varianza\n\n\n\n\n\n\n\n\nPerdita di Dati Tra Set\nIl ‚Äúdata leakage‚Äù [perdita di dati] si riferisce al trasferimento involontario di informazioni tra i set di training, convalida e test. Ci√≤ viola il presupposto fondamentale che le divisioni siano reciprocamente esclusive. La perdita di dati porta a risultati di valutazione seriamente compromessi e metriche di prestazioni gonfiate.\nUn modo comune in cui si verifica la perdita di dati √® se alcuni campioni del set di test vengono inavvertitamente inclusi nei dati di training. Quando si valuta il set di test, il modello ha gi√† visto alcuni dati, il che fornisce punteggi eccessivamente ottimistici. Ad esempio, se il 2% dei dati di test trapelano nel set di training di un classificatore binario, pu√≤ comportare un aumento della precisione fino al 20%!\nSe le divisioni dei dati non vengono eseguite con attenzione, possono verificarsi forme di perdita pi√π sottili. Se le divisioni non vengono randomizzate e mescolate correttamente, i campioni che sono vicini tra loro nel set di dati potrebbero finire nella stessa divisione, portando a distorsioni della distribuzione. Ci√≤ crea una fuga di informazioni basata sulla prossimit√† nel set di dati.\nUn altro caso √® quando i set di dati hanno campioni collegati, intrinsecamente connessi, come grafici, reti o dati di serie temporali. La suddivisione ‚Äúingenua‚Äù pu√≤ isolare nodi o intervalli di tempo connessi in set diversi. I modelli possono fare ipotesi non valide basate su informazioni parziali.\nPer prevenire la perdita di dati √® necessario creare una solida separazione tra le suddivisioni: nessun campione dovrebbe esistere in pi√π di una suddivisione. Il mescolamento e la suddivisione randomizzata aiutano a creare divisioni robuste. Le tecniche di ‚Äúcross-validation‚Äù [validazione incrociata] possono essere utilizzate per una valutazione pi√π rigorosa. Rilevare la perdita √® difficile, ma i segnali rivelatori includono modelli che funzionano molto meglio sui dati di test rispetto a quelli di validazione.\nLa perdita di dati compromette gravemente la validit√† della valutazione perch√© il modello ha gi√† visto parzialmente i dati di test. Nessuna quantit√† di messa a punto o architetture complesse pu√≤ sostituire le suddivisioni nette dei dati. √à meglio essere prudenti e creare una separazione completa tra le suddivisioni per evitare questo errore fondamentale nelle pipeline di machine learning.\n\n\nSet di Validazione Piccolo o Non Rappresentativo\nIl set di validazione viene utilizzato per valutare le prestazioni del modello durante l‚Äôaddestramento e per ottimizzare gli iperparametri. Per valutazioni affidabili e stabili, il set di validazione deve essere sufficientemente ampio e rappresentativo della distribuzione dei dati reali. Tuttavia, ci√≤ pu√≤ rendere pi√π impegnativa la selezione e l‚Äôottimizzazione del modello.\nAd esempio, se il set di validazione contiene solo 100 campioni, le metriche calcolate avranno un‚Äôelevata varianza. A causa del rumore, l‚Äôaccuratezza pu√≤ variare fino al 5-10% tra le epoche. Questo rende difficile sapere se un calo nell‚Äôaccuratezza della validazione √® dovuto a un overfitting o a una varianza naturale. Con un set di validazione pi√π ampio, diciamo 1000 campioni, le metriche saranno molto pi√π stabili.\nInoltre, se il set di validazione non √® rappresentativo, forse mancano alcune sottoclassi, la capacit√† stimata del modello potrebbe essere gonfiata. Ci√≤ potrebbe portare a scelte di iperparametri scadenti o a interruzioni premature dell‚Äôaddestramento. I modelli selezionati in base a tali set di validazione distorti non si generalizzano bene ai dati reali.\nUna buona regola pratica √® che la dimensione del set di convalida dovrebbe essere di almeno diverse centinaia di campioni e fino al 10-20% del set di addestramento, lasciando comunque campioni sufficienti per l‚Äôaddestramento. Le divisioni dovrebbero anche essere stratificate, il che significa che le proporzioni di classe nel set di validazione dovrebbero corrispondere a quelle nel set di dati completo, soprattutto se si lavora con set di dati sbilanciati. Un set di validazione pi√π ampio che rappresenti le caratteristiche dei dati originali √® essenziale per una corretta selezione e messa a punto del modello.\n\n\nRiutilizzo del Set di Test Pi√π Volte\nIl set di test √® progettato per fornire una valutazione imparziale del modello completamente addestrato solo una volta alla fine del processo di sviluppo del modello. Riutilizzare il set di test pi√π volte durante lo sviluppo per la valutazione del modello, la messa a punto degli iperparametri, la selezione del modello, ecc., pu√≤ causare un overfitting sui dati di test. Invece, si deve riservare il set di test per una valutazione finale del modello completamente addestrato, trattandolo come una scatola nera per simularne le prestazioni su dati reali. Questo approccio fornisce metriche affidabili per determinare se il modello √® pronto per la distribuzione in produzione.\nSe il set di test viene riutilizzato come parte del processo di validazione, il modello potrebbe iniziare a vedere e imparare dai campioni di test. Questo, insieme all‚Äôottimizzazione intenzionale o meno delle prestazioni del modello sul set di test, pu√≤ gonfiare artificialmente metriche come l‚Äôaccuratezza.\nAd esempio, supponiamo che il set di test venga utilizzato ripetutamente per la selezione del modello su 5 architetture. In tal caso, il modello potrebbe raggiungere il 99% di accuratezza del test memorizzando i campioni anzich√© apprendere pattern generalizzabili. Tuttavia, quando implementati nel mondo reale, l‚Äôaccuratezza dei nuovi dati potrebbe scendere del 60%.\nLa prassi migliore √® interagire con il set di test solo una volta alla fine per segnalare metriche imparziali su come il modello finale ottimizzato si comporterebbe nel mondo reale. Durante lo sviluppo del modello, il set di convalida dovrebbe essere utilizzato per tutte le attivit√† di ottimizzazione dei parametri, selezione del modello, arresto anticipato e simili. √à importante riservare una parte, come il 20-30% dell‚Äôintero set di dati, esclusivamente per la valutazione finale del modello. Questi dati non dovrebbero essere utilizzati per la convalida, l‚Äôottimizzazione o la selezione del modello durante lo sviluppo.\nNon mantenere un set ‚Äúhold-out‚Äù non visto per la convalida finale rischia di ottimizzare i risultati e trascurare potenziali errori prima del rilascio del modello. Avere alcuni dati nuovi fornisce un controllo di integrit√† finale sull‚Äôefficacia nel mondo reale. Mantenere la completa separazione di addestramento/validazione dal set di test √® essenziale per ottenere stime accurate delle prestazioni del modello. Anche piccole deviazioni da un singolo utilizzo del set di test potrebbero falsare positivamente i risultati e le metriche, fornendo una visione eccessivamente ottimistica dell‚Äôefficacia nel mondo reale.\n\n\nStesse Suddivisioni dei Dati negli Esperimenti\nQuando si confrontano diversi modelli di machine learning o si sperimentano varie architetture e iperparametri, utilizzare le stesse suddivisioni dei dati per l‚Äôaddestramento, la validazione e il test nei diversi esperimenti pu√≤ introdurre distorsioni e invalidare le comparazioni.\nSe le stesse suddivisioni vengono riutilizzate, i risultati della valutazione potrebbero essere pi√π bilanciati e misurare accuratamente quale modello funziona meglio. Ad esempio, una certa suddivisione casuale dei dati potrebbe favorire il modello A rispetto al modello B indipendentemente dagli algoritmi. Riutilizzare questa suddivisione causer√† quindi distorsioni a favore del modello A.\nInvece, le suddivisioni dei dati dovrebbero essere randomizzate o mescolate per ogni iterazione sperimentale. Ci√≤ garantisce che la casualit√† nel campionamento delle suddivisioni non conferisca un vantaggio ingiusto a nessun modello.\nCon diverse suddivisioni per esperimento, la valutazione diventa pi√π solida. Ogni modello viene testato su un‚Äôampia gamma di set di test estratti casualmente dalla popolazione complessiva, attenuando la variazione e rimuovendo la correlazione tra i risultati.\nLa prassi corretta √® quella di impostare un ‚Äúseed‚Äù casuale prima di suddividere i dati per ogni esperimento. La suddivisione dovrebbe avvenire dopo il rimescolamento/ricampionamento come parte della pipeline sperimentale. Eseguire confronti sulle stesse suddivisioni viola l‚Äôipotesi i.i.d (indipendenti e identicamente distribuite) richiesta per la validit√† statistica.\nLe suddivisioni univoche sono essenziali per confronti di modelli equi. Sebbene richieda un‚Äôelaborazione pi√π intensiva, l‚Äôallocazione randomizzata per esperimento rimuove la distorsione del campionamento e consente un benchmarking valido. Ci√≤ evidenzia le vere differenze nelle prestazioni del modello indipendentemente dalle caratteristiche di una particolare suddivisione.\n\n\nMancata Stratificazione delle Suddivisioni\nQuando si suddividono i dati in set di training, validazione e test, la mancata stratificazione delle suddivisioni pu√≤ comportare una rappresentazione non uniforme delle classi target tra le suddivisioni e introdurre un bias di campionamento. Ci√≤ √® particolarmente problematico per i set di dati sbilanciati.\nLa suddivisione stratificata implica il campionamento dei dati in modo che la proporzione di classi di output sia approssimativamente preservata in ogni suddivisione. Ad esempio, se si esegue una suddivisione training-test 70/30 su un set di dati con campioni negativi al 60% e positivi al 40%, la stratificazione garantisce esempi negativi al ~60% e positivi al ~40% sia nei set di training che nei set di test.\nSenza stratificazione, la casualit√† potrebbe comportare che la suddivisione di training abbia campioni positivi al 70% mentre il test ha campioni positivi al 30%. Il modello addestrato su questa distribuzione di training distorta non si generalizzer√† bene. Lo squilibrio delle classi compromette anche le metriche del modello come l‚Äôaccuratezza.\nLa stratificazione funziona meglio quando viene eseguita utilizzando etichette, sebbene proxy come il clustering possano essere utilizzati per l‚Äôapprendimento non supervisionato. Diventa essenziale per set di dati altamente distorti con classi rare che potrebbero essere facilmente omesse dalle suddivisioni.\nLibrerie come Scikit-Learn hanno metodi di suddivisione stratificati nativi. Non utilizzarli potrebbe inavvertitamente introdurre bias di campionamento e danneggiare le prestazioni del modello sui gruppi minoritari. Dopo aver eseguito le suddivisioni, il bilanciamento complessivo delle classi dovrebbe essere esaminato per garantire una rappresentazione uniforme tra le suddivisioni.\nLa stratificazione fornisce un set di dati bilanciato sia per l‚Äôaddestramento del modello che per la valutazione. Sebbene la semplice suddivisione casuale sia facile, tenendo conto delle esigenze di stratificazione, specialmente per dati sbilanciati nel mondo reale, si traduce in uno sviluppo e una valutazione del modello pi√π solidi.\n\n\nIgnorare le Dipendenze delle Serie Temporali\nI dati delle serie temporali hanno una struttura temporale intrinseca con osservazioni dipendenti dal contesto passato. Suddividere ingenuamente i dati delle serie temporali in set di training e test senza tenere conto di questa dipendenza porta a perdite di dati e bias di lookahead.\nAd esempio, suddividere semplicemente una serie temporale nel primo 70% di training e nell‚Äôultimo 30% come dati di test contaminer√† i dati di training con dati futuri. Il modello pu√≤ usare queste informazioni per ‚Äúsbirciare‚Äù in avanti durante il training.\nCi√≤ si traduce in una valutazione eccessivamente ottimistica delle prestazioni del modello. Il modello pu√≤ sembrare che preveda il futuro in modo accurato, ma in realt√† ha appreso implicitamente in base ai dati futuri, il che non si traduce in prestazioni nel mondo reale.\nDovrebbero essere utilizzate tecniche di validazione incrociata delle serie temporali appropriate, come il concatenamento in avanti, per preservare l‚Äôordine e la dipendenza. Il set di test dovrebbe contenere solo dati da una finestra temporale futura a cui il modello non √® stato esposto per il training.\nNon tenere conto delle relazioni temporali porta a ipotesi di causalit√† non valide. Se i dati di training contengono dati futuri, il modello potrebbe anche dover imparare come estrapolare ulteriormente le previsioni.\nMantenere il flusso temporale degli eventi ed evitare il bias di lookahead √® fondamentale per addestrare e testare correttamente i modelli di serie temporali. Ci√≤ garantisce che possano davvero prevedere pattern futuri e non solo memorizzare i dati di training passati.\n\n\nNessun Dato Non Visto per la Valutazione Finale\nUn errore comune quando si suddividono i dati √® non metterne da parte una porzione solo per la valutazione finale del modello completato. Tutti i dati vengono utilizzati per training, validazione e set di test durante lo sviluppo.\nQuesto non lascia dati non visti per ottenere una stima imparziale di come il modello finale ottimizzato si comporterebbe nel mondo reale. Le metriche sul set di test utilizzate durante lo sviluppo potrebbero riflettere solo parzialmente le reali capacit√† del modello.\nAd esempio, scelte come l‚Äôarresto anticipato e l‚Äôottimizzazione degli iperparametri sono spesso ottimizzate in base alle prestazioni del set di test. Questo accoppia il modello ai dati di test. √à necessario un set di dati non visto per interrompere questo accoppiamento e ottenere metriche reali del mondo reale.\nLa ‚Äúbest practice‚Äù √® quella di riservare una parte, come il 20-30% del set di dati completo, esclusivamente per la valutazione finale del modello. Questi dati non dovrebbero essere utilizzati per la convalida, l‚Äôottimizzazione o la selezione del modello durante lo sviluppo.\nIl salvataggio di alcuni dati non visti consente di valutare il modello completamente addestrato come una scatola nera su dati del mondo reale. Questo fornisce metriche affidabili per decidere se il modello √® pronto per la distribuzione in produzione.\nNon mantenere un set ‚Äúhold-out‚Äù non visto per la convalida finale rischia di ottimizzare i risultati e trascurare potenziali errori prima del rilascio del modello. Avere alcuni dati nuovi fornisce un controllo di integrit√† finale sull‚Äôefficacia nel mondo reale.\n\n\nSovra-ottimizzazione del Set di Validazione\nIl set di validazione √® pensato per guidare il processo di training del modello, non per fungere da dati di training aggiuntivi. L‚Äôeccessiva ottimizzazione del set di validazione per massimizzare le metriche delle prestazioni lo tratta pi√π come un set di training secondario, portando a metriche gonfiate e scarsa generalizzazione.\nAd esempio, tecniche come l‚Äôottimizzazione estensiva degli iperparametri o l‚Äôaggiunta di incrementi di dati mirati a migliorare l‚Äôaccuratezza della convalida possono far s√¨ che il modello si adatti troppo ai dati di validazione. Il modello pu√≤ raggiungere un‚Äôaccuratezza di validazione del 99% ma solo un‚Äôaccuratezza di test del 55%.\nAnalogamente, riutilizzare il set di validazione per un arresto anticipato pu√≤ anche ottimizzare il modello specificamente per quei dati. L‚Äôarresto alle migliori prestazioni di validazione sovra-adatta il rumore e le fluttuazioni causate dalle piccole dimensioni di validazione.\nIl set di validazione funge da proxy per ottimizzare e selezionare i modelli. Tuttavia, l‚Äôobiettivo rimane massimizzare le prestazioni dei dati del mondo reale, non il set di validazione. Ridurre al minimo la perdita o l‚Äôerrore sui dati di validazione non si traduce automaticamente in una buona generalizzazione.\nUn buon approccio √® quello di mantenere l‚Äôuso del set di validazione al minimo: gli iperparametri possono essere regolati grossolanamente prima sui dati di training, ad esempio. Il set di validazione guida il training ma non dovrebbe influenzare o alterare il modello stesso. √à uno strumento diagnostico, non di ottimizzazione.\nQuando si valutano le prestazioni sul set di validazione, bisogna fare attenzione a non sovra-adattare. Sono necessari dei compromessi per costruire modelli che funzionino bene sulla popolazione complessiva e non siano eccessivamente regolati sui campioni di validazione.",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Addestramento dell'IA</span>"
    ]
  },
  {
    "objectID": "contents/core/training/training.it.html#algoritmi-di-ottimizzazione",
    "href": "contents/core/training/training.it.html#algoritmi-di-ottimizzazione",
    "title": "7¬† Addestramento dell‚ÄôIA",
    "section": "7.5 Algoritmi di Ottimizzazione",
    "text": "7.5 Algoritmi di Ottimizzazione\nStochastic gradient descent (SGD) √® un algoritmo di ottimizzazione semplice ma potente per l‚Äôaddestramento di modelli di machine learning. Funziona stimando il gradiente della funzione di perdita relativa ai parametri del modello utilizzando un singolo esempio di addestramento e poi aggiornando i parametri nella direzione che riduce la perdita.\nSebbene concettualmente semplice, SGD necessita di alcune aree di miglioramento. Innanzitutto, scegliere un tasso di apprendimento appropriato pu√≤ essere difficile: troppo piccolo e i progressi sono molto lenti; troppo grande e i parametri possono oscillare e non convergere. In secondo luogo, SGD tratta tutti i parametri in modo uguale e indipendente, il che potrebbe non essere l‚Äôideale in tutti i casi. Infine, SGD vanilla [standard] utilizza solo informazioni sul gradiente di primo ordine, il che si traduce in progressi lenti su problemi mal condizionati.\n\n7.5.1 Ottimizzazioni\nNel corso degli anni, sono state proposte varie ottimizzazioni per accelerare e migliorare l‚ÄôSGD vanilla. Ruder (2016) fornisce un‚Äôeccellente panoramica dei diversi ottimizzatori. In breve, diverse tecniche di ottimizzazione SGD comunemente utilizzate includono:\n\nRuder, Sebastian. 2016. ¬´An overview of gradient descent optimization algorithms¬ª. ArXiv preprint abs/1609.04747 (settembre). http://arxiv.org/abs/1609.04747v2.\nMomentum: Accumula un vettore di velocit√† in direzioni di gradiente persistente attraverso le iterazioni. Ci√≤ aiuta ad accelerare i progressi smorzando le oscillazioni e mantiene i progressi in direzioni coerenti.\nNesterov Accelerated Gradient (NAG): Una variante di momentum che calcola i gradienti in ‚Äúlook ahead‚Äù anzich√© nella posizione del parametro corrente. Questo aggiornamento anticipatorio impedisce l‚Äôovershooting mentre il momentum mantiene il progresso accelerato.\nAdagrad: Un algoritmo di velocit√† di apprendimento adattivo che mantiene una velocit√† di apprendimento per parametro ridotta proporzionalmente alla somma storica dei gradienti di ciascun parametro. Aiuta a eliminare la necessit√† di regolare manualmente i tassi di apprendimento (Duchi, Hazan, e Singer 2010).\n\nDuchi, John C., Elad Hazan, e Yoram Singer. 2010. ¬´Adaptive Subgradient Methods for Online Learning and Stochastic Optimization¬ª. In COLT 2010 - The 23rd Conference on Learning Theory, Haifa, Israel, June 27-29, 2010, a cura di Adam Tauman Kalai e Mehryar Mohri, 257‚Äì69. Omnipress. http://colt2010.haifa.il.ibm.com/papers/COLT2010proceedings.pdf#page=265.\n\nZeiler, Matthew D. 2012. ¬´ADADELTA: An Adaptive Learning Rate Method¬ª, dicembre, 119‚Äì49. https://doi.org/10.1002/9781118266502.ch6.\nAdadelta: Una modifica ad Adagrad limita la finestra dei gradienti passati accumulati, riducendo cos√¨ il decadimento aggressivo dei tassi di apprendimento (Zeiler 2012).\nRMSProp: Divide il tasso di apprendimento per una media esponenzialmente decrescente dei gradienti quadrati. Ci√≤ ha un effetto di normalizzazione simile ad Adagrad ma non accumula i gradienti nel tempo, evitando un rapido decadimento dei tassi di apprendimento (Hinton 2017).\n\nHinton, Geoffrey. 2017. ¬´Overview of Minibatch Gradient Descent¬ª. University of Toronto; University Lecture.\n\nKingma, Diederik P., e Jimmy Ba. 2014. ¬´Adam: A Method for Stochastic Optimization¬ª. A cura di Yoshua Bengio e Yann LeCun, dicembre. http://arxiv.org/abs/1412.6980v9.\nAdam: Combinazione di momentum e rmsprop dove rmsprop modifica il tasso di apprendimento in base alla media delle recenti ampiezze dei gradienti. Mostra un progresso iniziale molto rapido e regola automaticamente le dimensioni dei passi (Kingma e Ba 2014).\nAMSGrad: Una variante di Adam che assicura una convergenza stabile mantenendo il massimo dei gradienti quadratici passati, impedendo al tasso di apprendimento di aumentare durante l‚Äôaddestramento (Reddi, Kale, e Kumar 2019).\n\nReddi, Sashank J., Satyen Kale, e Sanjiv Kumar. 2019. ¬´On the Convergence of Adam and Beyond¬ª. arXiv preprint arXiv:1904.09237, aprile. http://arxiv.org/abs/1904.09237v1.\nTra questi metodi, Adam √® ampiamente considerato l‚Äôalgoritmo di ottimizzazione di riferimento per molte attivit√† di deep-learning. Supera costantemente SGD vanilla in termini di velocit√† di addestramento e prestazioni. Altri ottimizzatori potrebbero essere pi√π adatti in alcuni casi, in particolare per modelli pi√π semplici.\n\n\n7.5.2 Compromessi\nTabella¬†7.2 √® una tabella di pro e contro per alcuni dei principali algoritmi di ottimizzazione per l‚Äôaddestramento di reti neurali:\n\n\n\nTabella¬†7.2: Confronto dei pro e dei contro di diversi algoritmi di ottimizzazione.\n\n\n\n\n\n\n\n\n\n\nAlgoritmo\nPro\nContro\n\n\n\n\nMomentum\n\nConvergenza pi√π rapida dovuta all‚Äôaccelerazione lungo i gradienti\nMinore oscillazione rispetto a SGD vanilla\n\n\nRichiede la messa a punto del parametro momentum\n\n\n\nNesterov Accelerated Gradient (NAG)\n\nPi√π veloce dello slancio standard in alcuni casi\nGli aggiornamenti anticipati impediscono il superamento\n\n\nPi√π complesso da comprendere intuitivamente\n\n\n\nAdagrad\n\nElimina la necessit√† di regolare manualmente i tassi di apprendimento\nFunziona bene su gradienti radi\n\n\nIl tasso di apprendimento potrebbe decadere troppo rapidamente su gradienti densi\n\n\n\nAdadelta\n\nDecadimento del tasso di apprendimento meno aggressivo rispetto ad Adagrad\n\n\nAncora sensibile al valore iniziale del tasso di apprendimento\n\n\n\nRMSProp\n\nRegola automaticamente i tassi di apprendimento\nFunziona bene nella pratica\n\n\nNessun aspetto negativo importante\n\n\n\nAdam\n\nCombinazione di momentum e tassi di apprendimento adattivo\nConvergenza efficiente e veloce\n\n\nPrestazioni di generalizzazione leggermente peggiori in alcuni casi\n\n\n\nAMSGrad\n\nMiglioramento di Adam che affronta il problema della generalizzazione\n\n\nNon √® stato utilizzato/testato cos√¨ ampiamente come Adam\n\n\n\n\n\n\n\n\n\n7.5.3 Algoritmi di Benchmarking\nNon esiste un singolo metodo migliore per tutti i tipi di problemi. Ci√≤ significa che abbiamo bisogno di un benchmarking completo per identificare l‚Äôottimizzatore pi√π efficace per set di dati e modelli specifici. Le prestazioni di algoritmi come Adam, RMSProp e Momentum variano a seconda delle dimensioni del batch, dei programmi di apprendimento, dell‚Äôarchitettura del modello, della distribuzione dei dati e della regolarizzazione. Queste variazioni sottolineano l‚Äôimportanza di valutare ogni ottimizzatore in diverse condizioni.\nPrendiamo ad esempio Adam, che spesso eccelle nelle attivit√† di visione artificiale, a differenza di RMSProp, che potrebbe mostrare una migliore generalizzazione in determinate attivit√† di elaborazione del linguaggio naturale. La forza di Momentum risiede nella sua accelerazione in scenari con direzioni di gradiente coerenti, mentre i tassi di apprendimento adattivo di Adagrad sono pi√π adatti per problemi di gradiente sparso.\nQuesta vasta gamma di interazioni tra ottimizzatori dimostra la difficolt√† di dichiarare un singolo algoritmo universalmente superiore. Ogni ottimizzatore ha punti di forza unici, rendendo fondamentale valutare vari metodi per scoprire empiricamente le loro condizioni di applicazione ottimali.\nUn approccio di benchmarking completo dovrebbe valutare la velocit√† di convergenza e fattori come errore di generalizzazione, stabilit√†, sensibilit√† degli iperparametri ed efficienza computazionale, tra gli altri. Ci√≤ comporta il monitoraggio delle curve di apprendimento di training e convalida su pi√π esecuzioni e il confronto degli ottimizzatori su vari set di dati e modelli per comprenderne i punti di forza e di debolezza.\nAlgoPerf, introdotto da D√ºrr et al. (2021), risponde alla necessit√† di un sistema di benchmarking robusto. Questa piattaforma valuta le prestazioni dell‚Äôottimizzatore utilizzando criteri quali curve di loss [perdita] di training, errore di generalizzazione, sensibilit√† agli iperparametri ed efficienza computazionale. AlgoPerf testa vari metodi di ottimizzazione, tra cui Adam, LAMB e Adafactor, su diversi tipi di modelli come CNN e RNN/LSTM su set di dati stabiliti. Utilizza la ‚Äúcontainerizzazione‚Äù e la raccolta automatica di metriche per ridurre al minimo le incongruenze e consente esperimenti controllati su migliaia di configurazioni, fornendo una base affidabile per confrontare gli ottimizzatori.\n\nD√ºrr, Marc, Gunnar Nissen, Kurt-Wolfram S√ºhs, Philipp Schwenkenbecher, Christian Geis, Marius Ringelstein, Hans-Peter Hartung, et al. 2021. ¬´CSF Findings in Acute NMDAR and LGI1 Antibody‚ÄìAssociated Autoimmune Encephalitis¬ª. Neurology Neuroimmunology &amp; Neuroinflammation 8 (6). https://doi.org/10.1212/nxi.0000000000001086.\nLe informazioni ottenute da AlgoPerf e benchmark simili sono inestimabili per guidare la scelta ottimale o la messa a punto degli ottimizzatori. Abilitando valutazioni riproducibili, questi benchmark contribuiscono a una comprensione pi√π approfondita delle prestazioni di ciascun ottimizzatore, aprendo la strada a innovazioni future e progressi accelerati nel settore.",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Addestramento dell'IA</span>"
    ]
  },
  {
    "objectID": "contents/core/training/training.it.html#ottimizzazione-degli-iperparametri",
    "href": "contents/core/training/training.it.html#ottimizzazione-degli-iperparametri",
    "title": "7¬† Addestramento dell‚ÄôIA",
    "section": "7.6 Ottimizzazione degli Iperparametri",
    "text": "7.6 Ottimizzazione degli Iperparametri\nGli iperparametri sono impostazioni importanti nei modelli di machine learning che incidono notevolmente sulle prestazioni finali dei modelli. A differenza di altri parametri del modello che vengono appresi durante l‚Äôaddestramento, gli iperparametri vengono specificati dai ‚Äúdata scientist‚Äù o dagli ingegneri del machine learning prima dell‚Äôaddestramento del modello.\nLa scelta dei valori degli iperparametri corretti consente ai modelli di apprendere pattern dai dati in modo efficace. Alcuni esempi di iperparametri chiave negli algoritmi di apprendimento automatico includono:\n\nReti neurali: Velocit√† di apprendimento, dimensione del batch, numero di unit√† nascoste, funzioni di attivazione\nMacchine a vettori di supporto: Forza di regolarizzazione, tipo di kernel e parametri\nRandom forest: Numero di alberi, profondit√† dell‚Äôalbero\nK-means: Numero di cluster\n\nIl problema √® che non ci sono regole pratiche affidabili per scegliere configurazioni ottimali degli iperparametri: in genere si devono provare valori diversi e valutare le prestazioni. Questo processo √® chiamato ‚Äúhyperparameter tuning‚Äù ottimizzazione degli iperparametri.\nNei primi anni del moderno deep learning, i ricercatori erano ancora alle prese con problemi di convergenza instabile e lenta. I punti dolenti comuni includevano perdite di training che fluttuavano selvaggiamente, gradienti che esplodevano o svanivano e un‚Äôampia serie di tentativi ed errori necessari per addestrare le reti in modo affidabile. Di conseguenza, un punto focale iniziale era l‚Äôutilizzo di iperparametri per controllare l‚Äôottimizzazione del modello. Ad esempio, tecniche seminali come la normalizzazione batch consentivano una convergenza pi√π rapida del modello regolando gli aspetti dello spostamento interno delle covariate. I metodi di velocit√† di apprendimento adattivo hanno anche mitigato la necessit√† di estese pianificazioni manuali. Questi affrontavano problemi di ottimizzazione durante l‚Äôaddestramento, come la divergenza incontrollata del gradiente. Le velocit√† di apprendimento adattate con attenzione sono anche il fattore di controllo primario per ottenere una convergenza rapida e stabile anche oggi.\nCon l‚Äôespansione esponenziale della capacit√† computazionale negli anni successivi, modelli molto pi√π grandi potevano essere addestrati senza cadere preda di problemi di pura ottimizzazione numerica. L‚Äôattenzione si √® spostata verso la generalizzazione, sebbene una convergenza efficiente fosse un prerequisito fondamentale. Tecniche all‚Äôavanguardia come ‚ÄúTransformers‚Äù hanno introdotto miliardi di parametri. A tali dimensioni, gli iperparametri relativi a capacit√†, regolarizzazione, ensembling [raggruppamento], ecc., hanno assunto un ruolo centrale per la messa a punto, anzich√© solo le metriche di convergenza grezze.\nLa lezione √® che comprendere l‚Äôaccelerazione e la stabilit√† del processo di ottimizzazione stesso costituisce il lavoro di base. Schemi di inizializzazione, dimensioni dei batch, decadimenti di peso e altri iperparametri di training rimangono indispensabili oggi. Dominare una convergenza rapida e impeccabile consente ai professionisti di espandere la propria attenzione sulle esigenze emergenti relative alla messa a punto di parametri quali accuratezza, robustezza ed efficienza su larga scala.\n\n7.6.1 Algoritmi di Ricerca\nQuando si tratta del processo critico di ottimizzazione degli iperparametri, ci sono diversi algoritmi sofisticati su cui gli specialisti del machine learning si affidano per cercare sistematicamente nel vasto spazio di possibili configurazioni dei modelli. Alcuni degli algoritmi di ricerca degli iperparametri pi√π importanti includono:\n\nGrid Search: Il metodo di ricerca pi√π elementare, in cui si definisce manualmente una griglia di valori da controllare per ogni iperparametro. Ad esempio, controllando velocit√† di apprendimento = [0.01, 0.1, 1] e dimensioni batch = [32, 64, 128]. Il vantaggio principale √® la semplicit√†, ma pu√≤ portare a un‚Äôesplosione esponenziale nello spazio di ricerca, rendendolo dispendioso in termini di tempo. √à pi√π adatto per l‚Äôottimizzazione di un piccolo numero di parametri.\nRandom Search: Invece di definire una griglia, si selezionano casualmente valori per ogni iperparametro da un intervallo o set predefinito. Questo metodo √® pi√π efficiente nell‚Äôesplorazione di un vasto spazio di iperparametri perch√© non richiede una ricerca esaustiva. Tuttavia, potrebbe comunque non trovare parametri ottimali poich√© non esplora sistematicamente tutte le possibili combinazioni.\nBayesian Optimization: Questo √® un approccio probabilistico avanzato per l‚Äôesplorazione adattiva basato su una funzione surrogata per modellare le prestazioni su iterazioni. √à semplice ed efficiente: trova iperparametri altamente ottimizzati in meno passaggi di valutazione. Tuttavia, richiede un maggiore investimento nella configurazione (Snoek, Larochelle, e Adams 2012).\nEvolutionary Algorithms: Questi algoritmi imitano i principi della selezione naturale. Generano popolazioni di combinazioni di iperparametri e le evolvono nel tempo in base alle prestazioni. Questi algoritmi offrono solide capacit√† di ricerca pi√π adatte per superfici di risposta complesse. Tuttavia, sono necessarie molte iterazioni per una convergenza ragionevole.\nPopulation Based Training (PBT): Un metodo che ottimizza gli iperparametri addestrando pi√π modelli in parallelo, consentendo loro di condividere e adattare configurazioni di successo durante l‚Äôaddestramento, combinando elementi di ricerca casuale e algoritmi evolutivi (Jaderberg et al. 2017).\nNeural Architecture Search: Un approccio alla progettazione di architetture ad alte prestazioni per reti neurali. Tradizionalmente, gli approcci NAS utilizzano una qualche forma di apprendimento di rinforzo per proporre architetture di reti neurali, che vengono poi ripetutamente valutate (Zoph e Le 2016).\n\n\nSnoek, Jasper, Hugo Larochelle, e Ryan P. Adams. 2012. ¬´Practical Bayesian Optimization of Machine Learning Algorithms¬ª. In Advances in Neural Information Processing Systems 25: 26th Annual Conference on Neural Information Processing Systems 2012. Proceedings of a meeting held December 3-6, 2012, Lake Tahoe, Nevada, United States, a cura di Peter L. Bartlett, Fernando C. N. Pereira, Christopher J. C. Burges, L√©on Bottou, e Kilian Q. Weinberger, 2960‚Äì68. https://proceedings.neurips.cc/paper/2012/hash/05311655a15b75fab86956663e1819cd-Abstract.html.\n\nJaderberg, Max, Valentin Dalibard, Simon Osindero, Wojciech M. Czarnecki, Jeff Donahue, Ali Razavi, Oriol Vinyals, et al. 2017. ¬´Population Based Training of Neural Networks¬ª. arXiv preprint arXiv:1711.09846, novembre. http://arxiv.org/abs/1711.09846v2.\n\nZoph, Barret, e Quoc V. Le. 2016. ¬´Neural Architecture Search with Reinforcement Learning¬ª, novembre, 367‚Äì92. https://doi.org/10.1002/9781394217519.ch17.\n\n\n7.6.2 Implicazioni di Sistema\nLa messa a punto degli iperparametri pu√≤ avere un impatto significativo sul tempo di convergenza durante l‚Äôaddestramento del modello, influenzando direttamente il runtime complessivo. I valori corretti per gli iperparametri chiave di training sono cruciali per un‚Äôefficiente convergenza del modello. Ad esempio, la velocit√† di apprendimento dell‚Äôiperparametro controlla la dimensione del passo durante l‚Äôottimizzazione della discesa del gradiente. Impostando correttamente uno scheduling della velocit√† di apprendimento assicura che l‚Äôalgoritmo di ottimizzazione converga rapidamente verso un buon minimo. Una velocit√† di apprendimento troppo bassa porta a una convergenza dolorosamente lenta, mentre un valore troppo grande causa una fluttuazione selvaggia delle perdite. Una messa a punto corretta assicura un rapido movimento verso pesi e bias ottimali.\nAnalogamente, la dimensione del batch per la discesa del gradiente stocastica influisce sulla stabilit√† della convergenza. La giusta dimensione del batch attenua le fluttuazioni negli aggiornamenti dei parametri per avvicinarsi pi√π rapidamente al minimo. Sono necessarie pi√π dimensioni del batch per evitare una convergenza rumorosa, mentre le dimensioni maggiori del batch non riescono a generalizzare e rallentano la convergenza a causa di aggiornamenti dei parametri meno frequenti. La messa a punto degli iperparametri per una convergenza pi√π rapida e una durata di addestramento ridotta ha implicazioni dirette sui costi e sui requisiti di risorse per il ridimensionamento dei sistemi di machine learning:\n\nCosti computazionali inferiori: Tempi di convergenza pi√π brevi significano costi computazionali inferiori per i modelli di training. Il training ML sfrutta spesso grandi istanze di cloud computing come cluster GPU e TPU che comportano pesanti costi orari. Ridurre al minimo i tempi di training riduce direttamente questo costo di noleggio delle risorse, che tende a dominare i budget ML per le organizzazioni. Un‚Äôiterazione pi√π rapida consente inoltre agli esperti di dati di sperimentare pi√π liberamente all‚Äôinterno dello stesso budget.\nTempo di training ridotto: Un tempo di training ridotto sblocca opportunit√† per addestrare pi√π modelli utilizzando lo stesso budget computazionale. Gli iperparametri ottimizzati estendono ulteriormente le risorse disponibili, consentendo alle aziende di sviluppare e sperimentare pi√π modelli con vincoli di risorse per massimizzare le prestazioni.\nEfficienza delle risorse: Un training pi√π rapido consente di allocare istanze di calcolo pi√π piccole nel cloud poich√© i modelli richiedono l‚Äôaccesso alle risorse per una durata pi√π breve. Ad esempio, un job di training di un‚Äôora consente di utilizzare istanze GPU meno potenti rispetto a un training di pi√π ore, che richiede un accesso di elaborazione sostenuto su intervalli pi√π lunghi. Ci√≤ consente di risparmiare sui costi, soprattutto per carichi di lavoro di grandi dimensioni.\n\nCi sono anche altri vantaggi. Ad esempio, una convergenza pi√π rapida riduce la pressione sui team di ingegneria ML in merito al provisioning delle risorse di training. Le semplici routine di riaddestramento del modello possono utilizzare risorse meno potenti anzich√© richiedere l‚Äôaccesso a code ad alta priorit√† per cluster GPU di livello di produzione vincolati, liberando risorse di distribuzione per altre applicazioni.\n\n\n7.6.3 Gli Auto Tuner\nData la sua importanza, esiste un‚Äôampia gamma di offerte commerciali per aiutare con l‚Äôottimizzazione degli iperparametri. Toccheremo brevemente due esempi: uno incentrato sull‚Äôottimizzazione per ML su scala cloud e l‚Äôaltro per modelli di apprendimento automatico mirati ai microcontrollori. Tabella¬†7.3 delinea le principali differenze:\n\n\n\nTabella¬†7.3: Confronto di piattaforme di ottimizzazione per diversi casi d‚Äôuso di machine learning.\n\n\n\n\n\n\n\n\n\n\n\nPiattaforma\nCaso d‚ÄôUso Target\nTecniche di ottimizzazione\nVantaggi\n\n\n\n\nVertex AI di Google\nApprendimento automatico su scala cloud\nOttimizzazione bayesiana, addestramento Population-Based\nNasconde la complessit√†, consentendo modelli rapidi e pronti per l‚Äôimplementazione con ottimizzazione iperparametrica all‚Äôavanguardia\n\n\nEON Tuner di Edge Impulse\nModelli di microcontrollori (TinyML)\nOttimizzazione bayesiana\nAdatta i modelli per dispositivi con risorse limitate, semplifica l‚Äôottimizzazione per l‚Äôimplementazione embedded\n\n\n\n\n\n\n\nBigML\nSono disponibili diverse piattaforme commerciali di auto-tuning per risolvere questo problema. Una soluzione √® Vertex AI Cloud di Google, che offre un ampio supporto integrato per tecniche di ottimizzazione all‚Äôavanguardia.\nUna delle funzionalit√† pi√π importanti della piattaforma di apprendimento automatico gestita da Vertex AI di Google √® l‚Äôottimizzazione efficiente e integrata degli iperparametri per lo sviluppo del modello. Per addestrare con successo modelli ML performanti √® necessario identificare configurazioni ottimali per un set di iperparametri esterni che determinano il comportamento del modello, ponendo un problema di ricerca ad alta dimensione impegnativo. Vertex AI semplifica questo processo tramite strumenti di Automated Machine Learning (AutoML).\nIn particolare, gli scienziati dei dati possono sfruttare i motori di ottimizzazione degli iperparametri di Vertex AI fornendo un set di dati etichettato e scegliendo un tipo di modello come un classificatore di reti neurali o Random Forest. Vertex avvia un job di ‚ÄúHyperparameter Search‚Äù in modo trasparente sul backend, gestendo completamente il provisioning delle risorse, l‚Äôaddestramento del modello, il monitoraggio delle metriche e l‚Äôanalisi dei risultati automaticamente utilizzando algoritmi di ottimizzazione avanzati.\nInternamente, Vertex AutoML impiega varie strategie di ricerca per esplorare in modo intelligente le configurazioni di iperparametri pi√π promettenti in base ai risultati delle valutazioni precedenti. Tra queste, l‚Äôottimizzazione bayesiana √® offerta in quanto fornisce un‚Äôefficienza di campionamento superiore, richiedendo meno iterazioni di training per ottenere una qualit√† del modello ottimizzata rispetto ai metodi standard di Grid Search o di Random Search. Per spazi di ricerca di architettura neurale pi√π complessi, Vertex AutoML utilizza il Population-Based Training, che addestra simultaneamente pi√π modelli e regola dinamicamente i loro iperparametri sfruttando le prestazioni di altri modelli nella popolazione, analogamente ai principi di selezione naturale.\nVertex AI democratizza le tecniche di ricerca di iperparametri all‚Äôavanguardia su scala cloud per tutti gli sviluppatori ML, astraendo la complessit√† di esecuzione e di orchestrazione sottostante. Gli utenti si concentrano esclusivamente sul loro set di dati, sui requisiti del modello e sugli obiettivi di accuratezza, mentre Vertex gestisce il ciclo di ottimizzazione, l‚Äôallocazione delle risorse, il training del modello, il monitoraggio dell‚Äôaccuratezza e l‚Äôarchiviazione degli artefatti internamente. Il risultato √® ottenere modelli ML ottimizzati e pronti per la distribuzione pi√π velocemente per il problema target.\n\n\nTinyML\nEdge Impulse‚Äôs Efficient On-device Neural Network Tuner (EON Tuner) √® uno strumento di ottimizzazione automatizzata degli iperparametri progettato per sviluppare modelli di apprendimento automatico per microcontrollori. Semplifica il processo di sviluppo del modello trovando automaticamente la migliore configurazione di rete neurale per un‚Äôimplementazione efficiente e accurata su dispositivi con risorse limitate.\nLa funzionalit√† chiave di EON Tuner √® la seguente. Innanzitutto, gli sviluppatori definiscono gli iperparametri del modello, come numero di layer, nodi per layer, funzioni di attivazione e pianificazione della velocit√† di ‚Äúannealing‚Äù [https://it.wikipedia.org/wiki/Ricottura_simulata] dell‚Äôapprendimento. Questi parametri costituiscono lo spazio di ricerca che verr√† ottimizzato. Successivamente, viene selezionata la piattaforma del microcontrollore target, fornendo vincoli hardware embedded. L‚Äôutente pu√≤ anche specificare obiettivi di ottimizzazione, come la riduzione dell‚Äôingombro di memoria, la riduzione della latenza, la riduzione del consumo energetico o la massimizzazione della precisione.\nCon lo spazio di ricerca definito e gli obiettivi di ottimizzazione, EON Tuner sfrutta l‚Äôottimizzazione degli iperparametri bayesiani per esplorare in modo intelligente possibili configurazioni. Ogni configurazione potenziale viene automaticamente implementata come specifica di modello completa, addestrata e valutata per metriche di qualit√†. Il processo continuo bilancia esplorazione e sfruttamento per arrivare a impostazioni ottimizzate su misura per l‚Äôarchitettura del chip scelta dallo sviluppatore e i requisiti di prestazioni.\nEON Tuner libera gli esperti di machine learning dal processo iterativo esigente di messa a punto manuale dei modelli, regolando automaticamente i modelli per il deployment embedded. Lo strumento si integra perfettamente nel flusso di lavoro Edge Impulse, portando i modelli dal concetto a implementazioni ottimizzate in modo efficiente sui microcontrollori. L‚Äôesperienza racchiusa in EON Tuner per quanto riguarda l‚Äôottimizzazione del modello ML per i microcontrollori garantisce che sia gli sviluppatori principianti che quelli esperti possano rapidamente iterare per ottenere modelli adatti alle esigenze del loro progetto.\n\n\n\n\n\n\nEsercizio¬†7.2: Ottimizzazione degli Iperparametri\n\n\n\n\n\nPrepariamoci a scoprire i segreti della messa a punto degli iperparametri e portiamo i modelli PyTorch al livello successivo! Gli iperparametri sono come i quadranti e le manopole nascosti che controllano i superpoteri di apprendimento del modello. In questo notebook Colab, si collaborer√† con Ray Tune per trovare le combinazioni perfette di iperparametri. Scopriamo come definire quali valori cercare, impostare il codice di training per l‚Äôottimizzazione e lasciare che Ray Tune faccia il grosso del lavoro. Alla fine, si diventer√† professionisti della messa a punto degli iperparametri!\n\n\n\n\nVideo¬†7.3 spiega l‚Äôorganizzazione sistematica del processo di ottimizzazione degli iperparametri.\n\n\n\n\n\n\nVideo¬†7.3: Iperparametro",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Addestramento dell'IA</span>"
    ]
  },
  {
    "objectID": "contents/core/training/training.it.html#regolarizzazione",
    "href": "contents/core/training/training.it.html#regolarizzazione",
    "title": "7¬† Addestramento dell‚ÄôIA",
    "section": "7.7 Regolarizzazione",
    "text": "7.7 Regolarizzazione\nLa regolarizzazione √® una tecnica critica per migliorare le prestazioni e la generalizzabilit√† dei modelli di machine learning in impostazioni applicate. Si riferisce alla limitazione matematica o alla penalizzazione della complessit√† del modello per evitare il sovra-adattamento dei dati di training. Senza regolarizzazione, i modelli ML complessi sono inclini al sovra-adattamento del set di dati e alla memorizzazione di peculiarit√† e rumore nel set di training anzich√© all‚Äôapprendimento di pattern significativi. Possono raggiungere un‚Äôelevata accuratezza di training ma hanno prestazioni scadenti quando valutano nuovi input non ancora visti.\nLa regolarizzazione aiuta ad affrontare questo problema ponendo vincoli che favoriscono modelli pi√π semplici e pi√π generalizzabili che non si agganciano a errori di campionamento. Tecniche come la regolarizzazione L1/L2 penalizzano direttamente valori di parametri elevati durante il training, costringendo il modello a utilizzare i parametri pi√π piccoli che possono spiegare adeguatamente il segnale. Le regole di arresto anticipato interrompono il training quando le prestazioni del set di validazione smettono di migliorare, prima che il modello inizi a sovra-adattarsi.\nUna regolarizzazione appropriata √® fondamentale quando si distribuiscono modelli a nuove popolazioni di utenti e ambienti in cui sono probabili cambiamenti di distribuzione. Ad esempio, un modello irregolare di rilevamento delle frodi addestrato presso una banca potrebbe funzionare inizialmente, ma accumulare debiti tecnici nel tempo man mano che emergono nuovi pattern di frode.\nLa regolarizzazione di reti neurali complesse offre anche vantaggi computazionali: modelli pi√π piccoli richiedono meno ‚Äúdata augmentation‚Äù, potenza di calcolo e archiviazione dei dati. La regolarizzazione consente anche sistemi di intelligenza artificiale pi√π efficienti, in cui accuratezza, robustezza e gestione delle risorse sono attentamente bilanciate rispetto alle limitazioni del set di addestramento.\nDiverse potenti tecniche di regolarizzazione sono comunemente utilizzate per migliorare la generalizzazione del modello. L‚Äôarchitettura della strategia ottimale richiede la comprensione di come ogni metodo influisce sull‚Äôapprendimento e sulla complessit√† del modello.\n\n7.7.1 L1 e L2\nDue delle forme di regolarizzazione pi√π ampiamente utilizzate sono la regolarizzazione L1 e la L2. Entrambe penalizzano la complessit√† del modello aggiungendo un termine extra alla funzione di costo ottimizzata durante l‚Äôaddestramento. Questo termine cresce all‚Äôaumentare dei parametri del modello.\nLa regolarizzazione L2, nota anche come ‚Äúridge regression‚Äù [https://it.wikipedia.org/wiki/Regolarizzazione_di_Tichonov], aggiunge la somma delle grandezze al quadrato di tutti i parametri moltiplicata per un coefficiente Œ±. Questa penalit√† quadratica riduce i valori dei parametri estremi in modo pi√π aggressivo rispetto alle tecniche L1. L‚Äôimplementazione richiede solo la modifica della funzione di costo e la messa a punto di Œ±.\n\\[R_{L2}(\\Theta) = \\alpha \\sum_{i=1}^{n}\\theta_{i}^2\\]\nDove:\n\n\\(R_{L2}(\\Theta)\\) - Il termine di regolarizzazione L2 che viene aggiunto alla funzione di costo\n\\(\\alpha\\) - L‚Äôiperparametro di regolarizzazione L2 che controlla la forza della regolarizzazione\n\\(\\theta_{i}\\) - L‚Äôi-esimo parametro del modello\n\\(n\\) - Il numero di parametri nel modello\n\\(\\theta_{i}^2\\) - Il quadrato di ciascun parametro\n\nE la funzione di costo regolarizzata L2 completa √®:\n\\[J(\\theta) = L(\\theta) + R_{L2}(\\Theta)\\]\nDove:\n\n\\(L(\\theta)\\) - La funzione di costo non regolarizzata originale\n\\(J(\\theta)\\) - La nuova funzione di costo regolarizzata\n\nSia la regolarizzazione L1 che L2 penalizzano i pesi elevati nella rete neurale. Tuttavia, la differenza fondamentale tra la regolarizzazione L1 e L2 √® che la regolarizzazione L2 penalizza i quadrati dei parametri anzich√© i valori assoluti. Questa differenza fondamentale ha un impatto considerevole sui pesi regolarizzati risultanti. La regolarizzazione L1, o regressione LASSO [https://it.wikipedia.org/wiki/Regolarizzazione_(matematica)], utilizza la somma assoluta delle grandezze anzich√© il quadrato moltiplicato per Œ±. La penalizzazione del valore assoluto dei pesi induce scarsit√† poich√© il gradiente degli errori estrapola linearmente mentre i termini dei pesi tendono a zero; questo √® diverso dalla penalizzazione del valore al quadrato dei pesi, dove la penalit√† si riduce man mano che i pesi tendono a 0. Inducendo scarsit√† nel vettore dei parametri, la regolarizzazione L1 esegue automaticamente la selezione delle feature, impostando i pesi delle feature irrilevanti a zero. A differenza della regolarizzazione L2, la L1 porta alla scarsit√† poich√© i pesi sono impostati su 0; nella regolarizzazione L2, i pesi sono impostati su un valore molto vicino a 0 ma generalmente non raggiungono mai esattamente 0. La regolarizzazione L1 incoraggia la scarsit√† ed √® stata utilizzata in alcuni lavori per addestrare reti sparse che potrebbero essere pi√π efficienti in termini di hardware (Hoefler et al. 2021).\n\nHoefler, Torsten, Dan Alistarh, Tal Ben-Nun, Nikoli Dryden, e Alexandra Peste. 2021. ¬´Sparsity in Deep Learning: Pruning and growth for efficient inference and training in neural networks¬ª, gennaio. http://arxiv.org/abs/2102.00554v1.\n\\[R_{L1}(\\Theta) = \\alpha \\sum_{i=1}^{n}||\\theta_{i}||\\]\nDove:\n\n\\(R_{L1}(\\Theta)\\) - Il termine di regolarizzazione L1 che viene aggiunto alla funzione di costo\n\\(\\alpha\\) - L‚Äôiperparametro di regolarizzazione L1 che controlla la forza della regolarizzazione\n\\(\\theta_{i}\\) - L‚Äôi-esimo parametro del modello\n\\(n\\) - Il numero di parametri nel modello\n\\(||\\theta_{i}||\\) - La norma L1, che assume il valore assoluto di ciascun parametro\n\nE la funzione di costo regolarizzata L1 completa √®:\n\\[J(\\theta) = L(\\theta) + R_{L1}(\\Theta)\\]\nDove:\n\n\\(L(\\theta)\\) - La funzione di costo non regolarizzata originale\n\\(J(\\theta)\\) - La nuova funzione di costo regolarizzata\n\nLa scelta tra L1 e L2 dipende dalla complessit√† del modello prevista e dalla necessit√† o meno di una selezione di feature intrinseche. Entrambi richiedono una messa a punto iterativa su un set di validazione per selezionare l‚Äôiperparametro Œ± ottimale.\nVideo¬†7.4 e Video¬†7.5 spiegano come funziona la regolarizzazione.\n\n\n\n\n\n\nVideo¬†7.4: Regolarizzazione\n\n\n\n\n\n\nVideo¬†7.5 spiega come la regolarizzazione pu√≤ aiutare a ridurre l‚Äôoverfitting del modello per migliorare le prestazioni.\n\n\n\n\n\n\nVideo¬†7.5: Perch√© la Regolarizzazione Riduce l‚ÄôOverfitting\n\n\n\n\n\n\n\n\n7.7.2 Dropout\nUn altro metodo di regolarizzazione ampiamente adottato √® ‚Äúdropout‚Äù (Srivastava et al. 2014). Durante l‚Äôaddestramento, dropout imposta casualmente una frazione \\(p\\) di output del nodo o attivazioni nascoste a zero. Questo incoraggia una maggiore distribuzione delle informazioni su pi√π nodi anzich√© affidarsi a un piccolo numero di nodi. Al momento della previsione, viene utilizzata l‚Äôintera rete neurale, con attivazioni intermedie scalate di \\(1 - p\\) per mantenere le ampiezze di output. Le ottimizzazioni GPU semplificano l‚Äôimplementazione efficiente di dropout tramite framework come PyTorch e TensorFlow.\n\nSrivastava, Nitish, Geoffrey E. Hinton, Alex Krizhevsky, Ilya Sutskever, e Ruslan Salakhutdinov. 2014. ¬´Dropout: a simple way to prevent neural networks from overfitting.¬ª J. Mach. Learn. Res. 15 (1): 1929‚Äì58. https://doi.org/10.5555/2627435.2670313.\nSiamo pi√π precisi. Durante l‚Äôaddestramento con dropout, l‚Äôoutput di ogni nodo \\(a_i\\) viene passato attraverso una maschera di dropout \\(r_i\\) prima di essere utilizzato dal layer successivo:\n\\[ √£_i = r_i \\odot a_i \\]\nDove:\n\n\\(a_i\\) - output del nodo \\(i\\)\n\\(√£_i\\) - output del nodo \\(i\\) dopo il dropout\n\\(r_i\\) - variabile casuale di Bernoulli indipendente con probabilit√† \\(1 - p\\) di essere 1\n\\(\\odot\\) - moltiplicazione elemento per elemento\n\nPer capire come funziona il dropout, √® importante sapere che la maschera di dropout \\(r_i\\) √® basata sulle variabili casuali di Bernoulli. Una variabile casuale di Bernoulli assume un valore di 1 con probabilit√† \\(1-p\\) (mantenendo l‚Äôattivazione) e un valore di 0 con probabilit√† \\(p\\) (dropping [perdendo] l‚Äôattivazione). Ci√≤ significa che l‚Äôattivazione di ciascun nodo viene mantenuta o eliminata indipendentemente durante l‚Äôaddestramento. Questa maschera di dropout \\(r_i\\) imposta casualmente una frazione \\(p\\) di attivazioni a 0 durante l‚Äôaddestramento, costringendo la rete a creare rappresentazioni ridondanti.\nAl momento del test, la maschera di dropout viene rimossa e le attivazioni vengono ridimensionate di \\(1 - p\\) per mantenere le ampiezze di output previste:\n\\[ a_i^{test} = (1 - p)  a_i\\]\nDove:\n\n\\(a_i^{test}\\) - output del nodo al momento del test\n\\(p\\) - la probabilit√† di effettuare il dropping [eliminare] di un nodo.\n\nL‚Äôiperparametro chiave √® \\(p\\), la probabilit√† di eliminare ogni nodo, spesso impostata tra 0.2 e 0.5. Le reti pi√π grandi tendono a trarre vantaggio da un dropout maggiore, mentre le reti pi√π piccole rischiano di non adattarsi se vengono eliminati troppi nodi. Tentativi ed errori combinati con il monitoraggio delle prestazioni di validazione aiutano a regolare il livello di dropout.\nVideo¬†7.6 discute l‚Äôintuizione alla base della tecnica di regolarizzazione del dropout e il suo funzionamento.\n\n\n\n\n\n\nVideo¬†7.6: Dropout\n\n\n\n\n\n\n\n\n7.7.3 Arresto Anticipato\nL‚Äôintuizione alla base di ‚Äúearly stopping‚Äù arresto anticipato implica il monitoraggio delle prestazioni del modello su un set di validazione ‚Äúheld-out‚Äù [esterno] in epoche di addestramento. Inizialmente, gli aumenti nell‚Äôidoneit√† del set di addestramento accompagnano i guadagni nell‚Äôaccuratezza della validazione man mano che il modello rileva pattern generalizzabili. Dopo un certo punto, tuttavia, il modello inizia a sovradimensionarsi, agganciandosi a peculiarit√† e rumore nei dati di addestramento che non si applicano pi√π in generale. Le prestazioni di validazione raggiungono il picco e poi si degradano se l‚Äôaddestramento continua. Le regole di ‚Äúarresto anticipato‚Äù interrompono l‚Äôaddestramento a questo picco per evitare il sovradimensionamento. Questa tecnica dimostra come le pipeline ML debbano monitorare il feedback del sistema, non solo massimizzare incondizionatamente le prestazioni su un set di addestramento statico. Lo stato del sistema evolve e gli endpoint ottimali cambiano.\nPertanto, i metodi formali di arresto anticipato richiedono il monitoraggio di una metrica come l‚Äôaccuratezza o la perdita di validazione dopo ogni epoca. Le curve comuni mostrano rapidi guadagni iniziali che si riducono gradualmente, alla fine raggiungendo un plateau e diminuiscono leggermente man mano che si verifica il sovradimensionamento. Il punto di arresto ottimale √® spesso compreso tra 5 e 15 epoche oltre il picco, a seconda dei ‚Äúpatient threshold‚Äù [limiti della pazienza!]. Il monitoraggio di pi√π metriche pu√≤ migliorare il segnale poich√© esiste una varianza tra le misure.\nLe semplici regole di arresto anticipato si interrompono immediatamente alla prima degradazione post-picco. Metodi pi√π robusti introducono un parametro di ‚Äúpazienza‚Äù, ovvero il numero di epoche di degradazione consentite prima dell‚Äôarresto. Ci√≤ evita di interrompere prematuramente l‚Äôaddestramento a causa di fluttuazioni transitorie. Le finestre di ‚Äúpazienza‚Äù tipiche vanno da 50 a 200 batch di validazione. Finestre pi√π ampie comportano il rischio di overfitting. Le strategie di ottimizzazione formali possono determinare la ‚Äúpazienza‚Äù ottimale.\n\n\n\n\n\n\nEsercizio¬†7.3: Regolarizzazione\n\n\n\n\n\nCombattere l‚ÄôOverfitting: Scoprire i Segreti della Regolarizzazione! L‚Äôoverfitting √® come se il modello memorizzasse le risposte a un test, per poi fallire l‚Äôesame reale. Le tecniche di regolarizzazione sono le guide di studio che aiutano il modello a generalizzare e ad affrontare nuovi problemi. In questo notebook Colab, impareremo come ottimizzare i parametri di regolarizzazione per risultati ottimali utilizzando la regolarizzazione L1 e L2, il dropout e l‚Äôarresto anticipato.\n\n\n\n\nVideo¬†7.7 tratta alcuni altri metodi di regolarizzazione che possono ridurre l‚Äôoverfitting del modello.\n\n\n\n\n\n\nVideo¬†7.7: Altri Metodi di Regolarizzazione",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Addestramento dell'IA</span>"
    ]
  },
  {
    "objectID": "contents/core/training/training.it.html#funzioni-di-attivazione",
    "href": "contents/core/training/training.it.html#funzioni-di-attivazione",
    "title": "7¬† Addestramento dell‚ÄôIA",
    "section": "7.8 Funzioni di Attivazione",
    "text": "7.8 Funzioni di Attivazione\nLe funzioni di attivazione svolgono un ruolo cruciale nelle reti neurali. Introducono comportamenti non lineari che consentono alle reti neurali di modellare pattern complessi. Le funzioni di attivazione elemento per elemento vengono applicate alle somme ponderate che arrivano a ciascun neurone nella rete. Senza funzioni di attivazione, le reti neurali sarebbero ridotte a modelli di regressione lineare.\nIdealmente, le funzioni di attivazione possiedono alcune qualit√† desiderabili:\n\nNon lineari: Consentono di modellare relazioni complesse tramite trasformazioni non lineari della somma degli input.\nDifferenziabili: Devono avere derivate prime ben definite per abilitare la retropropagazione e l‚Äôottimizzazione basata sul gradiente durante l‚Äôaddestramento.\nLimitazione dell‚ÄôIntervallo: Limitano il segnale di output, impedendo un‚Äôesplosione. Ad esempio, la sigmoide schiaccia gli input a (0,1).\n\nInoltre, propriet√† come efficienza computazionale, monotonicit√† e fluidit√† rendono alcune attivazioni pi√π adatte di altre in base all‚Äôarchitettura di rete e alla complessit√† del problema.\nEsamineremo brevemente alcune delle funzioni di attivazione pi√π ampiamente adottate e i loro punti di forza e limiti. Forniremo anche linee guida per la selezione di funzioni appropriate abbinate ai vincoli del sistema ML e alle esigenze dei casi d‚Äôuso.\n\n7.8.1 Sigmoide\nL‚Äôattivazione sigmoide applica una curva a forma di S schiacciante che lega strettamente l‚Äôoutput tra 0 e 1. Ha la forma matematica:\n\\[ sigmoid(x) = \\frac{1}{1+e^{-x}} \\]\nLa trasformazione esponenziale consente alla funzione di passare gradualmente da quasi 0 a quasi 1 quando l‚Äôinput passa da molto negativo a molto positivo. L‚Äôaumento monotono copre l‚Äôintero intervallo (0,1).\nLa funzione sigmoide presenta diversi vantaggi. Fornisce sempre un gradiente uniforme per la retropropagazione e il suo output √® limitato tra 0 e 1, il che aiuta a prevenire valori ‚Äúesplosivi‚Äù durante l‚Äôaddestramento. Inoltre, ha una semplice formula matematica che √® facile da calcolare.\nTuttavia, la funzione sigmoide presenta anche alcuni svantaggi. Tende a saturarsi a valori di input estremi, il che pu√≤ causare la ‚Äúscomparsa‚Äù dei gradienti, rallentando o addirittura interrompendo il processo di apprendimento. Inoltre, la funzione non √® centrata sullo zero, il che significa che i suoi output non sono distribuiti simmetricamente attorno allo zero, il che pu√≤ portare ad aggiornamenti inefficienti durante l‚Äôaddestramento.\n\n\n7.8.2 Tanh\nAnche Tanh o ‚Äútangente iperbolica‚Äù assume una forma a S ma √® centrata sullo zero, il che significa che il valore medio dell‚Äôoutput √® 0.\n\\[ tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}} \\]\nLa trasformazione numeratore/denominatore sposta l‚Äôintervallo da (0,1) in Sigmoide a (-1, 1) in tanh.\nLa maggior parte dei pro/contro sono condivisi con la Sigmoide, ma Tanh evita alcuni problemi di saturazione dell‚Äôoutput essendo centrata. Tuttavia, soffre ancora di gradienti che svaniscono con molti layer.\n\n\n7.8.3 ReLU\nLa Rectified Linear Unit (ReLU) introduce un semplice comportamento di soglia con la sua forma matematica:\n\\[ ReLU(x) = max(0, x) \\]\nLascia tutti gli input positivi invariati mentre taglia tutti i valori negativi a 0. Questa attivazione sparsa e il calcolo economico rendono ReLU ampiamente favorito rispetto a sigmoide/tanh.\nFigura¬†7.6 dimostra le 3 funzioni di attivazione di cui abbiamo discusso sopra in confronto a una funzione lineare:\n\n\n\n\n\n\nFigura¬†7.6: Funzioni di Attivazione Comuni. Fonte: AI Wiki.\n\n\n\n\n\n7.8.4 Softmax\nLa funzione di attivazione softmax √® generalmente utilizzata come ultimo layer per le attivit√† di classificazione per normalizzare il vettore del valore di attivazione in modo che i suoi elementi sommino a 1. Questo √® utile per le attivit√† di classificazione in cui vogliamo imparare a prevedere probabilit√† specifiche per classe di un input particolare, nel qual caso la probabilit√† cumulativa tra le classi √® uguale a 1. La funzione di attivazione softmax √® definita come\n\\[\\sigma(z_i) = \\frac{e^{z_{i}}}{\\sum_{j=1}^K e^{z_{j}}} \\ \\ \\ for\\ i=1,2,\\dots,K\\]\n\n\n7.8.5 Pro e Contro\nTabella¬†7.4 sono i pro e i contro riassuntivi di queste varie funzioni di attivazione standard:\n\n\n\nTabella¬†7.4: Confronto dei pro e dei contro di diversi algoritmi di ottimizzazione.\n\n\n\n\n\n\n\n\n\n\nAttivazione\nPro\nContro\n\n\n\n\nSigmoide\n\nGradiente uniforme per il backdrop [sfondo]\nOutput limitato tra 0 e 1\n\n\nLa saturazione elimina i gradienti\nNon centrato sullo zero\n\n\n\nTanh\n\nGradiente pi√π uniforme della sigmoide\nOutput centrato sullo zero [-1, 1]\n\n\nSoffre ancora di problemi di gradiente evanescente\n\n\n\nReLU\n\nEfficiente dal punto di vista computazionale\nIntroduce la ‚Äúsparsity‚Äù [scarsit√†]\nEvita gradienti evanescenti\n\n\nUnit√† ‚ÄúReLU morenti‚Äù\nNon limitato\n\n\n\nSoftmax\n\nUtilizzato per l‚Äôultimo livello per normalizzare gli output in modo che siano una distribuzione\nIn genere utilizzato per attivit√† di classificazione\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEsercizio¬†7.4: Funzioni di Attivazione\n\n\n\n\n\nSblocchiamo la potenza delle funzioni di attivazione! Questi piccoli ‚Äúmuletti‚Äù matematici sono ci√≤ che rende le reti neurali cos√¨ incredibilmente flessibili. In questo notebook Colab, ci si cimenter√† con funzioni come Sigmoid, tanh e la superstar ReLU. Guardiamo come trasformano gli input e scopriamo quale funziona meglio in diverse situazioni. √à la chiave per costruire reti neurali in grado di affrontare problemi complessi!",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Addestramento dell'IA</span>"
    ]
  },
  {
    "objectID": "contents/core/training/training.it.html#inizializzazione-dei-pesi",
    "href": "contents/core/training/training.it.html#inizializzazione-dei-pesi",
    "title": "7¬† Addestramento dell‚ÄôIA",
    "section": "7.9 Inizializzazione dei Pesi",
    "text": "7.9 Inizializzazione dei Pesi\nLa corretta inizializzazione dei pesi in una rete neurale prima dell‚Äôaddestramento √® un passaggio fondamentale che ha un impatto diretto sulle prestazioni del modello. L‚Äôinizializzazione casuale dei pesi a valori molto grandi o molto piccoli pu√≤ portare a problemi come gradienti che svaniscono/esplodono, convergenza lenta dell‚Äôaddestramento o intrappolati in minimi locali scadenti. La corretta inizializzazione del peso accelera la convergenza del modello durante l‚Äôaddestramento e comporta implicazioni per le prestazioni del sistema al momento dell‚Äôinferenza negli ambienti di produzione. Alcuni aspetti chiave sono:\n\nTempo di Accuratezza pi√π Rapido: Un‚Äôinizializzazione attentamente calibrata porta a una convergenza pi√π rapida, che si traduce nel raggiungimento da parte dei modelli di traguardi di accuratezza target in anticipo nel ciclo di training. Ad esempio, l‚Äôinizializzazione Xavier potrebbe ridurre il tempo di accuratezza del 20% rispetto a un‚Äôinizializzazione casuale errata. Poich√© l‚Äôaddestramento √® in genere la fase pi√π dispendiosa in termini di tempo e calcolo, ci√≤ migliora direttamente la velocit√† e la produttivit√† del sistema ML.\nEfficienza del Ciclo di Iterazione del Modello: Se i modelli vengono addestrati pi√π rapidamente, il tempo di risposta complessivo per le iterazioni di sperimentazione, valutazione e progettazione del modello diminuisce in modo significativo. I sistemi hanno maggiore flessibilit√† per esplorare architetture, pipeline di dati, ecc., entro determinati intervalli di tempo.\nImpatto sulle Epoche di Addestramento Necessarie: Il processo di addestramento viene eseguito per pi√π epoche, con ogni passaggio completo attraverso i dati che rappresenta un‚Äôepoca. Una buona inizializzazione pu√≤ ridurre le epoche necessarie per far convergere le curve di perdita e accuratezza sul set di addestramento del 10-30%. Ci√≤ significa risparmi tangibili sui costi di risorse e infrastruttura.\nEffetto sugli Iperparametri di Addestramento: I parametri di inizializzazione del peso interagiscono fortemente con determinati iperparametri di regolarizzazione che governano le dinamiche di addestramento, come i programmi di velocit√† di apprendimento e le probabilit√† di abbandono. Trovare la giusta combinazione di impostazioni non √® banale. Un‚Äôinizializzazione appropriata semplifica questa ricerca.\n\nL‚Äôinizializzazione dei pesi ha vantaggi a cascata per l‚Äôefficienza ingegneristica dell‚Äôapprendimento automatico e un overhead di risorse di sistema ridotto al minimo. √à una tattica facilmente trascurata che ogni professionista dovrebbe padroneggiare. La scelta di quale tecnica di inizializzazione del peso utilizzare dipende da fattori come l‚Äôarchitettura del modello (numero di layer, pattern di connettivit√†, ecc.), le funzioni di attivazione e il problema specifico da risolvere. Nel corso degli anni, i ricercatori hanno sviluppato e verificato empiricamente diverse strategie di inizializzazione mirate alle comuni architetture di reti neurali, di cui parleremo qui.\n\n7.9.1 Inizializzazione Uniforme e Normale\nQuando si inizializzano pesi in modo casuale, vengono comunemente utilizzate due distribuzioni di probabilit√† standard: uniforme e Gaussiana (normale). La distribuzione uniforme imposta una probabilit√† uguale che i parametri di peso iniziali rientrino in qualsiasi punto entro i limiti minimi e massimi impostati. Ad esempio, i limiti potrebbero essere -1 e 1, portando a una distribuzione uniforme dei pesi tra questi limiti. La distribuzione gaussiana, d‚Äôaltra parte, concentra la probabilit√† attorno a un valore medio, seguendo la forma di una curva a campana. La maggior parte dei valori di peso si raggrupper√† nella regione della media specificata, con meno campioni verso le estremit√†. Il parametro di deviazione standard controlla la distribuzione attorno alla media.\nLa scelta tra inizializzazione uniforme o normale dipende dall‚Äôarchitettura di rete e dalle funzioni di attivazione. Per reti poco profonde, si consiglia una distribuzione normale con una deviazione standard relativamente piccola (ad esempio, 0.01). La curva a campana impedisce valori di peso elevati che potrebbero innescare l‚Äôinstabilit√† di addestramento in reti piccole. Per reti pi√π profonde, una distribuzione normale con deviazione standard pi√π elevata (diciamo 0.5 o superiore) o una distribuzione uniforme pu√≤ essere preferita per tenere conto dei problemi di gradiente evanescente su molti layer. La maggiore diffusione determina una maggiore differenziazione tra i comportamenti dei neuroni. La messa a punto dei parametri di distribuzione di inizializzazione √® fondamentale per una convergenza stabile e rapida del modello. Il monitoraggio dei trend di ‚Äúloss‚Äù [perdita] di addestramento pu√≤ diagnosticare i problemi per modificare i parametri in modo iterativo.\n\n\n7.9.2 Inizializzazione Xavier\nProposta da Glorot e Bengio (2010), questa tecnica di inizializzazione √® progettata appositamente per le funzioni di attivazione sigmoide e tanh. Queste attivazioni saturate possono causare gradienti evanescenti o esplosivi durante la retro-propagazione su molti layer.\n\nGlorot, Xavier, e Yoshua Bengio. 2010. ¬´Understanding the difficulty of training deep feedforward neural networks.¬ª In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, 249‚Äì56. http://proceedings.mlr.press/v9/glorot10a.html.\nIl metodo Xavier imposta in modo intelligente la varianza della distribuzione dei pesi in base al numero di input e output per ciascun layer. L‚Äôintuizione √® che questo bilancia il flusso di informazioni e gradienti in tutta la rete. Ad esempio, si consideri un layer con 300 unit√† di input e 100 unit√† di output. Inserendo questo nella formula varianza = 2/(#inputs + #outputs) si ottiene una varianza di 2/(300+100) = 0.01.\nIl campionamento dei pesi iniziali da una distribuzione uniforme o normale centrata su 0 con questa varianza fornisce una convergenza di addestramento molto pi√π fluida per reti sigmoide/tanh profonde. I gradienti sono ben condizionati, impedendo la scomparsa o la crescita esponenziale.\n\n\n7.9.3 Inizializzazione He\nCome proposto da He et al. (2015), questa tecnica di inizializzazione √® adattata alle funzioni di attivazione ReLU (Rectified Linear Unit). Le ReLU introducono il problema del neurone morente in cui le unit√† rimangono bloccate e producono solo 0 se inizialmente ricevono forti input negativi. Ci√≤ rallenta e ostacola l‚Äôaddestramento.\n\nHe, Kaiming, Xiangyu Zhang, Shaoqing Ren, e Jian Sun. 2015. ¬´Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification¬ª. In 2015 IEEE International Conference on Computer Vision (ICCV), 1026‚Äì34. IEEE. https://doi.org/10.1109/iccv.2015.123.\n‚ÄúHe‚Äù supera questo problema campionando i pesi da una distribuzione con un set di varianza basato solo sul numero di input per layer, ignorando gli output. Ci√≤ mantiene i segnali in arrivo sufficientemente piccoli da attivare le ReLU nel loro regime lineare dall‚Äôinizio, evitando unit√† morte. Per un layer con 1024 input, la formula varianza = 2/1024 = 0.002 mantiene la maggior parte dei pesi concentrati strettamente attorno a 0.\nQuesta inizializzazione specializzata consente alle reti ReLU di convergere in modo efficiente fin dall‚Äôinizio. La scelta tra Xavier e He deve corrispondere alla funzione di attivazione della rete prevista.\n\n\n\n\n\n\nEsercizio¬†7.5: Inizializzazione dei Pesi\n\n\n\n\n\nFacciamo partire la rete neurale col piede giusto con l‚Äôinizializzazione dei pesi! Il modo in cui si impostano quei pesi iniziali pu√≤ fare la differenza nell‚Äôaddestramento del modello. Si immagini di accordare gli strumenti di un‚Äôorchestra prima del concerto. In questo notebook Colab, si imparer√† che la giusta strategia di inizializzazione pu√≤ far risparmiare tempo, migliorare le prestazioni del modello e rendere il percorso di deep-learning molto pi√π fluido.\n\n\n\n\nVideo¬†7.8 sottolinea l‚Äôimportanza di selezionare deliberatamente i valori di peso iniziale rispetto a scelte casuali.\n\n\n\n\n\n\nVideo¬†7.8: Inizializzazione dei Pesi",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Addestramento dell'IA</span>"
    ]
  },
  {
    "objectID": "contents/core/training/training.it.html#colli-di-bottiglia-del-sistema",
    "href": "contents/core/training/training.it.html#colli-di-bottiglia-del-sistema",
    "title": "7¬† Addestramento dell‚ÄôIA",
    "section": "7.10 ‚ÄúColli di Bottiglia‚Äù del Sistema",
    "text": "7.10 ‚ÄúColli di Bottiglia‚Äù del Sistema\nCome introdotto in precedenza, le reti neurali comprendono operazioni lineari (moltiplicazioni di matrici) intervallate da funzioni di attivazione non lineari elemento per elemento. La parte computazionalmente pi√π costosa delle reti neurali sono le trasformazioni lineari, in particolare le moltiplicazioni di matrici tra ogni layer. Questi layer lineari mappano le attivazioni dal layer precedente a uno spazio dimensionale superiore che funge da input per la funzione di attivazione del layer successivo.\n\n7.10.1 Complessit√† a Runtime della Moltiplicazione di Matrici\n\nMoltiplicazioni di Layer vs.¬†Attivazioni\nLa maggior parte del calcolo nelle reti neurali deriva dalle moltiplicazioni di matrici tra layer. Si consideri un layer di rete neurale con una dimensione di input di \\(M\\) = 500 e una dimensione di output di \\(N\\) = 1000; la moltiplicazione di matrici richiede \\(O(N \\cdot M) = O(1000 \\cdot 500) = 500,000\\) operazioni di moltiplicazione-accumulazione (MAC) tra quei layer.\nConfronta questo col layer precedente, che aveva \\(M\\) = 300 input, che richiedevano \\(O(500 \\cdot 300) = 150,000\\) operazioni. Man mano che le dimensioni dei layer aumentano, i requisiti computazionali aumentano quadraticamente con la dimensione del layer. I calcoli totali su \\(L\\) layer possono essere espressi come \\(\\sum_{l=1}^{L-1} O\\big(N^{(l)} \\cdot M^{(l-1)}\\big)\\), dove il calcolo richiesto per ogni layer dipende dal prodotto delle dimensioni di input e output delle matrici che vengono moltiplicate.\nOra, confrontando la moltiplicazione della matrice con la funzione di attivazione, che richiede solo \\(O(N) = 1000\\) non linearit√† elemento per elemento per \\(N = 1000\\) output, possiamo vedere le trasformazioni lineari che dominano le attivazioni computazionalmente.\nQueste grandi moltiplicazioni di matrici influiscono sulle scelte hardware, sulla latenza dell‚Äôinferenza e sui vincoli di potenza per le applicazioni di reti neurali nel mondo reale. Ad esempio, un tipico layer DNN potrebbe richiedere 500,000 moltiplicazioni-accumulazioni rispetto a solo 1000 attivazioni non lineari, dimostrando un aumento di 500x nelle operazioni matematiche.\nQuando si addestrano reti neurali, in genere utilizziamo la discesa del gradiente in mini-batch, operando su piccoli batch di dati contemporaneamente. Considerando una dimensione batch di \\(B\\) esempi di addestramento, l‚Äôinput per la moltiplicazione di matrice diventa una matrice \\(M \\times B\\), mentre l‚Äôoutput √® una matrice \\(N \\times B\\).\n\n\nMini-batch\nNell‚Äôaddestramento delle reti neurali, dobbiamo stimare ripetutamente il gradiente della funzione di perdita rispetto ai parametri di rete (ad esempio, pesi e bias). Questo gradiente indica in quale direzione i parametri devono essere aggiornati per ridurre al minimo la perdita. Come introdotto in precedenza, eseguiamo aggiornamenti su un batch di dati a ogni aggiornamento, noto anche come discesa del gradiente stocastico o ‚Äúdiscesa del gradiente mini-batch‚Äù.\nL‚Äôapproccio pi√π semplice √® stimare il gradiente in base a un singolo esempio di addestramento, calcolare l‚Äôaggiornamento dei parametri, riassettare tutto e ripetere per l‚Äôesempio successivo. Tuttavia, ci√≤ comporta aggiornamenti dei parametri molto piccoli e frequenti che possono essere computazionalmente inefficienti e potrebbero dover essere pi√π accurati in termini di convergenza a causa della stocasticit√† dell‚Äôutilizzo di un solo dato per un aggiornamento del modello.\nInvece, la discesa del gradiente in mini-batch bilancia la stabilit√† della convergenza e l‚Äôefficienza computazionale. Invece di calcolare il gradiente su singoli esempi, stimiamo il gradiente in base a piccoli ‚Äúmini-batch‚Äù di dati, solitamente tra 8 e 256 esempi in pratica.\nCi√≤ fornisce una stima del gradiente rumorosa ma coerente che porta a una convergenza pi√π stabile. Inoltre, l‚Äôaggiornamento dei parametri deve essere eseguito solo una volta per mini-batch anzich√© una volta per ogni esempio, riducendo il sovraccarico computazionale.\nRegolando la dimensione del mini-batch, possiamo controllare il compromesso tra la fluidit√† della stima (i batch pi√π grandi sono generalmente migliori) e la frequenza degli aggiornamenti (i batch pi√π piccoli consentono aggiornamenti pi√π frequenti). Le dimensioni del mini-batch sono solitamente potenze di 2, quindi possono sfruttare in modo efficiente il parallelismo tra i core GPU.\nQuindi, il calcolo totale esegue una moltiplicazione di matrici \\(N \\times M\\) per \\(M \\times B\\), producendo \\(O(N \\cdot M \\cdot B)\\) operazioni in virgola mobile. Come esempio numerico, \\(N=1000\\) unit√† nascoste, \\(M=500\\) unit√† di input e una dimensione del batch \\(B=64\\) equivale a 1000 x 500 x 64 = 32 milioni di moltiplicazioni-accumulazioni per iterazione di training!\nAl contrario, le funzioni di attivazione vengono applicate elemento per elemento alla matrice di output \\(N \\times B\\), richiedendo solo \\(O(N \\cdot B)\\) calcoli. Per \\(N=1000\\) e \\(B=64\\), si tratta di sole 64,000 non linearit√†, ovvero 500 volte meno lavoro della moltiplicazione di matrici.\nMan mano che aumentiamo le dimensioni del batch per sfruttare appieno hardware parallelo come le GPU, la discrepanza tra la moltiplicazione di matrici e il costo della funzione di attivazione aumenta ulteriormente. Ci√≤ rivela come l‚Äôottimizzazione delle operazioni di algebra lineare offra enormi guadagni di efficienza.\nPertanto, la moltiplicazione di matrici √® fondamentale nell‚Äôanalisi di dove e come le reti neurali impiegano i calcoli. Ad esempio, le moltiplicazioni di matrici spesso rappresentano oltre il 90% della latenza di inferenza e del tempo di addestramento nelle comuni reti neurali convoluzionali e ricorrenti.\n\n\nOttimizzazione della Moltiplicazione di Matrici\nDiverse tecniche migliorano l‚Äôefficienza delle operazioni generali matrice-matrice densa/sparsa e matrice-vettore per migliorare l‚Äôefficienza complessiva. Alcuni metodi chiave sono:\n\nSfruttamento di librerie matematiche ottimizzate come cuBLAS per l‚Äôaccelerazione GPU\nAbilitazione di formati di precisione inferiore come FP16 o INT8 dove l‚Äôaccuratezza lo consente\nUtilizzo di Tensor Processing Unit con moltiplicazione di matrici in hardware\nCalcoli consapevoli della sparsit√† e formati di archiviazione dati per sfruttare i parametri zero\nApprossimazione delle moltiplicazioni di matrici con algoritmi come le Fast Fourier Transform\nProgettazione dell‚Äôarchitettura del modello per ridurre le larghezze e le attivazioni degli layer\nQuantizzazione, pruning [potatura], distillazione e altre tecniche di compressione\nParallelizzazione del calcolo sull‚Äôhardware disponibile\nRisultati di caching/pre-calcolo ove possibile per ridurre le operazioni ridondanti\n\nLe potenziali tecniche di ottimizzazione sono vaste, data la porzione sproporzionata di tempo che i modelli trascorrono nella matematica di matrici e vettori. Anche i miglioramenti incrementali velocizzano i tempi di esecuzione e riducono il consumo di energia. Trovare nuovi modi per migliorare queste primitive di algebra lineare rimane un‚Äôarea di ricerca attiva allineata con le future esigenze di machine learning. Ne parleremo in dettaglio nei capitoli Ottimizzazioni e Accelerazione IA.\n\n\n\n7.10.2 Calcolo vs.¬†Collo di Bottiglia della Memoria\nA questo punto, la moltiplicazione matrice-matrice √® l‚Äôoperazione matematica fondamentale alla base delle reti neurali. Sia l‚Äôaddestramento che l‚Äôinferenza per le reti neurali utilizzano ampiamente queste operazioni di moltiplicazione di matrici. L‚Äôanalisi mostra che oltre il 90% dei requisiti computazionali nelle reti neurali attuali derivano da moltiplicazioni di matrici. Di conseguenza, le prestazioni della moltiplicazione di matrici hanno un‚Äôenorme influenza sul tempo complessivo di addestramento o inferenza del modello.\n\nAddestramento vs.¬†Inferenza\nMentre l‚Äôaddestramento e l‚Äôinferenza si basano ampiamente sulle prestazioni della moltiplicazione di matrici, i loro profili computazionali precisi differiscono. In particolare, l‚Äôinferenza della rete neurale tende a essere pi√π legata al calcolo rispetto all‚Äôaddestramento per una dimensione di batch equivalente. La differenza fondamentale risiede nel passaggio di backpropagation, che √® richiesto solo durante l‚Äôaddestramento. La backpropagation implica una sequenza di operazioni di moltiplicazione di matrici per calcolare i gradienti rispetto alle attivazioni su ogni layer della rete. Tuttavia, √® fondamentale che qui non sia necessaria alcuna larghezza di banda di memoria aggiuntiva: gli input, gli output e i gradienti vengono letti/scritti dalla cache o dai registri.\nDi conseguenza, l‚Äôaddestramento mostra intensit√† aritmetiche inferiori, con calcoli del gradiente limitati dall‚Äôaccesso alla memoria anzich√© dai FLOP (Floating Point Operations Per Second), una misura delle prestazioni computazionali che indica quanti calcoli in virgola mobile un sistema pu√≤ eseguire al secondo. Al contrario, la propagazione in avanti domina l‚Äôinferenza della rete neurale, che corrisponde a una serie di moltiplicazioni matrice-matrice. Senza una retrospettiva del gradiente che richiede molta memoria, le dimensioni dei batch pi√π grandi spingono facilmente l‚Äôinferenza a essere estremamente limitata dal calcolo. Le elevate intensit√† aritmetiche misurate mostrano questo. I tempi di risposta possono essere critici per alcune applicazioni di inferenza, costringendo il fornitore dell‚Äôapplicazione a utilizzare una dimensione di batch inferiore per soddisfare questi requisiti di tempo di risposta, riducendo cos√¨ l‚Äôefficienza dell‚Äôhardware; quindi, le inferenze potrebbero vedere un utilizzo inferiore dell‚Äôhardware.\nLe implicazioni sono che il provisioning hardware e i compromessi tra larghezza di banda e FLOP differiscono a seconda che un sistema miri al training o all‚Äôinferenza. I server ad alta produttivit√† e bassa latenza per l‚Äôinferenza dovrebbero enfatizzare la potenza di calcolo anzich√© la memoria, mentre i cluster di training richiedono un‚Äôarchitettura pi√π bilanciata.\nTuttavia, la moltiplicazione di matrici mostra un‚Äôinteressante tensione: la larghezza di banda della memoria dell‚Äôhardware sottostante o le capacit√† di throughput aritmetico possono vincolarla. La capacit√† del sistema di recuperare e fornire dati matriciali rispetto alla sua capacit√† di eseguire operazioni di calcolo determina questa direzione.\nQuesto fenomeno ha impatti profondi; l‚Äôhardware deve essere progettato giudiziosamente e devono essere prese in considerazione le ottimizzazioni del software. Ottimizzare e bilanciare il calcolo rispetto alla memoria per alleviare questo collo di bottiglia della moltiplicazione di matrici √® fondamentale per un training un deployment efficienti del modello.\nInfine, la dimensione del batch pu√≤ avere un impatto sui tassi di convergenza durante l‚Äôaddestramento della rete neurale, un‚Äôaltra considerazione importante. Ad esempio, ci sono generalmente rendimenti decrescenti nei benefici della convergenza con dimensioni di batch estremamente grandi (ad esempio, &gt; 16384). Al contrario, dimensioni di batch estremamente grandi possono essere sempre pi√π vantaggiose da una prospettiva di intensit√† hardware/aritmetica; l‚Äôutilizzo di batch cos√¨ grandi potrebbe non tradursi in una convergenza pi√π rapida rispetto al tempo a causa dei loro benefici decrescenti per la convergenza. Questi compromessi fanno parte delle decisioni di progettazione fondamentali per i sistemi per il tipo di ricerca basata sull‚Äôapprendimento automatico.\n\n\nDimensione del Batch\nLa dimensione del batch utilizzata durante l‚Äôaddestramento e l‚Äôinferenza della rete neurale ha un impatto significativo sul fatto che la moltiplicazione di matrici rappresenti un collo di bottiglia computazionale o di memoria. In concreto, la dimensione del batch si riferisce al numero di campioni propagati assieme attraverso la rete in un passaggio avanti/indietro. La moltiplicazione di matrici equivale a dimensioni di matrice maggiori.\nIn particolare, diamo un‚Äôocchiata all‚Äôintensit√† aritmetica della moltiplicazione di matrici durante l‚Äôaddestramento della rete neurale. Questa misura il rapporto tra operazioni computazionali e trasferimenti di memoria. La moltiplicazione di due matrici di dimensione \\(N \\times M\\) e \\(M \\times B\\) richiede \\(N \\times M \\times B\\) operazioni di moltiplicazione-accumulo, ma solo trasferimenti di \\(N \\times M + M \\times B\\) elementi di matrice.\nMan mano che aumentiamo la dimensione del batch \\(B\\), il numero di operazioni aritmetiche cresce pi√π velocemente dei trasferimenti di memoria. Ad esempio, con una dimensione del batch di 1, abbiamo bisogno di \\(N \\times M\\) operazioni e \\(N + M\\) trasferimenti, dando un rapporto di intensit√† aritmetica di circa \\(\\frac{N \\times M}{N+M}\\). Ma con una dimensione del batch di grandi dimensioni di 128, il rapporto di intensit√† diventa \\(\\frac{128 \\times N \\times M}{N \\times M + M \\times 128} \\approx 128\\).\nL‚Äôutilizzo di una dimensione del batch pi√π grande sposta il calcolo complessivo da vincolato alla memoria a pi√π vincolato al calcolo. L‚Äôaddestramento IA utilizza grandi dimensioni del batch ed √® generalmente limitato dalle massime prestazioni di calcolo aritmetiche, ovvero l‚ÄôApplicazione 3 in Figura¬†7.7. Pertanto, la moltiplicazione di matrici in batch √® molto pi√π intensiva dal punto di vista computazionale rispetto al limite di accesso alla memoria. Ci√≤ ha implicazioni per la progettazione hardware e le ottimizzazioni software, che tratteremo in seguito. L‚Äôintuizione chiave √® che possiamo modificare in modo significativo il profilo computazionale e i colli di bottiglia posti dall‚Äôaddestramento e dall‚Äôinferenza della rete neurale regolando la dimensione del batch.\n\n\n\n\n\n\nFigura¬†7.7: Modello a profilo a di tetto per il training di IA.\n\n\n\n\n\nCaratteristiche Hardware\nL‚Äôhardware moderno come CPU e GPU √® altamente ottimizzato per la produttivit√† computazionale piuttosto che per la larghezza di banda della memoria. Ad esempio, le GPU H100 Tensor Core di fascia alta possono fornire oltre 60 TFLOPS di prestazioni a doppia precisione, ma forniscono solo fino a 3 TB/s di larghezza di banda della memoria. Ci√≤ significa che c‚Äô√® uno squilibrio di quasi 20 volte tra unit√† aritmetiche e accesso alla memoria; di conseguenza, per hardware come gli acceleratori GPU, i carichi di lavoro di addestramento della rete neurale devono essere resi il pi√π intensivi possibile dal punto di vista computazionale per utilizzare appieno le risorse disponibili.\nCi√≤ motiva ulteriormente la necessit√† di utilizzare batch di grandi dimensioni durante l‚Äôaddestramento. Quando si utilizza un batch di piccole dimensioni, la moltiplicazione della matrice √® limitata dalla larghezza di banda della memoria, sottoutilizzando le abbondanti risorse di elaborazione. Tuttavia, possiamo spostare il collo di bottiglia verso l‚Äôelaborazione e ottenere un‚Äôintensit√† aritmetica molto pi√π elevata con batch sufficientemente grandi. Ad esempio, potrebbero essere necessari batch di 256 o 512 campioni per saturare una GPU di fascia alta. Lo svantaggio √® che batch pi√π grandi forniscono aggiornamenti dei parametri meno frequenti, il che pu√≤ influire sulla convergenza. Tuttavia, il parametro funge da importante manopola di sintonizzazione per bilanciare le limitazioni di memoria e quelle di elaborazione.\nPertanto, date le architetture di elaborazione-memoria sbilanciate dell‚Äôhardware moderno, l‚Äôimpiego di batch di grandi dimensioni √® essenziale per alleviare i colli di bottiglia e massimizzare la produttivit√†. Come accennato, anche il software e gli algoritmi successivi devono adattarsi a tali dimensioni di batch, poich√© dimensioni di batch pi√π grandi possono avere rendimenti decrescenti verso la convergenza della rete. L‚Äôutilizzo di dimensioni di batch molto piccole pu√≤ portare a un utilizzo non ottimale dell‚Äôhardware, limitando in ultima analisi l‚Äôefficienza del training. L‚Äôaumento di dimensioni dei batch di grandi dimensioni √® un argomento di ricerca esplorato in vari lavori che mirano a eseguire una training su larga scala (You et al. 2017).\n\nYou, Yang, Zhao Zhang, Cho-Jui Hsieh, James Demmel, e Kurt Keutzer. 2017. ¬´ImageNet Training in Minutes¬ª, settembre. http://arxiv.org/abs/1709.05011v10.\n\n\nArchitetture dei Modelli\nL‚Äôarchitettura della rete neurale influisce anche sul fatto che la moltiplicazione di matrici rappresenti un collo di bottiglia computazionale o di memoria maggiore durante l‚Äôesecuzione. I trasformatori e gli MLP sono molto pi√π vincolati al calcolo rispetto alle reti neurali convoluzionali CNN. Ci√≤ deriva dai tipi di operazioni di moltiplicazione di matrici coinvolte in ciascun modello. I trasformatori si basano sull‚Äôauto-attenzione, moltiplicando grandi matrici di attivazione per enormi matrici di parametri per correlare gli elementi. Gli MLP impilano layer completamente connessi, richiedendo anche grandi moltiplicazioni matriciali.\nAl contrario, i layer convoluzionali nelle CNN hanno una finestra scorrevole che riutilizza attivazioni e parametri nell‚Äôinput, il che significa che sono necessarie meno operazioni matriciali univoche. Tuttavia, le convoluzioni richiedono l‚Äôaccesso ripetuto a piccole parti di input e lo spostamento di somme parziali per popolare ciascuna finestra. Sebbene le operazioni aritmetiche nelle convoluzioni siano intense, questo spostamento di dati e la manipolazione del buffer impongono enormi overhead di accesso alla memoria. Le CNN comprendono diverse fasi a strati, quindi gli output intermedi devono materializzarsi frequentemente nella memoria.\nDi conseguenza, l‚Äôaddestramento CNN tende a essere pi√π vincolato alla larghezza di banda della memoria rispetto al limite aritmetico in confronto a Transformers e MLP. Pertanto, il profilo di moltiplicazione della matrice e, a sua volta, il collo di bottiglia posto, varia in modo significativo in base alla scelta del modello. Hardware e sistemi devono essere progettati con un appropriato equilibrio di larghezza di banda di elaborazione-memoria a seconda dell‚Äôimplementazione del modello target. I modelli che si basano maggiormente sull‚Äôattenzione e sui layer MLP richiedono una maggiore produttivit√† aritmetica rispetto alle CNN, il che richiede un‚Äôelevata larghezza di banda della memoria.",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Addestramento dell'IA</span>"
    ]
  },
  {
    "objectID": "contents/core/training/training.it.html#parallelizzazione-del-training",
    "href": "contents/core/training/training.it.html#parallelizzazione-del-training",
    "title": "7¬† Addestramento dell‚ÄôIA",
    "section": "7.11 Parallelizzazione del Training",
    "text": "7.11 Parallelizzazione del Training\nL‚Äôaddestramento delle reti neurali comporta richieste di calcolo e memoria intensive. L‚Äôalgoritmo di backpropagation per il calcolo dei gradienti e l‚Äôaggiornamento dei pesi consiste in ripetute moltiplicazioni di matrici e operazioni aritmetiche sull‚Äôintero set di dati. Ad esempio, un passaggio di backpropagation scala in complessit√† temporale con \\(O(num\\_parameters \\times batch\\_size \\times sequence\\_length)\\).\nI requisiti di calcolo aumentano rapidamente con l‚Äôaumento delle dimensioni del modello in parametri e layer. Inoltre, l‚Äôalgoritmo richiede l‚Äôarchiviazione di output di attivazione e parametri del modello per la fase di backward, che cresce con le dimensioni del modello.\nI modelli pi√π grandi non possono adattarsi e addestrarsi su un singolo dispositivo acceleratore come una GPU e l‚Äôingombro di memoria diventa proibitivo. Pertanto, dobbiamo parallelizzare l‚Äôaddestramento del modello su pi√π dispositivi per fornire elaborazione e memoria sufficienti per addestrare reti neurali all‚Äôavanguardia.\nCome mostrato in Figura¬†7.8, i due approcci principali sono il parallelismo dei dati, che replica il modello su pi√π dispositivi suddividendo i dati di input in batch, e il parallelismo del modello, che suddivide l‚Äôarchitettura del modello stesso su diversi dispositivi. Tramite il training in parallelo, possiamo sfruttare maggiori risorse aggregate di elaborazione e memoria per superare le limitazioni del sistema e accelerare i carichi di lavoro di deep learning.\n\n\n\n\n\n\nFigura¬†7.8: Parallelismo dei dati e parallelismo del modello.\n\n\n\n\n7.11.1 Parallelismo dei Dati\nLa parallelizzazione dei dati √® un approccio comune per parallelizzare il training di apprendimento automatico su pi√π unit√† di elaborazione, come GPU o risorse di elaborazione distribuite. Il set di dati di addestramento √® suddiviso in batch nel parallelismo dei dati e un‚Äôunit√† di elaborazione separata elabora ogni batch. I parametri del modello vengono poi aggiornati in base ai gradienti calcolati dall‚Äôelaborazione di ogni batch. Ecco una descrizione dettagliata della parallelizzazione dei dati per il training ML:\n\nDivisione del Dataset: Il set di dati di addestramento √® suddiviso in batch pi√π piccoli, ciascuno contenente un sottoinsieme degli esempi di training.\nReplica del Modello: Il modello di rete neurale √® replicato su tutte le unit√† di elaborazione e ogni unit√† di elaborazione ha la sua copia.\nCalcolo Parallelo: Ogni unit√† di elaborazione prende un batch diverso e calcola in modo indipendente i passaggi in forward e backward. Durante il passaggio forward [in avanti], il modello fa delle previsioni sui dati di input. La funzione di loss [perdita] determina i gradienti per i parametri del modello durante il passaggio backward [all‚Äôindietro].\nAggregazione dei Gradienti: Dopo l‚Äôelaborazione dei rispettivi batch, i gradienti di ogni unit√† di elaborazione vengono aggregati. I metodi di aggregazione comuni includono la sommatoria o la media dei gradienti.\nAggiornamento dei Parametri: I gradienti aggregati aggiornano i parametri del modello. L‚Äôaggiornamento pu√≤ essere eseguito utilizzando algoritmi di ottimizzazione come SGD o varianti come Adam.\nSincronizzazione: Dopo l‚Äôaggiornamento, tutte le unit√† di elaborazione sincronizzano i parametri del modello, assicurandosi che ciascuna ne abbia la versione pi√π recente.\n\nI passaggi precedenti vengono ripetuti per diverse iterazioni o fino alla convergenza.\nPrendiamo un esempio specifico. Abbiamo 256 dimensioni di batch e 8 GPU; ogni GPU ricever√† un micro-batch di 32 campioni. I loro passaggi forward e backward calcolano perdite e gradienti solo in base ai 32 campioni locali. I gradienti vengono aggregati tra i dispositivi con un server dei parametri o una libreria di comunicazioni collettiva per ottenere il gradiente effettivo per il batch globale. Gli aggiornamenti dei pesi avvengono indipendentemente su ogni GPU in base a questi gradienti. Dopo un numero configurato di iterazioni, i pesi aggiornati si sincronizzano e si equalizzano tra i dispositivi prima di passare alle iterazioni successive.\nIl parallelismo dei dati √® efficace quando il modello √® grande e il set di dati √® sostanziale, poich√© consente l‚Äôelaborazione parallela di diverse parti dei dati. √à ampiamente utilizzato in framework e librerie di deep learning che supportano il training distribuito, come TensorFlow e PyTorch. Tuttavia, per garantire una parallelizzazione efficiente, √® necessario prestare attenzione a gestire problemi come l sovraccarico della comunicazione, bilanciamento del carico e sincronizzazione.\n\n\n7.11.2 Parallelismo del Modello\nIl parallelismo del modello si riferisce alla distribuzione del modello di rete neurale su pi√π dispositivi anzich√© alla replica del modello completo come il parallelismo dei dati. Ci√≤ √® particolarmente utile quando un modello √® troppo grande per essere inserito nella memoria di una singola GPU o di un dispositivo acceleratore. Sebbene ci√≤ potrebbe non essere specificamente applicabile per casi d‚Äôuso embedded o TinyML poich√© la maggior parte dei modelli √® relativamente piccola, √® comunque utile saperlo.\nNell‚Äôaddestramento parallelo del modello, diverse parti o layer del modello vengono assegnati a dispositivi separati. Le attivazioni di input e gli output intermedi vengono partizionati e passati tra questi dispositivi durante i passaggi forward e backward per coordinare i calcoli del gradiente tra le partizioni del modello.\nIl ‚Äúfootprint‚Äù [impronta] di memoria e le operazioni di calcolo vengono distribuite suddividendo l‚Äôarchitettura del modello su pi√π dispositivi anzich√© concentrarsi su uno. Ci√≤ consente l‚Äôaddestramento di modelli molto grandi con miliardi di parametri che altrimenti supererebbero la capacit√† di un singolo dispositivo. Esistono diversi modi in cui possiamo eseguire il partizionamento:\n\nParallelismo di Layer: I layer consecutivi sono distribuiti su dispositivi diversi. Ad esempio, il dispositivo 1 contiene i layer 1-3; il dispositivo 2 contiene i layer 4-6. Le attivazioni di output dal layer 3 verrebbero trasferite al dispositivo 2 per avviare i layer successivi per i calcoli della fase di forward.\nParallelismo a Livello di Filtro: Nei layer convoluzionali, i filtri di output possono essere suddivisi tra pi√π dispositivi. Ogni dispositivo calcola gli output di attivazione per un sottoinsieme di filtri, che vengono concatenati prima di propagarsi ulteriormente.\nParallelismo Spaziale: Le immagini di input vengono divise spazialmente, quindi ogni dispositivo elabora una determinata regione come il quarto in alto a sinistra delle immagini. Le regioni di output si combinano poi per formare l‚Äôoutput completo.\n\nInoltre, le combinazioni ibride possono suddividere il modello a livello di layer e i dati in batch. Il tipo appropriato di parallelismo del modello dipende dai vincoli specifici dell‚Äôarchitettura neurale e dalla configurazione hardware. Ottimizzare il partizionamento e la comunicazione per la topologia del modello √® fondamentale per ridurre al minimo il sovraccarico.\nTuttavia, poich√© le parti del modello vengono eseguite su dispositivi fisicamente separati, devono comunicare e sincronizzare i loro parametri durante ogni fase di addestramento. La fase di backward deve garantire che gli aggiornamenti del gradiente si propaghino accuratamente tra le partizioni del modello. Quindi, il coordinamento e l‚Äôinterconnessione ad alta velocit√† tra i dispositivi sono fondamentali per ottimizzare le prestazioni dell‚Äôaddestramento parallelo. Sono necessari dei buoni protocolli di partizionamento e comunicazione per ridurre al minimo il sovraccarico di trasferimento.\n\n\n7.11.3 Confronto\nRiassumendo, Tabella¬†7.5 illustra alcune delle caratteristiche chiave per confrontare il parallelismo dei dati e quello dei modelli.\n\n\n\nTabella¬†7.5: Confronto tra parallelismo dei dati e parallelismo del modello.\n\n\n\n\n\n\n\n\n\n\nCaratteristica\nParallelismo dei dati\nParallelismo del modello\n\n\n\n\nDefinizione\nDistribuisce i dati tra i dispositivi con repliche\nDistribuisce il modello tra i dispositivi\n\n\nObiettivo\nAccelera il training tramite il ridimensionamento del calcolo\nAbilita un training del modello pi√π ampio\n\n\nMetodo di Ridimensionamento\nDispositivi/workers in scala\nDimensioni modello in scala\n\n\nVincolo Principale\nDimensione del modello per ogni dispositivo\nOverhead di coordinamento dispositivo\n\n\nRequisiti Hardware\nPi√π GPU/TPU\nSpesso interconnessione specializzata\n\n\nDifficolt√† Principale\nSincronizzazione dei parametri\nPartizionamento e comunicazione complicati\n\n\nTipi\nN/D\nPer livello, per filtro, spaziale\n\n\nComplessit√† del Codice\nModifiche minime\nIntervento pi√π significativa sul modello\n\n\nLibrerie Popolari\nHorovod, PyTorch Distributed\nMesh TensorFlow",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Addestramento dell'IA</span>"
    ]
  },
  {
    "objectID": "contents/core/training/training.it.html#conclusione",
    "href": "contents/core/training/training.it.html#conclusione",
    "title": "7¬† Addestramento dell‚ÄôIA",
    "section": "7.12 Conclusione",
    "text": "7.12 Conclusione\nIn questo capitolo abbiamo trattato le basi fondamentali che consentono un training efficace dei modelli di intelligenza artificiale. Abbiamo esplorato concetti matematici come funzioni di perdita, backpropagation e discesa del gradiente che rendono possibile l‚Äôottimizzazione delle reti neurali. Abbiamo anche discusso tecniche pratiche per sfruttare i dati di training, la regolarizzazione, la messa a punto degli iperparametri, l‚Äôinizializzazione dei pesi e strategie di parallelizzazione distribuita che migliorano convergenza, generalizzazione e scalabilit√†.\nQueste metodologie costituiscono il fondamento attraverso cui √® stato raggiunto il successo del deep learning nell‚Äôultimo decennio. Padroneggiare questi fondamenti prepara i professionisti a progettare sistemi e perfezionare modelli su misura per il loro contesto. Tuttavia, man mano che modelli e set di dati crescono in modo esponenziale, i sistemi di training devono ottimizzare parametri come tempo, costo e ‚Äúcarbon footprint‚Äù [impatto ambientale]. Il ridimensionamento hardware tramite grosse warehouse consente un throughput computazionale enorme, ma le ottimizzazioni relative a efficienza e specializzazione saranno fondamentali. Tecniche software come compressione e sfruttamento delle matrici sparse possono aumentare i guadagni hardware. Ne discuteremo diverse nei prossimi capitoli.\nNel complesso, i fondamenti trattati in questo capitolo preparano i professionisti a costruire, perfezionare e distribuire modelli. Tuttavia, le competenze interdisciplinari che abbracciano teoria, sistemi e hardware differenzieranno gli esperti in grado di portare l‚ÄôIA al livello successivo in modo sostenibile e responsabile, come richiesto dalla societ√†. Comprendere l‚Äôefficienza insieme all‚Äôaccuratezza costituisce l‚Äôapproccio ingegneristico bilanciato necessario per addestrare sistemi intelligenti che si integrano senza problemi in molti contesti del mondo reale.",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Addestramento dell'IA</span>"
    ]
  },
  {
    "objectID": "contents/core/training/training.it.html#sec-ai-training-resource",
    "href": "contents/core/training/training.it.html#sec-ai-training-resource",
    "title": "7¬† Addestramento dell‚ÄôIA",
    "section": "7.13 Risorse",
    "text": "7.13 Risorse\nEcco un elenco curato di risorse per supportare studenti e insegnanti nei loro percorsi di apprendimento e insegnamento. Stiamo lavorando costantemente per espandere questa raccolta e presto aggiungeremo nuovi esercizi.\n\n\n\n\n\n\nSlide\n\n\n\n\n\nQueste slide sono uno strumento prezioso per gli insegnanti per tenere lezioni e per gli studenti per rivedere il materiale secondo il proprio ritmo. Incoraggiamo studenti e docenti a sfruttare queste slide per migliorare la loro comprensione e facilitare un trasferimento efficace delle conoscenze.\n\nThinking About Loss.\nMinimizing Loss.\nTraining, Validation, and Test Data.\nContinuous Training:\n\nRetraining Trigger.\nData Processing Overview.\nData Ingestion.\nData Validation.\nData Transformation.\nTraining with AutoML.\nContinuous Training with Transfer Learning.\nContinuous Training Use Case Metrics.\nContinuous Training Impact on MLOps.\n\n\n\n\n\n\n\n\n\n\n\nVideo\n\n\n\n\n\n\nVideo¬†7.1\nVideo¬†7.2\nVideo¬†7.3\nVideo¬†7.4\nVideo¬†7.5\nVideo¬†7.6\nVideo¬†7.7\nVideo¬†7.8\n\n\n\n\n\n\n\n\n\n\nEsercizi\n\n\n\n\n\nPer rafforzare i concetti trattati in questo capitolo, abbiamo curato una serie di esercizi che sfidano gli studenti ad applicare le proprie conoscenze e ad approfondire la propria comprensione.\n\nEsercizio¬†7.1\nEsercizio¬†7.2\nEsercizio¬†7.3\nEsercizio¬†7.5\nEsercizio¬†7.4\n\n\n\n\n\n\n\n\n\n\nLaboratori\n\n\n\n\n\nOltre agli esercizi, offriamo una serie di laboratori pratici che consentono agli studenti di acquisire esperienza pratica con le tecnologie di intelligenza artificiale embedded. Questi laboratori forniscono una guida passo dopo passo, consentendo agli studenti di sviluppare le proprie competenze in un ambiente strutturato e di supporto. Siamo lieti di annunciare che presto saranno disponibili nuovi laboratori, che arricchiranno ulteriormente l‚Äôesperienza di apprendimento.\n\nProssimamente.",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Addestramento dell'IA</span>"
    ]
  },
  {
    "objectID": "contents/core/efficient_ai/efficient_ai.it.html",
    "href": "contents/core/efficient_ai/efficient_ai.it.html",
    "title": "8¬† IA Efficiente",
    "section": "",
    "text": "8.1 Panoramica\nI modelli di training possono consumare molta energia, a volte equivalente all‚Äôimpatto ambientale di processi industriali considerevoli. Tratteremo alcuni di questi dettagli sulla sostenibilit√† nel capitolo Sostenibilit√† dell‚ÄôIA. Dal punto di vista dell‚Äôimplementazione, se questi modelli non sono ottimizzati per l‚Äôefficienza, possono esaurire rapidamente le batterie dei dispositivi, richiedere una memoria eccessiva o non soddisfare le esigenze di elaborazione in tempo reale. In questo capitolo, miriamo a chiarire le sfumature dell‚Äôefficienza, gettando le basi per un‚Äôesplorazione completa nei capitoli successivi.",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>IA Efficiente</span>"
    ]
  },
  {
    "objectID": "contents/core/efficient_ai/efficient_ai.it.html#la-necessit√†-di-unia-efficiente",
    "href": "contents/core/efficient_ai/efficient_ai.it.html#la-necessit√†-di-unia-efficiente",
    "title": "8¬† IA Efficiente",
    "section": "8.2 La Necessit√† di un‚ÄôIA Efficiente",
    "text": "8.2 La Necessit√† di un‚ÄôIA Efficiente\nL‚Äôefficienza assume connotazioni diverse a seconda di dove si verificano i calcoli dell‚ÄôIA. Rivediamo Cloud, Edge e TinyML (come discusso in Sistemi di ML) e distinguiamoli in termini di efficienza. Figura¬†8.1 fornisce un confronto generale delle tre diverse piattaforme.\n\n\n\n\n\n\nFigura¬†8.1: Cloud, Mobile e TinyML. Fonte: Schizas et al. (2022).\n\n\nSchizas, Nikolaos, Aristeidis Karras, Christos Karras, e Spyros Sioutas. 2022. ¬´TinyML for Ultra-Low Power AI and Large Scale IoT Deployments: A Systematic Review¬ª. Future Internet 14 (12): 363. https://doi.org/10.3390/fi14120363.\n\n\nIA Cloud: I modelli IA tradizionali vengono spesso eseguiti in data center su larga scala dotati di potenti GPU e TPU (Barroso, H√∂lzle, e Ranganathan 2019). Qui, l‚Äôefficienza riguarda l‚Äôottimizzazione delle risorse di calcolo, la riduzione dei costi e la garanzia di elaborazione e restituzione tempestive dei dati. Tuttavia, fare affidamento sul cloud introduce latenza, soprattutto quando si ha a che fare con flussi di dati di grandi dimensioni che richiedono caricamento, elaborazione e download.\n\nBarroso, Luiz Andr√©, Urs H√∂lzle, e Parthasarathy Ranganathan. 2019. The Datacenter as a Computer: Designing Warehouse-Scale Machines. Springer International Publishing. https://doi.org/10.1007/978-3-031-01761-2.\n\nLi, En, Liekang Zeng, Zhi Zhou, e Xu Chen. 2020. ¬´Edge AI: On-demand Accelerating Deep Neural Network Inference via Edge Computing¬ª. IEEE Trans. Wireless Commun. 19 (1): 447‚Äì57. https://doi.org/10.1109/twc.2019.2946140.\nIA Edge: L‚Äôedge computing avvicina l‚Äôintelligenza artificiale alla fonte dei dati, elaborando le informazioni direttamente su dispositivi locali come smartphone, fotocamere o macchine industriali (Li et al. 2020). Qui, l‚Äôefficienza comprende risposte rapide in tempo reale e ridotte esigenze di trasmissione dei dati. Tuttavia, i vincoli sono pi√π severi: questi dispositivi, sebbene pi√π potenti dei microcontrollori, hanno una potenza di calcolo limitata rispetto alle configurazioni cloud.\nTinyML: TinyML supera i limiti consentendo ai modelli di intelligenza artificiale di funzionare su microcontrollori o ambienti con risorse estremamente limitate. La differenza di prestazioni del processore e della memoria tra TinyML e i sistemi cloud o mobili pu√≤ essere di diversi ordini di grandezza (Warden e Situnayake 2019). L‚Äôefficienza in TinyML consiste nell‚Äôassicurare che i modelli siano sufficientemente leggeri da adattarsi a questi dispositivi, consumino il minimo di energia (fondamentale per i dispositivi alimentati a batteria) e continuino a svolgere le loro attivit√† in modo efficace.\n\nWarden, Pete, e Daniel Situnayake. 2019. Tinyml: Machine learning with tensorflow lite on arduino and ultra-low-power microcontrollers. O‚ÄôReilly Media.\nLo spettro da Cloud a TinyML rappresenta un passaggio da vaste risorse di elaborazione centralizzate ad ambienti distribuiti, localizzati e limitati. Passando dall‚Äôuno all‚Äôaltro, i problemi e le strategie relative all‚Äôefficienza evolvono, sottolineando la necessit√† di approcci specializzati su misura per ogni scenario. Dopo aver stabilito la necessit√† di un‚Äôintelligenza artificiale efficiente, in particolare nel contesto di TinyML, passeremo all‚Äôesplorazione delle metodologie ideate per rispondere a queste sfide. Le sezioni seguenti delineano i concetti principali che approfondiremo in seguito. Dimostreremo l‚Äôampiezza e la profondit√† dell‚Äôinnovazione necessarie per ottenere un‚Äôintelligenza artificiale efficiente mentre esploriamo queste strategie.",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>IA Efficiente</span>"
    ]
  },
  {
    "objectID": "contents/core/efficient_ai/efficient_ai.it.html#architetture-di-modelli-efficienti",
    "href": "contents/core/efficient_ai/efficient_ai.it.html#architetture-di-modelli-efficienti",
    "title": "8¬† IA Efficiente",
    "section": "8.3 Architetture di Modelli Efficienti",
    "text": "8.3 Architetture di Modelli Efficienti\nSelezionare un‚Äôarchitettura del modello ottimale √® tanto cruciale quanto ottimizzarla. Negli ultimi anni, i ricercatori hanno compiuto passi da gigante nell‚Äôesplorazione di architetture innovative che possono avere intrinsecamente meno parametri pur mantenendo prestazioni elevate.\nMobileNet: MobileNet sono modelli di applicazioni di visione mobile ed embedded efficienti (Howard et al. 2017). L‚Äôidea chiave che ha portato al loro successo sono le convoluzioni separabili in profondit√†, che riducono significativamente il numero di parametri e calcoli nella rete. MobileNetV2 e V3 migliorano ulteriormente questo design introducendo residui invertiti e colli di bottiglia lineari.\n\nHoward, Andrew G., Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, e Hartwig Adam. 2017. ¬´MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications¬ª. ArXiv preprint. https://arxiv.org/abs/1704.04861.\n\nIandola, Forrest N, Song Han, Matthew W Moskewicz, Khalid Ashraf, William J Dally, e Kurt Keutzer. 2016. ¬´SqueezeNet: Alexnet-level accuracy with 50x fewer parameters and 0.5 MB model size¬ª. ArXiv preprint abs/1602.07360. https://arxiv.org/abs/1602.07360.\nSqueezeNet: SqueezeNet √® una classe di modelli ML noti per le sue dimensioni ridotte senza sacrificare la precisione. Ci√≤ si ottiene utilizzando un ‚Äúmodulo fire‚Äù che riduce il numero di canali di input a filtri 3x3, riducendo cos√¨ i parametri (Iandola et al. 2016). Inoltre, impiega il downsampling [sottocampionamento] ritardato per aumentare la precisione mantenendo una mappa delle feature pi√π ampia.\nVarianti ResNet: L‚Äôarchitettura Residual Network (ResNet) consente l‚Äôintroduzione di connessioni skip o scorciatoie (He et al. 2016). Alcune varianti di ResNet sono progettate per essere pi√π efficienti. Ad esempio, ResNet-SE incorpora il meccanismo ‚Äúsqueeze and excitation‚Äù per ricalibrare le feature map (Hu, Shen, e Sun 2018), mentre ResNeXt offre convoluzioni raggruppate per l‚Äôefficienza (Xie et al. 2017).\n\nHe, Kaiming, Xiangyu Zhang, Shaoqing Ren, e Jian Sun. 2016. ¬´Deep Residual Learning for Image Recognition¬ª. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 770‚Äì78. IEEE. https://doi.org/10.1109/cvpr.2016.90.\n\nHu, Jie, Li Shen, e Gang Sun. 2018. ¬´Squeeze-and-Excitation Networks¬ª. In 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, 7132‚Äì41. IEEE. https://doi.org/10.1109/cvpr.2018.00745.\n\nXie, Saining, Ross Girshick, Piotr Dollar, Zhuowen Tu, e Kaiming He. 2017. ¬´Aggregated Residual Transformations for Deep Neural Networks¬ª. In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 1492‚Äì1500. IEEE. https://doi.org/10.1109/cvpr.2017.634.",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>IA Efficiente</span>"
    ]
  },
  {
    "objectID": "contents/core/efficient_ai/efficient_ai.it.html#sec-efficient-model-compression",
    "href": "contents/core/efficient_ai/efficient_ai.it.html#sec-efficient-model-compression",
    "title": "8¬† IA Efficiente",
    "section": "8.4 Compressione Efficiente del Modello",
    "text": "8.4 Compressione Efficiente del Modello\nI metodi di compressione dei modelli sono essenziali per portare i modelli di apprendimento profondo su dispositivi con risorse limitate. Queste tecniche riducono le dimensioni dei modelli, il consumo energetico e le richieste di elaborazione senza perdere significativamente la precisione. Ad alto livello, i metodi possono essere categorizzati nei seguenti metodi fondamentali:\nPruning: L‚ÄôAbbiamo menzionato un paio di volte nei capitoli precedenti, ma non l‚Äôabbiamo ancora formalmente introdotta. Il pruning √® simile alla potatura dei rami di un albero. Questo √® stato pensato per la prima volta nel documento Optimal Brain Damage (LeCun, Denker, e Solla 1989) ed √® stato successivamente reso popolare nel contesto del deep learning da Han, Mao, e Dally (2016). Determinati pesi o interi neuroni vengono rimossi dalla rete nella potatura in base a criteri specifici. Questo pu√≤ ridurre significativamente le dimensioni del modello. In Sezione 9.2.1 esploreremo due delle principali strategie di potatura, quella strutturata e quella non-strutturata. Figura¬†8.2 √® un esempio di potatura della rete neurale, in cui la rimozione di alcuni nodi negli strati interni (in base a criteri specifici) riduce il numero di rami tra i nodi e, a sua volta, le dimensioni del modello.\n\nLeCun, Yann, John Denker, e Sara Solla. 1989. ¬´Optimal brain damage¬ª. Adv Neural Inf Process Syst 2.\n\nHan, Song, Huizi Mao, e William J. Dally. 2016. ¬´Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding¬ª. https://arxiv.org/abs/1510.00149.\n\n\n\n\n\n\nFigura¬†8.2: Neural Network Pruning.\n\n\n\nQuantizzazione: La quantizzazione √® il processo di limitazione di un input da un set ampio a un output in un set pi√π piccolo, principalmente nel deep learning; ci√≤ significa ridurre il numero di bit che rappresentano i pesi e i bias del modello. Ad esempio, l‚Äôutilizzo di rappresentazioni a 16 o 8 bit anzich√© a 32 bit pu√≤ ridurre la dimensione del modello e velocizzare i calcoli, con un piccolo compromesso in termini di accuratezza. Esploreremo questi aspetti pi√π in dettaglio in Sezione 9.3.4. Figura¬†8.3 mostra un esempio di quantizzazione mediante arrotondamento al numero pi√π vicino. La conversione da virgola mobile a 32 bit a 16 bit riduce l‚Äôutilizzo della memoria del 50%. Passare da un intero a 32 bit a uno a 8 bit riduce l‚Äôutilizzo della memoria del 75%. Mentre la perdita di precisione numerica e, di conseguenza, di prestazioni del modello √® minima, l‚Äôefficienza nell‚Äôutilizzo della memoria √® significativa.\n\n\n\n\n\n\nFigura¬†8.3: Diverse forme di quantizzazione.\n\n\n\nKnowledge Distillation: La ‚Äúdistillazione della conoscenza‚Äù comporta l‚Äôaddestramento di un modello pi√π piccolo (studente) per replicare il comportamento di un modello pi√π grande (insegnante). L‚Äôidea √® quella di trasferire la conoscenza dal modello ingombrante a quello leggero. Quindi, il modello pi√π piccolo raggiunge prestazioni vicine alla sua controparte pi√π grande ma con parametri significativamente inferiori. Figura¬†8.4 mostra la struttura tutor-studente per la distillazione della conoscenza. Esploreremo la ‚Äúdistillazione della conoscenza‚Äù in modo pi√π dettagliato in Sezione 9.2.2.1.\n\n\n\n\n\n\nFigura¬†8.4: Il framework tutor-studente per la distillazione della conoscenza. Fonte: Medium",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>IA Efficiente</span>"
    ]
  },
  {
    "objectID": "contents/core/efficient_ai/efficient_ai.it.html#hardware-di-inferenza-efficiente",
    "href": "contents/core/efficient_ai/efficient_ai.it.html#hardware-di-inferenza-efficiente",
    "title": "8¬† IA Efficiente",
    "section": "8.5 Hardware di Inferenza Efficiente",
    "text": "8.5 Hardware di Inferenza Efficiente\nNel capitolo Training, abbiamo discusso il processo di training dei modelli di intelligenza artificiale. Ora, dal punto di vista dell‚Äôefficienza, √® importante notare che il training √® un‚Äôattivit√† che richiede molte risorse e molto tempo, spesso richiede hardware potente e impiega da ore a settimane per essere completato. L‚Äôinferenza, d‚Äôaltra parte, deve essere il pi√π veloce possibile, soprattutto nelle applicazioni in tempo reale. √à qui che entra in gioco un hardware di inferenza efficiente. Ottimizzando l‚Äôhardware specificamente per le attivit√† di inferenza, possiamo ottenere tempi di risposta rapidi e un funzionamento efficiente dal punto di vista energetico, il che √® particolarmente cruciale per i dispositivi edge e i sistemi embedded.\nTPU (Tensor Processing Unit): Le TPU sono ASIC (Application-Specific Integrated Circuits) personalizzati da Google per accelerare i carichi di lavoro di apprendimento automatico (Jouppi et al. 2017). Sono ottimizzate per le operazioni tensoriali, offrono un throughput elevato per l‚Äôaritmetica a bassa precisione e sono progettate specificamente per il machine learning delle reti neurali. Le TPU accelerano significativamente l‚Äôaddestramento e l‚Äôinferenza del modello rispetto alle GPU/CPU generiche. Questo potenziamento si traduce in un addestramento pi√π rapido dei modelli e in capacit√† di inferenza in tempo reale o quasi reale, fondamentali per applicazioni come la ricerca vocale e la realt√† aumentata.\n\nJouppi, Norman P., Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal, Raminder Bajwa, Sarah Bates, et al. 2017. ¬´In-Datacenter Performance Analysis of a Tensor Processing Unit¬ª. In Proceedings of the 44th Annual International Symposium on Computer Architecture, 1‚Äì12. ISCA ‚Äô17. New York, NY, USA: ACM. https://doi.org/10.1145/3079856.3080246.\nLe Edge TPU sono una versione pi√π piccola e a basso consumo delle TPU di Google, studiate appositamente per i dispositivi edge. Forniscono un‚Äôinferenza ML veloce sul dispositivo per i modelli TensorFlow Lite. Le Edge TPU consentono un‚Äôinferenza a bassa latenza e ad alta efficienza su dispositivi edge come smartphone, dispositivi IoT e sistemi embedded. Le capacit√† di IA possono essere implementate in applicazioni in tempo reale senza comunicare con un server centrale, risparmiando cos√¨ larghezza di banda e riducendo la latenza. Si consideri la tabella in Figura¬†8.5. Mostra le differenze di prestazioni tra l‚Äôesecuzione di modelli diversi su CPU rispetto a un acceleratore Coral USB. L‚Äôacceleratore Coral USB √® un accessorio della piattaforma Coral AI di Google che consente agli sviluppatori di collegare le Edge TPU ai computer Linux. L‚Äôesecuzione dell‚Äôinferenza sulle Edge TPU √® stata da 70 a 100 volte pi√π veloce rispetto alle CPU.\n\n\n\n\n\n\nFigura¬†8.5: Confronto delle prestazioni tra acceleratore e CPU in diverse configurazioni hardware. Desktop CPU: 64-bit Intel(R) Xeon(R) E5-1650 v4 @ 3.60GHz. Embedded CPU: Quad-core Cortex-A53 @ 1.5GHz, ‚Ä†Dev Board: Quad-core Cortex-A53 @ 1.5GHz + Edge TPU. Fonte: TensorFlow Blog.\n\n\n\nAcceleratori NN (Neural Network): Gli acceleratori di reti neurali a funzione fissa sono acceleratori hardware progettati esplicitamente per i calcoli di reti neurali. Possono essere chip standalone o far parte di una soluzione di system-on-chip (SoC) pi√π ampia. Ottimizzando l‚Äôhardware per le operazioni specifiche richieste dalle reti neurali, come moltiplicazioni di matrici e convoluzioni, gli acceleratori NN possono ottenere tempi di inferenza pi√π rapidi e consumi energetici inferiori rispetto alle CPU e alle GPU per uso generico. Sono particolarmente utili nei dispositivi TinyML con vincoli di potenza o termici, come smartwatch, micro-droni o robotica.\nMa questi sono solo gli esempi pi√π comuni. Stanno emergendo diversi altri tipi di hardware che hanno il potenziale per offrire vantaggi significativi per l‚Äôinferenza. Questi includono, ma non solo, hardware neuromorfico, elaborazione fotonica, ecc. In Sezione 10.3, esploreremo questi aspetti in modo pi√π dettagliato.\nUn hardware efficiente per l‚Äôinferenza velocizza il processo, risparmia energia, prolunga la durata della batteria e pu√≤ funzionare in condizioni di tempo reale. Man mano che l‚Äôintelligenza artificiale viene integrata in innumerevoli applicazioni, dalle telecamere intelligenti agli assistenti vocali, il ruolo dell‚Äôhardware ottimizzato diventer√† sempre pi√π importante. Sfruttando questi componenti hardware specializzati, sviluppatori e ingegneri possono portare la potenza dell‚Äôintelligenza artificiale a dispositivi e situazioni che prima erano impensabili.",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>IA Efficiente</span>"
    ]
  },
  {
    "objectID": "contents/core/efficient_ai/efficient_ai.it.html#sec-efficient-numerics",
    "href": "contents/core/efficient_ai/efficient_ai.it.html#sec-efficient-numerics",
    "title": "8¬† IA Efficiente",
    "section": "8.6 Matematica Efficiente",
    "text": "8.6 Matematica Efficiente\nL‚Äôapprendimento automatico, e in particolare il deep learning, comporta enormi quantit√† di elaborazione. I modelli possono avere milioni o miliardi di parametri, spesso addestrati su vasti set di dati. Ogni operazione, ogni moltiplicazione o addizione, richiede risorse di elaborazione. Pertanto, la precisione dei numeri utilizzati in queste operazioni pu√≤ avere un impatto significativo sulla velocit√† di elaborazione, sul consumo di energia e sui requisiti di memoria. √à qui che entra in gioco il concetto di numeri efficienti.\n\n8.6.1 Formati Numerici\nEsistono molti tipi diversi di numeri. I numeri hanno una lunga storia nei sistemi di elaborazione.\nFloating point: Noto come ‚Äúvirgola mobile‚Äù a precisione singola, FP32 utilizza 32 bit per rappresentare un numero, incorporandone segno, esponente e mantissa. Comprendere come i numeri in virgola mobile sono rappresentati in modo approfondito √® fondamentale per comprendere le varie ottimizzazioni possibili nei calcoli numerici. Il bit del segno determina se il numero √® positivo o negativo, l‚Äôesponente controlla l‚Äôintervallo di valori che possono essere rappresentati e la mantissa determina la precisione del numero. La combinazione di questi componenti consente ai numeri in virgola mobile di rappresentare un‚Äôampia gamma di valori con vari gradi di precisione.\nVideo¬†8.1 fornisce una panoramica completa di questi tre componenti principali, segno, esponente e mantissa, e di come funzionano insieme per rappresentare i numeri in virgola mobile.\n\n\n\n\n\n\nVideo¬†8.1: Numeri in Virgola Mobile\n\n\n\n\n\n\nFP32 √® ampiamente adottato in molti framework di deep learning e bilancia accuratezza e requisiti computazionali. √à prevalente nella fase di training per molte reti neurali grazie alla sua sufficiente precisione nel catturare dettagli minuti durante gli aggiornamenti dei pesi. Noto anche come virgola mobile a mezza precisione, FP16 utilizza 16 bit per rappresentare un numero, inclusi il segno, l‚Äôesponente e la frazione. Offre un buon equilibrio tra precisione e risparmio di memoria. FP16 √® particolarmente popolare nella training di deep learning su GPU che supportano l‚Äôaritmetica a precisione mista, combinando i vantaggi di velocit√† di FP16 con la precisione di FP32 quando necessario.\nFigura¬†8.6 mostra tre diversi formati in virgola mobile: Float32, Float16 e BFloat16.\n\n\n\n\n\n\nFigura¬†8.6: Tre formati a virgola mobile.\n\n\n\nDiversi altri formati numerici rientrano in una classe esotica. Un esempio esotico √® BF16 o Brain Floating Point. √à un formato numerico a 16 bit progettato esplicitamente per applicazioni di deep learning. √à un compromesso tra FP32 e FP16, che mantiene l‚Äôesponente a 8 bit di FP32 riducendo la mantissa a 7 bit (rispetto alla mantissa a 23 bit di FP32). Questa struttura d√† priorit√† al range rispetto alla precisione. BF16 ha ottenuto risultati di training paragonabili in accuratezza a FP32, utilizzando significativamente meno memoria e risorse computazionali (Kalamkar et al. 2019). Ci√≤ lo rende adatto non solo per l‚Äôinferenza, ma anche per il training di reti neurali profonde.\n\nKalamkar, Dhiraj, Dheevatsa Mudigere, Naveen Mellempudi, Dipankar Das, Kunal Banerjee, Sasikanth Avancha, Dharma Teja Vooturi, et al. 2019. ¬´A Study of BFLOAT16 for Deep Learning Training¬ª. https://arxiv.org/abs/1905.12322.\nMantenendo l‚Äôesponente a 8 bit di FP32, BF16 offre un range simile, che √® fondamentale per le attivit√† di deep learning in cui determinate operazioni possono generare numeri molto grandi o molto piccoli. Allo stesso tempo, troncando la precisione, BF16 consente requisiti di memoria e computazionali ridotti rispetto a FP32. BF16 √® emerso come una promettente via di mezzo nel panorama dei formati numerici per il deep learning, fornendo un‚Äôalternativa efficiente ed efficace ai formati FP32 e FP16 pi√π tradizionali.\nIntero: Si tratta di rappresentazioni di numeri interi che utilizzano 8, 4 e 2 bit. Vengono spesso utilizzati durante la fase di inferenza delle reti neurali, in cui i pesi e le attivazioni del modello sono quantizzati a queste precisioni inferiori. Le rappresentazioni intere sono deterministiche e offrono notevoli vantaggi in termini di velocit√† e memoria rispetto alle rappresentazioni in virgola mobile. Per molte attivit√† di inferenza, in particolare sui dispositivi edge, la leggera perdita di accuratezza dovuta alla quantizzazione √® spesso accettabile, dati i guadagni di efficienza. Una forma estrema di numeri interi √® per le reti neurali binarie (BNN), in cui pesi e attivazioni sono vincolati a uno dei due valori: +1 o -1.\nLarghezze di bit variabili: Oltre alle larghezze standard, sono in corso ricerche su numeri con larghezze di bit estremamente basse, persino fino a rappresentazioni binarie o ternarie. Le operazioni con larghezze di bit estremamente basse possono offrire accelerazioni significative e ridurre ulteriormente il consumo di energia. Sebbene permangano dei problemi nel mantenere l‚Äôaccuratezza del modello con una quantizzazione cos√¨ drastica, si continuano a fare progressi in quest‚Äôarea.\nL‚Äôefficienza numerica non riguarda solo la riduzione della larghezza di bit dei numeri, ma anche la comprensione dei compromessi tra accuratezza ed efficienza. Man mano che i modelli di apprendimento automatico diventano pi√π pervasivi, soprattutto in ambienti reali con risorse limitate, l‚Äôattenzione su una numerica efficiente continuer√† a crescere. Selezionando e sfruttando attentamente la precisione numerica appropriata, √® possibile ottenere prestazioni di modello robuste ottimizzando al contempo velocit√†, memoria ed energia. Tabella¬†8.1 riassume questi compromessi.\n\n\n\nTabella¬†8.1: Confronto dei livelli di precisione nel deep learning.\n\n\n\n\n\n\n\n\n\n\nPrecisione\nPro\nContro\n\n\n\n\nFP32 (virgola mobile a 32 bit)\n\nPrecisione standard utilizzata nella maggior parte dei framework di deep learning.\nElevata accuratezza grazie all‚Äôampia capacit√† di rappresentazione.\nAdatto per il training\n\n\nElevato utilizzo di memoria.\nTempi di inferenza pi√π lenti rispetto ai modelli quantizzati.\nMaggiore consumo energetico.\n\n\n\nFP16 (virgola mobile a 16 bit)\n\nRiduce l‚Äôutilizzo di memoria rispetto a FP32.\nVelocizza i calcoli su hardware che supporta FP16.\nSpesso utilizzato nel training a precisione mista per bilanciare velocit√† e accuratezza.\n\n\nMinore capacit√† di rappresentazione rispetto a FP32.\nRischio di instabilit√† numerica in alcuni modelli o livelli.\n\n\n\nINT8 (intero a 8 bit)\n\nImpronta di memoria notevolmente ridotta rispetto alle rappresentazioni in virgola mobile.\nInferenza pi√π rapida se l‚Äôhardware supporta i calcoli INT8.\nAdatto a molti scenari di quantizzazione post-training.\n\n\nLa quantizzazione pu√≤ comportare una certa perdita di accuratezza.\nRichiede una calibrazione attenta durante la quantizzazione per ridurre al minimo il degrado della precisione.\n\n\n\nINT4 (intero a 4 bit)\n\nUtilizzo di memoria ancora inferiore rispetto a INT8.\nUlteriore potenziale di accelerazione per l‚Äôinferenza.\n\n\nRischio di perdita di precisione pi√π elevato rispetto a INT8.\nLa calibrazione durante la quantizzazione diventa pi√π critica.\n\n\n\nBinario\n\nIngombro di memoria minimo (solo 1 bit per parametro).\nInferenza estremamente rapida grazie alle operazioni bit a bit.\nEfficienza energetica.\n\n\nCalo significativo della precisione per molte attivit√†.\nDinamiche di training complesse grazie alla quantizzazione estrema.\n\n\n\nTernario\n\nBasso utilizzo di memoria ma leggermente superiore a quello binario.\nOffre una via di mezzo tra rappresentazione ed efficienza.\n\n\nL‚Äôaccuratezza potrebbe essere ancora inferiore a quella dei modelli di precisione pi√π elevata.\nLe dinamiche di addestramento possono essere complesse.\n\n\n\n\n\n\n\n\n\n8.6.2 Vantaggi dell‚ÄôEfficienza\nL‚Äôefficienza numerica √® importante per i carichi di lavoro di machine learning per diversi motivi:\nEfficienza Computazionale: I calcoli ad alta precisione (come FP32 o FP64) possono essere lenti e richiedere molte risorse. Ridurre la precisione numerica pu√≤ ottenere tempi di calcolo pi√π rapidi, specialmente su hardware specializzato che supporta una precisione inferiore.\nEfficienza della Memoria: I requisiti di archiviazione diminuiscono con una precisione numerica ridotta. Ad esempio, FP16 richiede met√† della memoria di FP32. Ci√≤ √® fondamentale quando si distribuiscono modelli su dispositivi edge con memoria limitata o si lavora con modelli di grandi dimensioni.\nEfficienza Energetica: I calcoli a precisione inferiore spesso consumano meno energia, il che √® particolarmente importante per i dispositivi alimentati a batteria.\nIntroduzione del Rumore: √à interessante notare che il rumore introdotto utilizzando una precisione inferiore pu√≤ talvolta fungere da regolarizzatore, contribuendo a prevenire l‚Äôoverfitting in alcuni modelli.\nAccelerazione Hardware: Molti acceleratori di IA e GPU moderni sono ottimizzati per operazioni di precisione inferiore, sfruttando i vantaggi dell‚Äôefficienza di tali numeri.",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>IA Efficiente</span>"
    ]
  },
  {
    "objectID": "contents/core/efficient_ai/efficient_ai.it.html#valutazione-dei-modelli",
    "href": "contents/core/efficient_ai/efficient_ai.it.html#valutazione-dei-modelli",
    "title": "8¬† IA Efficiente",
    "section": "8.7 Valutazione dei Modelli",
    "text": "8.7 Valutazione dei Modelli\nVale la pena notare che i vantaggi e i compromessi effettivi possono variare in base all‚Äôarchitettura specifica della rete neurale, al set di dati, all‚Äôattivit√† e all‚Äôhardware utilizzato. Prima di decidere una precisione numerica, √® consigliabile eseguire esperimenti per valutare l‚Äôimpatto sull‚Äôapplicazione desiderata.\n\n8.7.1 Metriche di Efficienza\nUna profonda comprensione dei metodi di valutazione dei modelli √® importante per guidare questo processo in modo sistematico. Quando si valuta l‚Äôefficacia e l‚Äôidoneit√† dei modelli di intelligenza artificiale per varie applicazioni, le metriche di efficienza vengono in primo piano.\nI FLOP (Floating Point Operations), introdotti in Training, misurano le esigenze computazionali di un modello. Ad esempio, una moderna rete neurale come BERT ha miliardi di FLOP, che potrebbero essere gestibili su un potente server cloud ma sarebbero gravosi su uno smartphone. FLOP pi√π elevati possono portare a tempi di inferenza pi√π prolungati e a un notevole consumo di energia, soprattutto su dispositivi senza acceleratori hardware specializzati. Quindi, per applicazioni in tempo reale come lo streaming video o i giochi, potrebbero essere pi√π desiderabili modelli con FLOP pi√π bassi.\nL‚ÄôUtilizzo della Memoria riguarda la quantit√† di spazio di archiviazione richiesta dal modello, che influisce sia sullo spazio di archiviazione del dispositivo che sulla RAM. Si prenda in considerazione l‚Äôimplementazione di un modello su uno smartphone: un modello che occupa diversi gigabyte di spazio non solo consuma prezioso spazio di archiviazione, ma potrebbe anche essere pi√π lento a causa della necessit√† di caricare grandi pesi nella memoria. Ci√≤ diventa particolarmente cruciale per dispositivi edge come telecamere di sicurezza o droni, dove impronte di memoria minime sono vitali per l‚Äôarchiviazione e l‚Äôelaborazione rapida dei dati.\nIl Consumo Energetico diventa particolarmente cruciale per i dispositivi che si basano sulle batterie. Ad esempio, un monitor sanitario indossabile che utilizza un modello ad alto consumo energetico potrebbe esaurire la batteria in poche ore, rendendolo poco pratico per il monitoraggio continuo. L‚Äôottimizzazione dei modelli per un basso consumo energetico diventa essenziale mentre ci muoviamo verso un‚Äôera dominata dai dispositivi IoT, dove molti dispositivi funzionano a batteria.\nIl Tempo di Inferenza riguarda la rapidit√† con cui un modello pu√≤ produrre risultati. In applicazioni come la guida autonoma, dove decisioni in frazioni di secondo fanno la differenza tra sicurezza e calamit√†, i modelli devono funzionare rapidamente. Se il modello di un‚Äôauto a guida autonoma impiega anche solo pochi secondi in pi√π per riconoscere un ostacolo, le conseguenze potrebbero essere disastrose. Quindi, garantire che il tempo di inferenza di un modello sia allineato con le richieste in tempo reale della sua applicazione √® fondamentale.\nIn sostanza, queste metriche di efficienza sono pi√π che dei numeri che stabiliscono dove e come un modello pu√≤ essere distribuito in modo efficace. Un modello potrebbe vantare un‚Äôelevata accuratezza, ma se i suoi FLOP, l‚Äôutilizzo della memoria, il consumo energetico o il tempo di inferenza lo rendono inadatto alla piattaforma prevista o agli scenari del mondo reale, la sua utilit√† pratica diventa limitata.\n\n\n8.7.2 Confronti di Efficienza\nIl panorama dei modelli di machine learning √® vasto, con ogni modello che offre un set unico di punti di forza e considerazioni di implementazione. Sebbene le cifre di accuratezza grezza o le velocit√† di training e inferenza possano essere parametri di riferimento allettanti, forniscono un quadro incompleto. Un‚Äôanalisi comparativa pi√π approfondita rivela diversi fattori critici che influenzano l‚Äôidoneit√† di un modello per le applicazioni TinyML. Spesso, incontriamo il delicato equilibrio tra accuratezza ed efficienza. Ad esempio, mentre un modello di deep learning e denso e una variante MobileNet leggera potrebbero eccellere nella classificazione delle immagini, le loro richieste di calcolo potrebbero essere ad estremi opposti. Questa differenziazione √® particolarmente pronunciata quando si confrontano le distribuzioni su server cloud con risorse abbondanti rispetto ai limitati dispositivi TinyML. In molti scenari del mondo reale, i guadagni marginali in termini di accuratezza potrebbero essere oscurati dalle inefficienze di un modello ad alta intensit√† di risorse richieste.\nInoltre, la scelta del modello ottimale non √® sempre universale, ma spesso dipende dalle specifiche di un‚Äôapplicazione. Ad esempio, un modello che eccelle in scenari di rilevamento di oggetti generali potrebbe avere difficolt√† in ambienti di nicchia, come il rilevamento di difetti di fabbricazione in una fabbrica. Questa adattabilit√†, o la sua mancanza, pu√≤ influenzare l‚Äôutilit√† reale di un modello.\nUn‚Äôaltra considerazione importante √® la relazione tra la complessit√† del modello e i suoi vantaggi pratici. Prendiamo gli assistenti attivati tramite comando vocale, come ‚ÄúAlexa‚Äù o ‚ÄúOK Google‚Äù. Mentre un modello complesso potrebbe dimostrare una comprensione marginalmente superiore del parlato dell‚Äôutente se √® pi√π lento a rispondere rispetto a una controparte pi√π semplice, l‚Äôesperienza utente potrebbe essere compromessa. Pertanto, l‚Äôaggiunta di layer o parametri solo a volte equivale a risultati migliori nel mondo reale.\nUn‚Äôaltra considerazione importante √® la relazione tra la complessit√† del modello e i suoi vantaggi pratici. Prendiamo gli assistenti vocali come ‚ÄúAlexa‚Äù o ‚ÄúOK Google‚Äù. Mentre un modello complesso potrebbe dimostrare una comprensione leggermente superiore del parlato dell‚Äôutente se √® pi√π lento a rispondere rispetto a una controparte pi√π semplice, l‚Äôesperienza utente potrebbe essere compromessa. Pertanto, l‚Äôaggiunta di layer o parametri solo a volte equivale a risultati migliori nel mondo reale.\nInoltre, mentre i set di dati di riferimento, come ImageNet (Russakovsky et al. 2015), COCO (Lin et al. 2014), Visual Wake Words (Wang e Zhan 2019), Google Speech Commands (Warden 2018), ecc. forniscono una metrica di prestazioni standardizzata, potrebbero non catturare la diversit√† e l‚Äôimprevedibilit√† dei dati del mondo reale. Due modelli di riconoscimento facciale con punteggi di riferimento simili potrebbero mostrare competenze diverse quando si trovano di fronte a background etnici diversi o condizioni di illuminazione difficili. Tali disparit√† sottolineano l‚Äôimportanza di robustezza e coerenza tra dati diversi. Ad esempio, Figura¬†8.7 dal set di dati Dollar Street mostra immagini di stufe su redditi mensili estremi. Le stufe hanno forme e livelli tecnologici diversi in diverse regioni e livelli di reddito. Un modello che non √® addestrato su set di dati diversi potrebbe funzionare bene su un benchmark ma fallire nelle applicazioni del mondo reale. Quindi, se un modello fosse addestrato solo su immagini di stufe trovate nei paesi ricchi, non riuscirebbe a riconoscere le stufe delle regioni pi√π povere.\n\nRussakovsky, Olga, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, et al. 2015. ¬´ImageNet Large Scale Visual Recognition Challenge¬ª. Int. J. Comput. Vision 115 (3): 211‚Äì52. https://doi.org/10.1007/s11263-015-0816-y.\n\nLin, Tsung-Yi, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll√°r, e C Lawrence Zitnick. 2014. ¬´Microsoft coco: Common objects in context¬ª. In Computer VisionECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, 740‚Äì55. Springer.\n\nWang, LingFeng, e YaQing Zhan. 2019. ¬´A conceptual peer review model for arXiv and other preprint databases¬ª. Learn. Publ. 32 (3): 213‚Äì19. https://doi.org/10.1002/leap.1229.\n\nWarden, Pete. 2018. ¬´Speech commands: A dataset for limited-vocabulary speech recognition¬ª. arXiv preprint arXiv:1804.03209.\n\n\n\n\n\n\nFigura¬†8.7: Diversi tipi di stufe. Fonte: Immagini di stufe di Dollar Street.\n\n\n\nIn sostanza, un‚Äôanalisi comparativa approfondita trascende le metriche numeriche. √à una valutazione olistica intrecciata con applicazioni del mondo reale, costi e le intricate sottigliezze che ogni modello porta con s√©. Ecco perch√© avere parametri di riferimento e metriche standard ampiamente stabiliti e adottati dalla comunit√† diventa importante.",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>IA Efficiente</span>"
    ]
  },
  {
    "objectID": "contents/core/efficient_ai/efficient_ai.it.html#conclusione",
    "href": "contents/core/efficient_ai/efficient_ai.it.html#conclusione",
    "title": "8¬† IA Efficiente",
    "section": "8.8 Conclusione",
    "text": "8.8 Conclusione\nL‚Äôintelligenza artificiale efficiente √® fondamentale mentre ci spingiamo verso un‚Äôimplementazione pi√π ampia e diversificata del machine learning nel mondo reale. Questo capitolo ha fornito una panoramica, esplorando le varie metodologie e considerazioni alla base del raggiungimento di un‚Äôintelligenza artificiale efficiente, a partire dall‚Äôesigenza fondamentale, dalle somiglianze e dalle differenze tra i sistemi cloud, Edge e TinyML.\nAbbiamo esaminato le architetture dei modelli efficienti e la loro utilit√† per l‚Äôottimizzazione. Le tecniche di compressione dei modelli come pruning, quantizzazione e distillazione della conoscenza esistono per aiutare a ridurre le richieste di calcolo e l‚Äôingombro della memoria senza influire in modo significativo sulla precisione. Hardware specializzati come TPU e acceleratori NN offrono chip ottimizzati per le operazioni di rete neurale e il flusso di dati. I numeri efficienti bilanciano precisione ed efficienza, consentendo ai modelli di ottenere prestazioni robuste utilizzando risorse minime. Esploreremo questi argomenti in modo approfondito e dettagliato nei capitoli successivi.\nInsieme, questi formano un quadro olistico per un‚Äôintelligenza artificiale efficiente. Ma il viaggio non finisce qui. Il raggiungimento di un‚Äôintelligenza efficiente in modo ottimale richiede ricerca e innovazione continue. Man mano che i modelli diventano pi√π sofisticati, i set di dati crescono e le applicazioni si diversificano in domini specializzati, l‚Äôefficienza deve evolversi di pari passo. La misura dell‚Äôimpatto nel mondo reale richiede parametri di riferimento adatti e metriche standardizzate che vadano oltre le semplicistiche cifre dell‚Äôaccuratezza.\nInoltre, l‚Äôintelligenza artificiale efficiente si espande oltre l‚Äôottimizzazione tecnologica e comprende costi, impatto ambientale e considerazioni etiche per il bene della societ√† in senso pi√π ampio. Man mano che l‚Äôintelligenza artificiale permea i settori e la vita quotidiana, una prospettiva completa sull‚Äôefficienza sostiene il suo progresso sostenibile e responsabile. I capitoli successivi si baseranno su questi concetti fondamentali, fornendo approfondimenti concreti e norme pratiche per lo sviluppo e l‚Äôimplementazione di soluzioni di intelligenza artificiale efficienti.",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>IA Efficiente</span>"
    ]
  },
  {
    "objectID": "contents/core/efficient_ai/efficient_ai.it.html#sec-efficient-ai-resource",
    "href": "contents/core/efficient_ai/efficient_ai.it.html#sec-efficient-ai-resource",
    "title": "8¬† IA Efficiente",
    "section": "8.9 Risorse",
    "text": "8.9 Risorse\nEcco un elenco curato di risorse per supportare studenti e insegnanti nei loro percorsi di apprendimento e insegnamento. Lavoriamo continuamente per espandere questa raccolta e presto aggiungeremo nuovi esercizi.\n\n\n\n\n\n\nSlide\n\n\n\n\n\nQueste slide sono uno strumento prezioso per gli insegnanti per tenere lezioni e per gli studenti per rivedere il materiale secondo il proprio ritmo. Incoraggiamo studenti e docenti a sfruttare queste slide per migliorare la loro comprensione e facilitare un trasferimento efficace delle conoscenze.\n\nDeploying on Edge Devices: challenges and techniques.\nModel Evaluation.\nContinuous Evaluation Challenges for TinyML.\n\n\n\n\n\n\n\n\n\n\nVideo\n\n\n\n\n\n\nProssimamente.\n\n\n\n\n\n\n\n\n\n\nEsercizi\n\n\n\n\n\nPer rafforzare i concetti trattati in questo capitolo, abbiamo curato una serie di esercizi che sfidano gli studenti ad applicare le proprie conoscenze e ad approfondire la propria comprensione.\n\nProssimamente.",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>IA Efficiente</span>"
    ]
  },
  {
    "objectID": "contents/core/optimizations/optimizations.it.html",
    "href": "contents/core/optimizations/optimizations.it.html",
    "title": "9¬† Ottimizzazioni dei Modelli",
    "section": "",
    "text": "9.1 Panoramica\nL‚Äôottimizzazione dei modelli di apprendimento automatico per l‚Äôimplementazione pratica √® un aspetto critico dei sistemi di IA. Questo capitolo si concentra sull‚Äôesplorazione delle tecniche di ottimizzazione dei modelli in relazione allo sviluppo di sistemi di apprendimento automatico, che vanno dalle considerazioni di architettura del modello di alto livello agli adattamenti hardware di basso livello. Figura¬†9.1 Illustra i tre livelli dello stack di ottimizzazione che trattiamo.\nAl livello pi√π alto, esaminiamo le metodologie per ridurre la complessit√† dei parametri del modello senza compromettere le capacit√† inferenziali. Tecniche come la potatura e la distillazione della conoscenza offrono approcci potenti per comprimere e perfezionare i modelli mantenendo o addirittura migliorandone le prestazioni, non solo in termini di qualit√† del modello ma anche nelle prestazioni effettive del runtime del sistema. Questi metodi sono fondamentali per creare modelli efficienti che possono essere implementati in ambienti con risorse limitate.\nInoltre, esploriamo il ruolo della precisione numerica nei calcoli del modello. Comprendere come diversi livelli di precisione numerica influenzino le dimensioni, la velocit√† e l‚Äôaccuratezza del modello √® essenziale per ottimizzare le prestazioni. Analizziamo vari formati numerici e l‚Äôapplicazione dell‚Äôaritmetica a precisione ridotta, particolarmente rilevante per le distribuzioni di sistemi embedded in cui le risorse computazionali sono spesso limitate.\nAl livello pi√π basso, esploriamo l‚Äôintricato panorama della progettazione congiunta hardware-software. Questa esplorazione rivela come i modelli possono essere personalizzati per sfruttare le caratteristiche e le capacit√† specifiche delle piattaforme hardware di destinazione. Allineando la progettazione del modello all‚Äôarchitettura hardware, possiamo migliorare significativamente le prestazioni e l‚Äôefficienza.\nQuesto approccio collettivo si concentra sull‚Äôaiutarci a sviluppare e distribuire modelli di apprendimento automatico efficienti, potenti e consapevoli dell‚Äôhardware. Dalla semplificazione delle architetture del modello alla messa a punto della precisione numerica e all‚Äôadattamento a hardware specifico, questo capitolo copre l‚Äôintero spettro di strategie di ottimizzazione. Alla conclusione di questo capitolo, i lettori avranno acquisito una conoscenza approfondita di varie tecniche di ottimizzazione e delle loro applicazioni pratiche in scenari del mondo reale. Questa conoscenza √® importante per creare modelli di apprendimento automatico che non solo funzionino bene, ma siano anche ottimizzati per i vincoli e le opportunit√† offerti dagli ambienti informatici moderni.",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Ottimizzazioni dei Modelli</span>"
    ]
  },
  {
    "objectID": "contents/core/optimizations/optimizations.it.html#panoramica",
    "href": "contents/core/optimizations/optimizations.it.html#panoramica",
    "title": "9¬† Ottimizzazioni dei Modelli",
    "section": "",
    "text": "Figura¬†9.1: Tre livelli da coprire.",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Ottimizzazioni dei Modelli</span>"
    ]
  },
  {
    "objectID": "contents/core/optimizations/optimizations.it.html#sec-model_ops_representation",
    "href": "contents/core/optimizations/optimizations.it.html#sec-model_ops_representation",
    "title": "9¬† Ottimizzazioni dei Modelli",
    "section": "9.2 Rappresentazione Efficiente del Modello",
    "text": "9.2 Rappresentazione Efficiente del Modello\nIl primo passo per l‚Äôottimizzazione del modello inizia in un territorio familiare per la maggior parte dei professionisti del ML: la rappresentazione efficiente del modello viene spesso affrontata per la prima volta al livello pi√π alto di astrazione della parametrizzazione, ovvero l‚Äôarchitettura stessa del modello.\nLa maggior parte dei professionisti del ML tradizionali progetta modelli con un obiettivo generale di alto livello in mente, che si tratti di classificazione delle immagini, rilevamento di persone o individuazione di parole chiave come menzionato in precedenza in questo testo. I loro progetti in genere finiscono per adattarsi naturalmente ad alcuni vincoli soft dovuti a risorse di elaborazione limitate durante lo sviluppo, ma in genere questi progetti non sono a conoscenza di vincoli successivi, come quelli richiesti se il modello deve essere distribuito su un dispositivo pi√π limitato anzich√© sul cloud.\nIn questa sezione, discuteremo di come i professionisti possono sfruttare i principi della progettazione congiunta hardware-software anche nell‚Äôarchitettura di alto livello di un modello per rendere i loro modelli compatibili con i dispositivi edge. Da quelli pi√π consapevoli dell‚Äôhardware a quelli meno consapevoli a questo livello di modifica, discutiamo alcune delle strategie pi√π comuni per una parametrizzazione efficiente del modello: pruning, compressione e architetture edge-friendly. Abbiamo gi√† parlato di pruning e compressione del modello in Sezione 8.4; questa sezione andr√† oltre le definizioni per fornire una comprensione tecnica del loro funzionamento.\n\n9.2.1 Il Pruning\n\nPanoramica\nIl model pruning [potatura] √® una tecnica di apprendimento automatico che riduce le dimensioni e la complessit√† di un modello di rete neurale, mantenendone il pi√π possibile le capacit√† predittive. L‚Äôobiettivo della potatura √® quello di rimuovere componenti ridondanti o non essenziali del modello, tra cui connessioni tra neuroni, singoli neuroni o persino interi layer della rete.\nQuesto processo in genere comporta l‚Äôanalisi del modello di machine learning per identificare e rimuovere pesi, nodi o layer che hanno scarso impatto sugli output del modello. Potando selettivamente un modello in questo modo, il numero totale di parametri pu√≤ essere ridotto in modo significativo senza cali sostanziali nell‚Äôaccuratezza del modello. Il modello compresso risultante richiede meno memoria e risorse di calcolo per l‚Äôaddestramento e l‚Äôesecuzione, consentendo tempi di inferenza pi√π rapidi.\nIl pruning del modello √® particolarmente utile quando si distribuiscono modelli di apprendimento automatico su dispositivi con risorse di calcolo limitate, come telefoni cellulari o sistemi TinyML. La tecnica facilita la distribuzione di modelli pi√π grandi e complessi su questi dispositivi riducendo le loro richieste di risorse. Inoltre, i modelli pi√π piccoli richiedono meno dati per generalizzare bene e sono meno inclini all‚Äôoverfitting [sovradattamento]. Fornendo un modo efficiente per semplificare i modelli, la potatura dei modelli √® diventata una tecnica fondamentale per ottimizzare le reti neurali nell‚Äôapprendimento automatico.\nEsistono diverse tecniche di potatura comuni utilizzate nell‚Äôapprendimento automatico, tra cui la potatura strutturata, la potatura non strutturata, la potatura iterativa, la potatura bayesiana e persino la potatura casuale. Oltre a potare i pesi, si possono anche potare le attivazioni. La potatura di attivazioni prende di mira specificamente neuroni o filtri che si attivano raramente o hanno un‚Äôattivazione complessivamente bassa. Esistono numerosi altri metodi, come la potatura di sensibilit√† e movimento. Per un elenco completo dei metodi, si consiglia al lettore di leggere il seguente articolo: ‚ÄúA Survey on Deep Neural Network Pruning: Taxonomy, Comparison, Analysis, and Recommendations‚Äù (2023).\nQuindi, come si scelgono i metodi di potatura? Esistono molte varianti di tecniche di potatura, ciascuna delle quali varia l‚Äôeuristica di ci√≤ che dovrebbe essere mantenuto e potato dal modello, nonch√© il numero di volte in cui cui deve essere eseguita. Tradizionalmente, la potatura avviene dopo che il modello √® completamente addestrato, dove il modello potato pu√≤ subire una lieve perdita di accuratezza. Tuttavia, come discuteremo pi√π avanti, recenti scoperte hanno trovato che la potatura pu√≤ essere utilizzata durante l‚Äôaddestramento (ad esempio, in modo iterativo) per identificare rappresentazioni del modello pi√π efficienti e accurate.\n\n\nPotatura Strutturata\nIniziamo con la ‚Äúpotatura strutturata‚Äù, una tecnica che riduce le dimensioni di una rete neurale eliminando intere sotto-strutture specifiche del modello mantenendone la struttura generale. Rimuove interi neuroni/canali o layer in base a criteri di importanza. Ad esempio, per una rete neurale convoluzionale (CNN), potrebbero essere determinate istanze di filtro o canali. Per reti completamente connesse, potrebbero essere i neuroni stessi mantenendo la piena connettivit√† o persino l‚Äôeliminazione di interi layer del modello che sono considerati insignificanti. Questo tipo di potatura spesso porta a reti sparse regolari e strutturate che sono compatibili con l‚Äôhardware.\nSono iniziate a emergere le ‚Äúbest practice‚Äù su come pensare alla potatura strutturata. Ci sono tre componenti principali:\n\n1. Strutture Candidate per il Pruning\nData la variet√† di approcci, diverse strutture all‚Äôinterno di una rete neurale vengono potate in base a criteri specifici. Le strutture primarie per la potatura includono neuroni, canali e talvolta interi layer, ognuno con le sue implicazioni e metodologie uniche. L‚Äôobiettivo di ogni approccio √® garantire che il modello ridotto mantenga il pi√π possibile la capacit√† predittiva del modello originale, migliorando al contempo l‚Äôefficienza computazionale e riducendo le dimensioni.\nQuando i neuroni vengono potati, rimuoviamo interi neuroni insieme ai loro pesi e bias associati, riducendo cos√¨ la larghezza del layer. Questo tipo di potatura viene spesso utilizzato in layer completamente connessi.\nLa potatura del canale, che viene applicata prevalentemente nelle reti neurali convoluzionali (CNN), comporta l‚Äôeliminazione di interi canali o filtri, il che a sua volta riduce la profondit√† delle mappe delle feature e influisce sulla capacit√† della rete di estrarre determinate feature dai dati di input. Ci√≤ √® particolarmente cruciale nelle attivit√† di elaborazione delle immagini in cui l‚Äôefficienza computazionale √® fondamentale.\nInfine, la potatura dei layer adotta un approccio pi√π aggressivo rimuovendo interi layer della rete. Ci√≤ riduce significativamente la profondit√† della rete e quindi la sua capacit√† di plasmare pattern e gerarchie complesse nei dati. Questo approccio richiede un attento equilibrio per garantire che la capacit√† predittiva del modello non venga indebitamente compromessa.\nFigura¬†9.2 mostra la differenza tra la potatura di canale/filtro e quella del layer. Quando potiamo un canale, dobbiamo riconfigurare l‚Äôarchitettura del modello per adattarla ai cambiamenti strutturali. Una modifica consiste nel cambiare il numero di canali di input nel layer successivo (qui, il terzo e il layer pi√π profondo): modificando le profondit√† dei filtri applicati al layer con il canale potato. D‚Äôaltra parte, la potatura di un intero layer (rimuovendo tutti i canali nel layer) richiede modifiche pi√π drastiche. Quella principale riguarda la modifica delle connessioni tra i layer rimanenti per sostituire o bypassare il layer potato. Nel nostro caso, riconfiguriamo per connettere il primo e l‚Äôultimo layer. In tutti i casi di potatura, dobbiamo mettere a punto la nuova struttura per regolare i pesi.\n\n\n\n\n\n\nFigura¬†9.2: Potatura del canale e quella del layer.\n\n\n\n\n\n2. Stabilire un criterio per il Pruning\nStabilire criteri ben definiti per determinare quali strutture specifiche potare da un modello di rete neurale √® una componente cruciale del processo di ‚Äúpruning‚Äù del modello. L‚Äôobiettivo principale qui √® identificare e rimuovere i componenti che contribuiscono meno alle capacit√† predittive del modello, mantenendo al contempo le strutture integrali per preservare l‚Äôaccuratezza.\nUna strategia ampiamente adottata ed efficace per potare sistematicamente le strutture si basa sul calcolo di punteggi di importanza per singoli componenti come neuroni, filtri, canali o layer. Questi punteggi servono come metriche quantitative per valutare la significativit√† di ciascuna struttura e il suo effetto sull‚Äôoutput del modello.\nEsistono diverse tecniche per assegnare questi punteggi sull‚Äôimportanza:\n\nPruning Basato sulla Magnitudo del peso: Questo approccio assegna punteggi di importanza a una struttura valutando la magnitudo aggregata dei pesi associati. Le strutture con magnitudo del peso complessivo inferiore sono considerate meno critiche per le prestazioni della rete.\nPruning Basato sul Bradiente: Questa tecnica utilizza i gradienti della funzione di los [perdita] rispetto ai pesi associati a una struttura. Le strutture con magnitudo del gradiente cumulativo basso, che indica un impatto minimo sulla perdita quando alterato, sono le candidate principali per la potatura.\nPruning Basato sull‚ÄôAttivazione: Questo metodo tiene traccia della frequenza con cui un neurone o un filtro viene attivato memorizzando queste informazioni in un parametro chiamato contatore delle attivazioni. Ogni volta che la struttura viene attivata, il contatore viene incrementato. Un conteggio di attivazione basso suggerisce che la struttura √® meno rilevante.\nPruning Basato sull‚ÄôEspansione di Taylor: Questo approccio approssima la modifica nella funzione di perdita derivante dalla rimozione di un dato peso. Valutando la perturbazione della perdita cumulativa derivante dalla rimozione di tutti i pesi associati a una struttura, √® possibile identificare le strutture con un impatto trascurabile sulla perdita, rendendole candidate idonee per la potatura.\n\nL‚Äôidea √® di misurare, direttamente o indirettamente, il contributo di ogni componente all‚Äôoutput del modello. Le strutture con un‚Äôinfluenza minima in base ai criteri definiti vengono potate per prime. Ci√≤ consente una potatura selettiva e ottimizzata che comprime al massimo i modelli preservando al contempo la capacit√† predittiva. In generale, √® importante valutare l‚Äôimpatto della rimozione di particolari strutture sull‚Äôoutput del modello, con lavori recenti come (Rachwan et al. 2022) e (Lubana e Dick 2020) che studiano combinazioni di tecniche come la potatura basata sulla magnitudine e la potatura basata sul gradiente.\n\nRachwan, John, Daniel Z√ºgner, Bertrand Charpentier, Simon Geisler, Morgane Ayle, e Stephan G√ºnnemann. 2022. ¬´Winning the lottery ahead of time: Efficient early network pruning¬ª. In International Conference on Machine Learning, 18293‚Äì309. PMLR.\n\nLubana, Ekdeep Singh, e Robert P Dick. 2020. ¬´A gradient flow framework for analyzing network pruning¬ª. arXiv preprint arXiv:2009.11839.\n\n\n3. Selezione di una Strategia di Potatura\nOra che abbiamo capito alcune tecniche per determinare l‚Äôimportanza delle strutture all‚Äôinterno di una rete neurale, il passo successivo √® decidere come applicare queste intuizioni. Ci√≤ comporta la selezione di una strategia di potatura appropriata, che stabilisce come e quando le strutture identificate vengono rimosse e come il modello viene messo a punto per mantenere le sue prestazioni. Esistono due principali strategie di potatura strutturata: quella iterativa e la one-shot.\nLa potatura iterativa rimuove gradualmente le strutture attraverso pi√π cicli di potatura seguiti da messa a punto. In ogni ciclo, un piccolo set di strutture viene potato in base a criteri di importanza. Il modello viene poi messo a punto, consentendogli di adattarsi senza problemi ai cambiamenti strutturali prima della successiva iterazione di potatura. Questo approccio graduale e ciclico impedisce bruschi cali di accuratezza. Consente al modello di adattarsi lentamente man mano che le strutture vengono ridotte attraverso le iterazioni.\nConsideriamo una situazione in cui desideriamo potare i 6 canali meno efficaci (in base ad alcuni criteri specifici) da una rete neurale convoluzionale. In Figura¬†9.3, mostriamo un processo di potatura semplificato eseguito su 3 iterazioni. In ogni iterazione, eliminiamo solo 2 canali. La rimozione dei canali comporta un degrado della precisione. Nella prima iterazione, la precisione scende da 0.995 a 0.971. Tuttavia, dopo aver perfezionato il modello sulla nuova struttura, siamo in grado di recuperare dalla perdita di prestazioni, portando la precisione a 0.992. Poich√© i cambiamenti strutturali sono minori e graduali, la rete pu√≤ adattarsi pi√π facilmente a essi. Eseguendo lo stesso processo altre 2 volte, finiamo con una precisione finale di 0.991 (una perdita di solo lo 0.4% rispetto all‚Äôoriginale) e una riduzione del 27% nel numero di canali. Pertanto, la potatura iterativa ci consente di mantenere le prestazioni beneficiando di una maggiore efficienza computazionale dovuta alla riduzione delle dimensioni del modello.\n\n\n\n\n\n\nFigura¬†9.3: Potatura iterativa.\n\n\n\nLa potatura one-shot adotta un approccio pi√π aggressivo, potando una grande porzione di strutture simultaneamente in un‚Äôunica operazione in base a criteri di importanza predefiniti. Segue un‚Äôampia messa a punto per recuperare l‚Äôaccuratezza del modello. Sebbene pi√π rapida, questa strategia aggressiva pu√≤ degradare l‚Äôaccuratezza se il modello non riesce a recuperare durante la messa a punto.\nLa scelta tra queste strategie comporta la valutazione di fattori quali dimensioni del modello, quanto √® sparso il target, calcolo disponibile e perdite di accuratezza accettabili. La potatura one-shot pu√≤ comprimere rapidamente i modelli, ma quella iterativa pu√≤ consentire una migliore conservazione dell‚Äôaccuratezza per un livello target di potatura. In pratica, la strategia √® personalizzata in base ai vincoli del caso d‚Äôuso. L‚Äôobiettivo generale √® quello di generare una strategia ottimale che rimuova la ridondanza, ottenga guadagni di efficienza tramite la potatura e metta a punto il modello per stabilizzare l‚Äôaccuratezza a un livello accettabile per l‚Äôimplementazione.\nOra si consideri la stessa rete che avevamo nell‚Äôesempio di potatura iterativa. Mentre nel processo iterativo abbiamo potato 2 canali alla volta, nel processo one-shot poteremo i 6 canali contemporaneamente, come mostrato in Figura¬†9.4. La rimozione simultanea del 27% del canale della rete altera significativamente la struttura, causando un calo della precisione da 0.995 a 0.914. Date le modifiche principali, la rete non √® in grado di adattarsi correttamente durante la messa a punto e la precisione √® salita a 0.943, un degrado del 5% rispetto alla precisione della rete non potata. Mentre le strutture finali nei processi di potatura iterativa e di potatura one-shot sono identiche, la prima √® in grado di mantenere prestazioni elevate mentre la seconda subisce degradi significativi.\n\n\n\n\n\n\nFigura¬†9.4: Potatura one-shot.\n\n\n\n\n\n\nVantaggi della Potatura Strutturata\nLa potatura strutturata offre una miriade di vantaggi che soddisfano vari aspetti dell‚Äôimplementazione e dell‚Äôutilizzo del modello, specialmente in ambienti in cui le risorse computazionali sono limitate.\n\nEfficienza Computazionale: Eliminando intere strutture, come neuroni o canali, si riduce significativamente il carico computazionale durante le fasi di training e inferenza, consentendo cos√¨ previsioni pi√π rapide del modello e convergenza del training. Inoltre, la rimozione delle strutture riduce intrinsecamente il ‚Äúfootprint‚Äù [impronta] di memoria del modello, assicurando che richieda meno spazio di archiviazione e memoria durante il funzionamento, il che √® particolarmente vantaggioso in ambienti con limiti di memoria come i sistemi TinyML.\nEfficienza Hardware: La potatura strutturata spesso si traduce in modelli pi√π adatti all‚Äôimplementazione su hardware specializzato, come i Field-Programmable Gate Arrays (FPGA) o Application-Specific Integrated Circuits (ASIC), a causa della regolarit√† e la semplicit√† dell‚Äôarchitettura potata. Con requisiti di elaborazione ridotti, si traduce in un consumo energetico inferiore, fondamentale per i dispositivi alimentati a batteria e i metodi di elaborazione sostenibili.\nManutenzione e Distribuzione: Il modello ridotto, sebbene pi√π piccolo, mantiene la sua forma architettonica originale, che pu√≤ semplificare la pipeline di distribuzione e garantire la compatibilit√† con i sistemi e i framework esistenti. Inoltre, con meno parametri e strutture pi√π semplici, il modello potato diventa pi√π facile da gestire e monitorare negli ambienti di produzione, riducendo potenzialmente le spese generali associate alla manutenzione e agli aggiornamenti del modello. Pi√π avanti, quando approfondiremo MLOps, questa necessit√† diventer√† evidente.\n\n\n\nPotatura non Strutturata\nIl ‚Äúpruning‚Äù non-strutturato √®, come suggerisce il nome, la potatura del modello senza riguardo alla sotto-struttura specifica del modello. Come accennato in precedenza, offre una maggiore aggressivit√† nella potatura e pu√≤ raggiungere maggiori diradazione del modello mantenendo la precisione, dati meno vincoli su ci√≤ che pu√≤ e non pu√≤ essere potato. In genere, la potatura non-strutturata post-training consiste in un criterio di importanza per i singoli parametri/pesi del modello, potatura/rimozione dei pesi che scendono al di sotto dei criteri e una successiva messa a punto facoltativa per provare a recuperare la precisione persa durante la rimozione dei pesi.\nLa potatura non-strutturata presenta alcuni vantaggi rispetto a quella strutturata: la rimozione di singoli pesi anzich√© di intere sotto-strutture del modello spesso porta in pratica a minori diminuzioni della precisione del modello. Inoltre, in genere determinare il criterio di importanza per un singolo peso √® molto pi√π semplice che per un‚Äôintera sotto-struttura di parametri nella potatura strutturata, rendendo la prima preferibile nei casi in cui tale overhead √® difficile o poco chiaro da calcolare. Analogamente, il processo effettivo di potatura strutturata √® generalmente meno flessibile, poich√© la rimozione di singoli pesi √® generalmente pi√π semplice della rimozione di intere sotto-strutture e della garanzia che il modello funzioni ancora.\nLa potatura non strutturata, pur offrendo il potenziale per una significativa riduzione delle dimensioni del modello e una migliore implementabilit√†, porta con s√© problemi legati alla gestione di rappresentazioni sparse e alla garanzia dell‚Äôefficienza computazionale. √à particolarmente utile in scenari in cui √® fondamentale ottenere la massima compressione possibile del modello e in cui l‚Äôambiente di distribuzione pu√≤ gestire in modo efficiente i calcoli sparsi.\nTabella¬†9.1 fornisce un confronto conciso tra potatura strutturata e la non-strutturata. In questa tabella, gli aspetti relativi alla natura e all‚Äôarchitettura del modello potato (Definizione, Regolarit√† del modello e Livello di compressione) sono raggruppati insieme, seguiti dagli aspetti relativi alle considerazioni computazionali (Efficienza computazionale e Compatibilit√† hardware) e terminando con gli aspetti relativi all‚Äôimplementazione e all‚Äôadattamento del modello potato (Complessit√† di implementazione e Complessit√† di messa a punto). Entrambe le strategie di potatura offrono vantaggi e problemi unici, come mostrato in Tabella¬†9.1, e la selezione tra di esse dovrebbe essere influenzata da requisiti specifici del progetto e della distribuzione.\n\n\n\nTabella¬†9.1: Confronto tra potatura strutturata e non-strutturata.\n\n\n\n\n\n\n\n\n\n\nAspetto\nPotatura strutturata\nPotatura non strutturata\n\n\n\n\nDefinizione\nPotatura di intere strutture (ad esempio, neuroni, canali, layer) all‚Äôinterno della rete\nPotatura di singoli pesi o neuroni, con conseguenti matrici sparse o strutture di rete non regolari\n\n\nRegolarit√† del Modello\nMantiene un‚Äôarchitettura di rete regolare e strutturata\nSi traduce in architetture di rete irregolari e sparse\n\n\nLivello di Compressione\nPu√≤ offrire una compressione del modello limitata rispetto alla potatura non-strutturata\nPu√≤ ottenere una compressione del modello pi√π elevata grazie alla potatura a grana fine\n\n\nEfficienza Computazionale\nIn genere pi√π efficiente computazionalmente grazie al mantenimento di strutture regolari\nPu√≤ essere inefficiente dal punto di vista computazionale a causa di matrici di peso sparse, a meno che non venga utilizzato hardware/software specializzato\n\n\nCompatibilit√† Hardware\nIn genere pi√π compatibile con vari hardware grazie alle strutture regolari\nPotrebbe richiedere hardware che gestisca in modo efficiente i calcoli sparsi per ottenere vantaggi\n\n\nComplessit√† di Implementazione\nSpesso pi√π semplice da implementare e gestire grazie al mantenimento della struttura della rete\nPu√≤ essere complesso da gestire e calcolare a causa delle rappresentazioni sparse\n\n\nComplessit√† di Messa a Punto Fine\nPotrebbe richiedere strategie di messa a punto fine meno complesse dopo la potatura\nPotrebbe richiedere strategie di riaddestramento o messa a punto fine pi√π complesse dopo la potatura\n\n\n\n\n\n\nIn Figura¬†9.5 abbiamo esempi che illustrano le differenze tra potatura non-strutturata e strutturata. Osservare che la potatura non-strutturata pu√≤ portare a modelli che non rispettano pi√π le garanzie strutturali di alto livello delle loro controparti originali non potate: la rete di sinistra non √® pi√π una rete completamente connessa dopo la potatura. La potatura strutturata, d‚Äôaltro canto, mantiene quelle invarianti: al centro, la rete completamente connessa viene potata in modo che resti ancora completamente connessa; allo stesso modo, la CNN mantiene la sua struttura convoluzionale, sebbene con meno filtri.\n\n\n\n\n\n\nFigura¬†9.5: Potatura non-strutturata e strutturata. Fonte: Qi et al. (2021).\n\n\nQi, Chen, Shibo Shen, Rongpeng Li, Zhifeng Zhao, Qing Liu, Jing Liang, e Honggang Zhang. 2021. ¬´An efficient pruning scheme of deep neural networks for Internet of Things applications¬ª. EURASIP Journal on Advances in Signal Processing 2021 (1): 31. https://doi.org/10.1186/s13634-021-00744-4.\n\n\n\n\nIpotesi del Biglietto della Lotteria\nLa potatura si √® evoluta da una tecnica puramente post-addestramento che comportava un costo per una certa accuratezza, a un potente approccio di meta-apprendimento applicato durante l‚Äôaddestramento per ridurre la complessit√† del modello. Questo progresso a sua volta migliora l‚Äôefficienza di calcolo, memoria e latenza sia nell‚Äôaddestramento che nell‚Äôinferenza.\nUna scoperta rivoluzionaria che ha catalizzato questa evoluzione √® stata l‚Äôipotesi del biglietto della lotteria di Frankle e Carbin (2019). Il loro lavoro afferma che all‚Äôinterno di reti neurali dense esistono sotto-reti sparse, denominate ‚Äúbiglietti vincenti‚Äù, che possono eguagliare o addirittura superare le prestazioni del modello originale quando addestrate in isolamento. In particolare, questi biglietti vincenti, quando inizializzati utilizzando gli stessi pesi della rete originale, possono raggiungere una convergenza e un‚Äôaccuratezza di addestramento altrettanto elevate su un dato compito. Vale la pena sottolineare che hanno scoperto empiricamente l‚Äôipotesi del biglietto della lotteria, che √® stata successivamente formalizzata.\n\nFrankle, Jonathan, e Michael Carbin. 2019. ¬´The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks¬ª. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net. https://openreview.net/forum?id=rJl-b3RcF7.\nL‚Äôintuizione alla base di questa ipotesi √® che, durante il processo di addestramento di una rete neurale, molti neuroni e connessioni diventano ridondanti o non importanti, in particolare con l‚Äôinclusione di tecniche di addestramento che incoraggiano la ridondanza come il ‚Äúdropout‚Äù [abbandono]. L‚Äôidentificazione, la potatura e l‚Äôinizializzazione di questi ‚Äúbiglietti vincenti‚Äù consentono un addestramento pi√π rapido e modelli pi√π efficienti, poich√© contengono le informazioni essenziali per la decisione del modello per l‚Äôattivit√†. Inoltre, come generalmente noto con la teoria del ‚Äúbias-variance tradeoff‚Äù [compromesso tra bias e varianza], questi biglietti soffrono meno di sovra-parametrizzazione e quindi si generalizzano meglio piuttosto che sovra-adattarsi all‚Äôattivit√†.\nIn Figura¬†9.6 abbiamo un esempio che mostra esperimenti di potatura e addestramento su una LeNet completamente connessa su una variet√† di rapporti di potatura. Nel grafico a sinistra, si nota come una potatura pesante riveli una sotto-rete pi√π efficiente (in verde) che √® il 21,1% delle dimensioni della rete originale (in blu). La sotto-rete raggiunge una maggiore accuratezza e in modo pi√π rapido rispetto alla versione non potata (la linea verde √® sopra la linea blu). Tuttavia, la potatura ha un limite (punto ottimale) e un‚Äôulteriore potatura produrr√† degradi delle prestazioni e alla fine scender√† al di sotto delle prestazioni della versione non potata (nota come le sotto-reti rossa, viola e marrone diminuiscono gradualmente nelle prestazioni di accuratezza) a causa della significativa perdita nel numero di parametri.\n\n\n\n\n\n\nFigura¬†9.6: Esperimenti sull‚Äôipotesi del biglietto della lotteria.\n\n\n\nPer scoprire questi biglietti vincenti della lotteria all‚Äôinterno di una rete neurale, viene seguito un processo sistematico. Questo processo, illustrato in Figura¬†9.7 (a sinistra), prevede l‚Äôaddestramento iterativo, la potatura e la reinizializzazione della rete. I passaggi seguenti delineano questo approccio:\n\nInizializzare i pesi della rete a valori casuali.\nAddestrare la rete finch√© non converge alle prestazioni desiderate.\nEliminare una percentuale di rami con i valori di peso pi√π bassi.\nReinizializzare la rete con gli stessi valori casuali del passaggio 1.\nRipetere i passaggi 2-4 pi√π volte o finch√© la precisione non peggiora in modo significativo.\n\nAlla fine, ci si ritrova con una rete potata (Figura¬†9.7 lato destro), che √® una sotto-rete di quella di partenza. La sotto-rete dovrebbe avere una struttura significativamente pi√π piccola, pur mantenendo un livello di precisione comparabile.\n\n\n\n\n\n\nFigura¬†9.7: Trovare la sottorete del biglietto vincente.\n\n\n\n\n\nProblemi e Limitazioni\nNon c‚Äô√® niente di gratuito con le ottimizzazioni di potatura, con alcune scelte che comportano sia miglioramenti che costi da considerare. Di seguito, discutiamo alcuni compromessi che gli esperti devono considerare.\n\nGestione di Matrici di Peso Sparse: Una matrice di peso sparsa √® una matrice in cui molti degli elementi sono pari a zero. La potatura non strutturata spesso produce matrici di peso sparse, in cui molti pesi vengono potati a zero. Sebbene ci√≤ riduca le dimensioni del modello, introduce anche diversi problemi. L‚Äôinefficienza computazionale pu√≤ sorgere perch√© l‚Äôhardware standard √® ottimizzato per operazioni di matrice densa. Senza ottimizzazioni che sfruttano la sparsit√†, i risparmi computazionali derivanti dalla potatura possono essere persi. Sebbene le matrici sparse possano essere archiviate senza formati specializzati, sfruttare efficacemente la loro sparsit√† richiede una gestione attenta per evitare di sprecare risorse. Algoritmicamente, la navigazione in strutture sparse richiede di saltare in modo efficiente le voci zero, il che aggiunge complessit√† al calcolo e agli aggiornamenti del modello.\nQualit√† vs.¬†Riduzione delle Dimensioni: Una sfida fondamentale sia nella potatura strutturata che in quella non-strutturata √® bilanciare la riduzione delle dimensioni con il mantenimento o il miglioramento delle prestazioni predittive. √à essenziale stabilire criteri di potatura robusti, sia per rimuovere intere strutture (potatura strutturata) sia singoli pesi (potatura non strutturata). Questi criteri di potatura scelti devono identificare accuratamente gli elementi la cui rimozione ha un impatto minimo sulle prestazioni. Spesso √® necessaria un‚Äôattenta sperimentazione per garantire che il modello potato rimanga efficiente mantenendo al contempo le sue prestazioni predittive.\nFine-Tuning e Riaddestramento: La messa a punto post-potatura √® fondamentale sia nella potatura strutturata che in quella non-strutturata per recuperare le prestazioni perse e stabilizzare il modello. La sfida comprende la determinazione dell‚Äôestensione, della durata e della natura del processo di messa a punto, che pu√≤ essere influenzato dal metodo di potatura e dal grado di potatura applicato.\nCompatibilit√† ed Efficienza Hardware: Particolarmente pertinenti alla potatura non-strutturata, la compatibilit√† e l‚Äôefficienza hardware diventano critiche. La potatura non strutturata spesso si traduce in matrici di peso sparse, che potrebbero non essere gestite in modo efficiente da un certo hardware, annullando potenzialmente i vantaggi computazionali della potatura (vedere Figura¬†9.8). Garantire che i modelli potati, in particolare quelli risultanti dall‚Äôeliminazione non-strutturata, siano scalabili, compatibili ed efficienti sull‚Äôhardware target √® una considerazione importante.\nConsiderazioni Legali ed Etiche: Ultimo ma non meno importante, il rispetto delle linee guida legali ed etiche √® importante, soprattutto in ambiti con conseguenze significative. I metodi di potatura devono essere sottoposti a rigorosi processi di validazione, test e potenzialmente certificazione per garantire la conformit√† alle normative e agli standard pertinenti, sebbene al momento non esistano standard formali e ‚Äúbest practice‚Äù che siano esaminati e convalidati da entit√† terze. Ci√≤ √® particolarmente cruciale in applicazioni ad alto rischio come l‚Äôintelligenza artificiale medica e la guida autonoma, dove i cali di qualit√† dovuti a ottimizzazioni simili alla potatura possono essere pericolosi per la vita. Inoltre, le considerazioni etiche si estendono oltre la sicurezza fino all‚Äôequit√† e all‚Äôuguaglianza; un recente lavoro di (Tran et al. 2022) ha rivelato che la potatura pu√≤ avere un impatto sproporzionato sulle persone di colore, sottolineando la necessit√† di una valutazione etica completa nel processo di potatura.\n\n\nTran, Cuong, Ferdinando Fioretto, Jung-Eun Kim, e Rakshit Naidu. 2022. ¬´Pruning has a disparate impact on model accuracy¬ª. Adv Neural Inf Process Syst 35: 17652‚Äì64.\n\n\n\n\n\n\nFigura¬†9.8: Matrice dei pesi sparsi.\n\n\n\n\n\n\n\n\n\nEsercizio¬†9.1: Pruning\n\n\n\n\n\nSi immagini che la rete neurale sia un cespuglio gigante e troppo cresciuto. La potatura √® come tagliare strategicamente i rami per renderla pi√π forte ed efficiente! Nel Colab, si imparer√† come fare questa potatura in TensorFlow. La comprensione di questi concetti fornir√† le basi per vedere come la potatura rende i modelli abbastanza piccoli da poter essere eseguiti sul telefono!\n\n\n\n\n\n\n\n9.2.2 Compressione del Modello\nLe tecniche di compressione del modello sono fondamentali per distribuire modelli di deep learning su dispositivi con risorse limitate. Queste tecniche mirano a creare modelli pi√π piccoli ed efficienti che preservino le prestazioni predittive dei modelli originali.\n\nDistillazione della Conoscenza\nUna tecnica popolare √® la knowledge distillation (KD) distillazione della conoscenza, che trasferisce la conoscenza da un modello ‚Äúinsegnante‚Äù ampio e complesso a un modello ‚Äústudente‚Äù pi√π piccolo. L‚Äôidea chiave √® addestrare il modello studente a imitare gli output dell‚Äôinsegnante. Il concetto di KD √® stato reso popolare per la prima volta da Hinton (2005).\n\nHinton, Geoffrey. 2005. ¬´Van Nostrand‚Äôs Scientific Encyclopedia¬ª. Wiley. https://doi.org/10.1002/0471743984.vse0673.\n\nPanoramica e Vantaggi\nLa distillazione della conoscenza implica il trasferimento della conoscenza da un modello insegnante ampio e complesso a un modello studente pi√π piccolo. L‚Äôidea di base √® quella di utilizzare gli output dell‚Äôinsegnante, noti come soft targets, per guidare il training del modello studente. A differenza dei tradizionali ‚Äúhard targets‚Äù (le vere etichette), quelli soft sono le distribuzioni di probabilit√† sulle classi che il modello insegnante prevede. Queste distribuzioni forniscono informazioni pi√π complete sulle relazioni tra le classi, il che pu√≤ aiutare il modello studente ad apprendere in modo pi√π efficace.\nAbbiamo imparato che la funzione softmax converte gli output grezzi di un modello in una distribuzione di probabilit√† sulle classi. Una tecnica chiave in KD √® la scalatura della temperatura, che viene applicata alla funzione softmax degli output del modello insegnante. Introducendo un parametro di temperatura, la distribuzione pu√≤ essere regolata: una temperatura pi√π alta produce probabilit√† pi√π soft, il che significa che le differenze tra le probabilit√† di classe diventano meno estreme. Questo effetto di ammorbidimento determina una distribuzione pi√π uniforme, in cui la fiducia del modello nella classe pi√π probabile √® ridotta e altre classi hanno probabilit√† pi√π elevate, diverse da zero. Ci√≤ √® prezioso per il modello studente perch√© gli consente di apprendere non solo dalla classe pi√π probabile, ma anche dalle probabilit√† relative di tutte le classi, catturando pattern sottili che potrebbero essere persi se addestrati solo su obiettivi difficili. Pertanto, la scalabilit√† della temperatura facilita il trasferimento di conoscenze pi√π sfumate dal modello insegnante a quello studente.\nLa funzione di perdita nella distillazione della conoscenza in genere combina due componenti: una perdita di distillazione e una perdita di classificazione. La perdita di distillazione, spesso calcolata utilizzando la divergenza di Kullback-Leibler (KL), misura la differenza tra gli soft target prodotti dal modello insegnante e gli output del modello studente, incoraggiando lo studente a imitare le previsioni dell‚Äôinsegnante. Nel frattempo, la perdita di classificazione assicura che il modello studente preveda correttamente le etichette vere in base ai dati originali. Insieme, queste due componenti aiutano lo studente modello a conservare le conoscenze dell‚Äôinsegnante, rispettando al contempo le etichette di verit√† di base.\nQuesti componenti, quando configurati e armonizzati abilmente, consentono al modello studente di assimilare la conoscenza del modello insegnante, creando un percorso verso modelli pi√π piccoli, efficienti e robusti, che mantengono la capacit√† predittiva delle loro controparti pi√π grandi. Figura¬†9.9 visualizza la procedura di training della ‚Äúknowledge distillation‚Äù. Notare come i logit o le soft label del modello insegnante vengono utilizzati per fornire una perdita di distillazione da cui il modello studente pu√≤ imparare.\n\n\n\n\n\n\nFigura¬†9.9: Processo di training della distillazione della conoscenza. Fonte: IntelLabs (2023).\n\n\nIntelLabs. 2023. ¬´Knowledge Distillation - Neural Network Distiller¬ª. https://intellabs.github.io/distiller/knowledge_distillation.html.\n\n\n\n\nSfide\nTuttavia, KD presenta una serie unica di sfide e considerazioni che ricercatori e professionisti devono affrontare attentamente. Una delle sfide √® nella messa a punto meticolosa degli iperparametri, come il parametro ‚Äútemperatura‚Äù nella funzione softmax e la ponderazione tra la distillazione e la perdita di classificazione nella funzione obiettivo. Raggiungere un equilibrio che sfrutti efficacemente gli output ammorbiditi del modello insegnante mantenendo al contempo la fedelt√† alle etichette dei dati reali non √® banale e pu√≤ avere un impatto significativo sulle prestazioni e sulle capacit√† di generalizzazione del modello studente.\nInoltre, l‚Äôarchitettura del modello studente stesso pone un problema considerevole. Progettare un modello compatto per soddisfare i vincoli di calcolo e memoria, pur essendo in grado di assimilare le conoscenze essenziali dal modello insegnante, richiede una comprensione sfumata della capacit√† del modello e dei compromessi intrinseci coinvolti nella compressione. Il modello studente deve essere attentamente progettato per navigare nella dicotomia di dimensioni e prestazioni, assicurando che la conoscenza distillata venga catturata e utilizzata in modo significativo. Inoltre, la scelta del modello dell‚Äôinsegnante, che influenza intrinsecamente la qualit√† e la natura della conoscenza da trasferire, √® importante e introduce un ulteriore livello di complessit√† nel processo KD.\nQueste sfide sottolineano la necessit√† di un approccio completo e sfumato all‚Äôimplementazione di KD, assicurando che i modelli degli studenti risultanti siano sia efficienti che efficaci nei loro contesti operativi.\n\n\n\nFattorizzazione di Matrici di Basso Rango\nSimile nel tema dell‚Äôapprossimazione, la Low-Rank Matrix Factorization (LRMF) fattorizzazione di matrici di basso rango √® una tecnica matematica utilizzata in algebra lineare e analisi dei dati per approssimare una matrice data scomponendola in due o pi√π matrici di dimensione inferiore. L‚Äôidea fondamentale √® di esprimere una matrice di grandi dimensioni come prodotto di matrici di rango inferiore, il che pu√≤ aiutare a ridurre la complessit√† dei dati preservandone la struttura essenziale. Matematicamente, data una matrice \\(A \\in \\mathbb{R}^{m \\times n}\\), LRMF cercare le matrici \\(U \\in \\mathbb{R}^{m \\times k}\\) e \\(V \\in \\mathbb{R}^{k \\times n}\\) tali che \\(A \\approx UV\\), dove \\(k\\) √® il rango ed √® in genere molto pi√π piccolo di \\(m\\) e \\(n\\).\n\nBackground e Benefici\nUno dei primo lavori nel campo della fattorizzazione di matrici, in particolare nel contesto dei sistemi di raccomandazione, √® il documento di Koren, Bell, e Volinsky (2009). Gli autori esaminano vari modelli di fattorizzazione, fornendo approfondimenti sulla loro efficacia nel catturare i pattern sottostanti nei dati e nel migliorare l‚Äôaccuratezza predittiva nel filtraggio collaborativo. LRMF √® stato ampiamente applicato nei sistemi di raccomandazione (come Netflix, Facebook, ecc.), dove la matrice di interazione utente-elemento √® fattorizzata per catturare fattori latenti corrispondenti alle preferenze dell‚Äôutente e agli attributi dell‚Äôelemento.\n\nKoren, Yehuda, Robert Bell, e Chris Volinsky. 2009. ¬´Matrix Factorization Techniques for Recommender Systems¬ª. Computer 42 (8): 30‚Äì37. https://doi.org/10.1109/mc.2009.263.\nIl vantaggio principale della ‚Äúfattorizzazione di matrici di basso rango‚Äù risiede nella sua capacit√† di ridurre la dimensionalit√† dei dati come mostrato in Figura¬†9.10, dove ci sono meno parametri da memorizzare, rendendola pi√π efficiente dal punto di vista computazionale e riducendo i requisiti di archiviazione a costo di un po‚Äô di elaborazione aggiuntiva. Ci√≤ pu√≤ portare a calcoli pi√π rapidi e rappresentazioni di dati pi√π compatte, il che √® particolarmente prezioso quando si ha a che fare con grandi set di dati. Inoltre, pu√≤ aiutare nella riduzione del rumore e pu√≤ rivelare pattern e relazioni sottostanti nei dati.\nFigura¬†9.10 illustra la diminuzione della parametrizzazione abilitata dalla fattorizzazione di matrici di basso rango. Osservare come la matrice \\(M\\) pu√≤ essere approssimata dal prodotto delle matrici \\(L_k\\) e \\(R_k^T\\). Per intuizione, la maggior parte dei layer completamente connessi nelle reti sono archiviati come matrice di proiezione \\(M\\), che richiede il caricamento di \\(m \\times n\\) parametri durante il calcolo. Tuttavia, scomponendola e approssimandola come prodotto di due matrici di rango inferiore, abbiamo bisogno di archiviare solo \\(m \\times k + k\\times n\\) parametri in termini di archiviazione, sostenendo al contempo un costo di calcolo aggiuntivo per la moltiplicazione delle matrici. Finch√© \\(k &lt; n/2\\), questa fattorizzazione ha meno parametri totali da archiviare, aggiungendo un calcolo di runtime \\(O(mkn)\\) (Gu 2023).\n\nGu, Ivy. 2023. ¬´Deep Learning Model Compression (ii) by Ivy Gu Medium¬ª. https://ivygdy.medium.com/deep-learning-model-compression-ii-546352ea9453.\n\n\n\n\n\n\nFigura¬†9.10: Fattorizzazione di matrici di basso rango. Fonte: The Clever Machine.\n\n\n\n\n\nSfide\nMa professionisti e ricercatori incontrano una serie di problemi e considerazioni che richiedono una particolare attenzione e approcci strategici. Come con qualsiasi tecnica di compressione lossy [con perdita], potremmo perdere informazioni durante questo processo di approssimazione: scegliere il rango corretto che bilanci le informazioni perse e i costi computazionali √® altrettanto complicato e aggiunge un ulteriore iperparametro da regolare.\nLa fattorizzazione di matrici di basso rango √® uno strumento prezioso per la riduzione della dimensionalit√† e per adattare il calcolo ai dispositivi edge ma, come altre tecniche, deve essere attentamente regolata in base al modello e all‚Äôattivit√† da svolgere. Una sfida fondamentale risiede nella gestione della complessit√† computazionale inerente a LRMF, soprattutto quando si hanno a che fare con dati ad alta dimensionalit√† e su larga scala. L‚Äôonere computazionale, in particolare nel contesto di applicazioni in tempo reale e set di dati massicci, rimane un ostacolo significativo per un utilizzo efficace di LRMF.\nInoltre, l‚Äôenigma della scelta del rango ottimale \\(k\\) per la fattorizzazione introduce un ulteriore livello di complessit√†. La selezione di \\(k\\) implica intrinsecamente un compromesso tra accuratezza dell‚Äôapprossimazione e semplicit√† del modello, e l‚Äôidentificazione di un rango che bilanci abilmente questi obiettivi contrastanti spesso richiede una combinazione di competenza di dominio, convalida empirica e, a volte, approcci euristici. La sfida √® ulteriormente amplificata quando i dati comprendono rumore o quando la struttura intrinseca di basso rango non √® pronunciata, rendendo la determinazione di un \\(k\\) adatto ancora pi√π sfuggente.\nLa gestione di dati mancanti o sparsi, un evento comune in applicazioni come i sistemi di raccomandazione, pone un‚Äôaltra sfida sostanziale. Le tecniche tradizionali di fattorizzazione delle matrici, come la Singular Value Decomposition (SVD), non sono direttamente applicabili alle matrici con voci mancanti, rendendo necessario lo sviluppo e l‚Äôapplicazione di algoritmi specializzati in grado di fattorizzare matrici incomplete mitigando al contempo i rischi di overfitting alle voci osservate. Ci√≤ spesso comporta l‚Äôincorporazione di termini di regolarizzazione o la limitazione della fattorizzazione in modi specifici, il che a sua volta introduce ulteriori iperparametri che devono essere selezionati giudiziosamente.\nInoltre, in scenari in cui i dati evolvono o crescono nel tempo, sviluppare modelli LRMF in grado di adattarsi a nuovi dati senza richiedere una completa rifattorizzazione √® un‚Äôimpresa critica ma impegnativa. Gli algoritmi di fattorizzazione di matrici incrementali e online cercano di risolvere questo problema consentendo l‚Äôaggiornamento delle matrici fattorizzate all‚Äôarrivo di nuovi dati, ma garantire stabilit√†, accuratezza ed efficienza computazionale in queste impostazioni dinamiche rimane un compito intricato. Ci√≤ √® particolarmente impegnativo nello spazio di TinyML, in cui la ridistribuzione dei rami per i modelli aggiornati pu√≤ essere piuttosto impegnativa.\n\n\n\nDecomposizione dei Tensori\nAbbiamo visto in Sezione 6.4.1 che i tensori sono strutture flessibili, comunemente utilizzate dai framework ML, che possono rappresentare dati in dimensioni superiori. Similmente alla fattorizzazione di matrici di basso rango, i modelli pi√π complessi possono memorizzare pesi in dimensioni superiori, come i tensori. La decomposizione tensoriale √® l‚Äôanalogo a pi√π dimensioni della fattorizzazione della matrice, in cui un tensore modello viene scomposto in componenti di rango inferiore (Figura¬†9.11). Questi componenti di rango inferiore sono pi√π facili da calcolare e memorizzare, ma possono soffrire degli stessi problemi menzionati sopra, come la perdita di informazioni e la necessit√† di una messa a punto sfumata degli iperparametri. Matematicamente, dato un tensore \\(\\mathcal{A}\\), la decomposizione tensoriale cerca di rappresentare \\(\\mathcal{A}\\) come una combinazione di tensori pi√π semplici, facilitando una rappresentazione compressa che approssima i dati originali riducendo al minimo la perdita di informazioni.\n\n\n\n\n\n\nFigura¬†9.11: Decomposizione dei Tensori. Fonte: Xinyu (s.d.).\n\n\nXinyu, Chen. s.d.\n\n\nIl lavoro di Tamara G. Kolda e Brett W. Bader, ‚ÄúTensor Decompositions and Applications‚Äù (2009), si distingue come un articolo fondamentale nel campo delle decomposizioni tensoriali. Gli autori forniscono una panoramica completa di vari metodi di decomposizione tensoriale, esplorandone i fondamenti matematici, gli algoritmi e un‚Äôampia gamma di applicazioni, che vanno dall‚Äôelaborazione del segnale al data mining. Naturalmente, il motivo per cui ne stiamo discutendo √® perch√© ha un enorme potenziale per i miglioramenti delle prestazioni del sistema, in particolare nello spazio di TinyML, dove la produttivit√† e i risparmi di memoria sono fondamentali per la fattibilit√† delle distribuzioni.\n\n\n\n\n\n\nEsercizio¬†9.2: Compressione di Modelli Scalabili con TensorFlow\n\n\n\n\n\nQuesto Colab si addentra in una tecnica per comprimere i modelli mantenendo un‚Äôelevata accuratezza. L‚Äôidea chiave √® quella di addestrare un modello con un termine di penalit√† extra che incoraggia il modello a essere pi√π comprimibile. Quindi, il modello viene codificato utilizzando uno schema di codifica speciale che si allinea con questa penalit√†. Questo approccio consente di ottenere modelli compressi che funzionano altrettanto bene dei modelli originali ed √® utile per distribuire modelli su dispositivi con risorse limitate come telefoni cellulari e dispositivi edge.\n\n\n\n\n\n\n\n9.2.3 Modelli Progettati per l‚ÄôEdge\nOra raggiungiamo l‚Äôaltro estremo del gradiente hardware-software, dove prendiamo decisioni specifiche sull‚Äôarchitettura del modello direttamente in base alla conoscenza dei dispositivi edge su cui desideriamo implementare.\nCome spiegato nelle sezioni precedenti, i dispositivi edge sono vincolati specificamente da limitazioni di memoria e calcoli parallelizzabili: in quanto tali, se ci sono requisiti critici di velocit√† di inferenza, i calcoli devono essere sufficientemente flessibili da soddisfare i vincoli hardware, qualcosa che pu√≤ essere progettato a livello di architettura del modello. Inoltre, cercare di stipare grandi modelli SOTA ML su dispositivi edge anche dopo potatura e compressione √® generalmente irrealizzabile puramente a causa delle dimensioni: la complessit√† del modello stesso deve essere scelta con pi√π sfumature per adattarsi pi√π fattibilmente al dispositivo. Gli sviluppatori di Edge ML hanno affrontato questa sfida architettonica sia attraverso la progettazione di architetture di modelli edge ML su misura sia attraverso la Neural Architecture Search (NAS) [ricerca di architettura neurale] avente il dispositivo come target, che pu√≤ generare in modo pi√π sistematico architetture fattibili di modelli su dispositivo.\n\nTecniche di Progettazione del Modello\nUn design di architettura edge friendly, comunemente utilizzato nel deep learning per l‚Äôelaborazione delle immagini, √® quello delle convoluzioni separabili in profondit√†. Consiste in due fasi distinte: la prima √® la convoluzione in profondit√†, in cui ogni canale di input viene convoluto in modo indipendente con il proprio set di filtri apprendibili, come mostrato in Figura¬†9.12. Questa fase riduce la complessit√† computazionale in modo significativo rispetto alle convoluzioni standard, poich√© riduce drasticamente il numero di parametri e calcoli coinvolti. La seconda fase √® la convoluzione puntuale, che combina l‚Äôoutput dei canali di convoluzione in profondit√† tramite una convoluzione 1x1, creando interazioni tra canali. Questo approccio offre diversi vantaggi. I vantaggi includono dimensioni ridotte del modello, tempi di inferenza pi√π rapidi e spesso una migliore generalizzazione grazie al minor numero di parametri, rendendolo adatto ad applicazioni mobili ed embedded. Tuttavia, le convoluzioni separabili in profondit√† potrebbero non catturare interazioni spaziali complesse in modo efficace come le convoluzioni standard e potrebbero richiedere pi√π profondit√† (livelli) per raggiungere lo stesso livello di potenza rappresentativa, portando potenzialmente a tempi di addestramento pi√π lunghi. Tuttavia, la loro efficienza in termini di parametri e calcolo le rende una scelta popolare nelle moderne architetture di reti neurali convoluzionali.\n\n\n\n\n\n\nFigura¬†9.12: Convoluzioni separabili in profondit√†. Fonte: Hegde (2023).\n\n\nHegde, Sumant. 2023. ¬´An Introduction to Separable Convolutions - Analytics Vidhya¬ª. https://www.analyticsvidhya.com/blog/2021/11/an-introduction-to-separable-convolutions/.\n\n\n\n\nArchitetture di Modello di Esempio\nIn quest‚Äôottica, diverse architetture recenti sono state, fin dall‚Äôinizio, progettate specificamente per massimizzare la precisione in un‚Äôimplementazione edge, in particolare SqueezeNet, MobileNet ed EfficientNet.\n\nSqueezeNet di Iandola et al. (2016), ad esempio, utilizza un‚Äôarchitettura compatta con convoluzioni 1x1 e moduli ‚Äúfire‚Äù per ridurre al minimo il numero di parametri mantenendo al contempo una forte accuratezza.\nMobileNet di Howard et al. (2017), d‚Äôaltra parte, impiega le suddette convoluzioni separabili in profondit√† per ridurre sia il calcolo che le dimensioni del modello.\nEfficientNet di Tan e Le (2023) adotta un approccio diverso ottimizzando il ridimensionamento della rete (ovvero variando la profondit√†, la larghezza e la risoluzione di una rete) e il ridimensionamento composto, una variazione pi√π sfumata del ridimensionamento della rete, per ottenere prestazioni superiori con meno parametri.\n\n\nIandola, Forrest N, Song Han, Matthew W Moskewicz, Khalid Ashraf, William J Dally, e Kurt Keutzer. 2016. ¬´SqueezeNet: Alexnet-level accuracy with 50x fewer parameters and 0.5 MB model size¬ª. ArXiv preprint abs/1602.07360. https://arxiv.org/abs/1602.07360.\n\nHoward, Andrew G., Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, e Hartwig Adam. 2017. ¬´MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications¬ª. ArXiv preprint. https://arxiv.org/abs/1704.04861.\n\nTan, Mingxing, e Quoc V. Le. 2023. ¬´Demystifying Deep Learning¬ª. Wiley. https://doi.org/10.1002/9781394205639.ch6.\nQuesti modelli sono essenziali nel contesto dell‚Äôedge computing in cui la limitazione di potenza di elaborazione e di memoria richiede modelli leggeri ma efficaci in grado di eseguire in modo efficiente attivit√† quali il riconoscimento delle immagini, il rilevamento di oggetti e altro ancora. I loro principi di progettazione mostrano l‚Äôimportanza di un‚Äôarchitettura di modelli intenzionalmente personalizzata per l‚Äôedge computing, in cui prestazioni ed efficienza devono rientrare nei vincoli.\n\n\nSemplificazione della Ricerca di Architetture di Modelli\nInfine, per affrontare la sfida di trovare architetture di modelli efficienti che siano compatibili con i dispositivi edge, i ricercatori hanno sviluppato pipeline sistematizzate che semplificano la ricerca di progetti performanti. Due framework degni di nota in questo spazio sono TinyNAS di J. Lin et al. (2020) e MorphNet di Gordon et al. (2018), che automatizzano il processo di ottimizzazione delle architetture di reti neurali per l‚Äôimplementazione edge.\n\nGordon, Ariel, Elad Eban, Ofir Nachum, Bo Chen, Hao Wu, Tien-Ju Yang, e Edward Choi. 2018. ¬´MorphNet: Fast &amp; Simple Resource-Constrained Structure Learning of Deep Networks¬ª. In 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, 1586‚Äì95. IEEE. https://doi.org/10.1109/cvpr.2018.00171.\nTinyNAS √® un innovativo framework di ricerca di architetture neurali introdotto nel documento MCUNet, progettato per scoprire in modo efficiente architetture di reti neurali leggere per dispositivi edge con risorse computazionali limitate. Sfruttando l‚Äôapprendimento per rinforzo e uno spazio di ricerca compatto di micromoduli neurali, TinyNAS ottimizza sia l‚Äôaccuratezza che la latenza, consentendo l‚Äôimplementazione di modelli di deep learning su microcontrollori, dispositivi IoT e altre piattaforme con risorse limitate. Nello specifico, TinyNAS, in combinazione con un ottimizzatore di rete, TinyEngine, genera diversi spazi di ricerca ridimensionando la risoluzione di input e la larghezza del modello, poi raccoglie la distribuzione FLOP di calcolo delle reti soddisfacenti all‚Äôinterno dello spazio di ricerca per valutarne la priorit√†. TinyNAS si basa sul presupposto che uno spazio di ricerca che ospita FLOP pi√π elevati con vincoli di memoria possa produrre modelli di accuratezza pi√π elevata, cosa che gli autori hanno verificato in pratica nel loro lavoro. In termini di prestazioni empiriche, TinyEngine ha ridotto l‚Äôutilizzo di memoria di picco dei modelli di circa 3.4 volte e ha accelerato l‚Äôinferenza da 1.7 a 3.3 volte rispetto a TFLite e a CMSIS-NN.\nAnalogamente, MorphNet √® un framework di ottimizzazione delle reti neurali progettato per rimodellare e trasformare automaticamente l‚Äôarchitettura delle reti neurali profonde, ottimizzandole per requisiti di distribuzione specifici. Ci√≤ avviene in due fasi: in primo luogo, sfrutta un set di operazioni di morphing della rete personalizzabili, come l‚Äôampliamento o l‚Äôapprofondimento dei layer, per regolare dinamicamente la struttura della rete. Queste operazioni consentono alla rete di adattarsi a vari vincoli computazionali, tra cui dimensioni del modello, latenza e obiettivi di accuratezza, che sono estremamente diffusi nell‚Äôutilizzo dell‚Äôedge computing. Nella seconda fase, MorphNet utilizza un approccio basato sull‚Äôapprendimento di rinforzo per cercare la permutazione ottimale delle operazioni di morphing, bilanciando efficacemente il compromesso tra dimensioni del modello e prestazioni. Questo metodo innovativo consente ai professionisti del deep learning di adattare automaticamente le architetture delle reti neurali a requisiti hardware e applicativi specifici, garantendo un‚Äôimplementazione efficiente ed efficace su diverse piattaforme.\nTinyNAS e MorphNet rappresentano alcuni dei numerosi progressi significativi nel campo dell‚Äôottimizzazione sistematica delle reti neurali, consentendo di scegliere e generare sistematicamente architetture per adattarsi perfettamente ai vincoli del problema.\n\n\n\n\n\n\nEsercizio¬†9.3: Modelli Progettati per l‚ÄôEdge\n\n\n\n\n\nSi Immagini di costruire un piccolo robot in grado di identificare diversi fiori. Deve essere intelligente, ma anche piccolo ed efficiente dal punto di vista energetico! Nel mondo dell‚Äô‚ÄúEdge-Aware Model Design‚Äù, abbiamo appreso tecniche come le convoluzioni separabili in base alla profondit√† e architetture come SqueezeNet, MobileNet ed EfficientNet, tutte progettate per concentrare l‚Äôintelligenza in modelli compatti. Ora, vediamo queste idee in azione con alcuni xColab:\nSqueezeNet in Action: Forse piacerebbe un Colab che mostra come addestrare un modello SqueezeNet su un set di dati di immagini di fiori. Ci√≤ dimostrerebbe le sue piccole dimensioni e come impara a riconoscere i pattern nonostante la sua efficienza.\n\nMobileNet Exploration: Ci si √® mai chiesto se quei piccoli modelli di immagini sono buoni quanto quelli grandi? Scopriamolo! In questo Colab, mettiamo a confronto MobileNet, il campione dei pesi leggeri, con un modello di classificazione delle immagini classico. Li faremo gareggiare per la velocit√†, misureremo le loro esigenze di memoria e vedremo chi vincer√† per accuratezza. Preparatevi per una battaglia di cervelli di immagini!",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Ottimizzazioni dei Modelli</span>"
    ]
  },
  {
    "objectID": "contents/core/optimizations/optimizations.it.html#sec-model_ops_numerics",
    "href": "contents/core/optimizations/optimizations.it.html#sec-model_ops_numerics",
    "title": "9¬† Ottimizzazioni dei Modelli",
    "section": "9.3 Rappresentazione Numerica Efficiente",
    "text": "9.3 Rappresentazione Numerica Efficiente\nLa rappresentazione numerica implica una miriade di considerazioni, tra cui, ma non solo, la precisione dei numeri, i loro formati di codifica e le operazioni aritmetiche facilitate. Implica invariabilmente una vasta gamma di diversi compromessi, in cui i professionisti sono incaricati di destreggiarsi tra accuratezza numerica ed efficienza computazionale. Ad esempio, mentre i numeri a bassa precisione possono offrire il fascino di un utilizzo di memoria ridotto e calcoli accelerati, presentano contemporaneamente sfide relative alla stabilit√† numerica e al potenziale degrado dell‚Äôaccuratezza del modello.\n\nMotivazione\nEmerge l‚Äôimperativo per una rappresentazione numerica efficiente, in particolare perch√© l‚Äôottimizzazione efficiente del modello da sola non √® sufficiente quando si adattano i modelli per l‚Äôimplementazione su dispositivi edge a bassa potenza che operano con vincoli rigorosi.\nOltre a ridurre al minimo le richieste di memoria, l‚Äôenorme potenziale di una rappresentazione numerica efficiente risiede, ma non √® limitato a, queste modalit√† fondamentali. Riducendo l‚Äôintensit√† computazionale, la matematica efficiente pu√≤ amplificare la velocit√† computazionale, consentendo di elaborare modelli pi√π complessi su dispositivi a bassa potenza. Ridurre la precisione in bit di pesi e attivazioni su modelli fortemente sovra-parametrizzati consente la condensazione delle dimensioni del modello per dispositivi edge senza danneggiare significativamente l‚Äôaccuratezza predittiva del modello. Con l‚Äôonnipresenza delle reti neurali nei modelli, la matematica efficiente ha un vantaggio unico nello sfruttare la struttura a layer delle NN per variare la precisione numerica tra i layer, riducendo al minimo la precisione nei layer resistenti e preservando una maggiore precisione in quelli sensibili.\nIn questa sezione, approfondiremo il modo in cui i professionisti possono sfruttare i principi della progettazione congiunta hardware-software ai livelli pi√π bassi di un modello per facilitare la compatibilit√† con i dispositivi edge. Iniziando con un‚Äôintroduzione ai numeri, esamineremo le sue implicazioni per la memoria del dispositivo e la complessit√† computazionale. Successivamente, intraprenderemo una discussione sui compromessi implicati nell‚Äôadozione di questa strategia, seguita da un‚Äôanalisi approfondita di un metodo fondamentale della matematica efficiente: la quantizzazione.\n\n\n9.3.1 Le Basi\n\nI Tipi\nI dati numerici, il fondamento su cui si basano i modelli di apprendimento automatico, si manifestano in due forme principali. Si tratta di numeri interi e numeri in virgola mobile.\nNumeri Interi: Numeri interi, privi di componenti frazionarie, (ad esempio, -3, 0, 42) sono fondamentali negli scenari che richiedono valori discreti. Ad esempio, in ML, le etichette di classe in un‚Äôattivit√† di classificazione potrebbero essere rappresentate come numeri interi, dove ‚Äúgatto‚Äù, ‚Äúcane‚Äù e ‚Äúuccello‚Äù potrebbero essere codificati rispettivamente come 0, 1 e 2.\nNumeri in virgola mobile: Comprendendo numeri reali, (ad esempio, -3.14, 0.01, 2.71828) consentono la rappresentazione di valori con componenti frazionarie. Nei parametri del modello ML, i pesi potrebbero essere inizializzati con piccoli valori a virgola mobile, ad esempio 0.001 o -0.045, per avviare il processo di training. Attualmente, ci sono 4 popolari formati di precisione discussi di seguito.\nLarghezze di bit variabili: Oltre alle larghezze standard, sono in corso ricerche su numeri con larghezze di bit estremamente basse, persino fino a rappresentazioni binarie o ternarie. Le operazioni con larghezza di bit estremamente ridotta possono offrire accelerazioni significative e ridurre ulteriormente il consumo energetico. Sebbene permangano dei problemi nel mantenere l‚Äôaccuratezza del modello con una quantizzazione cos√¨ drastica, si continuano a fare progressi in quest‚Äôarea.\n\n\nPrecisione\nLa precisione, che delinea l‚Äôesattezza con cui un numero √® rappresentato, si biforca tipicamente in singola, doppia, mezza e negli ultimi anni sono emerse numerose altre precisioni per supportare meglio e in modo efficiente le attivit√† di apprendimento automatico sull‚Äôhardware sottostante.\nDoppia precisione (Float64): Allocando 64 bit, la doppia precisione (ad esempio, 3.141592653589793) fornisce una precisione elevata, sebbene richieda pi√π memoria e pi√π risorse di calcolo. Nei calcoli scientifici, dove la precisione √® fondamentale, variabili come œÄ potrebbero essere rappresentate con Float64.\nSingola precisione (Float32): Con 32 bit a disposizione, la singola precisione (ad esempio, 3.1415927) raggiunge un equilibrio tra precisione numerica e risparmio della memoria. In ML, Float32 potrebbe essere impiegato per memorizzare i pesi durante l‚Äôaddestramento per mantenere un livello ragionevole di precisione.\nHalf Precision (Float16): Limitata a 16 bit, la half precision (ad esempio, 3.14) riduce l‚Äôutilizzo della memoria e pu√≤ velocizzare i calcoli, sebbene sacrifichi l‚Äôaccuratezza e l‚Äôintervallo numerico. In ML, specialmente durante l‚Äôinferenza su dispositivi con risorse limitate, Float16 potrebbe essere utilizzato per ridurre l‚Äôimpronta di memoria del modello.\nBfloat16: Brain Floating-Point Format o Bfloat16, impiega anche 16 bit ma li alloca in modo diverso rispetto a FP16: 1 bit per il segno, 8 bit per l‚Äôesponente (che si traduce nello stesso intervallo numerico di float32) e 7 bit per la frazione. Questo formato, sviluppato da Google, d√† priorit√† a un intervallo di esponenti pi√π ampio rispetto alla precisione, rendendolo particolarmente utile nelle applicazioni di apprendimento profondo in cui l‚Äôintervallo dinamico √® cruciale.\nFigura¬†9.13 illustra le differenze tra i tre formati a virgola mobile: Float32, Float16 e BFloat16.\n\n\n\n\n\n\nFigura¬†9.13: Tre formati a virgola mobile.\n\n\n\nIntero: Le rappresentazioni di numeri interi sono realizzate utilizzando 8, 4 e 2 bit. Vengono spesso utilizzati durante la fase di inferenza delle reti neurali, in cui i pesi e le attivazioni del modello sono quantizzati a queste precisioni inferiori. Le rappresentazioni intere sono deterministiche e offrono notevoli vantaggi in termini di velocit√† e memoria rispetto alle rappresentazioni in virgola mobile. Per molte attivit√† di inferenza, in particolare su dispositivi edge, la leggera perdita di accuratezza dovuta alla quantizzazione √® spesso accettabile dati i guadagni di efficienza. Una forma estrema di numeri interi √® per le reti neurali binarie (BNN), in cui pesi e attivazioni sono vincolati a uno di due valori: +1 o -1.\n√à possibile fare riferimento a Sezione 8.6.1 per una tabella di confronto tra i compromessi dei diversi tipi numerici.\n\n\nCodifica e Archiviazione Numerica\nLa codifica numerica, l‚Äôarte di trasformare i numeri in un formato utilizzabile dal computer e la loro successiva memorizzazione sono fondamentali per l‚Äôefficienza computazionale. Ad esempio, i numeri in virgola mobile potrebbero essere codificati utilizzando lo standard IEEE 754, che ripartisce i bit tra i componenti segno, esponente e frazione, consentendo cos√¨ la rappresentazione di una vasta gamma di valori con un singolo formato. Esistono alcuni nuovi formati in virgola mobile IEEE che sono stati definiti specificamente per i carichi di lavoro AI:\n\nbfloat16- Un formato in virgola mobile a 16 bit introdotto da Google. Ha 8 bit per esponente, 7 bit per mantissa e 1 bit per segno. Offre un compromesso di precisione ridotto tra float a 32 bit e interi a 8 bit. Supportato su molti acceleratori hardware.\nposit - Un formato configurabile che pu√≤ rappresentare diversi livelli di precisione in base ai bit esponente. √à pi√π efficiente dei numeri binari in virgola mobile IEEE 754. Ha una gamma dinamica e una precisione regolabili.\nFlexpoint - Un formato introdotto da Intel che pu√≤ regolare dinamicamente la precisione tra livelli o all‚Äôinterno di un layer. Consente di adattare la precisione all‚Äôaccuratezza e ai requisiti hardware.\nBF16ALT - Un formato a 16 bit proposto da ARM come alternativa a bfloat16. Utilizza un bit aggiuntivo nell‚Äôesponente per evitare overflow/underflow.\nTF32 - Introdotto da Nvidia per le GPU Ampere. Utilizza 10 bit per l‚Äôesponente invece di 8 bit come FP32. Migliora le prestazioni di training del modello mantenendo l‚Äôaccuratezza.\nFP8 - Formato a virgola mobile a 8 bit che mantiene 6 bit per la mantissa e 2 bit per l‚Äôesponente. Consente una gamma dinamica migliore rispetto agli interi.\n\nGli obiettivi principali di questi nuovi formati sono di fornire alternative di precisione inferiore ai float a 32 bit per una migliore efficienza computazionale e prestazioni sugli acceleratori AI, mantenendo al contempo l‚Äôaccuratezza del modello. Offrono diversi compromessi in termini di precisione, portata e costo/complessit√† di implementazione.\n\n\n\n9.3.2 Vantaggi dell‚ÄôEfficienza\nCome visto in Sezione 8.6.2, l‚Äôefficienza numerica √® importante per i carichi di lavoro di apprendimento automatico per una serie di motivi. L‚Äôefficienza numerica non riguarda solo la riduzione della larghezza di bit dei numeri, ma anche la comprensione dei compromessi tra accuratezza ed efficienza. Man mano che i modelli di apprendimento automatico diventano pi√π pervasivi, soprattutto in ambienti reali con risorse limitate, l‚Äôattenzione su una numerica efficiente continuer√† a crescere. Selezionando e sfruttando attentamente la precisione numerica appropriata, √® possibile ottenere prestazioni di modello robuste ottimizzando al contempo velocit√†, memoria ed energia.\n\n\n9.3.3 Sfumature della Rappresentazione Numerica\nCi sono diverse sfumature con le rappresentazioni numeriche per ML che richiedono di avere una comprensione sia degli aspetti teorici che pratici della rappresentazione numerica, nonch√© una profonda consapevolezza dei requisiti e dei vincoli specifici del dominio applicativo.\n\nUtilizzo della Memoria\nL‚Äôimpronta di memoria dei modelli ML, in particolare quelli di notevole complessit√† e profondit√†, pu√≤ essere sostanziale, ponendo quindi una sfida significativa sia nelle fasi di training che di deployment. Ad esempio, una rete neurale profonda con 100 milioni di parametri, rappresentata utilizzando Float32 (32 bit o 4 byte per parametro), richiederebbe circa 400 MB di memoria solo per l‚Äôarchiviazione dei pesi del modello. Ci√≤ non tiene conto dei requisiti di memoria aggiuntivi durante il training per l‚Äôarchiviazione di gradienti, stati dell‚Äôottimizzatore e cache di passaggio forward [in avanti], che possono amplificare ulteriormente l‚Äôutilizzo della memoria, potenzialmente mettendo a dura prova le risorse su determinati hardware, in particolare dispositivi edge con capacit√† di memoria limitata.\nLa scelta della rappresentazione numerica ha un impatto ulteriore sull‚Äôutilizzo della memoria e sull‚Äôefficienza computazionale. Ad esempio, l‚Äôutilizzo di Float64 per i pesi del modello raddoppierebbe i requisiti di memoria rispetto a Float32 e potrebbe potenzialmente aumentare anche il tempo di elaborazione. Per una matrice di peso con dimensioni [1000, 1000], Float64 consumerebbe circa 8 MB di memoria, mentre Float32 la ridurrebbe a circa 4 MB. Pertanto, la selezione di un formato numerico appropriato √® fondamentale per ottimizzare sia la memoria che l‚Äôefficienza computazionale.\n\n\nComplessit√† Computazionale\nLa precisione numerica ha un impatto diretto sulla complessit√† computazionale, influenzando il tempo e le risorse necessarie per eseguire operazioni aritmetiche. Ad esempio, le operazioni che utilizzano Float64 generalmente consumano pi√π risorse computazionali rispetto alle loro controparti Float32 o Float16 (vedere Figura¬†9.14). Nel regno del ML, dove i modelli potrebbero dover elaborare milioni di operazioni (ad esempio, moltiplicazioni e addizioni in operazioni di matrice durante passaggi in forward e backward), anche piccole differenze nella complessit√† computazionale per operazione possono aggregarsi in un impatto sostanziale sui tempi di training e inferenza. Come mostrato in Figura¬†9.15, i modelli quantizzati possono essere molte volte pi√π veloci delle loro versioni non-quantizzate.\n\n\n\n\n\n\nFigura¬†9.14: Utilizzo di energia da parte di operazioni quantizzate. Fonte: Mark Horowitz, Stanford University.\n\n\n\n\n\n\n\n\n\nFigura¬†9.15: Velocit√† di tre diversi modelli in forma normale e quantizzata.\n\n\n\nOltre ai tempi di esecuzione puri, c‚Äô√® anche una preoccupazione per l‚Äôefficienza energetica. Non tutti i calcoli numerici sono creati uguali dal punto di vista dell‚Äôhardware sottostante. Alcune operazioni numeriche sono pi√π efficienti dal punto di vista energetico di altre. Ad esempio, Figura¬†9.16 di seguito mostra che l‚Äôaddizione di interi √® molto pi√π efficiente dal punto di vista energetico della moltiplicazione di interi.\n\n\n\n\n\n\nFigura¬†9.16: Utilizzo di energia da parte di operazioni quantizzate. Fonte: Isscc (2014).\n\n\nIsscc. 2014. ¬´Computing‚Äôs energy problem (and what we can do about it)¬ª. https://ieeexplore.ieee.org/document/6757323.\n\n\n\n\nCompatibilit√† Hardware\nGarantire la compatibilit√† e le prestazioni ottimizzate su diverse piattaforme hardware √® un‚Äôaltra sfida nella rappresentazione numerica. Hardware diversi, come CPU, GPU, TPU e FPGA, hanno capacit√† e ottimizzazioni diverse per gestire diverse precisioni numeriche. Ad esempio, alcune GPU potrebbero essere ottimizzate per i calcoli Float32, mentre altre potrebbero fornire accelerazioni per Float16. Sviluppare e ottimizzare modelli ML in grado di sfruttare le capacit√† numeriche specifiche di hardware diversi, garantendo al contempo che il modello mantenga la sua accuratezza e robustezza, richiede un‚Äôattenta considerazione e potenzialmente ulteriori sforzi di sviluppo e test.\n\n\nCompromessi di Precisione e Accuratezza\nIl compromesso tra precisione numerica e accuratezza del modello √® una sfida ‚Äúsfumata‚Äù nella rappresentazione numerica. L‚Äôutilizzo di numeri a bassa precisione, come Float16, potrebbe risparmiare memoria e velocizzare i calcoli, ma pu√≤ anche introdurre problemi come errore di quantizzazione e intervallo numerico ridotto. Ad esempio, addestrare un modello con Float16 potrebbe introdurre problemi nella rappresentazione di valori di gradiente molto piccoli, potenzialmente influenzando la convergenza e la stabilit√† del processo di addestramento. Inoltre, in alcune applicazioni, come simulazioni scientifiche o calcoli finanziari, in cui l‚Äôelevata precisione √® fondamentale, l‚Äôuso di numeri a bassa precisione potrebbe non essere consentito a causa del rischio di accumulare errori significativi.\n\n\nEsempi di Compromessi\nPer comprendere e apprezzare le sfumature, prendiamo in considerazione alcuni esempi di casi d‚Äôuso. Attraverso questi, ci renderemo conto che la scelta della rappresentazione numerica non √® semplicemente una decisione tecnica, ma strategica, che influenza l‚Äôacume predittivo del modello, le sue esigenze computazionali e la sua implementabilit√† in diversi ambienti computazionali. In questa sezione esamineremo un paio di esempi per comprendere meglio i compromessi con i numeri e come si collegano al mondo reale.\n\nVeicoli Autonomi\nNel dominio dei veicoli autonomi, i modelli ML vengono impiegati per interpretare i dati dei sensori e prendere decisioni in tempo reale. I modelli devono elaborare dati ad alta dimensionalit√† da vari sensori (ad esempio, LiDAR, telecamere, radar) ed eseguire numerosi calcoli entro un intervallo di tempo limitato per garantire un funzionamento sicuro e reattivo del veicolo. Quindi i compromessi qui includerebbero:\n\nUtilizzo della Memoria: L‚Äôarchiviazione e l‚Äôelaborazione di dati dei sensori ad alta risoluzione, specialmente in formati a virgola mobile, possono consumare una quantit√† di memoria sostanziale.\nComplessit√† Computazionale: L‚Äôelaborazione in tempo reale richiede calcoli efficienti, in cui numeri di precisione pi√π elevata potrebbero impedire l‚Äôesecuzione tempestiva delle azioni di controllo.\n\n\n\nApplicazioni Sanitarie Mobili\nLe applicazioni sanitarie mobili spesso utilizzano modelli ML per attivit√† come il riconoscimento delle attivit√†, il monitoraggio della salute o l‚Äôanalisi predittiva, operando nell‚Äôambiente con risorse limitate dei dispositivi mobili. I compromessi in questo caso includerebbero:\n\nCompromessi di Precisione e Accuratezza: L‚Äôimpiego di numeri a bassa precisione per conservare risorse potrebbe influire sull‚Äôaccuratezza delle previsioni sanitarie o delle rilevazioni di anomalie, il che potrebbe avere implicazioni significative per la salute e la sicurezza degli utenti.\nCompatibilit√† Hardware: I modelli devono essere ottimizzati per diversi hardware mobili, garantendo un funzionamento efficiente su un‚Äôampia gamma di dispositivi con diverse capacit√† di calcolo numerico.\n\n\n\nSistemi di Trading ad Alta Frequenza (HFT)\nI sistemi HFT sfruttano i modelli ML per prendere decisioni di trading rapide basate su dati di mercato in tempo reale. Questi sistemi richiedono risposte a bassissima latenza per capitalizzare le opportunit√† di trading di breve durata.\n\nComplessit√† Computazionale: I modelli devono elaborare e analizzare vasti flussi di dati di mercato con una latenza minima, dove anche lievi ritardi, potenzialmente introdotti da numeri a precisione pi√π elevata, possono comportare opportunit√† perse.\nCompromessi di Precisione e Accuratezza: I calcoli finanziari spesso richiedono un‚Äôelevata precisione numerica per garantire valutazioni accurate dei prezzi e dei rischi, ponendo sfide nel bilanciamento tra efficienza computazionale e accuratezza numerica.\n\n\n\nSistemi di Sorveglianza Basati su Edge\nI sistemi di sorveglianza distribuiti su dispositivi edge, come le telecamere di sicurezza, utilizzano modelli ML per attivit√† come rilevamento di oggetti, riconoscimento di attivit√† e rilevamento di anomalie, spesso operando con vincoli di risorse rigorosi.\n\nUtilizzo della Memoria: L‚Äôarchiviazione di modelli pre-addestrati e l‚Äôelaborazione di feed video in tempo reale richiedono un utilizzo efficiente della memoria, il che pu√≤ essere impegnativo con numeri ad alta precisione.\nCompatibilit√† Hardware: Garantire che i modelli possano funzionare in modo efficiente su dispositivi edge con diverse capacit√† hardware e ottimizzazioni per diverse precisioni numeriche √® fondamentale per una distribuzione diffusa.\n\n\n\nSimulazioni Scientifiche\nI modelli ML vengono sempre pi√π utilizzati nelle simulazioni scientifiche, come la modellazione climatica o le simulazioni di dinamica molecolare, per migliorare le capacit√† predittive e ridurre le richieste di calcolo.\n\nCompromessi di Precisione e Accuratezza: Le simulazioni scientifiche spesso richiedono un‚Äôelevata precisione numerica per garantire risultati accurati e affidabili, il che pu√≤ entrare in conflitto con il desiderio di ridurre le richieste di calcolo tramite numeri a bassa precisione.\nComplessit√† Computazionale: I modelli devono gestire ed elaborare dati di simulazione complessi e ad alta dimensionalit√† in modo efficiente per garantire risultati tempestivi e consentire simulazioni su larga scala o di lunga durata.\n\nQuesti esempi illustrano diversi scenari in cui le sfide della rappresentazione numerica nei modelli ML sono palesemente manifestate. Ogni sistema presenta un set unico di requisiti e vincoli, che richiedono strategie e soluzioni personalizzate per affrontare i problemi dell‚Äôutilizzo della memoria, della complessit√† computazionale, dei compromessi tra precisione e accuratezza e della compatibilit√† hardware.\n\n\n\n\n9.3.4 Quantizzazione\nLa quantizzazione √® prevalente in vari domini scientifici e tecnologici e comporta essenzialmente la mappatura o la limitazione di un set o intervallo continuo in una controparte discreta per ridurre al minimo il numero di bit richiesti.\n\nAnalisi Iniziale\nIniziamo la nostra incursione nella quantizzazione con una breve analisi di un importante utilizzo della quantizzazione.\nNel signal processing [elaborazione del segnale], l‚Äôonda sinusoidale continua (mostrata in Figura¬†9.17) pu√≤ essere quantizzata in valori discreti tramite un processo noto come campionamento. Questo √® un concetto fondamentale nell‚Äôelaborazione del segnale digitale ed √® cruciale per convertire segnali analogici (come l‚Äôonda sinusoidale continua) in una forma digitale che possa essere elaborata dai computer. L‚Äôonda sinusoidale √® un esempio prevalente grazie alla sua natura periodica e regolare, il che la rende uno strumento utile per spiegare concetti come frequenza, ampiezza, fase e, naturalmente, quantizzazione.\n\n\n\n\n\n\nFigura¬†9.17: Onda Sinusoidale.\n\n\n\nNella versione quantizzata mostrata in Figura¬†9.18, l‚Äôonda sinusoidale continua (Figura¬†9.17) viene campionata a intervalli regolari (in questo caso, ogni \\(\\frac{\\pi}{4}\\) radianti) e solo questi valori campionati vengono rappresentati nella versione digitale del segnale. Le linee graduali tra i punti mostrano un modo per rappresentare il segnale quantizzato in una forma costante a tratti. Questo √® un esempio semplificato di come funziona la conversione analogico-digitale, in cui un segnale continuo viene mappato su un set discreto di valori, consentendone la rappresentazione e l‚Äôelaborazione digitale.\n\n\n\n\n\n\nFigura¬†9.18: Onda Sinusoidale Quantizzata.\n\n\n\nTornando al contesto del Machine Learning (ML), la quantizzazione si riferisce al processo di limitazione dei possibili valori che i parametri numerici (come pesi e bias) possono assumere in un set discreto, riducendo cos√¨ la precisione dei parametri e, di conseguenza, l‚Äôingombro di memoria del modello. Se implementata correttamente, la quantizzazione pu√≤ ridurre le dimensioni del modello fino a 4 volte e migliorare la latenza e la produttivit√† dell‚Äôinferenza fino a 2-3 volte. Figura¬†9.19 illustra l‚Äôimpatto che la quantizzazione ha sulle dimensioni di modelli diversi: ad esempio, un modello di classificazione delle immagini come ResNet-v2 pu√≤ essere compresso da 180 MB a 45 MB con quantizzazione a 8 bit. In genere, la perdita di accuratezza del modello √® inferiore all‚Äô1% con una quantizzazione ben fatta. L‚Äôaccuratezza pu√≤ spesso essere recuperata riaddestrando il modello quantizzato con tecniche di addestramento consapevoli della quantizzazione. Pertanto, questa tecnica √® emersa come molto importante nell‚Äôimplementazione di modelli ML in ambienti con risorse limitate, come dispositivi mobili, dispositivi IoT e piattaforme di edge computing, dove le risorse computazionali (memoria e potenza di elaborazione) sono limitate.\n\n\n\n\n\n\nFigura¬†9.19: Effetto della quantizzazione sulle dimensioni del modello. Fonte: HarvardX.\n\n\n\nEsistono diverse dimensioni della quantizzazione, come uniformit√†, stocasticit√† (o determinismo), simmetria, granularit√† (tra layer/canali/gruppi o persino all‚Äôinterno dei canali), considerazioni sulla calibrazione dell‚Äôintervallo (statico o dinamico) e metodi di messa a punto (QAT, PTQ, ZSQ). Esaminiamo questi di seguito.\n\n\n\n9.3.5 I Tipi\n\nQuantizzazione Uniforme\nLa quantizzazione uniforme implica la mappatura di valori continui o ad alta precisione su una rappresentazione a precisione inferiore utilizzando una scala uniforme. Ci√≤ significa che l‚Äôintervallo tra ogni possibile valore quantizzato √® coerente. Ad esempio, se i pesi di un layer di rete neurale sono quantizzati su numeri interi a 8 bit (valori tra 0 e 255), un peso con un valore in virgola mobile di 0.56 potrebbe essere mappato su un valore intero di 143, presupponendo una mappatura lineare tra le scale originale e quantizzata. Grazie all‚Äôuso di pipeline matematiche intere o a virgola fissa, questa forma di quantizzazione consente il calcolo sul dominio quantizzato senza la necessit√† di dequantizzare in anticipo.\nIl processo per implementare la quantizzazione uniforme inizia con la scelta di un intervallo di numeri reali da quantizzare. Il passaggio successivo consiste nel selezionare una funzione di quantizzazione e mappare i valori reali sugli interi rappresentabili dalla larghezza di bit della rappresentazione quantizzata. Ad esempio, una scelta popolare per una funzione di quantizzazione √®:\n\\[\nQ(r)=Int(r/S) - Z\n\\]\ndove \\(Q\\) √® l‚Äôoperatore di quantizzazione, \\(r\\) √® un input a valore reale (nel nostro caso, un‚Äôattivazione o un peso), \\(S\\) √® un fattore di scala a valore reale e \\(Z\\) √® un punto zero intero. La funzione Int mappa un valore reale in un valore intero tramite un‚Äôoperazione di arrotondamento. Tramite questa funzione, abbiamo mappato in modo efficace i valori reali \\(r\\) in alcuni valori interi, ottenendo livelli quantizzati uniformemente distanziati.\nQuando i professionisti hanno la necessit√† di recuperare i valori originali di precisione pi√π elevata, i valori reali \\(r\\) possono essere recuperati dai valori quantizzati tramite un‚Äôoperazione nota come dequantizzazione. Nell‚Äôesempio sopra, ci√≤ significherebbe eseguire la seguente operazione sul nostro valore quantizzato:\n\\[\n\\bar{r} = S(Q(r) + Z)\n\\]\nCome discusso, una certa precisione nel valore reale viene persa dalla quantizzazione. In questo caso, il valore recuperato \\(\\bar{r}\\) non corrisponder√† esattamente a \\(r\\) a causa dell‚Äôoperazione di arrotondamento. Questo √® un importante compromesso da notare; tuttavia, in molti utilizzi riusciti della quantizzazione, la perdita di precisione pu√≤ essere trascurabile e l‚Äôaccuratezza del test rimane elevata. Nonostante ci√≤, la quantizzazione uniforme continua a essere la scelta di fatto attuale per la sua semplicit√† e l‚Äôefficiente mappatura all‚Äôhardware.\n\n\nQuantizzazione Non-Uniforme\nLa quantizzazione non uniforme, d‚Äôaltro canto, non mantiene un intervallo coerente tra i valori quantizzati. Questo approccio potrebbe essere utilizzato per allocare pi√π possibili valori discreti in regioni in cui i valori dei parametri sono pi√π densamente popolati, preservando cos√¨ maggiori dettagli dove sono pi√π necessari. Ad esempio, nelle distribuzioni a campana di pesi con lunghe code, un set di pesi in un modello si trova prevalentemente all‚Äôinterno di un certo intervallo; quindi, pi√π livelli di quantizzazione potrebbero essere assegnati a tale intervallo per preservare dettagli pi√π fini, consentendoci di acquisire meglio le informazioni. Tuttavia, una delle principali debolezze della quantizzazione non uniforme √® che richiede la dequantizzazione prima di calcoli di precisione pi√π elevata a causa della sua non uniformit√†, limitando la sua capacit√† di accelerare il calcolo rispetto alla quantizzazione uniforme.\nIn genere, una quantizzazione non uniforme basata su regole utilizza una distribuzione logaritmica di passaggi e livelli esponenzialmente crescenti anzich√© linearmente. Un altra tipologia popolare risiede nella quantizzazione basata su codice binario in cui i vettori di numeri reali vengono quantizzati in vettori binari con un fattore di scala. In particolare, non esiste una soluzione in forma chiusa per minimizzare gli errori tra il valore reale e il valore non uniformemente quantizzato, quindi la maggior parte delle quantizzazioni in questo campo si basa su soluzioni euristiche. Ad esempio, un lavoro recente di Xu et al. (2018) formula la quantizzazione non uniforme come un problema di ottimizzazione in cui i passaggi/livelli di quantizzazione nel quantizzatore \\(Q\\) vengono regolati per ridurre al minimo la differenza tra il tensore originale e la controparte quantizzata.\n\nXu, Chen, Jianqiang Yao, Zhouchen Lin, Wenwu Ou, Yuanbin Cao, Zhirong Wang, e Hongbin Zha. 2018. ¬´Alternating Multi-bit Quantization for Recurrent Neural Networks¬ª. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net. https://openreview.net/forum?id=S19dR9x0b.\n\\[\n\\min_Q ||Q(r)-r||^2\n\\]\nInoltre, i quantizzatori addestrabili lo possono essere congiuntamente con parametri di modello e i passaggi/livelli di quantizzazione sono generalmente addestrati con ottimizzazione iterativa o discesa del gradiente. Inoltre, il clustering √® stato utilizzato per alleviare la perdita di informazioni dalla quantizzazione. Sebbene in grado di catturare livelli di dettaglio pi√π elevati, gli schemi di quantizzazione non uniformi possono essere difficili da implementare in modo efficiente su hardware di calcolo generale, rendendoli meno preferiti ai metodi che utilizzano la quantizzazione uniforme.\n\n\n\n\n\n\nFigura¬†9.20: Uniformit√† della Quantizzazione. Fonte: Gholami et al. (2021).\n\n\n\n\n\nQuantizzazione Stocastica\nA differenza dei due approcci precedenti che generano mappature deterministiche, c‚Äô√® un po‚Äô di lavoro che esplora l‚Äôidea della quantizzazione stocastica per l‚Äôaddestramento consapevole della quantizzazione e l‚Äôaddestramento a precisione ridotta. Questo approccio mappa numeri fluttuanti verso l‚Äôalto o verso il basso con una probabilit√† associata alla grandezza dell‚Äôaggiornamento del peso. La speranza generata dall‚Äôintuizione di alto livello √® che un tale approccio probabilistico possa consentire a una rete neurale di esplorare di pi√π, rispetto alla quantizzazione deterministica. Presumibilmente, abilitare un arrotondamento stocastico potrebbe consentire alle reti neurali di sfuggire agli ottimi locali, aggiornando cos√¨ i propri parametri. Di seguito sono riportati due esempi di funzioni di mappatura stocastica:\n\n\n\n\n\n\n\nFigura¬†9.21: Funzioni di quantizzazione Intera e Binaria.\n\n\n\n\n\nQuantizzazione ‚ÄúZero Shot‚Äù\nLa quantizzazione Zero-shot si riferisce al processo di conversione di un modello di deep learning a precisione completa direttamente in un modello quantizzato a bassa precisione senza la necessit√† di alcun riaddestramento o messa a punto sul modello quantizzato. Il vantaggio principale di questo approccio √® la sua efficienza, in quanto elimina il processo, spesso dispendioso in termini di tempo e risorse, del riaddestramento post-quantizzazione. Sfruttando tecniche che anticipano e riducono al minimo gli errori di quantizzazione, la quantizzazione zero-shot mantiene l‚Äôaccuratezza originale del modello anche dopo averne ridotto la precisione numerica. √à particolarmente utile per i provider di ‚ÄúMachine Learning as a Service (MLaaS)‚Äù che mirano ad accelerare la distribuzione dei carichi di lavoro dei propri clienti senza dover accedere ai loro set di dati.\n\n\n\n9.3.6 Calibrazione\nLa calibrazione √® il processo di selezione dell‚Äôintervallo di clipping [ritaglio] pi√π efficace [\\(\\alpha\\), \\(\\beta\\)] per pesi e attivazioni da quantizzare. Ad esempio, si consideri la quantizzazione delle attivazioni che originariamente hanno un intervallo in virgola mobile tra -6 e 6 a interi a 8 bit. Prendere solo i valori minimi e massimi possibili di interi a 8 bit (da -128 a 127) come intervallo di quantizzazione, potrebbe non essere il pi√π efficace. Invece, la calibrazione implicherebbe il passaggio di un set di dati rappresentativo e quindi l‚Äôutilizzo di questo intervallo osservato per la quantizzazione.\nEsistono molti metodi di calibrazione, ma alcuni comunemente utilizzati includono:\n\nMax: Utilizza il valore assoluto massimo visualizzato durante la calibrazione. Tuttavia, questo metodo √® suscettibile di dati anomali. Notare come in Figura¬†9.22, abbiamo un cluster anomalo intorno a 2.1, mentre il resto √® raggruppato attorno a valori pi√π piccoli.\nEntropia: Utilizza la divergenza KL per ridurre al minimo la perdita di informazioni tra i valori originali in virgola mobile e i valori che potrebbero essere rappresentati dal formato quantizzato. Questo √® il metodo predefinito utilizzato da TensorRT.\nPercentile: Imposta l‚Äôintervallo su un percentile della distribuzione dei valori assoluti osservati durante la calibrazione. Ad esempio, una calibrazione del 99% taglierebbe l‚Äô1% dei valori di magnitudine pi√π grandi.\n\n\n\n\n\n\n\nFigura¬†9.22: Attivazioni di input nel layer 3 in ResNet50. Fonte: @Wu, Judd, e Isaev (2020).\n\n\n\n√à importante notare che la qualit√† della calibrazione pu√≤ fare la differenza tra un modello quantizzato che conserva la maggior parte della sua accuratezza e uno che si degrada in modo significativo. Quindi, √® un passaggio essenziale nel processo di quantizzazione. Quando si sceglie un intervallo di calibrazione, ci sono due tipi: simmetrico e asimmetrico.\n\nQuantizzazione Simmetrica\nLa quantizzazione simmetrica mappa i valori reali su un intervallo di clipping simmetrico centrato su 0. Ci√≤ comporta la scelta di un intervallo [\\(\\alpha\\), \\(\\beta\\)] dove \\(\\alpha = -\\beta\\). Ad esempio, un intervallo simmetrico si baserebbe sui valori min/max dei valori reali in modo tale che:\n\\[\n\\alpha = \\beta = max(abs(r_{max}), abs(r_{min}))\n\\]\nGli intervalli di clipping simmetrici sono i pi√π ampiamente adottati nella pratica in quanto hanno il vantaggio di un‚Äôimplementazione pi√π semplice. In particolare, la mappatura da zero a zero nell‚Äôintervallo di clipping (talvolta chiamata ‚Äúazzeramento del punto zero‚Äù) pu√≤ portare a una riduzione del costo computazionale durante l‚Äôinferenza (Wu, Judd, e Isaev 2020).\n\n\nQuantizzazione Asimmetrica\nLa quantizzazione asimmetrica mappa i valori reali in un intervallo di clipping asimmetrico che non √® necessariamente centrato sullo 0, come mostrato in Figura¬†9.23 a destra. Comporta la scelta di un intervallo [\\(\\alpha\\), \\(\\beta\\)] dove \\(\\alpha \\neq -\\beta\\). Ad esempio, selezionando un intervallo basato sui valori reali minimi e massimi, o dove \\(\\alpha = r_{min}\\) and \\(\\beta = r_{max}\\), si crea un intervallo asimmetrico. In genere, la quantizzazione asimmetrica produce intervalli di clipping pi√π stretti rispetto a quella simmetrica, il che √® importante quando i pesi e le attivazioni target sono sbilanciati, ad esempio, l‚Äôattivazione dopo la ReLU ha sempre valori non negativi. Nonostante produca intervalli di clipping pi√π stretti, la quantizzazione asimmetrica √® meno preferita di quella simmetrica in quanto non azzera sempre il valore dello zero reale.\n\n\n\n\n\n\nFigura¬†9.23: (a)simmetria della Quantizzazione. Fonte: Gholami et al. (2021).\n\n\n\n\n\nGranularit√†\nDopo aver deciso il tipo di intervallo di clipping, √® essenziale restringerlo per consentire a un modello di mantenere la massima accuratezza possibile. Daremo un‚Äôocchiata alle reti neurali convoluzionali come nostro modo di esplorare metodi che ottimizzano la granularit√† degli intervalli di clipping per la quantizzazione. L‚Äôattivazione di input di un layer nella nostra CNN subisce una convoluzione con pi√π filtri convoluzionali. Ogni filtro convoluzionale pu√≤ possedere un intervallo di valori univoco. Si noti come in Figura¬†9.24 l‚Äôintervallo per il Filtro 1 sia molto pi√π piccolo di quello per il Filtro 3. Di conseguenza, una caratteristica distintiva degli approcci di quantizzazione √® la precisione con cui l‚Äôintervallo di clipping [Œ±,Œ≤] viene determinato per i pesi.\n\n\n\n\n\n\nFigura¬†9.24: Granularit√† di quantizzazione: intervalli variabili. Fonte: Gholami et al. (2021).\n\n\n\n\nQuantizzazione a Layer: Questo approccio determina l‚Äôintervallo di clipping considerando tutti i pesi nei filtri convoluzionali di un layer. Quindi, lo stesso intervallo di clipping viene utilizzato per tutti i filtri convoluzionali. √à il pi√π semplice da implementare e, come tale, spesso si traduce in una precisione non ottimale a causa dell‚Äôampia variet√† di intervalli diversi tra i filtri. Ad esempio, un kernel convoluzionale con un intervallo di parametri pi√π ristretto perde la sua risoluzione di quantizzazione a causa di un altro kernel nello stesso layer che ha un intervallo pi√π ampio.\nGroupwise Quantization: Questo approccio raggruppa diversi canali all‚Äôinterno di un layer per calcolare l‚Äôintervallo di clipping. Questo metodo pu√≤ essere utile quando la distribuzione dei parametri su una singola convoluzione/attivazione varia molto. In pratica, questo metodo √® stato utile in Q-BERT (Shen et al. 2020) per quantizzare i modelli Transformer (Vaswani et al. 2017) costituiti da layer di attenzione completamente connessi. Lo svantaggio di questo approccio √® il costo aggiuntivo di contabilizzazione di diversi fattori di scala.\nChannelwise Quantization: Questo metodo popolare utilizza un intervallo fisso per ogni filtro convoluzionale che √® indipendente dagli altri canali. Poich√© a ogni canale viene assegnato un fattore di scala dedicato, questo metodo garantisce una risoluzione di quantizzazione pi√π elevata e spesso si traduce in una maggiore accuratezza.\nSub-channelwise Quantization: Portando la quantizzazione canale per canale all‚Äôestremo, questo metodo determina l‚Äôintervallo di clipping rispetto a qualsiasi gruppo di parametri in una convoluzione o in un layer completamente connesso. Potrebbe comportare un overhead considerevole poich√© √® necessario tenere conto di diversi fattori di scala quando si elabora una singola convoluzione o un layer completamente connesso.\n\n\nShen, Sheng, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami, Michael W. Mahoney, e Kurt Keutzer. 2020. ¬´Q-BERT: Hessian Based Ultra Low Precision Quantization of BERT¬ª. Proceedings of the AAAI Conference on Artificial Intelligence 34 (05): 8815‚Äì21. https://doi.org/10.1609/aaai.v34i05.6409.\n\nVaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz Kaiser, e Illia Polosukhin. 2017. ¬´Attention is all you need¬ª. Adv Neural Inf Process Syst 30.\nTra questi, la quantizzazione canale per canale √® lo standard corrente utilizzato per quantizzare i kernel convoluzionali, poich√© consente la regolazione degli intervalli di clipping per ogni singolo kernel con overhead trascurabile.\n\n\nQuantizzazione Statica e Dinamica\nDopo aver determinato il tipo e la granularit√† dell‚Äôintervallo di clipping, gli esperti devono decidere quando gli intervalli vengono determinati nei loro algoritmi di calibrazione dell‚Äôintervallo. Esistono due approcci per quantizzare le attivazioni: quantizzazione statica e quella dinamica.\nLa quantizzazione statica √® l‚Äôapproccio pi√π frequentemente utilizzato. In questo, l‚Äôintervallo di clipping √® precalcolato e statico durante l‚Äôinferenza. Non aggiunge alcun sovraccarico computazionale, ma, di conseguenza, comporta una minore accuratezza rispetto alla quantizzazione dinamica. Un metodo popolare per implementarlo √® eseguire una serie di input di calibrazione per calcolare l‚Äôintervallo tipico di attivazioni (Jacob et al. 2018; Yao et al. 2021).\n\nJacob, Benoit, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard, Hartwig Adam, e Dmitry Kalenichenko. 2018. ¬´Quantization and training of neural networks for efficient integer-arithmetic-only inference¬ª. In Proceedings of the IEEE conference on computer vision and pattern recognition, 2704‚Äì13.\n\nYao, Zhewei, Zhen Dong, Zhangcheng Zheng, Amir Gholami, Jiali Yu, Eric Tan, Leyuan Wang, et al. 2021. ¬´Hawq-v3: Dyadic neural network quantization¬ª. In International Conference on Machine Learning, 11875‚Äì86. PMLR.\nLa quantizzazione dinamica √® un approccio alternativo che calcola dinamicamente l‚Äôintervallo per ogni mappa di attivazione durante il runtime. L‚Äôapproccio richiede calcoli in tempo reale che potrebbero avere un sovraccarico molto elevato. In questo modo, la quantizzazione dinamica spesso raggiunge la massima accuratezza poich√© l‚Äôintervallo viene calcolato specificamente per ogni input.\nTra i due, il calcolo dell‚Äôintervallo in modo dinamico √® solitamente molto costoso, quindi la maggior parte dei professionisti utilizzer√† spesso la quantizzazione statica.\n\n\n\n9.3.7 Tecniche\nQuando si ottimizzano i modelli di apprendimento automatico per l‚Äôimplementazione, vengono utilizzate varie tecniche di quantizzazione per bilanciare efficienza, accuratezza e adattabilit√† del modello. Ogni metodo (quantizzazione post-addestramento, addestramento consapevole della quantizzazione e quantizzazione dinamica) offre vantaggi e compromessi unici, che incidono su fattori quali complessit√† di implementazione, sovraccarico computazionale e ottimizzazione delle prestazioni.\nTabella¬†9.2 fornisce una panoramica di questi metodi di quantizzazione, evidenziandone i rispettivi punti di forza, limiti e compromessi. Ci addentreremo pi√π a fondo in ciascuno di questi metodi perch√© sono ampiamente distribuiti e utilizzati in tutti i sistemi ML di scale molto diverse.\n\n\n\nTabella¬†9.2: Confronto tra quantizzazione post-training, training sensibile alla quantizzazione e quantizzazione dinamica.\n\n\n\n\n\n\n\n\n\n\n\nAspetto\nQuantizzazione Post Training\nTraining Quantization-Aware\nQuantization Dinamica\n\n\n\n\nPro\n\n\n\n\n\nSemplicit√†\n‚úì\n‚úó\n‚úó\n\n\nPreservazione della Precisione\n‚úó\n‚úì\n‚úì\n\n\nAdattabilit√†\n‚úó\n‚úó\n‚úì\n\n\nPrestazioni Ottimizzate\n‚úó\n‚úì\nPotentialmente\n\n\nContro\n\n\n\n\n\nDegrado della Precisione\n‚úì\n‚úó\nPotentialmente\n\n\nOverhead Computazionale\n‚úó\n‚úì\n‚úì\n\n\nComplessit√† di Implementazione\n‚úó\n‚úì\n‚úì\n\n\nCompromessi\n\n\n\n\n\nVelocit√† vs.¬†Precisione\n‚úì\n‚úó\n‚úó\n\n\nPrecisione vs.¬†Costo\n‚úó\n‚úì\n‚úó\n\n\nAdaptability vs.¬†Overhead\n‚úó\n‚úó\n‚úì\n\n\n\n\n\n\nPost Training Quantization: La quantizzazione post-addestramento (PTQ) √® una tecnica di quantizzazione in cui il modello viene quantizzato dopo essere stato addestrato. Il modello viene addestrato in virgola mobile e poi i pesi e le attivazioni vengono quantizzati come fase di post-elaborazione. Questo √® l‚Äôapproccio pi√π semplice e non richiede l‚Äôaccesso ai dati di addestramento. Diversamente la ‚ÄúQuantization-Aware Training (QAT), PTQ‚Äù imposta direttamente i parametri di quantizzazione del peso e dell‚Äôattivazione, rendendolo poco costoso e adatto a situazioni con dati limitati o non etichettati. Tuttavia, non riaggiustare i pesi dopo la quantizzazione, specialmente nella quantizzazione a bassa precisione, pu√≤ portare a un comportamento molto diverso e quindi a una minore accuratezza. Per affrontare questo problema, sono state sviluppate tecniche come la correzione della distorsione, l‚Äôequalizzazione degli intervalli di peso e i metodi di arrotondamento adattivo. PTQ pu√≤ essere applicato anche in scenari zero-shot, in cui non sono disponibili dati di addestramento o di test. Questo metodo √® stato reso ancora pi√π efficiente per avvantaggiare modelli linguistici di grandi dimensioni che richiedono molta elaborazione e memoria. Di recente, √® stata sviluppata SmoothQuant, una soluzione PTQ senza training, che preserva l‚Äôaccuratezza ed √® di uso generale che consente la quantizzazione di peso a 8 bit e attivazione a 8 bit per LLM, dimostrando un‚Äôaccelerazione fino a 1.56x e una riduzione della memoria di 2x per LLM con una perdita trascurabile di accuratezza (Xiao et al. 2022).\nIn PTQ, un modello pre-addestrato subisce un processo di calibrazione, come mostrato in Figura¬†9.25. La calibrazione comporta l‚Äôutilizzo di un set di dati separato noto come dati di calibrazione, un sottoinsieme specifico dei dati di training riservato alla quantizzazione per aiutare a trovare gli intervalli di clipping e i fattori di scala appropriati.\n\n\n\n\n\n\nFigura¬†9.25: Quantizzazione e Calibrazione Post-Training. Fonte: Gholami et al. (2021).\n\n\n\nQuantization-Aware Training: L‚Äôaddestramento consapevole della quantizzazione (QAT) √® una messa a punto del modello PTQ. Il modello viene addestrato in modo consapevole della quantizzazione, consentendogli di adattarsi agli effetti della quantizzazione. Ci√≤ produce una migliore accuratezza con l‚Äôinferenza quantizzata. La quantizzazione di un modello di rete neurale addestrato con metodi come PTQ introduce perturbazioni che possono deviare il modello dal suo punto di convergenza originale. Ad esempio, Krishnamoorthi ha dimostrato che anche con la quantizzazione per canale, reti come MobileNet non raggiungono la precisione di base con int8 PTQ e richiedono QAT (Krishnamoorthi 2018). Per risolvere questo problema, QAT riqualifica il modello con parametri quantizzati, utilizzando passaggi forward [avanti] e backward [indietro] in virgola mobile ma quantizzando i parametri dopo ogni aggiornamento del gradiente. La gestione dell‚Äôoperatore di quantizzazione non differenziabile √® fondamentale; un metodo ampiamente utilizzato √® lo ‚ÄúStraight Through Estimator (STE)‚Äù, che approssima l‚Äôoperazione di arrotondamento come una funzione identit√†. Sebbene esistano altri metodi e varianti, STE rimane il pi√π comunemente utilizzato per la sua efficacia pratica. In QAT, un modello pre-addestrato viene quantizzato e poi messo a punto utilizzando i dati di addestramento per regolare i parametri e recuperare il degrado della precisione, come mostrato in Figura¬†9.26. Il processo di calibrazione viene spesso condotto parallelamente al processo di messa a punto per QAT.\n\n\n\n\n\n\nFigura¬†9.26: Quantization-Aware Training. Fonte: Gholami et al. (2021).\n\n\nGholami, Dong Kim, Mahoney Yao, e Keutzer. 2021. ¬´A Survey of Quantization Methods for Efficient Neural Network Inference)¬ª. ArXiv preprint. https://arxiv.org/abs/2103.13630.\n\n\nLa ‚ÄúQuantization-Aware Training‚Äù funge da estensione naturale della ‚ÄúPost-Training Quantization‚Äù. Dopo la quantizzazione iniziale eseguita da PTQ, QAT viene utilizzata per perfezionare e mettere a punto ulteriormente i parametri quantizzati: vedere come in Figura¬†9.27, il modello PTQ subisce un ulteriore passaggio, QAT. Comporta un processo di riqualificazione in cui il modello viene esposto a ulteriori iterazioni di training utilizzando i dati originali. Questo approccio di training dinamico consente al modello di adattare e regolare i suoi parametri, compensando il degrado delle prestazioni causato dalla quantizzazione.\n\n\n\n\n\n\nFigura¬†9.27: PTQ e QAT. Fonte: ¬´The Ultimate Guide to Deep Learning Model Quantization and Quantization-Aware Training¬ª (s.d.).\n\n\n¬´The Ultimate Guide to Deep Learning Model Quantization and Quantization-Aware Training¬ª. s.d. https://deci.ai/quantization-and-quantization-aware-training/.\n\n\nFigura¬†9.28 mostra l‚Äôaccuratezza relativa di diversi modelli dopo PTQ e QAT. In quasi tutti i casi, QAT produce un‚Äôaccuratezza migliore di PTQ. Si consideri ad esempio EfficientNet b0. Dopo PTQ, l‚Äôaccuratezza scende dal 76.85% a 72.06%. Ma quando applichiamo QAT, l‚Äôaccuratezza rimbalza al 76.95% (con persino un leggero miglioramento rispetto all‚Äôaccuratezza originale).\n\n\n\n\n\n\nFigura¬†9.28: Accuratezza relativa di PTQ e QAT. Fonte: Wu, Judd, e Isaev (2020).\n\n\n\n\n\n9.3.8 Pesi vs.¬†Attivazioni\nQuantizzazione del peso: Comporta la conversione dei pesi continui o ad alta precisione di un modello in pesi a bassa precisione, come la conversione dei pesi Float32 in pesi INT8 (interi) quantizzati - in Figura¬†9.29, la quantizzazione del peso avviene nel secondo passaggio (quadrati rossi) quando moltiplichiamo gli input. Ci√≤ riduce le dimensioni del modello, riducendo cos√¨ la memoria richiesta per archiviare il modello e le risorse computazionali necessarie per eseguire l‚Äôinferenza. Ad esempio, si consideri una matrice di pesi in un layer di rete neurale con pesi Float32 come [0.215, -1.432, 0.902, ‚Ä¶]. Attraverso la quantizzazione del peso, questi potrebbero essere mappati su valori INT8 come [27, -183, 115, ‚Ä¶], riducendo significativamente la memoria richiesta per memorizzarli.\n\n\n\n\n\n\nFigura¬†9.29: Quantizzazione del peso e dell‚Äôattivazione. Fonte: HarvardX.\n\n\n\nQuantizzazione dell‚ÄôAttivazione: Comporta la quantizzazione dei valori di attivazione (output dei livelli) durante l‚Äôinferenza del modello. Ci√≤ pu√≤ ridurre le risorse computazionali richieste durante l‚Äôinferenza, ma introduce ulteriori problemi nel mantenimento dell‚Äôaccuratezza del modello a causa della ridotta precisione dei calcoli intermedi. Ad esempio, in una rete neurale convoluzionale (CNN), le mappe di attivazione (mappe delle feature) prodotte dai layer convoluzionali, originariamente in Float32, potrebbero essere quantizzate su INT8 durante l‚Äôinferenza per accelerare il calcolo, in particolare su hardware ottimizzato per l‚Äôaritmetica degli interi. Inoltre, un lavoro recente ha esplorato l‚Äôuso della quantizzazione del ‚ÄúActivation-aware Weight Quantization‚Äù per la compressione e l‚Äôaccelerazione LLM, che comporta la protezione di solo l‚Äô1% dei pesi salienti pi√π importanti osservando le attivazioni, non i pesi (Lin et al. 2023).\n\n\n9.3.9 Compromessi\nLa quantizzazione introduce invariabilmente un compromesso tra dimensioni/prestazioni del modello e accuratezza. Sebbene riduca significativamente l‚Äôingombro della memoria e possa accelerare l‚Äôinferenza, specialmente su hardware ottimizzato per aritmetica a bassa precisione, la precisione ridotta pu√≤ degradare l‚Äôaccuratezza del modello.\nDimensioni del Modello: Un modello con pesi rappresentati come Float32 quantizzato a INT8 pu√≤ teoricamente ridurre le dimensioni del modello di un fattore 4, consentendone l‚Äôimplementazione su dispositivi con memoria limitata. Le dimensioni di grandi modelli linguistici si stanno sviluppando a un ritmo pi√π veloce della memoria GPU negli ultimi anni, portando a un grande divario tra domanda e offerta di memoria. Figura¬†9.30 illustra la recente tendenza del divario crescente tra le dimensioni del modello (linea rossa) e la memoria dell‚Äôacceleratore (linea gialla). Le tecniche di quantizzazione e compressione del modello possono aiutare a colmare il divario\n\n\n\n\n\n\nFigura¬†9.30: Dimensioni del modello vs.¬†memoria dell‚Äôacceleratore. Fonte: Xiao et al. (2022).\n\n\nXiao, Seznec Lin, Demouth Wu, e Han. 2022. ¬´SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models¬ª. ArXiv preprint. https://arxiv.org/abs/2211.10438.\n\n\nVelocit√† di Inferenza: La quantizzazione pu√≤ anche accelerare l‚Äôinferenza, poich√© l‚Äôaritmetica a precisione inferiore √® computazionalmente meno costosa. Ad esempio, alcuni acceleratori hardware, come Edge TPU di Google, sono ottimizzati per l‚Äôaritmetica INT8 e possono eseguire l‚Äôinferenza in modo significativamente pi√π rapido con modelli quantizzati INT8 rispetto alle loro controparti in virgola mobile. La riduzione della memoria dalla quantizzazione aiuta a ridurre la quantit√† di trasmissione dei dati, risparmiando memoria e velocizzando il processo. Figura¬†9.31 confronta l‚Äôaumento della produttivit√† e la riduzione della memoria della larghezza di banda per diversi tipi di dati sulla NVIDIA Turing GPU.\n\n\n\n\n\n\nFigura¬†9.31: Vantaggi dei tipi di dati a precisione inferiore. Fonte: Wu, Judd, e Isaev (2020).\n\n\nWu, Zhang Judd, e Micikevicius Isaev. 2020. ¬´Integer Quantization for Deep Learning Inference: Principles and Empirical Evaluation)¬ª. ArXiv preprint. https://arxiv.org/abs/2004.09602.\n\n\nPrecisione: La riduzione della precisione numerica post-quantizzazione pu√≤ portare a un degrado della precisione del modello, che potrebbe essere accettabile in alcune applicazioni (ad esempio, classificazione delle immagini) ma non in altre (ad esempio, diagnosi medica). Pertanto, dopo la quantizzazione, il modello richiede in genere una ricalibrazione o una messa a punto per mitigare la perdita di accuratezza. Inoltre, un lavoro recente ha esplorato l‚Äôuso di Activation-aware Weight Quantization (Lin et al. 2023) che si basa sull‚Äôosservazione che proteggere solo l‚Äô1% dei pesi salienti pu√≤ ridurre notevolmente l‚Äôerrore di quantizzazione.\n\n\n9.3.10 Quantizzazione e Potatura\nPruning [potatura] e quantizzazione funzionano bene insieme ed √® stato scoperto che il pruning non ostacola la quantizzazione. In effetti, il pruning pu√≤ aiutare a ridurre l‚Äôerrore di quantizzazione. Intuitivamente, ci√≤ √® dovuto al pruning che riduce il numero di pesi da quantizzare, riducendo cos√¨ l‚Äôerrore accumulato dalla quantizzazione. Ad esempio, una AlexNet non potata ha 60 milioni di pesi da quantizzare mentre una AlexNet potata ha solo 6.7 milioni di pesi da quantizzare. Questa significativa riduzione dei pesi aiuta a ridurre l‚Äôerrore tra la quantizzazione dell‚ÄôAlexNet non potato rispetto all‚ÄôAlexNet potato. Inoltre, studi recenti hanno scoperto che il pruning consapevole della quantizzazione genera modelli pi√π efficienti dal punto di vista computazionale rispetto al pruning o alla quantizzazione da soli; in genere, ha prestazioni simili o migliori in termini di efficienza computazionale rispetto ad altre tecniche di ricerca dell‚Äôarchitettura neurale come l‚Äôottimizzazione bayesiana (Hawks et al. 2021).\n\n\n\n\n\n\nFigura¬†9.32: Precisione rispetto al tasso di compressione con diversi metodi di compressione. Fonte: Han, Mao, e Dally (2015).\n\n\nHan, Song, Huizi Mao, e William J Dally. 2015. ¬´Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding¬ª. arXiv preprint arXiv:1510.00149.\n\n\n\n\n9.3.11 Quantizzazione Edge-aware\nLa quantizzazione non solo riduce le dimensioni del modello, ma consente anche calcoli pi√π rapidi e consuma meno energia, rendendola fondamentale per lo sviluppo per edge. I dispositivi edge in genere hanno vincoli di risorse rigidi con elaborazione, memoria e potenza, impossibili da soddisfare per molti dei modelli deep NN profondi odierni. Inoltre, i processori edge non supportano le operazioni in virgola mobile, rendendo la quantizzazione intera particolarmente importante per chip come GAP-8, un SoC RISC-V per l‚Äôinferenza edge con un acceleratore CNN dedicato, che supporta solo l‚Äôaritmetica intera.\nUna piattaforma hardware che utilizza la quantizzazione √® il gruppo ARM Cortex-M di core di processori ARM RISC a 32 bit. Sfruttano la quantizzazione a virgola fissa con fattori di scala di potenza di due, in modo che la quantizzazione e la de-quantizzazione possano essere eseguite in modo efficiente tramite spostamento di bit. Inoltre, Google Edge TPU, la soluzione emergente di Google per l‚Äôesecuzione di inferenze in periferia, √® progettata per dispositivi piccoli e a bassa potenza e pu√≤ supportare solo l‚Äôaritmetica a 8 bit. Molti modelli di reti neurali complesse che potevano essere distribuiti solo su server a causa delle loro elevate esigenze di elaborazione possono ora essere eseguiti su dispositivi edge grazie ai recenti progressi (ad esempio metodi di quantizzazione) nel campo dell‚Äôedge computing.\nOltre a essere una tecnica indispensabile per molti processori edge, la quantizzazione ha anche apportato notevoli miglioramenti ai processori non edge, incoraggiando tali processori a soddisfare i requisiti del Service Level Agreement (SLA) come la latenza del 99¬∞ percentile.\nPertanto, la quantizzazione combinata con una logica efficiente a bassa precisione e acceleratori dedicati di deep learning, √® stata una forza trainante cruciale per l‚Äôevoluzione di tali processori edge.\nVideo¬†9.1 √® una lezione sulla quantizzazione e sui diversi metodi di quantizzazione.\n\n\n\n\n\n\nVideo¬†9.1: Quantizzazione",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Ottimizzazioni dei Modelli</span>"
    ]
  },
  {
    "objectID": "contents/core/optimizations/optimizations.it.html#sec-model_ops_hw",
    "href": "contents/core/optimizations/optimizations.it.html#sec-model_ops_hw",
    "title": "9¬† Ottimizzazioni dei Modelli",
    "section": "9.4 Implementazione Hardware Efficiente",
    "text": "9.4 Implementazione Hardware Efficiente\nL‚Äôimplementazione hardware efficiente trascende la selezione di componenti adatti; richiede una comprensione olistica di come il software interagir√† con le architetture sottostanti. L‚Äôessenza del raggiungimento delle massime prestazioni nelle applicazioni TinyML non risiede solo nell‚Äôaffinare gli algoritmi per l‚Äôhardware, ma anche nell‚Äôassicurare che l‚Äôhardware sia strategicamente adattato per supportare questi algoritmi. Questa sinergia tra hardware e software √® fondamentale. Mentre esaminiamo pi√π a fondo le complessit√† dell‚Äôimplementazione hardware efficiente, il significato di un approccio di progettazione congiunta, in cui hardware e software vengono sviluppati in tandem, diventa sempre pi√π evidente. Questa sezione fornisce una panoramica delle tecniche di come l‚Äôhardware e le interazioni tra hardware e software possono essere ottimizzati per migliorare le prestazioni dei modelli.\n\n9.4.1 Ricerca di Architettura Neurale Basata sull‚ÄôHardware\nConcentrarsi solo sulla precisione durante l‚Äôesecuzione della ricerca di architettura neurale porta a modelli esponenzialmente complessi e che richiedono memoria e capacit√† di elaborazione crescenti. Ci√≤ ha portato a vincoli hardware che limitano lo sfruttamento dei modelli di apprendimento profondo al loro pieno potenziale. Progettare manualmente l‚Äôarchitettura del modello √® ancora pi√π difficile se si considerano la variet√† e le limitazioni dell‚Äôhardware. Ci√≤ ha portato alla creazione di Hardware-aware Neural Architecture Search che incorpora le contrazioni hardware nella loro ricerca e ottimizza lo spazio di ricerca per un hardware e una precisione specifici. HW-NAS pu√≤ essere categorizzato in base a come ottimizza per l‚Äôhardware. Esploreremo brevemente queste categorie e lasceremo dei link a documenti correlati per il lettore interessato.\n\nConfigurazione Single Target, Fixed Platfrom\nL‚Äôobiettivo qui √® trovare la migliore architettura in termini di precisione ed efficienza hardware per un hardware target fisso. Per un hardware specifico, ad esempio Arduino Nicla Vision, questa categoria di HW-NAS cercher√† l‚Äôarchitettura che ottimizza precisione, latenza, consumo energetico, ecc.\n\nStrategia di Ricerca Hardware-aware\nQui, la ricerca √® un problema di ottimizzazione multi-obiettivo, in cui sia l‚Äôaccuratezza che il costo dell‚Äôhardware guidano l‚Äôalgoritmo di ricerca per trovare l‚Äôarchitettura pi√π efficiente (Tan et al. 2019; Cai, Zhu, e Han 2019; B. Wu et al. 2019).\n\nTan, Mingxing, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew Howard, e Quoc V. Le. 2019. ¬´MnasNet: Platform-aware Neural Architecture Search for Mobile¬ª. In 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2820‚Äì28. IEEE. https://doi.org/10.1109/cvpr.2019.00293.\n\nCai, Han, Ligeng Zhu, e Song Han. 2019. ¬´ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware¬ª. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net. https://openreview.net/forum?id=HylVB3AqYm.\n\nWu, Bichen, Kurt Keutzer, Xiaoliang Dai, Peizhao Zhang, Yanghan Wang, Fei Sun, Yiming Wu, Yuandong Tian, Peter Vajda, e Yangqing Jia. 2019. ¬´FBNet: Hardware-aware Efficient ConvNet Design via Differentiable Neural Architecture Search¬ª. In 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 10734‚Äì42. IEEE. https://doi.org/10.1109/cvpr.2019.01099.\n\n\nSpazio di Ricerca Hardware-aware\nQui, lo spazio di ricerca √® limitato alle architetture che funzionano bene sull‚Äôhardware specifico. Questo pu√≤ essere ottenuto misurando le prestazioni degli operatori (operatore Conv, operatore Pool, ‚Ä¶) o definendo un set di regole che limitano lo spazio di ricerca. (L. L. Zhang et al. 2020)\n\nZhang, Li Lyna, Yuqing Yang, Yuhang Jiang, Wenwu Zhu, e Yunxin Liu. 2020. ¬´Fast Hardware-Aware Neural Architecture Search¬ª. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW). IEEE. https://doi.org/10.1109/cvprw50498.2020.00354.\n\n\n\nConfigurazioni Single Target, Multiple Platform\nAlcuni hardware possono avere configurazioni diverse. Ad esempio, gli FPGA hanno blocchi logici configurabili (CLB) che possono essere configurati dal firmware. Questo metodo consente all‚ÄôHW-NAS di esplorare diverse configurazioni. (Hu et al. 2023; Ho Yoon et al. 2012)\n\nHo Yoon, Jung, Hyung-Suk Jung, Min Hwan Lee, Gun Hwan Kim, Seul Ji Song, Jun Yeong Seok, Kyung Jean Yoon, et al. 2012. ¬´Frontiers in Electronic Materials¬ª. Wiley. https://doi.org/10.1002/9783527667703.ch67.\n\n\nTarget Multipli\nQuesta categoria mira a ottimizzare un singolo modello per pi√π hardware. Questo pu√≤ essere utile per lo sviluppo di dispositivi mobili in quanto pu√≤ ottimizzare diversi modelli di telefoni. (Chu et al. 2021; Hu et al. 2023)\n\nChu, Grace, Okan Arikan, Gabriel Bender, Weijun Wang, Achille Brighton, Pieter-Jan Kindermans, Hanxiao Liu, Berkin Akin, Suyog Gupta, e Andrew Howard. 2021. ¬´Discovering Multi-Hardware Mobile Models via Architecture Search¬ª. In 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), 3022‚Äì31. IEEE. https://doi.org/10.1109/cvprw53098.2021.00337.\n\nHu, Yang, Jie Jiang, Lifu Zhang, Yunfeng Shi, e Jian Shi. 2023. ¬´Halide Perovskite Semiconductors¬ª. Wiley. https://doi.org/10.1002/9783527829026.ch13.\n\n\nEsempi di ‚ÄúHardware-Aware Neural Architecture Search‚Äù\n\nTinyNAS\nTinyNAS adotta un approccio in due fasi per trovare un‚Äôarchitettura ottimale per il modello tenendo a mente i vincoli del microcontrollore specifico.\nInnanzitutto, TinyNAS genera pi√π spazi di ricerca variando la risoluzione di input del modello e il numero di canali dei layer. Quindi, TinyNAS sceglie uno spazio di ricerca in base ai FLOP (operazioni in virgola mobile al secondo) di ogni spazio di ricerca. Gli spazi con una probabilit√† maggiore di contenere architetture con un numero elevato di FLOP producono modelli con maggiore accuratezza: confrontare la linea rossa con la linea nera in Figura¬†9.33. Poich√© un numero maggiore di FLOP significa che il modello ha una maggiore capacit√† di calcolo, √® pi√π probabile che il modello abbia una maggiore accuratezza.\nPoi, TinyNAS esegue un‚Äôoperazione di ricerca sullo spazio scelto per trovare l‚Äôarchitettura ottimale per i vincoli specifici del microcontrollore. (J. Lin et al. 2020)\n\n\n\n\n\n\nFigura¬†9.33: Precisione degli spazi di ricerca. Fonte: J. Lin et al. (2020).\n\n\nLin, Ji, Wei-Ming Chen, Yujun Lin, John Cohn, Chuang Gan, e Song Han. 2020. ¬´MCUNet: Tiny Deep Learning on IoT Devices¬ª. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, a cura di Hugo Larochelle, Marc‚ÄôAurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, e Hsuan-Tien Lin. https://proceedings.neurips.cc/paper/2020/hash/86c51678350f656dcc7f490a43946ee5-Abstract.html.\n\n\n\n\n\nTopology-Aware NAS\nSi concentra sulla creazione e l‚Äôottimizzazione di uno spazio di ricerca allineato alla topologia hardware del dispositivo. (T. Zhang et al. 2020)\n\nZhang, Tunhou, Hsin-Pai Cheng, Zhenwen Li, Feng Yan, Chengyu Huang, Hai Helen Li, e Yiran Chen. 2020. ¬´AutoShrink: A Topology-Aware NAS for Discovering Efficient Neural Architecture¬ª. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, 6829‚Äì36. AAAI Press. https://aaai.org/ojs/index.php/AAAI/article/view/6163.\n\n\n\n9.4.2 Sfide nella ‚ÄúHardware-Aware Neural Architecture Search‚Äù\nSebbene HW-NAS abbia un potenziale elevato per trovare architetture ottimali per TinyML, presenta alcuni problemi. Le metriche hardware come latenza, consumo energetico e utilizzo dell‚Äôhardware sono pi√π difficili da valutare rispetto alle metriche di accuratezza o di perdita. Spesso richiedono strumenti specializzati per misure precise. Inoltre, l‚Äôaggiunta di tutte queste metriche porta a uno spazio di ricerca molto pi√π grande. Ci√≤ fa s√¨ che HW-NAS sia dispendioso in termini di tempo e denaro. Deve essere applicato a ogni hardware per risultati ottimali, tra le altre cose, il che significa che se si deve distribuire il modello su pi√π dispositivi, la ricerca deve essere condotta pi√π volte e produrr√† modelli diversi, a meno che non si ottimizzi per tutti, il che significa una minore accuratezza. Infine, l‚Äôhardware cambia frequentemente e potrebbe essere necessario eseguire HW-NAS su ogni versione.\n\n\n9.4.3 Ottimizzazioni del Kernel\nLe ottimizzazioni del kernel sono modifiche apportate al kernel per migliorare le prestazioni dei modelli di apprendimento automatico su dispositivi con risorse limitate. Separeremo le ottimizzazioni del kernel in due tipi.\n\nOttimizzazioni del kernel Generali\nQueste sono ottimizzazioni del kernel da cui tutti i dispositivi possono trarre vantaggio. Forniscono tecniche per convertire il codice in istruzioni pi√π efficienti.\n\n‚ÄúSrotolamento‚Äù del Loop\nInvece di avere un loop con ‚Äúloop control‚Äù (incrementando il contatore, si controlla la condizione di terminazione del loop), il loop pu√≤ essere srotolato e il sovraccarico del ‚Äúloop control‚Äù pu√≤ essere omesso. Questo pu√≤ anche fornire ulteriori opportunit√† di parallelismo che potrebbero non essere possibili con la struttura con loop. Questo pu√≤ essere particolarmente utile per loop stretti, in cui il corpo del loop √® un piccolo numero di istruzioni con molte iterazioni.\n\n\nBlocking\nIl Blocking viene utilizzato per rendere pi√π efficienti i pattern di accesso alla memoria. Se abbiamo tre calcoli, il primo e l‚Äôultimo devono accedere alla cache A e il secondo deve accedere alla cache B, il ‚Äúblocking‚Äù ferma i primi due calcoli per ridurre il numero di letture di memoria necessarie.\n\n\nTiling\nAnalogamente al blocking, il tiling [piastrellatura] divide i dati e il calcolo in blocchi, ma si estende oltre i miglioramenti della cache. Il tiling crea partizioni di calcolo indipendenti che possono essere eseguite in parallelo, il che pu√≤ comportare significativi miglioramenti delle prestazioni.\n\n\nLibrerie Kernel Ottimizzate\nQuesto comprende lo sviluppo di kernel ottimizzati che sfruttano appieno un hardware specifico. Un esempio √® la libreria CMSIS-NN, che √® una raccolta di kernel di reti neurali efficienti sviluppati per ottimizzare le prestazioni e ridurre al minimo l‚Äôingombro di memoria dei modelli sui processori Arm Cortex-M, comuni sui dispositivi edge IoT. Il kernel sfrutta pi√π capacit√† hardware dei processori Cortex-M come Single Instruction Multiple Data (SIMD), Floating Point Unit (FPU) e M-Profile Vector Extensions (MVE). Queste ottimizzazioni rendono pi√π efficienti le operazioni comuni come le moltiplicazioni di matrici, aumentando le prestazioni delle operazioni del modello sui processori Cortex-M. (Lai, Suda, e Chandra 2018)\n\nLai, Liangzhen, Naveen Suda, e Vikas Chandra. 2018. ¬´CMSIS-NN: Efficient Neural Network Kernels for Arm Cortex-M CPUs¬ª. https://arxiv.org/abs/1801.06601.\n\n\n\n\n9.4.4 Compute-in-Memory (CiM)\nQuesto √® un esempio di progettazione congiunta di algoritmo e hardware. CiM √® un paradigma di elaborazione che esegue calcoli all‚Äôinterno della memoria. Pertanto, le architetture CiM consentono di eseguire operazioni direttamente sui dati archiviati, senza la necessit√† di spostare i dati avanti e indietro tra unit√† di elaborazione e memoria separate. Questo paradigma di progettazione √® particolarmente utile in scenari in cui lo spostamento dei dati √® una fonte primaria di consumo energetico e latenza, come nelle applicazioni TinyML su dispositivi edge. Figura¬†9.34 √® un esempio di utilizzo di CiM in TinyML: l‚Äôindividuazione delle parole chiave richiede un processo sempre attivo che cerca determinate parole di attivazione (come ‚ÄúHey, Siri‚Äù). Data la natura ad alta intensit√† di risorse di questa attivit√†, l‚Äôintegrazione di CiM per il modello di rilevamento delle parole chiave sempre attivo pu√≤ migliorare l‚Äôefficienza.\nAttraverso la progettazione congiunta di algoritmo e hardware, gli algoritmi possono essere ottimizzati per sfruttare le caratteristiche uniche delle architetture CiM e, l‚Äôhardware CiM pu√≤ essere personalizzato o configurato per supportare meglio i requisiti di elaborazione e le caratteristiche degli algoritmi. Ci√≤ si ottiene utilizzando le propriet√† analogiche delle celle di memoria, come l‚Äôaddizione e la moltiplicazione nella DRAM. (Zhou et al. 2021)\n\n\n\n\n\n\nFigura¬†9.34: CiM per l‚Äôindividuazione delle parole chiave. Fonte: Zhou et al. (2021).\n\n\nZhou, Chuteng, Fernando Garcia Redondo, Julian B√ºchel, Irem Boybat, Xavier Timoneda Comas, S. R. Nandakumar, Shidhartha Das, Abu Sebastian, Manuel Le Gallo, e Paul N. Whatmough. 2021. ¬´AnalogNets: Ml-hw Co-Design of Noise-robust TinyML Models and Always-On Analog Compute-in-Memory Accelerator¬ª. https://arxiv.org/abs/2111.06503.\n\n\n\n\n9.4.5 Ottimizzazione dell‚ÄôAccesso alla Memoria\nDispositivi diversi possono avere gerarchie di memorie diverse. L‚Äôottimizzazione per la gerarchia di memoria specifica nell‚Äôhardware specifico pu√≤ portare a grandi miglioramenti delle prestazioni riducendo le costose operazioni di lettura e scrittura nella memoria. L‚Äôottimizzazione del flusso di dati pu√≤ essere ottenuta ottimizzando il riutilizzo dei dati all‚Äôinterno di un singolo layer e tra pi√π layer. Questa ottimizzazione del flusso di dati pu√≤ essere adattata alla gerarchia di memoria specifica dell‚Äôhardware, il che pu√≤ portare a maggiori vantaggi rispetto alle ottimizzazioni generali per diversi hardware.\n\nSfruttamento dei Dati Sparsi\nIl Pruning [potatura] √® un approccio fondamentale per comprimere i modelli e renderli compatibili con dispositivi con risorse limitate. Ci√≤ si traduce in modelli sparsi in cui molti pesi sono 0. Pertanto, sfruttare questa diradazione pu√≤ portare a miglioramenti significativi nelle prestazioni. Sono stati creati degli strumenti per ottenere esattamente questo. RAMAN, √® un acceleratore TinyML sparse progettato per l‚Äôinferenza su dispositivi edge. RAMAN sovrappone le attivazioni di input e output sullo stesso spazio di memoria, riducendo i requisiti di archiviazione fino al 50%. (Krishna et al. 2023)\n\nKrishna, Adithya, Srikanth Rohit Nudurupati, Chandana D G, Pritesh Dwivedi, Andr√© van Schaik, Mahesh Mehendale, e Chetan Singh Thakur. 2023. ¬´RAMAN: A Re-configurable and Sparse TinyML Accelerator for Inference on Edge¬ª. https://arxiv.org/abs/2306.06493.\n\n\nFramework di Ottimizzazione\nI framework di ottimizzazione sono stati introdotti per sfruttare le capacit√† specifiche dell‚Äôhardware per accelerare il software. Un esempio di tale framework √® hls4ml: Figura¬†9.35 fornisce una panoramica del flusso di lavoro del framework. Questo flusso di lavoro di co-progettazione software-hardware open source aiuta a interpretare e tradurre algoritmi di machine learning per l‚Äôimplementazione con tecnologie FPGA e ASIC. Funzionalit√† quali ottimizzazione di rete, nuove API Python, potatura consapevole della quantizzazione e flussi di lavoro FPGA end-to-end sono integrate nel framework hls4ml, sfruttando unit√† di elaborazione parallele, gerarchie di memoria e set di istruzioni specializzati per ottimizzare i modelli per hardware edge. Inoltre, hls4ml √® in grado di tradurre algoritmi di apprendimento automatico direttamente nel firmware FPGA.\n\n\n\n\n\n\nFigura¬†9.35: workflow del framework hls4ml. Fonte: Fahim et al. (2021).\n\n\nFahim, Farah, Benjamin Hawks, Christian Herwig, James Hirschauer, Sergo Jindariani, Nhan Tran, Luca P. Carloni, et al. 2021. ¬´hls4ml: An Open-Source Codesign Workflow to Empower Scientific Low-Power Machine Learning Devices¬ª. https://arxiv.org/abs/2103.05579.\n\n\nUn altro framework per FPGA che si concentra su un approccio olistico √® CFU Playground (Prakash et al. 2023)\n\nPrakash, Shvetank, Tim Callahan, Joseph Bushagour, Colby Banbury, Alan V. Green, Pete Warden, Tim Ansell, e Vijay Janapa Reddi. 2023. ¬´CFU Playground: Full-stack Open-Source Framework for Tiny Machine Learning (TinyML) Acceleration on FPGAs¬ª. In 2023 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS). Vol. abs/2201.01863. IEEE. https://doi.org/10.1109/ispass57527.2023.00024.\n\n\nHardware Costruito Attorno al Software\nIn un approccio contrastante, l‚Äôhardware pu√≤ essere progettato su misura attorno ai requisiti software per ottimizzare le prestazioni per un‚Äôapplicazione specifica. Questo paradigma crea hardware specializzato per adattarsi meglio alle specifiche del software, riducendo cos√¨ il sovraccarico computazionale e migliorando l‚Äôefficienza operativa. Un esempio di questo approccio √® un‚Äôapplicazione di riconoscimento vocale di (Kwon e Park 2021). Il documento propone una struttura in cui le operazioni di pre-elaborazione, tradizionalmente gestite dal software, sono assegnate ad un hardware progettato su misura. Questa tecnica √® stata ottenuta introducendo la logica resistore-transistor in un modulo audio a circuito inter-integrato per il windowing e l‚Äôacquisizione di dati audio grezzi nell‚Äôapplicazione di riconoscimento vocale. Di conseguenza, questa ‚Äúdelega‚Äù delle operazioni di pre-elaborazione ha portato a una riduzione del carico computazionale sul software, mostrando un‚Äôapplicazione pratica della creazione di hardware attorno al software per migliorare l‚Äôefficienza e le prestazioni.\n\n\n\n\n\n\nFigura¬†9.36: Delega dell‚Äôelaborazione dei dati a un FPGA. Fonte: Kwon e Park (2021).\n\n\nKwon, Jisu, e Daejin Park. 2021. ¬´Hardware/Software Co-Design for TinyML Voice-Recognition Application on Resource Frugal Edge Devices¬ª. Applied Sciences 11 (22): 11073. https://doi.org/10.3390/app112211073.\n\n\n\n\nSplitNet\nLi SplitNet sono state introdotte nel contesto dei sistemi Head-Mounted. Distribuiscono il carico di lavoro delle Deep Neural Network (DNN) tra i sensori della telecamera e un aggregatore. Ci√≤ √® particolarmente interessante nel contesto di TinyML. Il framework SplitNet √® un NAS split-aware per trovare l‚Äôarchitettura di rete neurale ottimale per ottenere una buona accuratezza, dividere il modello tra i sensori e l‚Äôaggregatore e ridurre al minimo la comunicazione tra i sensori e l‚Äôaggregatore.\nFigura¬†9.37 dimostra come le SplitNet (in rosso) ottengano una maggiore accuratezza per una latenza inferiore (in esecuzione su ImageNet) rispetto ad altri approcci, come l‚Äôesecuzione del DNN sul sensore (All-on-sensor; in verde) o sul cellulare (All-on-aggregator; in blu). La comunicazione minima √® importante in TinyML dove la memoria √® fortemente limitata, in questo modo i sensori conducono parte dell‚Äôelaborazione sui loro chip e poi inviano solo le informazioni necessarie all‚Äôaggregatore. Durante i test su ImageNet, SplitNets √® stato in grado di ridurre la latenza di un ordine di grandezza sui dispositivi di visione artificiale montati sulla testa [occhiali o visori]. Ci√≤ pu√≤ essere utile quando il sensore ha il suo chip. (Dong et al. 2022)\n\n\n\n\n\n\nFigura¬†9.37: Le SplitNet rispetto ad altri approcci. Fonte: Dong et al. (2022).\n\n\nDong, Xin, Barbara De Salvo, Meng Li, Chiao Liu, Zhongnan Qu, H. T. Kung, e Ziyun Li. 2022. ¬´SplitNets: Designing Neural Architectures for Efficient Distributed Computing on Head-Mounted Systems¬ª. In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 12549‚Äì59. IEEE. https://doi.org/10.1109/cvpr52688.2022.01223.\n\n\n\n\nHardware Specifico per il ‚ÄúData Augmentation‚Äù\nOgni dispositivo edge pu√≤ possedere caratteristiche di sensore uniche, che portano a specifici pattern di rumore che possono influire sulle prestazioni del modello. Un esempio sono i dati audio, in cui sono prevalenti le variazioni derivanti dalla scelta del microfono. Applicazioni come le Keyword Spotting possono sperimentare miglioramenti sostanziali incorporando dati registrati da dispositivi simili a quelli destinati all‚Äôimplementazione. La messa a punto dei modelli esistenti pu√≤ essere impiegata per adattare i dati in modo preciso alle caratteristiche distintive del sensore.",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Ottimizzazioni dei Modelli</span>"
    ]
  },
  {
    "objectID": "contents/core/optimizations/optimizations.it.html#supporto-software-e-framework",
    "href": "contents/core/optimizations/optimizations.it.html#supporto-software-e-framework",
    "title": "9¬† Ottimizzazioni dei Modelli",
    "section": "9.5 Supporto Software e Framework",
    "text": "9.5 Supporto Software e Framework\nSebbene tutte le tecniche sopra menzionate come pruning, quantizzazione e numeri efficienti siano ben note, rimarrebbero poco pratiche e inaccessibili senza un ampio supporto software. Ad esempio, la quantizzazione diretta di pesi e attivazioni in un modello richiederebbe la modifica manuale della definizione del modello e l‚Äôinserimento di operazioni di quantizzazione. Allo stesso modo, la potatura diretta dei pesi del modello richiede la manipolazione dei tensori dei pesi. Tali approcci noiosi diventano impraticabili su larga scala.\nSenza l‚Äôampia innovazione software nei framework, negli strumenti di ottimizzazione e nell‚Äôintegrazione hardware, la maggior parte di queste tecniche rimarrebbe teorica o praticabile solo per gli esperti. Senza API del framework e automazione per semplificare l‚Äôapplicazione di queste ottimizzazioni, non verrebbero adottate. Il supporto software le rende accessibili al pubblico e sblocca vantaggi concreti. Inoltre, problemi come la messa a punto degli iperparametri per la potatura, la gestione del compromesso tra dimensioni del modello e accuratezza e la garanzia della compatibilit√† con i dispositivi target pongono ostacoli che gli sviluppatori devono superare.\n\n9.5.1 API Native di Ottimizzazione\nI principali framework di machine learning come TensorFlow, PyTorch e MXNet forniscono librerie e API per consentire l‚Äôapplicazione di tecniche comuni di ottimizzazione dei modelli senza richiedere implementazioni personalizzate. Ad esempio, TensorFlow offre il TensorFlow Model Optimization Toolkit che contiene moduli come:\n\nQuantization: Applica un training che tiene conto della quantizzazione per convertire i modelli in virgola mobile in una precisione inferiore come int8 con una perdita di accuratezza minima. Gestisce la quantizzazione del peso e dell‚Äôattivazione.\nSparsity: Fornisce API di potatura per indurre la ‚Äúsparsit√†‚Äù e rimuovere connessioni non necessarie in modelli come le reti neurali. Pu√≤ potare pesi, livelli, ecc.\nClustering: Supporta la compressione del modello raggruppando i pesi per tassi di compressione pi√π elevati.\n\nQueste API consentono agli utenti di abilitare tecniche di ottimizzazione come la quantizzazione e la potatura senza modificare direttamente il codice del modello. √à possibile configurare parametri come i tassi di ‚Äúsparsit√†‚Äù del target, le larghezze di bit di quantizzazione, ecc. Allo stesso modo, PyTorch fornisce torch.quantization per convertire i modelli in rappresentazioni di precisione inferiore. TorchTensor e TorchModule formano le classi di base per il supporto della quantizzazione. Offre inoltre torch.nn.utils.prune per la potatura nativa dei modelli. MXNet offre layer gluon.contrib che aggiungono funzionalit√† di quantizzazione come l‚Äôarrotondamento a punto fisso e l‚Äôarrotondamento stocastico di pesi/attivazioni durante l‚Äôaddestramento. Ci√≤ consente di includere facilmente la quantizzazione nei modelli gluon.\nIl vantaggio principale delle ottimizzazioni integrate √® che gli utenti possono applicarle senza dover reimplementare tecniche complesse. Ci√≤ rende i modelli ottimizzati accessibili a un‚Äôampia gamma di professionisti. Garantisce inoltre che le best practice siano seguite basandosi sulla ricerca e sull‚Äôesperienza nell‚Äôimplementazione dei metodi. Man mano che emergono nuove ottimizzazioni, i framework si sforzano di fornire supporto nativo e API ove possibile per abbassare ulteriormente la barriera verso un ML efficiente. La disponibilit√† di questi strumenti √® fondamentale per un‚Äôadozione diffusa.\n\n\n9.5.2 Strumenti di Ottimizzazione Automatizzata\nGli strumenti di ottimizzazione automatizzati forniti dai framework possono analizzare i modelli e applicare automaticamente ottimizzazioni come quantizzazione, potatura e fusione degli operatori per rendere il processo pi√π semplice e accessibile senza un‚Äôeccessiva messa a punto manuale. In effetti, questo si basa sulla sezione precedente. Ad esempio, TensorFlow fornisce il TensorFlow Model Optimization Toolkit che contiene moduli come:\n\nQuantizationAwareTraining: Quantizza automaticamente pesi e attivazioni in un modello per ridurre la precisione come UINT8 o INT8 con una perdita di accuratezza minima. Inserisce nodi di quantizzazione falsi durante l‚Äôaddestramento in modo che il modello possa imparare a essere compatibile con la quantizzazione.\nPruning: Rimuove automaticamente le connessioni non necessarie in un modello in base all‚Äôanalisi dell‚Äôimportanza del peso. Pu√≤ potare interi filtri in livelli convoluzionali o ‚Äúattention head‚Äù [teste di attenzione] nei trasformatori. Gestisce il ri-addestramento iterativo per recuperare qualsiasi perdita di accuratezza.\nGraphOptimizer: Applica ottimizzazioni grafiche come la fusione degli operatori per consolidare le operazioni e ridurre la latenza di esecuzione, in particolare per l‚Äôinferenza. In Figura¬†9.38, si pu√≤ vedere l‚Äôoriginale (Source Graph) a sinistra e come le sue operazioni vengono trasformate (consolidate) a destra. Notare come Block1 in Source Graph abbia 3 passaggi separati (Convolution, BiasAdd e Activation), che vengono poi consolidati insieme in Block1 su Optimized Graph.\n\n\n\n\n\n\n\nFigura¬†9.38: GraphOptimizer. Fonte: Wess et al. (2020).\n\n\nWess, Matthias, Matvey Ivanov, Christoph Unger, e Anvesh Nookala. 2020. ¬´ANNETTE: Accurate Neural Network Execution Time Estimation with Stacked Models¬ª. IEEE. https://doi.org/10.1109/ACCESS.2020.3047259.\n\n\nQuesti moduli automatizzati richiedono solo all‚Äôutente di fornire il modello originale in virgola mobile e di gestire la pipeline di ottimizzazione end-to-end, inclusa qualsiasi riqualificazione per ripristinare la precisione. Anche altri framework come PyTorch offrono un crescente supporto all‚Äôautomazione, ad esempio tramite torch.quantization.quantize_dynamic. L‚Äôottimizzazione automatizzata rende l‚Äôapprendimento automatico efficiente accessibile ai professionisti senza competenze di ottimizzazione.\n\n\n9.5.3 Librerie di Ottimizzazione Hardware\nLibrerie hardware come TensorRT e TensorFlow XLA consentono di ottimizzare i modelli per l‚Äôhardware target tramite tecniche di cui abbiamo discusso in precedenza.\n\nQuantizzazione: Ad esempio, TensorRT e TensorFlow Lite supportano entrambi la quantizzazione dei modelli durante la conversione nel loro formato. Ci√≤ fornisce accelerazioni sui SoC mobili con supporto INT8/INT4.\nOttimizzazione del Kernel: ad esempio, TensorRT esegue l‚Äôauto-tuning per ottimizzare i kernel CUDA in base all‚Äôarchitettura GPU per ogni layer nel grafo del modello. Ci√≤ estrae la massima produttivit√†.\nFusione degli Operatori: TensorFlow XLA esegue una fusione aggressiva per creare un binario ottimizzato per le TPU. Sui dispositivi mobili, framework come NCNN supportano anche operatori fusi [unificati].\nCodice Specifico per l‚ÄôHardware: Le librerie vengono utilizzate per generare codice binario ottimizzato specializzato per l‚Äôhardware target. Per esempio, TensorRT usa librerie Nvidia CUDA/cuDNN che sono ottimizzate manualmente per ogni architettura GPU. Questa codifica specifica per hardware √® fondamentale per le prestazioni. Sui dispositivi TinyML, questo pu√≤ significare codice assembly ottimizzato per una CPU Cortex M4, ad esempio. I fornitori forniscono CMSIS-NN e altre librerie.\nOttimizzazioni del Layout dei Dati: Possiamo sfruttare in modo efficiente la gerarchia della memoria di hardware come cache e registri tramite tecniche come riorganizzazione tensore/peso, tiling e riutilizzo. Ad esempio, TensorFlow XLA ottimizza i layout dei buffer per massimizzare l‚Äôutilizzo della TPU. Questo aiuta qualsiasi sistema con limiti di memoria.\nOttimizzazione Basata sulla Profilazione: Possiamo usare strumenti di profilazione per identificare i colli di bottiglia. Ad esempio, regolare i livelli di fusione del kernel in base alla profilazione della latenza. Sui SoC mobili, fornitori come Qualcomm forniscono profiler in SNPE per trovare opportunit√† di ottimizzazione nelle CNN. Questo approccio basato sui dati √® importante per le prestazioni.\n\nIntegrando i modelli di framework con queste librerie hardware tramite pipeline di conversione ed esecuzione, gli sviluppatori di ML possono ottenere significativi incrementi di velocit√† e guadagni di efficienza da ottimizzazioni di basso livello su misura per l‚Äôhardware target. La stretta integrazione tra software e hardware √® fondamentale per consentire un‚Äôimplementazione performante delle applicazioni di ML, in particolare su dispositivi mobili e TinyML.\n\n\n9.5.4 Visualizzazione delle Ottimizzazioni\nL‚Äôimplementazione di tecniche di ottimizzazione del modello senza visibilit√† degli effetti sul modello pu√≤ essere impegnativa. Strumenti dedicati o strumenti di visualizzazione possono fornire informazioni critiche e utili sulle modifiche del modello e aiutano a tracciare il processo di ottimizzazione. Consideriamo le ottimizzazioni che abbiamo considerato in precedenza, come la potatura per la ‚Äúsparsity‚Äù [diradazione] e la quantizzazione.\n\nSparsit√†\nAd esempio, si considerino le ottimizzazioni di sparsity. Gli strumenti di visualizzazione di sparsity possono fornire informazioni critiche sui modelli potati, mappando esattamente quali pesi sono stati rimossi. Ad esempio, le mappe di calore di sparsity possono utilizzare gradienti di colore per indicare la percentuale di pesi potati in ogni layer di una rete neurale. I layer con percentuali di potatura pi√π elevate appaiono pi√π scuri (cfr. Figura¬†9.39). Questo identifica quali layer sono stati semplificati di pi√π tramite potatura (Souza 2020).\n\n\n\n\n\n\nFigura¬†9.39: Mappa ‚Äútermica‚Äù della rete sparsa. Fonte: Numenta.\n\n\n\nI grafici di tendenza possono anche tracciare la scarsit√† nei successivi round di potatura: possono mostrare una rapida potatura iniziale seguita da incrementi pi√π graduali. Il tracciamento della diradazione globale corrente insieme a statistiche come la diradazione media, minima e massima per ogli layer in tabelle o grafici fornisce una panoramica della composizione del modello. Per una rete convoluzionale di esempio, questi strumenti potrebbero rivelare che il primo layer di convoluzione viene potato del 20% mentre quello di classificazione finale viene potato del 70% data la sua ridondanza. La diradazione del modello globale pu√≤ aumentare dal 10% dopo la potatura iniziale al 40% dopo cinque round.\nRendendo i dati di diradazione visivamente accessibili, i professionisti possono comprendere meglio esattamente come il loro modello viene ottimizzato e quali aree vengono interessate. La visibilit√† consente loro di mettere a punto e controllare il processo di potatura per una determinata architettura.\nLa visualizzazione della diradazione trasforma la potatura in una tecnica trasparente anzich√© in un‚Äôoperazione ‚Äúblack-box‚Äù.\n\n\nQuantizzazione\nLa conversione di modelli in precisioni numeriche inferiori tramite quantizzazione introduce errori che possono influire sulla precisione del modello se non vengono monitorati e affrontati correttamente. La visualizzazione delle distribuzioni degli errori di quantizzazione fornisce informazioni preziose sugli effetti dei numeri di precisione ridotti applicati a diverse parti di un modello. Per questo, √® possibile generare istogrammi degli errori di quantizzazione per pesi e attivazioni. Questi istogrammi possono rivelare la forma della distribuzione degli errori, se assomigliano a una distribuzione gaussiana o contengono valori anomali e picchi significativi. Figura¬†9.40 mostra le distribuzioni di diversi metodi di quantizzazione. Valori anomali elevati possono indicare problemi con particolari layer che gestiscono la quantizzazione. Il confronto degli istogrammi tra layer evidenzia eventuali aree problematiche che si distinguono con errori anormalmente elevati.\n\n\n\n\n\n\nFigura¬†9.40: Errori di Quantizzazione. Fonte: Kuzmin et al. (2022).\n\n\nKuzmin, Andrey, Mart Van Baalen, Yuwei Ren, Markus Nagel, Jorn Peters, e Tijmen Blankevoort. 2022. ¬´FP8 Quantization: The Power of the Exponent¬ª. https://arxiv.org/abs/2208.09225.\n\n\nLe visualizzazioni di attivazione sono importanti anche per rilevare problemi di overflow. Con la mappatura a colori delle attivazioni prima e dopo la quantizzazione, tutti i valori spinti al di fuori degli intervalli previsti diventano visibili. Ci√≤ rivela problemi di saturazione e troncamento che potrebbero alterare le informazioni che fluiscono attraverso il modello. Il rilevamento di questi errori consente di ricalibrare le attivazioni per evitare la perdita di informazioni (Mandal 2022). Figura¬†9.41 √® una mappatura a colori dei kernel convoluzionali AlexNet.\n\n\n\n\n\n\nFigura¬†9.41: Mappatura a colori delle attivazioni. Fonte: Krizhevsky, Sutskever, e Hinton (2017).\n\n\nKrizhevsky, Alex, Ilya Sutskever, e Geoffrey E. Hinton. 2017. ¬´ImageNet classification with deep convolutional neural networks¬ª. A cura di F. Pereira, C. J. Burges, L. Bottou, e K. Q. Weinberger. Commun. ACM 60 (6): 84‚Äì90. https://doi.org/10.1145/3065386.\n\n\nAltre tecniche, come il tracciamento dell‚Äôerrore di quantizzazione quadratico medio complessivo a ogni passaggio del processo di addestramento consapevole della quantizzazione, identificano fluttuazioni e divergenze. Picchi improvvisi nel grafico di tracciamento possono indicare punti in cui la quantizzazione sta interrompendo l‚Äôaddestramento del modello. Il monitoraggio di questa metrica crea intuizione sul comportamento del modello in fase di quantizzazione. Insieme, queste tecniche trasformano la quantizzazione in un processo trasparente. Le intuizioni empiriche consentono ai professionisti di valutare correttamente gli effetti della quantizzazione. Individuano le aree dell‚Äôarchitettura del modello o del processo di training da ricalibrare in base ai problemi di quantizzazione osservati. Ci√≤ aiuta a ottenere modelli quantizzati numericamente stabili e accurati.\nFornire questi dati consente ai professionisti di valutare correttamente l‚Äôimpatto della quantizzazione e identificare potenziali aree problematiche del modello da ricalibrare o riprogettare per renderlo pi√π adatto alla quantizzazione. Questa analisi empirica sviluppa l‚Äôintuizione sul raggiungimento di una quantizzazione ottimale.\nGli strumenti di visualizzazione possono fornire approfondimenti che aiutano i professionisti a comprendere meglio gli effetti delle ottimizzazioni sui loro modelli. La visibilit√† consente di correggere i problemi in anticipo prima che l‚Äôaccuratezza o le prestazioni siano influenzate in modo significativo. Aiuta anche ad applicare le ottimizzazioni in modo pi√π efficace per modelli specifici. Queste analisi di ottimizzazione aiutano a sviluppare l‚Äôintuizione quando si trasferiscono i modelli a rappresentazioni pi√π efficienti.\n\n\n\n9.5.5 Conversione e Distribuzione del Modello\nUna volta che i modelli sono stati ottimizzati con successo in framework come TensorFlow e PyTorch, sono necessarie piattaforme specializzate di conversione e deployment [distribuzione] del modello per colmare il divario con l‚Äôesecuzione sui dispositivi target.\nTensorFlow Lite - La piattaforma di TensorFlow per convertire i modelli in un formato leggero ottimizzato per dispositivi mobili, embedded ed edge. Supporta ottimizzazioni come quantizzazione, fusione del kernel e rimozione di operazioni inutilizzate. I modelli possono essere eseguiti utilizzando kernel TensorFlow Lite ottimizzati sull‚Äôhardware del dispositivo. Fondamentale per la distribuzione mobile e TinyML.\nONNX Runtime - Esegue la conversione e l‚Äôinferenza per i modelli nel formato ‚Äúopen ONNX‚Äù. Fornisce kernel ottimizzati, supporta acceleratori hardware come GPU e distribuzione multipiattaforma dal cloud all‚Äôedge. Consente la distribuzione indipendente dal framework. Figura¬†9.42 √® una mappa di interoperabilit√† ONNX, inclusi i principali framework pi√π diffusi.\n\n\n\n\n\n\nFigura¬†9.42: Interoperabilit√† di ONNX. Fonte: TowardsDataScience.\n\n\n\nPyTorch Mobile - Consente l‚Äôesecuzione dei modelli PyTorch su iOS e Android convertendoli in rappresentazioni ottimizzate per dispositivi mobili. Fornisce implementazioni mobili efficienti di operazioni come convoluzione e funzioni speciali ottimizzate per hardware mobile.\nQueste piattaforme si integrano con driver hardware, sistemi operativi e librerie di acceleratori sui dispositivi per eseguire modelli in modo efficiente utilizzando l‚Äôottimizzazione hardware. Inoltre, delegano le operazioni ad acceleratori ML dedicati, ove presenti. La disponibilit√† di queste piattaforme di distribuzione collaudate e robuste colma il divario tra l‚Äôottimizzazione dei modelli nei framework e la distribuzione effettiva su miliardi di dispositivi. Consentono agli utenti di concentrarsi sullo sviluppo del modello anzich√© sulla creazione di runtime mobili personalizzati. L‚Äôinnovazione continua per supportare nuovi hardware e ottimizzazioni in queste piattaforme √® fondamentale per le ottimizzazioni di ML diffuse.\nFornendo queste pipeline di distribuzione ottimizzate, l‚Äôintero flusso di lavoro, dal training al deployment [distribuzione] del dispositivo, pu√≤ sfruttare le ottimizzazioni del modello per fornire applicazioni ML performanti. Questa infrastruttura software end-to-end ha contribuito a guidare l‚Äôadozione di ML sul dispositivo.",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Ottimizzazioni dei Modelli</span>"
    ]
  },
  {
    "objectID": "contents/core/optimizations/optimizations.it.html#conclusione",
    "href": "contents/core/optimizations/optimizations.it.html#conclusione",
    "title": "9¬† Ottimizzazioni dei Modelli",
    "section": "9.6 Conclusione",
    "text": "9.6 Conclusione\nIn questo capitolo abbiamo discusso l‚Äôottimizzazione del modello nell‚Äôambito software-hardware. Ci siamo immersi in una rappresentazione efficiente del modello, dove abbiamo trattato le sfumature della potatura strutturata e non-strutturata e altre tecniche per la compressione del modello come la distillazione della conoscenza e la decomposizione di matrice e tensore. Ci siamo anche immersi brevemente nella progettazione del modello specifico per l‚Äôedge a livello di parametri e architettura del modello, esplorando argomenti come modelli specifici per l‚Äôedge e NAS basati sull‚Äôhardware.\nAbbiamo quindi esplorato rappresentazioni numeriche efficienti, dove abbiamo trattato le basi della matematica, codifiche numeriche e archiviazione, vantaggi della matematica efficiente e le sfumature della rappresentazione numerica con utilizzo della memoria, complessit√† computazionale, compatibilit√† hardware e scenari di compromesso. Abbiamo concluso concentrandoci su un elemento fondamentale della matematica efficiente: la quantizzazione, dove abbiamo esaminato la sua storia, calibrazione, tecniche e interazione con la potatura.\nInfine, abbiamo esaminato come possiamo apportare ottimizzazioni specifiche per l‚Äôhardware che abbiamo. Abbiamo esplorato come possiamo trovare architetture modello su misura per l‚Äôhardware, apportare ottimizzazioni nel kernel per gestire meglio il modello e framework creati per sfruttare al meglio l‚Äôhardware. Abbiamo anche esaminato come possiamo fare il contrario e creare hardware attorno al nostro software specifico e abbiamo parlato di come suddividere le reti per l‚Äôesecuzione su pi√π processori disponibili sul dispositivo edge.\nComprendendo il quadro completo dei gradi di libert√† all‚Äôinterno dell‚Äôottimizzazione del modello sia lontano che vicino all‚Äôhardware e i compromessi da considerare quando si implementano questi metodi, i professionisti possono sviluppare una pipeline pi√π ponderata per comprimere i loro carichi di lavoro sui dispositivi edge.",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Ottimizzazioni dei Modelli</span>"
    ]
  },
  {
    "objectID": "contents/core/optimizations/optimizations.it.html#sec-model-optimizations-resource",
    "href": "contents/core/optimizations/optimizations.it.html#sec-model-optimizations-resource",
    "title": "9¬† Ottimizzazioni dei Modelli",
    "section": "9.7 Risorse",
    "text": "9.7 Risorse\nEcco un elenco curato di risorse per supportare sia gli studenti che gli insegnanti nel loro percorso di apprendimento e insegnamento. Stiamo lavorando costantemente per espandere questa raccolta e aggiungeremo nuovi esercizi nel prossimo futuro.\n\n\n\n\n\n\nSlide\n\n\n\n\n\nQueste slide sono uno strumento prezioso per gli insegnanti per tenere lezioni e per gli studenti per rivedere il materiale al proprio ritmo. Incoraggiamo sia gli studenti che gli insegnanti a sfruttare queste slide per migliorare la loro comprensione e facilitare un trasferimento efficace delle conoscenze.\n\nQuantizzazione:\n\nQuantization: Part 1.\nQuantization: Part 2.\nPost-Training Quantization (PTQ).\nQuantization-Aware Training (QAT).\n\nPruning:\n\nPruning: Part 1.\nPruning: Part 2.\n\nKnowledge Distillation.\nClustering.\nNeural Architecture Search (NAS):\n\nNAS overview.\nNAS: Part 1.\nNAS: Part 2.\n\n\n\n\n\n\n\n\n\n\n\nVideo\n\n\n\n\n\n\nVideo¬†9.1\n\n\n\n\n\n\n\n\n\n\nEsercizi\n\n\n\n\n\nPer rafforzare i concetti trattati in questo capitolo, abbiamo curato una serie di esercizi che sfidano gli studenti ad applicare le proprie conoscenze e ad approfondire la propria comprensione.\n\nEsercizio¬†9.1\nEsercizio¬†9.2\nEsercizio¬†9.3",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Ottimizzazioni dei Modelli</span>"
    ]
  },
  {
    "objectID": "contents/core/hw_acceleration/hw_acceleration.it.html",
    "href": "contents/core/hw_acceleration/hw_acceleration.it.html",
    "title": "10¬† Accelerazione IA",
    "section": "",
    "text": "10.1 Panoramica\nProbabilmente avrete notato la crescente domanda di integrazione dell‚Äôapprendimento automatico nei dispositivi di uso quotidiano, come gli smartphone nelle nostre tasche, gli elettrodomestici intelligenti e persino i veicoli autonomi. Portare le funzionalit√† di ML in questi ambienti del mondo reale √® entusiasmante, ma comporta una serie di sfide. A differenza dei potenti server dei data center, questi dispositivi edge hanno risorse di elaborazione limitate, il che rende difficile eseguire modelli complessi in modo efficace.\nL‚Äôaccelerazione hardware specializzata √® la chiave per rendere possibile l‚Äôapprendimento automatico ad alte prestazioni su dispositivi edge con risorse limitate. Quando parliamo di accelerazione hardware, ci riferiamo all‚Äôuso di chip e architetture personalizzati progettati per gestire il pesante lavoro delle operazioni di ML, alleggerendo il carico del processore principale. Nelle reti neurali, alcune delle attivit√† pi√π impegnative riguardano le moltiplicazioni di matrici durante l‚Äôinferenza. Gli acceleratori hardware sono progettati per ottimizzare queste operazioni, spesso offrendo accelerazioni da 10 a 100 volte superiori rispetto alle CPU per uso generico. Questo tipo di accelerazione √® ci√≤ che rende fattibile l‚Äôesecuzione di modelli di reti neurali avanzate su dispositivi limitati da dimensioni, peso e potenza, e di fare tutto in tempo reale.\nIn questo capitolo, esamineremo pi√π da vicino le diverse tecniche di accelerazione hardware disponibili per l‚Äôapprendimento automatico embedded e i compromessi che derivano da ciascuna opzione. L‚Äôobiettivo √® fornire una solida comprensione di come funzionano queste tecniche, in modo che si possano prendere decisioni informate quando si tratta di scegliere l‚Äôhardware giusto e ottimizzare il software. Alla fine, sarete ben equipaggiati per sviluppare capacit√† di apprendimento automatico ad alte prestazioni su dispositivi edge, anche con i loro vincoli.",
    "crumbs": [
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Accelerazione IA</span>"
    ]
  },
  {
    "objectID": "contents/core/hw_acceleration/hw_acceleration.it.html#background-e-basi",
    "href": "contents/core/hw_acceleration/hw_acceleration.it.html#background-e-basi",
    "title": "10¬† Accelerazione IA",
    "section": "10.2 Background e Basi",
    "text": "10.2 Background e Basi\n\n10.2.1 Background Storico\nLe origini dell‚Äôaccelerazione hardware risalgono agli anni ‚Äô60, con l‚Äôavvento dei coprocessori matematici in virgola mobile per eliminare i calcoli dalla CPU principale. Un primo esempio √® stato il chip Intel 8087 rilasciato nel 1980 per accelerare le operazioni in virgola mobile per il processore 8086. Ci√≤ ha stabilito la pratica di utilizzare processori specializzati per gestire in modo efficiente carichi di lavoro ad alta intensit√† di calcolo.\nNegli anni ‚Äô90, sono emerse le prime Graphics Processing Units (GPU) [Unit√† di elaborazione grafica] per elaborare rapidamente pipeline grafiche per rendering e giochi. La GeForce 256 di Nvidia nel 1999 √® stata una delle prime GPU programmabili in grado di eseguire algoritmi software personalizzati. Le GPU esemplificano acceleratori a funzione fissa specifici per dominio e si sono evolute in acceleratori programmabili paralleli.\nNegli anni 2000, le GPU sono state applicate all‚Äôelaborazione generica in GPGPU. La loro elevata larghezza di banda di memoria e la produttivit√† computazionale le hanno rese adatte a carichi di lavoro ad alta intensit√† di calcolo. Ci√≤ ha incluso innovazioni nell‚Äôuso di GPU per accelerare il training di modelli di deep learning come AlexNet nel 2012.\nNegli ultimi anni, le Tensor Processing Unit (TPU) di Google rappresentano ASIC personalizzati specificamente progettati per la moltiplicazione di matrici nel deep learning. Durante l‚Äôinferenza, i loro core tensoriali ottimizzati raggiungono TeraOPS/watt pi√π elevati rispetto a CPU o GPU. L‚Äôinnovazione continua include tecniche di compressione del modello come pruning e quantizzazione per adattare reti neurali pi√π grandi su dispositivi edge.\nQuesta evoluzione dimostra come l‚Äôaccelerazione hardware si sia concentrata sulla risoluzione di colli di bottiglia ad alta intensit√† di calcolo, dalla matematica in virgola mobile alla grafica alla moltiplicazione di matrici per ML. Comprendere questa storia fornisce un contesto cruciale per gli acceleratori AI specializzati odierni.\n\n\n10.2.2 La Necessit√† di Accelerazione\nL‚Äôevoluzione dell‚Äôaccelerazione hardware √® strettamente legata alla storia pi√π ampia dell‚Äôinformatica. Centrale in questa storia √® il ruolo dei transistor, i mattoni fondamentali dell‚Äôelettronica moderna. I transistor agiscono come piccoli interruttori che possono accendersi o spegnersi, consentendo i calcoli complessi che guidano tutto, dalle semplici calcolatrici ai modelli avanzati di apprendimento automatico. Nei primi decenni, la progettazione dei chip era governata dalla legge di Moore, che prevedeva che il numero di transistor su un circuito integrato sarebbe raddoppiato approssimativamente ogni due anni, e dal Dennard Scaling, che osservava che man mano che i transistor diventavano pi√π piccoli, le loro prestazioni (velocit√†) aumentavano, mentre la densit√† di potenza (potenza per unit√† di area) rimaneva costante. Queste due leggi sono state mantenute durante l‚Äôera single-core. Figura¬†10.1 mostra le tendenze di diverse metriche dei microprocessori. Come indica la figura, il Dennard Scaling fallisce intorno alla met√† degli anni 2000; si noti come la velocit√† di clock (frequenza) rimanga pressoch√© costante anche se il numero di transistor continua ad aumentare.\n\n\n\n\n\n\nFigura¬†10.1: Tendenze dei Microprocessori. Fonte: Karl Rupp.\n\n\n\nTuttavia, come descrive Patterson e Hennessy (2016), i vincoli tecnologici alla fine hanno imposto una transizione all‚Äôera multicore, con chip contenenti pi√π core di elaborazione per offrire guadagni in termini di prestazioni. Le limitazioni di potenza hanno impedito un ulteriore ridimensionamento, il che ha portato al ‚Äúsilicio scuro‚Äù (Dark Silicon), in cui non tutte le aree del chip potevano essere attive simultaneamente (Xiu 2019).\n\nPatterson, David A, e John L Hennessy. 2016. Computer organization and design ARM edition: The hardware software interface. Morgan kaufmann.\n\nXiu, Liming. 2019. ¬´Time Moore: Exploiting Moore‚Äôs Law From The Perspective of Time¬ª. IEEE Solid-State Circuits Mag. 11 (1): 39‚Äì55. https://doi.org/10.1109/mssc.2018.2882285.\n‚ÄúDark silicon‚Äù si riferisce a parti del chip che non possono essere alimentate simultaneamente a causa di limitazioni termiche e di potenza. In sostanza, con l‚Äôaumento della densit√† dei transistor, la quota del chip che poteva essere utilizzata attivamente senza surriscaldarsi o superare i budget di potenza si √® ridotta.\nQuesto fenomeno ha comportato che, sebbene i chip avessero pi√π transistor, non tutti potevano essere operativi simultaneamente, limitando i potenziali guadagni in termini di prestazioni. Questa crisi energetica ha reso necessario un passaggio all‚Äôera degli acceleratori, con unit√† hardware specializzate su misura per attivit√† specifiche per massimizzare l‚Äôefficienza. L‚Äôesplosione dei carichi di lavoro dell‚Äôintelligenza artificiale ha ulteriormente spinto la domanda di acceleratori personalizzati. I fattori abilitanti includevano nuovi linguaggi di programmazione, strumenti software e progressi nella produzione.\nFondamentalmente, gli acceleratori hardware vengono valutati in base a Prestazioni, Potenza e Area di silicio (PPA); la natura dell‚Äôapplicazione target, sia essa legata alla memoria o al calcolo, influenza notevolmente la progettazione. Ad esempio, i carichi di lavoro legati alla memoria richiedono un‚Äôelevata larghezza di banda e un accesso a bassa latenza, mentre le applicazioni legate al calcolo richiedono la massima produttivit√† di elaborazione.\n\n\n10.2.3 Principi Generali\nLa progettazione di acceleratori hardware specializzati comporta la gestione di compromessi complessi tra prestazioni, efficienza energetica, area di silicio e ottimizzazioni specifiche del carico di lavoro. Questa sezione delinea considerazioni e metodologie fondamentali per raggiungere un equilibrio ottimale in base ai requisiti dell‚Äôapplicazione e ai vincoli hardware.\n\nPrestazioni entro i Budget di Potenza\nPer capire come raggiungere il giusto equilibrio tra prestazioni e budget di potenza, √® importante definire prima alcuni concetti chiave che svolgono un ruolo cruciale in questo processo. Le prestazioni si riferiscono in generale alla capacit√† complessiva di un sistema di completare efficacemente le attivit√† di calcolo entro determinati vincoli. Uno dei componenti chiave delle prestazioni √® il throughput, ovvero la velocit√† con cui vengono elaborate queste attivit√†, comunemente misurata in ‚Äúfloating point operations per second (FLOPS)‚Äù [operazioni in virgola mobile al secondo ] o frame al secondo (FPS). Il throughput dipende fortemente dal parallelismo, ovvero la capacit√† dell‚Äôhardware di eseguire pi√π operazioni contemporaneamente, e dalla frequenza di clock, ovvero la velocit√† con cui il processore esegue ciclicamente queste operazioni. Un throughput pi√π elevato in genere comporta prestazioni migliori, ma aumenta anche il consumo di energia all‚Äôaumentare dell‚Äôattivit√†.\nLa semplice massimizzazione del throughput non √® sufficiente; anche l‚Äôefficienza dell‚Äôhardware √® importante. L‚Äôefficienza √® la misura di quante operazioni vengono eseguite per watt di potenza consumata, riflettendo la relazione tra lavoro di calcolo e consumo di energia. In scenari in cui la potenza √® un fattore limitante, come nei dispositivi edge, ottenere un‚Äôelevata efficienza √® fondamentale. Per aiutare a ricordare come questi concetti si interconnettono, considerare le seguenti relazioni:\n\nPrestazioni = Throughput * Efficienza\nThroughput ~= Parallelismo * Frequenza di Clock\nEfficienza = Operazioni / Watt\n\nGli acceleratori hardware mirano a massimizzare le prestazioni entro budget di potenza stabiliti. Ci√≤ richiede un attento bilanciamento del parallelismo, della frequenza di clock del chip, della tensione di esercizio, dell‚Äôottimizzazione del carico di lavoro e di altre tecniche per massimizzare le operazioni per watt.\nAd esempio, le GPU raggiungono un throughput elevato tramite architetture massivamente parallele. Tuttavia, la loro efficienza √® inferiore a quella dei circuiti integrati specifici per applicazione (ASIC) personalizzati come il TPU di Google, che ottimizzano per un carico di lavoro specifico.\n\n\nGestione dell‚ÄôArea e dei Costi del Silicio\nLa dimensione dell‚Äôarea di un chip ha un impatto diretto sul suo costo di produzione. Per capirne il motivo, √® utile conoscere un po‚Äô il processo di produzione.\nI chip vengono creati da grandi e sottili fette di materiale semiconduttore note come wafer. Durante la produzione, ogni wafer viene suddiviso in blocchi pi√π piccoli chiamati ‚Äúdie‚Äù, e ogni die contenente i circuiti per un singolo chip. Dopo che il wafer √® stato elaborato, viene tagliato in questi singoli die, che vengono poi confezionati per formare i chip finali utilizzati nei dispositivi elettronici.\nI die pi√π grandi richiedono pi√π materiale e sono pi√π inclini a difetti, il che pu√≤ ridurre la resa, il che significa che vengono prodotti meno chip utilizzabili da ogni wafer. Mentre i produttori possono scalare i progetti combinando pi√π die pi√π piccoli in un singolo pacchetto (pacchetti multi-die), ci√≤ aggiunge complessit√† e costi al processo di confezionamento e produzione.\nLa quantit√† di area di silicio necessaria su un die dipende da diversi fattori:\n\nRisorse di Calcolo, ad esempio numero di core, memoria, cache\nNodo del Processo di Produzione, transistor pi√π piccoli consentono una maggiore densit√†\nModello di Programmazione, acceleratori programmati richiedono maggiore flessibilit√†\n\nLa progettazione dell‚Äôacceleratore implica la compressione delle massime prestazioni entro questi vincoli di area del silicio. Tecniche come la potatura e la compressione aiutano ad adattare modelli pi√π grandi al chip senza superare lo spazio disponibile.\n\n\nOttimizzazioni Specifiche del Carico di Lavoro\nLa progettazione di acceleratori hardware efficaci richiede di adattare l‚Äôarchitettura alle esigenze specifiche del carico di lavoro target. Diversi tipi di carichi di lavoro, che siano in AI, grafica o robotica, hanno caratteristiche uniche che stabiliscono come l‚Äôacceleratore dovrebbe essere ottimizzato.\nAlcune delle considerazioni chiave quando si ottimizza l‚Äôhardware per carichi di lavoro specifici includono:\n\nMemoria vs Limiti di Calcolo: I carichi di lavoro vincolati alla memoria richiedono una maggiore larghezza di banda di memoria, mentre le app vincolate al calcolo necessitano di un throughput [produttivit√†] aritmetico.\nLocalit√† dei Dati: Lo spostamento dei dati dovrebbe essere ridotto al minimo per l‚Äôefficienza. La memoria vicina al calcolo aiuta.\nOperazioni a Livello di Bit: I tipi di dati a bassa precisione come INT8/INT4 ottimizzano la densit√† di calcolo.\nParallelismo dei Dati: Pi√π unit√† di calcolo replicate consentono l‚Äôesecuzione parallela.\nPipelining: L‚Äôesecuzione sovrapposta delle operazioni aumenta la produttivit√†.\n\nLa comprensione delle caratteristiche del carico di lavoro consente un‚Äôaccelerazione personalizzata. Ad esempio, le reti neurali convoluzionali utilizzano operazioni di ‚Äúfinestra scorrevole‚Äù mappate in modo ottimale su array spaziali di elementi di elaborazione.\nGrazie alla comprensione di questi compromessi architettonici, i progettisti possono prendere decisioni informate sull‚Äôarchitettura dell‚Äôacceleratore hardware, assicurandosi che fornisca le migliori prestazioni possibili per l‚Äôuso previsto.\n\n\nProgettazione Hardware Sostenibile\nNegli ultimi anni, la sostenibilit√† dell‚ÄôIA √® diventata una preoccupazione urgente, guidata da due fattori chiave: la scala crescente dei carichi di lavoro dell‚ÄôIA e il consumo energetico associato.\nInnanzitutto, le dimensioni dei modelli e dei set di dati dell‚ÄôIA sono cresciute rapidamente. Ad esempio, in base alle tendenze di elaborazione dell‚ÄôIA di OpenAI, la quantit√† di elaborazione utilizzata per addestrare modelli all‚Äôavanguardia raddoppia ogni 3,5 mesi. Questa crescita esponenziale richiede enormi risorse di elaborazione nei data center.\nIn secondo luogo, l‚Äôuso di energia per l‚Äôaddestramento e l‚Äôinferenza dell‚ÄôIA presenta problemi di sostenibilit√†. I data center che eseguono applicazioni di IA consumano molta energia, contribuendo a elevate emissioni di carbonio. Si stima che l‚Äôaddestramento di un grande modello di IA possa avere un‚Äôimpronta di carbonio di 626.000 libbre di CO2 equivalente, quasi 5 volte le emissioni di un‚Äôauto media nel corso della sua vita.\nPer affrontare queste sfide, la progettazione hardware sostenibile si concentra sull‚Äôottimizzazione dell‚Äôefficienza energetica senza compromettere le prestazioni. Ci√≤ comporta lo sviluppo di acceleratori specializzati che riducono al minimo il consumo di energia massimizzando al contempo la produttivit√† computazionale.\nParleremo di IA sostenibile in un capitolo successivo, dove ne discuteremo pi√π in dettaglio.",
    "crumbs": [
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Accelerazione IA</span>"
    ]
  },
  {
    "objectID": "contents/core/hw_acceleration/hw_acceleration.it.html#sec-aihw",
    "href": "contents/core/hw_acceleration/hw_acceleration.it.html#sec-aihw",
    "title": "10¬† Accelerazione IA",
    "section": "10.3 Tipi di acceleratori",
    "text": "10.3 Tipi di acceleratori\nGli acceleratori hardware possono assumere molte forme. Possono esistere come widget (come il Neural Engine nel chip Apple M1) o come interi chip appositamente progettati per svolgere molto bene determinate attivit√†. Questa sezione esaminer√† i processori per carichi di lavoro di apprendimento automatico lungo lo spettro che va dagli ASIC altamente specializzati alle CPU pi√π generiche.\nCi concentriamo prima sull‚Äôhardware personalizzato appositamente progettato per l‚Äôintelligenza artificiale per comprendere le ottimizzazioni pi√π estreme possibili quando vengono rimossi i vincoli di progettazione. Questo stabilisce un limite massimo per prestazioni ed efficienza. Poi prendiamo in considerazione progressivamente architetture pi√π programmabili e adattabili, discutendo di GPU e FPGA. Queste fanno compromessi nella personalizzazione per mantenere la flessibilit√†. Infine, trattiamo le CPU generiche che sacrificano le ottimizzazioni per un carico di lavoro particolare in cambio di una programmabilit√† versatile tra le applicazioni.\nStrutturando l‚Äôanalisi lungo questo spettro, miriamo a illustrare i compromessi fondamentali tra utilizzo, efficienza, programmabilit√† e flessibilit√† nella progettazione dell‚Äôacceleratore. Il punto di equilibrio ottimale dipende dai vincoli e dai requisiti dell‚Äôapplicazione target. Questa prospettiva dello spettro fornisce un quadro per ragionare sulle scelte hardware per l‚Äôapprendimento automatico e sulle capacit√† richieste a ciascun livello di specializzazione.\nFigura¬†10.2 illustra la complessa interazione tra flessibilit√†, prestazioni, diversit√† funzionale e area di progettazione dell‚Äôarchitettura. Notare come l‚ÄôASIC si trovi nell‚Äôangolo in basso a destra, con area minima, flessibilit√† e consumo energetico e prestazioni massime, a causa della sua natura altamente specializzata per l‚Äôapplicazione. Un compromesso chiave √® la diversit√† funzionale rispetto alle prestazioni: le architetture per uso generico possono servire applicazioni diverse, ma le loro prestazioni applicative sono degradate rispetto alle architetture pi√π personalizzate.\n\n\n\n\n\n\nFigura¬†10.2: Compromessi di Progettazione. Fonte: El-Rayis (2014).\n\n\nEl-Rayis, A. O. 2014. ¬´Reconfigurable architectures for the next generation of mobile device telecommunications systems¬ª. : https://www.researchgate.net/publication/292608967.\n\n\nLa progressione inizia con l‚Äôopzione pi√π specializzata, gli ASIC appositamente progettati per l‚Äôintelligenza artificiale, per basare la nostra comprensione sulle massime ottimizzazioni possibili prima di espanderci ad architetture pi√π generalizzabili. Questo approccio strutturato chiarisce lo spazio di progettazione dell‚Äôacceleratore.\n\n10.3.1 Application-Specific Integrated Circuits (ASIC)\nUn ‚Äúcircuito integrato specifico per applicazione‚Äù (ASIC) √® un tipo di circuito integrato (IC) progettato su misura per un‚Äôapplicazione o un carico di lavoro specifico, anzich√© per un uso generico. A differenza di CPU e GPU, gli ASIC non supportano pi√π applicazioni o carichi di lavoro. Piuttosto, sono ottimizzati per eseguire un singolo compito in modo estremamente efficiente. Google TPU √® un esempio di ASIC.\nGli ASIC raggiungono questa efficienza adattando ogni aspetto del design del chip, ovvero le porte logiche sottostanti, i componenti elettronici, l‚Äôarchitettura, la memoria, l‚ÄôI/O e il processo di produzione, specificamente per l‚Äôapplicazione target. Questo livello di personalizzazione consente di rimuovere qualsiasi logica o funzionalit√† non necessaria richiesta per il calcolo generale. Il risultato √® un IC che massimizza le prestazioni e l‚Äôefficienza energetica sul carico di lavoro desiderato. I guadagni di efficienza derivanti dall‚Äôhardware specifico per applicazione sono cos√¨ sostanziali che queste aziende incentrate sul software dedicano enormi risorse ingegneristiche alla progettazione di ASIC personalizzati.\nL‚Äôascesa di algoritmi di apprendimento automatico pi√π complessi ha reso i vantaggi prestazionali abilitati dall‚Äôaccelerazione hardware personalizzata un fattore di differenziazione competitiva chiave, anche per le aziende tradizionalmente concentrate sull‚Äôingegneria del software. Gli ASIC sono diventati un investimento ad alta priorit√† per i principali provider cloud che mirano a offrire un calcolo AI pi√π veloce.\n\nVantaggi\nGrazie alla loro natura personalizzata, gli ASIC offrono vantaggi significativi rispetto ai processori generici come CPU e GPU. I principali vantaggi includono quanto segue.\n\nPrestazioni ed efficienza massimizzate\nIl vantaggio pi√π fondamentale degli ASIC √® la massimizzazione delle prestazioni e dell‚Äôefficienza energetica personalizzando l‚Äôarchitettura hardware specificamente per l‚Äôapplicazione target. Ogni transistor e aspetto della progettazione √® ottimizzato per il carico di lavoro desiderato: non √® necessaria alcuna logica o sovraccarico non necessario per supportare il calcolo generico.\nAd esempio, le Tensor Processing Units (TPU) di Google contengono architetture su misura esattamente per le operazioni di moltiplicazione di matrici utilizzate nelle reti neurali. Per progettare gli ASIC TPU, i team di ingegneria di Google devono definire chiaramente le specifiche del chip, scrivere la descrizione dell‚Äôarchitettura utilizzando linguaggi di descrizione hardware come Verilog, sintetizzare il design per mapparlo sui componenti hardware e posizionare e instradare con cura transistor e collegamenti in base alle regole di progettazione del processo di fabbricazione. Questo complesso processo di progettazione, noto come ‚Äúvery-large-scale integration‚Äù (VLSI) [integrazione su larga scala ], consente loro di creare un IC ottimizzato per carichi di lavoro di apprendimento automatico.\nDi conseguenza, gli ASIC TPU raggiungono un‚Äôefficienza di oltre un ordine di grandezza superiore nelle operazioni per watt rispetto alle GPU per uso generico sui carichi di lavoro di apprendimento automatico massimizzando le prestazioni e riducendo al minimo il consumo energetico tramite un design hardware full-stack personalizzato.\n\n\nMemoria On-Chip Specializzata\nGli ASIC incorporano memoria on-chip, come SRAM (Static Random Access Memory) e cache specificamente ottimizzate per fornire dati alle unit√† di elaborazione. La SRAM √® un tipo di memoria pi√π veloce e affidabile della DRAM (Dynamic Random Access Memory) perch√© non deve essere aggiornata periodicamente. Tuttavia, richiede pi√π transistor per bit di dati, il che la rende pi√π ingombrante e costosa da produrre rispetto alla DRAM.\nLa SRAM √® ideale per la memoria on-chip, dove la velocit√† √® fondamentale. Il vantaggio di avere grandi quantit√† di SRAM on-chip ad alta larghezza di banda √® che i dati possono essere archiviati vicino agli elementi di elaborazione, consentendo un rapido accesso. Ci√≤ fornisce enormi vantaggi in termini di velocit√† rispetto all‚Äôaccesso alla DRAM off-chip, che, sebbene di capacit√† maggiore, pu√≤ essere fino a 100 volte pi√π lenta. Ad esempio, il system-on-a-chip M1 di Apple contiene una speciale SRAM a bassa latenza per accelerare le prestazioni del suo hardware di machine learning Neural Engine.\nLa localit√† dei dati e l‚Äôottimizzazione della gerarchia di memoria sono fondamentali per un throughput elevato e un basso consumo energetico. Tabella¬†10.1 mostra ‚ÄúNumeri che Tutti Dovrebbero Conoscere‚Äù, di Jeff Dean.\n\n\n\nTabella¬†10.1: Confronto della latenza delle operazioni di elaborazione e di rete.\n\n\n\n\n\n\n\n\n\nOperazione\nLatenza\n\n\n\n\nRiferimento alla cache L1\n0,5 ns\n\n\nBranch mispredict\n5 ns\n\n\nRiferimento alla cache L2\n7 ns\n\n\nBlocco/sblocco mutex\n25 ns\n\n\nRiferimento alla memoria principale\n100 ns\n\n\nComprimere 1K byte con Zippy\n3.000 ns (3 us)\n\n\nInviare 1 KB byte su una rete da 1 Gbps\n10.000 ns (10 us)\n\n\nLeggere 4 KB casualmente da SSD\n150.000 ns (150 us)\n\n\nLeggere 1 MB in sequenza dalla memoria\n250.000 ns (250 us)\n\n\nAndata e ritorno all‚Äôinterno dello stesso data center\n500.000 ns (0,5 ms)\n\n\nLeggere 1 MB in sequenza da SSD\n1.000.000 ns (1 ms)\n\n\nRicerca su disco\n10.000.000 ns (10 ms)\n\n\nLeggere 1 MB in sequenza da disco\n20.000.000 ns (20 ms)\n\n\nInviare un pacchetto CA ‚Üí Paesi Bassi ‚Üí CA\n150.000.000 ns (150 ms)\n\n\n\n\n\n\n\n\nTipi di Dati e Operazioni Personalizzati\nA differenza dei processori generici, gli ASIC possono essere progettati per supportare in modo nativo tipi di dati personalizzati come INT4 o bfloat16, ampiamente utilizzati nei modelli di ML. Ad esempio, l‚Äôarchitettura GPU Ampere di Nvidia ha un bfloat16 dedicato ai Tensor Core per accelerare i carichi di lavoro AI. I tipi di dati a bassa precisione consentono una maggiore densit√† aritmetica e prestazioni. Per ulteriori dettagli fare riferimento a Sezione 8.6. Gli ASIC possono anche incorporare direttamente operazioni non standard negli algoritmi ML come operazioni primitive, ad esempio, il supporto nativo di funzioni di attivazione come ReLU rende l‚Äôesecuzione pi√π efficiente.\n\n\nParallelismo Elevato\nLe architetture ASIC possono sfruttare un parallelismo pi√π elevato ottimizzato per il carico di lavoro del target rispetto alle CPU o GPU generiche. Un maggior numero di unit√† di calcolo personalizzate per l‚Äôapplicazione significa pi√π operazioni eseguite simultaneamente. Gli ASIC altamente paralleli raggiungono un throughput enorme per carichi di lavoro paralleli di dati come l‚Äôinferenza di reti neurali.\n\n\nNodi di Processo Avanzati\nI processi di produzione all‚Äôavanguardia consentono di impacchettare pi√π transistor in aree di die pi√π piccole, aumentando la densit√†. Gli ASIC progettati specificamente per applicazioni ad alto volume possono ammortizzare meglio i costi dei nodi.\n\n\n\nSvantaggi\n\nTempistiche di Progettazione Lunghe\nIl processo di progettazione e validazione di un ASIC pu√≤ richiedere 2-3 anni. La sintesi dell‚Äôarchitettura utilizzando linguaggi di descrizione hardware, la definizione del layout del chip e la fabbricazione del chip su nodi di processo avanzati comportano lunghi cicli di sviluppo. Ad esempio, per realizzare un chip da 7 nm, i team devono definire attentamente le specifiche, scrivere l‚Äôarchitettura in HDL, sintetizzare le porte logiche, posizionare i componenti, instradare tutte le interconnessioni e finalizzare il layout da inviare per la fabbricazione. Questa ‚ÄúVery Large-Scale Integration (VLSI)‚Äù significa che la progettazione e la produzione di ASIC possono tradizionalmente richiedere 2-5 anni.\nCi sono alcuni motivi chiave per cui le lunghe tempistiche di progettazione degli ASIC, spesso 2-3 anni, possono essere difficili per i carichi di lavoro di apprendimento automatico:\n\nGli algoritmi ML si evolvono rapidamente: Nuove architetture di modelli, tecniche di training e ottimizzazioni di rete emergono continuamente. Ad esempio, i Transformers sono diventati estremamente popolari nell‚ÄôNLP negli ultimi anni. Quando un ASIC termina il tapeout, l‚Äôarchitettura ottimale per un carico di lavoro potrebbe essere cambiata.\nI dataset crescono rapidamente: Gli ASIC progettati per determinate dimensioni di modello o tipi di dati possono diventare sottodimensionati rispetto alla domanda. Ad esempio, i modelli di linguaggio naturale stanno aumentando esponenzialmente con pi√π dati e parametri. Un chip progettato per BERT potrebbe non supportare GPT-3.\nLe applicazioni ML cambiano frequentemente: L‚Äôattenzione del settore cambia tra visione artificiale, parlato, NLP, sistemi di raccomandazione, ecc. Un ASIC ottimizzato per la classificazione delle immagini potrebbe avere meno rilevanza in pochi anni.\nCicli di progettazione pi√π rapidi con GPU/FPGA: Gli acceleratori programmabili come le GPU possono adattarsi molto pi√π rapidamente aggiornando le librerie software e i framework. I nuovi algoritmi possono essere implementati senza modifiche hardware.\nEsigenze di time-to-market: Ottenere un vantaggio competitivo in ML richiede di sperimentare e implementare rapidamente nuove idee. Attendere diversi anni per un ASIC √® diverso da un‚Äôiterazione rapida.\n\nIl ritmo dell‚Äôinnovazione in ML deve essere adattato meglio alla scala temporale pluriennale per lo sviluppo di ASIC. Sono necessari notevoli sforzi ingegneristici per estendere la durata di vita di ASIC tramite architetture modulari, ridimensionamento dei processi, compressione dei modelli e altre tecniche. Tuttavia, la rapida evoluzione di ML rende l‚Äôhardware a funzione fissa una sfida.\n\n\nElevati Costi di Progettazione Non Ricorrenti\nI costi fissi per portare un ASIC dalla progettazione alla produzione ad alto volume possono essere molto dispendiosi in termini di capitale, spesso decine di milioni di dollari. La fabbricazione di fotomaschere per il tape-out dei chip in nodi di processo avanzati, il packaging e il lavoro di progettazione una tantum sono costosi. Ad esempio, un solo tape-out del chip da 7 nm potrebbe costare milioni. L‚Äôelevato ‚Äúnon-recurring engineering (NRE)‚Äù [investimento di progettazione non ricorrente] riduce la fattibilit√† dell‚ÄôASIC ai casi di utilizzo della produzione ad alto volume in cui il costo iniziale pu√≤ essere ammortizzato.\n\n\nIntegrazione e Programmazione Complesse\nGli ASIC richiedono un ampio lavoro di integrazione software, inclusi driver, compilatori, supporto del sistema operativo e strumenti di debug. Hanno anche bisogno di competenza nel packaging elettrico e termico. Inoltre, programmare in modo efficiente le architetture ASIC pu√≤ comportare sfide come il partizionamento del carico di lavoro e la pianificazione su molte unit√† parallele. La natura personalizzata richiede notevoli sforzi di integrazione per trasformare l‚Äôhardware grezzo in acceleratori completamente operativi.\nMentre gli ASIC forniscono enormi guadagni di efficienza nelle applicazioni target adattando ogni aspetto della progettazione hardware a un‚Äôattivit√† specifica, la loro natura fissa comporta compromessi in termini di flessibilit√† e costi di sviluppo rispetto agli acceleratori programmabili, che devono essere soppesati in base all‚Äôapplicazione.\n\n\n\n\n10.3.2 Field-Programmable Gate Array (FPGA)\nGli FPGA sono circuiti integrati programmabili che possono essere riconfigurati per diverse applicazioni. La loro natura personalizzabile offre vantaggi per accelerare gli algoritmi AI rispetto agli ASIC fissi o alle GPU inflessibili. Mentre Google, Meta e NVIDIA stanno valutando di installare gli ASIC nei data center, Microsoft ha distribuito gli FPGA nei suoi data center (Putnam et al. 2014) nel 2011 per servire in modo efficiente diversi carichi di lavoro.\n\nXiong, Siyu, Guoqing Wu, Xitian Fan, Xuan Feng, Zhongcheng Huang, Wei Cao, Xuegong Zhou, et al. 2021. ¬´MRI-based brain tumor segmentation using FPGA-accelerated neural network¬ª. BMC Bioinf. 22 (1): 421. https://doi.org/10.1186/s12859-021-04347-6.\nGli FPGA hanno trovato ampia applicazione in vari campi, tra cui l‚Äôimaging medico, la robotica e la finanza, dove eccellono nella gestione di attivit√† di machine learning ad alta intensit√† di calcolo. Nell‚Äôimaging medico, un esempio illustrativo √® l‚Äôapplicazione degli FPGA per la segmentazione dei tumori cerebrali, un processo tradizionalmente dispendioso in termini di tempo e soggetto a errori. Rispetto alle implementazioni tradizionali di GPU e CPU, gli FPGA hanno dimostrato rispettivamente miglioramenti delle prestazioni di oltre 5 e 44 volte e guadagni di 11 e 82 volte in termini di efficienza energetica, evidenziando il loro potenziale per applicazioni esigenti (Xiong et al. 2021).\n\nVantaggi\nGli FPGA offrono diversi vantaggi rispetto alle GPU e agli ASIC per accelerare i carichi di lavoro di apprendimento automatico.\n\nFlessibilit√† Tramite ‚ÄúReconfigurable Fabric‚Äù\nIl vantaggio principale degli FPGA √® la capacit√† di riconfigurare il ‚Äúfabric‚Äù [tessuto] sottostante per implementare architetture personalizzate ottimizzate per diversi modelli, a differenza degli ASIC a funzione fissa. Ad esempio, le societ√† di trading quantitativo utilizzano gli FPGA per accelerare i loro algoritmi perch√© cambiano frequentemente e il basso costo NRE degli FPGA √® pi√π fattibile rispetto acquistare i nuovi ASIC. Figura¬†10.3 contiene una tabella che confronta tre diversi FPGA.\n\n\n\n\n\n\nFigura¬†10.3: Confronto di FPGA. Fonte: Gwennap (s.d.).\n\n\nGwennap, Linley. s.d. ¬´Certus-NX Innovates General-Purpose FPGAs¬ª.\n\n\nGli FPGA comprendono blocchi di base: blocchi logici configurabili, blocchi RAM e interconnessioni. Vengono fornite una quantit√† base di queste risorse e gli ingegneri programmano i chip compilando il codice HDL in flussi di bit che riorganizzano la struttura in diverse configurazioni. Questo rende gli FPGA adattabili man mano che gli algoritmi evolvono.\nSebbene gli FPGA possano non raggiungere le massime prestazioni ed efficienza degli ASIC specifici per il carico di lavoro, la loro programmabilit√† offre maggiore flessibilit√† man mano che gli algoritmi cambiano. Questa adattabilit√† rende gli FPGA una scelta interessante per accelerare le applicazioni di machine learning in evoluzione.\n\n\nParallelismo e Pipeline Personalizzati\nLe architetture FPGA possono sfruttare il parallelismo spaziale e il pipelining adattando la progettazione hardware per rispecchiare il parallelismo nei modelli ML. Ad esempio, su una piattaforma FPGA HARPv2 di Intel √® possibile suddividere i layer di una rete convoluzionale su elementi di elaborazione separati per massimizzare la produttivit√†. Sugli FPGA sono possibili anche pattern paralleli unici come le valutazioni di ‚Äúensemble‚Äù ad albero. Pipeline profonde con buffering e flusso di dati ottimizzati possono essere personalizzate in base alla struttura e ai tipi di dati di ogni modello. Questo livello di parallelismo e pipeline su misura non √® fattibile sulle GPU.\n\n\nMemoria On-Chip a Bassa Latenza\nGrandi quantit√† di memoria on-chip ad alta larghezza di banda consentono l‚Äôarchiviazione localizzata per pesi e attivazioni. Ad esempio, gli FPGA Xilinx Versal contengono 32 MB di blocchi RAM a bassa latenza e interfacce DDR4 a doppio canale per la memoria esterna. Avvicinare fisicamente la memoria alle unit√† di elaborazione riduce la latenza di accesso. Ci√≤ fornisce significativi vantaggi di velocit√† rispetto alle GPU che attraversano PCIe (Peripheral Component Interconnect Express) o altri bus di sistema per raggiungere la memoria GDDR6 off-chip.\n\n\nSupporto Nativo per Bassa Precisione\nUn vantaggio fondamentale degli FPGA √® la capacit√† di implementare in modo nativo qualsiasi larghezza di bit per unit√† aritmetiche, come INT4 o bfloat16, utilizzate nei modelli ML quantizzati. Ad esempio, Intel Stratix 10 NX FPGA ha core INT8 dedicati che possono raggiungere fino a 143 INT8 TOPS (Tera Operations Per Second) a ~1 TOPS/W (Tera Operations Per Second per Watt). TOPS √® una misura di prestazioni simile a FLOPS, ma mentre FLOPS misura calcoli in virgola mobile, TOPS misura il numero di operazioni intere che un sistema pu√≤ eseguire al secondo. Le larghezze di bit inferiori, come INT8 o INT4, aumentano la densit√† aritmetica e le prestazioni. Gli FPGA possono persino supportare la sintonizzazione a precisione mista o dinamica in fase di esecuzione.\n\n\n\nSvantaggi\n\nThroughput di Picco Inferiore Rispetto agli ASIC\nGli FPGA non possono eguagliare i numeri di throughput grezzi degli ASIC, personalizzati per un modello e una precisione specifici. I sovraccarichi del ‚Äúfabric‚Äù riconfigurabile rispetto all‚Äôhardware a funzione fissa comportano prestazioni di picco inferiori. Ad esempio, i pod TPU v5e consentono di connettere fino a 256 chip con oltre 100 PetaOps (Peta Operations Per Second) di prestazioni INT8, mentre gli FPGA possono offrire fino a 143 INT8 TOPS o 286 INT4 TOPS come sull‚ÄôFPGA Intel Stratix 10 NX; PetaOps rappresenta quadrilioni di operazioni al secondo, mentre TOPS misura trilioni, evidenziando la capacit√† di elaborazione molto maggiore dei pod TPU rispetto agli FPGA.\nQuesto perch√© gli FPGA comprendono blocchi di base: blocchi logici configurabili, blocchi RAM e interconnessioni. Vengono forniti con una quantit√† stabilita di queste risorse. Per programmare gli FPGA, gli ingegneri scrivono codice HDL e lo compilano in flussi di bit che riorganizzano il ‚Äúfabric‚Äù, che ha sovraccarichi intrinseci rispetto a un ASIC appositamente progettato per un calcolo.\n\n\nComplessit√† di Programmazione\nPer ottimizzare le prestazioni FPGA, gli ingegneri devono programmare le architetture in linguaggi di descrizione hardware di basso livello come Verilog o VHDL. Ci√≤ richiede competenza nella progettazione hardware e cicli di sviluppo pi√π lunghi rispetto a framework software di livello superiore come TensorFlow. Massimizzare l‚Äôutilizzo pu√≤ essere difficile nonostante i progressi nella sintesi di alto livello da C/C++.\n\n\nSovraccarichi di Riconfigurazione\nLa modifica delle configurazioni FPGA richiede il ricaricamento di un nuovo flusso di bit, che ha costi di latenza e dimensioni di archiviazione considerevoli. Ad esempio, la riconfigurazione parziale su FPGA Xilinx pu√≤ richiedere centinaia di millisecondi. Questo rende impossibile lo scambio dinamico di architetture in tempo reale. L‚Äôarchiviazione del flusso di bit consuma anche memoria on-chip.\n\n\nGuadagni in diminuzione sui nodi avanzati\nSebbene i nodi di processo pi√π piccoli siano molto vantaggiosi per gli ASIC, offrono meno vantaggi per gli FPGA. A 7 nm e al di sotto, effetti come variazione di processo, vincoli termici e invecchiamento hanno un impatto sproporzionato sulle prestazioni degli FPGA. Anche le spese generali della struttura configurabile riducono i guadagni rispetto agli ASIC a funzione fissa.\n\n\n\n\n10.3.3 Digital Signal Processor (DSP)\nIl primo core di elaborazione del segnale digitale √® stato costruito nel 1948 da Texas Instruments (The Evolution of Audio DSPs). Tradizionalmente, i DSP avrebbero avuto una logica per accedere direttamente ai dati digitali/audio nella memoria, eseguire un‚Äôoperazione aritmetica (moltiplica-addiziona-accumula-MAC era una delle operazioni pi√π comuni) e quindi scrivere il risultato nella memoria. Il DSP avrebbe incluso componenti analogici specializzati per recuperare i dati digitali/audio.\nUna volta entrati nell‚Äôera degli smartphone, i DSP hanno iniziato a comprendere attivit√† pi√π sofisticate. Richiedevano Bluetooth, Wi-Fi e connettivit√† cellulare. Anche i media sono diventati molto pi√π complessi. Oggi, √® raro avere chip interi dedicati solo al DSP, ma un System on Chip includerebbe DSP e CPU per uso generico. Ad esempio, l‚ÄôHexagon Digital Signal Processor di Qualcomm afferma di essere un ‚Äúprocessore di livello mondiale con funzionalit√† sia CPU che DSP per supportare le esigenze di elaborazione profondamente integrate della piattaforma mobile per funzioni sia multimediali che modem‚Äù. Google Tensors, il chip nei telefoni Google Pixel, include anche CPU e motori DSP specializzati.\n\nVantaggi\nI DSP offrono vantaggi architettonici in termini di throughput della matematica vettoriale, accesso alla memoria a bassa latenza, efficienza energetica e supporto per diversi tipi di dati, rendendoli adatti all‚Äôaccelerazione ML embedded.\n\nArchitettura Ottimizzata per la Matematica Vettoriale\nI DSP contengono percorsi dati specializzati, file di registro e istruzioni ottimizzati specificamente per le operazioni di matematica vettoriale comunemente utilizzate nei modelli di apprendimento automatico. Ci√≤ include motori di prodotto scalare, unit√† MAC e funzionalit√† SIMD su misura per calcoli vettoriali/matriciali. Ad esempio, il DSP CEVA-XM6 (‚ÄúCeva SensPro fonde AI e Vector DSP‚Äù) ha unit√† vettoriali a 512 bit per accelerare le convoluzioni. Questa efficienza sui carichi di lavoro di matematica vettoriale va ben oltre le CPU generiche.\n\n\nMemoria On-Chip a Bassa Latenza\nI DSP integrano grandi quantit√† di memoria SRAM veloce su chip per conservare i dati localmente per l‚Äôelaborazione. Avvicinare fisicamente la memoria alle unit√† di calcolo riduce la latenza di accesso. Ad esempio, il DSP SHARC+ di Analog contiene 10 MB di SRAM su chip. Questa memoria locale ad alta larghezza di banda offre vantaggi di velocit√† per le applicazioni in tempo reale.\n\n\nEfficienza Energetica\nI DSP sono progettati per fornire elevate prestazioni per watt su carichi di lavoro di segnali digitali. Percorsi dati efficienti, parallelismo e architetture di memoria consentono trilioni di operazioni matematiche al secondo entro budget di potenza mobili ridotti. Ad esempio, l‚ÄôHexagon DSP di Qualcomm pu√≤ fornire 4 TOPS consumando un numero minimo di watt.\n\n\nSupporto per Matematica a Virgola Mobile e Intera\nA differenza delle GPU che eccellono in precisione singola o dimezzata, i DSP possono supportare nativamente tipi di dati a virgola mobile e intera a 8/16 bit utilizzati nei modelli ML. Alcuni DSP supportano l‚Äôaccelerazione del prodotto scalare a precisione INT8 per reti neurali quantizzate.\n\n\n\nSvantaggi\nI DSP fanno compromessi architettonici che limitano il throughput di picco, la precisione e la capacit√† del modello rispetto ad altri acceleratori AI. Tuttavia, i loro vantaggi in termini di efficienza energetica e matematica intera li rendono una valida opzione di edge computing. Quindi, mentre i DSP offrono alcuni vantaggi rispetto alle CPU, presentano anche delle limitazioni per i carichi di lavoro di apprendimento automatico:\n\nThroughput di Picco Inferiore Rispetto ad ASIC/GPU\nI DSP non possono eguagliare il throughput computazionale grezzo delle GPU o degli ASIC personalizzati progettati specificamente per l‚Äôapprendimento automatico. Ad esempio, l‚ÄôASIC Cloud AI 100 di Qualcomm fornisce 480 TOPS su INT8, mentre il loro DSP Hexagon fornisce 10 TOPS. I DSP non hanno il massiccio parallelismo delle unit√† GPU SM.\n\n\nPrestazioni a Doppia Precisione pi√π Lente\nLa maggior parte dei DSP deve essere ottimizzata per la virgola mobile di precisione pi√π elevata necessaria in alcuni modelli ML. I loro motori di prodotto scalare si concentrano su INT8/16 e FP32, che forniscono una migliore efficienza energetica. Tuttavia, la produttivit√† in virgola mobile a 64 bit √® molto pi√π bassa, il che pu√≤ limitare l‚Äôutilizzo nei modelli che richiedono un‚Äôelevata precisione.\n\n\nCapacit√† del Modello Limitata\nLa limitata memoria on-chip dei DSP limita le dimensioni del modello che possono eseguire. Grandi modelli di deep learning con centinaia di megabyte di parametri supererebbero la capacit√† delle SRAM on-chip. I DSP sono pi√π adatti per modelli di piccole e medie dimensioni destinati a dispositivi edge.\n\n\nComplessit√† di Programmazione\nLa programmazione efficiente delle architetture DSP richiede competenza nella programmazione parallela e nell‚Äôottimizzazione dei pattern di accesso ai dati. Le loro microarchitetture specializzate hanno una curva di apprendimento pi√π ripida rispetto ai framework software di alto livello, rendendo lo sviluppo pi√π complesso.\n\n\n\n\n10.3.4 Graphics Processing Unit (GPU)\nIl termine ‚Äúgraphics processing unit‚Äù [unit√† di elaborazione grafica] esiste almeno dagli anni ‚Äô80. C‚Äô√® sempre stata una richiesta di hardware grafico nelle console per videogiochi (elevata richiesta, doveva avere un costo relativamente basso) e nelle simulazioni scientifiche (richiesta inferiore, ma risoluzione pi√π alta, poteva avere un prezzo elevato).\nIl termine √® stato reso popolare, tuttavia, nel 1999 quando NVIDIA ha lanciato la GeForce 256, mirando principalmente al settore di mercato dei giochi per PC (Lindholm et al. 2008). Man mano che i giochi per PC diventavano pi√π sofisticati, le GPU NVIDIA diventavano pi√π programmabili. Presto, gli utenti si resero conto che potevano sfruttare questa programmabilit√†, eseguire vari carichi di lavoro non correlati alla grafica sulle GPU e trarre vantaggio dall‚Äôarchitettura sottostante. E cos√¨, alla fine degli anni 2000, le GPU divennero unit√† di elaborazione grafica per uso generale o GP-GPU.\n\nLindholm, Erik, John Nickolls, Stuart Oberman, e John Montrym. 2008. ¬´NVIDIA Tesla: A Unified Graphics and Computing Architecture¬ª. IEEE Micro 28 (2): 39‚Äì55. https://doi.org/10.1109/mm.2008.31.\nIn seguito a questo cambiamento, altri importanti attori come Intel con la sua Arc Graphics e AMD con la sua serie Radeon RX hanno anche evoluto le loro GPU per supportare una gamma pi√π ampia di applicazioni oltre al rendering grafico tradizionale. Questa espansione delle capacit√† delle GPU ha aperto nuove possibilit√†, in particolare nei campi che richiedono un‚Äôenorme potenza di calcolo.\nUn esempio lampante di questo potenziale √® la recente ricerca rivoluzionaria condotta da OpenAI (Brown et al. 2020) con GPT-3, un modello di linguaggio con 175 miliardi di parametri. L‚Äôaddestramento di un modello cos√¨ massiccio, che avrebbe richiesto mesi su CPU convenzionali, √® stato completato in pochi giorni utilizzando potenti GPU, dimostrando l‚Äôimpatto trasformativo delle GPU nell‚Äôaccelerazione di complesse attivit√† di apprendimento automatico.\n\nVantaggi\n\nElevata Capacit√† di Elaborazione\nIl vantaggio principale delle GPU √® la loro capacit√† di eseguire calcoli in virgola mobile paralleli massivi ottimizzati per la computer grafica e l‚Äôalgebra lineare (Raina, Madhavan, e Ng 2009). Le GPU moderne come la A100 di Nvidia offrono fino a 19,5 teraflop di prestazioni FP32 con 6912 core CUDA e 40 GB di memoria grafica strettamente accoppiati a 1,6 TB/s di larghezza di banda della memoria grafica.\n\nRaina, Rajat, Anand Madhavan, e Andrew Y. Ng. 2009. ¬´Large-scale deep unsupervised learning using graphics processors¬ª. In Proceedings of the 26th Annual International Conference on Machine Learning, a cura di Andrea Pohoreckyj Danyluk, L√©on Bottou, e Michael L. Littman, 382:873‚Äì80. ACM International Conference Proceeding Series. ACM. https://doi.org/10.1145/1553374.1553486.\nQuesta capacit√† di elaborazione grezza deriva dall‚Äôarchitettura ‚ÄúStreaming Multiprocessor‚Äù (SM) altamente parallela, pensata per carichi di lavoro paralleli ai dati (Zhihao Jia, Zaharia, e Aiken 2019). Ogni SM contiene centinaia di core scalari ottimizzati per la matematica float32/64. Con migliaia di SM su un chip, le GPU sono appositamente progettate per la moltiplicazione di matrici e le operazioni vettoriali utilizzate in tutte le reti neurali.\nAd esempio, l‚Äôultima GPU H100 di Nvidia fornisce 4000 TFLOP di FP8, 2000 TFLOP di FP16, 1000 TFLOP di TF32, 67 TFLOP di FP32 e 34 TFLOP di prestazioni di elaborazione FP64, che possono accelerare notevolmente l‚Äôaddestramento di grandi batch su modelli come BERT, GPT-3 e altre architetture di trasformatori. Il parallelismo scalabile delle GPU √® fondamentale per accelerare il deep learning computazionalmente intensivo.\n\n\nEcosistema Software Maturo\nNvidia fornisce ampie librerie di runtime come cuDNN e cuBLAS che sono altamente ottimizzate per primitive di deep learning. Framework come TensorFlow e PyTorch si integrano con queste librerie per abilitare l‚Äôaccelerazione GPU senza programmazione diretta. Queste librerie sono basate su CUDA, la piattaforma di elaborazione parallela e il modello di programmazione di Nvidia.\nCUDA (Compute Unified Device Architecture) √® il framework sottostante che consente a queste librerie di alto livello di interagire con l‚Äôhardware della GPU. Fornisce agli sviluppatori un accesso di basso livello alle risorse della GPU, consentendo calcoli e ottimizzazioni personalizzate che sfruttano appieno le capacit√† di elaborazione parallela della GPU. Utilizzando CUDA, gli sviluppatori possono scrivere software che sfruttano l‚Äôarchitettura della GPU per attivit√† di elaborazione ad alte prestazioni.\nQuesto ecosistema consente di sfruttare rapidamente le GPU ad alto livello tramite Python senza competenze di programmazione GPU. Flussi di lavoro e astrazioni noti forniscono una comoda rampa di accesso per scalare gli esperimenti di deep learning. La maturit√† del software integra i vantaggi della produttivit√†.\n\n\nAmpia Disponibilit√†\nLe economie di scala dell‚Äôelaborazione grafica rendono le GPU ampiamente accessibili nei data center, nelle piattaforme cloud come AWS e GCP e nelle workstation desktop. La loro disponibilit√† negli ambienti di ricerca ha fornito una comoda piattaforma di sperimentazione e innovazione nel ML. Ad esempio, quasi tutti i risultati di deep learning all‚Äôavanguardia hanno coinvolto l‚Äôaccelerazione GPU per merito di questa ubiquit√†. L‚Äôampio accesso integra la maturit√† del software per rendere le GPU l‚Äôacceleratore ML standard.\n\n\nArchitettura Programmabile\nSebbene non siano flessibili come gli FPGA, le GPU offrono programmabilit√† tramite linguaggi CUDA e shader per personalizzare i calcoli. Gli sviluppatori possono ottimizzare i pattern di accesso ai dati, creare nuove operazioni e regolare le precisioni per modelli e algoritmi in evoluzione.\n\n\n\nSvantaggi\nSebbene le GPU siano diventate l‚Äôacceleratore standard per il deep learning, la loro architettura presenta alcuni svantaggi importanti.\n\nMeno Efficienti degli ASIC Custom\nL‚Äôaffermazione ‚ÄúLe GPU sono meno efficienti degli ASIC‚Äù potrebbe scatenare un acceso dibattito nel campo ML/AI e far esplodere questo libro.\nIn genere, le GPU sono percepite come meno efficienti degli ASIC perch√© questi ultimi sono realizzati su misura per attivit√† specifiche e quindi possono funzionare in modo pi√π efficiente nativamente. Con la loro architettura generica, le GPU sono intrinsecamente pi√π versatili e programmabili, soddisfacendo un ampio spettro di attivit√† computazionali oltre a ML/AI.\nTuttavia, le GPU moderne si sono evolute per includere un supporto hardware specializzato per operazioni AI essenziali, come la moltiplicazione di matrici generalizzata (GEMM) e altre operazioni di matrice, supporto nativo per la quantizzazione e supporto nativo per la potatura, che sono fondamentali per l‚Äôesecuzione efficace dei modelli ML. Questi miglioramenti hanno notevolmente migliorato l‚Äôefficienza delle GPU per le attivit√† AI al punto che possono rivaleggiare con le prestazioni degli ASIC per determinate applicazioni.\nDi conseguenza, le GPU contemporanee sono convergenti, incorporando capacit√† specializzate simili ad ASIC all‚Äôinterno di un framework di elaborazione flessibile e di uso generale. Questa adattabilit√† ha offuscato i confini tra i due tipi di hardware. Le GPU offrono un forte equilibrio tra specializzazione e programmabilit√† che si adatta bene alle esigenze dinamiche della ricerca e sviluppo ML/AI.\n\n\nElevate Esigenze di Larghezza di Banda di Memoria\nL‚Äôarchitettura massicciamente parallela richiede un‚Äôenorme larghezza di banda di memoria per alimentare migliaia di core. Ad esempio, la GPU Nvidia A100 richiede 1.6 TB/sec per saturare completamente il suo computer. Le GPU si affidano ad ampi bus di memoria a 384 bit per RAM GDDR6 ad alta larghezza di banda, ma anche la GDDR6 pi√π veloce raggiunge il massimo a circa 1 TB/sec.¬†Questa dipendenza dalla DRAM esterna comporta latenza e sovraccarico di potenza.\n\n\nComplessit√† di Programmazione\nSebbene strumenti come CUDA siano utili, la mappatura e il partizionamento ottimali dei carichi di lavoro ML nell‚Äôarchitettura GPU massivamente parallela rimangono una sfida, il raggiungimento di un utilizzo elevato e della localit√† della memoria richiede una messa a punto di basso livello (Zhe Jia et al. 2018). Astrazioni come TensorFlow possono tralasciare le prestazioni.\n\nJia, Zhe, Marco Maggioni, Benjamin Staiger, e Daniele P. Scarpazza. 2018. ¬´Dissecting the NVIDIA Volta GPU Architecture via Microbenchmarking¬ª. ArXiv preprint. https://arxiv.org/abs/1804.06826.\n\n\nMemoria On-Chip Limitata\nLe GPU hanno cache di memoria on-chip relativamente piccole rispetto ai grandi requisiti di working set dei modelli ML durante l‚Äôaddestramento. Si basano su un accesso ad alta larghezza di banda alla DRAM esterna, che gli ASIC riducono al minimo con una grande SRAM on-chip.\n\n\nArchitettura Fissa\nA differenza degli FPGA, l‚Äôarchitettura fondamentale della GPU non pu√≤ essere modificata dopo la produzione. Questo vincolo limita l‚Äôadattamento a nuovi carichi di lavoro o layer ML. Il confine CPU-GPU crea anche overhead di spostamento dei dati.\n\n\n\n\n10.3.5 Central Processing Unit (CPU)\nIl termine CPU ha una lunga storia che risale al 1955 (Weik 1955) mentre la prima CPU a microprocessore, l‚ÄôIntel 4004, √® stata inventata nel 1971 (Chi ha inventato il microprocessore?). I compilatori traducono linguaggi di programmazione di alto livello come Python, Java o C per assemblare istruzioni (x86, ARM, RISC-V, ecc.) che le CPU devono elaborare. Il set di istruzioni che una CPU comprende √® chiamato ‚Äúinstruction set architecture‚Äù (ISA), che definisce i comandi che il processore pu√≤ eseguire direttamente. Deve essere concordato sia dall‚Äôhardware che dal software ci gira sopra.\n\nWeik, Martin H. 1955. A Survey of Domestic Electronic Digital Computing Systems. Ballistic Research Laboratories.\nUna panoramica degli sviluppi significativi nelle CPU:\n\nEra del Single-core (anni ‚Äô50-2000): Questa era √® nota per i miglioramenti microarchitettonici aggressivi. Tecniche come l‚Äôesecuzione speculativa (esecuzione di un‚Äôistruzione prima che quella precedente fosse finita), ‚Äúout-of-order execution‚Äù [esecuzione fuori ordine] (riordinamento delle istruzioni per renderle pi√π efficaci) e ‚Äúwider issue widths‚Äù [larghezze di emissione pi√π ampie] (esecuzione di pi√π istruzioni contemporaneamente) sono state implementate per aumentare la produttivit√† delle istruzioni. Anche il termine ‚ÄúSystem on Chip‚Äù ha avuto origine in questa era, poich√© diversi componenti analogici (componenti progettati con transistor) e componenti digitali (componenti progettati con linguaggi di descrizione hardware mappati su transistor) sono stati inseriti sulla stessa piattaforma per realizzare un‚Äôattivit√†.\nEra Multicore (anni 2000): Guidata dalla diminuzione della legge di Moore, questa √® caratterizzata dall‚Äôaumento del numero di core all‚Äôinterno di una CPU. Ora, le attivit√† possono essere suddivise su pi√π core diversi, ognuno con il proprio percorso dati e unit√† di controllo. Molti dei problemi di quest‚Äôepoca riguardavano come condividere determinate risorse, quali risorse condividere e come mantenere coerenza e consistenza in tutti i core.\nUn Mare di acceleratori (anni 2010): Ancora una volta, spinta dalla diminuzione della legge di Moore, quest‚Äôepoca √® caratterizzata dal delegare le attivit√† pi√π complicate su acceleratori (widget) collegati al datapath principale nelle CPU. √à comune vedere acceleratori dedicati a vari carichi di lavoro di intelligenza artificiale, nonch√© elaborazione di immagini/digitali e crittografia. In queste progettazioni, le CPU sono spesso descritte pi√π come giudici, che decidono quali attivit√† devono essere elaborate piuttosto che eseguire l‚Äôelaborazione stessa. Qualsiasi attivit√† potrebbe comunque essere eseguita sulla CPU anzich√© sugli acceleratori, ma la CPU sarebbe generalmente pi√π lenta. Tuttavia, il costo di progettazione e programmazione dell‚Äôacceleratore √® diventato un ostacolo non banale che ha suscitato interesse per le librerie specifiche per la progettazione (DSL).\nPresenza nei data center: Sebbene sentiamo spesso dire che le GPU dominano il mercato dei data center, le CPU sono comunque adatte per attivit√† che non possiedono intrinsecamente un elevato grado di parallelismo. Le CPU spesso gestiscono attivit√† seriali e di piccole dimensioni e coordinano il data center.\nSull‚Äôedge: Dati i vincoli pi√π rigidi sulle risorse sull‚Äôedge, le CPU edge spesso implementano solo un sottoinsieme delle tecniche sviluppate nell‚Äôera single-core perch√© queste ottimizzazioni tendono a essere pesanti in termini di consumo di energia e area. Le CPU edge mantengono comunque un datapath relativamente semplice con capacit√† di memoria limitate.\n\nTradizionalmente, le CPU sono state sinonimo di elaborazione generica, un termine che √® cambiato anche perch√© il carico di lavoro ‚Äúmedio‚Äù che un consumatore esegue cambia nel tempo. Ad esempio, i componenti in virgola mobile erano un tempo considerati riservati alla ‚Äúelaborazione scientifica‚Äù, di solito venivano implementati come un coprocessore (un componente modulare che funzionava con il datapath) e raramente distribuiti ai consumatori medi. Confrontate questo atteggiamento con quello odierno, in cui le FPU sono integrate in ogni datapath.\n\nVantaggi\nSebbene la produttivit√† in s√© sia limitata, le CPU per uso generico offrono vantaggi pratici di accelerazione AI.\n\nProgrammabilit√† Generale\nLe CPU supportano carichi di lavoro diversi oltre al ML, offrendo una programmabilit√† flessibile per uso generico. Questa versatilit√† deriva dai loro set di istruzioni standardizzati e dagli ecosistemi di compilatori maturi, che consentono di eseguire qualsiasi applicazione, dai database e server Web alle pipeline analitiche (Hennessy e Patterson 2019).\n\nHennessy, John L., e David A. Patterson. 2019. ¬´A new golden age for computer architecture¬ª. Commun. ACM 62 (2): 48‚Äì60. https://doi.org/10.1145/3282307.\nQuesto evita la necessit√† di acceleratori ML dedicati e consente di sfruttare l‚Äôinfrastruttura basata su CPU esistenti per la distribuzione ML di base. Ad esempio, i server X86 di fornitori come Intel e AMD possono eseguire framework ML comuni utilizzando pacchetti Python e TensorFlow insieme ad altri carichi di lavoro aziendali.\n\n\nEcosistema Software Maturo\nPer decenni, librerie matematiche altamente ottimizzate come BLAS, LAPACK e FFTW hanno sfruttato istruzioni vettorializzate e multithreading su CPU (Dongarra 2009). I principali framework ML come PyTorch, TensorFlow e SciKit-Learn sono progettati per integrarsi perfettamente con questi kernel matematici di CPU.\n\nDongarra, Jack J. 2009. ¬´The evolution of high performance computing on system z¬ª. IBM J. Res. Dev. 53: 3‚Äì4.\nI fornitori di hardware come Intel e AMD forniscono anche librerie di basso livello per ottimizzare completamente le prestazioni per primitive di deep learning (accelerazione dell‚Äôinferenza AI su CPU). Questo ecosistema software robusto e maturo consente di distribuire rapidamente ML su infrastrutture di CPU esistenti.\n\n\nAmpia Disponibilit√†\nLe economie di scala della produzione di CPU, guidate dalla domanda in molti mercati come PC, server e dispositivi mobili, le rendono disponibili ovunque. Le CPU Intel, ad esempio, hanno alimentato la maggior parte dei server per decenni (Ranganathan 2011). Questa ampia disponibilit√† nei data center riduce i costi hardware per l‚Äôimplementazione di ML di base.\n\nRanganathan, Parthasarathy. 2011. ¬´From Microprocessors to Nanostores: Rethinking Data-Centric Systems¬ª. Computer 44 (1): 39‚Äì48. https://doi.org/10.1109/mc.2011.18.\nAnche i piccoli dispositivi embedded in genere integrano una certa CPU, consentendo l‚Äôinferenza edge. L‚Äôubiquit√† riduce la necessit√† di acquistare acceleratori ML specializzati in molte situazioni.\n\n\nBasso Consumo per L‚Äôinferenza\nOttimizzazioni come ARM Neon e le estensioni vettoriali Intel AVX forniscono un throughput di numeri interi e in virgola mobile a basso consumo ottimizzato per carichi di lavoro ‚Äúa raffica‚Äù come l‚Äôinferenza (Ignatov et al. 2018). Sebbene pi√π lenta delle GPU, l‚Äôinferenza CPU pu√≤ essere implementata in ambienti con vincoli energetici. Ad esempio, le CPU Cortex-M di ARM ora offrono oltre 1 TOPS di prestazioni INT8 sotto 1 W, consentendo l‚Äôindividuazione di parole chiave e applicazioni di visione su dispositivi edge (ARM).\n\n\n\nSvantaggi\nPur offrendo alcuni vantaggi, le CPU per uso generico presentano anche delle limitazioni per i carichi di lavoro AI.\n\nThroughput Inferiore Rispetto agli Acceleratori\nLe CPU non dispongono delle architetture specializzate per l‚Äôelaborazione parallela massiva che GPU e altri acceleratori forniscono. Il loro design per uso generico riduce il throughput computazionale per le operazioni matematiche altamente parallelizzabili comuni nei modelli ML (N. P. Jouppi et al. 2017a).\n\nJouppi, Norman P., Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal, Raminder Bajwa, Sarah Bates, et al. 2017a. ¬´In-Datacenter Performance Analysis of a Tensor Processing Unit¬ª. In Proceedings of the 44th Annual International Symposium on Computer Architecture, 1‚Äì12. ISCA ‚Äô17. New York, NY, USA: ACM. https://doi.org/10.1145/3079856.3080246.\n\n\nNon Ottimizzato per il Parallelismo dei Dati\nLe architetture delle CPU non sono specificamente ottimizzate per i carichi di lavoro paralleli dei dati inerenti all‚ÄôAI (Sze et al. 2017). Assegnano un‚Äôarea di silicio sostanziale alla decodifica delle istruzioni, all‚Äôesecuzione speculativa, alla memorizzazione nella cache e al controllo del flusso che fornisce pochi vantaggi per le operazioni su array utilizzate nelle reti neurali (accelerazione dell‚Äôinferenza AI sulle CPU). Tuttavia, le CPU moderne sono dotate di istruzioni vettoriali come AVX-512 specificamente per accelerare determinate operazioni chiave come la moltiplicazione matriciale.\nI multiprocessori di streaming GPU, ad esempio, dedicano la maggior parte dei transistor alle unit√† a virgola mobile anzich√© alla logica di predizione di diramazione complessa. Questa specializzazione consente un utilizzo molto pi√π elevato per la matematica ML.\n\n\nMaggiore Latenza della Memoria\nLe CPU soffrono di una latenza maggiore nell‚Äôaccesso alla memoria principale rispetto alle GPU e ad altri acceleratori (DDR). Tecniche come il tiling e il caching possono aiutare, ma la separazione fisica dalla RAM off-chip crea colli di bottiglia nei carichi di lavoro ML ad alta intensit√† di dati. Ci√≤ sottolinea la necessit√† di architetture di memoria specializzate nell‚Äôhardware ML.\n\n\nInefficienza Energetica in Caso di Carichi di Lavoro Pesanti\nSebbene sia adatto per l‚Äôinferenza intermittente, il mantenimento di una produttivit√† quasi di picco per l‚Äôaddestramento comporta un consumo energetico inefficiente sulle CPU, in particolare sulle CPU mobili (Ignatov et al. 2018). Gli acceleratori ottimizzano esplicitamente il flusso di dati, la memoria e il calcolo per carichi di lavoro ML sostenuti. Le CPU sono inefficienti dal punto di vista energetico per l‚Äôaddestramento di modelli di grandi dimensioni.\n\n\n\n\n10.3.6 Confronto\nTabella¬†10.2 confronta i diversi tipi di funzionalit√† hardware.\n\n\n\nTabella¬†10.2: Confronto di diversi acceleratori hardware per carichi di lavoro AI.\n\n\n\n\n\n\n\n\n\n\n\nAcceleratore\nDescrizione\nPrincipali vantaggi\nPrincipali svantaggi\n\n\n\n\nASIC\nIC personalizzati progettati per carichi di lavoro target come l‚Äôinferenza AI\n\nMassimizza le prestazioni/watt.\nOttimizzato per le operazioni tensoriali\nMemoria on-chip a bassa latenza\n\n\nL‚Äôarchitettura fissa manca di flessibilit√†\nElevato costo NRE\nLunghi cicli di progettazione\n\n\n\nFPGA\nFabric riconfigurabile con logica programmabile e routing\n\nArchitettura flessibile\nAccesso alla memoria a bassa latenza\n\n\nPrestazioni/watt inferiori rispetto agli ASIC\nProgrammazione complessa\n\n\n\nGPU\nOriginariamente per la grafica, ora utilizzate per l‚Äôaccelerazione della rete neurale\n\nElevata produttivit√†\nScalabilit√† parallela\nEcosistema software con CUDA\n\n\nNon efficienti dal punto di vista energetico come gli ASIC\nRichiede un‚Äôelevata larghezza di banda della memoria\n\n\n\nCPU\nProcessori per uso generico\n\nProgrammabilit√†\nDisponibilit√† ubiqua\n\n\nPrestazioni inferiori per carichi di lavoro AI\n\n\n\n\n\n\n\nIn generale, le CPU forniscono una baseline prontamente disponibile, le GPU offrono un‚Äôaccelerazione ampiamente accessibile, gli FPGA offrono programmabilit√† e gli ASIC massimizzano l‚Äôefficienza per funzioni fisse. La scelta ottimale dipende dalla scala, dal costo, dalla flessibilit√† e da altri requisiti dell‚Äôapplicazione target.\nSebbene inizialmente sviluppati per l‚Äôimplementazione del data center, Google ha anche profuso notevoli sforzi nello sviluppo di TPU Edge. Questi TPU Edge mantengono l‚Äôispirazione degli array sistolici [https://it.wikipedia.org/wiki/Array_sistolico], ma sono adattati alle risorse limitate accessibili all‚Äôedge.",
    "crumbs": [
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Accelerazione IA</span>"
    ]
  },
  {
    "objectID": "contents/core/hw_acceleration/hw_acceleration.it.html#co-progettazione-hardware-software",
    "href": "contents/core/hw_acceleration/hw_acceleration.it.html#co-progettazione-hardware-software",
    "title": "10¬† Accelerazione IA",
    "section": "10.4 Co-Progettazione Hardware-Software",
    "text": "10.4 Co-Progettazione Hardware-Software\nLa co-progettazione hardware-software si basa sul principio secondo cui i sistemi AI raggiungono prestazioni ed efficienza ottimali quando i componenti hardware e software sono progettati in stretta integrazione. Ci√≤ comporta un ciclo di progettazione iterativo e collaborativo in cui l‚Äôarchitettura hardware e gli algoritmi software vengono sviluppati e perfezionati contemporaneamente con un feedback continuo tra i team.\nAd esempio, un nuovo modello di rete neurale pu√≤ essere prototipato su una piattaforma di accelerazione basata su FPGA per ottenere dati sulle prestazioni reali all‚Äôinizio del processo di progettazione. Questi risultati forniscono un feedback ai progettisti hardware su potenziali ottimizzazioni e agli sviluppatori software su perfezionamenti del modello o framework per sfruttare meglio le capacit√† hardware. Questo livello di sinergia √® difficile da raggiungere con la pratica comune di software sviluppato in modo indipendente per essere distribuito su hardware fisso.\nLa progettazione congiunta √® fondamentale per i sistemi di intelligenza artificiale embedded che affrontano notevoli vincoli di risorse come budget di potenza ridotti, memoria e capacit√† di elaborazione limitate e requisiti di latenza in tempo reale. La stretta integrazione tra sviluppatori di algoritmi e architetti hardware aiuta a sbloccare le ottimizzazioni in tutto lo stack per soddisfare queste restrizioni. Le tecniche di abilitazione includono miglioramenti algoritmici come la ricerca e il pruning [potatura] dell‚Äôarchitettura neurale e progressi hardware come flussi di dati specializzati e gerarchie di memoria.\nRiunendo la progettazione hardware e software, anzich√© svilupparli separatamente, √® possibile realizzare ottimizzazioni olistiche che massimizzano prestazioni ed efficienza. Le sezioni successive forniscono maggiori dettagli su specifici approcci di progettazione congiunta.\n\n10.4.1 La Necessit√† della Progettazione Congiunta\nDiversi fattori chiave rendono essenziale un approccio di progettazione congiunta hardware-software collaborativo per la creazione di sistemi di intelligenza artificiale efficienti.\n\nAumento delle Dimensioni e della Complessit√† del Modello\nI modelli di intelligenza artificiale all‚Äôavanguardia sono cresciuti rapidamente in termini di dimensioni, abilitati dai progressi nella progettazione dell‚Äôarchitettura neurale e dalla disponibilit√† di grandi set di dati. Ad esempio, il modello linguistico GPT-3 contiene 175 miliardi di parametri (Brown et al. 2020), che richiedono enormi risorse di calcolo per l‚Äôaddestramento. Questa esplosione nella complessit√† del modello richiede una progettazione congiunta per sviluppare hardware e algoritmi efficienti in tandem. Tecniche come la compressione del modello (Cheng et al. 2018) e la quantizzazione devono essere co-ottimizzate con l‚Äôarchitettura hardware.\n\nBrown, Tom B., Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, et al. 2020. ¬´Language Models are Few-Shot Learners¬ª. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, a cura di Hugo Larochelle, Marc‚ÄôAurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, e Hsuan-Tien Lin. https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html.\n\nCheng, Yu, Duo Wang, Pan Zhou, e Tao Zhang. 2018. ¬´Model Compression and Acceleration for Deep Neural Networks: The Principles, Progress, and Challenges¬ª. IEEE Signal Process Mag. 35 (1): 126‚Äì36. https://doi.org/10.1109/msp.2017.2765695.\n\n\nVincoli della Distribuzione Embedded\nL‚Äôimplementazione di applicazioni AI su dispositivi edge come telefoni cellulari o elettrodomestici intelligenti introduce vincoli significativi su energia, memoria e area di silicio (Sze et al. 2017). Abilitare l‚Äôinferenza in tempo reale con queste restrizioni richiede la co-esplorazione di ottimizzazioni hardware come flussi di dati specializzati e compressione con progettazione efficiente di reti neurali e tecniche di potatura. La co-progettazione massimizza le prestazioni entro rigidi vincoli di distribuzione.\n\n\nRapida Evoluzione degli Algoritmi AI\nL‚Äôintelligenza artificiale si sta evolvendo rapidamente, con nuove architetture di modelli, metodologie di training e framework software che emergono costantemente. Ad esempio, i Transformers sono diventati di recente molto popolari per l‚ÄôNLP (Young et al. 2018). Per tenere il passo con queste innovazioni algoritmiche √® necessaria una progettazione congiunta hardware-software per adattare le piattaforme ed evitare rapidamente il debito tecnico accumulato.\n\nYoung, Tom, Devamanyu Hazarika, Soujanya Poria, e Erik Cambria. 2018. ¬´Recent Trends in Deep Learning Based Natural Language Processing [Review Article]¬ª. IEEE Comput. Intell. Mag. 13 (3): 55‚Äì75. https://doi.org/10.1109/mci.2018.2840738.\n\n\nInterazioni Complesse Hardware-Software\nMolte interazioni e compromessi sottili tra scelte architettoniche hardware e ottimizzazioni software hanno un impatto significativo sull‚Äôefficienza complessiva. Ad esempio, tecniche come il partizionamento tensoriale e il batching influenzano il parallelismo e i pattern di accesso ai dati influenzano l‚Äôutilizzo della memoria. La progettazione congiunta fornisce una prospettiva multilivello per svelare queste dipendenze.\n\n\nNecessit√† di Specializzazione\nI carichi di lavoro dell‚Äôintelligenza artificiale traggono vantaggio da operazioni specializzate come matematica a bassa precisione e gerarchie di memoria personalizzate. Ci√≤ motiva l‚Äôincorporazione di hardware personalizzato su misura per algoritmi di reti neurali piuttosto che affidarsi esclusivamente a software flessibile in esecuzione su hardware generico (Sze et al. 2017). Tuttavia, lo stack software deve mirare esplicitamente alle operazioni hardware personalizzate per realizzare i vantaggi.\n\n\nRichiesta di Maggiore Efficienza\nCon la crescente complessit√† del modello, si verificano rendimenti decrescenti e spese generali derivanti dall‚Äôottimizzazione del solo hardware o software in isolamento (Putnam et al. 2014). Si presentano inevitabili compromessi che richiedono un‚Äôottimizzazione globale su pi√π livelli. La progettazione congiunta di hardware e software fornisce grandi guadagni di efficienza composti.\n\nPutnam, Andrew, Adrian M. Caulfield, Eric S. Chung, Derek Chiou, Kypros Constantinides, John Demme, Hadi Esmaeilzadeh, et al. 2014. ¬´A reconfigurable fabric for accelerating large-scale datacenter services¬ª. ACM SIGARCH Computer Architecture News 42 (3): 13‚Äì24. https://doi.org/10.1145/2678373.2665678.\n\n\n\n10.4.2 Principi di Progettazione Congiunta Hardware-Software\nL‚Äôarchitettura hardware e lo stack software devono essere strettamente integrati e co-ottimizzati per creare sistemi di intelligenza artificiale efficienti e ad alte prestazioni. Nessuno dei due pu√≤ essere progettato in isolamento; massimizzare le loro sinergie richiede un approccio olistico noto come progettazione congiunta hardware-software.\nL‚Äôobiettivo principale √® adattare le capacit√† hardware in modo che corrispondano agli algoritmi e ai carichi di lavoro eseguiti dal software. Ci√≤ richiede un ciclo di feedback tra architetti hardware e sviluppatori software per convergere su soluzioni ottimizzate. Diverse tecniche consentono un‚Äôefficace co-progettazione:\n\nOttimizzazione Software Consapevole dell‚ÄôHardware\nLo stack software pu√≤ essere ottimizzato per sfruttare meglio le capacit√† hardware:\n\nParallelismo: Parallelizzare i calcoli matriciali come convoluzione o layer di attenzione per massimizzare la produttivit√† sui motori vettoriali.\nOttimizzazione della Memoria: Ottimizzare i layout dei dati per migliorare la localit√† della cache in base alla profilazione hardware. Ci√≤ massimizza il riutilizzo e riduce al minimo l‚Äôaccesso DRAM costoso.\nCompressione: Utilizzare la sparsity [diradazione] nei modelli per ridurre lo spazio di archiviazione e risparmiare sui calcoli tramite operazioni di zero-skipping.\nOperazioni Personalizzate: Incorporare operazioni specializzate come INT4 a bassa precisione o bfloat16 nei modelli per sfruttare al meglio il supporto hardware dedicato.\nMappatura del Flusso di Dati: Mappare esplicitamente le fasi del modello alle unit√† di calcolo per ottimizzare lo spostamento dei dati sull‚Äôhardware.\n\n\n\nSpecializzazione Hardware Algorithm-Driven\nL‚Äôhardware pu√≤ essere adattato alle caratteristiche degli algoritmi ML:\n\nTipi di Dati Personalizzati: Supportare INT8/4 o bfloat16 a bassa precisione nell‚Äôhardware per una maggiore densit√† aritmetica.\nMemoria su Chip: Aumentare la larghezza di banda SRAM e ridurre la latenza di accesso per adattarla ai pattern di accesso alla memoria del modello.\nOperazioni Specifiche del Dominio: Aggiungere unit√† hardware per funzioni ML chiave come FFT o moltiplicazione di matrici per ridurre latenza ed energia.\nProfilazione del Modello: Utilizzare la simulazione e la profilazione del modello per identificare hotspot computazionali e ottimizzare l‚Äôhardware.\n\nLa chiave √® il feedback collaborativo: le informazioni dalla profilazione dell‚Äôhardware guidano le ottimizzazioni del software, mentre i progressi algoritmici informano la specializzazione dell‚Äôhardware. Questo miglioramento reciproco fornisce guadagni di efficienza moltiplicativa rispetto agli sforzi isolati.\n\n\nCo-esplorazione Algoritmo-Hardware\nUna potente tecnica di co-progettazione prevede l‚Äôesplorazione congiunta di innovazioni nelle architetture di reti neurali e nella progettazione custom dell‚Äôhardware. A powerful co-design technique involves jointly exploring innovations in neural network architectures and custom hardware design. Ci√≤ consente di trovare abbinamenti ideali su misura per i rispettivi punti di forza (Sze et al. 2017).\n\nSze, Vivienne, Yu-Hsin Chen, Tien-Ju Yang, e Joel S. Emer. 2017. ¬´Efficient Processing of Deep Neural Networks: A Tutorial and Survey¬ª. Proc. IEEE 105 (12): 2295‚Äì2329. https://doi.org/10.1109/jproc.2017.2761740.\n\nHoward, Andrew G., Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, e Hartwig Adam. 2017. ¬´MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications¬ª. ArXiv preprint. https://arxiv.org/abs/1704.04861.\n\nJacob, Benoit, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard, Hartwig Adam, e Dmitry Kalenichenko. 2018. ¬´Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference¬ª. In 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2704‚Äì13. IEEE. https://doi.org/10.1109/cvpr.2018.00286.\n\nGale, Trevor, Erich Elsen, e Sara Hooker. 2019. ¬´The state of sparsity in deep neural networks¬ª. ArXiv preprint abs/1902.09574. https://arxiv.org/abs/1902.09574.\n\nMishra, Asit K., Jorge Albericio Latorre, Jeff Pool, Darko Stosic, Dusan Stosic, Ganesh Venkatesh, Chong Yu, e Paulius Micikevicius. 2021. ¬´Accelerating Sparse Deep Neural Networks¬ª. CoRR abs/2104.08378. https://arxiv.org/abs/2104.08378.\nAd esempio, il passaggio ad architetture mobili come MobileNets (Howard et al. 2017) √® stato guidato dai vincoli dei dispositivi edge come dimensioni del modello e latenza. La quantizzazione (Jacob et al. 2018) e le tecniche di pruning [potatura] (Gale, Elsen, e Hooker 2019) che hanno reso questi modelli efficienti sono diventate possibili grazie ad acceleratori hardware con supporto nativo per interi a bassa precisione e supporto per potatura (Mishra et al. 2021).\nI modelli basati sull‚Äôattenzione hanno prosperato su GPU e ASIC massivamente paralleli, dove il loro calcolo si mappa bene nello spazialmente, al contrario delle architetture RNN, che si basano sull‚Äôelaborazione sequenziale. La co-evoluzione di algoritmi e hardware ha evidenziato nuove capacit√†.\nUna co-esplorazione efficace richiede una stretta collaborazione tra ricercatori di algoritmi e architetti hardware. La prototipazione rapida su FPGA (C. Zhang et al. 2015) o simulatori di intelligenza artificiale specializzati consente una rapida valutazione di diverse coppie di architetture di modelli e progetti hardware pre-silicio.\n\nZhang, Chen, Peng Li, Guangyu Sun, Yijin Guan, Bingjun Xiao, e Jason Optimizing Cong. 2015. ¬´FPGA-based Accelerator Design for Deep Convolutional Neural Networks Proceedings of the 2015 ACM¬ª. In SIGDA International Symposium on Field-Programmable Gate Arrays-FPGA, 15:161‚Äì70.\nAd esempio, l‚Äôarchitettura TPU di Google si √® evoluta con ottimizzazioni verso i modelli TensorFlow per massimizzare le prestazioni sulla classificazione delle immagini. Questo stretto ciclo di feedback ha prodotto modelli su misura per la TPU che sarebbero stati improbabili in isolamento.\nGli studi hanno mostrato guadagni di prestazioni ed efficienza da 2 a 5 volte superiori con la co-esplorazione algoritmo-hardware rispetto agli sforzi isolati di ottimizzazione di algoritmi o hardware (Suda et al. 2016). Parallelizzare lo sviluppo congiunto riduce anche i ‚Äútime-to-deployment‚Äù [tempi di distribuzione].\n\nSuda, Naveen, Vikas Chandra, Ganesh Dasika, Abinash Mohanty, Yufei Ma, Sarma Vrudhula, Jae-sun Seo, e Yu Cao. 2016. ¬´Throughput-Optimized OpenCL-based FPGA Accelerator for Large-Scale Convolutional Neural Networks¬ª. In Proceedings of the 2016 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays, 16‚Äì25. ACM. https://doi.org/10.1145/2847263.2847276.\nNel complesso, esplorare le strette interdipendenze tra innovazione del modello e progressi hardware crea opportunit√† che devono essere visibili quando vengono affrontate in sequenza. Questa progettazione sinergica congiunta produce soluzioni maggiori della somma delle loro parti.\n\n\n\n10.4.3 Sfide\nSebbene la progettazione collaborativa possa migliorare l‚Äôefficienza, l‚Äôadattabilit√† e il time-to-market, presenta anche sfide ingegneristiche e organizzative.\n\nAumento dei Costi di Prototipazione\n√à richiesta una prototipazione pi√π estesa per valutare diverse accoppiate hardware-software. La necessit√† di prototipi rapidi e iterativi su FPGA o emulatori aumenta il sovraccarico della validazione. Ad esempio, Microsoft ha scoperto che erano necessari pi√π prototipi per la progettazione collaborativa di un acceleratore AI rispetto alla progettazione sequenziale (Fowers et al. 2018).\n\nFowers, Jeremy, Kalin Ovtcharov, Michael Papamichael, Todd Massengill, Ming Liu, Daniel Lo, Shlomi Alkalay, et al. 2018. ¬´A Configurable Cloud-Scale DNN Processor for Real-Time AI¬ª. In 2018 ACM/IEEE 45th Annual International Symposium on Computer Architecture (ISCA), 1‚Äì14. IEEE; IEEE. https://doi.org/10.1109/isca.2018.00012.\n\n\nOstacoli Organizzativi e di Team\nLa progettazione collaborativa richiede uno stretto coordinamento tra gruppi hardware e software tradizionalmente scollegati. Ci√≤ potrebbe causare problemi di comunicazione o priorit√† e pianificazioni non allineate. Anche la navigazione di diversi flussi di lavoro di progettazione √® impegnativa. Potrebbe esistere una certa inerzia organizzativa nell‚Äôadottare pratiche integrate.\n\n\nComplessit√† di Simulazione e Modellazione\nCatturare interazioni sottili tra layer hardware e software per la simulazione e la modellazione congiunte aggiunge una complessit√† significativa. Le astrazioni complete ‚Äúcross-layer‚Äù sono difficili da costruire quantitativamente prima dell‚Äôimplementazione, rendendo pi√π difficile quantificare in anticipo le ottimizzazioni olistiche.\n\n\nRischi di Eccessiva Specializzazione\nUna progettazione congiunta rigorosa comporta il rischio di adattare eccessivamente le ottimizzazioni agli algoritmi correnti, sacrificando la generalit√†. Ad esempio, l‚Äôhardware ottimizzato esclusivamente per i modelli Transformer potrebbe avere prestazioni inferiori con le tecniche future. Mantenere la flessibilit√† richiede lungimiranza.\n\n\nProblemi sui Cambiamenti\nGli ingegneri che hanno familiarit√† con le consolidate pratiche di progettazione hardware o software discrete potrebbero accettare solo flussi di lavoro collaborativi familiari. Nonostante i vantaggi a lungo termine, i progetti potrebbero incontrare attriti nella transizione alla progettazione congiunta.",
    "crumbs": [
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Accelerazione IA</span>"
    ]
  },
  {
    "objectID": "contents/core/hw_acceleration/hw_acceleration.it.html#software-per-hardware-ai",
    "href": "contents/core/hw_acceleration/hw_acceleration.it.html#software-per-hardware-ai",
    "title": "10¬† Accelerazione IA",
    "section": "10.5 Software per Hardware AI",
    "text": "10.5 Software per Hardware AI\nAcceleratori hardware specializzati come GPU, TPU e FPGA sono essenziali per fornire applicazioni di intelligenza artificiale ad alte prestazioni. Tuttavia, √® necessario un ampio stack software per sfruttare efficacemente queste piattaforme hardware, che coprano l‚Äôintero ciclo di vita di sviluppo e distribuzione. Framework e librerie costituiscono la spina dorsale dell‚Äôhardware AI, offrendo set di codice, algoritmi e funzioni pre-costruiti e robusti, specificamente ottimizzati per eseguire varie attivit√† AI su hardware diversi. Sono progettati per semplificare le complessit√† dell‚Äôutilizzo dell‚Äôhardware da zero, che pu√≤ richiedere molto tempo ed essere soggetto a errori. Il software svolge un ruolo importante:\n\nFornendo astrazioni di programmazione e modelli come CUDA e OpenCL per mappare i calcoli sugli acceleratori.\nIntegrando gli acceleratori in framework di deep learning popolari come TensorFlow e PyTorch.\nOttimizzando l‚Äôintero stack hardware-software con compilatori e tool.\nCon piattaforme di simulazione per modellare insieme hardware e software.\nCon l‚Äôinfrastruttura per gestire la distribuzione sugli acceleratori.\n\nQuesto vasto ecosistema software √® importante quanto l‚Äôhardware nel fornire applicazioni AI performanti ed efficienti. Questa sezione fornisce una panoramica degli strumenti disponibili a ogni livello dello stack per consentire agli sviluppatori di creare ed eseguire sistemi AI basati sull‚Äôaccelerazione hardware.\n\n10.5.1 Modelli di Programmazione\nI modelli di programmazione forniscono astrazioni per mappare calcoli e dati su acceleratori hardware eterogenei:\n\nCUDA: Modello di programmazione parallela di Nvidia per sfruttare le GPU utilizzando estensioni a linguaggi come C/C++. Consente di avviare kernel su core GPU (Luebke 2008).\nOpenCL: Standard aperto per scrivere programmi che spaziano tra CPU, GPU, FPGA e altri acceleratori. Specifica un framework di elaborazione eterogeneo (Munshi 2009).\nOpenGL/WebGL: Interfacce di programmazione grafica 3D in grado di mappare codice generico su core GPU (Segal e Akeley 1999).\nVerilog/VHDL: ‚ÄúHardware description languages (HDL)‚Äù [Linguaggi di descrizione hardware] utilizzati per configurare FPGA come acceleratori AI specificando circuiti digitali (Gannot e Ligthart 1994).\nTVM: Un framework di compilazione che fornisce un frontend Python per ottimizzare e mappare modelli di deep learning su diversi backend hardware (Chen et al. 2018).\n\n\nLuebke, David. 2008. ¬´CUDA: Scalable parallel programming for high-performance scientific computing¬ª. In 2008 5th IEEE International Symposium on Biomedical Imaging: From Nano to Macro, 836‚Äì38. IEEE. https://doi.org/10.1109/isbi.2008.4541126.\n\nMunshi, Aaftab. 2009. ¬´The OpenCL specification¬ª. In 2009 IEEE Hot Chips 21 Symposium (HCS), 1‚Äì314. IEEE. https://doi.org/10.1109/hotchips.2009.7478342.\n\nSegal, Mark, e Kurt Akeley. 1999. ¬´The OpenGL graphics system: A specification (version 1.1)¬ª.\n\nGannot, G., e M. Ligthart. 1994. ¬´Verilog HDL based FPGA design¬ª. In International Verilog HDL Conference, 86‚Äì92. IEEE. https://doi.org/10.1109/ivc.1994.323743.\n\nChen, Tianqi, Thierry Moreau, Ziheng Jiang, Lianmin Zheng, Eddie Yan, Haichen Shen, Meghan Cowan, et al. 2018. ¬´TVM: An automated End-to-End optimizing compiler for deep learning¬ª. In 13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18), 578‚Äì94.\nLe sfide principali includono l‚Äôespressione del parallelismo, la gestione della memoria tra dispositivi e l‚Äôabbinamento di algoritmi alle capacit√† hardware. Le astrazioni devono bilanciare la portabilit√† con la possibilit√† di personalizzazione hardware. I modelli di programmazione consentono agli sviluppatori di sfruttare gli acceleratori senza competenze hardware. Questi dettagli sono discussi nella sezione AI frameworks section.\n\n\n\n\n\n\nEsercizio¬†10.1: Software per hardware AI - TVM\n\n\n\n\n\nAbbiamo imparato che l‚Äôhardware AI sofisticato ha bisogno di un software speciale per fare magie. TVM √® come un traduttore super intelligente, che trasforma il codice in istruzioni che gli acceleratori capiscono. In questo Colab, useremo TVM per creare un acceleratore finto chiamato VTA che esegue la moltiplicazione di matrici super velocemente. Pronti a vedere come il software alimenta l‚Äôhardware?\n\n\n\n\n\n\n10.5.2 Librerie e Runtime\nLibrerie e runtime specializzati forniscono astrazioni software per accedere e massimizzare l‚Äôutilizzo degli acceleratori AI:\n\nLibrerie Matematiche: Implementazioni altamente ottimizzate di primitive di algebra lineare come GEMM, FFT, convoluzioni, ecc., su misura per l‚Äôhardware target. Nvidia cuBLAS, Intel MKL e librerie di elaborazione Arm sono esempi.\nIntegrazioni di Framework: Librerie per accelerare framework di deep learning come TensorFlow, PyTorch e MXNet su hardware supportato. Ad esempio, cuDNN accelera le CNN sulle GPU Nvidia.\nRuntime: Software per gestire l‚Äôesecuzione dell‚Äôacceleratore, tra cui pianificazione, sincronizzazione, gestione della memoria e altre attivit√†. Nvidia TensorRT √® un ottimizzatore di inferenza e runtime.\nDriver e firmware: Software di basso livello per interfacciarsi con l‚Äôhardware, inizializzare i dispositivi e gestire l‚Äôesecuzione. Fornitori come Xilinx forniscono driver per le loro schede acceleratrici.\n\nAd esempio, gli integratori PyTorch utilizzano librerie cuDNN e cuBLAS per accelerare l‚Äôaddestramento sulle GPU Nvidia. Il runtime TensorFlow XLA ottimizza e compila modelli per acceleratori come le TPU. I driver inizializzano i dispositivi e delegano le operazioni.\nLe sfide includono il partizionamento e la pianificazione efficienti dei carichi di lavoro su dispositivi eterogenei come nodi multi-GPU. I runtime devono anche ridurre al minimo il sovraccarico dei trasferimenti di dati e della sincronizzazione.\nLibrerie, runtime e driver forniscono i mattoni ottimizzati che gli sviluppatori di deep learning possono sfruttare per le prestazioni dell‚Äôacceleratore senza competenze di programmazione hardware. La loro ottimizzazione √® essenziale per le distribuzioni.\n\n\n10.5.3 Ottimizzazione dei Compilatori\nL‚Äôottimizzazione dei compilatori √® fondamentale per estrarre le massime prestazioni ed efficienza dagli acceleratori hardware per i carichi di lavoro AI. Applicano ottimizzazioni che spaziano tra modifiche algoritmiche, trasformazioni a livello di grafico e generazione di codice di basso livello.\n\nOttimizzazione degli Algoritmi: Tecniche come quantizzazione, potatura e ricerca di architettura neurale per migliorare l‚Äôefficienza del modello e abbinare le capacit√† hardware.\nOttimizzazioni dei Grafi: Ottimizzazioni a livello di grafo come fusione degli operatori, riscrittura e trasformazioni di layout per ottimizzare le prestazioni sull‚Äôhardware target.\nGenerazione di Codice: Generazione di codice di basso livello ottimizzato per acceleratori da modelli e framework di alto livello.\n\nAd esempio, lo stack di compilatori ‚Äúopen‚Äù TVM applica la quantizzazione per un modello BERT che ha come target le GPU Arm. Fonde le operazioni di convoluzione puntuale e trasforma il layout dei pesi per ottimizzare l‚Äôaccesso alla memoria. Infine, emette codice OpenGL ottimizzato per eseguire il carico di lavoro GPU.\nLe ottimizzazioni chiave del compilatore includono la massimizzazione del parallelismo, il miglioramento della localit√† e del riutilizzo dei dati, la riduzione al minimo dell‚Äôingombro della memoria e lo sfruttamento delle operazioni hardware personalizzate. I compilatori creano e ottimizzano i carichi di lavoro di machine learning in modo olistico su componenti hardware come CPU, GPU e altri acceleratori.\nTuttavia, la mappatura efficiente di modelli complessi introduce sfide come il partizionamento efficiente dei carichi di lavoro su dispositivi eterogenei. I compilatori a livello di produzione richiedono anche molto tempo per la messa a punto su carichi di lavoro rappresentativi. Tuttavia, l‚Äôottimizzazione dei compilatori √® per sfruttare tutte le capacit√† degli acceleratori AI.\n\n\n10.5.4 Simulazione e Modellazione\nIl software di simulazione √® importante nella progettazione congiunta hardware-software. Consente accoppiare la modellazione di architetture hardware e stack software proposti:\n\nSimulazione Hardware: Piattaforme come Gem5 consentono la simulazione dettagliata di componenti hardware come pipeline, cache, interconnessioni e gerarchie di memoria. Gli ingegneri possono modellare le modifiche hardware senza prototipazione fisica (Binkert et al. 2011).\nSimulazione Software: Stack di compilatori come TVM supportano la simulazione di carichi di lavoro di machine learning per stimare le prestazioni sulle architetture hardware target. Questo aiuta con le ottimizzazioni software.\nCo-simulazione: Piattaforme unificate come SCALE-Sim (Samajdar et al. 2018) integrano la simulazione hardware e software in un unico strumento. Ci√≤ consente un‚Äôanalisi ‚Äúwhat-if‚Äù per quantificare gli impatti a livello di sistema delle ottimizzazioni cross-layer all‚Äôinizio del ciclo di progettazione.\n\n\nBinkert, Nathan, Bradford Beckmann, Gabriel Black, Steven K. Reinhardt, Ali Saidi, Arkaprava Basu, Joel Hestness, et al. 2011. ¬´The gem5 simulator¬ª. ACM SIGARCH Computer Architecture News 39 (2): 1‚Äì7. https://doi.org/10.1145/2024716.2024718.\n\nSamajdar, Ananda, Yuhao Zhu, Paul Whatmough, Matthew Mattina, e Tushar Krishna. 2018. ¬´Scale-sim: Systolic cnn accelerator simulator¬ª. ArXiv preprint abs/1811.02883. https://arxiv.org/abs/1811.02883.\nAd esempio, un progetto di acceleratore AI basato su FPGA potrebbe essere simulato utilizzando il linguaggio di descrizione hardware Verilog e sintetizzato in un modello Gem5. Verilog √® adatto per descrivere la logica digitale e le interconnessioni dell‚Äôarchitettura dell‚Äôacceleratore. Verilog consente al progettista di specificare i datapath [percorsi dati], la logica di controllo, le memorie on-chip e altri componenti implementati nella struttura FPGA. Una volta completato il progetto Verilog, pu√≤ essere sintetizzato in un modello che simula il comportamento dell‚Äôhardware, ad esempio utilizzando il simulatore Gem5. Gem5 √® utile per questa attivit√† perch√© consente la modellazione di sistemi completi, inclusi processori, cache, bus e acceleratori personalizzati. Gem5 supporta l‚Äôinterfacciamento dei modelli Verilog dell‚Äôhardware alla simulazione, consentendo la modellazione unificata del sistema.\nIl modello di acceleratore FPGA sintetizzato potrebbe quindi avere carichi di lavoro ML simulati utilizzando TVM compilato su di esso all‚Äôinterno dell‚Äôambiente Gem5 per una modellazione unificata. TVM consente la compilazione ottimizzata di modelli di ML su hardware eterogeneo come FPGA. L‚Äôesecuzione di carichi di lavoro compilati con TVM sull‚Äôacceleratore all‚Äôinterno della simulazione Gem5 fornisce un modo integrato per convalidare e perfezionare la progettazione hardware, lo stack software e l‚Äôintegrazione di sistema prima di realizzare fisicamente l‚Äôacceleratore su un FPGA reale.\nQuesto tipo di co-simulazione fornisce stime di metriche complessive come throughput, latenza e potenza per guidare la progettazione congiunta prima della costosa prototipazione fisica. Aiutano anche con le ottimizzazioni di partizionamento tra hardware e software per guidare i compromessi di progettazione.\nTuttavia, la precisione nella modellazione di interazioni sottili di basso livello tra componenti √® limitata. Le simulazioni quantificate sono stime ma non possono sostituire completamente i prototipi fisici e i test. Tuttavia, la simulazione e la modellazione unificate forniscono preziose informazioni iniziali sulle opportunit√† di ottimizzazione a livello di sistema durante il processo di co-progettazione.",
    "crumbs": [
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Accelerazione IA</span>"
    ]
  },
  {
    "objectID": "contents/core/hw_acceleration/hw_acceleration.it.html#benchmarking-dellhardware-ai",
    "href": "contents/core/hw_acceleration/hw_acceleration.it.html#benchmarking-dellhardware-ai",
    "title": "10¬† Accelerazione IA",
    "section": "10.6 Benchmarking dell‚ÄôHardware AI",
    "text": "10.6 Benchmarking dell‚ÄôHardware AI\nIl benchmarking √® un processo critico che quantifica e confronta le prestazioni di varie piattaforme hardware progettate per accelerare le applicazioni di intelligenza artificiale. Guida le decisioni di acquisto, l‚Äôattenzione allo sviluppo e gli sforzi di ottimizzazione delle prestazioni per i produttori di hardware e gli sviluppatori di software.\nIl capitolo sul benchmarking esplora questo argomento in modo molto dettagliato, spiegando perch√© √® diventato una parte indispensabile del ciclo di sviluppo dell‚Äôhardware AI e come influisce sul pi√π ampio panorama tecnologico. Qui, esamineremo brevemente i concetti principali, ma consigliamo di fare riferimento al capitolo per maggiori dettagli.\nSuite di benchmarking come MLPerf, Fathom e AI Benchmark offrono una serie di test standardizzati utilizzabili su diverse piattaforme hardware. Queste suite misurano le prestazioni dell‚Äôacceleratore AI su varie reti neurali e attivit√† di apprendimento automatico, dalla classificazione di immagini di base all‚Äôelaborazione complessa del linguaggio. Fornendo un terreno comune per il confronto, aiutano a garantire che le dichiarazioni sulle prestazioni siano coerenti e verificabili. Questi ‚Äútool‚Äù vengono applicati non solo per guidare lo sviluppo dell‚Äôhardware, ma anche per garantire che lo stack software sfrutti appieno il potenziale dell‚Äôarchitettura sottostante.\n\nMLPerf: Include un ampio set di benchmark che coprono sia l‚Äôaddestramento (Mattson et al. 2020) che l‚Äôinferenza (Reddi et al. 2020) per una gamma di attivit√† di machine learning. Figura¬†11.5 illustra la diversit√† dei casi d‚Äôuso dell‚ÄôIA trattati da MLPerf.\nFathom: Si concentra sulle operazioni principali nei modelli di deep learning, enfatizzandone l‚Äôesecuzione su diverse architetture (Adolf et al. 2016).\nAI Benchmark: Mira a dispositivi mobili e consumer, valutando le prestazioni dell‚ÄôIA nelle applicazioni per utenti finali (Ignatov et al. 2018).\n\n\nMattson, Peter, Vijay Janapa Reddi, Christine Cheng, Cody Coleman, Greg Diamos, David Kanter, Paulius Micikevicius, et al. 2020. ¬´MLPerf: An Industry Standard Benchmark Suite for Machine Learning Performance¬ª. IEEE Micro 40 (2): 8‚Äì16. https://doi.org/10.1109/mm.2020.2974843.\n\nReddi, Vijay Janapa, Christine Cheng, David Kanter, Peter Mattson, Guenther Schmuelling, Carole-Jean Wu, Brian Anderson, et al. 2020. ¬´MLPerf Inference Benchmark¬ª. In 2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA), 446‚Äì59. IEEE; IEEE. https://doi.org/10.1109/isca45697.2020.00045.\n\nAdolf, Robert, Saketh Rama, Brandon Reagen, Gu-yeon Wei, e David Brooks. 2016. ¬´Fathom: Reference workloads for modern deep learning methods¬ª. In 2016 IEEE International Symposium on Workload Characterization (IISWC), 1‚Äì10. IEEE; IEEE. https://doi.org/10.1109/iiswc.2016.7581275.\n\nIgnatov, Andrey, Radu Timofte, William Chou, Ke Wang, Max Wu, Tim Hartley, e Luc Van Gool. 2018. ¬´AI Benchmark: Running deep neural networks on Android smartphones¬ª, 0‚Äì0.\n\n\n\n\n\n\nFigura¬†10.4: MLPerf Training v3.0 e suoi utilizzi. Fonte: Forbes\n\n\n\nI benchmark hanno anche metriche delle prestazioni che sono misure quantificabili utilizzate per valutare l‚Äôefficacia degli acceleratori di IA. Queste metriche forniscono una visione completa delle capacit√† di un acceleratore e vengono utilizzate per guidare il processo di progettazione e selezione per i sistemi di IA. Le metriche comuni comprendono:\n\nThroughput: Solitamente misurato in operazioni al secondo, questo parametro indica il volume di calcoli che un acceleratore pu√≤ gestire.\nLatenza: Il ritardo temporale tra input e output in un sistema √® fondamentale per le attivit√† di elaborazione in tempo reale.\nEfficienza Energetica: Calcolato come elaborazione per watt, che rappresenta il compromesso tra prestazioni e consumo energetico.\nEfficienza dei Costi: Valuta il costo operativo in relazione alle prestazioni, un parametro essenziale per le distribuzioni attente al budget.\nPrecisione: Nelle attivit√† di inferenza, la precisione dei calcoli √® fondamentale e talvolta bilanciata rispetto alla velocit√†.\nScalabilit√†: La capacit√† del sistema di mantenere i guadagni in termini di prestazioni man mano che il carico computazionale aumenta.\n\nI risultati del benchmark forniscono informazioni che vanno oltre i semplici numeri: possono rivelare colli di bottiglia nello stack software e nell‚Äôhardware. Ad esempio, i benchmark possono mostrare come l‚Äôaumento delle dimensioni del batch migliori l‚Äôutilizzo della GPU fornendo pi√π parallelismo o come le ottimizzazioni del compilatore aumentino le prestazioni della TPU. Questi insegnamenti consentono un‚Äôottimizzazione continua (Zhihao Jia, Zaharia, e Aiken 2019).\n\nJia, Zhihao, Matei Zaharia, e Alex Aiken. 2019. ¬´Beyond Data and Model Parallelism for Deep Neural Networks¬ª. In Proceedings of Machine Learning and Systems 2019, MLSys 2019, Stanford, CA, USA, March 31 - April 2, 2019, a cura di Ameet Talwalkar, Virginia Smith, e Matei Zaharia. mlsys.org. https://proceedings.mlsys.org/book/265.pdf.\n\nZhu, Hongyu, Mohamed Akrout, Bojian Zheng, Andrew Pelegris, Anand Jayarajan, Amar Phanishayee, Bianca Schroeder, e Gennady Pekhimenko. 2018. ¬´Benchmarking and Analyzing Deep Neural Network Training¬ª. In 2018 IEEE International Symposium on Workload Characterization (IISWC), 88‚Äì100. IEEE; IEEE. https://doi.org/10.1109/iiswc.2018.8573476.\nIl benchmarking standardizzato fornisce una valutazione quantificata e comparabile degli acceleratori AI per informare la progettazione, l‚Äôacquisto e l‚Äôottimizzazione. Tuttavia, anche la convalida delle prestazioni nel mondo reale rimane essenziale (H. Zhu et al. 2018).",
    "crumbs": [
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Accelerazione IA</span>"
    ]
  },
  {
    "objectID": "contents/core/hw_acceleration/hw_acceleration.it.html#sfide-e-soluzioni",
    "href": "contents/core/hw_acceleration/hw_acceleration.it.html#sfide-e-soluzioni",
    "title": "10¬† Accelerazione IA",
    "section": "10.7 Sfide e Soluzioni",
    "text": "10.7 Sfide e Soluzioni\nGli acceleratori AI offrono notevoli miglioramenti delle prestazioni, ma spesso √® necessario migliorare i significativi problemi di portabilit√† e compatibilit√† nella loro integrazione nel pi√π ampio panorama AI. Il nocciolo della questione risiede nella diversit√† dell‚Äôecosistema AI: esiste una vasta gamma di acceleratori, framework e linguaggi di programmazione per l‚Äôapprendimento automatico, ognuno con le sue caratteristiche e requisiti unici.\n\n10.7.1 Problemi di Portabilit√†/Compatibilit√†\nGli sviluppatori incontrano spesso difficolt√† nel trasferire i loro modelli AI da un ambiente hardware a un altro. Ad esempio, un modello di machine learning sviluppato per un ambiente desktop in Python utilizzando il framework PyTorch, ottimizzato per una GPU Nvidia, potrebbe non essere facilmente trasferito a un dispositivo pi√π vincolato come Arduino Nano 33 BLE. Questa complessit√† deriva da nette differenze nei requisiti di programmazione: Python e PyTorch sul desktop rispetto a un ambiente C++ su un Arduino, per non parlare del passaggio dall‚Äôarchitettura x86 ad ARM ISA.\nQueste divergenze evidenziano la complessit√† della portabilit√† all‚Äôinterno dei sistemi AI. Inoltre, il rapido progresso negli algoritmi e nei modelli di intelligenza artificiale implica che gli acceleratori hardware debbano adattarsi continuamente, creando un obiettivo mobile per la compatibilit√†. L‚Äôassenza di standard e interfacce universali aggrava il problema, rendendo difficile l‚Äôimplementazione di soluzioni di intelligenza artificiale in modo coerente su vari dispositivi e piattaforme.\n\nSoluzioni e Strategie\nPer affrontare questi ostacoli, il settore dell‚Äôintelligenza artificiale si sta muovendo verso diverse soluzioni:\n\nIniziative di Standardizzazione\nOpen Neural Network Exchange (ONNX) √® in prima linea in questa ricerca, proponendo un ecosistema aperto e condiviso che promuove l‚Äôintercambiabilit√† dei modelli. ONNX facilita l‚Äôuso di modelli di intelligenza artificiale su vari framework, consentendo ai modelli addestrati in un ambiente di essere distribuiti in modo efficiente in un altro, riducendo significativamente la necessit√† di riscritture o modifiche che richiedono molto tempo.\n\n\nFramework Multipiattaforma\nA complemento degli sforzi di standardizzazione, framework multipiattaforma come TensorFlow Lite e PyTorch Mobile sono stati sviluppati specificamente per creare coesione tra diversi ambienti di calcolo che vanno dai desktop ai dispositivi mobili ed embedded. Questi framework offrono versioni semplificate e leggere delle loro versioni principali, garantendo compatibilit√† e integrit√† funzionale su diversi tipi di hardware senza sacrificare le prestazioni. Ci√≤ garantisce che gli sviluppatori possano creare applicazioni con la certezza che funzioneranno su molti dispositivi, colmando un divario che tradizionalmente ha rappresentato una sfida considerevole nello sviluppo dell‚Äôintelligenza artificiale.\n\n\nPiattaforme Indipendenti dall‚ÄôHardware\nL‚Äôascesa delle piattaforme indipendenti dall‚Äôhardware ha anche svolto un ruolo importante nella democratizzazione dell‚Äôuso dell‚ÄôIA. Creando ambienti in cui le applicazioni di IA possono essere eseguite su vari acceleratori, queste piattaforme eliminano l‚Äôonere della codifica specifica per l‚Äôhardware dagli sviluppatori. Questa astrazione semplifica il processo di sviluppo e apre nuove possibilit√† per l‚Äôinnovazione e l‚Äôimplementazione delle applicazioni, libere dai vincoli delle specifiche hardware.\n\n\nStrumenti di Compilazione Avanzati\nInoltre, l‚Äôavvento di strumenti di compilazione avanzati come TVM, un compilatore di tensori end-to-end, offre un percorso ottimizzato attraverso la giungla delle diverse architetture hardware. TVM fornisce agli sviluppatori i mezzi per mettere a punto modelli di machine learning per un ampio spettro di substrati computazionali, garantendo prestazioni ottimali ed evitando la regolazione manuale del modello ogni volta che si verifica uno spostamento nell‚Äôhardware sottostante.\n\n\nCollaborazione tra Comunit√† e Settore\nLa collaborazione tra comunit√† open source e consorzi di settore non pu√≤ essere sottovalutata. Questi organismi collettivi sono fondamentali per la formazione di standard condivisi e best practice a cui tutti gli sviluppatori e i produttori possono aderire. Tale collaborazione promuove un ecosistema AI pi√π unificato e sinergico, riducendo significativamente la prevalenza di problemi di portabilit√† e spianando la strada verso l‚Äôintegrazione e l‚Äôavanzamento dell‚ÄôAI globale. Attraverso questi lavori combinati, l‚ÄôAI si sta muovendo costantemente verso un futuro in cui la distribuzione di modelli senza soluzione di continuit√† su varie piattaforme diventa uno standard piuttosto che un‚Äôeccezione.\nRisolvere le sfide della portabilit√† √® fondamentale per il campo dell‚ÄôIA per realizzare il pieno potenziale degli acceleratori hardware in un panorama tecnologico dinamico e diversificato. Richiede uno sforzo concertato da parte dei produttori di hardware, degli sviluppatori di software e degli enti normativi per creare un ambiente pi√π interoperabile e flessibile. Con innovazione e collaborazione continue, la comunit√† dell‚ÄôIA pu√≤ aprire la strada a un‚Äôintegrazione e a un‚Äôimplementazione senza soluzione di continuit√† dei modelli di IA su molte piattaforme.\n\n\n\n\n10.7.2 Problemi di Consumo Energetico\nIl consumo energetico √® un problema cruciale nello sviluppo e nel funzionamento degli acceleratori AI dei data center, come le unit√† di elaborazione grafica (GPU) e le unit√† di elaborazione tensoriale (TPU) (N. P. Jouppi et al. 2017b) (Norrie et al. 2021) (N. Jouppi et al. 2023). Questi potenti componenti sono la spina dorsale dell‚Äôinfrastruttura AI contemporanea, ma le loro elevate richieste di energia contribuiscono all‚Äôimpatto ambientale della tecnologia e aumentano significativamente i costi operativi. Man mano che le esigenze di elaborazione dei dati diventano pi√π complesse, con la crescente popolarit√† dell‚ÄôAI e del deep learning, c‚Äô√® una richiesta pressante di GPU e TPU in grado di fornire la potenza di calcolo necessaria in modo pi√π efficiente. L‚Äôimpatto di tali progressi √® duplice: possono ridurre l‚Äôimpatto ambientale di queste tecnologie e ridurre i costi di esecuzione delle applicazioni AI.\n\n‚Äî‚Äî‚Äî, et al. 2017b. ¬´In-Datacenter Performance Analysis of a Tensor Processing Unit¬ª. In Proceedings of the 44th Annual International Symposium on Computer Architecture, 1‚Äì12. ISCA ‚Äô17. New York, NY, USA: ACM. https://doi.org/10.1145/3079856.3080246.\n\nNorrie, Thomas, Nishant Patil, Doe Hyun Yoon, George Kurian, Sheng Li, James Laudon, Cliff Young, Norman Jouppi, e David Patterson. 2021. ¬´The Design Process for Google‚Äôs Training Chips: Tpuv2 and TPUv3¬ª. IEEE Micro 41 (2): 56‚Äì63. https://doi.org/10.1109/mm.2021.3058217.\n\nJouppi, Norm, George Kurian, Sheng Li, Peter Ma, Rahul Nagarajan, Lifeng Nai, Nishant Patil, et al. 2023. ¬´TPU v4: An Optically Reconfigurable Supercomputer for Machine Learning with Hardware Support for Embeddings¬ª. In Proceedings of the 50th Annual International Symposium on Computer Architecture. ISCA ‚Äô23. New York, NY, USA: ACM. https://doi.org/10.1145/3579371.3589350.\nLe tecnologie hardware emergenti sono sul punto di rivoluzionare l‚Äôefficienza energetica in questo settore. L‚Äôinformatica fotonica, ad esempio, utilizza la luce anzich√© l‚Äôelettricit√† per trasportare informazioni, offrendo la promessa di un‚Äôelaborazione ad alta velocit√† con una frazione del consumo energetico. Analizziamo pi√π approfonditamente questa e altre tecnologie innovative nella sezione ‚ÄúTecnologie Hardware Emergenti‚Äù, esplorando il loro potenziale per affrontare le attuali sfide del consumo energetico.\nAi margini della rete, gli acceleratori AI sono progettati per elaborare dati su dispositivi come smartphone, sensori IoT e dispositivi indossabili intelligenti. Questi dispositivi spesso funzionano con gravi limitazioni di potenza, rendendo necessario un attento bilanciamento tra prestazioni e consumo energetico. Un modello AI ad alte prestazioni pu√≤ fornire risultati rapidi, ma a costo di esaurire rapidamente la durata della batteria e aumentare la produzione termica, il che pu√≤ influire sulla funzionalit√† e sulla durata del dispositivo. La posta in gioco √® pi√π alta per i dispositivi distribuiti in aree remote o difficili da raggiungere, dove non √® possibile garantire un‚Äôalimentazione costante, il che sottolinea la necessit√† di soluzioni a basso consumo energetico.\nI problemi di latenza aggravano ulteriormente la sfida dell‚Äôefficienza energetica ai margini. Le applicazioni AI Edge in settori quali la guida autonoma e il monitoraggio sanitario richiedono velocit√†, precisione e affidabilit√†, poich√© i ritardi nell‚Äôelaborazione possono comportare gravi rischi per la sicurezza. Per queste applicazioni, gli sviluppatori devono ottimizzare sia gli algoritmi AI sia la progettazione hardware per raggiungere un equilibrio ottimale tra consumo energetico e latenza.\nQuesto sforzo di ottimizzazione non riguarda solo l‚Äôapporto di miglioramenti incrementali alle tecnologie esistenti; riguarda il ripensamento di come e dove elaboriamo le attivit√† AI. Progettando acceleratori AI che siano sia efficienti dal punto di vista energetico sia in grado di elaborare rapidamente, possiamo garantire che questi dispositivi svolgano i loro scopi previsti senza un consumo energetico non necessario o prestazioni compromesse. Tali sviluppi potrebbero promuovere l‚Äôadozione diffusa dell‚ÄôAI in vari settori, consentendo un uso pi√π intelligente, sicuro e sostenibile della tecnologia.\n\n\n10.7.3 Superare i Vincoli delle Risorse\nAnche i vincoli di risorse rappresentano una sfida significativa per gli acceleratori Edge AI, poich√© queste soluzioni hardware e software specializzate devono fornire prestazioni robuste entro i limiti dei dispositivi edge. A causa dei limiti di potenza e dimensioni, gli acceleratori Edge AI hanno spesso capacit√† di calcolo, memoria e archiviazione limitate (L. Zhu et al. 2023). Questa scarsit√† di risorse richiede un‚Äôattenta allocazione delle capacit√† di elaborazione per eseguire modelli di apprendimento automatico in modo efficiente.\n\nZhu, Ligeng, Lanxiang Hu, Ji Lin, Wei-Ming Chen, Wei-Chen Wang, Chuang Gan, e Song Han. 2023. ¬´PockEngine: Sparse and Efficient Fine-tuning in a Pocket¬ª. In 56th Annual IEEE/ACM International Symposium on Microarchitecture. ACM. https://doi.org/10.1145/3613424.3614307.\n\nLin, Ji, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, e Song Han. 2023. ¬´AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration¬ª. arXiv.\n\nLi, Yuhang, Xin Dong, e Wei Wang. 2020. ¬´Additive Powers-of-Two Quantization: An Efficient Non-uniform Discretization for Neural Networks¬ª. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net. https://openreview.net/forum?id=BkgXT24tDS.\n\nWang, Tianzhe, Kuan Wang, Han Cai, Ji Lin, Zhijian Liu, Hanrui Wang, Yujun Lin, e Song Han. 2020. ¬´APQ: Joint Search for Network Architecture, Pruning and Quantization Policy¬ª. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2075‚Äì84. IEEE. https://doi.org/10.1109/cvpr42600.2020.00215.\nInoltre, la gestione di risorse limitate richiede approcci innovativi, tra cui la quantizzazione del modello (Lin et al. 2023) (Li, Dong, e Wang 2020), pruning (Wang et al. 2020) e l‚Äôottimizzazione delle pipeline di inferenza. Gli acceleratori Edge AI devono trovare un delicato equilibrio tra la fornitura di funzionalit√† AI significative e il non esaurire le risorse disponibili, mantenendo al contempo un basso consumo energetico. Superare questi vincoli di risorse √® fondamentale per garantire l‚Äôimplementazione di successo dell‚Äôintelligenza artificiale ai margini, dove molte applicazioni, dall‚ÄôIoT ai dispositivi mobili, si basano sull‚Äôuso efficiente di risorse hardware limitate per fornire un processo decisionale intelligente e in tempo reale.",
    "crumbs": [
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Accelerazione IA</span>"
    ]
  },
  {
    "objectID": "contents/core/hw_acceleration/hw_acceleration.it.html#tecnologie-emergenti",
    "href": "contents/core/hw_acceleration/hw_acceleration.it.html#tecnologie-emergenti",
    "title": "10¬† Accelerazione IA",
    "section": "10.8 Tecnologie Emergenti",
    "text": "10.8 Tecnologie Emergenti\nFinora abbiamo discusso la tecnologia hardware AI nel contesto della progettazione dell‚Äôarchitettura von Neumann convenzionale e dell‚Äôimplementazione basata su CMOS. Questi chip AI specializzati offrono vantaggi come una maggiore produttivit√† ed efficienza energetica, ma si basano sui principi di elaborazione tradizionali. La crescita inarrestabile della domanda di potenza di elaborazione AI sta guidando le innovazioni nei metodi di integrazione per l‚Äôhardware AI.\nSono emersi due approcci principali per massimizzare la densit√† di elaborazione, l‚Äôintegrazione su ‚Äúscala wafer‚Äù e le architetture basate su ‚Äúchiplet‚Äù, di cui parleremo in questa sezione. Guardando molto pi√π avanti, esamineremo le tecnologie emergenti che divergono dalle architetture convenzionali e adottano approcci fondamentalmente diversi per l‚Äôelaborazione specializzata AI.\nAlcuni di questi paradigmi non convenzionali includono l‚Äôelaborazione neuromorfica, che imita le reti neurali biologiche; l‚Äôelaborazione quantistica, che sfrutta gli effetti della meccanica quantistica; e l‚Äôelaborazione ottica, che utilizza fotoni anzich√© elettroni. Oltre ai nuovi substrati di elaborazione, le nuove tecnologie dei dispositivi stanno consentendo ulteriori guadagni attraverso una migliore memoria e interconnessione.\nEsempi includono i ‚Äúmemristor‚Äù [https://it.wikipedia.org/wiki/Memristore] per l‚Äôelaborazione in memoria e la nanofotonica per la comunicazione fotonica integrata. Insieme, queste tecnologie offrono il potenziale per miglioramenti di ordini di grandezza in termini di velocit√†, efficienza e scalabilit√† rispetto all‚Äôattuale hardware AI. Esamineremo questi aspetti in questa sezione.\n\n10.8.1 Metodi di Integrazione\nI metodi di integrazione si riferiscono agli approcci utilizzati per combinare e interconnettere i vari componenti di elaborazione e memoria di un chip o sistema AI. Collegando strettamente gli elementi di elaborazione chiave, l‚Äôintegrazione cerca di massimizzare le prestazioni, l‚Äôefficienza energetica e la densit√†.\nIn passato, l‚Äôelaborazione AI veniva eseguita principalmente su CPU e GPU costruite utilizzando metodi di integrazione convenzionali. Questi componenti discreti venivano fabbricati separatamente e collegati insieme su una scheda. Tuttavia, questa integrazione poco stretta crea colli di bottiglia, come i sovraccarichi dei trasferimento di dati.\nCon l‚Äôaumento dei carichi di lavoro AI, aumenta la domanda di una pi√π stretta integrazione tra elementi di elaborazione, memoria e comunicazione. Alcuni fattori chiave dell‚Äôintegrazione includono:\n\nRiduzione al minimo dello spostamento dei dati: Una stretta integrazione riduce la latenza e l‚Äôenergia per lo spostamento dei dati tra i componenti. Ci√≤ migliora l‚Äôefficienza.\nPersonalizzazione: Adattare tutti i componenti del sistema ai carichi di lavoro AI consente ottimizzazioni in tutto lo stack hardware.\nParallelismo: L‚Äôintegrazione di molti elementi di elaborazione consente un calcolo parallelo massiccio.\nDensit√†: Una pi√π stretta integrazione consente di impacchettare pi√π transistor e memoria in una determinata area.\nCosto: Le economie di scala derivanti da grandi sistemi integrati possono ridurre i costi.\n\nIn risposta, nuove tecniche di produzione come la fabbricazione su scala di wafer e il confezionamento avanzato consentono ora livelli di integrazione molto pi√π elevati. L‚Äôobiettivo √® creare complessi di elaborazione AI unificati e specializzati, su misura per il deep learning e altri algoritmi AI. Un‚Äôintegrazione pi√π stretta √® fondamentale per fornire le prestazioni e l‚Äôefficienza necessarie per la prossima generazione di AI.\n\nAI su Scala Wafer\nL‚Äôintelligenza artificiale su ‚Äúwafer-scale‚Äù adotta un approccio estremamente integrato, producendo un intero wafer di silicio come un gigantesco chip. Ci√≤ differisce drasticamente dalle CPU e GPU convenzionali, che tagliano ogni wafer in molti chip singoli pi√π piccoli. Figura¬†10.5 mostra un confronto tra Cerebras Wafer Scale Engine 2, che √® il chip pi√π grande mai costruito, e la GPU pi√π grande. Mentre alcune GPU possono contenere miliardi di transistor, impallidiscono comunque rispetto alla scala di un chip delle dimensioni di un wafer con oltre un trilione di transistor.\n\n\n\n\n\n\nFigura¬†10.5: Wafer-scale vs.¬†GPU. Fonte: Cerebras.\n\n\n\nL‚Äôapproccio su scala di wafer diverge anche dai progetti system-on-chip pi√π modulari che hanno ancora componenti discreti che comunicano tramite bus. Invece, l‚Äôintelligenza artificiale su scala di wafer consente la personalizzazione completa e la stretta integrazione di elaborazione, memoria e interconnessioni nell‚Äôintero di die.\nProgettando il wafer come un‚Äôunit√† logica integrata, il trasferimento dati tra gli elementi √® ridotto al minimo. Ci√≤ fornisce una latenza e un consumo energetico inferiori rispetto ai design discreti system-on-chip o chiplet. Mentre i chiplet possono offrire flessibilit√† mescolando e abbinando i componenti, la comunicazione tra chiplet √® impegnativa. La natura monolitica dell‚Äôintegrazione su scala wafer elimina questi colli di bottiglia nella comunicazione tra chip.\nTuttavia, la scala ultra-large pone anche difficolt√† per la producibilit√† e la resa con i design su scala wafer. Difetti in qualsiasi regione del wafer possono rendere (alcune parti del) chip inutilizzabile. Sono necessarie tecniche di litografia specializzate per produrre tali matrici di grandi dimensioni. Quindi, l‚Äôintegrazione su scala wafer persegue i massimi guadagni in termini di prestazioni dall‚Äôintegrazione ma richiede il superamento di sostanziali sfide di fabbricazione.\nVideo¬†10.1 fornisce ulteriore contesto sui chip AI su scala wafer.\n\n\n\n\n\n\nVideo¬†10.1: Wafer-scale AI Chips\n\n\n\n\n\n\n\n\nChiplet per AI\nIl design chiplet si riferisce a un‚Äôarchitettura semiconduttrice in cui un singolo circuito integrato (IC) √® costruito da pi√π componenti pi√π piccoli e individuali noti come chiplet. Ogni chiplet √® un blocco funzionale autonomo, in genere specializzato per un‚Äôattivit√† o funzionalit√† specifica. Questi chiplet sono quindi interconnessi su un substrato o un package pi√π grande per creare un sistema coeso.\nFigura¬†10.6 illustra questo concetto. Per l‚Äôhardware AI, i chiplet consentono di combinare diversi tipi di chip ottimizzati per attivit√† come moltiplicazione di matrici, spostamento di dati, I/O analogico e memorie specializzate. Questa integrazione eterogenea differisce notevolmente dall‚Äôintegrazione wafer-scale, in cui tutta la logica √® prodotta come un unico chip monolitico. Aziende come Intel e AMD hanno adottato design chiplet per le loro CPU.\nI chiplet sono interconnessi utilizzando tecniche di packaging avanzate come interposer di substrato ad alta densit√†, impilamento 2.5D/3D e packaging a livello di wafer. Ci√≤ consente di combinare chiplet realizzati con diversi nodi di processo, memorie specializzate e vari motori AI ottimizzati.\n\n\n\n\n\n\nFigura¬†10.6: Partizionamento chiplet. Fonte: Vivet et al. (2021).\n\n\nVivet, Pascal, Eric Guthmuller, Yvain Thonnart, Gael Pillonnet, Cesar Fuguet, Ivan Miro-Panades, Guillaume Moritz, et al. 2021. ¬´IntAct: A 96-Core Processor With Six Chiplets 3D-Stacked on an Active Interposer With Distributed Interconnects and Integrated Power Management¬ª. IEEE J. Solid-State Circuits 56 (1): 79‚Äì97. https://doi.org/10.1109/jssc.2020.3036341.\n\n\nEcco alcuni vantaggi chiave dell‚Äôuso di chiplet per l‚Äôintelligenza artificiale:\n\nFlessibilit√†: I chiplet consentono la combinazione di diversi tipi di chip, nodi di processo e memorie su misura per ogni funzione. Questo √® pi√π modulare rispetto a un design fisso su scala wafer.\nResa: I chiplet pi√π piccoli hanno una resa maggiore rispetto a un gigantesco chip su scala wafer. I difetti sono contenuti nei singoli chiplet.\nCosto: Sfrutta le capacit√† di produzione esistenti anzich√© richiedere nuovi processi specializzati. Riduce i costi riutilizzando la fabbricazione assestata.\nCompatibilit√†: Pu√≤ integrarsi con architetture di sistema pi√π convenzionali come PCIe e interfacce di memoria DDR standard.\n\nTuttavia, i chiplet devono anche affrontare sfide di integrazione e prestazioni:\n\nDensit√† inferiore rispetto alla scala wafer, poich√© i chiplet sono limitati in termini di dimensioni.\nLatenza aggiuntiva durante la comunicazione tra chiplet rispetto all‚Äôintegrazione monolitica. Richiede ottimizzazione per interconnessioni a bassa latenza.\nIl packaging avanzato aggiunge complessit√† rispetto all‚Äôintegrazione su scala wafer, sebbene ci√≤ sia discutibile.\n\nL‚Äôobiettivo principale dei chiplet √® trovare il giusto equilibrio tra flessibilit√† modulare e densit√† di integrazione per prestazioni AI ottimali. I chiplet mirano a un‚Äôaccelerazione AI efficiente pur lavorando entro i vincoli delle tecniche di produzione convenzionali. I chiplet prendono una via di mezzo tra gli estremi dell‚Äôintegrazione su scala wafer e dei componenti completamente discreti. Ci√≤ fornisce vantaggi pratici ma pu√≤ sacrificare una certa densit√† computazionale ed efficienza rispetto a un sistema teorico a livello di wafer.\n\n\n\n10.8.2 Elaborazione N√πeuromorfica\nL‚Äôelaborazione neuromorfica √® un campo emergente che mira a emulare l‚Äôefficienza e la robustezza dei sistemi neurali biologici per applicazioni di machine learning. Una differenza fondamentale rispetto alle classiche architetture di Von Neumann √® la fusione di memoria ed elaborazione nello stesso circuito (Schuman et al. 2022; Markoviƒá et al. 2020; Furber 2016), come illustrato in Figura¬†10.7. La struttura del cervello ispira questo approccio integrato. Un vantaggio fondamentale √® il potenziale per un miglioramento di ordini di grandezza nel calcolo efficiente dal punto di vista energetico rispetto all‚Äôhardware AI convenzionale. Ad esempio, le stime prevedono guadagni di 100x-1000x nell‚Äôefficienza energetica rispetto agli attuali sistemi basati su GPU per carichi di lavoro equivalenti.\n\nMarkoviƒá, Danijela, Alice Mizrahi, Damien Querlioz, e Julie Grollier. 2020. ¬´Physics for neuromorphic computing¬ª. Nature Reviews Physics 2 (9): 499‚Äì510. https://doi.org/10.1038/s42254-020-0208-2.\n\nFurber, Steve. 2016. ¬´Large-scale neuromorphic computing systems¬ª. J. Neural Eng. 13 (5): 051001. https://doi.org/10.1088/1741-2560/13/5/051001.\n\n\n\n\n\n\nFigura¬†10.7: Confronto tra l‚Äôarchitettura di von Neumann e l‚Äôarchitettura neuromorfica. Fonte: Schuman et al. (2022).\n\n\nSchuman, Catherine D., Shruti R. Kulkarni, Maryam Parsa, J. Parker Mitchell, Prasanna Date, e Bill Kay. 2022. ¬´Opportunities for neuromorphic computing algorithms and applications¬ª. Nature Computational Science 2 (1): 10‚Äì19. https://doi.org/10.1038/s43588-021-00184-y.\n\n\nIntel e IBM stanno guidando gli sforzi commerciali nell‚Äôhardware neuromorfico. I chip Loihi e Loihi 2 di Intel (Davies et al. 2018, 2021) offrono core neuromorfici programmabili con apprendimento on-chip. Il dispositivo Northpole (Modha et al. 2023) di IBM comprende oltre 100 milioni di sinapsi a giunzione a tunnel magnetico e 68 miliardi di transistor. Questi chip specializzati offrono vantaggi come un basso consumo energetico per l‚Äôinferenza edge.\n\nDavies, Mike, Narayan Srinivasa, Tsung-Han Lin, Gautham Chinya, Yongqiang Cao, Sri Harsha Choday, Georgios Dimou, et al. 2018. ¬´Loihi: A Neuromorphic Manycore Processor with On-Chip Learning¬ª. IEEE Micro 38 (1): 82‚Äì99. https://doi.org/10.1109/mm.2018.112130359.\n\nDavies, Mike, Andreas Wild, Garrick Orchard, Yulia Sandamirskaya, Gabriel A. Fonseca Guerra, Prasad Joshi, Philipp Plank, e Sumedh R. Risbud. 2021. ¬´Advancing Neuromorphic Computing With Loihi: A Survey of Results and Outlook¬ª. Proc. IEEE 109 (5): 911‚Äì34. https://doi.org/10.1109/jproc.2021.3067593.\n\nModha, Dharmendra S., Filipp Akopyan, Alexander Andreopoulos, Rathinakumar Appuswamy, John V. Arthur, Andrew S. Cassidy, Pallab Datta, et al. 2023. ¬´Neural inference at the frontier of energy, space, and time¬ª. Science 382 (6668): 329‚Äì35. https://doi.org/10.1126/science.adh1174.\n\nMaass, Wolfgang. 1997. ¬´Networks of spiking neurons: The third generation of neural network models¬ª. Neural Networks 10 (9): 1659‚Äì71. https://doi.org/10.1016/s0893-6080(97)00011-7.\nLe ‚ÄúSpiking neural network (SNN)‚Äù (Maass 1997) sono modelli computazionali per hardware neuromorfico. A differenza delle reti neurali profonde che comunicano tramite valori continui, le SNN utilizzano picchi discreti che sono pi√π simili ai neuroni biologici. Questo consente un calcolo efficiente basato sugli eventi anzich√© un‚Äôelaborazione costante. Inoltre, le SNN considerano le caratteristiche temporali e spaziali dei dati di input. Ci√≤ imita meglio le reti neurali biologiche, in cui la tempistica dei picchi neuronali svolge un ruolo importante.\nTuttavia, l‚Äôaddestramento delle SNN rimane impegnativo a causa della complessit√† temporale aggiunta. Figura¬†10.8 fornisce una panoramica della metodologia spiking: (a) illustrazione di un neurone; (b) Misura di un potenziale d‚Äôazione propagato lungo l‚Äôassone di un neurone. Solo il potenziale d‚Äôazione √® rilevabile lungo l‚Äôassone; (c) Il picco del neurone √® approssimato con una rappresentazione binaria; (d) Elaborazione guidata dagli eventi; (e) Active Pixel Sensor e Dynamic Vision Sensor.\n\n\n\n\n\n\nFigura¬†10.8: Spiking neuromorfico. Fonte: Eshraghian et al. (2023).\n\n\nEshraghian, Jason K., Max Ward, Emre O. Neftci, Xinxin Wang, Gregor Lenz, Girish Dwivedi, Mohammed Bennamoun, Doo Seok Jeong, e Wei D. Lu. 2023. ¬´Training Spiking Neural Networks Using Lessons From Deep Learning¬ª. Proc. IEEE 111 (9): 1016‚Äì54. https://doi.org/10.1109/jproc.2023.3308088.\n\n\nSi pu√≤ anche guardare Video¬†10.2 linkato di seguito per una spiegazione pi√π dettagliata.\n\n\n\n\n\n\nVideo¬†10.2: Neuromorphic Computing\n\n\n\n\n\n\nDispositivi nanoelettronici specializzati chiamati memristor (Chua 1971) sono componenti sinaptici nei sistemi neuromorfici. I memristor agiscono come memoria non volatile con conduttanza regolabile, emulando la plasticit√† delle sinapsi reali. I memristor consentono l‚Äôapprendimento in situ senza trasferimenti di dati separati combinando funzioni di memoria ed elaborazione. Tuttavia, la tecnologia dei memristor deve ancora raggiungere la maturit√† e la scalabilit√† per l‚Äôhardware commerciale.\n\nChua, L. 1971. ¬´Memristor-The missing circuit element¬ª. #IEEE_J_CT# 18 (5): 507‚Äì19. https://doi.org/10.1109/tct.1971.1083337.\nL‚Äôintegrazione della fotonica con il calcolo neuromorfico (Shastri et al. 2021) √® emersa di recente come un‚Äôarea di ricerca attiva. L‚Äôuso della luce per il calcolo e la comunicazione consente alte velocit√† e un consumo energetico ridotto. Tuttavia, la piena realizzazione di sistemi neuromorfici fotonici richiede il superamento di problemi di progettazione e integrazione.\nIl calcolo neuromorfico offre promettenti capacit√† per un‚Äôefficace inferenza edge, ma incontra ostacoli in merito ad algoritmi di addestramento, integrazione dei nanodispositivi e progettazione del sistema. La ricerca multidisciplinare in corso in informatica, ingegneria, scienza dei materiali e fisica sar√† fondamentale per sbloccare il pieno potenziale di questa tecnologia per i casi d‚Äôuso dell‚Äôintelligenza artificiale.\n\n\n10.8.3 Calcolo Analogico\nIl computing analogico √® un approccio emergente che utilizza segnali e componenti analogici come condensatori, induttori e amplificatori anzich√© la logica digitale per il calcolo. Rappresenta le informazioni come segnali elettrici continui anzich√© 0 e 1 discreti. Ci√≤ consente al calcolo di riflettere direttamente la natura analogica dei dati del mondo reale, evitando errori di digitalizzazione e overhead.\nIl computing analogico ha generato un rinnovato interesse per l‚Äôhardware AI efficiente, in particolare per l‚Äôinferenza direttamente su dispositivi edge a basso consumo. I circuiti analogici, come la moltiplicazione e la sommatoria al centro delle reti neurali, possono essere utilizzati con un consumo energetico molto basso. Ci√≤ rende l‚Äôanalogico adatto per l‚Äôimplementazione di modelli ML su nodi finali con vincoli energetici. Startup come Mythic stanno sviluppando acceleratori AI analogici.\nMentre il computing analogico era popolare nei primi computer, il boom della logica digitale ha portato al suo declino. Tuttavia, l‚Äôanalogico √® convincente per applicazioni di nicchia che richiedono estrema efficienza (Haensch, Gokmen, e Puri 2019). Contrasta con gli approcci neuromorfici digitali che utilizzano ancora picchi digitali per il calcolo. L‚Äôanalogico pu√≤ consentire un calcolo di precisione inferiore, ma richiede competenza nella progettazione di circuiti analogici. I compromessi su precisione, complessit√† di programmazione e costi di fabbricazione rimangono aree di ricerca attive.\n\nHaensch, Wilfried, Tayfun Gokmen, e Ruchir Puri. 2019. ¬´The Next Generation of Deep Learning Hardware: Analog Computing¬ª. Proc. IEEE 107 (1): 108‚Äì22. https://doi.org/10.1109/jproc.2018.2871057.\n\nHazan, Avi, e Elishai Ezra Tsur. 2021. ¬´Neuromorphic Analog Implementation of Neural Engineering Framework-Inspired Spiking Neuron for High-Dimensional Representation¬ª. Front. Neurosci. 15 (febbraio): 627221. https://doi.org/10.3389/fnins.2021.627221.\nIl calcolo neuromorfico, che emula i sistemi neurali biologici per un‚Äôinferenza ML efficiente, pu√≤ utilizzare circuiti analogici per implementare i componenti e i comportamenti chiave del cervello. Ad esempio, i ricercatori hanno progettato circuiti analogici per modellare neuroni e sinapsi utilizzando condensatori, transistor e amplificatori operazionali (Hazan e Ezra Tsur 2021). I condensatori possono esibire le dinamiche di picco dei neuroni biologici, mentre gli amplificatori e i transistor forniscono una somma ponderata di input per imitare i dendriti. Le tecnologie a resistore variabile come i memristor possono realizzare sinapsi analogiche con plasticit√† dipendente dal tempo di picco, che pu√≤ rafforzare o indebolire le connessioni in base all‚Äôattivit√† di picco.\nStartup come SynSense hanno sviluppato chip neuromorfici analogici contenenti questi componenti biomimetici (Bains 2020). Questo approccio analogico si traduce in un basso consumo energetico e un‚Äôelevata scalabilit√† per i dispositivi edge rispetto alle complesse implementazioni SNN digitali.\n\nBains, Sunny. 2020. ¬´The business of building brains¬ª. Nature Electronics 3 (7): 348‚Äì51. https://doi.org/10.1038/s41928-020-0449-1.\nTuttavia, l‚Äôaddestramento di SNN analogiche sui chip rimane una sfida aperta. Nel complesso, la realizzazione analogica √® una tecnica promettente per fornire l‚Äôefficienza, la scalabilit√† e la plausibilit√† biologica previste con il calcolo neuromorfico. La fisica dei componenti analogici combinata con la progettazione dell‚Äôarchitettura neurale potrebbe migliorare l‚Äôefficienza dell‚Äôinferenza rispetto alle reti neurali digitali convenzionali.\n\n\n10.8.4 Elettronica Flessibile\nMentre gran parte della nuova tecnologia hardware nell‚Äôarea di lavoro ML si √® concentrata sull‚Äôottimizzazione e sulla creazione di sistemi pi√π efficienti, c‚Äô√® una traiettoria parallela che mira ad adattare l‚Äôhardware per applicazioni specifiche (Gates 2009; Musk et al. 2019; Tang et al. 2023; Tang, He, e Liu 2022; Kwon e Dong 2022). Una di queste strade √® lo sviluppo di elettronica flessibile per casi d‚Äôuso AI.\n\nGates, Byron D. 2009. ¬´Flexible Electronics¬ª. Science 323 (5921): 1566‚Äì67. https://doi.org/10.1126/science.1171230.\n\nTang, Xin, Hao Shen, Siyuan Zhao, Na Li, e Jia Liu. 2023. ¬´Flexible braincomputer interfaces¬ª. Nature Electronics 6 (2): 109‚Äì18. https://doi.org/10.1038/s41928-022-00913-9.\n\nTang, Xin, Yichun He, e Jia Liu. 2022. ¬´Soft bioelectronics for cardiac interfaces¬ª. Biophysics Reviews 3 (1). https://doi.org/10.1063/5.0069516.\nL‚Äôelettronica flessibile si riferisce a circuiti elettronici e dispositivi fabbricati su substrati flessibili in plastica o polimeri anzich√© in silicio rigido. A differenza delle schede e dei chip rigidi convenzionali, ci√≤ consente all‚Äôelettronica di piegarsi, torcersi e adattarsi a forme irregolari. Figura¬†10.9 mostra un esempio di un prototipo di dispositivo flessibile che misura in modalit√† wireless la temperatura corporea, che pu√≤ essere integrato senza soluzione di continuit√† in indumenti o cerotti cutanei. La flessibilit√† e la piegabilit√† dei materiali elettronici emergenti consentono di integrarli in fattori di forma sottili e leggeri, adatti per applicazioni AI e TinyML embedded.\nL‚Äôhardware AI flessibile pu√≤ adattarsi a superfici curve e funzionare in modo efficiente con budget di potenza in microwatt. La flessibilit√† consente inoltre fattori di forma arrotolabili o pieghevoli per ridurre al minimo l‚Äôingombro e il peso del dispositivo, ideali per piccoli dispositivi intelligenti portatili e dispositivi indossabili che incorporano TinyML. Un altro vantaggio fondamentale dell‚Äôelettronica flessibile rispetto alle tecnologie convenzionali sono i costi di produzione inferiori e i processi di fabbricazione pi√π semplici, che potrebbero democratizzare l‚Äôaccesso a queste tecnologie. Mentre le maschere in silicio e i costi di fabbricazione in genere costano milioni di dollari, l‚Äôhardware flessibile in genere costa solo decine di centesimi per la produzione (Huang et al. 2011; Biggs et al. 2021). Il potenziale di fabbricare elettronica flessibile direttamente su pellicole di plastica utilizzando processi di stampa e rivestimento ad alta produttivit√† pu√≤ ridurre i costi e migliorare la producibilit√† su larga scala rispetto ai chip AI rigidi (Musk et al. 2019).\n\nHuang, Tsung-Ching, Kenjiro Fukuda, Chun-Ming Lo, Yung-Hui Yeh, Tsuyoshi Sekitani, Takao Someya, e Kwang-Ting Cheng. 2011. ¬´Pseudo-CMOS: A Design Style for Low-Cost and Robust Flexible Electronics¬ª. IEEE Trans. Electron Devices 58 (1): 141‚Äì50. https://doi.org/10.1109/ted.2010.2088127.\n\nBiggs, John, James Myers, Jedrzej Kufel, Emre Ozer, Simon Craske, Antony Sou, Catherine Ramsdale, Ken Williamson, Richard Price, e Scott White. 2021. ¬´A natively flexible 32-bit Arm microprocessor¬ª. Nature 595 (7868): 532‚Äì36. https://doi.org/10.1038/s41586-021-03625-w.\n\n\n\n\n\n\nFigura¬†10.9: Prototipo di dispositivo flessibile. Fonte: Jabil Circuit.\n\n\n\nIl campo √® abilitato dai progressi nei semiconduttori organici e nei nanomateriali che possono essere depositati su pellicole sottili e flessibili. Tuttavia, la fabbricazione rimane impegnativa rispetto ai processi maturi del silicio. I circuiti flessibili attualmente presentano in genere prestazioni inferiori rispetto agli equivalenti rigidi. Tuttavia, promettono di trasformare l‚Äôelettronica in materiali leggeri e pieghevoli.\nI casi d‚Äôuso dell‚Äôelettronica flessibile sono adatti per l‚Äôintegrazione intima con il corpo umano. Le potenziali applicazioni dell‚Äôintelligenza artificiale medica includono sensori biointegrati, ‚Äúsoft robot‚Äù e impianti che monitorano o stimolano il sistema nervoso in modo intelligente. In particolare, gli array di elettrodi flessibili potrebbero consentire interfacce neurali a densit√† pi√π elevata e meno invasive rispetto agli equivalenti rigidi.\nPertanto, l‚Äôelettronica flessibile sta inaugurando una nuova era di dispositivi indossabili e sensori corporei, in gran parte grazie alle innovazioni nei transistor organici. Questi componenti consentono un‚Äôelettronica pi√π leggera e pieghevole, ideale per dispositivi indossabili, pelle elettronica e dispositivi medici che si adattano al corpo.\nSono adatti per dispositivi bioelettronici in termini di biocompatibilit√†, aprendo la strada ad applicazioni in interfacce cerebrali e cardiache. Ad esempio, la ricerca sulle interfacce flessibili cervello-computer e sulla bioelettronica morbida per applicazioni cardiache dimostra il potenziale per applicazioni mediche di vasta portata.\nAziende e istituti di ricerca non stanno solo sviluppando e investendo grandi quantit√† di risorse in elettrodi flessibili, come mostrato nel lavoro di Neuralink (Musk et al. 2019). Tuttavia, stanno anche spingendo i confini per integrare modelli di apprendimento automatico nei sistemi (Kwon e Dong 2022). Questi sensori intelligenti mirano a una simbiosi fluida e duratura con il corpo umano.\n\nMusk, Elon et al. 2019. ¬´An Integrated Brain-Machine Interface Platform With Thousands of Channels¬ª. J. Med. Internet Res. 21 (10): e16194. https://doi.org/10.2196/16194.\n\nKwon, Sun Hwa, e Lin Dong. 2022. ¬´Flexible sensors and machine learning for heart monitoring¬ª. Nano Energy 102 (novembre): 107632. https://doi.org/10.1016/j.nanoen.2022.107632.\n\nSegura Anaya, L. H., Abeer Alsadoon, N. Costadopoulos, e P. W. C. Prasad. 2017. ¬´Ethical Implications of User Perceptions of Wearable Devices¬ª. Sci. Eng. Ethics 24 (1): 1‚Äì28. https://doi.org/10.1007/s11948-017-9872-8.\n\nGoodyear, Victoria A. 2017. ¬´Social media, apps and wearable technologies: Navigating ethical dilemmas and procedures¬ª. Qualitative Research in Sport, Exercise and Health 9 (3): 285‚Äì302. https://doi.org/10.1080/2159676x.2017.1303790.\n\nFarah, Martha J. 2005. ¬´Neuroethics: The practical and the philosophical¬ª. Trends Cogn. Sci. 9 (1): 34‚Äì40. https://doi.org/10.1016/j.tics.2004.12.001.\n\nRoskies, Adina. 2002. ¬´Neuroethics for the New Millenium¬ª. Neuron 35 (1): 21‚Äì23. https://doi.org/10.1016/s0896-6273(02)00763-8.\nEticamente, l‚Äôincorporazione di sensori intelligenti basati sull‚Äôapprendimento automatico nel corpo solleva importanti questioni. Le problematiche relative alla privacy dei dati, al consenso informato e alle implicazioni sociali a lungo termine di tali tecnologie sono al centro del lavoro in corso in neuroetica e bioetica (Segura Anaya et al. 2017; Goodyear 2017; Farah 2005; Roskies 2002). Il campo sta progredendo a un ritmo che richiede progressi paralleli nei parametri etici per guidare lo sviluppo e l‚Äôimplementazione responsabili di queste tecnologie. Sebbene vi siano limitazioni e ostacoli etici da superare, le prospettive per l‚Äôelettronica flessibile sono ampie e promettono molto per la ricerca e le applicazioni future.\n\n\n10.8.5 Tecnologie delle Memorie\nLe tecnologie delle memorie sono fondamentali per l‚Äôhardware AI, ma la DDR DRAM e la SRAM convenzionali creano colli di bottiglia. I carichi di lavoro AI richiedono un‚Äôelevata larghezza di banda (&gt;1 TB/s). Le applicazioni scientifiche estreme dell‚ÄôAI richiedono una latenza estremamente bassa (&lt;50 ns) per alimentare i dati alle unit√† di calcolo (Duarte et al. 2022), un‚Äôelevata densit√† (&gt;128 Gb) per archiviare grandi parametri di modelli e set di dati e un‚Äôeccellente efficienza energetica (&lt;100 fJ/b) per uso embedded (Verma et al. 2019). Sono necessarie nuove memorie per soddisfare queste esigenze. Le opzioni emergenti includono diverse nuove tecnologie:\n\nDuarte, Javier, Nhan Tran, Ben Hawks, Christian Herwig, Jules Muhizi, Shvetank Prakash, e Vijay Janapa Reddi. 2022. ¬´FastML Science Benchmarks: Accelerating Real-Time Scientific Edge Machine Learning¬ª. ArXiv preprint abs/2207.07958. https://arxiv.org/abs/2207.07958.\n\nVerma, Naveen, Hongyang Jia, Hossein Valavi, Yinqi Tang, Murat Ozatay, Lung-Yen Chen, Bonan Zhang, e Peter Deaville. 2019. ¬´In-Memory Computing: Advances and Prospects¬ª. IEEE Solid-State Circuits Mag. 11 (3): 43‚Äì55. https://doi.org/10.1109/mssc.2019.2922889.\n\nLa RAM resistiva (ReRAM) pu√≤ migliorare la densit√† con semplici array passivi. Tuttavia, permangono dei problemi legati alla variabilit√† (Chi et al. 2016).\nLa ‚ÄúPhase change memory (PCM)‚Äù [memoria a cambiamento di fase ] sfrutta le propriet√† uniche del vetro calcogenuro. Le fasi cristalline e amorfe hanno resistenze diverse. L‚ÄôOptane DCPMM di Intel fornisce PCM veloci (100 ns) e ad alta resistenza. Tuttavia, le sfide includono cicli di scrittura limitati e corrente di reset elevata (Burr et al. 2016).\nLo stacking 3D pu√≤ anche aumentare la densit√† di memoria e la larghezza di banda integrando verticalmente strati di memoria con interconnessioni TSV (Loh 2008). Ad esempio, HBM fornisce interfacce larghe 1024 bit.\n\n\nBurr, Geoffrey W., Matthew J. BrightSky, Abu Sebastian, Huai-Yu Cheng, Jau-Yi Wu, Sangbum Kim, Norma E. Sosa, et al. 2016. ¬´Recent Progress in Phase-Change?Pub _newline ?Memory Technology¬ª. IEEE Journal on Emerging and Selected Topics in Circuits and Systems 6 (2): 146‚Äì62. https://doi.org/10.1109/jetcas.2016.2547718.\n\nLoh, Gabriel H. 2008. ¬´3D-Stacked Memory Architectures for Multi-core Processors¬ª. ACM SIGARCH Computer Architecture News 36 (3): 453‚Äì64. https://doi.org/10.1145/1394608.1382159.\nLe nuove tecnologie di memoria, con le loro innovative architetture e materiali cellulari, sono fondamentali per sbloccare il prossimo livello di prestazioni ed efficienza hardware AI. Realizzare i loro vantaggi nei sistemi commerciali rimane una sfida continua.\nL‚Äôelaborazione in-memory sta guadagnando terreno come promettente strada per ottimizzare l‚Äôapprendimento automatico e i carichi di lavoro di elaborazione ad alte prestazioni. Al centro, la tecnologia colloca l‚Äôarchiviazione e l‚Äôelaborazione dei dati per migliorare l‚Äôefficienza energetica e ridurre la latenza Wong et al. (2012). Due tecnologie chiave sotto questo ombrello sono la ‚ÄúResistive RAM (ReRAM)‚Äù e il ‚ÄúProcessing-In-Memory (PIM)‚Äù.\n\nWong, H.-S. Philip, Heng-Yuan Lee, Shimeng Yu, Yu-Sheng Chen, Yi Wu, Pang-Shiu Chen, Byoungil Lee, Frederick T. Chen, e Ming-Jinn Tsai. 2012. ¬´MetalOxide RRAM¬ª. Proc. IEEE 100 (6): 1951‚Äì70. https://doi.org/10.1109/jproc.2012.2190369.\n\nChi, Ping, Shuangchen Li, Cong Xu, Tao Zhang, Jishen Zhao, Yongpan Liu, Yu Wang, e Yuan Xie. 2016. ¬´Prime: a novel processing-in-memory architecture for neural network computation in ReRAM-based main memory¬ª. ACM SIGARCH Computer Architecture News 44 (3): 27‚Äì39. https://doi.org/10.1145/3007787.3001140.\nReRAM (Wong et al. 2012) e PIM (Chi et al. 2016) sono le colonne portanti per l‚Äôelaborazione in memoria, l‚Äôarchiviazione e l‚Äôelaborazione dei dati nella stessa posizione. ReRAM si concentra su questioni di uniformit√†, resistenza, conservazione, funzionamento multi-bit e scalabilit√†. D‚Äôaltro canto, PIM coinvolge unit√† CPU integrate direttamente in array di memoria, specializzate per attivit√† come la moltiplicazione di matrici, che sono centrali nei calcoli AI.\nQueste tecnologie trovano applicazioni nei carichi di lavoro AI e nell‚Äôelaborazione ad alte prestazioni, dove la sinergia di storage e calcolo pu√≤ portare a significativi guadagni in termini di prestazioni. L‚Äôarchitettura √® particolarmente utile per le attivit√† di elaborazione intensiva comuni nei modelli di apprendimento automatico.\nMentre le tecnologie di elaborazione in memoria come ReRAM e PIM offrono interessanti prospettive di efficienza e prestazioni, presentano le loro sfide, come l‚Äôuniformit√† dei dati e i problemi di scalabilit√† in ReRAM (Imani, Rahimi, e S. Rosing 2016). Tuttavia, il campo √® maturo per l‚Äôinnovazione e affrontare queste limitazioni pu√≤ aprire nuove frontiere nell‚ÄôAI e nell‚Äôelaborazione ad alte prestazioni.\n\nImani, Mohsen, Abbas Rahimi, e Tajana S. Rosing. 2016. ¬´Resistive Configurable Associative Memory for Approximate Computing¬ª. In Proceedings of the 2016 Design, Automation &amp; Test in Europe Conference &amp; Exhibition (DATE), 1327‚Äì32. IEEE; Research Publishing Services. https://doi.org/10.3850/9783981537079_0454.\n\n\n10.8.6 Calcolo Ottico\nNell‚Äôaccelerazione dell‚Äôintelligenza artificiale, un‚Äôarea di interesse in rapida crescita risiede nelle nuove tecnologie che si discostano dai paradigmi tradizionali. Alcune tecnologie emergenti menzionate sopra, come l‚Äôelettronica flessibile, il calcolo in memoria o persino il calcolo neuromorfico, stanno per diventare realt√†, date le loro innovazioni e applicazioni rivoluzionarie. Una delle frontiere promettenti e all‚Äôavanguardia della prossima generazione √® la tecnologia del calcolo ottico H. Zhou et al. (2022). Aziende come LightMatter sono pioniere nell‚Äôuso della fotonica luminosa per i calcoli, utilizzando quindi fotoni al posto degli elettroni per la trasmissione dei dati e l‚Äôelaborazione.\n\nZhou, Hailong, Jianji Dong, Junwei Cheng, Wenchan Dong, Chaoran Huang, Yichen Shen, Qiming Zhang, et al. 2022. ¬´Photonic matrix multiplication lights up photonic accelerator and beyond¬ª. Light: Science &amp; Applications 11 (1): 30. https://doi.org/10.1038/s41377-022-00717-8.\n\nShastri, Bhavin J., Alexander N. Tait, T. Ferreira de Lima, Wolfram H. P. Pernice, Harish Bhaskaran, C. D. Wright, e Paul R. Prucnal. 2021. ¬´Photonics for artificial intelligence and neuromorphic computing¬ª. Nat. Photonics 15 (2): 102‚Äì14. https://doi.org/10.1038/s41566-020-00754-y.\nIl calcolo ottico utilizza fotoni e dispositivi fotonici anzich√© i tradizionali circuiti elettronici per il calcolo e l‚Äôelaborazione dei dati. Trae ispirazione dai collegamenti di comunicazione in fibra ottica che si basano sulla luce per un trasferimento dati rapido ed efficiente (Shastri et al. 2021). La luce pu√≤ propagarsi con una perdita molto inferiore rispetto agli elettroni dei semiconduttori, consentendo vantaggi intrinseci in termini di velocit√† ed efficienza.\nAlcuni vantaggi specifici dell‚Äôelaborazione ottica includono:\n\nAlta produttivit√†: I fotoni possono trasmettere con larghezze di banda &gt;100 Tb/s utilizzando il multiplexing a divisione di lunghezza d‚Äôonda.\nBassa latenza: I fotoni interagiscono su scale temporali di femtosecondi, milioni di volte pi√π velocemente dei transistor al silicio.\nParallelismo: Pi√π segnali di dati possono propagarsi simultaneamente attraverso lo stesso mezzo ottico.\nBassa potenza: I circuiti fotonici che utilizzano guide d‚Äôonda e risonatori possono ottenere una logica e una memoria complesse con solo microwatt di potenza.\n\nTuttavia, l‚Äôelaborazione ottica deve attualmente affrontare sfide significative:\n\nMancanza di memoria ottica equivalente alla RAM elettronica\nRichiede la conversione tra domini ottici ed elettrici.\nSet limitato di componenti ottici disponibili rispetto al ricco ecosistema elettronico.\nMetodi di integrazione immaturi per combinare la fotonica con i tradizionali chip CMOS.\nModelli di programmazione complessi richiesti per gestire il parallelismo.\n\nDi conseguenza, l‚Äôelaborazione ottica √® ancora in una fase di ricerca molto precoce nonostante il suo potenziale promettente. Tuttavia, le innovazioni tecniche potrebbero consentirgli di integrare l‚Äôelettronica e sbloccare guadagni di prestazioni per i carichi di lavoro AI. Aziende come Lightmatter sono pioniere nei primi acceleratori ottici AI. A lungo termine, se le sfide chiave saranno superate, potrebbe rappresentare un substrato di elaborazione rivoluzionario.\n\n\n10.8.7 Quantum Computing\nI computer quantistici sfruttano fenomeni unici della fisica quantistica, come la sovrapposizione e l‚Äôentanglement, per rappresentare ed elaborare informazioni in modi non possibili in modo classico. Invece dei bit binari, l‚Äôunit√† fondamentale √® il bit quantistico o qubit. A differenza dei bit classici, che sono limitati a 0 o 1, i qubit possono esistere simultaneamente in una sovrapposizione di entrambi gli stati a causa degli effetti quantistici.\nAnche pi√π qubit possono essere entangled, portando a una densit√† di informazioni esponenziale ma introducendo risultati probabilistici. La sovrapposizione consente il calcolo parallelo su tutti gli stati possibili, mentre l‚Äôentanglement consente correlazioni non locali tra qubit. Figura¬†10.10 trasmette visivamente le differenze tra i bit classici nell‚Äôinformatica e i bit quantistici (qbit).\n\n\n\n\n\n\nFigura¬†10.10: Qubit, i mattoni del calcolo quantistico. Fonte: Microsoft\n\n\n\nGli algoritmi quantistici manipolano attentamente questi effetti meccanici quantistici intrinseci per risolvere problemi come l‚Äôottimizzazione o la ricerca in modo pi√π efficiente rispetto alle loro controparti classiche in teoria.\n\nTraining pi√π rapido di reti neurali profonde sfruttando il parallelismo quantistico per operazioni di algebra lineare.\nGli algoritmi ML quantistici efficienti sfruttano le capacit√† uniche dei qubit.\nReti neurali quantistiche con effetti quantistici intrinseci integrati nell‚Äôarchitettura del modello.\nOttimizzatori quantistici che sfruttano algoritmi di ‚Äúannealing‚Äù quantistica o adiabatici per problemi di ottimizzazione combinatoria.\n\nTuttavia, gli stati quantistici sono fragili e soggetti a errori che richiedono protocolli di correzione degli errori. La natura non intuitiva della programmazione quantistica introduce anche sfide non presenti nell‚Äôinformatica classica.\n\nI bit quantistici rumorosi e fragili sono difficili da scalare. Il pi√π grande computer quantistico odierno ha meno di 1000 qubit.\nInsieme limitato di porte e circuiti quantistici disponibili rispetto alla programmazione classica.\nMancanza di set di dati e benchmark per valutare l‚Äôapprendimento automatico quantistico in domini pratici.\n\nSebbene un vantaggio quantistico significativo per l‚Äôapprendimento automatico sia ancora lontano, la ricerca attiva presso aziende come D-Wave, Rigetti e IonQ sta facendo progredire l‚Äôingegneria informatica quantistica e gli algoritmi quantistici. Le principali aziende tecnologiche come Google, IBM e Microsoft stanno esplorando attivamente l‚Äôinformatica quantistica. Google ha recentemente annunciato un processore quantistico a 72 qubit chiamato Bristlecone e prevede di costruire un sistema quantistico commerciale a 49 qubit. Microsoft ha anche un programma di ricerca attivo nell‚Äôinformatica quantistica topologica e collabora con la startup quantistica IonQ\nLe tecniche quantistiche potrebbero prima fare breccia nell‚Äôottimizzazione prima di un‚Äôadozione pi√π generalizzata dell‚Äôapprendimento automatico. La realizzazione del pieno potenziale dell‚Äôapprendimento automatico quantistico attende importanti traguardi nello sviluppo dell‚Äôhardware quantistico e nella maturit√† dell‚Äôecosistema. Figura¬†10.11 confronta a titolo esemplificativo l‚Äôinformatica quantistica e quella classica.\n\n\n\n\n\n\nFigura¬†10.11: Confronto tra il calcolo quantistico e il calcolo classico. Fonte: Devopedia",
    "crumbs": [
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Accelerazione IA</span>"
    ]
  },
  {
    "objectID": "contents/core/hw_acceleration/hw_acceleration.it.html#tendenze-future",
    "href": "contents/core/hw_acceleration/hw_acceleration.it.html#tendenze-future",
    "title": "10¬† Accelerazione IA",
    "section": "10.9 Tendenze Future",
    "text": "10.9 Tendenze Future\nIn questo capitolo, l‚Äôattenzione principale √® stata rivolta alla progettazione di hardware specializzato ottimizzato per carichi di lavoro e algoritmi di machine learning. Questa discussione ha riguardato le architetture personalizzate di GPU e TPU per l‚Äôaddestramento e l‚Äôinferenza delle reti neurali. Tuttavia, una direzione di ricerca emergente sta sfruttando l‚Äôapprendimento automatico per facilitare il processo di progettazione hardware stesso.\nIl processo di progettazione hardware comprende molte fasi complesse, tra cui specifica, modellazione di alto livello, simulazione, sintesi, verifica, prototipazione e fabbricazione. Gran parte di questo processo richiede tradizionalmente una vasta competenza umana, impegno e tempo. Tuttavia, i recenti progressi nell‚Äôapprendimento automatico stanno consentendo l‚Äôautomazione e il miglioramento di parti del flusso di lavoro di progettazione hardware utilizzando tecniche di apprendimento automatico.\nEcco alcuni esempi di come l‚Äôapprendimento automatico sta trasformando la progettazione hardware:\n\nSintesi di circuiti automatizzata tramite apprendimento per rinforzo: Anzich√© realizzare manualmente progetti a livello di transistor, gli agenti di apprendimento automatico come l‚Äôapprendimento per rinforzo possono imparare a collegare porte logiche e generare automaticamente layout di circuiti. Ci√≤ pu√≤ accelerare il lungo processo di sintesi.\nSimulazione ed emulazione hardware basate su ML: I modelli di reti neurali profonde possono essere addestrati per prevedere come si comporter√† un progetto hardware in diverse condizioni. Ad esempio, i modelli di apprendimento profondo possono essere addestrati per prevedere i conteggi dei cicli per determinati carichi di lavoro. Ci√≤ consente una simulazione pi√π rapida e accurata rispetto alle simulazioni RTL tradizionali.\nPianificazione automatizzata dei chip mediante algoritmi ML: La pianificazione dei chip comporta il posizionamento ottimale di diversi componenti su un die. Algoritmi evolutivi come quelli genetici e altri algoritmi ML come l‚Äôapprendimento per rinforzo vengono utilizzati per esplorare le opzioni di pianificazione. Ci√≤ pu√≤ migliorare significativamente i posizionamenti manuali di pianificazione in termini di tempi di consegna pi√π rapidi e qualit√† dei posizionamenti.\nOttimizzazione dell‚Äôarchitettura basata su ML: Le nuove architetture hardware, come quelle per gli acceleratori ML efficienti, possono essere generate e ottimizzate automaticamente tramite la ricerca nello spazio di progettazione architettonica. Gli algoritmi di apprendimento automatico possono cercare efficacemente ampi spazi di progettazione architettonica.\n\nL‚Äôapplicazione del ML all‚Äôautomazione della progettazione hardware promette di rendere il processo pi√π veloce, pi√π economico e pi√π efficiente. Apre possibilit√† di progettazione che richiederebbero pi√π di una progettazione manuale. L‚Äôuso del ML nella progettazione hardware √® un‚Äôarea di ricerca attiva e di distribuzione precoce, e studieremo le tecniche coinvolte e il loro potenziale trasformativo.\n\n10.9.1 ML per l‚Äôautomazione della progettazione hardware\nUna grande opportunit√† per l‚Äôapprendimento automatico nella progettazione hardware √® l‚Äôautomazione di parti del complesso e noioso flusso di lavoro di progettazione. Con ‚ÄúHardware design automation (HDA)‚Äù ci si riferisce in generale all‚Äôuso di tecniche ML come l‚Äôapprendimento per rinforzo, algoritmi genetici e reti neurali per automatizzare attivit√† come sintesi, verifica, floorplanning e altro. Ecco alcuni esempi di dove l‚ÄôML per HDA mostra una vera promessa:\n\nSintesi di circuiti automatizzata: La sintesi di circuiti comporta la conversione di una descrizione di alto livello della logica desiderata in un‚Äôimplementazione di netlist a livello di gate ottimizzata. Questo processo complesso ha molte considerazioni e compromessi di progettazione. Gli agenti ML possono essere addestrati tramite l‚Äôapprendimento per rinforzo G. Zhou e Anderson (2023) per esplorare lo spazio di progettazione e produrre automaticamente sintesi ottimizzate. Startup come Symbiotic EDA stanno portando questa tecnologia sul mercato.\nAutomated chip floorplanning: Il Floorplanning si riferisce al posizionamento strategico di diversi componenti su un‚Äôarea del chip. Algoritmi di ricerca come algoritmi genetici (Valenzuela e Wang 2000) e apprendimento per rinforzo (Mirhoseini et al. (2021), Agnesina et al. (2023)) possono essere utilizzati per automatizzare l‚Äôottimizzazione il floorplan per ridurre al minimo la lunghezza dei collegamenti, il consumo di energia e altri obiettivi. Questi ‚Äúfloor planners‚Äù assistiti da ML automatizzati sono estremamente preziosi man mano che aumenta la complessit√† dei chip.\nSimulatori hardware ML: L‚Äôaddestramento di modelli di reti neurali profonde per prevedere le prestazioni dei progetti hardware, poich√© i simulatori possono accelerare il processo di simulazione di oltre 100 volte rispetto alle simulazioni architettoniche e RTL tradizionali.\nTraduzione automatica del codice: La conversione di linguaggi di descrizione hardware come Verilog in implementazioni RTL ottimizzate √® fondamentale ma richiede molto tempo. I modelli ML possono essere addestrati per agire come agenti traduttori e automatizzare questo processo.\n\n\nZhou, Guanglei, e Jason H. Anderson. 2023. ¬´Area-Driven FPGA Logic Synthesis Using Reinforcement Learning¬ª. In Proceedings of the 28th Asia and South Pacific Design Automation Conference, 159‚Äì65. ACM. https://doi.org/10.1145/3566097.3567894.\n\nValenzuela, Christine L, e Pearl Y Wang. 2000. ¬´A genetic algorithm for VLSI floorplanning¬ª. In Parallel Problem Solving from Nature PPSN VI: 6th International Conference Paris, France, September 1820, 2000 Proceedings 6, 671‚Äì80. Springer.\n\nMirhoseini, Azalia, Anna Goldie, Mustafa Yazgan, Joe Wenjie Jiang, Ebrahim Songhori, Shen Wang, Young-Joon Lee, et al. 2021. ¬´A graph placement methodology for fast chip design¬ª. Nature 594 (7862): 207‚Äì12. https://doi.org/10.1038/s41586-021-03544-w.\n\nAgnesina, Anthony, Puranjay Rajvanshi, Tian Yang, Geraldo Pradipta, Austin Jiao, Ben Keller, Brucek Khailany, e Haoxing Ren. 2023. ¬´AutoDMP: Automated DREAMPlace-based Macro Placement¬ª. In Proceedings of the 2023 International Symposium on Physical Design, 149‚Äì57. ACM. https://doi.org/10.1145/3569052.3578923.\nI vantaggi dell‚ÄôHDA che utilizza ML sono tempi di progettazione ridotti, ottimizzazioni superiori ed esplorazione di spazi di progettazione troppo complessi per approcci manuali. Ci√≤ pu√≤ accelerare lo sviluppo hardware e portare a progetti migliori.\nLe sfide includono i limiti della generalizzazione ML, la natura black-box di alcune tecniche e compromessi sull‚Äôaccuratezza. Tuttavia, la ricerca sta rapidamente avanzando per affrontare questi problemi e rendere le soluzioni HDA ML robuste e affidabili per l‚Äôuso in produzione. HDA fornisce un‚Äôimportante via per ML per trasformare la progettazione hardware.\n\n\n10.9.2 Simulazione e Verifica Hardware Basate su ML\nLa simulazione e la verifica dei progetti hardware sono fondamentali prima della produzione per garantire che il progetto si comporti come previsto. Gli approcci tradizionali come la simulazione ‚Äúregister-transfer level‚Äù (RTL) sono complessi e richiedono molto tempo. Il ML introduce nuove opportunit√† per migliorare la simulazione e la verifica dell‚Äôhardware. Ecco alcuni esempi:\n\nModellazione surrogata per la simulazione: Modelli surrogati di un progetto altamente accurati possono essere creati utilizzando reti neurali. Questi modelli prevedono gli output dagli input molto pi√π velocemente della simulazione RTL, consentendo una rapida esplorazione dello spazio di progettazione. Aziende come Ansys utilizzano questa tecnica.\nSimulatori ML: Grandi modelli di reti neurali possono essere addestrati su simulazioni RTL per imparare a imitare la funzionalit√† di un progetto hardware. Una volta addestrato, il modello NN pu√≤ essere un simulatore altamente efficiente per test di regressione e altre attivit√†. Graphcore ha dimostrato un‚Äôaccelerazione di oltre 100 volte con questo approccio.\nVerifica formale tramite ML: La verifica formale dimostra matematicamente le propriet√† di un progetto. Le tecniche di ML possono aiutare a generare propriet√† di verifica e imparare a risolvere le complesse prove formali necessarie, automatizzando parti di questo processo impegnativo. Startup come Cortical.io stanno introducendo sul mercato soluzioni di verifica ML formali.\nRilevamento di bug: I modelli ML possono essere addestrati per elaborare progetti hardware e identificare potenziali problemi. Ci√≤ aiuta i progettisti umani a ispezionare progetti complessi e a trovare bug. Facebook ha mostrato modelli di rilevamento di bug per l‚Äôhardware dei suoi server.\n\nI principali vantaggi dell‚Äôapplicazione di ML alla simulazione e alla verifica sono tempi di esecuzione pi√π rapidi per la convalida del progetto, test pi√π rigorosi e riduzione del lavoro umano. Le sfide includono la verifica della correttezza del modello ML e la gestione dei casi limite. ML promette di accelerare significativamente i flussi di lavoro di test.\n\n\n10.9.3 ML per Architetture Hardware Efficienti\nUn obiettivo chiave √® la progettazione di architetture hardware ottimizzate per prestazioni, potenza ed efficienza. ML introduce nuove tecniche per automatizzare e migliorare l‚Äôesplorazione dello spazio di progettazione dell‚Äôarchitettura per hardware generico e specializzato come gli acceleratori ML. Alcuni esempi promettenti sono:\n\nRicerca di architetture per hardware: Tecniche di ricerca come algoritmi evolutivi (Kao e Krishna 2020), ottimizzazione bayesiana (Reagen et al. (2017), Bhardwaj et al. (2020)), apprendimento per rinforzo (Kao, Jeong, e Krishna (2020), Krishnan et al. (2022)) possono generare automaticamente nuove architetture hardware mutando e mescolando attributi di progettazione come dimensione della cache, numero di unit√† parallele, larghezza di banda della memoria e cos√¨ via. Ci√≤ consente un‚Äôesplorazione efficiente di ampi spazi di progettazione.\nModellazione predittiva per l‚Äôottimizzazione: I modelli ML possono essere addestrati per prevedere metriche di prestazioni, potenza ed efficienza hardware per una determinata architettura. Questi diventano ‚Äúmodelli surrogati‚Äù (Krishnan et al. 2023) per una rapida ottimizzazione ed esplorazione dello spazio sostituendo lunghe simulazioni.\nOttimizzazione dell‚Äôacceleratore specializzato: Per chip specializzati come unit√† di elaborazione tensore per AI, tecniche di ricerca architettura automatizzata basate su algoritmi ML (D. Zhang et al. 2022) promettono di trovare progetti rapidi ed efficienti.\n\n\nKao, Sheng-Chun, e Tushar Krishna. 2020. ¬´Gamma: automating the HW mapping of DNN models on accelerators via genetic algorithm¬ª. In Proceedings of the 39th International Conference on Computer-Aided Design, 1‚Äì9. ACM. https://doi.org/10.1145/3400302.3415639.\n\nReagen, Brandon, Jose Miguel Hernandez-Lobato, Robert Adolf, Michael Gelbart, Paul Whatmough, Gu-Yeon Wei, e David Brooks. 2017. ¬´A case for efficient accelerator design space exploration via Bayesian optimization¬ª. In 2017 IEEE/ACM International Symposium on Low Power Electronics and Design (ISLPED), 1‚Äì6. IEEE; IEEE. https://doi.org/10.1109/islped.2017.8009208.\n\nBhardwaj, Kshitij, Marton Havasi, Yuan Yao, David M. Brooks, Jos√© Miguel Hern√°ndez-Lobato, e Gu-Yeon Wei. 2020. ¬´A comprehensive methodology to determine optimal coherence interfaces for many-accelerator SoCs¬ª. In Proceedings of the ACM/IEEE International Symposium on Low Power Electronics and Design, 145‚Äì50. ACM. https://doi.org/10.1145/3370748.3406564.\n\nKao, Sheng-Chun, Geonhwa Jeong, e Tushar Krishna. 2020. ¬´ConfuciuX: Autonomous Hardware Resource Assignment for DNN Accelerators using Reinforcement Learning¬ª. In 2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO), 622‚Äì36. IEEE; IEEE. https://doi.org/10.1109/micro50266.2020.00058.\n\nKrishnan, Srivatsan, Natasha Jaques, Shayegan Omidshafiei, Dan Zhang, Izzeddin Gur, Vijay Janapa Reddi, e Aleksandra Faust. 2022. ¬´Multi-Agent Reinforcement Learning for Microprocessor Design Space Exploration¬ª. https://arxiv.org/abs/2211.16385.\n\nZhang, Dan, Safeen Huda, Ebrahim Songhori, Kartik Prabhu, Quoc Le, Anna Goldie, e Azalia Mirhoseini. 2022. ¬´A full-stack search technique for domain optimized deep learning accelerators¬ª. In Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, 27‚Äì42. ASPLOS ‚Äô22. New York, NY, USA: ACM. https://doi.org/10.1145/3503222.3507767.\nI vantaggi dell‚Äôutilizzo di ML includono un‚Äôesplorazione dello spazio di progettazione superiore, ottimizzazione automatizzata e riduzione dello sforzo manuale. Le sfide includono lunghi tempi di training per alcune tecniche e limitazioni degli ottimi locali. Tuttavia, ML per l‚Äôarchitettura hardware ha un grande potenziale per rivelare miglioramenti in termini di prestazioni ed efficienza.\n\n\n10.9.4 ML per Ottimizzare la Produzione e Ridurre i Difetti\nUna volta completata la progettazione hardware, si passa alla produzione. Tuttavia, variabilit√† e difetti durante la produzione possono influire su rese e qualit√†. Le tecniche ML vengono ora applicate per migliorare i processi di fabbricazione e ridurre i difetti. Ecco alcuni esempi:\n\nManutenzione predittiva: I modelli ML possono analizzare i dati dei sensori delle apparecchiature nel tempo e identificare segnali che prevedono le esigenze di manutenzione prima del guasto. Ci√≤ consente una manutenzione proattiva, che pu√≤ essere molto utile nel costoso processo di fabbricazione.\nOttimizzazione del processo: I modelli di apprendimento supervisionato possono essere addestrati sui dati di processo per identificare i fattori che portano a basse rese. I modelli possono quindi ottimizzare i parametri per migliorare rese, produttivit√† o coerenza.\nPrevisione della resa: Analizzando i dati di prova da progetti realizzati utilizzando tecniche come alberi di regressione, i modelli ML possono prevedere le rese all‚Äôinizio della produzione, consentendo aggiustamenti del processo.\nRilevamento dei difetti: Le tecniche di visione artificiale ML possono essere applicate alle immagini dei progetti per identificare difetti invisibili all‚Äôocchio umano. Ci√≤ consente un controllo di qualit√† di precisione e un‚Äôanalisi delle cause principali.\nAnalisi proattiva dei guasti: I modelli ML possono aiutare a prevedere, diagnosticare e prevenire i problemi che portano a difetti e guasti a valle analizzando i dati di processo strutturati e non strutturati.\n\nL‚Äôapplicazione del ML alla produzione consente l‚Äôottimizzazione dei processi, il controllo di qualit√† in tempo reale, la manutenzione predittiva e rese pi√π elevate. Le sfide includono la gestione di dati di produzione complessi e varianti. Ma il ML √® pronto a trasformare la produzione di semiconduttori.\n\n\n10.9.5 Verso Modelli di Base per la Progettazione Hardware\nCome abbiamo visto, l‚Äôapprendimento automatico sta aprendo nuove possibilit√† nel flusso di lavoro di progettazione hardware, dalle specifiche alla produzione. Tuttavia, le attuali tecniche di ML hanno ancora una portata limitata e richiedono un‚Äôampia progettazione specifica per dominio. La visione a lungo termine √® lo sviluppo di sistemi di intelligenza artificiale generali che possono essere applicati con versatilit√† in tutte le attivit√† di progettazione hardware.\nPer realizzare appieno questa visione, sono necessari investimenti e ricerca per sviluppare modelli di base per la progettazione hardware. Si tratta di modelli e architetture ML unificati e generici che possono apprendere complesse competenze di progettazione hardware con i dati di training e gli obiettivi corretti.\nLa realizzazione di modelli di base per la progettazione hardware end-to-end richieder√† quanto segue:\n\nAccumulare grandi set di dati di alta qualit√† ed etichettati in tutte le fasi di progettazione hardware per addestrare i modelli di base.\nProgressi nelle tecniche ML multimodali e multi-task per gestire la diversit√† di dati e attivit√† di progettazione hardware.\nInterfacce e layer di astrazione per collegare i modelli di base ai flussi e agli strumenti di progettazione esistenti.\nSviluppo di ambienti di simulazione e benchmark per addestrare e testare i modelli di base sulle capacit√† di progettazione hardware.\nMetodi per spiegare e interpretare le decisioni di progettazione dei modelli ML e le ottimizzazioni per attendibilit√† e verifica.\nTecniche di compilazione per ottimizzare i modelli di base per un‚Äôimplementazione efficiente su piattaforme hardware.\n\nSebbene siano ancora in corso ricerche significative, i modelli di base rappresentano l‚Äôobiettivo a lungo termine pi√π trasformativo per l‚Äôinfusione dell‚ÄôIA nel processo della progettazione hardware. Democratizzare la progettazione hardware tramite sistemi ML versatili e automatizzati promette di aprire una nuova era di progettazione di chip ottimizzata, efficiente e innovativa. Il viaggio che ci attende √® pieno di sfide e opportunit√† aperte.\nSe sei interessato alla progettazione di architetture per computer assistite da ML (Krishnan et al. 2023), invitiamo a leggere Architecture 2.0.\n\nKrishnan, Srivatsan, Amir Yazdanbakhsh, Shvetank Prakash, Jason Jabbour, Ikechukwu Uchendu, Susobhan Ghosh, Behzad Boroujerdian, et al. 2023. ¬´ArchGym: An Open-Source Gymnasium for Machine Learning Assisted Architecture Design¬ª. In Proceedings of the 50th Annual International Symposium on Computer Architecture, 1‚Äì16. ACM. https://doi.org/10.1145/3579371.3589049.\nIn alternativa, si pu√≤ guardare Video¬†10.3 for more details.\n\n\n\n\n\n\nVideo¬†10.3: Architecture 2.0",
    "crumbs": [
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Accelerazione IA</span>"
    ]
  },
  {
    "objectID": "contents/core/hw_acceleration/hw_acceleration.it.html#conclusione",
    "href": "contents/core/hw_acceleration/hw_acceleration.it.html#conclusione",
    "title": "10¬† Accelerazione IA",
    "section": "10.10 Conclusione",
    "text": "10.10 Conclusione\nL‚Äôaccelerazione hardware specializzata √® diventata indispensabile per abilitare applicazioni di intelligenza artificiale performanti ed efficienti, poich√© modelli e set di dati esplodono in complessit√†. Questo capitolo ha esaminato i limiti dei processori generici come le CPU per i carichi di lavoro di intelligenza artificiale. La loro mancanza di parallelismo e di throughput computazionale non consente di addestrare o eseguire rapidamente reti neurali profonde all‚Äôavanguardia. Queste motivazioni hanno guidato le innovazioni negli acceleratori personalizzati.\nAbbiamo esaminato GPU, TPU, FPGA e ASIC progettati specificamente per le operazioni matematiche intensive inerenti alle reti neurali. Coprendo questo spettro di opzioni, abbiamo mirato a fornire un framework per ragionare attraverso la selezione dell‚Äôacceleratore in base a vincoli relativi a flessibilit√†, prestazioni, potenza, costi e altri fattori.\nAbbiamo anche esplorato il ruolo del software nell‚Äôabilitazione e nell‚Äôottimizzazione attive dell‚Äôaccelerazione dell‚Äôintelligenza artificiale. Ci√≤ abbraccia astrazioni di programmazione, framework, compilatori e simulatori. Abbiamo discusso della progettazione congiunta hardware-software come metodologia proattiva per la creazione di sistemi di intelligenza artificiale pi√π olistici integrando strettamente l‚Äôinnovazione degli algoritmi e i progressi hardware.\nMa c‚Äô√® molto di pi√π in arrivo! Frontiere entusiasmanti come l‚Äôinformatica analogica, le reti neurali ottiche e l‚Äôapprendimento automatico quantistico rappresentano direzioni di ricerca attive che potrebbero sbloccare miglioramenti di ordini di grandezza in termini di efficienza, velocit√† e scala rispetto ai paradigmi attuali.\nIn definitiva, l‚Äôaccelerazione hardware specializzata rimane indispensabile per sbloccare le prestazioni e l‚Äôefficienza necessarie per soddisfare la promessa dell‚Äôintelligenza artificiale dal cloud all‚Äôedge. Ci auguriamo che questo capitolo fornisca utili informazioni di base e approfondimenti sulla rapida innovazione che si sta verificando in questo dominio.",
    "crumbs": [
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Accelerazione IA</span>"
    ]
  },
  {
    "objectID": "contents/core/hw_acceleration/hw_acceleration.it.html#sec-ai-acceleration-resource",
    "href": "contents/core/hw_acceleration/hw_acceleration.it.html#sec-ai-acceleration-resource",
    "title": "10¬† Accelerazione IA",
    "section": "10.11 Risorse",
    "text": "10.11 Risorse\nEcco un elenco curato di risorse per supportare studenti e insegnanti nei loro percorsi di apprendimento e insegnamento. Lavoriamo continuamente per espandere questa raccolta e presto aggiungeremo nuovi esercizi.\n\n\n\n\n\n\nSlide\n\n\n\n\n\n\nProssimamente.\n\n\n\n\n\n\n\n\n\n\nVideo\n\n\n\n\n\n\nVideo¬†10.1\nVideo¬†10.2\nVideo¬†10.3\n\n\n\n\n\n\n\n\n\n\nEsercizi\n\n\n\n\n\n\nEsercizio¬†10.1",
    "crumbs": [
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Accelerazione IA</span>"
    ]
  },
  {
    "objectID": "contents/core/benchmarking/benchmarking.it.html",
    "href": "contents/core/benchmarking/benchmarking.it.html",
    "title": "11¬† Benchmarking dell‚ÄôIA",
    "section": "",
    "text": "11.1 Panoramica\nIl benchmarking fornisce le misure essenziali necessarie per guidare il progresso dell‚Äôapprendimento automatico e comprendere veramente le prestazioni del sistema. Come ha affermato il fisico Lord Kelvin, ‚ÄúMisurare √® conoscere‚Äù. I benchmark ci consentono di conoscere quantitativamente le capacit√† di diversi modelli, software e hardware. Consentono agli sviluppatori di ML di misurare il tempo di inferenza, l‚Äôutilizzo della memoria, il consumo energetico e altre metriche che caratterizzano un sistema. Inoltre, i benchmark creano processi standardizzati per la misurazione, consentendo confronti equi tra diverse soluzioni.\nQuando i benchmark vengono mantenuti nel tempo, diventano fondamentali per catturare i progressi attraverso generazioni di algoritmi, set di dati e hardware. I modelli e le tecniche che stabiliscono nuovi record sui benchmark di ML da un anno all‚Äôaltro dimostrano miglioramenti tangibili in ci√≤ che √® possibile per l‚Äôapprendimento automatico ‚Äúon-device‚Äù. Utilizzando i benchmark per misurare, i professionisti di ML possono conoscere le capacit√† reali dei loro sistemi e avere la certezza che ogni passaggio rifletta un progresso autentico verso lo stato dell‚Äôarte.\nIl benchmarking ha diversi obiettivi e scopi importanti che guidano la sua implementazione per i sistemi di apprendimento automatico.\nQuesto capitolo tratter√† i 3 tipi di benchmark AI, le metriche standard, gli strumenti e le tecniche che i progettisti utilizzano per ottimizzare i loro sistemi e le sfide e le tendenze nel benchmarking.",
    "crumbs": [
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Benchmarking dell'IA</span>"
    ]
  },
  {
    "objectID": "contents/core/benchmarking/benchmarking.it.html#panoramica",
    "href": "contents/core/benchmarking/benchmarking.it.html#panoramica",
    "title": "11¬† Benchmarking dell‚ÄôIA",
    "section": "",
    "text": "Valutazione delle prestazioni. Ci√≤ comporta la valutazione di parametri chiave come la velocit√†, l‚Äôaccuratezza e l‚Äôefficienza di un dato modello. Ad esempio, in un contesto TinyML, √® fondamentale confrontare la rapidit√† con cui un assistente vocale pu√≤ riconoscere i comandi, poich√© ci√≤ valuta le prestazioni in tempo reale.\nValutazione della potenza. Valutare la potenza assorbita da un carico di lavoro insieme alle sue prestazioni equivale alla sua efficienza energetica. Poich√© l‚Äôimpatto ambientale dell‚Äôelaborazione ML continua a crescere, il benchmarking dell‚Äôenergia pu√≤ consentirci di ottimizzare meglio i sistemi per la sostenibilit√†.\nValutazione delle risorse. Ci√≤ significa valutare l‚Äôimpatto del modello sulle risorse critiche del sistema, tra cui durata della batteria, utilizzo della memoria e sovraccarico computazionale. Un esempio rilevante √® il confronto del consumo della batteria di due diversi algoritmi di riconoscimento delle immagini in esecuzione su un dispositivo indossabile.\nValidazione e verifica. Il benchmarking aiuta a garantire che il sistema funzioni correttamente e soddisfi i requisiti specificati. Un modo √® quello di controllare l‚Äôaccuratezza di un algoritmo, come un cardiofrequenzimetro su uno smartwatch, rispetto alle letture di apparecchiature di livello medico come forma di validazione clinica.\nAnalisi competitiva. Ci√≤ consente di confrontare le soluzioni con le offerte concorrenti sul mercato. Ad esempio, il benchmarking di un modello personalizzato di rilevamento di oggetti rispetto ai benchmark TinyML comuni come MobileNet e Tiny-YOLO.\nCredibilit√†. I benchmark accurati sostengono la credibilit√† delle soluzioni AI e delle organizzazioni che le sviluppano. Dimostrano un impegno verso trasparenza, onest√† e qualit√†, essenziali per creare fiducia con utenti e stakeholder.\nRegolamentazione e Standardizzazione. Man mano che il settore dell‚ÄôAI continua a crescere, cresce anche la necessit√† di regolamentazione e standardizzazione per garantire che le soluzioni AI siano sicure, etiche ed efficaci. I benchmark accurati e affidabili sono essenziali per questo quadro normativo, poich√© forniscono i dati e le prove necessari per valutare la conformit√† con gli standard del settore e i requisiti legali.",
    "crumbs": [
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Benchmarking dell'IA</span>"
    ]
  },
  {
    "objectID": "contents/core/benchmarking/benchmarking.it.html#contesto-storico",
    "href": "contents/core/benchmarking/benchmarking.it.html#contesto-storico",
    "title": "11¬† Benchmarking dell‚ÄôIA",
    "section": "11.2 Contesto Storico",
    "text": "11.2 Contesto Storico\n\n11.2.1 Benchmark delle Prestazioni\nL‚Äôevoluzione dei benchmark nell‚Äôinformatica illustra vividamente l‚Äôincessante ricerca dell‚Äôeccellenza e dell‚Äôinnovazione da parte del settore. Nei primi giorni dell‚Äôinformatica, negli anni ‚Äô60 e ‚Äô70, i benchmark erano rudimentali e progettati per i mainframe. Ad esempio, il benchmark Whetstone, che prende il nome dal compilatore Whetstone ALGOL, √® stato uno dei primi test standardizzati per misurare le prestazioni aritmetiche in virgola mobile di una CPU. Questi benchmark pionieristici hanno spinto i produttori a perfezionare le loro architetture e algoritmi per ottenere punteggi di benchmark migliori.\nGli anni ‚Äô80 hanno segnato un cambiamento significativo con l‚Äôascesa dei personal computer. Mentre aziende come IBM, Apple e Commodore gareggiavano per quote di mercato, i benchmark sono diventati strumenti essenziali per consentire una concorrenza leale. I benchmark CPU SPEC, introdotti dalla System Performance Evaluation Cooperative (SPEC), hanno stabilito test standardizzati che consentono confronti oggettivi tra diverse macchine. Questa standardizzazione ha creato un ambiente competitivo, spingendo i produttori di chip e i creatori di sistemi a migliorare continuamente le loro offerte hardware e software.\nGli anni ‚Äô90 hanno portato l‚Äôera delle applicazioni e dei videogiochi ‚Äúgraphics-intensive‚Äù. La necessit√† di benchmark per valutare le prestazioni delle schede grafiche ha portato alla creazione di 3DMark da parte di Futuremark. Mentre i giocatori e i professionisti cercavano schede grafiche ad alte prestazioni, aziende come NVIDIA e AMD sono state spinte a una rapida innovazione, portando a importanti progressi nella tecnologia GPU come gli shader programmabili.\nGli anni 2000 hanno visto un‚Äôimpennata di telefoni cellulari e dispositivi portatili come i tablet. Con la portabilit√† √® arrivata la sfida di bilanciare prestazioni e consumo energetico. Benchmark come MobileMark di BAPCo hanno valutato velocit√† e durata della batteria. Ci√≤ ha spinto le aziende a sviluppare System-on-Chip (SOC) pi√π efficienti dal punto di vista energetico, portando all‚Äôemergere di architetture come ARM che hanno dato priorit√† all‚Äôefficienza energetica.\nL‚Äôattenzione dell‚Äôultimo decennio si √® spostata verso il cloud computing, i big data e l‚Äôintelligenza artificiale. I provider di servizi cloud come Amazon Web Services e Google Cloud competono su prestazioni, scalabilit√† e convenienza. I benchmark specifici del cloud come CloudSuite sono diventati essenziali, spingendo i provider a ottimizzare la propria infrastruttura per servizi migliori.\n\n\n11.2.2 Benchmark Energetici\nIl consumo energetico e le preoccupazioni ambientali hanno acquisito importanza negli ultimi anni, rendendo il benchmarking energetico sempre pi√π importante nel settore. Questo cambiamento √® iniziato a met√† degli anni 2000, quando i processori e i sistemi hanno iniziato a raggiungere i limiti di raffreddamento e la scalabilit√† √® diventata un aspetto cruciale della costruzione di sistemi su larga scala grazie ai progressi di Internet. Da allora, le considerazioni energetiche si sono espanse fino a comprendere tutte le aree dell‚Äôinformatica, dai dispositivi personali ai data center su larga scala.\nIl benchmarking energetico mira a misurare l‚Äôefficienza energetica dei sistemi informatici, valutando le prestazioni in relazione al consumo energetico. Ci√≤ √® fondamentale per diversi motivi:\n\nImpatto ambientale: Con la crescente impronta di carbonio del settore tecnologico, c‚Äô√® un‚Äôurgente necessit√† di ridurre il consumo energetico.\nCosti operativi: Le spese energetiche costituiscono una parte significativa dei costi operativi del data center.\nLongevit√† del dispositivo: Per i dispositivi mobili, l‚Äôefficienza energetica ha un impatto diretto sulla durata della batteria e sull‚Äôesperienza utente.\n\nIn questo ambito sono emersi diversi benchmark chiave:\n\nSPEC Power: Introdotto nel 2007, SPEC Power √® stato uno dei primi benchmark standard del settore per la valutazione delle caratteristiche di potenza e prestazioni dei server.\nGreen500: L‚Äôelenco Green500 classifica i supercomputer in base all‚Äôefficienza energetica, integrando l‚Äôelenco TOP500 incentrato sulle prestazioni.\nEnergy Star: Pur non essendo un benchmark in s√©, il programma di certificazione ENERGY STAR for Computers ha spinto i produttori a migliorare l‚Äôefficienza energetica dell‚Äôelettronica di consumo.\n\nIl benchmarking energetico affronta sfide uniche, come la contabilizzazione di diversi carichi di lavoro e configurazioni di sistema e la misura accurata del consumo energetico su una gamma di hardware che varia da microWatt a megawatt nel consumo energetico. Man mano che l‚ÄôIA e l‚Äôedge computing continuano a crescere, √® probabile che il benchmarking energetico diventi ancora pi√π critico, guidando lo sviluppo di ottimizzazioni hardware e software AI specializzate ed efficienti dal punto di vista energetico.\n\n\n11.2.3 Benchmark Personalizzati\nOltre ai benchmark standard del settore, ci sono benchmark personalizzati specificamente progettati per soddisfare i requisiti unici di una particolare applicazione o attivit√†. Sono personalizzati in base alle esigenze specifiche dell‚Äôutente o dello sviluppatore, assicurando che le metriche delle prestazioni siano direttamente pertinenti all‚Äôuso previsto del modello o del sistema di intelligenza artificiale. I benchmark personalizzati possono essere creati da singole organizzazioni, ricercatori o sviluppatori e sono spesso utilizzati insieme ai benchmark standard del settore per fornire una valutazione completa delle prestazioni dell‚Äôintelligenza artificiale.\nAd esempio, un ospedale potrebbe sviluppare un benchmark per valutare un modello di intelligenza artificiale per prevedere la riammissione dei pazienti. Questo benchmark incorporerebbe metriche pertinenti alla popolazione di pazienti dell‚Äôospedale, come dati demografici, anamnesi e fattori sociali. Allo stesso modo, il benchmark di rilevamento delle frodi di un istituto finanziario potrebbe concentrarsi sull‚Äôidentificazione accurata delle transazioni fraudolente riducendo al minimo i falsi positivi. Nel settore automobilistico, un benchmark di veicoli autonomi potrebbe dare priorit√† alle prestazioni in diverse condizioni, alla risposta agli ostacoli e alla sicurezza. I rivenditori potrebbero confrontare i sistemi di raccomandazione utilizzando il tasso di clic, il tasso di conversione e la soddisfazione del cliente. Le aziende manifatturiere potrebbero confrontare i sistemi di controllo qualit√† in base all‚Äôidentificazione dei difetti, all‚Äôefficienza e alla riduzione degli sprechi. In ogni settore, i benchmark personalizzati forniscono alle organizzazioni criteri di valutazione su misura per le loro esigenze e il loro contesto unici. Ci√≤ consente una valutazione pi√π significativa di quanto i sistemi di intelligenza artificiale soddisfino i requisiti.\nIl vantaggio dei benchmark personalizzati risiede nella loro flessibilit√† e pertinenza. Possono essere progettati per testare aspetti specifici delle prestazioni critici per il successo della soluzione di intelligenza artificiale nella sua applicazione prevista. Ci√≤ consente una valutazione pi√π mirata e accurata delle capacit√† del modello o del sistema di intelligenza artificiale. I benchmark personalizzati forniscono anche informazioni preziose sulle prestazioni delle soluzioni di intelligenza artificiale in scenari reali, il che pu√≤ essere cruciale per identificare potenziali problemi e aree di miglioramento.\nNell‚Äôintelligenza artificiale, i benchmark svolgono un ruolo cruciale nel guidare il progresso e l‚Äôinnovazione. Sebbene i benchmark siano stati a lungo utilizzati nell‚Äôinformatica, la loro applicazione all‚Äôapprendimento automatico √® relativamente recente. I benchmark incentrati sull‚Äôintelligenza artificiale forniscono metriche standardizzate per valutare e confrontare le prestazioni di diversi algoritmi, architetture di modelli e piattaforme hardware.\n\n\n11.2.4 Consenso della Comunit√†\nUna prerogativa fondamentale affinch√© un benchmark abbia un impatto √® che deve riflettere le priorit√† e i valori condivisi della pi√π ampia comunit√† di ricerca. I benchmark progettati in modo isolato rischiano di non ottenere accettazione se trascurano metriche chiave considerate importanti dai gruppi leader. Attraverso uno sviluppo collaborativo con la partecipazione aperta di laboratori accademici, aziende e altri stakeholder, i benchmark possono incorporare un contributo collettivo su capacit√† critiche che vale la pena misurare. Ci√≤ aiuta a garantire che i benchmark valutino aspetti che la comunit√† concorda siano essenziali per far progredire il campo. Il processo di raggiungimento dell‚Äôallineamento su attivit√† e metriche supporta di per s√© la convergenza su ci√≤ che conta di pi√π.\nInoltre, i benchmark pubblicati con ampia co-paternit√† da istituzioni rispettate hanno autorit√† e validit√† che convincono la comunit√† ad adottarli come standard affidabili. I benchmark percepiti come distorti da particolari interessi aziendali o istituzionali generano scetticismo. Anche il coinvolgimento continuo della comunit√† attraverso workshop e sfide √® fondamentale dopo la versione iniziale, ed √® ci√≤ che, ad esempio, ha portato al successo di ImageNet. Col progredire della ricerca, la partecipazione collettiva consente un continuo perfezionamento ed espansione dei benchmark nel tempo.\nInfine, rilasciare benchmark sviluppati dalla comunit√† con accesso aperto ne promuove l‚Äôadozione e l‚Äôuso coerente. Fornendo codice open source, documentazione, modelli e infrastrutture, riduciamo le barriere all‚Äôingresso, consentendo ai gruppi di confrontare le soluzioni su un piano di parit√† con le implementazioni standardizzate. Questa coerenza √® essenziale per confronti equi. Senza coordinamento, laboratori e aziende potrebbero implementare i benchmark in modo diverso, il che pu√≤ compromettere la riproducibilit√† e la comparabilit√† dei risultati.\nIl consenso della comunit√† conferisce ai benchmark una rilevanza duratura, mentre la frammentazione confonde. Attraverso lo sviluppo collaborativo e un funzionamento trasparente, i benchmark possono diventare standard autorevoli per monitorare i progressi. Molti dei benchmark di cui parliamo in questo capitolo sono stati sviluppati e creati dalla comunit√†, per la comunit√†, ed √® questo che alla fine ha portato al loro successo.",
    "crumbs": [
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Benchmarking dell'IA</span>"
    ]
  },
  {
    "objectID": "contents/core/benchmarking/benchmarking.it.html#benchmark-ai-sistema-modello-e-dati",
    "href": "contents/core/benchmarking/benchmarking.it.html#benchmark-ai-sistema-modello-e-dati",
    "title": "11¬† Benchmarking dell‚ÄôIA",
    "section": "11.3 Benchmark AI: Sistema, Modello e Dati",
    "text": "11.3 Benchmark AI: Sistema, Modello e Dati\nLa necessit√† di un benchmarking completo diventa fondamentale man mano che i sistemi AI diventano pi√π complessi e onnipresenti. In questo contesto, i benchmark sono spesso classificati in tre categorie principali: Hardware, Modello e Dati. Analizziamo perch√© ognuno di questi gruppi √® essenziale e il significato della valutazione dell‚ÄôAI da queste tre dimensioni distinte:\n\n11.3.1 Benchmark di Sistema\nI calcoli AI, in particolare quelli nel deep learning, richiedono molte risorse. L‚Äôhardware su cui vengono eseguiti questi calcoli svolge un ruolo importante nel determinare la velocit√†, l‚Äôefficienza e la scalabilit√† delle soluzioni AI. Di conseguenza, i benchmark hardware aiutano a valutare le prestazioni di CPU, GPU, TPU e altri acceleratori nelle attivit√† AI. Comprendendone le prestazioni, gli sviluppatori possono scegliere quali piattaforme hardware si adattano meglio a specifiche applicazioni AI. Inoltre, i produttori di hardware utilizzano questi benchmark per identificare aree di miglioramento, guidando l‚Äôinnovazione nei progetti di chip specifici per AI.\n\n\n11.3.2 Benchmark del Modello\nL‚Äôarchitettura, le dimensioni e la complessit√† dei modelli AI variano notevolmente. Modelli diversi hanno diverse esigenze di calcolo e offrono diversi livelli di accuratezza ed efficienza. I benchmark dei modelli aiutano a valutare le prestazioni di varie architetture AI su attivit√† standardizzate. Forniscono informazioni sulla velocit√†, l‚Äôaccuratezza e le richieste di risorse di diversi modelli. Eseguendo il benchmarking dei modelli, i ricercatori possono identificare le architetture pi√π performanti per attivit√† specifiche, guidando la comunit√† AI verso soluzioni pi√π efficienti ed efficaci. Inoltre, questi benchmark aiutano a monitorare i progressi della ricerca sull‚Äôintelligenza artificiale, mostrando i progressi nella progettazione e nell‚Äôottimizzazione dei modelli.\n\n\n11.3.3 Benchmark dei Dati\nNell‚Äôapprendimento automatico, i dati sono fondamentali perch√© la qualit√†, la scala e la diversit√† dei set di dati influiscono direttamente sull‚Äôefficacia e sulla generalizzazione del modello. I benchmark dei dati si concentrano sui set di dati utilizzati nel training e nella valutazione. Forniscono set di dati standardizzati che la comunit√† pu√≤ utilizzare per addestrare e testare i modelli, garantendo parit√† di condizioni per i confronti. Inoltre, questi parametri di riferimento evidenziano le sfide relative alla qualit√† dei dati, alla diversit√† e alla rappresentazione, spingendo la comunit√† ad affrontare i ‚Äúbias‚Äù [pregiudizi] e i ‚Äúgap‚Äù [lacune] nei dati di addestramento. Comprendendo i benchmark dei dati, i ricercatori possono anche valutare come i modelli potrebbero comportarsi in scenari reali, garantendo robustezza e affidabilit√†.\nNelle restanti sezioni, discuteremo ciascuno di questi tipi di benchmark. L‚Äôattenzione sar√† rivolta a un‚Äôesplorazione approfondita dei benchmark di sistema, poich√© sono fondamentali per comprendere e migliorare le prestazioni del sistema di apprendimento automatico. Parleremo brevemente dei benchmark dei modelli e dei dati per una prospettiva completa, ma l‚Äôenfasi e la maggior parte del contenuto saranno dedicati ai benchmark di sistema.",
    "crumbs": [
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Benchmarking dell'IA</span>"
    ]
  },
  {
    "objectID": "contents/core/benchmarking/benchmarking.it.html#benchmarking-di-sistema",
    "href": "contents/core/benchmarking/benchmarking.it.html#benchmarking-di-sistema",
    "title": "11¬† Benchmarking dell‚ÄôIA",
    "section": "11.4 Benchmarking di Sistema",
    "text": "11.4 Benchmarking di Sistema\n\n11.4.1 Granularit√†\nIl benchmarking del sistema di apprendimento automatico fornisce un approccio strutturato e sistematico per valutare le prestazioni di un sistema in diverse dimensioni. Data la complessit√† dei sistemi ML, possiamo analizzare le loro prestazioni attraverso diversi livelli di granularit√† e ottenere una visione completa dell‚Äôefficienza del sistema, identificare potenziali colli di bottiglia e individuare le aree di miglioramento. A tal fine, nel corso degli anni si sono evoluti vari tipi di benchmark che continuano a persistere.\nFigura¬†11.1 illustra i diversi livelli di granularit√† di un sistema ML. A livello di applicazione, i benchmark end-to-end valutano le prestazioni complessive del sistema, considerando fattori come la pre-elaborazione dei dati, l‚Äôaddestramento del modello e l‚Äôinferenza. Mentre a livello di modello, i benchmark si concentrano sulla valutazione dell‚Äôefficienza e dell‚Äôaccuratezza di modelli specifici. Ci√≤ include la valutazione di quanto bene i modelli si generalizzano a nuovi dati e della loro efficienza computazionale durante l‚Äôaddestramento e l‚Äôinferenza. Inoltre, il benchmarking pu√≤ estendersi all‚Äôinfrastruttura hardware e software, esaminando le prestazioni di singoli componenti come GPU o TPU.\n\n\n\n\n\n\nFigura¬†11.1: Granularit√† del sistema ML.\n\n\n\n\nMicro Benchmark\nI micro-benchmark sono specializzati e valutano componenti distinti o operazioni specifiche all‚Äôinterno di un processo di apprendimento automatico pi√π ampio. Questi benchmark si concentrano su singole attivit√†, offrendo approfondimenti sulle richieste computazionali di un particolare layer di rete neurale, l‚Äôefficienza di un‚Äôunica tecnica di ottimizzazione o la produttivit√† di una specifica funzione di attivazione. Ad esempio, i professionisti potrebbero utilizzare i micro-benchmark per misurare il tempo di calcolo richiesto da un layer convoluzionale in un modello di deep learning o per valutare la velocit√† di preelaborazione che alimenta i dati nel modello. Tali valutazioni granulari sono fondamentali per la messa a punto e l‚Äôottimizzazione di aspetti discreti dei modelli, assicurando che ogni componente funzioni al massimo del suo potenziale.\nQuesti tipi di microbenchmark includono lo zoom su operazioni o componenti molto specifiche della pipeline AI, come le seguenti:\n\nOperazioni Tensoriali: Librerie come cuDNN (di NVIDIA) spesso hanno benchmark per misurare le prestazioni di singole operazioni tensoriali, come convoluzioni o moltiplicazioni di matrici, che sono fondamentali per i calcoli del deep learning.\nFunzioni di Attivazione: Benchmark che misurano la velocit√† e l‚Äôefficienza di varie funzioni di attivazione come ReLU, Sigmoid o Tanh in isolamento.\nBenchmark di Layer: Valutazioni dell‚Äôefficienza computazionale di distinti layer di rete neurale, come blocchi LSTM o Transformer, quando si opera su dimensioni di input standardizzate.\n\nEsempio: DeepBench, introdotto da Baidu, √® un buon benchmark che valuta le operazioni fondamentali di deep learning, come quelle menzionate sopra. DeepBench valuta le prestazioni delle operazioni di base nei modelli di deep learning, fornendo informazioni su come diverse piattaforme hardware gestiscono l‚Äôaddestramento e l‚Äôinferenza delle reti neurali.\n\n\n\n\n\n\nEsercizio¬†11.1: Benchmarking di Sistema - Operazioni Tensoriali\n\n\n\n\n\nCi si √® mai chiesto come mai i filtri immagine diventano cos√¨ veloci? Librerie speciali come cuDNN potenziano quei calcoli su determinati hardware. In questo Colab, useremo cuDNN con PyTorch per velocizzare il filtraggio delle immagini. Lo si consideri un piccolo benchmark, che mostra come il software giusto pu√≤ sbloccare la potenza della GPU!\n\n\n\n\n\n\nMacro Benchmark\nI macro benchmark forniscono una visione olistica, valutando le prestazioni end-to-end di interi modelli di apprendimento automatico o sistemi di ML completi. Invece di concentrarsi sulle singole operazioni, i macro benchmark valutano l‚Äôefficacia collettiva dei modelli in scenari o attivit√† del mondo reale. Ad esempio, un macro benchmark potrebbe valutare le prestazioni complete di un modello di apprendimento profondo che esegue la classificazione delle immagini su un set di dati come ImageNet. Ci√≤ include la misura dell‚Äôaccuratezza, della velocit√† di calcolo e del consumo di risorse. Allo stesso modo, si potrebbero misurare il tempo e le risorse cumulativi necessari per addestrare un modello di elaborazione del linguaggio naturale su corpora di testo estesi o valutare le prestazioni di un intero sistema di raccomandazione, dall‚Äôinserimento dei dati agli output finali specifici dell‚Äôutente.\nEsempi: Questi benchmark valutano il modello di intelligenza artificiale:\n\nMLPerf Inference (Reddi et al. 2020): Un set di benchmark standard per misurare le prestazioni di software e hardware di apprendimento automatico. MLPerf ha una suite di benchmark dedicati per scale specifiche, come MLPerf Mobile per dispositivi di classe mobile e MLPerf Tiny, che si concentra su microcontrollori e altri dispositivi con risorse limitate.\nMLMark di EEMBC: Una suite di benchmarking per valutare le prestazioni e l‚Äôefficienza energetica dei dispositivi embedded che eseguono carichi di lavoro di apprendimento automatico. Questo benchmark fornisce informazioni su come diverse piattaforme hardware gestiscono attivit√† come il riconoscimento delle immagini o l‚Äôelaborazione audio.\nAI-Benchmark (Ignatov et al. 2019): Uno strumento di benchmarking progettato per dispositivi Android, valuta le prestazioni delle attivit√† di intelligenza artificiale sui dispositivi mobili, comprendendo vari scenari del mondo reale come il riconoscimento delle immagini, l‚Äôanalisi dei volti e il riconoscimento ottico dei caratteri.\n\n\nReddi, Vijay Janapa, Christine Cheng, David Kanter, Peter Mattson, Guenther Schmuelling, Carole-Jean Wu, Brian Anderson, et al. 2020. ¬´MLPerf Inference Benchmark¬ª. In 2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA), 446‚Äì59. IEEE; IEEE. https://doi.org/10.1109/isca45697.2020.00045.\n\nIgnatov, Andrey, Radu Timofte, Andrei Kulik, Seungsoo Yang, Ke Wang, Felix Baum, Max Wu, Lirong Xu, e Luc Van Gool. 2019. ¬´AI Benchmark: All About Deep Learning on Smartphones in 2019¬ª. In 2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW), 3617‚Äì35. IEEE. https://doi.org/10.1109/iccvw.2019.00447.\n\n\nBenchmark end-to-end\nI benchmark end-to-end forniscono una valutazione completa che si estende oltre i confini del modello di ML stesso. Invece di concentrarsi esclusivamente sull‚Äôefficienza o l‚Äôaccuratezza computazionale di un modello di apprendimento automatico, questi benchmark comprendono l‚Äôintera pipeline di un sistema di IA. Ci√≤ include la pre-elaborazione iniziale dei dati, le prestazioni del modello principale, la post-elaborazione degli output del modello e altri componenti integrali come l‚Äôarchiviazione e le interazioni di rete.\nLa pre-elaborazione dei dati √® la prima fase in molti sistemi di IA, trasformando i dati grezzi in un formato adatto per l‚Äôaddestramento o l‚Äôinferenza del modello. L‚Äôefficienza, la scalabilit√† e l‚Äôaccuratezza di queste fasi di pre-elaborazione sono vitali per le prestazioni complessive del sistema. I benchmark end-to-end valutano questa fase, assicurando che la pulizia dei dati, la normalizzazione, l‚Äôaumento o qualsiasi altro processo di trasformazione non diventi un collo di bottiglia.\nAnche la fase di post-elaborazione √® al centro dell‚Äôattenzione. Ci√≤ comporta l‚Äôinterpretazione degli output grezzi del modello, eventualmente la conversione dei punteggi in categorie significative, il filtraggio dei risultati o persino l‚Äôintegrazione con altri sistemi. Nelle applicazioni del mondo reale, questa fase √® fondamentale per fornire informazioni fruibili e i benchmark end-to-end ne garantiscono l‚Äôefficienza e l‚Äôefficacia.\nOltre alle operazioni di base dell‚ÄôIA, altri componenti del sistema sono importanti per le prestazioni complessive e l‚Äôesperienza utente. Le soluzioni di archiviazione, basate su cloud, on-premise o ibride, possono avere un impatto significativo sui tempi di recupero e archiviazione dei dati, in particolare con vasti set di dati di IA. Allo stesso modo, le interazioni di rete, vitali per le soluzioni di IA basate su cloud o per i sistemi distribuiti, possono diventare colli di bottiglia delle prestazioni se non ottimizzate. I benchmark end-to-end valutano in modo olistico questi componenti, assicurando che l‚Äôintero sistema funzioni senza problemi, dal recupero dei dati alla consegna dell‚Äôoutput finale.\nAd oggi, non esistono benchmark end-to-end pubblici che tengano conto del ruolo dell‚Äôarchiviazione dei dati, della rete e delle prestazioni di elaborazione. Si pu√≤ sostenere che MLPerf Training and Inference si avvicini all‚Äôidea di un benchmark end-to-end, ma si concentrano esclusivamente sulle prestazioni del modello ML e non rappresentano scenari di distribuzione nel mondo reale di come i modelli vengono utilizzati sul campo. Tuttavia, forniscono un segnale molto utile che aiuta a valutare le prestazioni del sistema AI.\nData la specificit√† intrinseca del benchmarking end-to-end, viene in genere eseguito internamente in un‚Äôazienda ‚Äústrumentando‚Äù [inserendo punti di controllo] distribuzioni di produzione reali di AI. Ci√≤ consente agli ingegneri di avere una comprensione e una ripartizione realistiche delle prestazioni, ma data la sensibilit√† e la specificit√† delle informazioni, raramente vengono segnalate all‚Äôesterno dell‚Äôazienda.\n\n\nComprendere i Compromessi\nDiversi problemi sorgono nelle diverse fasi di un sistema di intelligenza artificiale. I micro-benchmark aiutano a mettere a punto i singoli componenti, i macro-benchmark aiutano a perfezionare le architetture o gli algoritmi del modello e i benchmark end-to-end guidano l‚Äôottimizzazione dell‚Äôintero flusso di lavoro. Comprendendo dove si trova un problema, gli sviluppatori possono applicare ottimizzazioni mirate.\nInoltre, mentre i singoli componenti di un sistema di intelligenza artificiale potrebbero funzionare in modo ottimale in isolamento, possono emergere colli di bottiglia quando interagiscono. I benchmark end-to-end, in particolare, sono fondamentali per garantire che l‚Äôintero sistema, quando funziona collettivamente, soddisfi gli standard di prestazioni ed efficienza desiderati.\nInfine, le organizzazioni possono prendere decisioni informate su dove allocare le risorse individuando colli di bottiglia o inefficienze nelle prestazioni. Ad esempio, se i micro-benchmark rivelano inefficienze in specifiche operazioni tensoriali, gli investimenti possono essere indirizzati verso acceleratori hardware specializzati. Al contrario, se i benchmark end-to-end indicano problemi di recupero dei dati, gli investimenti potrebbero essere incanalati verso soluzioni di archiviazione migliori.\n\n\n\n11.4.2 Componenti dei Benchmark\nIn sostanza, un benchmark AI √® pi√π di un semplice test o punteggio; √® un framework di valutazione completo. Per comprenderlo in modo approfondito, analizziamo i componenti tipici che compongono un benchmark AI.\n\nDataset Standardizzati\nI set di dati fungono da base per la maggior parte dei benchmark AI. Forniscono un set di dati coerente su cui i modelli vengono addestrati e valutati, garantendo parit√† di condizioni per i confronti.\nEsempio: ImageNet, un set di dati su larga scala contenente milioni di immagini etichettate che abbracciano migliaia di categorie, √® uno standard di benchmarking popolare per le attivit√† di classificazione delle immagini.\n\n\nAttivit√† Predefinite\nUn benchmark dovrebbe avere un obiettivo o un compito chiaro che i modelli mirano a raggiungere. Questo compito definisce il problema che il sistema AI sta cercando di risolvere.\nEsempio: I compiti per i benchmark di elaborazione del linguaggio naturale potrebbero includere analisi del ‚Äúsentiment‚Äù, riconoscimento di entit√† denominate o traduzione automatica.\n\n\nMetriche di Valutazione\nUna volta definito un task, i benchmark richiedono parametri per quantificare le prestazioni. Questi parametri offrono misure oggettive per confrontare diversi modelli o sistemi. Nei task di classificazione, parametri come accuratezza, precisione, richiamo e punteggio F1 sono comunemente utilizzati. Errori quadratici medi o assoluti potrebbero essere utilizzati per i task di regressione. Possiamo anche misurare la potenza consumata dall‚Äôesecuzione del benchmark per calcolare l‚Äôefficienza energetica.\n\n\nBaseline e Modelli Baseline\nI benchmark spesso includono modelli ‚Äúbaseline‚Äù o implementazioni di riferimento. Di solito servono come punti di partenza o standard minimi di prestazione per confrontare nuovi modelli o nuove tecniche. I modelli ‚Äúbaseline‚Äù aiutano i ricercatori a misurare l‚Äôefficacia di nuovi algoritmi.\nNelle suite di benchmark, modelli semplici come la regressione lineare o le reti neurali di base sono spesso le baseline comuni. Queste forniscono un contesto quando si valutano modelli pi√π complessi. Confrontando questi modelli pi√π semplici, i ricercatori possono quantificare i miglioramenti derivanti da approcci avanzati.\nLe metriche delle prestazioni variano in base all‚Äôattivit√†, ma ecco alcuni esempi:\n\nLe attivit√† di classificazione utilizzano metriche come accuratezza, precisione, richiamo e punteggio F1.\nLe attivit√† di regressione utilizzano spesso l‚Äôerrore quadratico medio o l‚Äôerrore assoluto medio.\n\n\n\nSpecifiche Hardware e Software\nData la variabilit√† introdotta da diverse configurazioni hardware e software, i benchmark spesso specificano o documentano gli ambienti hardware e software in cui vengono condotti i test.\nEsempio: Un benchmark AI potrebbe indicare che le valutazioni sono state condotte su una GPU NVIDIA Tesla V100 utilizzando TensorFlow v2.4.\n\n\nCondizioni Ambientali\nPoich√© fattori esterni possono influenzare i risultati del benchmark, √® essenziale controllare o documentare condizioni come temperatura, fonte di alimentazione o processi di background del sistema.\nEsempio: I benchmark AI mobili potrebbero specificare che i test sono stati condotti a temperatura ambiente con dispositivi collegati a una fonte di alimentazione per eliminare le variazioni del livello della batteria.\n\n\nRegole di Riproducibilit√†\nPer garantire che i benchmark siano credibili e possano essere replicati da altri nella comunit√†, spesso includono protocolli dettagliati che coprono tutto, dai ‚Äúrandom seed‚Äù utilizzati agli iperparametri esatti.\nEsempio: Un benchmark per un‚Äôattivit√† di learning di rinforzo potrebbe specificare gli episodi esatti dell‚Äôaddestramento, i rapporti di esplorazione-sfruttamento e le strutture di ricompensa utilizzate.\n\n\nLinee Guida per l‚ÄôInterpretazione dei Risultati\nOltre ai punteggi o alle metriche pure, i benchmark spesso forniscono linee guida o contesto per interpretare i risultati, aiutando i professionisti a comprendere le implicazioni pi√π ampie.\nEsempio: Un benchmark potrebbe evidenziare che, sebbene il Modello A abbia ottenuto un punteggio pi√π alto del Modello B in termini di accuratezza, offre migliori prestazioni in tempo reale, rendendolo pi√π adatto per applicazioni sensibili al fattore tempo.\n\n\n\n11.4.3 I Benchmark del Training\nIl ciclo di vita dello sviluppo di un modello di apprendimento automatico prevede due fasi critiche: addestramento e inferenza. Il training [addestramento] rappresenta la fase in cui il sistema elabora e assimila dati grezzi per adattare e perfezionare i propri parametri. Il benchmarking della fase di training rivela come le scelte nella pipeline di dati, soluzioni di storage, architetture di modelli, risorse di elaborazione, impostazioni di iperparametri e algoritmi di ottimizzazione influiscono sull‚Äôefficienza e sulle richieste di risorse del training del modello. L‚Äôobiettivo √® garantire che il sistema ML possa apprendere in modo efficiente dai dati, ottimizzando sia le prestazioni del modello sia l‚Äôutilizzo delle risorse del sistema.\n\nScopo\nDal punto di vista dei sistemi, l‚Äôaddestramento dei modelli di apprendimento automatico richiede molte risorse, soprattutto quando si lavora con modelli di grandi dimensioni. Questi modelli spesso contengono miliardi o addirittura trilioni di parametri addestrabili e richiedono enormi quantit√† di dati, spesso su una scala di molti terabyte. Ad esempio, GPT-3 di OpenAI (Brown et al. 2020) ha 175 miliardi di parametri, √® stato addestrato su 45 TB di dati compressi in testo normale e ha richiesto 3.640 petaflop-giorni di elaborazione per il pre-addestramento. I benchmark di training ML valutano i sistemi e le risorse necessari per gestire il carico computazionale dell‚Äôaddestramento di tali modelli.\nAnche l‚Äôarchiviazione e la distribuzione efficienti dei dati durante l‚Äôaddestramento svolgono un ruolo importante nel processo di addestramento. Ad esempio, in un modello di apprendimento automatico che prevede riquadri di delimitazione attorno agli oggetti in un‚Äôimmagine, potrebbero essere necessarie migliaia di immagini. Tuttavia, caricare un intero set di dati di immagini nella memoria √® in genere irrealizzabile, quindi i professionisti si affidano ai caricatori di dati (come discusso in Sezione 6.4.3.1) dai framework ML. Il training di successo del modello dipende dalla consegna tempestiva ed efficiente dei dati, rendendo essenziale il benchmarking di strumenti come caricatori di dati, pipeline di dati, velocit√† di pre-elaborazione e tempi di recupero dell‚Äôarchiviazione per comprenderne l‚Äôimpatto sulle prestazioni del training.\nLa selezione dell‚Äôhardware √® un altro fattore chiave nel training dei sistemi di machine learning, in quanto pu√≤ avere un impatto significativo sui tempi. I benchmark di training valutano l‚Äôutilizzo di CPU, GPU, memoria e rete durante la fase di training per guidare le ottimizzazioni del sistema. √à essenziale comprendere come vengono utilizzate le risorse: le GPU vengono sfruttate appieno? C‚Äô√® un sovraccarico di memoria non necessario? I benchmark possono scoprire colli di bottiglia o inefficienze nell‚Äôutilizzo delle risorse, con conseguenti risparmi sui costi e miglioramenti delle prestazioni.\nIn molti casi, l‚Äôutilizzo di un singolo acceleratore hardware, come una singola GPU, non √® sufficiente per soddisfare le esigenze computazionali del training di modelli su larga scala. I modelli di apprendimento automatico vengono spesso addestrati in data center con pi√π GPU o TPU, dove il calcolo distribuito consente l‚Äôelaborazione parallela tra i nodi. I benchmark di addestramento valutano l‚Äôefficienza con cui il sistema si ridimensiona su pi√π nodi, gestisce lo sharding dei dati e gestisce sfide come guasti o taglio dei nodi durante l‚Äôaddestramento.\n\n\nMetriche\nSe viste da una prospettiva di sistema, le metriche di training offrono informazioni che trascendono gli indicatori di prestazioni algoritmiche convenzionali. Queste metriche misurano l‚Äôefficacia di apprendimento del modello e misurano l‚Äôefficienza, la scalabilit√† e la robustezza dell‚Äôintero sistema ML durante la fase di training. Analizziamo pi√π a fondo queste metriche e il loro significato.\nLe seguenti metriche sono spesso considerate importanti:\n\nTempo di training: Il tempo necessario per addestrare un modello da zero fino a raggiungere un livello di prestazioni soddisfacente. Misura direttamente le risorse di elaborazione necessarie per addestrare un modello. Ad esempio, il BERT di Google (Devlin et al. 2019) √® un modello di elaborazione del linguaggio naturale che richiede diversi giorni per l‚Äôaddestramento su un corpus enorme di dati di testo utilizzando pi√π GPU. Il lungo tempo di training √® una sfida significativa in termini di consumo di risorse e costi. In alcuni casi, i benchmark possono invece misurare la produttivit√† del training (campioni di training per unit√† di tempo). La produttivit√† pu√≤ essere calcolata molto pi√π velocemente e facilmente del tempo di addestramento, ma potrebbe oscurare le metriche che ci interessano davvero (ad esempio, il tempo di addestramento).\nScalabilit√†: Quanto bene il processo di addestramento pu√≤ gestire gli aumenti delle dimensioni dei dati o della complessit√† del modello. La scalabilit√† pu√≤ essere valutata misurando il tempo di addestramento, l‚Äôutilizzo della memoria e altri consumi di risorse all‚Äôaumentare delle dimensioni dei dati o della complessit√† del modello. Ad esempio, l‚Äôaddestramento del GPT-3 di OpenAI ha richiesto notevoli sforzi ingegneristici per adattare il processo di training a numerosi nodi GPU, in modo da gestire le enormi dimensioni del modello. Ci√≤ ha comportato l‚Äôutilizzo di hardware specializzato, addestramento distribuito e altre tecniche per garantire che il modello potesse essere addestrato in modo efficiente.\nUtilizzo delle Risorse: La misura in cui il processo di addestramento utilizza le risorse di calcolo disponibili come CPU, GPU, memoria e I/O del disco. Un elevato utilizzo delle risorse pu√≤ indicare un processo di training efficiente, mentre un basso utilizzo pu√≤ suggerire colli di bottiglia o inefficienze. Ad esempio, il training di una rete neurale convoluzionale (CNN) per la classificazione delle immagini richiede notevoli risorse GPU. L‚Äôutilizzo di configurazioni multi-GPU e l‚Äôottimizzazione del codice di training per l‚Äôaccelerazione GPU possono migliorare notevolmente l‚Äôutilizzo delle risorse e l‚Äôefficienza del training.\nConsumo di Memoria: La quantit√† di memoria utilizzata dal processo di training. Il consumo di memoria pu√≤ essere un fattore limitante per il training di modelli o set di dati di grandi dimensioni. Ad esempio, i ricercatori di Google hanno dovuto affrontare notevoli sfide di consumo di memoria durante il training di BERT. Il modello ha centinaia di milioni di parametri, che richiedono grandi quantit√† di memoria. I ricercatori hanno dovuto sviluppare tecniche per ridurre il consumo di memoria, come il checkpointing del gradiente e il parallelismo del modello.\nConsumo Energetico: L‚Äôenergia consumata durante il training. Man mano che i modelli di apprendimento automatico diventano pi√π complessi, il consumo energetico √® diventato un fattore importante da considerare. Il training di grandi modelli di apprendimento automatico pu√≤ consumare molta energia, e quindi molto carbonio. Ad esempio, si √® stimato che l‚Äôaddestramento di GPT-3 di OpenAI abbia un‚Äôimpronta di carbonio equivalente a un viaggio in auto di 700.000 chilometri (~435,000 miglia).\nThroughput: l numero di campioni di addestramento elaborati per unit√† di tempo. Un throughput [produttivit√†] pi√π elevato indica generalmente un processo di addestramento pi√π efficiente. La produttivit√† √® una metrica importante da considerare quando si addestra un sistema di raccomandazione per una piattaforma di e-commerce. Una produttivit√† elevata assicura che il modello possa elaborare rapidamente grandi volumi di dati di interazione dell‚Äôutente, il che √® fondamentale per mantenere la pertinenza e l‚Äôaccuratezza delle raccomandazioni. Ma √® anche importante capire come bilanciare la produttivit√† con i limiti di latenza. Pertanto, un vincolo di produttivit√† limitato dalla latenza viene spesso imposto agli accordi sul livello di servizio per le distribuzioni di applicazioni del data center.\nCosto: Il costo della training di un modello pu√≤ includere sia risorse computazionali che umane. Il costo √® importante quando si considera la praticit√† e la fattibilit√† del training di modelli grandi o complessi. Si stima che l‚Äôaddestramento di modelli di linguaggio grandi come GPT-3 costi milioni di dollari. Questo costo include risorse computazionali, elettriche e umane necessarie per lo sviluppo e l‚Äôaddestramento del modello.\nTolleranza agli Errori e Robustezza: La capacit√† del processo di training di gestire guasti o errori senza bloccarsi o produrre risultati errati. Questo √® importante per garantire l‚Äôaffidabilit√† del processo di addestramento. Errori di rete o malfunzionamenti hardware possono verificarsi in uno scenario reale in cui un modello di apprendimento automatico viene addestrato su un sistema distribuito. Negli ultimi anni, √® diventato abbondantemente chiaro che gli errori derivanti dalla corruzione ‚Äúsilenziosa‚Äù dei dati sono emersi come un problema importante. Un processo di addestramento affidabile e tollerante agli errori pu√≤ recuperare da tali errori senza compromettere l‚Äôintegrit√† del modello.\nFacilit√† d‚ÄôUso e Flessibilit√†: La facilit√† con cui il processo di addestramento pu√≤ essere impostato e utilizzato e la sua flessibilit√† nella gestione di diversi tipi di dati e modelli. In aziende come Google, l‚Äôefficienza pu√≤ talvolta essere misurata dal numero di anni di ‚ÄúSoftware Engineer (SWE)‚Äù risparmiati poich√© ci√≤ si traduce direttamente in impatto. La facilit√† d‚Äôuso e la flessibilit√† possono ridurre il tempo e lo sforzo necessari per addestrare un modello. TensorFlow e PyTorch sono popolari framework di apprendimento automatico che forniscono interfacce intuitive e API flessibili per la creazione e l‚Äôaddestramento di modelli di machine-learning. Questi framework supportano molte architetture di modelli e sono dotati di strumenti che semplificano il processo di addestramento.\nRiproducibilit√†: La capacit√† di riprodurre i risultati del processo di training. La riproducibilit√† √® importante per verificare la correttezza e la validit√† di un modello. Tuttavia, le variazioni dovute alle caratteristiche stocastiche della rete spesso rendono difficile riprodurre il comportamento preciso delle applicazioni in fase di addestramento, il che pu√≤ rappresentare una sfida per il benchmarking.\n\nEseguendo il benchmarking per questi tipi di metriche, possiamo ottenere una visione completa delle prestazioni e dell‚Äôefficienza del processo di training da una prospettiva di sistema. Ci√≤ pu√≤ aiutare a identificare le aree di miglioramento e garantire che le risorse siano utilizzate in modo efficace.\n\n\nI Benchmark\nEcco alcuni lavori originali che hanno gettato le basi fondamentali per lo sviluppo di benchmark sistematici per l‚Äôaddestramento di sistemi di apprendimento automatico.\nMLPerf Training Benchmark: MLPerf √® una suite di benchmark progettata per misurare le prestazioni di hardware, software e servizi di apprendimento automatico. Il benchmark di MLPerf Training (Mattson et al. 2020a) si concentra sul tempo necessario per addestrare i modelli a una metrica di qualit√† target. Include carichi di lavoro diversi, come classificazione delle immagini, rilevamento di oggetti, traduzione e apprendimento per rinforzo. Figura¬†11.2 evidenzia i miglioramenti delle prestazioni nelle versioni progressive dei benchmark di MLPerf Training, che hanno tutti superato la legge di Moore. L‚Äôutilizzo di trend di benchmarking standardizzati ci consente di mostrare rigorosamente la rapida evoluzione del ML computing.\n\n\n\n\n\n\nFigura¬†11.2: Tendenze delle prestazioni di MLPerf Training. Fonte: Mattson et al. (2020a).\n\n\n‚Äî‚Äî‚Äî, et al. 2020a. ¬´MLPerf: An Industry Standard Benchmark Suite for Machine Learning Performance¬ª. IEEE Micro 40 (2): 8‚Äì16. https://doi.org/10.1109/mm.2020.2974843.\n\n\nMetriche:\n\nTempo di training per la qualit√† target\nThroughput (esempi al secondo)\nUtilizzo delle risorse (CPU, GPU, memoria, I/O del disco)\n\nDAWNBench: DAWNBench (Coleman et al. 2019) √® una suite di benchmark incentrata sui tempi di training end-to-end del deep learning e sulle prestazioni di inferenza. Include attivit√† comuni come la classificazione delle immagini e la risposta alle domande.\n\nColeman, Cody, Daniel Kang, Deepak Narayanan, Luigi Nardi, Tian Zhao, Jian Zhang, Peter Bailis, Kunle Olukotun, Chris R√©, e Matei Zaharia. 2019. ¬´Analysis of DAWNBench, a Time-to-Accuracy Machine Learning Performance Benchmark¬ª. ACM SIGOPS Operating Systems Review 53 (1): 14‚Äì25. https://doi.org/10.1145/3352020.3352024.\nMetriche:\n\nTempo di training per la precisione target\nLatenza dell‚Äôinferenza\nCosto (in termini di risorse di cloud computing e storage)\n\nFathom: Fathom (Adolf et al. 2016) √® un benchmark dell‚ÄôUniversit√† di Harvard che valuta le prestazioni dei modelli di deep learning utilizzando un set diversificato di carichi di lavoro. Questi includono attivit√† comuni come la classificazione delle immagini, il riconoscimento vocale e la modellazione del linguaggio.\n\nAdolf, Robert, Saketh Rama, Brandon Reagen, Gu-yeon Wei, e David Brooks. 2016. ¬´Fathom: reference workloads for modern deep learning methods¬ª. In 2016 IEEE International Symposium on Workload Characterization (IISWC), 1‚Äì10. IEEE; IEEE. https://doi.org/10.1109/iiswc.2016.7581275.\nMetriche:\n\nOperazioni al secondo (per misurare l‚Äôefficienza computazionale)\nTempo di completamento per ogni carico di lavoro\nLarghezza di banda della memoria\n\n\n\nCaso d‚ÄôUso di Esempio\nSi immagini di essere stati incaricati di effettuare il benchmarking delle prestazioni di training di un modello di classificazione delle immagini su una piattaforma hardware specifica. Analizziamo come si potrebbe affrontare questa situazione:\n\nDefinire l‚ÄôAttivit√†: Per prima cosa, si sceglie un modello e un set di dati. In questo caso, si allener√† una CNN per classificare le immagini nel set di dati CIFAR-10, un benchmark ampiamente utilizzato nella visione artificiale.\nSelezionare il benchmark: La scelta di un benchmark ampiamente accettato aiuta a garantire che la configurazione sia confrontabile con altre valutazioni del mondo reale. Si potrebbe scegliere di utilizzare il benchmark di training MLPerf perch√© fornisce un carico di lavoro di classificazione delle immagini strutturato, rendendolo un‚Äôopzione pertinente e standardizzata per valutare le prestazioni di training su CIFAR-10. L‚Äôutilizzo di MLPerf consente di valutare il sistema rispetto a metriche standard del settore, contribuendo a garantire che i risultati siano significativi e confrontabili con quelli ottenuti su altre piattaforme hardware.\nIdentificare le Metriche Chiave: Ora, si decidono le metriche che aiuteranno a valutare le prestazioni di training del sistema. Per questo esempio, si potrebbero tracciare:\n\nTempo di Training: Quanto tempo ci vuole per raggiungere il 90% di accuratezza?\nProduttivit√†: Quante immagini vengono elaborate al secondo?\nUtilizzo delle Risorse: Qual √® l‚Äôutilizzo di GPU e CPU durante il training?\n\n\nAnalizzando queste metriche, si otterranno informazioni sulle prestazioni di training del modello sulla piattaforma hardware scelta. Valutare se il tempo di training soddisfa le aspettative, se ci sono colli di bottiglia, come GPU sottoutilizzate o caricamento lento dei dati. Questo processo aiuta a identificare aree per una potenziale ottimizzazione, come il miglioramento della gestione dei dati o la regolazione dell‚Äôallocazione delle risorse, e pu√≤ guidare le future decisioni di benchmarking.\n\n\n\n11.4.4 Benchmark di Inferenza\nL‚Äôinferenza nell‚Äôapprendimento automatico si riferisce all‚Äôuso di un modello addestrato per fare previsioni su dati nuovi e mai visti prima. √à la fase in cui il modello applica le conoscenze apprese per risolvere il problema per cui √® stato progettato, come la classificazione di immagini, il riconoscimento vocale o la traduzione di testo.\n\nScopo\nQuando creiamo modelli di machine learning, il nostro obiettivo finale √® di distribuirli in applicazioni del mondo reale in cui possano fornire previsioni accurate e affidabili su dati nuovi e mai visti. Questo processo di utilizzo di un modello addestrato per fare previsioni √® noto come inferenza. Le prestazioni reali di un modello di apprendimento automatico possono differire in modo significativo dalle sue prestazioni su set di dati di addestramento o validazione, il che rende l‚Äôinferenza di benchmarking un passaggio cruciale nello sviluppo e nell‚Äôimplementazione di modelli di machine learning.\nIl benchmarking dell‚Äôinferenza ci consente di valutare quanto bene un modello di apprendimento automatico funziona in scenari del mondo reale. Questa valutazione garantisce che il modello sia pratico e affidabile quando distribuito in applicazioni, fornendo una comprensione pi√π completa del comportamento del modello con dati reali. Inoltre, il benchmarking pu√≤ aiutare a identificare potenziali colli di bottiglia o limitazioni nelle prestazioni del modello. Ad esempio, se un modello impiega troppo tempo per dedurre, potrebbe non essere pratico per applicazioni in tempo reale come la guida autonoma o gli assistenti vocali.\nL‚Äôefficienza delle risorse √® un altro aspetto critico dell‚Äôinferenza, poich√© pu√≤ essere computazionalmente intensiva e richiedere memoria e potenza di elaborazione significative. Il benchmarking aiuta a garantire che il modello sia efficiente per quanto riguarda l‚Äôutilizzo delle risorse, il che √® particolarmente importante per i dispositivi edge con capacit√† computazionali limitate, come smartphone o dispositivi IoT. Inoltre, il benchmarking ci consente di confrontare le prestazioni del nostro modello con quelli concorrenti o versioni precedenti dello stesso modello. Questo confronto √® essenziale per prendere decisioni informate su quale modello implementare in un‚Äôapplicazione specifica.\nInfine, √® fondamentale garantire che le previsioni del modello non siano solo accurate, ma anche coerenti tra diversi dati. Il benchmarking aiuta a verificare l‚Äôaccuratezza e la coerenza del modello, assicurando che soddisfi i requisiti dell‚Äôapplicazione. Valuta inoltre la robustezza del modello, assicurando che possa gestire la variabilit√† dei dati del mondo reale e comunque fare previsioni accurate.\n\n\nMetriche\n\nPrecisione: La precisione √® una delle metriche pi√π importanti quando si confrontano i modelli di machine learning. Quantifica la percentuale di previsioni corrette effettuate dal modello rispetto ai valori o alle etichette reali. Ad esempio, se un modello di rilevamento dello spam riesce a classificare correttamente 95 messaggi e-mail su 100, la sua precisione verrebbe calcolata al 95%.\nLatenza: La latenza √® una metrica delle prestazioni che calcola il ritardo o l‚Äôintervallo di tempo tra la ricezione dell‚Äôinput e la produzione dell‚Äôoutput corrispondente da parte del sistema di apprendimento automatico. Un esempio che descrive chiaramente la latenza √® un‚Äôapplicazione di traduzione in tempo reale; se esiste un ritardo di mezzo secondo dal momento in cui un utente inserisce una frase al momento in cui l‚Äôapp visualizza il testo tradotto, la latenza del sistema √® di 0.5 secondi.\nLatency-Bounded Throughput: Il throughput limitato dalla latenza √® una metrica preziosa che combina gli aspetti di latenza e throughput, misurando il throughput massimo di un sistema pur rispettando un vincolo di latenza specificato. Ad esempio, in un‚Äôapplicazione di streaming video che utilizza un modello di apprendimento automatico per generare e visualizzare automaticamente i sottotitoli, il throughput limitato dalla latenza misurerebbe quanti frame video il sistema pu√≤ elaborare al secondo (throughput) garantendo al contempo che i sottotitoli vengano visualizzati con un ritardo non superiore a 1 secondo (latenza). Questa metrica √® particolarmente importante nelle applicazioni in tempo reale in cui soddisfare i requisiti di latenza √® fondamentale per l‚Äôesperienza utente.\nThroughput: Il throughput valuta la capacit√† del sistema misurando il numero di inferenze o previsioni che un modello di apprendimento automatico pu√≤ gestire entro un‚Äôunit√† di tempo specifica. Si consideri un sistema di riconoscimento vocale che utilizza una Recurrent Neural Network (RNN) come modello sottostante; se questo sistema riesce a elaborare e comprendere 50 diverse clip audio in un minuto, allora la sua velocit√† di elaborazione √® di 50 clip al minuto.\nEfficienza energetica: L‚Äôefficienza energetica √® una metrica che determina la quantit√† di energia consumata dal modello di apprendimento automatico per eseguire una singola inferenza. Un esempio lampante di ci√≤ sarebbe un modello di elaborazione del linguaggio naturale basato su un‚Äôarchitettura di rete Transformer; se utilizza 0,1 Joule di energia per tradurre una frase dall‚Äôinglese al francese, la sua efficienza energetica √® misurata a 0,1 Joule per inferenza.\nUtilizzo della memoria: L‚Äôutilizzo della memoria quantifica il volume di RAM necessario a un modello di apprendimento automatico per svolgere attivit√† di inferenza. Un esempio rilevante per illustrare questo sarebbe un sistema di riconoscimento facciale basato su una CNN; se un tale sistema richiede 150 MB di RAM per elaborare e riconoscere i volti all‚Äôinterno di un‚Äôimmagine, il suo utilizzo della memoria √® di 150 MB.\n\n\n\nI Benchmark\nEcco alcuni lavori originali che hanno gettato le basi fondamentali per lo sviluppo di benchmark sistematici per sistemi di apprendimento automatico inferenziale.\nMLPerf Inference Benchmark: MLPerf Inference √® una suite di benchmark completa che valuta le prestazioni dei modelli di apprendimento automatico durante la fase di inferenza. Comprende una variet√† di carichi di lavoro, tra cui classificazione delle immagini, rilevamento di oggetti ed elaborazione del linguaggio naturale, con l‚Äôobiettivo di fornire metriche standardizzate e approfondite per la valutazione di diversi sistemi di inferenza. Le sue metriche includono:\nMLPerf Inference √® una suite di benchmark completa che valuta le prestazioni dei modelli di apprendimento automatico durante la fase di inferenza. Comprende una variet√† di carichi di lavoro, tra cui classificazione delle immagini, rilevamento di oggetti ed elaborazione del linguaggio naturale, con l‚Äôobiettivo di fornire metriche standardizzate e approfondite per la valutazione di diversi sistemi di inferenza.\nMetriche:\n\nTempo di inferenza\nLatenza\nThroughput [Produttivit√†]\nPrecisione\nConsumo energetico\n\nAI Benchmark: AI Benchmark √® uno strumento di benchmarking che valuta le prestazioni dei modelli di intelligenza artificiale e apprendimento automatico su dispositivi mobili e piattaforme di edge computing. Include test per attivit√† di classificazione delle immagini, rilevamento di oggetti ed elaborazione del linguaggio naturale, fornendo un‚Äôanalisi dettagliata delle prestazioni di inferenza su diverse piattaforme hardware. Le sue metriche includono:\nAI Benchmark √® uno strumento di benchmarking che valuta le prestazioni dei modelli di intelligenza artificiale e di apprendimento automatico su dispositivi mobili e piattaforme di edge computing. Include test per attivit√† di classificazione delle immagini, rilevamento di oggetti ed elaborazione del linguaggio naturale, fornendo un‚Äôanalisi dettagliata delle prestazioni di inferenza su diverse piattaforme hardware.\nMetriche:\n\nTempo di inferenza\nLatenza\nConsumo energetico\nUtilizzo della memoria\nThroughput [Produttivit√†]\n\nToolkit OpenVINO: Il toolkit OpenVINO fornisce uno strumento di benchmark per misurare le prestazioni dei modelli di apprendimento profondo per varie attivit√†, come la classificazione delle immagini, il rilevamento degli oggetti e il riconoscimento facciale, su hardware Intel. Offre approfondimenti dettagliati sulle prestazioni di inferenza dei modelli su diverse configurazioni hardware. Le sue metriche includono:\nMetriche:\n\nTempo di inferenza\nThroughput [Produttivit√†]\nLatenza\nUtilizzo di CPU e GPU\n\n\n\nCaso d‚ÄôUso di Esempio\nSupponiamo che sia stato assegnato il compito di valutare le prestazioni di inferenza di un modello di rilevamento di oggetti su uno specifico dispositivo edge. Ecco come ci si potrebbe approcciare alla strutturazione di questo benchmark:\n\nDefinire l‚ÄôAttivit√†: In questo caso, l‚Äôattivit√† √® il rilevamento di oggetti in tempo reale su flussi video, identificando oggetti come veicoli, pedoni e segnali stradali.\nSelezionare il Benchmark: Per allinearsi all‚Äôobiettivo di valutare l‚Äôinferenza su un dispositivo edge, l‚ÄôAI Benchmark √® una scelta adatta. Fornisce un framework standardizzato specificamente per valutare le prestazioni di inferenza su hardware edge, rendendolo rilevante per questo scenario.\nIdentificare le Metriche Chiave: Ora, si determinano le metriche che aiuteranno a valutare le prestazioni di inferenza del modello. Per questo esempio, si potrebbero tracciare:\n\nTempo di Inferenza: Quanto tempo ci vuole per elaborare ogni fotogramma video?\nLatenza: Qual √® il ritardo nella generazione di bounding box per gli oggetti rilevati?\nConsumo Energetico: Quanta energia viene utilizzata durante l‚Äôinferenza?\nProduttivit√†: Quanti frame video vengono elaborati al secondo?\n\n\nMisurando queste metriche, si otterranno informazioni su quanto bene funziona il modello di rilevamento degli oggetti sul dispositivo edge. Ci√≤ pu√≤ aiutare a identificare eventuali colli di bottiglia, come l‚Äôelaborazione lenta dei frame o l‚Äôelevato consumo energetico, e a evidenziare aree per una potenziale ottimizzazione per migliorare le prestazioni in tempo reale.\n\n\n\n\n\n\nEsercizio¬†11.2: Benchmark di Inferenza - MLPerf\n\n\n\n\n\nPrepararsi a mettere alla prova i propri modelli di intelligenza artificiale! MLPerf √® come le Olimpiadi per le prestazioni del machine learning. In questo Colab, utilizzeremo un toolkit chiamato CK per eseguire benchmark MLPerf ufficiali, misurare la velocit√† e l‚Äôaccuratezza di un proprio modello e persino utilizzare TVM per dargli una spinta super veloce. Pronti a vedere il modello vincere la sua medaglia?\n\n\n\n\n\n\n\n11.4.5 Selezione delle Attivit√† di Benchmark\nLa selezione di attivit√† rappresentative per il benchmarking dei sistemi di machine learning √® complessa a causa delle diverse applicazioni, tipi di dati e requisiti nei diversi domini. L‚Äôapprendimento automatico viene applicato in settori quali sanit√†, finanza, elaborazione del linguaggio naturale e visione artificiale, ognuno con attivit√† uniche che potrebbero non essere pertinenti o paragonabili ad altre. Le principali sfide nella selezione delle attivit√† includono:\n\nDiversit√† di Applicazioni e Tipi di Dati: Le attivit√† nei vari domini coinvolgono diversi tipi di dati (ad esempio testo, immagini, video) e qualit√†, rendendo difficile trovare benchmark che rappresentino universalmente le sfide dell‚Äôapprendimento automatico.\nComplessit√† delle Attivit√† e Necessit√† di Risorse: Le attivit√† variano in complessit√† e richieste di risorse, con alcune che richiedono una notevole potenza di calcolo e modelli sofisticati, mentre altre possono essere affrontate con risorse e metodi pi√π semplici.\nProblemi di Privacy: Le attivit√† che coinvolgono dati sensibili, come cartelle cliniche o informazioni personali, introducono problemi etici e di privacy, rendendole inadatte per benchmark generali.\nMetriche di Valutazione: Le metriche delle prestazioni variano notevolmente tra le attivit√† e i risultati di un‚Äôattivit√† spesso non si generalizzano ad altre, complicando i confronti e limitando le informazioni da un‚Äôattivit√† di benchmarking a un‚Äôaltra.\n\nAffrontare queste sfide √® essenziale per progettare benchmark significativi che siano pertinenti tra le diverse attivit√† incontrate nell‚Äôapprendimento automatico, assicurando che i benchmark forniscano informazioni utili e generalizzabili sia per la formazione che per l‚Äôinferenza.\n\n\n11.4.6 Misura dell‚ÄôEfficienza Energetica\nCon l‚Äôespansione delle capacit√† di apprendimento automatico, sia nel training che nell‚Äôinferenza, le preoccupazioni relative all‚Äôaumento del consumo energetico e al suo impatto ecologico si sono intensificate. Affrontare la sostenibilit√† dei sistemi ML, un argomento esplorato pi√π approfonditamente nel capitolo IA Sostenibile, √® quindi diventata una priorit√† fondamentale. Questa attenzione alla sostenibilit√† ha portato allo sviluppo di benchmark standardizzati progettati per misurare con precisione l‚Äôefficienza energetica. Tuttavia, la standardizzazione di queste metodologie pone delle sfide dovute alla necessit√† di adattarsi a scale molto diverse, dal consumo di microwatt dei dispositivi TinyML alle richieste di megawatt dei sistemi di training dei data center. Inoltre, per garantire che il benchmarking sia equo e riproducibile √® necessario adattarsi alla vasta gamma di configurazioni hardware e architetture in uso oggi.\nUn esempio √® la metodologia di benchmarking MLPerf Power (Tschand et al. 2024), che affronta queste sfide adattando le metodologie per data center, edge inference e tiny inference systems, misurando al contempo il consumo energetico nel modo pi√π completo possibile per ogni scala. Questa metodologia si adatta a una variet√† di hardware, dalle CPU generiche agli acceleratori AI specializzati, mantenendo principi di misurazione uniformi per garantire che i confronti siano equi e accurati su diverse piattaforme.\nFigura¬†11.3 illustra i limiti di misurazione dell‚Äôalimentazione per diverse scale di sistema, dai dispositivi TinyML ai nodi di inferenza e ai rack di training. Ciascun esempio evidenzia i componenti all‚Äôinterno del limite di misurazione e quelli al di fuori di esso. Questa configurazione consente una riflessione accurata dei veri costi energetici associati all‚Äôesecuzione di carichi di lavoro ML in vari scenari del mondo reale e garantisce che il benchmark catturi l‚Äôintero spettro di consumo energetico.\n\n\n\n\n\n\nFigura¬†11.3: Diagramma di misurazione del sistema MLPerf Power. Fonte: Tschand et al. (2024).\n\n\nTschand, Arya, Arun Tejusve Raghunath Rajan, Sachin Idgunji, Anirban Ghosh, Jeremy Holleman, Csaba Kiraly, Pawan Ambalkar, et al. 2024. ¬´MLPerf Power: Benchmarking the Energy Efficiency of Machine Learning Systems from Microwatts to Megawatts for Sustainable AI¬ª. arXiv preprint arXiv:2410.12032, ottobre. http://arxiv.org/abs/2410.12032v1.\n\n\n√à importante notare che l‚Äôottimizzazione di un sistema per le prestazioni potrebbe non portare all‚Äôesecuzione pi√π efficiente dal punto di vista energetico. Spesso, sacrificare una piccola quantit√† di prestazioni o accuratezza pu√≤ portare a guadagni significativi nell‚Äôefficienza energetica, evidenziando l‚Äôimportanza di un benchmarking accurato delle metriche della potenza. Le future intuizioni dal benchmarking dell‚Äôefficienza energetica e della sostenibilit√† ci consentiranno di ottimizzare per sistemi ML pi√π sostenibili.\n\n\n11.4.7 Esempio di Benchmark\nPer illustrare correttamente i componenti di un benchmark di sistema, possiamo esaminare il benchmark di individuazione delle parole chiave in MLPerf Tiny e spiegare la motivazione alla base di ogni decisione.\n\nTask\nL‚Äôindividuazione delle parole chiave √® stata selezionata come attivit√† perch√© √® un caso d‚Äôuso comune in TinyML che √® stato ben consolidato per anni. Inoltre, l‚Äôhardware tipico utilizzato per l‚Äôindividuazione delle parole chiave differisce sostanzialmente dalle offerte di altri benchmark, come l‚Äôattivit√† di riconoscimento vocale di MLPerf Inference.\n\n\nIl Dataset\nGoogle Speech Commands (Warden 2018) √® stato selezionato come il miglior dataset per rappresentare l‚Äôattivit√†. Il dataset √® ben consolidato nella comunit√† di ricerca e ha una licenza permissiva, che consente di utilizzarlo facilmente in un benchmark.\n\nWarden, Pete. 2018. ¬´Speech Commands: A Dataset for Limited-Vocabulary Speech Recognition¬ª. ArXiv preprint abs/1804.03209 (aprile). http://arxiv.org/abs/1804.03209v1.\n\n\nModello\nIl componente principale successivo √® il modello, che funger√† da carico di lavoro primario per il benchmark. Il modello dovrebbe essere ben consolidato come soluzione per l‚Äôattivit√† selezionata piuttosto che una soluzione all‚Äôavanguardia. Il modello selezionato √® un semplice modello di convoluzione separabile in profondit√†. Questa architettura non √® la soluzione all‚Äôavanguardia per l‚Äôattivit√†, ma √® ben consolidata e non progettata per una piattaforma hardware specifica come molte soluzioni all‚Äôavanguardia. Nonostante sia un benchmark di inferenza, stabilisce anche una ricetta di training di riferimento per essere completamente riproducibile e trasparente.\n\n\nMetriche\nLa latenza √® stata selezionata come metrica primaria per il benchmark, poich√© i sistemi di individuazione delle parole chiave devono reagire rapidamente per mantenere la soddisfazione dell‚Äôutente. Inoltre, dato che i sistemi TinyML sono spesso alimentati a batteria, il consumo energetico viene misurato per garantire l‚Äôefficienza della piattaforma hardware. L‚Äôaccuratezza del modello viene misurata anche per garantire che le ottimizzazioni applicate da un submitter, come la quantizzazione, non degradino l‚Äôaccuratezza oltre una soglia.\n\n\nBenchmark Harness\nMLPerf Tiny utilizza EEMBCs EnergyRunner benchmark harness per caricare gli input nel modello e isolare e misurare il consumo energetico del dispositivo. Quando si misura il consumo energetico, √® fondamentale selezionare un ‚Äúharness‚Äù [imbracatura] che sia accurato ai livelli di potenza previsti dei dispositivi sottoposti a test e sufficientemente semplice da non diventare un peso per i partecipanti al benchmark.\n\n\nLa Baseline\nGli invii di baseline sono fondamentali per contestualizzare i risultati e come punto di riferimento per aiutare i partecipanti a iniziare. L‚Äôinvio di base dovrebbe dare priorit√† alla semplicit√† e alla leggibilit√† rispetto alle prestazioni avanzate. L‚Äôindividuazione della parola chiave della baseline utilizza un microcontrollore STM standard come hardware e TensorFlow Lite come microcontrollore (David et al. 2021) come framework di inferenza.\n\nDavid, Robert, Jared Duke, Advait Jain, Vijay Janapa Reddi, Nat Jeffries, Jian Li, Nick Kreeger, et al. 2021. ¬´Tensorflow lite micro: Embedded machine learning for tinyml systems¬ª. Proceedings of Machine Learning and Systems 3: 800‚Äì811.\n\n\n\n11.4.8 Sfide e Limitazioni\nSebbene il benchmarking fornisca una metodologia strutturata per la valutazione delle prestazioni in domini complessi come l‚Äôintelligenza artificiale e l‚Äôinformatica, il processo pone anche diverse sfide. Se non affrontati correttamente, questi ostacoli possono minare la credibilit√† e l‚Äôaccuratezza dei risultati del benchmarking. Alcune delle difficolt√† predominanti affrontate nel benchmarking includono quanto segue:\n\nCopertura incompleta del problema: Le attivit√† di benchmarking potrebbero non rappresentare completamente lo spazio del problema. Ad esempio, i set di dati di classificazione delle immagini comuni come CIFAR-10 hanno una diversit√† limitata nei tipi di immagini. Gli algoritmi ottimizzati per tali benchmark potrebbero non riuscire a generalizzare bene con i set di dati del mondo reale.\nInsignificanza statistica: I benchmark devono avere prove e campioni di dati sufficienti per produrre risultati statisticamente significativi. Ad esempio, il benchmarking di un modello OCR su solo poche scansioni di testo potrebbe non catturare adeguatamente i suoi veri tassi di errore.\nRiproducibilit√† limitata: Variazioni di hardware, versioni software, basi di codice e altri fattori possono ridurre la riproducibilit√† dei risultati di benchmark. MLPerf affronta questo problema fornendo implementazioni di riferimento e specifiche ambientali.\nDisallineamento con gli obiettivi finali: I benchmark che si concentrano solo su metriche di velocit√† o accuratezza possono disallineare gli obiettivi reali come costi ed efficienza energetica. I benchmark devono riflettere tutti gli assi prestazionali critici.\nRapida obsolescenza: A causa del rapido ritmo dei progressi nell‚Äôintelligenza artificiale e nell‚Äôinformatica, i benchmark e i loro set di dati possono rapidamente diventare obsoleti. Mantenere benchmark aggiornati √® quindi una sfida persistente.\n\nMa di tutte queste, la sfida pi√π importante √® l‚Äôingegneria dei benchmark.\n\nLotteria Hardware\nLa lotteria hardware, descritta per la prima volta da Hooker (2021), si riferisce alla situazione in cui il successo o l‚Äôefficienza di un modello di apprendimento automatico sono significativamente influenzati dalla sua compatibilit√† con l‚Äôhardware sottostante (Chu et al. 2021). Alcuni modelli hanno prestazioni eccezionali non perch√© sono intrinsecamente superiori, ma perch√© sono ottimizzati per caratteristiche hardware specifiche, come le capacit√† di elaborazione parallela delle unit√† di elaborazione grafica (GPU) o delle unit√† di elaborazione tensoriale (TPU).\n\nHooker, Sara. 2021. ¬´The hardware lottery¬ª. Communications of the ACM 64 (12): 58‚Äì65. https://doi.org/10.1145/3467017.\nAd esempio, Figura¬†11.4 confronta le prestazioni dei modelli su diverse piattaforme hardware. I modelli multi-hardware mostrano risultati comparabili a ‚ÄúMobileNetV3 Large min‚Äù sia sulle configurazioni CPU uint8 che GPU. Tuttavia, questi modelli multi-hardware dimostrano miglioramenti significativi delle prestazioni rispetto alla baseline MobileNetV3 Large quando eseguiti su hardware EdgeTPU e DSP. Ci√≤ sottolinea l‚Äôefficienza variabile dei modelli multi-hardware in ambienti di elaborazione specializzati.\n\n\n\n\n\n\nFigura¬†11.4: Compromessi tra precisione e latenza di pi√π modelli ML e modalit√† di funzionamento su vari hardware. Fonte: Chu et al. (2021)\n\n\nChu, Grace, Okan Arikan, Gabriel Bender, Weijun Wang, Achille Brighton, Pieter-Jan Kindermans, Hanxiao Liu, Berkin Akin, Suyog Gupta, e Andrew Howard. 2021. ¬´Discovering Multi-Hardware Mobile Models via Architecture Search¬ª. In 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), 3016‚Äì25. IEEE. https://doi.org/10.1109/cvprw53098.2021.00337.\n\n\nLa lotteria hardware pu√≤ introdurre sfide e pregiudizi nel benchmarking dei sistemi di apprendimento automatico, poich√© le prestazioni del modello non dipendono esclusivamente dall‚Äôarchitettura o dall‚Äôalgoritmo del modello ma anche dalla compatibilit√† e dalle sinergie con l‚Äôhardware sottostante. Ci√≤ pu√≤ rendere difficile confrontare equamente diversi modelli e identificare il modello migliore in base ai suoi meriti intrinseci. Pu√≤ anche portare a una situazione in cui la comunit√† converge su modelli che sono adatti all‚Äôhardware pi√π diffuso del momento, trascurando potenzialmente altri modelli che potrebbero essere superiori ma incompatibili con le attuali tendenze hardware.\n\n\nBenchmark Engineering\nLa lotteria hardware si verifica quando un modello di apprendimento automatico funziona in modo eccezionalmente bene o male su una configurazione hardware specifica a causa di compatibilit√† o incompatibilit√† impreviste. Il modello non √® esplicitamente progettato o ottimizzato per quell‚Äôhardware specifico dagli sviluppatori o dagli ingegneri; piuttosto, capita che si allinei o (non si allinei) con le capacit√† o le limitazioni dell‚Äôhardware. In questo caso, le prestazioni del modello sull‚Äôhardware sono un prodotto della coincidenza piuttosto che della progettazione.\nContrariamente alla lotteria hardware accidentale, il benchmark engineering implica l‚Äôottimizzazione o la progettazione deliberata di un modello di apprendimento automatico per funzionare eccezionalmente bene su hardware specifico, spesso per vincere benchmark o competizioni. Questa ottimizzazione intenzionale potrebbe includere la modifica dell‚Äôarchitettura, degli algoritmi o dei parametri del modello per sfruttare appieno le funzionalit√† e le capacit√† dell‚Äôhardware.\n\nProblema\nIl benchmark engineering si riferisce alla modifica o all‚Äôottimizzazione di un sistema di intelligenza artificiale per ottimizzare le prestazioni su test di benchmark specifici, spesso a scapito della generalizzabilit√† o delle prestazioni nel mondo reale. Ci√≤ pu√≤ includere la regolazione di iperparametri, dati di training o altri aspetti del sistema specificamente per ottenere punteggi elevati sulle metriche di benchmark senza necessariamente migliorare la funzionalit√† o l‚Äôutilit√† complessiva del sistema.\nLa motivazione alla base dell‚Äôingegneria dei benchmark spesso deriva dal desiderio di ottenere punteggi di prestazioni elevate per scopi di marketing o competitivi. Punteggi di benchmark elevati possono dimostrare la superiorit√† di un sistema di intelligenza artificiale rispetto ai concorrenti e possono essere un argomento chiave per la vendita per potenziali utenti o investitori. Questa pressione per ottenere buoni risultati nei benchmark a volte porta a dare priorit√† alle ottimizzazioni specifiche del benchmark rispetto a miglioramenti pi√π olistici del sistema.\nPu√≤ comportare diversi rischi e sfide. Uno dei rischi principali √® che il sistema di intelligenza artificiale possa funzionare meglio nelle applicazioni del mondo reale rispetto a quanto suggeriscono i punteggi di benchmark. Ci√≤ pu√≤ portare a insoddisfazione dell‚Äôutente, danni alla reputazione e potenziali problemi di sicurezza o etici. Inoltre, l‚Äôingegneria dei benchmark pu√≤ contribuire a una mancanza di trasparenza e responsabilit√† nella comunit√† dell‚Äôintelligenza artificiale, poich√© pu√≤ essere difficile discernere quanta parte delle prestazioni di un sistema di intelligenza artificiale sia dovuta a miglioramenti genuini rispetto a ottimizzazioni specifiche del benchmark.\nLa comunit√† AI deve dare priorit√† alla trasparenza e alla responsabilit√† per mitigare i rischi associati all‚Äôingegneria dei benchmark. Ci√≤ pu√≤ includere la divulgazione di eventuali ottimizzazioni o modifiche apportate specificamente per i test di benchmark e la fornitura di valutazioni pi√π complete dei sistemi AI che includono metriche delle prestazioni del mondo reale e punteggi di benchmark. I ricercatori e gli sviluppatori devono dare priorit√† a miglioramenti olistici dei sistemi AI che ne migliorino la generalizzabilit√† e la funzionalit√† in varie applicazioni anzich√© concentrarsi esclusivamente su ottimizzazioni specifiche del benchmark.\n\n\nProblemi\nUno dei problemi principali dell‚Äôingegneria del benchmark √® che pu√≤ compromettere le prestazioni reali dei sistemi di intelligenza artificiale. Quando gli sviluppatori si concentrano sull‚Äôottimizzazione dei loro sistemi per ottenere punteggi elevati in specifici test di benchmark, potrebbero trascurare altri importanti aspetti delle prestazioni del sistema, cruciali nelle applicazioni del mondo reale. Ad esempio, un sistema di intelligenza artificiale progettato per il riconoscimento delle immagini potrebbe essere progettato per funzionare eccezionalmente bene in un test di benchmark che include un set specifico di immagini, ma necessita di aiuto per riconoscere accuratamente immagini leggermente diverse da quelle nel set di test.\nUn‚Äôaltra area di miglioramento con l‚Äôingegneria di benchmark √® che pu√≤ comportare sistemi di intelligenza artificiale privi di generalizzabilit√†. In altre parole, mentre il sistema pu√≤ funzionare bene nel test di benchmark, potrebbe aver bisogno di aiuto per gestire una vasta gamma di input o scenari. Ad esempio, un modello di intelligenza artificiale sviluppato per l‚Äôelaborazione del linguaggio naturale potrebbe essere progettato per ottenere punteggi elevati in un test di benchmark che include un tipo specifico di testo, ma non riesce a elaborare accuratamente il testo che non rientra in quel tipo specifico.\nPu√≤ anche portare a risultati fuorvianti. Quando i sistemi di intelligenza artificiale sono progettati per funzionare bene nei test di benchmark, i risultati potrebbero non riflettere accuratamente le reali capacit√† del sistema. Questo pu√≤ essere problematico per gli utenti o gli investitori che si affidano ai punteggi di benchmark per prendere decisioni informate su quali sistemi di intelligenza artificiale utilizzare o in cui investire. Ad esempio, un sistema di intelligenza artificiale progettato per ottenere punteggi elevati in un test di benchmark per il riconoscimento vocale potrebbe dover essere pi√π in grado di riconoscere accuratamente il parlato in situazioni reali, portando gli utenti o gli investitori a prendere decisioni basate su informazioni imprecise.\n\n\nAttenuazione\nEsistono diversi modi per mitigare l‚Äôingegneria dei benchmark. La trasparenza nel processo di benchmarking √® fondamentale per mantenere l‚Äôaccuratezza e l‚Äôaffidabilit√† dei benchmark. Ci√≤ implica la divulgazione chiara delle metodologie, dei set di dati e dei criteri di valutazione utilizzati nei test di benchmark, nonch√© di eventuali ottimizzazioni o modifiche apportate al sistema di intelligenza artificiale ai fini del benchmark.\nUn modo per ottenere trasparenza √® attraverso l‚Äôuso di benchmark open source. I benchmark open source vengono resi disponibili al pubblico, consentendo a ricercatori, sviluppatori e altre parti interessate di esaminarli, criticarli e contribuire, garantendone cos√¨ l‚Äôaccuratezza e l‚Äôaffidabilit√†. Questo approccio collaborativo facilita anche la condivisione delle ‚Äúbest practice‚Äù e lo sviluppo di benchmark pi√π solidi e completi.\nIl design modulare di MLPerf Tiny si collega al problema dell‚Äôingegneria dei benchmark fornendo un approccio strutturato ma flessibile che incoraggia una valutazione equilibrata di TinyML. Nell‚Äôingegneria dei benchmark, i sistemi possono essere eccessivamente ottimizzati per benchmark specifici, portando a punteggi di prestazioni gonfiati che non si traducono necessariamente in efficacia nel mondo reale. Il design modulare di MLPerf Tiny mira ad affrontare questo problema consentendo ai collaboratori di scambiare e testare componenti specifici all‚Äôinterno di un framework standardizzato, come hardware, tecniche di quantizzazione o modelli di inferenza. Le implementazioni di riferimento, evidenziate in verde e arancione in Figura¬†11.5, forniscono una base di riferimento per i risultati, consentendo test flessibili ma controllati specificando quali componenti possono essere modificati. Questa struttura supporta trasparenza e flessibilit√†, consentendo di concentrarsi su miglioramenti genuini piuttosto che su ottimizzazioni specifiche del benchmark.\n\n\n\n\n\n\nFigura¬†11.5: Design modulare del benchmark MLPerf Tiny, che mostra l‚Äôimplementazione di riferimento con componenti modificabili. Questo approccio modulare consente test flessibili e mirati mantenendo una base di riferimento standardizzata. Fonte: Banbury et al. (2021).\n\n\nBanbury, Colby, Vijay Janapa Reddi, Peter Torelli, Jeremy Holleman, Nat Jeffries, Csaba Kiraly, Pietro Montino, et al. 2021. ¬´MLPerf Tiny Benchmark¬ª. arXiv preprint arXiv:2106.07597, giugno. http://arxiv.org/abs/2106.07597v4.\n\n\nUn altro metodo per ottenere trasparenza √® attraverso la revisione paritaria dei benchmark. Ci√≤ comporta che esperti indipendenti esaminino e convalidino la metodologia, i set di dati e i risultati del benchmark per garantirne la credibilit√† e l‚Äôaffidabilit√†. La revisione paritaria pu√≤ fornire un mezzo prezioso per verificare l‚Äôaccuratezza dei test di benchmark e contribuire a creare fiducia nei risultati.\nLa standardizzazione dei benchmark √® un‚Äôaltra importante soluzione per mitigare l‚Äôingegneria dei benchmark. I benchmark standardizzati forniscono un quadro comune per la valutazione dei sistemi di intelligenza artificiale, garantendo coerenza e comparabilit√† tra diversi sistemi e applicazioni. Ci√≤ pu√≤ essere ottenuto sviluppando standard e ‚Äúbest practice‚Äù per l‚Äôintero settore per il benchmarking e tramite metriche e criteri di valutazione comuni.\nAnche la verifica da parte di terze parti dei risultati pu√≤ essere preziosa per mitigare l‚Äôingegneria dei benchmark. Ci√≤ comporta che una terza parte indipendente verifichi i risultati di un test di benchmark per garantirne la credibilit√† e l‚Äôaffidabilit√†. La verifica di terze parti pu√≤ creare fiducia nei risultati e fornire un mezzo prezioso per convalidare le prestazioni e le capacit√† dei sistemi di intelligenza artificiale.",
    "crumbs": [
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Benchmarking dell'IA</span>"
    ]
  },
  {
    "objectID": "contents/core/benchmarking/benchmarking.it.html#benchmarking-del-modello",
    "href": "contents/core/benchmarking/benchmarking.it.html#benchmarking-del-modello",
    "title": "11¬† Benchmarking dell‚ÄôIA",
    "section": "11.5 Benchmarking del Modello",
    "text": "11.5 Benchmarking del Modello\nIl benchmarking dei modelli di machine learning √® importante per determinare l‚Äôefficacia e l‚Äôefficienza di vari algoritmi di apprendimento automatico nella risoluzione di compiti o problemi specifici. Analizzando i risultati ottenuti dal benchmarking, sviluppatori e ricercatori possono identificare i punti di forza e di debolezza dei loro modelli, portando a decisioni pi√π informate sulla selezione del modello e su un‚Äôulteriore ottimizzazione.\nL‚Äôevoluzione e il progresso dei modelli di apprendimento automatico sono intrinsecamente collegati alla disponibilit√† e alla qualit√† dei set di dati. Nell‚Äôapprendimento automatico, i dati fungono da materia prima che alimenta gli algoritmi, consentendo loro di apprendere, adattarsi e, in definitiva, eseguire compiti che erano tradizionalmente di dominio degli esseri umani. Pertanto, √® importante comprendere questa storia.\n\n11.5.1 Contesto Storico\nI dataset di apprendimento automatico hanno una storia ricca e si sono evoluti in modo significativo nel corso degli anni, crescendo in dimensioni, complessit√† e diversit√† per soddisfare le richieste sempre crescenti del settore. Diamo un‚Äôocchiata pi√π da vicino a questa evoluzione, partendo da uno dei primi e pi√π iconici set di dati: MNIST.\n\nMNIST (1998)\nIl dataset MNIST, creato da Yann LeCun, Corinna Cortes e Christopher J.C. Burges nel 1998, pu√≤ essere considerato una pietra miliare nella storia dei dataset di machine learning. Comprende 70.000 immagini in scala di grigi da 28x28 pixel etichettate di cifre scritte a mano (0-9). MNIST √® stato ampiamente utilizzato per il benchmarking degli algoritmi nell‚Äôelaborazione delle immagini e nell‚Äôapprendimento automatico come punto di partenza per molti ricercatori e professionisti. Figura¬†11.6 mostra alcuni esempi di cifre scritte a mano.\n\n\n\n\n\n\nFigura¬†11.6: Cifre scritte a mano in MNIST. Fonte: Suvanjanprasai\n\n\n\n\n\nImageNet (2009)\nFacciamo un salto al 2009 e vediamo l‚Äôintroduzione di ImageNet, che ha segnato un balzo significativo nella scala e nella complessit√† dei dataset. ImageNet √® composto da oltre 14 milioni di immagini etichettate che abbracciano pi√π di 20.000 categorie. Fei-Fei Li e il suo team lo hanno sviluppato per far progredire il riconoscimento degli oggetti e la ricerca sulla visione artificiale. Il dataset √® diventato sinonimo della ImageNet Large Scale Visual Recognition Challenge (LSVRC), una competizione annuale cruciale nello sviluppo di modelli di deep learning, tra cui il famoso AlexNet nel 2012.\n\n\nCOCO (2014)\nIl Common Objects in Context (COCO) dataset (Lin et al. 2014), rilasciato nel 2014, ha ulteriormente ampliato il panorama dei set di dati di apprendimento automatico introducendo un set pi√π ricco di annotazioni. COCO √® costituito da immagini contenenti scene complesse con pi√π oggetti e ogni immagine √® annotata con riquadri di delimitazione degli oggetti, maschere di segmentazione e didascalie, come mostrato in Figura¬†11.7. Questo set di dati √® stato determinante nel far progredire la ricerca nel rilevamento degli oggetti, nella segmentazione e nella didascalia delle immagini.\n\nLin, Tsung-Yi, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll√°r, e C. Lawrence Zitnick. 2014. ¬´Microsoft COCO: Common Objects in Context¬ª. In Computer Vision ‚Äì ECCV 2014, 740‚Äì55. Springer; Springer International Publishing. https://doi.org/10.1007/978-3-319-10602-1\\_48.\n\n\n\n\n\n\nFigura¬†11.7: Immagini di esempio dal set di dati COCO. Fonte: Coco\n\n\n\n\n\nGPT-3 (2020)\nSebbene gli esempi sopra riportati si concentrino principalmente sui dataset di immagini, si sono verificati anche sviluppi significativi nei dataset di testo. Un esempio degno di nota √® GPT-3 (Brown et al. 2020), sviluppato da OpenAI. GPT-3 √® un modello linguistico addestrato su testo Internet eterogeneo. Sebbene il dataset utilizzato per addestrare GPT-3 non sia disponibile al pubblico, il modello stesso, costituito da 175 miliardi di parametri, √® una testimonianza della scala e della complessit√† dei moderni dataset e modelli di apprendimento automatico.\n\nBrown, Tom B., Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, et al. 2020. ¬´Language Models are Few-Shot Learners¬ª. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, a cura di Hugo Larochelle, Marc‚ÄôAurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, e Hsuan-Tien Lin. https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html.\n\n\nPresente e Futuro\nOggi disponiamo di una pletora di dataset che abbracciano vari domini, tra cui sanit√†, finanza, scienze sociali e altro ancora. Le seguenti caratteristiche ci aiutano a classificare lo spazio e la crescita dei dataset di apprendimento automatico che alimentano lo sviluppo del modello.\n\nDiversit√† dei Set di Dati: La variet√† di set di dati disponibili per ricercatori e ingegneri si √® ampliata notevolmente, coprendo molti campi, tra cui l‚Äôelaborazione del linguaggio naturale, il riconoscimento delle immagini e altro ancora. Questa diversit√† ha alimentato lo sviluppo di modelli di apprendimento automatico specializzati, su misura per attivit√† specifiche, come la traduzione, il riconoscimento vocale e il riconoscimento facciale.\nVolume di Dati: L‚Äôenorme volume di dati che √® diventato disponibile nell‚Äôera digitale ha anche svolto un ruolo cruciale nel progresso dei modelli di apprendimento automatico. I grandi set di dati consentono ai modelli di catturare la complessit√† e le sfumature dei fenomeni del mondo reale, portando a previsioni pi√π accurate e affidabili.\nQualit√† e Pulizia dei Dati: La qualit√† dei dati √® un altro fattore critico che influenza le prestazioni dei modelli di apprendimento automatico. Set di dati puliti, ben etichettati e imparziali sono essenziali per modelli di addestramento solidi ed equi.\nAccesso Aperto ai Dati: La disponibilit√† di set di dati ‚Äúopen-access‚Äù ha contribuito in modo significativo anche al progresso dell‚Äôapprendimento automatico. I dati ‚Äúaperti‚Äù consentono ai ricercatori di tutto il mondo di collaborare, condividere approfondimenti e basarsi sul lavoro degli altri, portando a un‚Äôinnovazione pi√π rapida e allo sviluppo di modelli pi√π avanzati.\nProblemi di Etica e Privacy: Man mano che i set di dati crescono in dimensioni e complessit√†, le considerazioni etiche e i problemi di privacy diventano sempre pi√π importanti. √à in corso un dibattito sull‚Äôequilibrio tra lo sfruttamento dei dati per i progressi dell‚Äôapprendimento automatico e la protezione dei diritti alla privacy degli individui.\n\nLo sviluppo di modelli di apprendimento automatico si basa in larga misura sulla disponibilit√† di set di dati diversificati, grandi, di alta qualit√† e ad accesso libero. Mentre andiamo avanti, affrontare le considerazioni etiche e le preoccupazioni sulla privacy associate all‚Äôuso di grandi set di dati √® fondamentale per garantire che le tecnologie di apprendimento automatico siano vantaggiose per la societ√†. C‚Äô√® una crescente consapevolezza che i dati agiscono come carburante per l‚Äôapprendimento automatico, guidando e alimentando lo sviluppo di modelli di apprendimento automatico. Di conseguenza, si sta ponendo maggiore attenzione sullo sviluppo dei set di dati stessi. Esploreremo questo aspetto in modo pi√π dettagliato nella sezione del benchmarking dei dati.\n\n\n\n11.5.2 Metriche del Modello\nLa valutazione del modello di machine learning si √® evoluta da un focus ristretto sulla precisione a un approccio pi√π completo che considera una serie di fattori, da considerazioni etiche e applicabilit√† nel mondo reale a vincoli pratici come dimensioni ed efficienza del modello. Questo cambiamento riflette la maturazione del campo poich√© i modelli di apprendimento automatico vengono sempre pi√π applicati in scenari reali diversi e complessi.\n\nPrecisione\nLa precisione √® una delle metriche pi√π intuitive e comunemente utilizzate per valutare i modelli di apprendimento automatico. Nelle prime fasi dell‚Äôapprendimento automatico, la precisione era spesso la metrica principale, se non l‚Äôunica, considerata quando si valutavano le prestazioni del modello. Tuttavia, con l‚Äôevoluzione del campo, √® diventato chiaro che fare affidamento esclusivamente sull‚Äôaccuratezza pu√≤ essere fuorviante, soprattutto in applicazioni in cui determinati tipi di errori comportano conseguenze significative.\nSi consideri l‚Äôesempio di un modello di diagnosi medica con una precisione del 95%. Sebbene a prima vista possa sembrare impressionante, dobbiamo guardare pi√π a fondo per valutare appieno le prestazioni del modello. Supponiamo che il modello non riesca a diagnosticare accuratamente condizioni gravi che, sebbene rare, possono avere gravi conseguenze; la sua elevata precisione potrebbe non essere cos√¨ significativa. Un esempio ben noto di questa limitazione √® il modello di retinopatia diabetica di Google. Sebbene abbia raggiunto un‚Äôelevata accuratezza in laboratorio, ha incontrato delle difficolt√† quando √® stato implementato in cliniche reali in Thailandia, dove le variazioni nelle popolazioni di pazienti, la qualit√† delle immagini e i fattori ambientali ne hanno ridotto l‚Äôefficacia. Questo esempio illustra che anche i modelli con elevata accuratezza devono essere testati per la loro capacit√† di generalizzare in condizioni diverse e imprevedibili per garantire affidabilit√† e impatto in contesti reali.\nAllo stesso modo, se il modello funziona bene in media ma mostra significative disparit√† nelle prestazioni tra diversi gruppi demografici, anche questo sarebbe motivo di preoccupazione. L‚Äôevoluzione dell‚Äôapprendimento automatico ha quindi visto uno spostamento verso un approccio pi√π olistico alla valutazione del modello, tenendo conto non solo dell‚Äôaccuratezza, ma anche di altri fattori cruciali come la correttezza, trasparenza e applicabilit√† nel mondo reale. Un esempio lampante √® il progetto Gender Shades del MIT Media Lab, guidato da Joy Buolamwini, che evidenzia i pregiudizi ottenendo risultati migliori sui volti maschili e dalla pelle pi√π chiara rispetto ai volti femminili e dalla pelle pi√π scura.\nSebbene l‚Äôaccuratezza resti essenziale per valutare i modelli di apprendimento automatico, √® necessario un approccio completo per valutare appieno le prestazioni. Ci√≤ include metriche aggiuntive per l‚Äôequit√†, la trasparenza e l‚Äôapplicabilit√† nel mondo reale, insieme a test rigorosi su diversi set di dati per identificare e affrontare i pregiudizi. Questo approccio di valutazione olistico riflette la crescente consapevolezza del settore delle implicazioni nel mondo reale nell‚Äôimplementazione dei modelli.\n\n\nEquit√†\nIl ‚Äúfairness‚Äù equit√† nell‚Äôapprendimento automatico implica la garanzia che i modelli funzionino in modo coerente su diversi gruppi, in particolare in applicazioni ad alto impatto come approvazioni di prestiti, assunzioni e giustizia penale. Affidarsi esclusivamente all‚Äôaccuratezza pu√≤ essere fuorviante se il modello mostra risultati distorti su gruppi demografici. Ad esempio, un modello di approvazione dei prestiti con elevata accuratezza potrebbe comunque negare sistematicamente i prestiti a determinati gruppi, sollevando dubbi sulla sua equit√†.\nIl ‚Äúbias‚Äù [distorsione] nei modelli pu√≤ sorgere direttamente, quando attributi sensibili come razza o genere influenzano le decisioni, o indirettamente, quando caratteristiche neutre sono correlate a questi attributi, influenzando i risultati. Affidarsi semplicemente all‚Äôaccuratezza pu√≤ essere insufficiente quando si valutano i modelli. Ad esempio, si consideri un modello di approvazione dei prestiti con un tasso di accuratezza del 95%. Sebbene questa cifra possa sembrare impressionante a prima vista, non rivela come il modello si comporta nei diversi gruppi demografici. Un esempio ben noto √® lo strumento COMPAS utilizzato nel sistema di giustizia penale degli Stati Uniti, che ha mostrato pregiudizi razziali nel prevedere la recidiva nonostante non utilizzasse esplicitamente la razza come variabile.\nPer affrontare l‚Äôequit√† √® necessario analizzare le prestazioni di un modello tra i gruppi, identificare i pregiudizi e applicare misure correttive come il ribilanciamento dei set di dati o l‚Äôutilizzo di algoritmi consapevoli dell‚Äôequit√†. Ricercatori e professionisti sviluppano continuamente metriche e metodologie su misura per casi d‚Äôuso specifici per valutare la correttezza e l‚Äôequit√† in scenari del mondo reale. Ad esempio, l‚Äôanalisi di impatto disparato, la parit√† demografica e le pari opportunit√† sono alcune delle metriche impiegate per valutare l‚Äôequit√†/correttezza. Inoltre, la trasparenza e l‚Äôinterpretabilit√† dei modelli sono fondamentali per raggiungere la correttezza. Strumenti come AI Fairness 360 e Fairness Indicators aiutano a spiegare come un modello prende decisioni, consentendo agli sviluppatori di rilevare e correggere problemi di equit√† nei modelli di apprendimento automatico.\nSebbene l‚Äôaccuratezza sia una metrica preziosa, non sempre fornisce il quadro completo; la valutazione dell‚Äôequit√† garantisce che i modelli siano efficaci in scenari del mondo reale. Garantire l‚Äôequit√†/correttezza nei modelli di apprendimento automatico, in particolare nelle applicazioni che hanno un impatto significativo sulla vita delle persone, richiede una rigorosa valutazione delle prestazioni del modello in gruppi diversi, un‚Äôattenta identificazione e attenuazione dei pregiudizi e l‚Äôimplementazione di misure di trasparenza e interpretabilit√†.\n\n\nComplessit√†\n\nParametri\nNelle fasi iniziali del machine learning, il benchmarking dei modelli si basava spesso sui conteggi dei parametri come proxy [sostituto] per la complessit√† del modello. La logica era che pi√π parametri in genere portano a un modello pi√π complesso, che dovrebbe, a sua volta, fornire prestazioni migliori. Tuttavia, questo approccio trascura i costi pratici associati all‚Äôelaborazione di modelli di grandi dimensioni. Man mano che aumenta il numero dei parametri, aumentano anche le risorse computazionali richieste, rendendo tali modelli poco pratici per l‚Äôimplementazione in scenari reali, in particolare su dispositivi con potenza di elaborazione limitata.\nAffidarsi ai conteggi dei parametri come proxy per la complessit√† del modello non riesce a considerare anche l‚Äôefficienza del modello. Un modello ben ottimizzato con meno parametri pu√≤ spesso ottenere prestazioni paragonabili o addirittura superiori a un modello pi√π grande. Ad esempio, MobileNets, sviluppato da Google, √® una famiglia di modelli progettati specificamente per dispositivi mobili ed edge. Hanno utilizzato convoluzioni separabili in profondit√† per ridurre il numero dei parametri e le richieste computazionali mantenendo comunque prestazioni elevate.\nAlla luce di queste limitazioni, il settore si √® spostato verso un approccio pi√π olistico al benchmarking dei modelli che considera i conteggi dei parametri e altri fattori cruciali come le operazioni in virgola mobile al secondo (FLOP), il consumo di memoria e la latenza. Questo approccio completo bilancia le prestazioni con la distribuibilit√†, assicurando che i modelli non siano solo accurati, ma anche efficienti e adatti alle applicazioni del mondo reale.\n\n\nFLOP\nI ‚Äúfloating-point operation‚Äù (FLOP), o operazioni in virgola mobile al secondo, sono diventati una metrica critica per rappresentare il carico computazionale di un modello. Tradizionalmente, il numero dei parametri veniva utilizzato come indicatore per la complessit√† del modello, basandosi sul presupposto che pi√π parametri avrebbero prodotto prestazioni migliori. Tuttavia, questo approccio trascura il costo computazionale dell‚Äôelaborazione di questi parametri, che pu√≤ influire sull‚Äôusabilit√† di un modello in scenari del mondo reale con risorse limitate.\nI FLOP misurano il numero di operazioni in virgola mobile eseguite da un modello per generare una previsione. Un modello con molti FLOP richiede risorse computazionali sostanziali per elaborare il vasto numero di operazioni, il che potrebbe renderlo poco pratico per alcune applicazioni. Al contrario, un modello con un numero di FLOP inferiore √® pi√π leggero e pu√≤ essere facilmente distribuito in scenari in cui le risorse computazionali sono limitate. Figura¬†11.8, da (Bianco et al. 2018), illustra il compromesso tra accuratezza di ImageNet, FLOP e numero dei parametri, dimostrando che alcune architetture raggiungono un‚Äôefficienza maggiore di altre.\n\n\n\n\n\n\nFigura¬†11.8: Un grafico che raffigura la top-1 Imagenet Accuracy rispetto al numero di FLOP di un modello insieme al conteggio dei parametri del modello. La figura mostra un compromesso complessivo tra complessit√† e accuratezza del modello, sebbene alcune architetture del modello siano pi√π efficienti di altre. Fonte: Bianco et al. (2018).\n\n\nBianco, Simone, Remi Cadene, Luigi Celona, e Paolo Napoletano. 2018. ¬´Benchmark Analysis of Representative Deep Neural Network Architectures¬ª. IEEE Access 6: 64270‚Äì77. https://doi.org/10.1109/access.2018.2877890.\n\n\nConsideriamo un esempio. BERT‚ÄîBidirectional Encoder Representations from Transformers (Devlin et al. 2019)‚Äî√® un modello di elaborazione del linguaggio naturale molto diffuso, con oltre 340 milioni di parametri, il che lo rende un modello di grandi dimensioni con elevata accuratezza e prestazioni impressionanti in diverse attivit√†. Tuttavia, le dimensioni di BERT, unite al suo elevato numero di FLOP, lo rendono un modello computazionalmente intensivo che potrebbe non essere adatto per applicazioni in tempo reale o per l‚Äôimplementazione su dispositivi edge con capacit√† computazionali limitate. Alla luce di ci√≤, c‚Äô√® stato un crescente interesse nello sviluppo di modelli pi√π piccoli in grado di raggiungere livelli di prestazioni simili alle loro controparti pi√π grandi, pur essendo pi√π efficienti nel carico computazionale. DistilBERT, ad esempio, √® una versione pi√π piccola di BERT che mantiene il 97% delle sue prestazioni, pur essendo il 40% pi√π piccola in termini di numero di parametri. La riduzione delle dimensioni si traduce anche in un numero di FLOP inferiore, rendendo DistilBERT una scelta pi√π pratica per scenari con risorse limitate.\n\nDevlin, Jacob, Ming-Wei Chang, Kenton Lee, e Kristina Toutanova. 2019. ¬´None¬ª. In Proceedings of the 2019 Conference of the North, 4171‚Äì86. Minneapolis, Minnesota: Association for Computational Linguistics. https://doi.org/10.18653/v1/n19-1423.\nSebbene il numero dei parametri indichi la dimensione del modello, non cattura completamente il costo computazionale. I FLOP forniscono una misura pi√π accurata del carico computazionale, evidenziando i compromessi pratici nell‚Äôimplementazione del modello. Questo passaggio dal conteggio dei parametri ai FLOP riflette la crescente consapevolezza del settore delle sfide di implementazione in contesti diversi.\n\n\nEfficienza\nAnche le metriche di efficienza, come il consumo di memoria e la latenza/capacit√† di elaborazione, hanno acquisito importanza. Queste metriche sono particolarmente cruciali quando si distribuiscono modelli su dispositivi edge o in applicazioni in tempo reale, poich√© misurano la velocit√† con cui un modello pu√≤ elaborare i dati e la quantit√† di memoria richiesta. In questo contesto, le curve di Pareto vengono spesso utilizzate per visualizzare il compromesso tra diverse metriche, aiutando le parti interessate a decidere quale modello si adatta meglio alle loro esigenze.\n\n\n\n\n11.5.3 Lezioni Apprese\nIl benchmarking dei modelli ci ha offerto diverse preziose intuizioni che possono essere sfruttate per guidare l‚Äôinnovazione nei benchmark di sistema. La progressione dei modelli di apprendimento automatico √® stata profondamente influenzata dall‚Äôavvento delle classifiche e dalla disponibilit√† open source di modelli e set di dati. Questi elementi hanno svolto il ruolo di catalizzatori significativi, spingendo l‚Äôinnovazione e accelerando l‚Äôintegrazione di modelli all‚Äôavanguardia negli ambienti di produzione. Tuttavia, come approfondiremo ulteriormente, questi non sono gli unici fattori che contribuiscono allo sviluppo dei benchmark di apprendimento automatico.\nLe classifiche svolgono un ruolo fondamentale nel fornire un metodo oggettivo e trasparente per ricercatori e professionisti per valutare l‚Äôefficacia di diversi modelli, classificandoli in base alle loro prestazioni nei benchmark. Questo sistema promuove un ambiente competitivo, incoraggiando lo sviluppo di modelli che non siano solo accurati ma anche efficienti. L‚ÄôImageNet Large Scale Visual Recognition Challenge (ILSVRC) ne √® un ottimo esempio, con la sua classifica annuale che contribuisce in modo significativo allo sviluppo di modelli innovativi come AlexNet.\nL‚Äôaccesso open source a modelli e set di dati all‚Äôavanguardia diffonde ulteriormente l‚Äôapprendimento automatico, facilitando la collaborazione tra ricercatori e professionisti in tutto il mondo. Questo accesso aperto accelera il processo di test, convalida e distribuzione di nuovi modelli in ambienti di produzione, come dimostrato dall‚Äôadozione diffusa di modelli come BERT e GPT-3 in varie applicazioni, dall‚Äôelaborazione del linguaggio naturale a compiti multimodali pi√π complessi.\nPiattaforme di collaborazione della comunit√† come Kaggle hanno rivoluzionato il settore ospitando competizioni che uniscono data scientist da tutto il mondo per risolvere problemi intricati. Benchmark specifici fungono da paletti per l‚Äôinnovazione e lo sviluppo di modelli.\nInoltre, la disponibilit√† di set di dati diversi e di alta qualit√† √® fondamentale per l‚Äôaddestramento e il test dei modelli di apprendimento automatico. Set di dati come ImageNet hanno svolto un ruolo fondamentale nell‚Äôevoluzione dei modelli di riconoscimento delle immagini, mentre ampi set di dati di testo hanno facilitato i progressi nei modelli di elaborazione del linguaggio naturale.\nInfine, √® necessario supportare i contributi di istituti accademici e di ricerca. Il loro ruolo nella pubblicazione di articoli di ricerca, nella condivisione di risultati in conferenze e nella promozione della collaborazione tra varie istituzioni ha contribuito in modo significativo al progresso dei modelli e dei benchmark di apprendimento automatico.\n\nTendenze Emergenti\nMan mano che i modelli di apprendimento automatico diventano pi√π sofisticati, lo diventano anche i benchmark necessari per valutarli in modo accurato. Ci sono diversi benchmark e dataset emergenti che stanno guadagnando popolarit√† grazie alla loro capacit√† di valutare i modelli in scenari pi√π complessi e realistici:\nDataset Multimodali: Questi set di dati contengono pi√π tipi di dati, come testo, immagini e audio, per rappresentare meglio le situazioni del mondo reale. Un esempio √® VQA (Visual Question Answering) (Antol et al. 2015), in cui viene testata la capacit√† dei modelli di rispondere a domande basate su testo sulle immagini.\n\nAntol, Stanislaw, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C. Lawrence Zitnick, e Devi Parikh. 2015. ¬´VQA: Visual Question Answering¬ª. In 2015 IEEE International Conference on Computer Vision (ICCV), 2425‚Äì33. IEEE. https://doi.org/10.1109/iccv.2015.279.\nValutazione di Correttezza e Bias: C‚Äô√® una crescente attenzione alla creazione di benchmark che valutino l‚Äôequit√†/Correttezza e i bias [pregiudizi] dei modelli di apprendimento automatico. Esempi includono il toolkit AI Fairness 360, che offre un set completo di metriche e set di dati per valutare il bias nei modelli.\nGeneralizzazione Out-of-Distribution: Test di quanto bene i modelli funzionano su dati diversi dalla distribuzione di training originale. Questo valuta la capacit√† del modello di generalizzare a dati nuovi e inediti. Esempi di benchmark sono Wilds (Koh et al. 2021), RxRx e ANC-Bench.\n\nKoh, Pang Wei, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani, Weihua Hu, et al. 2021. ¬´WILDS: A Benchmark of in-the-Wild Distribution Shifts.¬ª In Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, a cura di Marina Meila e Tong Zhang, 139:5637‚Äì64. Proceedings of Machine Learning Research. PMLR. http://proceedings.mlr.press/v139/koh21a.html.\n\nHendrycks, Dan, Kevin Zhao, Steven Basart, Jacob Steinhardt, e Dawn Song. 2021. ¬´Natural Adversarial Examples¬ª. In 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 15257‚Äì66. IEEE. https://doi.org/10.1109/cvpr46437.2021.01501.\n\nXie, Cihang, Mingxing Tan, Boqing Gong, Jiang Wang, Alan L. Yuille, e Quoc V. Le. 2020. ¬´Adversarial Examples Improve Image Recognition¬ª. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 816‚Äì25. IEEE. https://doi.org/10.1109/cvpr42600.2020.00090.\nRobustezza Avversaria: Valutazione delle prestazioni del modello in caso di attacchi avversari o perturbazioni ai dati di input. Questo testa la robustezza del modello. Esempi di benchmark sono ImageNet-A (Hendrycks et al. 2021), ImageNet-C (Xie et al. 2020) e CIFAR-10.1.\nPrestazioni nel Mondo Reale: Test di modelli su set di dati del mondo reale che corrispondono da vicino alle attivit√† finali anzich√© solo su set di dati di benchmark predefiniti. Esempi sono set di dati di imaging medico per attivit√† sanitarie o log di chat di assistenza clienti per sistemi di dialogo.\nEfficienza Energetica e di Calcolo: Benchmark che misurano le risorse di calcolo necessarie per ottenere una particolare accuratezza. Questo valuta l‚Äôefficienza del modello. Esempi sono MLPerf e Greenbench, gi√† discussi nella sezione Benchmarking dei sistemi.\nInterpretabilit√† e Spiegabilit√†: Benchmark che valutano quanto sia facile comprendere e spiegare la logica interna e le previsioni di un modello. Esempi di parametri sono la fedelt√† ai gradienti di input e la coerenza delle spiegazioni.\n\n\n\n11.5.4 Limitazioni e Sfide\nSebbene i benchmark dei modelli siano uno strumento essenziale per valutare i modelli di machine learning, √® necessario affrontare diverse limitazioni e sfide per garantire che riflettano accuratamente le prestazioni in scenari reali.\nIl dataset non corrisponde a scenari reali: Spesso, i dati utilizzati nei benchmark dei modelli vengono puliti e preelaborati a tal punto che potrebbe essere necessario rappresentare accuratamente i dati che un modello incontrerebbe in applicazioni reali. Questa versione idealizzata dei dati pu√≤ portare a una sovrastima delle prestazioni di un modello. Nel caso del set di dati ImageNet, le immagini sono ben etichettate e categorizzate. Tuttavia, in uno scenario reale, un modello potrebbe dover gestire immagini sfocate che potrebbero essere meglio illuminate o scattate da angolazioni scomode. Questa discrepanza pu√≤ influire in modo significativo sulle prestazioni del modello.\nSim2Real Gap: Il Sim2Real Gap si riferisce alla differenza nelle prestazioni di un modello quando si passa da un ambiente simulato a un ambiente reale. Questo gap √® spesso osservato nella robotica, dove un robot addestrato in un ambiente simulato ha difficolt√† a svolgere compiti nel mondo reale a causa della complessit√† e dell‚Äôimprevedibilit√† degli ambienti reali. Un robot addestrato a raccogliere oggetti in un ambiente simulato potrebbe aver bisogno di aiuto per svolgere lo stesso compito nel mondo reale perch√© l‚Äôambiente simulato non rappresenta accuratamente le complessit√† della fisica, dell‚Äôilluminazione e della variabilit√† degli oggetti del mondo reale.\nSfide nella Creazione di Dataset: La creazione di un set di dati per il benchmarking del modello √® un‚Äôattivit√† impegnativa che richiede un‚Äôattenta considerazione di vari fattori come qualit√† dei dati, diversit√† e rappresentazione. Come discusso nella sezione di ingegneria dei dati, garantire che i dati siano puliti, imparziali e rappresentativi dello scenario del mondo reale √® fondamentale per l‚Äôaccuratezza e l‚Äôaffidabilit√† del benchmark. Ad esempio, quando si crea un set di dati per un‚Äôattivit√† correlata all‚Äôassistenza sanitaria, √® importante assicurarsi che i dati siano rappresentativi dell‚Äôintera popolazione e non distorti verso un particolare gruppo demografico. Ci√≤ garantisce che il modello funzioni bene in diverse popolazioni di pazienti.\nI benchmark del modello sono essenziali per misurare la capacit√† di un‚Äôarchitettura di modello di risolvere un‚Äôattivit√† fissa, ma √® importante affrontare le limitazioni e le sfide ad essi associate. Ci√≤ include il garantire che il set di dati rappresenti accuratamente scenari del mondo reale, affrontare il divario Sim2Real e superare le sfide della creazione di set di dati imparziali e rappresentativi. Affrontando queste sfide e molte altre, possiamo garantire che i benchmark del modello forniscano una valutazione pi√π accurata e affidabile delle prestazioni di un modello in applicazioni del mondo reale.\nLo Speech Commands dataset e il suo successore MSWC sono benchmark comuni per una delle applicazioni TinyML per eccellenza, l‚Äôindividuazione delle parole chiave. I comandi vocali stabiliscono metriche di errore di streaming oltre la precisione di classificazione standard top-1 pi√π pertinenti al caso d‚Äôuso di individuazione delle parole chiave. L‚Äôutilizzo di metriche pertinenti ai casi √® ci√≤ che eleva un dataset a un benchmark del modello.",
    "crumbs": [
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Benchmarking dell'IA</span>"
    ]
  },
  {
    "objectID": "contents/core/benchmarking/benchmarking.it.html#benchmarking-dei-dati",
    "href": "contents/core/benchmarking/benchmarking.it.html#benchmarking-dei-dati",
    "title": "11¬† Benchmarking dell‚ÄôIA",
    "section": "11.6 Benchmarking dei Dati",
    "text": "11.6 Benchmarking dei Dati\nNegli ultimi anni, l‚Äôintelligenza artificiale si √® concentrata sullo sviluppo di modelli di apprendimento automatico sempre pi√π sofisticati, come i grandi modelli linguistici. L‚Äôobiettivo √® stato quello di creare modelli in grado di prestazioni di livello umano o sovrumane su un‚Äôampia gamma di attivit√†, addestrandoli su enormi set di dati. Questo approccio incentrato sul modello ha prodotto rapidi progressi, con modelli che hanno ottenuto risultati all‚Äôavanguardia su molti benchmark consolidati. Figura¬†11.9 mostra le prestazioni dei sistemi di intelligenza artificiale rispetto alle prestazioni umane (contrassegnate dalla linea orizzontale a 0) in cinque applicazioni: riconoscimento della scrittura a mano, riconoscimento vocale, riconoscimento delle immagini, comprensione della lettura e comprensione del linguaggio. Negli ultimi dieci anni, le prestazioni dell‚Äôintelligenza artificiale hanno superato quelle degli esseri umani.\n\n\n\n\n\n\nFigura¬†11.9: IA e prestazioni umane. Fonte: Kiela et al. (2021).\n\n\n\nTuttavia, le crescenti preoccupazioni su questioni come pregiudizi, sicurezza e robustezza persistono anche nei modelli che raggiungono un‚Äôelevata accuratezza sui benchmark standard. Inoltre, alcuni set di dati popolari utilizzati per la valutazione dei modelli stanno iniziando a saturarsi, con modelli che raggiungono prestazioni quasi perfette su divisioni di test esistenti (Kiela et al. 2021). Come semplice esempio, ci sono immagini di test nel classico dataset di cifre scritte a mano MNIST che potrebbero sembrare indecifrabili per la maggior parte dei valutatori umani, ma a cui √® stata assegnata un‚Äôetichetta quando √® stato creato il set di dati: i modelli che concordano con quelle etichette potrebbero sembrare esibire prestazioni sovrumane, ma potrebbero invece catturare solo idiosincrasie del processo di etichettatura e acquisizione dalla creazione del set di dati nel 1994. Con lo stesso spirito, i ricercatori di visione artificiale ora chiedono: ‚ÄúAbbiamo finito con ImageNet?‚Äù (Beyer et al. 2020). Ci√≤ evidenzia i limiti nell‚Äôapproccio convenzionale incentrato sul modello di ottimizzazione dell‚Äôaccuratezza su set di dati fissi tramite innovazioni architettoniche.\n\nKiela, Douwe, Max Bartolo, Yixin Nie, Divyansh Kaushik, Atticus Geiger, Zhengxuan Wu, Bertie Vidgen, et al. 2021. ¬´Dynabench: Rethinking Benchmarking in NLP¬ª. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 4110‚Äì24. Online: Association for Computational Linguistics. https://doi.org/10.18653/v1/2021.naacl-main.324.\n\nBeyer, Lucas, Olivier J. H√©naff, Alexander Kolesnikov, Xiaohua Zhai, e A√§ron van den Oord. 2020. ¬´Are we done with ImageNet?¬ª ArXiv preprint abs/2006.07159 (giugno). http://arxiv.org/abs/2006.07159v1.\nSta emergendo un paradigma alternativo chiamato IA incentrata sui dati. Invece di trattare i dati come statici e concentrarsi strettamente sulle prestazioni del modello, questo approccio riconosce che i modelli sono validi solo quanto i loro dati di training. Quindi, l‚Äôenfasi si sposta sulla cura di dataset di alta qualit√† che riflettano meglio la complessit√† del mondo reale, sviluppando benchmark di valutazione pi√π informativi e considerando attentamente come i dati vengono campionati, preelaborati e aumentati. L‚Äôobiettivo √® ottimizzare il comportamento del modello migliorando i dati anzich√© semplicemente ottimizzando le metriche su set di dati imperfetti. L‚Äôintelligenza artificiale incentrata sui dati esamina e migliora criticamente i dati stessi per produrre un‚Äôintelligenza artificiale utile. Ci√≤ riflette un‚Äôimportante evoluzione nella mentalit√†, poich√© il campo affronta le carenze di un benchmarking ristretto.\nQuesta sezione esplorer√† le principali differenze tra gli approcci all‚Äôintelligenza artificiale incentrati sui modelli e sui dati. Questa distinzione ha importanti implicazioni sul modo in cui eseguiamo il benchmarking dei sistemi di intelligenza artificiale. In particolare, vedremo come concentrarsi sulla qualit√† dei dati e sull‚Äôefficienza pu√≤ migliorare direttamente le prestazioni dell‚Äôapprendimento automatico come alternativa all‚Äôottimizzazione delle sole architetture dei modelli. L‚Äôapproccio incentrato sui dati riconosce che i modelli sono validi solo quanto i loro dati di addestramento. Quindi, migliorare la cura dei dati, i benchmark di valutazione e i processi di gestione dei dati pu√≤ produrre sistemi di intelligenza artificiale pi√π sicuri, pi√π equi e pi√π robusti. Ripensare il benchmarking per dare priorit√† ai dati insieme ai modelli rappresenta un‚Äôimportante evoluzione, poich√© il settore si sforza di fornire un impatto affidabile nel mondo reale.\n\n11.6.1 Limitazioni dell‚ÄôIA Incentrata sul Modello\nNell‚Äôera dell‚ÄôIA incentrata sul modello, una caratteristica importante era lo sviluppo di architetture di modelli complesse. Ricercatori e professionisti hanno dedicato notevoli sforzi alla progettazione di modelli sofisticati e intricati nella ricerca di prestazioni superiori. Ci√≤ ha spesso comportato l‚Äôincorporazione di livelli aggiuntivi e la messa a punto di una moltitudine di iperparametri per ottenere miglioramenti nell‚Äôaccuratezza. Contemporaneamente, c‚Äôera una notevole enfasi sullo sfruttamento di algoritmi avanzati. Questi algoritmi, spesso in prima linea nelle ultime ricerche, sono stati impiegati per migliorare le prestazioni dei modelli di IA. L‚Äôobiettivo principale di questi algoritmi era ottimizzare il processo di apprendimento dei modelli, estraendo cos√¨ il massimo delle informazioni dai dati di addestramento.\nSebbene l‚Äôapproccio incentrato sul modello sia stato centrale per molti progressi nell‚ÄôIA, ha diverse aree di miglioramento. Innanzitutto, lo sviluppo di architetture di modelli complesse pu√≤ spesso portare a un overfitting. Questo √® quando il modello funziona bene sui dati di addestramento ma deve generalizzare a nuovi dati mai visti. I layer aggiuntivi e la complessit√† possono catturare il rumore nei dati di training come se fosse un pattern reale, danneggiando le prestazioni del modello su nuovi dati.\nIn secondo luogo, affidarsi ad algoritmi avanzati pu√≤ a volte oscurare la reale comprensione del funzionamento di un modello. Questi algoritmi spesso agiscono come una scatola nera, rendendo difficile interpretare il modo in cui il modello prende decisioni. Questa mancanza di trasparenza pu√≤ essere un ostacolo significativo, specialmente in applicazioni critiche come sanit√† e finanza, dove la comprensione del processo decisionale del modello √® fondamentale.\nIn terzo luogo, l‚Äôenfasi sul raggiungimento di risultati all‚Äôavanguardia su set di dati di riferimento pu√≤ a volte essere fuorviante. Questi dataset devono rappresentare in modo pi√π completo le complessit√† e la variabilit√† dei dati del mondo reale. Un modello che funziona bene su un set di dati di riferimento potrebbe non essere necessariamente generalizzato bene a dati nuovi e mai visti in un‚Äôapplicazione del mondo reale. Questa discrepanza pu√≤ portare a una falsa fiducia nelle capacit√† del modello e ostacolarne l‚Äôapplicabilit√† pratica.\nInfine, l‚Äôapproccio incentrato sul modello spesso si basa su grandi set di dati etichettati per l‚Äôaddestramento. Tuttavia, ottenere tali set di dati richiede tempo e impegno in molti scenari del mondo reale. Questa dipendenza da grandi dataset limita anche l‚Äôapplicabilit√† dell‚ÄôIA in domini in cui i dati sono scarsi o costosi da etichettare.\nCome risultato delle ragioni di cui sopra, e di molte altre, la comunit√† dell‚ÄôIA sta passando a un approccio pi√π incentrato sui dati. Invece di concentrarsi solo sull‚Äôarchitettura del modello, i ricercatori stanno ora dando priorit√† alla cura di set di dati di alta qualit√†, allo sviluppo di migliori benchmark di valutazione e alla considerazione di come i dati vengono campionati e preelaborati. L‚Äôidea chiave √® che i modelli sono validi solo quanto i loro dati di training. Quindi, concentrandoci sull‚Äôottenimento dei dati giusti, potremo sviluppare sistemi di intelligenza artificiale pi√π equi, sicuri e allineati con i valori umani. Questo cambiamento incentrato sui dati rappresenta un importante cambiamento di mentalit√† man mano che l‚Äôintelligenza artificiale progredisce.\n\n\n11.6.2 Verso un‚ÄôIntelligenza Artificiale Incentrata sui Dati\nL‚Äôintelligenza artificiale incentrata sui dati √® un paradigma che sottolinea l‚Äôimportanza di dataset di alta qualit√†, ben etichettati e diversificati nello sviluppo di modelli di intelligenza artificiale. Contrariamente all‚Äôapproccio incentrato sul modello, che si concentra sulla rifinitura e l‚Äôiterazione dell‚Äôarchitettura e dell‚Äôalgoritmo del modello per migliorare le prestazioni, l‚Äôintelligenza artificiale incentrata sui dati d√† priorit√† alla qualit√† dei dati di input come motore principale per migliorare le prestazioni del modello. I dati di alta qualit√† sono puliti, ben etichettati e rappresentativi degli scenari del mondo reale che il modello incontrer√†. Al contrario, i dati di bassa qualit√† possono portare a scarse prestazioni del modello, indipendentemente dalla complessit√† o dalla sofisticatezza dell‚Äôarchitettura del modello.\nL‚Äôintelligenza artificiale incentrata sui dati pone una forte enfasi sulla pulizia e l‚Äôetichettatura dei dati. La pulizia comporta la rimozione di valori anomali, la gestione dei valori mancanti e la risoluzione di altre incongruenze nei dati. L‚Äôetichettatura, d‚Äôaltro canto, comporta l‚Äôassegnazione di etichette significative e accurate ai dati. Entrambi questi processi sono fondamentali per garantire che il modello di intelligenza artificiale venga addestrato su dati accurati e pertinenti. Un altro aspetto importante dell‚Äôapproccio incentrato sui dati √® il ‚Äúdata augmentation‚Äù [l‚Äôaumento dei dati]. Ci√≤ comporta l‚Äôaumento artificiale delle dimensioni e della diversit√† del set di dati applicando varie trasformazioni ai dati, come rotazione, ridimensionamento e capovolgimento delle immagini di addestramento. L‚Äôaumento dei dati aiuta a migliorare la robustezza del modello e le capacit√† di generalizzazione.\nCi sono diversi vantaggi nell‚Äôadottare un approccio incentrato sui dati per lo sviluppo dell‚Äôintelligenza artificiale. Innanzitutto, porta a prestazioni del modello migliorate e capacit√† di generalizzazione. Assicurandosi che il modello venga addestrato su dati diversi e di alta qualit√†, il modello pu√≤ generalizzare meglio a dati nuovi e mai visti (Mattson et al. 2020b).\nInoltre, un approccio incentrato sui dati pu√≤ spesso portare a modelli pi√π semplici che sono pi√π facili da interpretare e gestire. Questo perch√© l‚Äôenfasi √® sui dati piuttosto che sull‚Äôarchitettura del modello, il che significa che i modelli pi√π semplici possono raggiungere prestazioni elevate quando addestrati su dati di alta qualit√†.\nIl passaggio all‚ÄôIA incentrata sui dati rappresenta un significativo cambiamento di paradigma. Dando priorit√† alla qualit√† dei dati di input, questo approccio cerca di modellare le prestazioni e le capacit√† di generalizzazione, portando in ultima analisi a sistemi di intelligenza artificiale pi√π solidi e affidabili. Figura¬†11.10 illustra questa differenza. Mentre continuiamo ad avanzare nella nostra comprensione e applicazione dell‚ÄôIA, √® probabile che l‚Äôapproccio incentrato sui dati svolga un ruolo importante nel plasmare il futuro di questo campo.\n\n\n\n\n\n\nFigura¬†11.10: Sviluppo di machine learning incentrato sul modello e incentrato sui dati. Fonte: NVIDIA\n\n\n\n\n\n11.6.3 Benchmarking dei Dati\nIl benchmarking dei dati mira a valutare problemi comuni nei set di dati, come l‚Äôidentificazione di errori di etichetta, caratteristiche rumorose, squilibrio di rappresentazione (ad esempio, su 1000 classi in Imagenet-1K, ci sono oltre 100 categorie che sono solo tipi di cani), squilibrio di classe (dove alcune classi hanno molti pi√π campioni di altre), se i modelli addestrati su un dato set di dati possono generalizzare a caratteristiche fuori distribuzione o quali tipi di bias potrebbero esistere in un dato set di dati (Mattson et al. 2020b). Nella sua forma pi√π semplice, il benchmarking dei dati mira a migliorare l‚Äôaccuratezza su un set di test rimuovendo campioni di addestramento rumorosi o etichettati in modo errato mantenendo fissa l‚Äôarchitettura del modello. Recenti competizioni nel benchmarking dei dati hanno invitato i partecipanti a presentare nuove strategie di ‚Äúaugmentation‚Äù e tecniche di apprendimento attivo.\n\nMattson, Peter, Vijay Janapa Reddi, Christine Cheng, Cody Coleman, Greg Diamos, David Kanter, Paulius Micikevicius, et al. 2020b. ¬´MLPerf: An Industry Standard Benchmark Suite for Machine Learning Performance¬ª. IEEE Micro 40 (2): 8‚Äì16. https://doi.org/10.1109/mm.2020.2974843.\nLe tecniche incentrate sui dati continuano a guadagnare attenzione nel benchmarking, soprattutto perch√© i modelli di base sono sempre pi√π addestrati su obiettivi auto-supervisionati. Rispetto ai set di dati pi√π piccoli come Imagenet-1K, i set di dati pi√π grandi comunemente usati nell‚Äôapprendimento auto-supervisionato, come Common Crawl, OpenImages e LAION-5B, contengono quantit√† maggiori di rumore, duplicati, bias e dati potenzialmente offensivi.\nDataComp √® una competizione di dataset lanciata di recente che ha come obiettivo la valutazione di grandi corpora. DataComp si concentra sulle coppie linguaggio-immagine usate per addestrare i modelli CLIP. Il documento introduttivo rileva che quando il budget di elaborazione totale per l‚Äôaddestramento √® costante, i modelli CLIP pi√π performanti nelle attivit√† downstream, come la classificazione ImageNet, vengono addestrati solo sul 30% del pool di campioni disponibile. Ci√≤ suggerisce che un corretto filtraggio di grandi corpora √® fondamentale per migliorare l‚Äôaccuratezza dei modelli di base. Analogamente, Demystifying CLIP Data (Xu et al. 2023) chiede se il successo di CLIP sia attribuibile all‚Äôarchitettura o al set di dati.\n\nXu, Hu, Saining Xie, Xiaoqing Ellen Tan, Po-Yao Huang, Russell Howes, Vasu Sharma, Shang-Wen Li, Gargi Ghosh, Luke Zettlemoyer, e Christoph Feichtenhofer. 2023. ¬´Demystifying CLIP Data¬ª. ArXiv preprint abs/2309.16671 (settembre). http://arxiv.org/abs/2309.16671v4.\nDataPerf √® un altro recente lavoro incentrato sul benchmarking dei dati in varie modalit√†. DataPerf offre round di competizione online per stimolare il miglioramento dei dataset. L‚Äôofferta inaugurale √® stata lanciata con sfide in termini di visione, parlato, acquisizione, debug e prompt di testo per la generazione di immagini.\n\n\n11.6.4 Efficienza dei Dati\nMan mano che i modelli di apprendimento automatico diventano pi√π grandi e complessi e le risorse di elaborazione diventano pi√π scarse di fronte alla crescente domanda, diventa difficile soddisfare i requisiti di elaborazione anche con le flotte di machine learning pi√π grandi. Per superare queste sfide e garantire la scalabilit√† del sistema di apprendimento automatico, √® necessario esplorare nuove opportunit√† che aumentino gli approcci convenzionali alla scalabilit√† delle risorse.\nMigliorare la qualit√† dei dati pu√≤ essere un metodo utile per avere un impatto significativo sulle prestazioni del sistema di apprendimento automatico. Uno dei principali vantaggi del miglioramento della qualit√† dei dati √® il potenziale di poter ridurre le dimensioni del set di dati di addestramento mantenendo o addirittura migliorando le prestazioni del modello. Questa riduzione delle dimensioni dei dati √® direttamente correlata alla quantit√† di tempo di addestramento richiesto, consentendo cos√¨ ai modelli di convergere in modo pi√π rapido ed efficiente. Raggiungere questo equilibrio tra qualit√† dei dati e dimensioni del set di dati √® un compito impegnativo che richiede lo sviluppo di metodi, algoritmi e tecniche sofisticati.\nPossono essere adottati diversi approcci per migliorare la qualit√† dei dati. Questi metodi includono e non sono limitati a quanto segue:\n\nPulizia dei Dati: Ci√≤ comporta la gestione dei valori mancanti, la correzione degli errori e la rimozione dei valori anomali. I dati puliti assicurano che il modello non stia imparando da rumore o imprecisioni.\nInterpretabilit√† e Spiegabilit√† dei Dati: Le tecniche comuni includono LIME (Ribeiro, Singh, e Guestrin 2016), che fornisce informazioni sui limiti decisionali dei classificatori, e valori Shapley (Lundberg e Lee 2017), che stimano l‚Äôimportanza dei singoli campioni nel contribuire alle previsioni di un modello.\nFeature Engineering: Trasformare o creare nuove funzionalit√† pu√≤ migliorare significativamente le prestazioni del modello fornendo informazioni pi√π pertinenti per l‚Äôapprendimento.\nData Augmentation: Aumentare i dati creando nuovi campioni tramite varie trasformazioni pu√≤ aiutare a migliorare la robustezza e la generalizzazione del modello.\nActive Learning: Questo √® un approccio di apprendimento semi-supervisionato in cui il modello interroga attivamente un ‚Äúoracolo‚Äù umano per etichettare i campioni pi√π informativi (Coleman et al. 2022). Ci√≤ garantisce che il modello venga addestrato sui dati pi√π rilevanti.\nRiduzione della Dimensionalit√†: Tecniche come PCA possono ridurre il numero di feature in un set di dati, riducendo cos√¨ la complessit√† e il tempo di training.\n\n\nRibeiro, Marco Tulio, Sameer Singh, e Carlos Guestrin. 2016. ¬´‚Äù Why should i trust you?‚Äù Explaining the predictions of any classifier¬ª. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, 1135‚Äì44.\n\nLundberg, Scott M., e Su-In Lee. 2017. ¬´A Unified Approach to Interpreting Model Predictions¬ª. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, a cura di Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, e Roman Garnett, 4765‚Äì74. https://proceedings.neurips.cc/paper/2017/hash/8a20a8621978632d76c43dfd28b67767-Abstract.html.\n\nColeman, Cody, Edward Chou, Julian Katz-Samuels, Sean Culatana, Peter Bailis, Alexander C. Berg, Robert Nowak, Roshan Sumbaly, Matei Zaharia, e I. Zeki Yalniz. 2022. ¬´Similarity Search for Efficient Active Learning and Search of Rare Concepts¬ª. Proceedings of the AAAI Conference on Artificial Intelligence 36 (6): 6402‚Äì10. https://doi.org/10.1609/aaai.v36i6.20591.\nEsistono molti altri metodi in circolazione. Ma l‚Äôobiettivo √® lo stesso. Affinare il set di dati e garantire che sia della massima qualit√† pu√≤ ridurre il tempo di addestramento necessario per la convergenza dei modelli. Tuttavia, per raggiungere questo obiettivo √® necessario sviluppare e implementare metodi, algoritmi e tecniche sofisticati in grado di pulire, preelaborare e aumentare i dati, mantenendo al contempo i campioni pi√π informativi. Questa √® una sfida continua che richieder√† una continua ricerca e innovazione nel campo dell‚Äôapprendimento automatico.",
    "crumbs": [
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Benchmarking dell'IA</span>"
    ]
  },
  {
    "objectID": "contents/core/benchmarking/benchmarking.it.html#la-tripletta",
    "href": "contents/core/benchmarking/benchmarking.it.html#la-tripletta",
    "title": "11¬† Benchmarking dell‚ÄôIA",
    "section": "11.7 La Tripletta",
    "text": "11.7 La Tripletta\nMentre i benchmark di sistema, modello e dati sono stati tradizionalmente studiati in modo isolato, si sta diffondendo la consapevolezza che per comprendere e far progredire completamente l‚ÄôIA, dobbiamo adottare una visione pi√π olistica. Iterando tra sistemi di benchmarking, modelli e dataset insieme, potrebbero emergere nuove intuizioni che non sono evidenti quando questi componenti vengono analizzati separatamente. Le prestazioni del sistema influiscono sulla precisione del modello, le capacit√† del modello determinano le esigenze dei dati e le caratteristiche dei dati determinano i requisiti del sistema.\nIl benchmarking della triade di sistema, modello e dati in modo integrato porter√† probabilmente a scoperte sulla progettazione congiunta dei sistemi di IA, sulle propriet√† di generalizzazione dei modelli e sul ruolo della cura e della qualit√† dei dati nel consentire le prestazioni. Piuttosto che benchmark ristretti di singoli componenti, il futuro dell‚ÄôIA richiede benchmark che valutino la relazione simbiotica tra piattaforme di elaborazione, algoritmi e dati di training. Questa prospettiva a livello di sistema sar√† fondamentale per superare le attuali limitazioni e sbloccare il prossimo livello di capacit√† dell‚ÄôIA.\nFigura¬†11.11 illustra i molti modi potenziali per far interagire tra loro il benchmarking dei dati, quello dei modelli e quello dell‚Äôinfrastruttura di sistema. L‚Äôesplorazione di queste complesse interazioni probabilmente porter√† alla scoperta di nuove opportunit√† di ottimizzazione e capacit√† di miglioramento. La tripletta di benchmark di dati, modelli e sistemi offre un ricco spazio per la progettazione congiunta e la co-ottimizzazione.\n\n\n\n\n\n\nFigura¬†11.11: La tripletta del Benchmarking.\n\n\n\nSebbene questa prospettiva integrata rappresenti una tendenza emergente, il settore ha ancora molto da scoprire sulle sinergie e i compromessi tra questi componenti. Mentre eseguiamo il benchmarking iterativo di combinazioni di dati, modelli e sistemi, emergeranno nuove intuizioni che rimangono nascoste quando questi elementi vengono studiati separatamente. Questo approccio di benchmarking multiforme che traccia le intersezioni di dati, algoritmi e hardware promette di essere una strada fruttuosa per importanti progressi nell‚Äôintelligenza artificiale, anche se √® ancora nelle sue fasi iniziali.",
    "crumbs": [
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Benchmarking dell'IA</span>"
    ]
  },
  {
    "objectID": "contents/core/benchmarking/benchmarking.it.html#benchmark-per-tecnologie-emergenti",
    "href": "contents/core/benchmarking/benchmarking.it.html#benchmark-per-tecnologie-emergenti",
    "title": "11¬† Benchmarking dell‚ÄôIA",
    "section": "11.8 Benchmark per Tecnologie Emergenti",
    "text": "11.8 Benchmark per Tecnologie Emergenti\nDate le loro significative differenze rispetto alle tecniche esistenti, le tecnologie emergenti possono essere particolarmente difficili da progettare per i benchmark. I benchmark standard utilizzati per le tecnologie esistenti potrebbero non evidenziare le feature chiave del nuovo approccio. Al contrario, i nuovi benchmark potrebbero essere visti come artificiosi per favorire la tecnologia emergente rispetto ad altre. Potrebbero essere cos√¨ diversi dai benchmark esistenti da non poter essere compresi e perdere significato. Pertanto, i benchmark per le tecnologie emergenti devono bilanciare equit√†, applicabilit√† e facilit√† di confronto con quelli esistenti.\nUn esempio di tecnologia emergente in cui il benchmarking si √® dimostrato particolarmente difficile √® nel Neuromorphic Computing. Utilizzando il cervello come fonte di ispirazione per un‚Äôintelligenza generale scalabile, robusta ed efficiente dal punto di vista energetico, il calcolo neuromorfico (Schuman et al. 2022) incorpora direttamente meccanismi biologicamente realistici sia negli algoritmi di calcolo che nell‚Äôhardware, come le reti neurali spiking (Maass 1997) e le architetture non-von Neumann architectures per eseguirle (Davies et al. 2018; Modha et al. 2023). Da una prospettiva full-stack di modelli, tecniche di training e sistemi hardware, il calcolo neuromorfico differisce dall‚Äôhardware e dall‚Äôintelligenza artificiale convenzionali. Pertanto, esiste una sfida fondamentale nello sviluppo di benchmark equi e utili per guidare la tecnologia.\n\nSchuman, Catherine D., Shruti R. Kulkarni, Maryam Parsa, J. Parker Mitchell, Prasanna Date, e Bill Kay. 2022. ¬´Opportunities for neuromorphic computing algorithms and applications¬ª. Nature Computational Science 2 (1): 10‚Äì19. https://doi.org/10.1038/s43588-021-00184-y.\n\nMaass, Wolfgang. 1997. ¬´Networks of spiking neurons: The third generation of neural network models¬ª. Neural Networks 10 (9): 1659‚Äì71. https://doi.org/10.1016/s0893-6080(97)00011-7.\n\nDavies, Mike, Narayan Srinivasa, Tsung-Han Lin, Gautham Chinya, Yongqiang Cao, Sri Harsha Choday, Georgios Dimou, et al. 2018. ¬´Loihi: A Neuromorphic Manycore Processor with On-Chip Learning¬ª. IEEE Micro 38 (1): 82‚Äì99. https://doi.org/10.1109/mm.2018.112130359.\n\nModha, Dharmendra S., Filipp Akopyan, Alexander Andreopoulos, Rathinakumar Appuswamy, John V. Arthur, Andrew S. Cassidy, Pallab Datta, et al. 2023. ¬´Neural inference at the frontier of energy, space, and time¬ª. Science 382 (6668): 329‚Äì35. https://doi.org/10.1126/science.adh1174.\n\nYik, Jason, Korneel Van den Berghe, Douwe den Blanken, Younes Bouhadjar, Maxime Fabre, Paul Hueber, Denis Kleyko, et al. 2023. ¬´NeuroBench: A Framework for Benchmarking Neuromorphic Computing Algorithms and Systems¬ª, aprile. http://arxiv.org/abs/2304.04640v3.\nUn‚Äôiniziativa in corso per sviluppare benchmark neuromorfici standard √® NeuroBench (Yik et al. 2023). Per un benchmarking adeguato del neuromorfico, NeuroBench segue principi di alto livello di inclusivit√† attraverso l‚Äôapplicabilit√† di attivit√† e metriche sia alle soluzioni neuromorfiche che non neuromorfiche, attuabilit√† dell‚Äôimplementazione utilizzando strumenti comuni e aggiornamenti iterativi per continuare a garantire la pertinenza man mano che il campo cresce rapidamente. NeuroBench e altri benchmark per le tecnologie emergenti forniscono una guida critica per le tecniche future, che potrebbero essere necessarie man mano che i limiti di scalabilit√† degli approcci esistenti si avvicinano.",
    "crumbs": [
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Benchmarking dell'IA</span>"
    ]
  },
  {
    "objectID": "contents/core/benchmarking/benchmarking.it.html#conclusione",
    "href": "contents/core/benchmarking/benchmarking.it.html#conclusione",
    "title": "11¬† Benchmarking dell‚ÄôIA",
    "section": "11.9 Conclusione",
    "text": "11.9 Conclusione\nCi√≤ che viene misurato viene migliorato. Questo capitolo ha esplorato la natura multiforme del benchmarking che abbraccia sistemi, modelli e dati. Il benchmarking √® importante per far progredire l‚ÄôIA in quanto fornisce le misurazioni essenziali per monitorare i progressi.\nI benchmark del sistema ML consentono l‚Äôottimizzazione attraverso metriche di velocit√†, efficienza e scalabilit√†. I benchmark del modello guidano l‚Äôinnovazione attraverso attivit√† e metriche standardizzate oltre l‚Äôaccuratezza. I benchmark dei dati evidenziano problemi di qualit√†, equilibrio e rappresentazione.\n√à importante notare che la valutazione di questi componenti in modo isolato presenta dei limiti. In futuro, sar√† probabilmente utilizzato un benchmarking pi√π integrato per esplorare l‚Äôinterazione tra benchmark di sistema, modello e dati. Questa visione promette nuove intuizioni sulla progettazione congiunta di dati, algoritmi e infrastrutture.\nMan mano che l‚ÄôIA diventa pi√π complessa, il benchmarking completo diventa ancora pi√π critico. Gli standard devono evolversi continuamente per misurare nuove capacit√† e rivelare limitazioni. Una stretta collaborazione tra settore, mondo accademico, etichette nazionali, ecc. √® essenziale per sviluppare benchmark rigorosi, trasparenti e socialmente utili.\nIl benchmarking fornisce la bussola per guidare il progresso nell‚ÄôIA. Misurando costantemente e condividendo apertamente i risultati, possiamo orientarci verso sistemi performanti, robusti e affidabili. Se l‚ÄôIA deve soddisfare adeguatamente le esigenze sociali e umane, deve essere sottoposta a benchmarking tenendo a mente gli interessi dell‚Äôumanit√†. A tal fine, ci sono aree emergenti, come il benchmarking della sicurezza dei sistemi di IA, ma questo √® per un altro giorno e qualcosa di cui possiamo discutere ulteriormente in ‚ÄúGenerative AI‚Äù!\nIl benchmarking √® un argomento in continua evoluzione. L‚Äôarticolo The Olympics of AI: Benchmarking Machine Learning Systems copre diversi sottocampi emergenti nel benchmarking dell‚ÄôIA, tra cui robotica, realt√† estesa e calcolo neuromorfico che incoraggiamo il lettore ad approfondire.",
    "crumbs": [
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Benchmarking dell'IA</span>"
    ]
  },
  {
    "objectID": "contents/core/benchmarking/benchmarking.it.html#sec-benchmarking-ai-resource",
    "href": "contents/core/benchmarking/benchmarking.it.html#sec-benchmarking-ai-resource",
    "title": "11¬† Benchmarking dell‚ÄôIA",
    "section": "11.10 Risorse",
    "text": "11.10 Risorse\nEcco un elenco curato di risorse per supportare studenti e insegnanti nei loro percorsi di apprendimento e insegnamento. Lavoriamo continuamente per espandere questa raccolta e presto aggiungeremo nuovi esercizi.\n\n\n\n\n\n\nSlide\n\n\n\n\n\nQueste slide sono uno strumento prezioso per gli insegnanti per tenere lezioni e per gli studenti per rivedere il materiale secondo il proprio ritmo. Incoraggiamo studenti e docenti a sfruttare queste slide per migliorare la loro comprensione e facilitare un trasferimento efficace delle conoscenze.\n\nPerch√© il benchmarking √® importante?\nBenchmarking di inferenza embedded.\n\n\n\n\n\n\n\n\n\n\nVideo\n\n\n\n\n\n\nProssimamente.\n\n\n\n\n\n\n\n\n\n\nEsercizi\n\n\n\n\n\nPer rafforzare i concetti trattati in questo capitolo, abbiamo curato una serie di esercizi che sfidano gli studenti ad applicare le proprie conoscenze e ad approfondire la propria comprensione.\n\nEsercizio¬†11.1\nEsercizio¬†11.2",
    "crumbs": [
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Benchmarking dell'IA</span>"
    ]
  },
  {
    "objectID": "contents/core/ondevice_learning/ondevice_learning.it.html",
    "href": "contents/core/ondevice_learning/ondevice_learning.it.html",
    "title": "12¬† Apprendimento On-Device",
    "section": "",
    "text": "12.1 Panoramica\nL‚Äôapprendimento su dispositivo si riferisce all‚Äôaddestramento di modelli ML direttamente sul dispositivo in cui vengono distribuiti, al contrario dei metodi tradizionali in cui i modelli vengono addestrati su server potenti e poi distribuiti sui dispositivi. Questo metodo √® particolarmente rilevante per TinyML, in cui i sistemi ML sono integrati in dispositivi minuscoli e con risorse limitate.\nUn esempio di apprendimento su dispositivo pu√≤ essere visto in un termostato intelligente che si adatta al comportamento dell‚Äôutente nel tempo. Inizialmente, il termostato pu√≤ avere un modello generico che comprende pattern di utilizzo di base. Tuttavia, poich√© √® esposto a pi√π dati, come gli orari in cui l‚Äôutente √® a casa o fuori, le temperature preferite e le condizioni meteorologiche esterne, il termostato pu√≤ perfezionare il suo modello direttamente sul dispositivo per fornire un‚Äôesperienza personalizzata. Tutto ci√≤ avviene senza inviare dati a un server centrale per l‚Äôelaborazione.\nUn altro esempio √® nel testo predittivo sugli smartphone. Mentre gli utenti digitano, il telefono impara dai pattern linguistici dell‚Äôutente e suggerisce parole o frasi che probabilmente verranno utilizzate in seguito. Questo apprendimento avviene direttamente sul dispositivo e il modello si aggiorna in tempo reale man mano che vengono raccolti pi√π dati. Un esempio pratico di apprendimento su dispositivo ampiamente utilizzato √® Gboard. Su un telefono Android, Gboard impara da modelli di digitazione e dettatura per migliorare l‚Äôesperienza per tutti gli utenti.\nIn alcuni casi, l‚Äôapprendimento sul dispositivo pu√≤ essere abbinato a una configurazione di apprendimento federato, in cui ogni dispositivo perfeziona il proprio modello localmente utilizzando solo i dati archiviati su quel dispositivo. Questo approccio consente al modello di apprendere dai dati univoci di ogni dispositivo senza trasmetterne nessuno a un server centrale. Come mostrato in Figura¬†12.1, l‚Äôapprendimento federato preserva la privacy mantenendo tutti i dati personali sul dispositivo, assicurando che il processo di addestramento rimanga interamente sul dispositivo, con solo aggiornamenti riepilogativi del modello condivisi tra i dispositivi.",
    "crumbs": [
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Apprendimento On-Device</span>"
    ]
  },
  {
    "objectID": "contents/core/ondevice_learning/ondevice_learning.it.html#panoramica",
    "href": "contents/core/ondevice_learning/ondevice_learning.it.html#panoramica",
    "title": "12¬† Apprendimento On-Device",
    "section": "",
    "text": "Figura¬†12.1: Ciclo di apprendimento federato. Fonte: Google Research.",
    "crumbs": [
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Apprendimento On-Device</span>"
    ]
  },
  {
    "objectID": "contents/core/ondevice_learning/ondevice_learning.it.html#vantaggi-e-limiti",
    "href": "contents/core/ondevice_learning/ondevice_learning.it.html#vantaggi-e-limiti",
    "title": "12¬† Apprendimento On-Device",
    "section": "12.2 Vantaggi e Limiti",
    "text": "12.2 Vantaggi e Limiti\nL‚Äôapprendimento su dispositivo offre diversi vantaggi rispetto al tradizionale ML basato su cloud. Mantenendo dati e modelli sul dispositivo, elimina la necessit√† di costose trasmissioni di dati e risolve i problemi di privacy. Ci√≤ consente esperienze pi√π personalizzate e reattive, poich√© il modello pu√≤ adattarsi in tempo reale al comportamento dell‚Äôutente.\nTuttavia, l‚Äôapprendimento sul dispositivo presenta anche degli svantaggi. Le limitate risorse di elaborazione sui dispositivi dei consumatori possono rendere difficile l‚Äôesecuzione di modelli complessi in locale. Anche i set di dati sono pi√π limitati poich√© sono costituiti solo da dati generati dall‚Äôutente da un singolo dispositivo. Inoltre, l‚Äôaggiornamento dei modelli su ogni dispositivo pu√≤ essere pi√π complicato, poich√© spesso richiede di distribuire le nuove versioni su ogni dispositivo singolarmente, anzich√© aggiornare senza problemi un singolo modello nel cloud.\nL‚Äôapprendimento su dispositivo apre nuove possibilit√† abilitando l‚Äôintelligenza artificiale offline mantenendo al contempo la privacy dell‚Äôutente. Tuttavia, richiede una gestione attenta della complessit√† dei modelli e dei dati entro i limiti dei dispositivi dei consumatori. Trovare il giusto equilibrio tra localizzazione e offload dal cloud √® fondamentale per ottimizzare le esperienze su dispositivo.\n\n12.2.1 Vantaggi\n\nPrivacy e Sicurezza dei Dati\nUno dei vantaggi significativi dell‚Äôapprendimento sul dispositivo √® la maggiore privacy e sicurezza dei dati degli utenti. Ad esempio, si consideri uno smartwatch che monitora parametri sanitari sensibili come la frequenza cardiaca e la pressione sanguigna. Elaborando i dati e adattando i modelli direttamente sul dispositivo, i dati biometrici rimangono localizzati, aggirando la necessit√† di trasmettere dati grezzi ai server cloud dove potrebbero essere soggetti a violazioni.\nLe violazioni dei server sono tutt‚Äôaltro che rare, con milioni di record compromessi ogni anno. Ad esempio, la violazione di Equifax del 2017 ha esposto i dati personali di 147 milioni di persone. Mantenendo i dati sul dispositivo, il rischio di tali esposizioni √® drasticamente ridotto. L‚Äôapprendimento sul dispositivo elimina la dipendenza dall‚Äôarchiviazione cloud centralizzata e protegge dall‚Äôaccesso non autorizzato da varie minacce, tra cui attori malintenzionati, minacce interne ed esposizione accidentale.\nRegolamenti come l‚ÄôHealth Insurance Portability and Accountability Act (HIPAA) e il General Data Protection Regulation (GDPR) impongono rigorosi requisiti di riservatezza dei dati che l‚Äôapprendimento sul dispositivo affronta abilmente. Garantendo che i dati rimangano localizzati e non vengano trasferiti ad altri sistemi, l‚Äôapprendimento sul dispositivo facilita la conformit√† a tali regolamenti.\nL‚Äôapprendimento sul dispositivo non √® solo vantaggioso per i singoli utenti; ha implicazioni significative per le organizzazioni e i settori che gestiscono dati altamente sensibili. Ad esempio, in ambito militare, l‚Äôapprendimento sul dispositivo consente ai sistemi di prima linea di adattare modelli e funzioni indipendentemente dalle connessioni ai server centrali che potrebbero essere potenzialmente compromessi. Le informazioni critiche e sensibili sono saldamente protette dalla localizzazione dell‚Äôelaborazione e dell‚Äôapprendimento dei dati. Tuttavia, lo svantaggio √® che i singoli dispositivi assumono un valore maggiore e possono incentivarne il furto o la distruzione, poich√© diventano gli unici portatori di modelli di IA specializzati. √à necessario prestare attenzione alla protezione dei dispositivi stessi durante la transizione all‚Äôapprendimento sul dispositivo.\n√à inoltre importante preservare la privacy, la sicurezza e la conformit√† normativa dei dati personali e sensibili. Invece che nel cloud, i modelli di training e operativi aumentano sostanzialmente le misure di privacy a livello locale, assicurando che i dati degli utenti siano protetti da potenziali minacce.\nTuttavia, questo √® solo parzialmente intuitivo perch√© l‚Äôapprendimento sul dispositivo potrebbe invece esporre i sistemi a nuovi attacchi alla privacy. Con preziosi riepiloghi dei dati e aggiornamenti dei modelli archiviati in modo permanente su singoli dispositivi, potrebbe essere molto pi√π difficile proteggerli fisicamente e digitalmente rispetto a un grande cluster di elaborazione. Mentre l‚Äôapprendimento sul dispositivo riduce la quantit√† di dati compromessi in una qualsiasi violazione, potrebbe anche introdurre nuovi pericoli disperdendo informazioni sensibili su molti terminali decentralizzati. Le pratiche di sicurezza attente sono ancora essenziali per i sistemi ‚Äúon-device‚Äù.\n\n\nNormativa di Conformit√†\nL‚Äôapprendimento sul dispositivo aiuta ad affrontare le principali normative sulla privacy come GDPR)e CCPA. Queste normative richiedono la localizzazione dei dati, limitando i trasferimenti di dati transfrontalieri a paesi approvati con controlli adeguati. Il GDPR impone inoltre requisiti di ‚Äúprivacy by design‚Äù e consenso per la raccolta dei dati. Mantenendo l‚Äôelaborazione dei dati e il training del modello localizzati sul dispositivo, i dati sensibili degli utenti non vengono trasferiti altrove. Ci√≤ evita importanti grattacapi di conformit√† per le organizzazioni.\nAd esempio, un fornitore di servizi sanitari che monitora i parametri vitali dei pazienti con dispositivi indossabili deve garantire che i trasferimenti di dati transfrontalieri siano conformi a HIPAA e GDPR se utilizza il cloud. Determinare le leggi del paese applicabili e garantire le approvazioni per i flussi di dati internazionali introduce oneri legali e ingegneristici. Con l‚Äôapprendimento on-device, nessun dato lascia il dispositivo, semplificando la conformit√†. Il tempo e le risorse spesi per la conformit√† vengono ridotti in modo significativo.\nSettori come sanit√†, finanza e governo, che hanno dati altamente regolamentati, possono trarre grandi vantaggi dal training sul dispositivo. Localizzando i dati e l‚Äôapprendimento, i requisiti normativi di privacy e sovranit√† dei dati vengono soddisfatti pi√π facilmente. Le soluzioni su dispositivo forniscono un modo efficiente per creare applicazioni di IA conformi.\nLe principali normative sulla privacy impongono restrizioni sullo spostamento transfrontaliero dei dati che l‚Äôapprendimento su dispositivo affronta intrinsecamente tramite elaborazione localizzata. Ci√≤ riduce l‚Äôonere di conformit√† per le organizzazioni che lavorano con dati regolamentati.\n\n\nRiduzione della Larghezza di Banda, dei Costi e Maggiore Efficienza\nUno dei principali vantaggi dell‚Äôapprendimento su dispositivo √® la significativa riduzione dell‚Äôutilizzo della larghezza di banda e dei costi associati all‚Äôinfrastruttura cloud. Mantenendo i dati localizzati per l‚Äôaddestramento del modello anzich√© trasmettere dati grezzi al cloud, l‚Äôapprendimento su dispositivo pu√≤ comportare notevoli risparmi di larghezza di banda. Ad esempio, una rete di telecamere che analizzano i filmati video pu√≤ ottenere significative riduzioni nel trasferimento di dati addestrando i modelli sul dispositivo anzich√© trasmettere in streaming tutti i filmati video al cloud per l‚Äôelaborazione.\nQuesta riduzione nella trasmissione dei dati consente di risparmiare larghezza di banda e si traduce in costi inferiori per server, reti e archiviazione dei dati nel cloud. Le grandi organizzazioni, che potrebbero spendere milioni in infrastrutture cloud per addestrare i modelli, possono riscontrare notevoli riduzioni dei costi grazie all‚Äôapprendimento sul dispositivo. Nell‚Äôera dell‚Äôintelligenza artificiale generativa, in cui i costi sono aumentati in modo significativo, trovare modi per contenere le spese √® diventato sempre pi√π importante.\nInoltre, anche i costi energetici e ambientali della gestione di grandi server farm sono diminuiti. I data center consumano grandi quantit√† di energia, contribuendo alle emissioni di gas serra. Riducendo la necessit√† di un‚Äôampia infrastruttura basata su cloud, l‚Äôapprendimento sui dispositivi contribuisce a mitigare l‚Äôimpatto ambientale dell‚Äôelaborazione dei dati (Wu et al. 2022).\n\nWu, Carole-Jean, Ramya Raghavendra, Udit Gupta, Bilge Acun, Newsha Ardalani, Kiwan Maeng, Gloria Chang, et al. 2022. ¬´Sustainable ai: Environmental implications, challenges and opportunities¬ª. Proceedings of Machine Learning and Systems 4: 795‚Äì813.\nSpecificamente per le applicazioni endpoint [finali], l‚Äôapprendimento sui dispositivi riduce al minimo il numero di chiamate API di rete necessarie per eseguire l‚Äôinferenza tramite un provider cloud. I costi cumulativi associati alla larghezza di banda e alle chiamate API possono aumentare rapidamente per le applicazioni con milioni di utenti. Al contrario, eseguire training e inferenze localmente √® notevolmente pi√π efficiente e conveniente. Con ottimizzazioni all‚Äôavanguardia, √® stato dimostrato che l‚Äôapprendimento on-device riduce i requisiti di memoria del training, migliora drasticamente l‚Äôefficienza della memoria e riduce fino al 20% la latenza per iterazione (Dhar et al. 2021).\n\n\nAddestramento Permanente\nUno dei principali vantaggi dell‚Äôapprendimento sul dispositivo √® la sua capacit√† di supportare l‚Äôaddestramento permanente, consentendo ai modelli di adattarsi continuamente ai nuovi dati e all‚Äôevoluzione del comportamento dell‚Äôutente direttamente sul dispositivo. In ambienti dinamici, i modelli di dati possono cambiare nel tempo, un fenomeno noto come deriva dei dati, che pu√≤ degradare l‚Äôaccuratezza e la pertinenza del modello se questo rimane statico. Ad esempio, le preferenze dell‚Äôutente, le tendenze stagionali o persino le condizioni esterne (come i modelli di traffico di rete o le condizioni meteorologiche) possono evolversi, richiedendo ai modelli di adattarsi per mantenere prestazioni ottimali.\nL‚Äôapprendimento sul dispositivo consente ai modelli di affrontare questo problema adattandosi in modo incrementale man mano che diventano disponibili nuovi dati. Questo processo di adattamento continuo consente ai modelli di rimanere pertinenti ed efficaci, riducendo la necessit√† di frequenti aggiornamenti cloud. Gli adattamenti locali riducono la necessit√† di trasmettere grandi set di dati al cloud per la riqualificazione, risparmiando larghezza di banda e garantendo la privacy dei dati.\n\n\n\n12.2.2 Limitazioni\nMentre i tradizionali sistemi ML basati su cloud hanno accesso a risorse di elaborazione pressoch√© infinite, l‚Äôapprendimento sul dispositivo √® spesso limitato nella potenza di elaborazione e di archiviazione del dispositivo edge su cui viene addestrato il modello. Per definizione, un dispositivo edge √® un dispositivo con risorse di elaborazione, memoria ed energia limitate che non possono essere facilmente aumentate o diminuite. Pertanto, la dipendenza dai dispositivi edge pu√≤ limitare la complessit√†, l‚Äôefficienza e le dimensioni dei modelli ML sul dispositivo.\n\nRisorse di elaborazione\nI tradizionali sistemi ML basati su cloud utilizzano grandi server con pi√π GPU o TPU di fascia alta, che forniscono una potenza di calcolo e una memoria pressoch√© infinite. Ad esempio, servizi come Amazon Web Services (AWS) EC2 consentono di configurare cluster di istanze GPU per un training parallelo massiccio.\nAl contrario, l‚Äôapprendimento sul dispositivo √® limitato dall‚Äôhardware del dispositivo edge su cui viene eseguito. I dispositivi edge si riferiscono a endpoint come smartphone, elettronica embedded e dispositivi IoT. Per definizione, questi dispositivi hanno risorse di elaborazione, memoria ed energia molto limitate rispetto al cloud.\nAd esempio, uno smartphone tipico o Raspberry Pi pu√≤ avere solo pochi core CPU, pochi GB di RAM e una piccola batteria. Ancora pi√π limitati in termini di risorse sono i dispositivi microcontrollore TinyML come Arduino Nano BLE Sense. Le risorse sono fisse su questi dispositivi e non possono essere facilmente aumentate su richiesta, come il ridimensionamento dell‚Äôinfrastruttura cloud. Questa dipendenza dai dispositivi edge limita direttamente la complessit√†, l‚Äôefficienza e le dimensioni dei modelli che possono essere distribuiti per l‚Äôaddestramento sul dispositivo:\n\nComplessit√†: I limiti di memoria, elaborazione e potenza limitano la progettazione dell‚Äôarchitettura del modello, cos√¨ come il numero di layer e dei parametri.\nEfficienza: I modelli devono essere fortemente ottimizzati tramite metodi come la quantizzazione e la potatura per essere eseguiti pi√π velocemente e consumare meno energia.\nDimensioni: I file del modello effettivo devono essere compressi il pi√π possibile per rientrare nei limiti di archiviazione dei dispositivi edge.\n\nPertanto, mentre il cloud offre una scalabilit√† infinita, l‚Äôapprendimento sul dispositivo deve operare entro i rigidi vincoli di risorse dell‚Äôhardware. Ci√≤ richiede un‚Äôattenta progettazione congiunta di modelli semplificati, metodi di addestramento e ottimizzazioni su misura specificamente per i dispositivi edge.\n\n\nDimensioni, Accuratezza e Generalizzazione del Dataset\nOltre alle risorse di elaborazione limitate, l‚Äôapprendimento sul dispositivo √® anche limitato dal set di dati disponibile per i modelli di training.\nNel cloud, i modelli vengono addestrati su dataset enormi e diversi come ImageNet o Common Crawl. Ad esempio, ImageNet contiene oltre 14 milioni di immagini attentamente categorizzate in migliaia di classi.\nL‚Äôapprendimento sul dispositivo si basa invece su ‚Äúdata silos‚Äù pi√π piccoli e decentralizzati, unici per ogni dispositivo. Il rullino fotografico di uno smartphone potrebbe contenere solo migliaia di foto degli interessi e degli ambienti degli utenti.\nNell‚Äôapprendimento automatico, un training efficace del modello spesso presuppone che i dati siano ‚Äúindependent and identically distributed (IID)‚Äù [indipendenti e distribuiti in modo identico]. Ci√≤ significa che ogni punto dati viene generato in modo indipendente (senza influenzare altri punti) e segue la stessa distribuzione statistica del resto dei dati. Quando i dati sono IID, i modelli addestrati su di essi hanno maggiori probabilit√† di generalizzare bene a nuovi dati simili. Tuttavia, nell‚Äôapprendimento su dispositivo, questa condizione IID √® raramente soddisfatta, poich√© i dati sono altamente specifici per singoli utenti e contesti. Ad esempio, due amici possono scattare foto simili degli stessi luoghi, creando dati correlati che non rappresentano una popolazione pi√π ampia o la variet√† necessaria per la generalizzazione.\nMotivi per cui i dati potrebbero essere non IID nelle impostazioni sul dispositivo:\n\nEterogeneit√† degli utenti: Utenti diversi hanno interessi e ambienti diversi.\nDifferenze tra dispositivi: Sensori, regioni e dati demografici influenzano i dati.\nEffetti temporali: Ora del giorno, impatti stagionali sui dati.\n\nL‚Äôefficacia del ML si basa in gran parte su dati di training ampi e diversificati. Con set di dati piccoli e localizzati, i modelli on-device potrebbero non riuscire a generalizzare tra diverse popolazioni di utenti e ambienti. Ad esempio, un modello di rilevamento delle malattie addestrato solo su immagini di un singolo ospedale non si generalizzerebbe bene ad altri dati demografici dei pazienti. Le prestazioni nel mondo reale non potranno che migliorare con progressi medici estesi e diversificati. Quindi, mentre l‚Äôapprendimento basato su cloud sfrutta enormi set di dati, l‚Äôapprendimento su dispositivo si basa su ‚Äúsilo di dati‚Äù decentralizzati molto pi√π piccoli, unici per ogni utente.\nI dati limitati e le ottimizzazioni richieste per l‚Äôapprendimento on-device possono avere un impatto negativo sulla precisione e sulla generalizzazione del modello:\n\nI piccoli dataset aumentano il rischio di overfitting. Ad esempio, un classificatore di frutta addestrato su 100 immagini rischia di overfitting rispetto a uno addestrato su 1 milione di immagini diverse.\nI dati rumorosi generati dall‚Äôutente riducono la qualit√†. Il rumore del sensore o l‚Äôetichettatura impropria dei dati da parte di non esperti possono degradare l‚Äôaddestramento.\nOttimizzazioni come la potatura e la quantizzazione compromettono la precisione per l‚Äôefficienza. Un modello quantizzato a 8 bit funziona pi√π velocemente ma meno accuratamente di un modello a 32 bit.\n\nQuindi, mentre i modelli cloud raggiungono un‚Äôelevata precisione con enormi set di dati e senza vincoli, i modelli su dispositivo possono avere difficolt√† a generalizzare. Alcuni studi dimostrano che il training sul dispositivo corrisponde all‚Äôaccuratezza del cloud su determinate attivit√†. Tuttavia, le prestazioni sui carichi di lavoro reali richiedono ulteriori studi (Lin et al. 2022). Ad esempio, un modello cloud pu√≤ rilevare con precisione la polmonite nelle radiografie del torace di migliaia di ospedali. Tuttavia, un modello sul dispositivo addestrato solo su una piccola popolazione locale di pazienti potrebbe non riuscire a generalizzare. Ci√≤ limita l‚Äôapplicabilit√† pratica dell‚Äôapprendimento sui dispositivi per utilizzi critici come la diagnosi delle malattie o i veicoli a guida autonoma.\nIl training sul dispositivo √® anche pi√π lento del cloud a causa delle risorse limitate. Anche se ogni iterazione √® pi√π veloce, il processo di training complessivo richiede pi√π tempo. Ad esempio, un‚Äôapplicazione di robotica in tempo reale potrebbe richiedere aggiornamenti del modello entro millisecondi. L‚ÄôOn-device training su un piccolo hardware embedded potrebbe richiedere secondi o minuti per l‚Äôaggiornamento, troppo lento per l‚Äôuso in tempo reale.\nLe sfide relative a precisione, generalizzazione e velocit√† pongono ostacoli all‚Äôadozione dell‚Äôapprendimento on-device per sistemi di produzione reali, soprattutto quando affidabilit√† e bassa latenza sono fondamentali.",
    "crumbs": [
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Apprendimento On-Device</span>"
    ]
  },
  {
    "objectID": "contents/core/ondevice_learning/ondevice_learning.it.html#adattamento-on-device",
    "href": "contents/core/ondevice_learning/ondevice_learning.it.html#adattamento-on-device",
    "title": "12¬† Apprendimento On-Device",
    "section": "12.3 Adattamento On-device",
    "text": "12.3 Adattamento On-device\nIn un‚Äôattivit√† ML, il consumo di risorse proviene principalmente da tre fonti:\n\nIl modello ML stesso;\nIl processo di ottimizzazione durante l‚Äôapprendimento del modello\nArchiviazione ed elaborazione del dataset utilizzato per l‚Äôapprendimento.\n\nDi conseguenza, ci sono tre approcci per adattare gli algoritmi ML esistenti su dispositivi con risorse limitate:\n\nRiduzione della complessit√† del modello ML\nModifica delle ottimizzazioni per ridurre i requisiti delle risorse di training\nCreazione di nuove rappresentazioni dei dati pi√π efficienti in termini di archiviazione\n\nNella sezione seguente, esamineremo questi metodi di adattamento dell‚Äôapprendimento on-device. Il capitolo Ottimizzazioni dei Modelli fornisce maggiori dettagli sulle ottimizzazioni del modello.\n\n12.3.1 Riduzione della Complessit√† del Modello\nIn questa sezione, discuteremo brevemente i modi per ridurre la complessit√† del modello quando si adattano i modelli ML sul dispositivo. Per i dettagli sulla riduzione della complessit√† del modello, fare riferimento al capitolo Ottimizzazioni dei Modelli.\n\nAlgoritmi ML tradizionali\nA causa delle limitazioni di elaborazione e memoria dei dispositivi edge, alcuni algoritmi ML tradizionali sono ottimi candidati per applicazioni di apprendimento on-device grazie alla loro natura leggera. Alcuni esempi di algoritmi con basso impatto sulle risorse includono Naive Bayes Classifiers, Support Vector Machines (SVM), Linear Regression, Logistic Regression e algoritmi Decision Tree selezionati.\nCon alcuni perfezionamenti, questi algoritmi ML classici possono essere adattati a specifiche architetture hardware ed eseguire attivit√† semplici. I loro bassi requisiti di prestazioni semplificano l‚Äôintegrazione dell‚Äôapprendimento continuo anche su dispositivi edge.\n\n\nPruning\nCome discusso in Sezione 9.2.1, la potatura √® una tecnica fondamentale per ridurre le dimensioni e la complessit√† dei modelli ML. Per l‚Äôapprendimento su dispositivo, la potatura √® particolarmente preziosa, poich√© riduce al minimo il consumo di risorse mantenendo un‚Äôaccuratezza competitiva. Rimuovendo i componenti meno informativi di un modello, la potatura consente ai modelli ML di funzionare in modo pi√π efficiente su dispositivi con risorse limitate.\nNel contesto dell‚Äôapprendimento su dispositivo, la potatura viene applicata per adattare modelli di deep learning complessi alla memoria limitata e alla potenza di elaborazione dei dispositivi edge. Ad esempio, la potatura pu√≤ ridurre il numero di neuroni o connessioni in una DNN, con conseguente consumo di meno memoria e minori calcoli. Questo approccio semplifica la struttura della rete neurale, con conseguente modello pi√π compatto ed efficiente.\n\n\nRiduzione della Complessit√† dei Modelli di Deep Learning\nI framework DNN tradizionali basati su cloud hanno un sovraccarico di memoria troppo elevato per essere utilizzati sul dispositivo. Ad esempio, i sistemi di deep learning come PyTorch e TensorFlow richiedono centinaia di megabyte di overhead di memoria durante l‚Äôaddestramento di modelli come MobilenetV2 e l‚Äôoverhead aumenta con l‚Äôaumentare del numero di parametri di addestramento.\nLa ricerca attuale per DNN leggeri esplora principalmente architetture CNN. Esistono anche diversi framework ‚Äúbare-metal‚Äù [tutto in hardware] progettati per eseguire reti neurali su MCU mantenendo bassi l‚Äôoverhead computazionale e l‚Äôingombro di memoria. Alcuni esempi includono MNN, TVM e TensorFlow Lite. Tuttavia, possono eseguire l‚Äôinferenza solo durante i passaggi in avanti e non supportano la backpropagation. Sebbene questi modelli siano progettati per l‚Äôimplementazione edge, la loro riduzione nei pesi del modello e nelle connessioni architettoniche ha portato a minori requisiti di risorse per l‚Äôapprendimento continuo.\nIl compromesso tra prestazioni e supporto del modello √® chiaro quando si adattano i sistemi DNN pi√π diffusi. Come adattiamo i modelli DNN esistenti a impostazioni con risorse limitate mantenendo il supporto per la backpropagation e l‚Äôapprendimento continuo? Le ultime ricerche suggeriscono tecniche di progettazione congiunta di algoritmi e sistemi che aiutano a ridurre il consumo di risorse dell‚Äôaddestramento ML sui dispositivi edge. Utilizzando tecniche come il ‚Äúquantization-aware scaling‚Äù (QAS) [ridimensionamento consapevole della quantizzazione], aggiornamenti sparsi e altre tecniche all‚Äôavanguardia, l‚Äôapprendimento sul dispositivo √® possibile su sistemi embedded con poche centinaia di kilobyte di RAM senza memoria aggiuntiva mantenendo un‚Äôelevata precisione.\n\n\n\n12.3.2 Modifica dei Processi di Ottimizzazione\nLa scelta della giusta strategia di ottimizzazione √® importante per l‚Äôaddestramento DNN su un dispositivo, poich√© consente di trovare un buon minimo locale. Poich√© l‚Äôaddestramento avviene su un dispositivo, questa strategia deve anche considerare la memoria e la potenza limitate.\n\nQuantization-Aware Scaling\nLa quantizzazione √® un metodo comune per ridurre l‚Äôimpronta di memoria dell‚Äôaddestramento DNN. Sebbene ci√≤ possa introdurre nuovi errori, questi possono essere mitigati progettando un modello per caratterizzare questo errore statistico. Ad esempio, i modelli potrebbero utilizzare l‚Äôarrotondamento stocastico o introdurre l‚Äôerrore di quantizzazione negli aggiornamenti del gradiente.\nUna tecnica algoritmica specifica √® Quantization-Aware Scaling (QAS), che migliora le prestazioni delle reti neurali su hardware a bassa precisione, come dispositivi edge, dispositivi mobili o sistemi TinyML, regolando i fattori di scala durante il processo di quantizzazione.\nCome abbiamo discusso nel capitolo Ottimizzazioni dei Modelli, la quantizzazione √® il processo di mappatura di un intervallo continuo di valori in un set discreto di valori. Nel contesto delle reti neurali, questo spesso comporta la riduzione della precisione di pesi e attivazioni da virgola mobile a 32 bit a formati di precisione inferiore come gli interi a 8 bit. Questa riduzione della precisione pu√≤ ridurre significativamente il costo computazionale e l‚Äôingombro di memoria del modello, rendendolo adatto per l‚Äôimplementazione su hardware a bassa precisione. Figura¬†12.2 illustra questo concetto, mostrando un esempio di quantizzazione da float a intero in cui i valori in virgola mobile ad alta precisione vengono mappati in una rappresentazione intera pi√π compatta. Questa rappresentazione visiva aiuta a chiarire come la quantizzazione pu√≤ mantenere la struttura essenziale dei dati riducendone al contempo la complessit√† e i requisiti di archiviazione.\n\n\n\n\n\n\nFigura¬†12.2: Quantizzazione float-to-integer. Fonte: Nvidia.\n\n\n\nTuttavia, il processo di quantizzazione pu√≤ anche introdurre errori di quantizzazione che possono degradare le prestazioni del modello. La scalatura basata sulla quantizzazione √® una tecnica che riduce al minimo questi errori regolando i fattori di scala utilizzati nel processo di quantizzazione.\nIl processo QAS prevede due fasi principali:\n\nAddestramento basato sulla quantizzazione: In questa fase, la rete neurale viene addestrata tenendo conto della quantizzazione, simulandola per imitarne gli effetti durante i passaggi ‚Äúforward‚Äù e ‚Äúbackward‚Äù. Ci√≤ consente al modello di imparare a compensare gli errori di quantizzazione e migliorarne le prestazioni su hardware a bassa precisione. Per i dettagli, fare riferimento alla sezione QAT in Ottimizzazioni del modello.\nQuantizzazione e ridimensionamento: Dopo l‚Äôaddestramento, il modello viene quantizzato in un formato a bassa precisione e i fattori di scala vengono regolati per ridurre al minimo gli errori di quantizzazione. I fattori di scala vengono scelti in base alla distribuzione dei pesi e delle attivazioni nel modello e vengono regolati per garantire che i valori quantizzati siano compresi nell‚Äôintervallo del formato a bassa precisione.\n\nQAS viene utilizzato per superare le difficolt√† di ottimizzazione dei modelli su dispositivi minuscoli senza dover effettuare la messa a punto degli iperparametri; QAS ridimensiona automaticamente i gradienti tensoriali con varie precisioni di bit. Ci√≤ stabilizza il processo di addestramento e corrisponde all‚Äôaccuratezza della precisione in virgola mobile.\n\n\nAggiornamenti Sparsi\nSebbene QAS consenta l‚Äôottimizzazione di un modello quantizzato, utilizza una grande quantit√† di memoria, il che non √® realistico per l‚Äôaddestramento sul dispositivo. Quindi, gli aggiornamenti ‚Äúspare‚Äù vengono utilizzati per ridurre l‚Äôingombro di memoria del calcolo ‚Äúfull backward‚Äù. Invece di potare i pesi per l‚Äôinferenza, l‚Äôaggiornamento sparso pota il gradiente durante la ‚Äúbackward propagation‚Äù [propagazione all‚Äôindietro] per aggiornare il modello in modo sparso. In altre parole, l‚Äôaggiornamento sparso salta i gradienti del calcolo di layer e sottotensori meno importanti.\nTuttavia, determinare lo schema di un aggiornamento sparso ottimale dato un budget di memoria vincolante pu√≤ essere difficile a causa dell‚Äôampio spazio di ricerca. Ad esempio, il modello MCUNet ha 43 layer convoluzionali e uno spazio di ricerca di circa \\(10^{30}\\). Una tecnica per affrontare questo problema √® l‚Äôanalisi del contributo. L‚Äôanalisi del contributo misura il miglioramento dell‚Äôaccuratezza dai bias (aggiornamento degli ultimi bias rispetto al solo aggiornamento del classificatore) e pesi (aggiornamento del peso di un layer extra rispetto al solo aggiornamento del bias). Cercando di massimizzare questi miglioramenti, l‚Äôanalisi del contributo deriva automaticamente uno schema di aggiornamento sparso ottimale per abilitare l‚Äôaddestramento sul dispositivo.\n\n\nTraining Layer-Wise\nAltri metodi oltre alla quantizzazione possono aiutare a ottimizzare le routine. Uno di questi metodi √® l‚Äôaddestramento ‚Äúlayer-wise‚Äù. Un consumatore significativo di memoria dell‚Äôaddestramento DNN √® la backpropagation end-to-end, che richiede che tutte le feature map intermedie siano archiviate in modo che il modello possa calcolare i gradienti. Un‚Äôalternativa a questo approccio che riduce l‚Äôimpronta di memoria dell‚Äôaddestramento DNN √® l‚Äôaddestramento sequenziale ‚Äúlayer-by-layer‚Äù (T. Chen et al. 2016). Invece dell‚Äôaddestramento end-to-end, l‚Äôaddestramento di un singolo layer alla volta aiuta a evitare di dover archiviare le feature map intermedie.\n\nChen, Tianqi, Bing Xu, Chiyuan Zhang, e Carlos Guestrin. 2016. ¬´Training Deep Nets with Sublinear Memory Cost¬ª. ArXiv preprint abs/1604.06174 (aprile). http://arxiv.org/abs/1604.06174v2.\n\n\nTrading Computation for Memory\nLa strategia ‚Äútrading computation for memory‚Äù [scambio di elaborazione per memoria] comporta il rilascio di parte della memoria utilizzata per archiviare i risultati intermedi. Invece, questi risultati possono essere ricalcolati in base alle necessit√†. √à stato dimostrato che la riduzione della memoria in cambio di pi√π elaborazione riduce l‚Äôimpronta di memoria dell‚Äôaddestramento DNN per adattarsi a quasi tutti i budget, riducendo al minimo anche i costi di elaborazione (Gruslys et al. 2016).\n\nGruslys, Audrunas, R√©mi Munos, Ivo Danihelka, Marc Lanctot, e Alex Graves. 2016. ¬´Memory-Efficient Backpropagation Through Time¬ª. In Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, a cura di Daniel D. Lee, Masashi Sugiyama, Ulrike von Luxburg, Isabelle Guyon, e Roman Garnett, 4125‚Äì33. https://proceedings.neurips.cc/paper/2016/hash/a501bebf79d570651ff601788ea9d16d-Abstract.html.\n\n\n\n12.3.3 Sviluppo di Nuove Rappresentazioni dei Dati\nLa dimensionalit√† e il volume dei dati di training possono avere un impatto significativo sull‚Äôadattamento sul dispositivo. Quindi, un‚Äôaltra tecnica per adattare i modelli su dispositivi con risorse limitate √® quella di rappresentare i set di dati in modo pi√π efficiente.\n\nCompressione dei Dati\nL‚Äôobiettivo della compressione dei dati √® raggiungere elevate precisioni limitando al contempo la quantit√† di dati di training. Un metodo per raggiungere questo obiettivo √® dare priorit√† alla complessit√† del campione: la quantit√† di dati di training necessari affinch√© l‚Äôalgoritmo raggiunga una precisione target (Dhar et al. 2021).\n\nDhar, Sauptik, Junyao Guo, Jiayi (Jason) Liu, Samarth Tripathi, Unmesh Kurup, e Mohak Shah. 2021. ¬´A Survey of On-Device Machine Learning: An Algorithms and Learning Theory Perspective¬ª. ACM Transactions on Internet of Things 2 (3): 1‚Äì49. https://doi.org/10.1145/3450494.\n\nDarvish Rouhani, Bita, Azalia Mirhoseini, e Farinaz Koushanfar. 2017. ¬´TinyDL: Just-in-time deep learning solution for constrained embedded systems¬ª. In 2017 IEEE International Symposium on Circuits and Systems (ISCAS), 1‚Äì4. IEEE. https://doi.org/10.1109/iscas.2017.8050343.\n\nLi, Xiang, Tao Qin, Jian Yang, e Tie-Yan Liu. 2016. ¬´LightRNN: Memory and Computation-Efficient Recurrent Neural Networks¬ª. In Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, a cura di Daniel D. Lee, Masashi Sugiyama, Ulrike von Luxburg, Isabelle Guyon, e Roman Garnett, 4385‚Äì93. https://proceedings.neurips.cc/paper/2016/hash/c3e4035af2a1cde9f21e1ae1951ac80b-Abstract.html.\nAltri metodi pi√π comuni di compressione dei dati si concentrano sulla riduzione della dimensionalit√† e del volume dei dati di training. Ad esempio, un approccio potrebbe sfruttare la sparsit√† della matrice per ridurre l‚Äôingombro di memoria per l‚Äôarchiviazione dei dati di training. I dati di training possono essere trasformati in un embedding a dimensione inferiore e fattorizzati in una matrice di dizionario moltiplicata per una matrice di coefficienti blocchi sparsi (Darvish Rouhani, Mirhoseini, e Koushanfar 2017). Un altro esempio potrebbe riguardare la rappresentazione di parole provenienti da un ampio set di dati di training linguistica in un formato vettoriale pi√π compresso (Li et al. 2016).",
    "crumbs": [
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Apprendimento On-Device</span>"
    ]
  },
  {
    "objectID": "contents/core/ondevice_learning/ondevice_learning.it.html#il-transfer-learning",
    "href": "contents/core/ondevice_learning/ondevice_learning.it.html#il-transfer-learning",
    "title": "12¬† Apprendimento On-Device",
    "section": "12.4 Il Transfer Learning",
    "text": "12.4 Il Transfer Learning\nIl transfer learning √® una tecnica in cui un modello sviluppato per un‚Äôattivit√† specifica viene riutilizzato come punto di partenza per un modello su una seconda attivit√†. Il transfer learning ci consente di sfruttare modelli pre-addestrati che hanno gi√† appreso rappresentazioni utili da grandi set di dati e di perfezionarli per attivit√† specifiche utilizzando set di dati pi√π piccoli direttamente sul dispositivo. Ci√≤ pu√≤ ridurre significativamente le risorse di calcolo e il tempo necessari per l‚Äôaddestramento dei modelli da zero.\nPu√≤ essere compreso attraverso esempi intuitivi del mondo reale, come illustrato in Figura¬†12.3. La figura mostra scenari in cui le competenze di un dominio possono essere applicate per accelerare l‚Äôapprendimento in un campo correlato. Un esempio lampante √® la relazione tra andare in bicicletta e andare in moto. Se si sa andare in bicicletta, si ha gi√† l‚Äôabilit√† di stare in equilibrio su un veicolo a due ruote. La conoscenza di base di questa abilit√† rende significativamente pi√π facile per te imparare a guidare una moto rispetto a qualcuno senza alcuna esperienza ciclistica. La figura illustra questo e altri scenari simili, dimostrando come l‚Äôapprendimento per trasferimento sfrutti le conoscenze esistenti per accelerare l‚Äôacquisizione di nuove competenze correlate.\n\n\n\n\n\n\nFigura¬†12.3: Trasferimento di conoscenze tra attivit√†. Fonte: Zhuang et al. (2021).\n\n\nZhuang, Fuzhen, Zhiyuan Qi, Keyu Duan, Dongbo Xi, Yongchun Zhu, Hengshu Zhu, Hui Xiong, e Qing He. 2021. ¬´A Comprehensive Survey on Transfer Learning¬ª. Proceedings of the IEEE 109 (1): 43‚Äì76. https://doi.org/10.1109/jproc.2020.3004555.\n\n\nPrendiamo l‚Äôesempio di un‚Äôapplicazione di sensore intelligente che utilizza l‚Äôintelligenza artificiale on-device per riconoscere gli oggetti nelle immagini acquisite dal dispositivo. Tradizionalmente, ci√≤ richiederebbe l‚Äôinvio dei dati dell‚Äôimmagine a un server, dove un ampio modello di rete neurale elabora i dati e invia i risultati. Con l‚Äôintelligenza artificiale on-device, il modello viene archiviato ed eseguito direttamente sul dispositivo, eliminando la necessit√† di inviare dati a un server.\nPer personalizzare il modello per le caratteristiche on-device, addestrare un modello di rete neurale da zero sul dispositivo sarebbe poco pratico a causa delle risorse di calcolo limitate e della durata della batteria. √à qui che entra in gioco il ‚Äútransfer learning‚Äù [apprendimento tramite trasferimento]. Invece di addestrare un modello da zero, possiamo prendere un modello pre-addestrato, come una rete neurale convoluzionale (CNN) o una rete di trasformatori addestrata su un ampio set di dati di immagini, e perfezionarlo per la nostra specifica attivit√† di riconoscimento degli oggetti. Questa messa a punto pu√≤ essere eseguita direttamente sul dispositivo utilizzando un set di dati pi√π piccolo di immagini pertinenti all‚Äôattivit√†. Sfruttando il modello pre-addestrato, possiamo ridurre le risorse di calcolo e il tempo necessari per il training, ottenendo comunque un‚Äôelevata precisione per l‚Äôattivit√† di riconoscimento degli oggetti. Figura¬†12.4 illustra ulteriormente i vantaggi dell‚Äôapprendimento tramite trasferimento rispetto al training da zero.\n\n\n\n\n\n\nFigura¬†12.4: Training da zero vs.¬†apprendimento tramite trasferimento.\n\n\n\nIl transfer learning √® importante per rendere praticabile l‚Äôintelligenza artificiale on-device, consentendoci di sfruttare modelli pre-addestrati e di perfezionarli per attivit√† specifiche, riducendo cos√¨ le risorse di calcolo e il tempo necessari per il training. La combinazione di intelligenza artificiale sul dispositivo e il ‚Äútransfer learning‚Äù apre nuove possibilit√† per applicazioni di intelligenza artificiale pi√π attente alla privacy e pi√π reattive alle esigenze degli utenti.\nIl transfer learning ha rivoluzionato il modo in cui i modelli vengono sviluppati e distribuiti, sia nel cloud che nell‚Äôedge. Il transfer learning viene utilizzato nel mondo reale. Un esempio del genere √® l‚Äôuso del transfer learning per sviluppare modelli di intelligenza artificiale in grado di rilevare e diagnosticare malattie da immagini mediche, come raggi X, scansioni MRI [risonanza magnetica] e TAC. Ad esempio, i ricercatori della Stanford University hanno sviluppato un modello di apprendimento di trasferimento in grado di rilevare il cancro nelle immagini della pelle con una precisione del 97% (Esteva et al. 2017). Questo modello √® stato pre-addestrato su 1.28 milioni di immagini per classificare un‚Äôampia gamma di oggetti e poi specializzato per il rilevamento del cancro tramite l‚Äôaddestramento su un set di dati di immagini della pelle curato da dermatologi.\n\nEsteva, Andre, Brett Kuprel, Roberto A. Novoa, Justin Ko, Susan M. Swetter, Helen M. Blau, e Sebastian Thrun. 2017. ¬´Dermatologist-level classification of skin cancer with deep neural networks¬ª. Nature 542 (7639): 115‚Äì18. https://doi.org/10.1038/nature21056.\nIn contesti di produzione, l‚Äôimplementazione del transfer learning in genere comporta due fasi chiave: pre-distribuzione e post-distribuzione. La pre-distribuzione si concentra sulla preparazione del modello per il suo compito specializzato prima del rilascio, mentre la post-distribuzione consente al modello di adattarsi ulteriormente in base ai dati dei singoli utenti, migliorando la personalizzazione e l‚Äôaccuratezza nel tempo.\n\n12.4.1 Specializzazione Pre-Distribuzione\nNella fase di pre-implementazione, il transfer learning funge da catalizzatore per accelerare il processo di sviluppo. Ecco come funziona tipicamente: Si immagini di creare un sistema per riconoscere diverse razze di cani. Invece di partire da zero, possiamo utilizzare un modello pre-addestrato che ha gi√† padroneggiato il compito pi√π ampio di riconoscere gli animali nelle immagini.\nQuesto modello pre-addestrato funge da solida base e contiene una vasta conoscenza acquisita da dati estesi. Quindi perfezioniamo questo modello utilizzando un set di dati specializzato contenente immagini di varie razze di cani. Questo processo di messa a punto adatta il modello alle nostre esigenze specifiche, ovvero identificare con precisione le razze di cani. Una volta perfezionato e convalidato per soddisfare i criteri di prestazione, questo modello specializzato √® pronto per l‚Äôimplementazione.\nEcco come funziona in pratica:\n\nInizia con un Modello Pre-Addestrato: Si inizia selezionando un modello che √® gi√† stato addestrato su un set di dati completo, solitamente correlato a un‚Äôattivit√† generale. Questo modello funge da base per l‚Äôattivit√† in questione.\nFine-tuning: Il modello pre-addestrato viene poi perfezionato su un set di dati pi√π piccolo e specifico per l‚Äôattivit√† desiderata. Questo passaggio consente al modello di adattare e specializzare la sua conoscenza ai requisiti specifici dell‚Äôapplicazione.\nValidazione: Dopo la messa a punto, il modello viene convalidato per garantire che soddisfi i criteri di prestazione per l‚Äôattivit√† specializzata.\nDeployment: Una volta convalidato, il modello specializzato viene distribuito nell‚Äôambiente di produzione.\n\nQuesto metodo riduce significativamente il tempo e le risorse di calcolo necessarie per addestrare un modello da zero (Pan e Yang 2010). Adottando l‚Äôapprendimento tramite trasferimento, i sistemi embedded possono raggiungere un‚Äôelevata precisione su attivit√† specializzate senza la necessit√† di raccogliere dati estesi o di impiegare risorse di calcolo significative per l‚Äôaddestramento da zero.\n\nPan, Sinno Jialin, e Qiang Yang. 2010. ¬´A Survey on Transfer Learning¬ª. IEEE Transactions on Knowledge and Data Engineering 22 (10): 1345‚Äì59. https://doi.org/10.1109/tkde.2009.191.\n\n\n12.4.2 Adattamento Post-Distribuzione\nL‚Äôimplementazione su un dispositivo non deve necessariamente segnare il culmine del percorso educativo di un modello ML. Con l‚Äôavvento dell‚Äôapprendimento per trasferimento, apriamo le porte all‚Äôimplementazione di modelli ML adattivi in scenari del mondo reale, soddisfacendo le esigenze personalizzate degli utenti.\nConsideriamo un‚Äôapplicazione reale in cui un genitore desidera identificare il proprio figlio in una raccolta di immagini di un evento scolastico sul proprio smartphone. In questo scenario, il genitore si trova di fronte alla sfida di localizzare il proprio figlio in mezzo alle immagini di molti altri bambini. L‚Äôapprendimento per trasferimento pu√≤ essere impiegato qui per perfezionare il modello di un sistema embedded per questo compito unico e specializzato. Inizialmente, il sistema potrebbe utilizzare un modello generico addestrato per riconoscere i volti nelle immagini. Tuttavia, con l‚Äôapprendimento per trasferimento, il sistema pu√≤ adattare questo modello per riconoscere le caratteristiche specifiche del figlio dell‚Äôutente.\nEcco come funziona:\n\nRaccolta Dati: Il sistema embedded raccoglie immagini che includono il bambino, idealmente con l‚Äôinput del genitore per garantire accuratezza e pertinenza. Ci√≤ pu√≤ essere fatto direttamente sul dispositivo, mantenendo la privacy dei dati dell‚Äôutente.\nPerfezionamento sul Dispositivo: Il modello di riconoscimento facciale preesistente, che √® stato addestrato su un set di dati ampio e diversificato, viene poi perfezionato utilizzando le nuove immagini raccolte del bambino. Questo processo adatta il modello per riconoscere le caratteristiche facciali specifiche del bambino, distinguendolo dagli altri bambini nelle immagini.\nValidazione: Il modello rifinito viene poi convalidato per garantire che riconosca accuratamente il bambino in varie immagini. Ci√≤ pu√≤ comportare che il genitore verifichi le prestazioni del modello e fornisca feedback per ulteriori miglioramenti.\nUso Localizzato: Una volta adattato, il modello √® in grado di localizzare istantaneamente il bambino nelle foto, offrendo un‚Äôesperienza personalizzata senza dover ricorrere a risorse cloud o al trasferimento di dati.\n\nQuesta personalizzazione al volo migliora l‚Äôefficacia del modello per il singolo utente, assicurando che tragga vantaggio dalla personalizzazione ML. Questo √®, in parte, il modo in cui iPhotos o Google Photos funzionano quando ci chiedono di riconoscere un volto e poi, in base a queste informazioni, indicizzano tutte le foto di quel volto. Poich√© l‚Äôapprendimento e l‚Äôadattamento avvengono sul dispositivo stesso, non ci sono rischi per la privacy personale. Le immagini dei genitori non vengono caricate su un server cloud o condivise con terze parti, proteggendo la privacy della famiglia e continuando a raccogliere i benefici di un modello ML personalizzato. Questo approccio rappresenta un significativo passo avanti nella ricerca per fornire agli utenti soluzioni ML personalizzate che rispettino e sostengano la loro privacy.\n\n\n12.4.3 Vantaggi\nIl transfer learning √® diventato una tecnica importante in ML e intelligenza artificiale, ed √® particolarmente prezioso per diversi motivi.\n\nScarsit√† di Dati: In molte applicazioni del mondo reale, raccogliere un ampio set di dati etichettato per addestrare un modello ML da zero √® difficile, costoso e richiede molto tempo. Il transfer learning affronta questa sfida consentendo l‚Äôuso di modelli pre-addestrati che hanno gi√† appreso funzionalit√† preziose da vasti set di dati etichettati, riducendo cos√¨ la necessit√† di dati annotati estesi nella nuova attivit√†.\nSpese Computazionali: Addestrare un modello da zero richiede risorse computazionali e tempo significativi, specialmente per modelli complessi come reti neurali profonde. Utilizzando il transfer learning, possiamo sfruttare il calcolo che √® gi√† stato eseguito durante l‚Äôaddestramento del modello sorgente, risparmiando cos√¨ tempo e potenza computazionale.\n\nCi sono vantaggi nel riutilizzare le funzionalit√†:\n\nApprendimento di Feature Gerarchiche: I modelli di deep learning, in particolare le CNN, possono apprendere feature gerarchiche. I layer inferiori in genere apprendono funzionalit√† generiche come bordi e forme, mentre quelli superiori apprendono funzionalit√† pi√π complesse e specifiche per l‚Äôattivit√†. Il transfer learning ci consente di riutilizzare le funzionalit√† generiche apprese da un modello e di perfezionare i livelli superiori per la nostra attivit√† specifica.\nAumento delle Prestazioni: √à stato dimostrato che il transfer learning aumenta le prestazioni dei modelli su attivit√† con dati limitati. La conoscenza acquisita dall‚Äôattivit√† dall‚Äôattivit√† sorgente pu√≤ fornire un prezioso punto di partenza e portare a una convergenza pi√π rapida e a una maggiore accuratezza nell‚Äôattivit√† target.\n\n\n\n\n\n\n\nEsercizio¬†12.1: Il Transfer Learning\n\n\n\n\n\nSi immagini di addestrare un‚ÄôIA a riconoscere i fiori come un professionista, ma senza aver bisogno di un milione di immagini di fiori! Questo √® il potere del transfer learning. In questo Colab, prenderemo un‚ÄôIA che conosce gi√† le immagini e le insegneremo a diventare un‚Äôesperta di fiori con meno sforzo. Prepararsi a rendere la propria IA pi√π intelligente, non √® pi√π difficile!\n\n\n\n\n\n\n12.4.4 Concetti Fondamentali\nComprendere i concetti fondamentali del transfer learning √® essenziale per utilizzare efficacemente questo potente approccio in ML. Qui, analizzeremo alcuni dei principi e dei componenti principali che stanno alla base del processo di transfer learning.\n\nAttivit√† di Origine e di Destinazione\nNel transfer learning, sono coinvolte due attivit√† principali: l‚Äôattivit√† di origine e quella di destinazione. L‚Äôattivit√† di origine √® quella per la quale il modello √® gi√† stato addestrato e ha appreso informazioni preziose. L‚Äôattivit√† di destinazione √® la nuova attivit√† che vogliamo che il modello esegua. L‚Äôobiettivo del transfer learning √® sfruttare le conoscenze acquisite dall‚Äôattivit√† di origine per migliorare le prestazioni nell‚Äôattivit√† di destinazione.\nSupponiamo di avere un modello addestrato per riconoscere vari frutti nelle immagini (attivit√† di origine) e di voler creare un nuovo modello per riconoscere diverse verdure nelle immagini (attivit√† di destinazione). In tal caso, possiamo utilizzare il transfer learning per sfruttare le conoscenze acquisite durante l‚Äôattivit√† di riconoscimento della frutta per migliorare le prestazioni del modello di riconoscimento della verdura.\n\n\nTrasferimento della Rappresentazione\nIl trasferimento della rappresentazione riguarda le rappresentazioni apprese (caratteristiche) dall‚Äôattivit√† di origine all‚Äôattivit√† di destinazione. Esistono tre tipi principali di trasferimento della rappresentazione:\n\nTrasferimento di Istanza: Implica il riutilizzo delle istanze di dati dall‚Äôattivit√† di origine nell‚Äôattivit√† di destinazione.\nTrasferimento della Rappresentazione delle Feature: Implica il trasferimento delle rappresentazioni di Feature [funzionalit√†] apprese dall‚Äôattivit√† di origine all‚Äôattivit√† di destinazione.\nTrasferimento di Parametri: Implica il trasferimento dei parametri appresi del modello (pesi) dall‚Äôattivit√† di origine all‚Äôattivit√† di destinazione.\n\nNell‚Äôelaborazione del linguaggio naturale, un modello addestrato per comprendere la sintassi e la grammatica di una lingua (attivit√† di origine) pu√≤ trasferire le sue rappresentazioni apprese a un nuovo modello progettato per eseguire l‚Äôanalisi del sentiment (attivit√† di destinazione).\n\n\nFinetuning\nIl ‚Äúfinetuning‚Äù [messa a punto] √® il processo di regolazione dei parametri di un modello pre-addestrato per adattarlo all‚Äôattivit√† di destinazione. In genere, ci√≤ comporta l‚Äôaggiornamento dei pesi dei layer del modello, in particolare degli ultimi layer, per rendere il modello pi√π pertinente per la nuova attivit√†. Nella classificazione delle immagini, un modello pre-addestrato su un set di dati generale come ImageNet (attivit√† di origine) pu√≤ essere messo a punto regolando i pesi dei suoi livelli per ottenere buone prestazioni in un‚Äôattivit√† di classificazione specifica, come il riconoscimento di specie animali specifiche (attivit√† di destinazione).\n\n\nEstrazione delle Feature\nL‚Äôestrazione delle ‚Äúfeature‚Äù [caratteristiche] comporta l‚Äôutilizzo di un modello pre-addestrato come estrattore di feature fisse, in cui l‚Äôoutput dei layer intermedi del modello viene utilizzato come feature per l‚Äôattivit√† di destinazione. Questo approccio √® particolarmente utile quando l‚Äôattivit√† di destinazione ha un set di dati di piccole dimensioni, poich√© le feature apprese dal modello pre-addestrato possono migliorare significativamente le prestazioni. Nell‚Äôanalisi delle immagini mediche, un modello pre-addestrato su un ampio set di dati di immagini mediche generali (attivit√† di origine) pu√≤ essere utilizzato come estrattore di feature per fornire funzionalit√† preziose per un nuovo modello progettato per riconoscere specifici tipi di tumori nelle immagini radiografiche (attivit√† di destinazione).\n\n\n\n12.4.5 Tipi di Apprendimento Tramite Trasferimento\nL‚Äôapprendimento tramite trasferimento pu√≤ essere classificato in tre tipi principali in base alla natura delle attivit√† e dei dati di origine e di destinazione. Esploriamo ciascun tipo in dettaglio:\n\nApprendimento Tramite Trasferimento Induttivo\nNell‚Äôapprendimento tramite trasferimento induttivo, l‚Äôobiettivo √® apprendere la funzione predittiva di destinazione con l‚Äôaiuto dei dati di origine. In genere comporta la messa a punto di un modello pre-addestrato sull‚Äôattivit√† di destinazione con dati etichettati disponibili. Un esempio comune di apprendimento tramite trasferimento induttivo sono le attivit√† di classificazione delle immagini. Ad esempio, un modello pre-addestrato sul set di dati ImageNet (attivit√† di origine) pu√≤ essere messo a punto per classificare tipi specifici di uccelli (attivit√† di destinazione) utilizzando un set di dati etichettato pi√π piccolo di immagini di uccelli.\n\n\nApprendimento Tramite Trasferimento Transduttivo\nL‚Äôapprendimento tramite trasferimento transduttivo comporta l‚Äôutilizzo di dati di origine e destinazione, ma solo dell‚Äôattivit√† di origine. L‚Äôobiettivo principale √® trasferire la conoscenza dal dominio di origine al dominio di destinazione, anche se le attivit√† rimangono le stesse. L‚Äôanalisi del ‚Äúsentiment‚Äù per diverse lingue pu√≤ servire come esempio di apprendimento tramite trasferimento transduttivo. Un modello addestrato per eseguire l‚Äôanalisi del sentiment in inglese (attivit√† di origine) pu√≤ essere adattato per eseguire l‚Äôanalisi del sentiment in un‚Äôaltra lingua, come il francese (attivit√† di destinazione), sfruttando set di dati paralleli di frasi in inglese e francese con gli stessi sentimenti.\n\n\nApprendimento con Trasferimento Non Supervisionato\nL‚Äôapprendimento con trasferimento non supervisionato viene utilizzato quando le attivit√† di origine e di destinazione sono correlate, ma non sono disponibili dati etichettati per l‚Äôattivit√† di destinazione. L‚Äôobiettivo √® sfruttare la conoscenza acquisita dall‚Äôattivit√† di origine per migliorare le prestazioni nell‚Äôattivit√† di destinazione, anche senza dati etichettati. Un esempio di apprendimento di trasferimento non supervisionato √® la modellazione degli argomenti nei dati di testo. Un modello addestrato per estrarre argomenti da articoli di notizie (attivit√† di origine) pu√≤ essere adattato per estrarre argomenti da post sui social media (attivit√† di destinazione) senza aver bisogno di dati etichettati per i post sui social media.\n\n\nConfronto e Compromessi\nSfruttando questi diversi tipi di apprendimento per trasferimento, i professionisti possono scegliere l‚Äôapproccio che meglio si adatta alla natura dei loro compiti e ai dati disponibili, portando infine a modelli di ML pi√π efficaci ed efficienti. Quindi, in sintesi:\n\nInduttivo: Diversi compiti di origine e destinazione, domini diversi\nTrasduttivo: diversi compiti di origine e destinazione, stesso dominio\nNon supervisionato: dati di origine non etichettati, trasferisce le rappresentazioni delle feature\n\nTabella¬†12.1 presenta una matrice che delinea in modo un po‚Äô pi√π dettagliato le somiglianze e le differenze tra i tipi di apprendimento per trasferimento:\n\n\n\nTabella¬†12.1: Confronto dei tipi di apprendimento per trasferimento.\n\n\n\n\n\n\n\n\n\n\n\nAspetto\nApprendimento Induttivo per Trasferimento\nApprendimento Trasduttivo per Trasferimento\nApprendimento Non Supervisionato\n\n\n\n\nDati Etichettati per l‚ÄôAttivit√† di Destinazione\nObbligatorio\nNon obbligatorio\nNon obbligatorio\n\n\nAttivit√† di origine\nPu√≤ essere diversa\nLo stesso\nLo stesso o diverso\n\n\nAttivit√† di destinazione\nPu√≤ essere diversa\nLo stesso\nPu√≤ essere diverso\n\n\nObiettivo\nMigliorare le prestazioni dell‚Äôattivit√† target con i dati sorgente\nTrasferisci la conoscenza dal dominio sorgente a quello target\nSfrutta l‚Äôattivit√† sorgente per migliorare le prestazioni dell‚Äôattivit√† target senza dati etichettati\n\n\nEsempio\nDa ImageNet alla classificazione degli uccelli\nAnalisi del sentiment in diverse lingue\nModellazione degli argomenti per diversi dati di testo\n\n\n\n\n\n\n\n\n\n12.4.6 Vincoli e Considerazioni\nQuando si intraprende un apprendimento per trasferimento, ci sono diversi fattori che devono essere considerati per garantire un trasferimento di conoscenze di successo e prestazioni del modello. Ecco una ripartizione di alcuni fattori chiave:\n\nSomiglianza dei Domini\nLa similarit√† di dominio si riferisce al grado di somiglianza tra i tipi di dati utilizzati nelle applicazioni di origine e di destinazione. Pi√π simili sono i domini, pi√π √® probabile che l‚Äôapprendimento per trasferimento abbia successo. Ad esempio, trasferire la conoscenza da un modello addestrato su immagini esterne (dominio di origine) a una nuova applicazione che coinvolge immagini interne (dominio di destinazione) √® pi√π fattibile che trasferire la conoscenza da immagini esterne a un‚Äôapplicazione basata su testo. Poich√© immagini e testo sono fondamentalmente tipi di dati diversi, i domini sono dissimili, rendendo l‚Äôapprendimento per trasferimento pi√π impegnativo.\n\n\nSimilarit√† dell‚ÄôAttivit√†\nLa similarit√† dell‚Äôattivit√†, d‚Äôaltra parte, si riferisce a quanto sono simili gli obiettivi o le funzioni delle attivit√† di origine e di destinazione. Se le attivit√† sono simili, √® pi√π probabile che l‚Äôapprendimento per trasferimento sia efficace. Ad esempio, un modello addestrato per classificare diverse razze di cani (attivit√† di origine) pu√≤ essere adattato pi√π facilmente per classificare diverse razze di gatti (attivit√† di destinazione) rispetto a quanto potrebbe essere adattato a un‚Äôattivit√† meno correlata, come l‚Äôidentificazione di immagini satellitari. Poich√© entrambi i compiti comportano la classificazione visiva degli animali, la somiglianza dei compiti favorisce un trasferimento efficace, mentre il passaggio a un compito non correlato potrebbe rendere l‚Äôapprendimento tramite trasferimento meno efficace.\n\n\nQualit√† e Quantit√† dei Dati\nLa qualit√† e la quantit√† dei dati disponibili per il compito di destinazione possono avere un impatto significativo sul successo dell‚Äôapprendimento per trasferimento. Pi√π dati di alta qualit√† possono comportare migliori prestazioni del modello. Supponiamo di avere un ampio set di dati con immagini chiare e ben etichettate per riconoscere specie di uccelli specifiche. In tal caso, il processo di apprendimento per trasferimento avr√† probabilmente pi√π successo rispetto a un set di dati piccolo e rumoroso.\n\n\nSovrapposizione dello Spazio delle Feature\nLa sovrapposizione dello spazio delle feature si riferisce a quanto bene le feature apprese dal modello sorgente si allineano con quelle necessarie per l‚Äôattivit√† di destinazione. Una maggiore sovrapposizione pu√≤ portare a un apprendimento per trasferimento pi√π efficace. Un modello addestrato su immagini ad alta risoluzione (attivit√† di origine) potrebbe non trasferirsi bene a un‚Äôattivit√† di destinazione che coinvolge immagini a bassa risoluzione, poich√© lo spazio delle feature (alta risoluzione rispetto a bassa risoluzione) √® diverso.\n\n\nComplessit√† del Modello\nAnche la complessit√† del modello sorgente pu√≤ influire sul successo dell‚Äôapprendimento per trasferimento. A volte, un modello pi√π semplice potrebbe trasferirsi meglio di uno complesso, poich√© √® meno probabile che si adatti eccessivamente all‚Äôattivit√† di origine. Ad esempio, un semplice modello CNN addestrato su dati di immagini (attivit√† di origine) pu√≤ essere trasferito con maggiore successo a una nuova attivit√† di classificazione di immagini (attivit√† di destinazione) rispetto a una CNN complessa con molti layer, poich√© √® meno probabile che il modello pi√π semplice si adatti eccessivamente all‚Äôattivit√† di origine.\nConsiderando questi fattori, i professionisti del ML possono prendere decisioni informate su quando e come utilizzare l‚Äôapprendimento per trasferimento, portando infine a prestazioni del modello pi√π efficaci nell‚Äôattivit√† di destinazione. Il successo dell‚Äôapprendimento per trasferimento dipende dal grado di similarit√† tra i domini di origine e di destinazione. L‚Äôoverfitting [adattamento eccessivo] √® rischioso, soprattutto quando la messa a punto avviene su un set di dati limitato. Sul fronte computazionale, alcuni modelli pre-addestrati, a causa delle loro dimensioni, potrebbero non adattarsi comodamente ai vincoli di memoria di alcuni dispositivi o potrebbero essere eseguiti in modo proibitivamente lento. Nel tempo, con l‚Äôevoluzione dei dati, c‚Äô√® il potenziale per la ‚Äúdrift‚Äù [deriva] del modello, che indica la necessit√† di un riaddestramento periodico o di un adattamento continuo.\nScoprire di pi√π sull‚Äôapprendimento per trasferimento in Video¬†12.1 di seguito.\n\n\n\n\n\n\nVideo¬†12.1: Il Transfer Learning",
    "crumbs": [
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Apprendimento On-Device</span>"
    ]
  },
  {
    "objectID": "contents/core/ondevice_learning/ondevice_learning.it.html#sec-fl",
    "href": "contents/core/ondevice_learning/ondevice_learning.it.html#sec-fl",
    "title": "12¬† Apprendimento On-Device",
    "section": "12.5 Apprendimento Automatico Federato",
    "text": "12.5 Apprendimento Automatico Federato\n\n12.5.1 Panoramica dell‚ÄôApprendimento Federato\nL‚ÄôInternet moderna √® piena di grandi reti di dispositivi connessi. Che si tratti di telefoni cellulari, termostati, smart speaker o altri prodotti IoT, innumerevoli dispositivi edge sono una miniera d‚Äôoro per dati iperpersonalizzati e ricchi. Tuttavia, con quei dati ricchi arriva una serie di problemi con il trasferimento delle informazioni e la privacy. Costruire un set di dati di training nel cloud da questi dispositivi comporterebbe un‚Äôampia larghezza di banda, costi per il trasferimento dati e violazione della privacy degli utenti.\nL‚Äôapprendimento federato offre una soluzione a questi problemi: addestrare i modelli parzialmente sui dispositivi edge e comunicare solo gli aggiornamenti al cloud. Nel 2016, un team di Google ha progettato un‚Äôarchitettura per l‚Äôapprendimento federato che tenta di risolvere questi problemi. Nel loro articolo iniziale, McMahan et al. (2017) delinea un algoritmo di apprendimento federato di principio chiamato FederatedAveraging, mostrato in Figura¬†12.5. In particolare, FederatedAveraging esegue la ‚Äústochastic gradient descent (SGD) [discesa del gradiente stocastico] su diversi dispositivi edge. In questo processo, ogni dispositivo calcola un gradiente \\(g_k = \\nabla F_k(w_t)\\) che viene poi applicato per aggiornare i pesi lato server come (con \\(\\eta\\) come tasso di apprendimento su \\(k\\) client):\n\nMcMahan, Brendan, Eider Moore, Daniel Ramage, Seth Hampson, e Blaise Ag√ºera y Arcas. 2017. ¬´Communication-Efficient Learning of Deep Networks from Decentralized Data.¬ª In Artificial intelligence and statistics, 1273‚Äì82. PMLR. http://proceedings.mlr.press/v54/mcmahan17a.html.\n\\[\nw_{t+1} \\rightarrow w_t - \\eta \\sum_{k=1}^{K} \\frac{n_k}{n}g_k\n\\] Questo riassume l‚Äôalgoritmo di base per l‚Äôapprendimento federato sulla destra. Per ogni round di addestramento, il server prende un set casuale di dispositivi client e chiama ogni client per addestrare sul suo batch locale usando i pesi lato server pi√π recenti. Tali pesi vengono poi restituiti al server, dove vengono raccolti individualmente e calcolati per aggiornare i pesi del modello globale.\n\n\n\n\n\n\nFigura¬†12.5: Algoritmo FederatedAverage proposto da Google. Fonte: McMahan et al.¬†(2017).\n\n\n\nCon questa struttura proposta, ci sono alcuni vettori chiave per ottimizzare ulteriormente l‚Äôapprendimento federato. Descriveremo ciascuno di essi nelle seguenti sottosezioni.\nVideo¬†12.2 fornisce una panoramica dell‚Äôapprendimento federato.\n\n\n\n\n\n\nVideo¬†12.2: Il Transfer Learning\n\n\n\n\n\n\nFigura¬†12.6 delinea l‚Äôimpatto trasformativo dell‚Äôapprendimento federato sull‚Äôapprendimento on-device.\n\n\n\n\n\n\nFigura¬†12.6: L‚Äôapprendimento federato sta rivoluzionando l‚Äôapprendimento on-device.\n\n\n\n\n\n12.5.2 Efficienza della Comunicazione\nUno dei principali colli di bottiglia nell‚Äôapprendimento federato √® la comunicazione. Ogni volta che un client addestra il modello, deve comunicare i propri aggiornamenti al server. Analogamente, una volta che il server ha calcolato la media di tutti gli aggiornamenti, deve inviarli al client. Ci√≤ comporta enormi costi di larghezza di banda e risorse su grandi reti di milioni di dispositivi. Con l‚Äôavanzare del campo dell‚Äôapprendimento federato, sono state sviluppate alcune ottimizzazioni per ridurre al minimo questa comunicazione. Per affrontare l‚Äôingombro del modello, i ricercatori hanno sviluppato tecniche di compressione del modello. Nel protocollo client-server, l‚Äôapprendimento federato pu√≤ anche ridurre al minimo la comunicazione tramite la condivisione selettiva degli aggiornamenti sui client. Infine, anche tecniche di aggregazione efficienti possono semplificare il processo di comunicazione.\n\n\n12.5.3 Compressione del Modello\nNell‚Äôapprendimento federato standard, il server comunica l‚Äôintero modello a ciascun client, quindi il client invia tutti i pesi aggiornati. Ci√≤ significa che il modo pi√π semplice per ridurre l‚Äôingombro di memoria e comunicazione del client √® ridurre al minimo le dimensioni del modello che deve essere comunicato. Possiamo impiegare tutte le strategie di ottimizzazione del modello discusse in precedenza per farlo.\nNel 2022, un altro team di Google ha proposto che ogni client comunichi tramite un formato compresso e decomprima il modello al volo per l‚Äôaddestramento (Yang et al. 2023), allocando e deallocando l‚Äôintera memoria per il modello solo per un breve periodo durante l‚Äôaddestramento. Il modello viene compresso tramite una gamma di diverse strategie di quantizzazione elaborate nel loro documento. Nel frattempo, il server pu√≤ aggiornare il modello non compresso decomprimendolo e applicando gli aggiornamenti man mano che arrivano.\n\nYang, Tien-Ju, Yonghui Xiao, Giovanni Motta, Fran√ßoise Beaufays, Rajiv Mathews, e Mingqing Chen. 2023. ¬´Online Model Compression for Federated Learning with Large Models¬ª. In ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 1‚Äì5. IEEE; IEEE. https://doi.org/10.1109/icassp49357.2023.10097124.\n\n\n12.5.4 Condivisione Selettiva degli Aggiornamenti\nEsistono molti metodi per condividere selettivamente gli aggiornamenti. Il principio generale √® che la riduzione della porzione del modello che i client stanno addestrando lato edge riduce la memoria necessaria per l‚Äôaddestramento e la dimensione della comunicazione con il server. Nell‚Äôapprendimento federato di base, il client addestra l‚Äôintero modello. Ci√≤ significa che quando un client invia un aggiornamento al server, ha gradienti per ogni peso nella rete.\nTuttavia, non possiamo semplicemente ridurre la comunicazione inviando parti di quei gradienti da ogni client al server perch√© i gradienti fanno parte di un intero aggiornamento necessario per migliorare il modello. Invece, si deve progettare architettonicamente il modello in modo che ogni client addestri solo una piccola parte del modello pi√π ampio, riducendo la comunicazione totale e ottenendo comunque il vantaggio dell‚Äôaddestramento sui dati del client. Shi e Radu (2022) applica questo concetto a una CNN suddividendo il modello globale in due parti: una superiore e una inferiore, come mostrato in Z. Chen e Xu (2023).\n\nShi, Hongrui, e Valentin Radu. 2022. ¬´Data selection for efficient model update in federated learning¬ª. In Proceedings of the 2nd European Workshop on Machine Learning and Systems, 72‚Äì78. ACM. https://doi.org/10.1145/3517207.3526980.\n\n\n\n\n\n\nFigura¬†12.7: Apprendimento federato con addestramento del modello diviso. Il modello √® diviso in una parte inferiore, addestrata localmente su ogni client, e una parte superiore, perfezionata sul server. I client eseguono aggiornamenti locali, generando mappe di attivazione dai loro dati, che vengono inviati al server anzich√© dati grezzi per garantire la privacy. Il server utilizza queste mappe di attivazione per aggiornare la parte superiore, poi combina entrambe le parti e ridistribuisce il modello aggiornato ai client. Questa configurazione riduce al minimo la comunicazione, preserva la privacy e adatta il modello ai diversi dati dei client. Fonte: Shi et al., (2022).\n\n\n\nLa parte inferiore del modello, responsabile dell‚Äôestrazione di feature generiche, viene addestrata direttamente su ciascun dispositivo client. Utilizzando la media federata, questa parte inferiore apprende funzionalit√† fondamentali condivise su tutti i client, consentendole di generalizzare bene su dati diversi. Nel frattempo, la parte superiore del modello, che cattura modelli pi√π specifici e complessi, viene addestrata sul server. Invece di inviare dati grezzi al server, ogni client genera mappe di attivazione, una rappresentazione compressa delle feature pi√π rilevanti dei suoi dati locali, e le invia al server. Il server utilizza queste mappe di attivazione per perfezionare la parte superiore del modello, consentendogli di diventare pi√π sensibile alle diverse distribuzioni di dati riscontrate nei client senza compromettere la privacy dell‚Äôutente.\nQuesto approccio riduce significativamente il carico di comunicazione, poich√© vengono trasmesse solo mappe di attivazione riepilogative anzich√© set di dati completi. Concentrandosi sull‚Äôaddestramento condiviso per la parte inferiore e sulla messa a punto specializzata per la parte superiore, il sistema raggiunge un equilibrio: riduce al minimo il trasferimento di dati, preserva la privacy e rende il modello robusto ai vari tipi di input riscontrati sui dispositivi client.\n\n\n12.5.5 Aggregazione Ottimizzata\nOltre a ridurre il sovraccarico di comunicazione, l‚Äôottimizzazione della funzione di aggregazione pu√≤ migliorare la velocit√† e l‚Äôaccuratezza dell‚Äôaddestramento del modello in determinati casi d‚Äôuso di apprendimento federato. Mentre lo standard per l‚Äôaggregazione √® solo la media, vari altri approcci possono migliorare l‚Äôefficienza, l‚Äôaccuratezza e la sicurezza del modello.\nUn‚Äôalternativa √® la ‚Äúmedia ritagliata‚Äù, che limita gli aggiornamenti del modello entro un intervallo specifico. Un‚Äôaltra strategia per preservare la sicurezza √® l‚Äôaggregazione media della privacy differenziale. Questo approccio integra la privacy differenziale nella fase di aggregazione per proteggere le identit√† dei client. Ogni client aggiunge uno strato di rumore casuale ai propri aggiornamenti prima di comunicare al server. Il server si aggiorna quindi con gli aggiornamenti rumorosi, il che significa che la quantit√† di rumore deve essere regolata attentamente per bilanciare privacy e precisione.\nOltre ai metodi di aggregazione che migliorano la sicurezza, ci sono diverse modifiche ai metodi di aggregazione che possono migliorare la velocit√† di training e le prestazioni aggiungendo metadati client insieme agli aggiornamenti del peso. Il ‚ÄúMomentum aggregation‚Äù √® una tecnica che aiuta ad affrontare il problema della convergenza. Nell‚Äôapprendimento federato, i dati client possono essere estremamente eterogenei a seconda dei diversi ambienti in cui vengono utilizzati i dispositivi. Ci√≤ significa che molti modelli con dati eterogenei potrebbero aver bisogno di aiuto per convergere. Ogni client memorizza localmente un termine di ‚Äúmomentum‚Äù, che traccia il ritmo del cambiamento su diversi aggiornamenti. Con i client che comunicano questo ‚Äúmomentum‚Äù, il server pu√≤ tenere conto della velocit√† di cambiamento di ogni aggiornamento quando si modifica il modello globale per accelerare la convergenza. Allo stesso modo, l‚Äôaggregazione ponderata pu√≤ tenere conto delle prestazioni del client o di altri parametri come il tipo di dispositivo o la potenza della connessione di rete per regolare il peso con cui il server dovrebbe incorporare gli aggiornamenti del modello. Ulteriori descrizioni di algoritmi di aggregazione specifici sono fornite da Moshawrab et al. (2023).\n\nMoshawrab, Mohammad, Mehdi Adda, Abdenour Bouzouane, Hussein Ibrahim, e Ali Raad. 2023. ¬´Reviewing Federated Learning Aggregation Algorithms; Strategies, Contributions, Limitations and Future Perspectives¬ª. Electronics 12 (10): 2287. https://doi.org/10.3390/electronics12102287.\n\n\n12.5.6 Gestione dei Dati non-IID\nQuando si utilizza l‚Äôapprendimento federato per addestrare un modello su molti dispositivi client, √® conveniente considerare i dati come indipendenti e distribuiti in modo identico (IID) su tutti i client. Quando i dati sono IID, il modello converger√† pi√π velocemente e funzioner√† meglio perch√© ogni aggiornamento locale su un dato client √® pi√π rappresentativo del set di dati pi√π ampio. Questo semplifica l‚Äôaggregazione, poich√© √® possibile calcolare direttamente la media di tutti i client. Tuttavia, differisce dal modo in cui i dati spesso appaiono nel mondo reale. Si considerino alcuni dei seguenti modi in cui i dati possono essere non IID:\n\nImparando su un set di dispositivi di monitoraggio sanitari, diversi modelli di dispositivi potrebbero significare diverse qualit√† e propriet√† dei sensori. Ci√≤ significa che sensori e dispositivi di bassa qualit√† possono produrre dati e, pertanto, aggiornamenti del modello nettamente diversi da quelli di alta qualit√†\nUna tastiera intelligente addestrata per eseguire la correzione automatica. Se si ha una quantit√† sproporzionata di dispositivi da una determinata regione, lo slang, la struttura della frase o persino il linguaggio che stavano usando potrebbero deviare pi√π aggiornamenti verso un certo stile di digitazione\nSe si hanno sensori per la fauna selvatica in aree remote, la connettivit√† potrebbe non essere distribuita equamente, facendo s√¨ che alcuni client in determinate regioni non siano in grado di inviare pi√π aggiornamenti rispetto ad altri. Se quelle regioni hanno un‚Äôattivit√† di fauna selvatica diversa da alcune specie, ci√≤ potrebbe distorcere gli aggiornamenti verso quegli animali\n\nEsistono alcuni approcci per affrontare i dati non IID nell‚Äôapprendimento federato. Uno potrebbe essere quello di modificare l‚Äôalgoritmo di aggregazione. Se si utilizza un algoritmo di aggregazione ponderato, √® possibile regolarlo in base a diverse propriet√† del client come regione, propriet√† del sensore o connettivit√† (Zhao et al. 2018).\n\nZhao, Yue, Meng Li, Liangzhen Lai, Naveen Suda, Damon Civin, e Vikas Chandra. 2018. ¬´Federated Learning with Non-IID Data¬ª. ArXiv preprint abs/1806.00582 (giugno). http://arxiv.org/abs/1806.00582v2.\n\n\n12.5.7 Selezione del Client\nConsiderando tutti i fattori che influenzano l‚Äôefficacia dell‚Äôapprendimento federato, come i dati IID e la comunicazione, la selezione del client √® una componente chiave per garantire che un sistema si alleni bene. La selezione dei client sbagliati pu√≤ distorcere il dataset, con conseguenti dati non IID. Analogamente, la scelta casuale di client con cattive connessioni di rete pu√≤ rallentare la comunicazione. Pertanto, √® necessario considerare diverse caratteristiche chiave quando si seleziona il sottoinsieme corretto di client.\nQuando si selezionano i client, ci sono tre componenti principali da considerare: eterogeneit√† dei dati, allocazione delle risorse e costo della comunicazione. Possiamo selezionare i client in base alle metriche proposte in precedenza nella sezione non IID per affrontare l‚Äôeterogeneit√† dei dati. Nell‚Äôapprendimento federato, tutti i dispositivi possono avere diverse quantit√† di elaborazione, con il risultato che alcuni sono pi√π inefficienti di altri nell‚Äôaddestramento. Quando si seleziona un sottoinsieme di client per l‚Äôaddestramento, si deve considerare un equilibrio tra eterogeneit√† dei dati e risorse disponibili. In uno scenario ideale, √® sempre possibile selezionare il sottoinsieme di client con le maggiori risorse. Tuttavia, questo potrebbe distorcere il set di dati, quindi √® necessario trovare un equilibrio. Le differenze di comunicazione aggiungono un altro layer; si desidera evitare di essere bloccati dall‚Äôattesa che i dispositivi con connessioni scadenti trasmettano tutti i loro aggiornamenti. Pertanto, √® anche necessario considerare la scelta di un sottoinsieme di dispositivi diversi ma ben collegati.\n\n\n12.5.8 L‚ÄôEsempio di Gboard\nUn esempio primario di un sistema di apprendimento federato distribuito √® la tastiera di Google, Gboard, per dispositivi Android. Nell‚Äôimplementare l‚Äôapprendimento federato per la tastiera, Google si √® concentrata sull‚Äôimpiego di tecniche di privacy differenziali per proteggere i dati e l‚Äôidentit√† dell‚Äôutente. Gboard sfrutta modelli linguistici per diverse funzionalit√† chiave, come Next Word Prediction (NWP), Smart Compose (SC) e On-The-Fly rescoring (OTF) (Xu et al. 2023), come mostrato in Figura¬†12.8.\n\nXu, Zheng, Yanxiang Zhang, Galen Andrew, Christopher A. Choquette-Choo, Peter Kairouz, H. Brendan McMahan, Jesse Rosenstock, e Yuanbo Zhang. 2023. ¬´Federated Learning of Gboard Language Models with Differential Privacy¬ª. ArXiv preprint abs/2305.18465 (maggio). http://arxiv.org/abs/2305.18465v2.\nNWP anticiper√† la parola successiva che l‚Äôutente tenta di digitare in base a quella precedente. SC fornisce suggerimenti in linea per velocizzare la digitazione in base a ciascun carattere. OTF riclassificher√† le parole successive proposte in base al processo di digitazione attivo. Tutti e tre questi modelli devono essere eseguiti rapidamente sull‚Äôedge e l‚Äôapprendimento federato pu√≤ accelerare l‚Äôaddestramento sui dati degli utenti. Tuttavia, caricare ogni parola digitata da un utente sul cloud per l‚Äôaddestramento costituirebbe una violazione massiccia della privacy. Pertanto, l‚Äôapprendimento federato enfatizza la privacy differenziale, che protegge l‚Äôutente consentendo al contempo una migliore esperienza utente.\n\n\n\n\n\n\nFigura¬†12.8: Funzionalit√† di Google G Board. Fonte: Zheng et al., (2023).\n\n\n\nPer raggiungere questo obiettivo, Google ha impiegato il suo algoritmo DP-FTRL, che fornisce una garanzia formale che i modelli addestrati non memorizzeranno dati o identit√† utente specifici. La progettazione del sistema dell‚Äôalgoritmo √® mostrata in Figura¬†12.9. DP-FTRL, combinato con l‚Äôaggregazione sicura, crittografa gli aggiornamenti del modello e fornisce un equilibrio ottimale tra privacy e utilit√†. Inoltre, il clipping adattivo viene applicato nel processo di aggregazione per limitare l‚Äôimpatto dei singoli utenti sul modello globale (passaggio 3 in Figura¬†12.9). Combinando tutte queste tecniche, Google pu√≤ perfezionare continuamente la sua tastiera preservando al contempo la privacy dell‚Äôutente in un modo formalmente dimostrabile.\n\n\n\n\n\n\nFigura¬†12.9: Privacy Differenziale in G Board. Fonte: Zheng et al., (2023).\n\n\n\n\n\n\n\n\n\nEsercizio¬†12.2: Apprendimento Federato - Generazione di Testo\n\n\n\n\n\nAvete mai usato quelle tastiere intelligenti che suggeriscono la parola successiva? Con l‚Äôapprendimento federato, possiamo renderle ancora migliori senza sacrificare la privacy. In questo Colab, insegneremo a un‚ÄôIA a prevedere le parole tramite l‚Äôaddestramento su dati di testo distribuiti su pi√π dispositivi. Prepariamoci a rendere la digitazione ancora pi√π fluida!\n\n\n\n\n\n\n\n\n\n\nEsercizio¬†12.3: Apprendimento Federato - Classificazione delle Immagini\n\n\n\n\n\nVogliamo addestrare un‚ÄôIA esperta di immagini senza inviare le proprie foto al cloud? L‚Äôapprendimento federato √® la risposta! In questo Colab, addestreremo un modello su pi√π dispositivi, ognuno dei quali apprende dalle proprie immagini. La privacy √® protetta e il lavoro di squadra fa funzionare il sogno dell‚ÄôIA!\n\n\n\n\n\n\n12.5.9 Benchmarking Federated Learning: MedPerf\nI dispositivi medici rappresentano uno degli esempi pi√π ricchi di dati edge. Questi dispositivi memorizzano alcuni dei dati utente pi√π personali, offrendo allo stesso tempo progressi significativi nel trattamento personalizzato e una maggiore accuratezza nell‚ÄôIA medica. Questa combinazione di dati sensibili e potenziale di innovazione rende i dispositivi medici un caso d‚Äôuso ideale per l‚Äôapprendimento federato.\nUno sviluppo chiave in questo campo √® MedPerf, una piattaforma open source progettata per il benchmarking dei modelli utilizzando la valutazione federata (Karargyris et al. 2023). MedPerf va oltre il tradizionale apprendimento federato portando il modello sui dispositivi periferici per testare i dati personalizzati mantenendo la privacy. Questo approccio consente a un comitato di benchmark di valutare vari modelli in scenari reali su dispositivi edge senza compromettere l‚Äôanonimato del paziente.\n\nKarargyris, Alexandros, Renato Umeton, Micah J. Sheller, Alejandro Aristizabal, Johnu George, Anna Wuest, Sarthak Pati, et al. 2023. ¬´Federated benchmarking of medical artificial intelligence with MedPerf¬ª. Nature Machine Intelligence 5 (7): 799‚Äì810. https://doi.org/10.1038/s42256-023-00652-2.\nLa piattaforma MedPerf, descritta in dettaglio in uno studio recente (https://doi.org/10.1038/s42256-023-00652-2), dimostra come le tecniche federate possano essere applicate non solo all‚Äôaddestramento dei modelli, ma anche alla valutazione e al benchmarking dei modelli. Questo progresso √® particolarmente cruciale nel campo medico, dove l‚Äôequilibrio tra lo sfruttamento di grandi set di dati per migliorare le prestazioni dell‚ÄôIA e la protezione della privacy individuale √® di fondamentale importanza.",
    "crumbs": [
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Apprendimento On-Device</span>"
    ]
  },
  {
    "objectID": "contents/core/ondevice_learning/ondevice_learning.it.html#problemi-di-sicurezza",
    "href": "contents/core/ondevice_learning/ondevice_learning.it.html#problemi-di-sicurezza",
    "title": "12¬† Apprendimento On-Device",
    "section": "12.6 Problemi di Sicurezza",
    "text": "12.6 Problemi di Sicurezza\nL‚Äôesecuzione di training e adattamento del modello ML sui dispositivi degli utenti finali introduce anche rischi per la sicurezza che devono essere affrontati. Alcune preoccupazioni chiave per la sicurezza includono:\n\nEsposizione di dati privati: I dati di training potrebbero essere trapelati o rubati dai dispositivi\nAvvelenamento dei dati: Gli avversari possono manipolare i dati di training per degradare le prestazioni del modello\nEstrazione del modello: Degli aggressori potrebbero tentare di rubare i parametri del modello addestrato\nInferenza di appartenenza: I modelli potrebbero rivelare la partecipazione di dati di utenti specifici\nAttacchi di evasione: Input appositamente creati possono causare una classificazione errata\n\nQualsiasi sistema che esegue l‚Äôapprendimento sul dispositivo introduce preoccupazioni per la sicurezza, poich√© potrebbe esporre vulnerabilit√† in modelli su larga scala. Numerosi rischi per la sicurezza sono associati a qualsiasi modello ML, ma questi rischi hanno conseguenze specifiche per l‚Äôapprendimento on-device. Fortunatamente, esistono metodi per mitigare questi rischi e migliorare le prestazioni reali dell‚Äôapprendimento su dispositivo.\n\n12.6.1 Avvelenamento dei Dati\nL‚Äôapprendimento automatico on-device introduce sfide uniche per la sicurezza dei dati rispetto all‚Äôaddestramento tradizionale basato su cloud. In particolare, gli attacchi di avvelenamento dei dati rappresentano una seria minaccia durante l‚Äôapprendimento su dispositivo. Gli avversari possono manipolare i dati di training per degradare le prestazioni del modello quando vengono distribuiti.\nEsistono diverse tecniche di attacco di avvelenamento dei dati:\n\nLabel Flipping: Comporta l‚Äôapplicazione di etichette errate ai campioni. Ad esempio, nella classificazione delle immagini, le foto di gatti possono essere etichettate come cani per confondere il modello. Anche capovolgere il 10% delle etichette pu√≤ avere conseguenze significative sul modello.\nInserimento dei Dati: Introduce input falsi o distorti nel set di training. Ci√≤ potrebbe includere immagini pixelate, audio rumoroso o testo distorto.\nCorruzione logica: Altera i pattern sottostanti nei dati per confondere il modello. Nell‚Äôanalisi del ‚Äúsentiment‚Äù, le recensioni altamente negative possono essere contrassegnate come positive tramite questa tecnica. Per questo motivo, recenti sondaggi hanno dimostrato che molte aziende hanno pi√π paura dell‚Äôavvelenamento dei dati rispetto ad altre preoccupazioni di ML avversarie.\n\nCi√≤ che rende l‚Äôavvelenamento dei dati allarmante √® il modo in cui sfrutta la discrepanza tra set di dati curati e dati di training in tempo reale. Consideriamo un set di dati di foto di gatti raccolti da Internet. Nelle settimane successive, quando questi dati addestrano un modello on-device, le nuove foto di gatti sul Web differiscono in modo significativo.\nCon l‚Äôavvelenamento dei dati, gli aggressori acquistano domini e caricano contenuti che influenzano una parte dei dati di training. Anche piccole modifiche ai dati hanno un impatto significativo sul comportamento appreso dal modello. Di conseguenza, l‚Äôavvelenamento pu√≤ instillare pregiudizi razzisti, sessisti o altri pregiudizi dannosi se non controllato.\nMicrosoft Tay √® un chatbot lanciato da Microsoft nel 2016. √à stato progettato per imparare dalle sue interazioni con gli utenti su piattaforme di social media come Twitter. Sfortunatamente, Microsoft Tay √® diventato un esempio lampante di avvelenamento dei dati nei modelli di ML. Entro 24 ore dal suo lancio, Microsoft ha dovuto mettere Tay offline perch√© aveva iniziato a produrre messaggi offensivi e inappropriati, tra cui incitamento all‚Äôodio e commenti razzisti. Ci√≤ √® accaduto perch√© alcuni utenti sui social media hanno intenzionalmente fornito a Tay input dannosi e offensivi, da cui il chatbot ha poi imparato e incorporato nelle sue risposte.\nQuesto incidente √® un chiaro esempio di avvelenamento dei dati, poich√© i malintenzionati hanno intenzionalmente manipolato i dati utilizzati per addestrare il chatbot e modellarne le risposte. L‚Äôavvelenamento dei dati ha portato il chatbot ad adottare pregiudizi dannosi e a produrre output che i suoi sviluppatori non avevano previsto. Dimostra come anche piccole quantit√† di dati creati in modo dannoso possano avere un impatto significativo sul comportamento dei modelli ML e sottolinea l‚Äôimportanza di implementare solidi meccanismi di filtraggio e convalida dei dati per impedire che tali incidenti si verifichino.\nTali pregiudizi potrebbero avere pericolosi impatti nel mondo reale. La convalida rigorosa dei dati, il rilevamento delle anomalie e il monitoraggio della provenienza dei dati sono misure difensive fondamentali. L‚Äôadozione di framework come Five Safes garantisce che i modelli siano addestrati su dati rappresentativi di alta qualit√† (Desai et al. 2016).\n\nDesai, Tanvi, Felix Ritchie, Richard Welpton, et al. 2016. ¬´Five Safes: Designing data access for research¬ª. Economics Working Paper Series 1601: 28.\nL‚Äôavvelenamento dei dati √® una preoccupazione urgente per l‚Äôapprendimento sicuro sul dispositivo poich√© i dati dell‚Äôendpoint non possono essere facilmente monitorati in tempo reale. Se ai modelli viene consentito di adattarsi da soli, corriamo il rischio che il dispositivo agisca in modo dannoso. Tuttavia, √® necessaria una ricerca continua sull‚Äôapprendimento automatico avversario per sviluppare soluzioni efficaci per rilevare e mitigare tali attacchi ai dati.\n\n\n12.6.2 Attacchi Avversari\nDurante la fase di addestramento, gli aggressori potrebbero iniettare dati dannosi nel dataset di training, il che pu√≤ alterare sottilmente il comportamento del modello. Ad esempio, un aggressore potrebbe aggiungere immagini di gatti etichettati come cani a un set di dati utilizzato per addestrare un modello di classificazione delle immagini. Se fatto in modo intelligente, l‚Äôaccuratezza del modello potrebbe non diminuire in modo significativo e l‚Äôattacco potrebbe essere notato. Il modello classificherebbe quindi erroneamente alcuni gatti come cani, il che potrebbe avere conseguenze a seconda dell‚Äôapplicazione.\nIn un sistema di telecamere di sicurezza embedded, ad esempio, ci√≤ potrebbe consentire a un intruso di evitare il rilevamento indossando uno specifico pattern che il modello √® stato ingannato a classificare come non minaccioso.\nDurante la fase di inferenza, gli aggressori possono utilizzare esempi avversari per ingannare il modello. Gli esempi avversari sono input che sono stati leggermente alterati in un modo da far s√¨ che il modello faccia previsioni errate. Ad esempio, un aggressore potrebbe aggiungere una piccola quantit√† di rumore a un‚Äôimmagine in un modo che un sistema di riconoscimento facciale identifichi erroneamente una persona. Questi attacchi possono essere particolarmente preoccupanti nelle applicazioni in cui √® in gioco la sicurezza, come i veicoli autonomi. Un esempio concreto di ci√≤ √® quando i ricercatori sono riusciti a far s√¨ che un sistema di riconoscimento della segnaletica stradale classificasse erroneamente un segnale di stop come un segnale di limite di velocit√†. Questo tipo di classificazione errata potrebbe causare incidenti se si verificasse in un sistema di guida autonoma nel mondo reale.\nPer mitigare questi rischi, possono essere impiegate diverse difese:\n\nValidazione e Sanificazione dei Dati: Prima di incorporare nuovi dati nel dataset di addestramento, questi devono essere convalidati e sanificati a fondo per garantire che non siano dannosi.\nAddestramento Avversario: Il modello pu√≤ essere addestrato su esempi avversari per renderlo pi√π robusto a questi tipi di attacchi.\nValidazione degli Input: Durante l‚Äôinferenza, gli input devono essere convalidati per garantire che non siano stati manipolati per creare esempi avversari.\nAudit e Monitoraggio Regolari: L‚Äôaudit e il monitoraggio regolari del comportamento del modello possono aiutare a rilevare e mitigare gli attacchi avversari. Tuttavia, √® pi√π facile a dirsi che a farsi nel contesto di piccoli sistemi ML. Spesso √® difficile monitorare i sistemi ML embedded all‚Äôendpoint a causa delle limitazioni della larghezza di banda della comunicazione, di cui parleremo nel capitolo MLOps.\n\nComprendendo i potenziali rischi e implementando queste difese, possiamo contribuire a proteggere il training on-device all‚Äôendpoint/edge e mitigare l‚Äôimpatto degli attacchi avversari. La maggior parte delle persone confonde facilmente l‚Äôavvelenamento dei dati e gli attacchi avversari. Quindi Tabella¬†12.2 confronta l‚Äôavvelenamento dei dati e gli attacchi avversari:\n\n\n\nTabella¬†12.2: Confronto tra avvelenamento dei dati e attacchi avversari.\n\n\n\n\n\n\n\n\n\n\nAspetto\nAvvelenamento dei dati\nAttacchi avversari\n\n\n\n\nTempistica\nFase di addestramento\nFase di inferenza\n\n\nTarget\nDati di addestramento\nDati di input\n\n\nObiettivo\nInfluenza negativamente le prestazioni del modello\nCausa previsioni errate\n\n\nMetodo\nInserire esempi dannosi nei dati di training, spesso con etichette errate\nAggiungere rumore attentamente elaborato ai dati di input\n\n\nEsempio\nAggiungere immagini di gatti etichettati come cani a un set di dati utilizzato per addestrare un modello di classificazione delle immagini\nAggiungere una piccola quantit√† di rumore a un‚Äôimmagine in modo che un sistema di riconoscimento facciale identifichi erroneamente una persona\n\n\nEffetti Potenziali\nIl modello apprende pattern errati e fa previsioni errate\nPrevisioni errate immediate e potenzialmente pericolose\n\n\nApplicazioni Interessate\nQualsiasi modello ML\nVeicoli autonomi, sistemi di sicurezza, ecc.\n\n\n\n\n\n\n\n\n12.6.3 Inversione del Modello\nGli attacchi di inversione del modello rappresentano una minaccia per la privacy dei modelli di machine learning su dispositivo addestrati su dati utente sensibili (Nguyen et al. 2023). Comprendere questo vettore di attacco e le strategie di mitigazione saranno importanti per creare un‚Äôintelligenza artificiale su dispositivo sicura ed etica. Ad esempio, si immagini un‚Äôapp per iPhone che utilizza l‚Äôapprendimento su dispositivo per categorizzare le foto in gruppi come ‚Äúspiaggia‚Äù, ‚Äúcibo‚Äù o ‚Äúselfie‚Äù per una ricerca pi√π semplice.\n\nNguyen, Ngoc-Bao, Keshigeyan Chandrasegaran, Milad Abdollahzadeh, e Ngai-Man Cheung. 2023. ¬´Re-Thinking Model Inversion Attacks Against Deep Neural Networks¬ª. In 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 16384‚Äì93. IEEE. https://doi.org/10.1109/cvpr52729.2023.01572.\nIl modello su dispositivo potrebbe essere addestrato da Apple su un set di dati di foto iCloud di utenti consenzienti. Un aggressore malintenzionato potrebbe tentare di estrarre parti di quelle foto di addestramento iCloud originali utilizzando l‚Äôinversione del modello. In particolare, l‚Äôaggressore inserisce input sintetici creati ad arte nel classificatore di foto su dispositivo. Modificando gli input sintetici e osservando come il modello li categorizza, possono perfezionare gli input fino a ricostruire copie dei dati di training originali, come una foto di una spiaggia dall‚ÄôiCloud di un utente. Ora, l‚Äôaggressore ha violato la privacy di quell‚Äôutente ottenendo una delle sue foto senza consenso. Questo dimostra perch√© l‚Äôinversione del modello √® pericolosa: pu√≤ potenzialmente far trapelare dati di training altamente sensibili.\nLe foto sono un tipo di dati particolarmente rischioso perch√© spesso contengono persone identificabili, informazioni sulla posizione e momenti privati. Tuttavia, la stessa metodologia di attacco potrebbe essere applicata ad altri dati personali, come registrazioni audio, messaggi di testo o dati sanitari degli utenti.\nPer difendersi dall‚Äôinversione del modello, sarebbe necessario prendere precauzioni come l‚Äôaggiunta di rumore agli output del modello o l‚Äôutilizzo di tecniche di apprendimento automatico che preservano la privacy come l‚Äôapprendimento federato per addestrare il modello sul dispositivo. L‚Äôobiettivo √® impedire agli aggressori di ricostruire i dati di training originali.\n\n\n12.6.4 Problemi di Sicurezza dell‚ÄôApprendimento On-Device\nSebbene l‚Äôavvelenamento dei dati e gli attacchi avversari siano preoccupazioni comuni per i modelli ML in generale, l‚Äôapprendimento su dispositivo introduce rischi di sicurezza unici. Quando vengono pubblicate varianti su dispositivo di modelli su larga scala, gli avversari possono sfruttare questi modelli pi√π piccoli per attaccare le loro controparti pi√π grandi. La ricerca ha dimostrato che man mano che i modelli su dispositivo e i modelli su scala reale diventano pi√π simili, la vulnerabilit√† dei modelli originali su larga scala aumenta in modo significativo. Ad esempio, le valutazioni su 19 reti neurali profonde (DNN) hanno rivelato che lo sfruttamento dei modelli su dispositivo potrebbe aumentare la vulnerabilit√† dei modelli originali su larga scala di fino a 100 volte.\nEsistono tre tipi principali di rischi per la sicurezza specifici dell‚Äôapprendimento on-device:\n\nAttacchi Basati sul Trasferimento: Questi attacchi sfruttano la propriet√† di trasferibilit√† tra un modello surrogato (un‚Äôapprossimazione del modello di destinazione, simile a un modello su dispositivo) e un modello di destinazione remoto (il modello originale su scala reale). Gli aggressori generano esempi avversari utilizzando il modello surrogato, che pu√≤ quindi essere utilizzato per ingannare il modello di destinazione. Ad esempio, si immagini un modello on-device progettato per identificare le e-mail di spam. Un aggressore potrebbe usare questo modello per generare un‚Äôe-mail di spam che non viene rilevata dal sistema di filtraggio pi√π grande e completo.\nAttacchi Basati sull‚ÄôOttimizzazione: Questi attacchi generano esempi avversari per attacchi basati sul trasferimento usando una qualche forma di funzione obiettivo e modificano iterativamente gli input per ottenere il risultato desiderato. Gli attacchi di stima del gradiente, ad esempio, approssimano il gradiente del modello usando output di query (come punteggi di confidenza softmax), mentre gli attacchi senza gradiente usano la decisione finale del modello (la classe prevista) per approssimare il gradiente, sebbene richiedano molte pi√π query.\nAttacchi di Query con Priorit√† di Trasferimento: Questi attacchi combinano elementi di attacchi basati sul trasferimento e basati sull‚Äôottimizzazione. Eseguono il reverse engineering dei modelli sul dispositivo per fungere da surrogati del modello completo di destinazione. In altre parole, gli aggressori usano il modello sul dispositivo pi√π piccolo per capire come funziona il modello pi√π grande e quindi usano questa conoscenza per attaccare il modello completo.\n\nGrazie alla comprensione di questi rischi specifici associati all‚Äôapprendimento on-device, possiamo sviluppare protocolli di sicurezza pi√π solidi per proteggere sia i modelli on-device che quelli su scala reale da potenziali attacchi.\n\n\n12.6.5 Attenuazione dei Rischi dell‚ÄôApprendimento On-Device\nSi possono impiegare vari metodi per mitigare i numerosi rischi per la sicurezza associati all‚Äôapprendimento on-device. Questi metodi possono essere specifici per il tipo di attacco o fungere da strumento generale per rafforzare la sicurezza.\nUna strategia per ridurre i rischi per la sicurezza √® quella di ridurre la somiglianza tra modelli on-device e modelli su scala reale, riducendo cos√¨ la trasferibilit√† fino al 90%. Questo metodo, noto come similarity-unpairing, affronta il problema che si verifica quando gli avversari sfruttano la somiglianza del gradiente di input tra i due modelli. Ottimizzando il modello su scala reale per creare una nuova versione con accuratezza simile ma gradienti di input diversi, possiamo costruire il modello on-device quantizzando questo modello su scala reale aggiornato. Questa disassociazione riduce la vulnerabilit√† dei modelli su dispositivo limitando l‚Äôesposizione del modello su scala reale originale. √à importante notare che l‚Äôordine di ottimizzazione e quantizzazione pu√≤ essere variato pur ottenendo la mitigazione del rischio (Hong, Carlini, e Kurakin 2023).\nPer contrastare l‚Äôavvelenamento dei dati, √® fondamentale reperire set di dati da fornitori affidabili e fidati.\nPer combattere gli attacchi avversari, si possono impiegare diverse strategie. Un approccio proattivo prevede la generazione di esempi avversari e la loro incorporazione nel set di dati di training del modello, rafforzando cos√¨ il modello contro tali attacchi. Strumenti come CleverHans, una libreria di training open source, sono fondamentali per creare esempi avversari. La ‚ÄúDefense distillation‚Äù [distillazione della difesa] √® un‚Äôaltra strategia efficace, in cui il modello sul dispositivo genera probabilit√† di classificazioni diverse anzich√© decisioni definitive (Hong, Carlini, e Kurakin 2023), rendendo pi√π difficile per gli esempi avversari sfruttare il modello.\n\nHong, Sanghyun, Nicholas Carlini, e Alexey Kurakin. 2023. ¬´Publishing Efficient On-device Models Increases Adversarial Vulnerability¬ª. In 2023 IEEE Conference on Secure and Trustworthy Machine Learning (SaTML), abs 1603 5279:271‚Äì90. IEEE; IEEE. https://doi.org/10.1109/satml54575.2023.00026.\nIl furto di propriet√† intellettuale √® un altro problema significativo quando si distribuiscono modelli on-device. Il furto di propriet√† intellettuale √® un problema quando si distribuiscono modelli on-device, poich√© gli avversari potrebbero tentare di sottoporre a reverse engineering il modello per rubare la tecnologia sottostante. Per proteggersi dal furto di propriet√† intellettuale, l‚Äôeseguibile binario del modello addestrato dovrebbe essere archiviato su un‚Äôunit√† microcontrollore con software crittografato e interfacce fisiche protette del chip. Inoltre, il set di dati finale utilizzato per l‚Äôaddestramento del modello dovrebbe essere mantenuto privato.\nInoltre, i modelli on-device utilizzano spesso set di dati noti o open source, come Visual Wake Words di MobileNet. Pertanto, √® importante mantenere la privacy del set di dati finale utilizzato per l‚Äôaddestramento del modello. Inoltre, proteggere il processo di ‚Äúdata augmentation‚Äù e incorporare casi d‚Äôuso specifici pu√≤ ridurre al minimo il rischio di reverse engineering di un modello on-device.\nInfine, l‚ÄôAdversarial Threat Landscape for Artificial Intelligence Systems (ATLAS) funge da prezioso strumento matriciale che aiuta a valutare il profilo di rischio dei modelli su dispositivo, consentendo agli sviluppatori di identificare e mitigare i potenziali rischi in modo proattivo.\n\n\n12.6.6 Protezione dei Dati di Training\nEsistono vari modi per proteggere i dati di training sul dispositivo. Ogni concetto √® molto profondo e potrebbe valere una lezione a s√© stante. Quindi, qui, faremo un breve accenno a quei concetti in modo che si sappia cosa approfondire.\n\nCrittografia\nLa crittografia funge da prima linea di difesa per i dati di training. Ci√≤ comporta l‚Äôimplementazione della crittografia end-to-end per l‚Äôarchiviazione locale su dispositivi e canali di comunicazione per impedire l‚Äôaccesso non autorizzato ai dati di training grezzi. Ambienti di esecuzione affidabili, come Intel SGX e ARM TrustZone, sono essenziali per facilitare il training sicuro su dati crittografati.\nInoltre, quando si aggregano aggiornamenti da pi√π dispositivi, √® possibile impiegare protocolli di elaborazione ‚Äúmulti-party‚Äù sicuri per migliorare la sicurezza (Kairouz, Oh, e Viswanath 2015); un‚Äôapplicazione pratica di ci√≤ √® nell‚Äôapprendimento collaborativo on-device, in cui √® possibile implementare l‚Äôaggregazione crittografica che preserva la privacy degli aggiornamenti del modello utente. Questa tecnica nasconde efficacemente i dati dei singoli utenti anche durante la fase di aggregazione.\n\nKairouz, Peter, Sewoong Oh, e Pramod Viswanath. 2015. ¬´Secure Multi-party Differential Privacy.¬ª In Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada, a cura di Corinna Cortes, Neil D. Lawrence, Daniel D. Lee, Masashi Sugiyama, e Roman Garnett, 2008‚Äì16. https://proceedings.neurips.cc/paper/2015/hash/a01610228fe998f515a72dd730294d87-Abstract.html.\n\n\nPrivacy Differenziale\nLa privacy differenziale √® un‚Äôaltra strategia cruciale per proteggere i dati di training. Iniettando rumore statistico calibrato nei dati, possiamo mascherare i singoli record estraendo comunque preziosi pattern di popolazione (Dwork e Roth 2013). Anche la gestione del budget per la privacy su pi√π iterazioni di training e la riduzione del rumore man mano che il modello converge sono essenziali (Abadi et al. 2016). Possono essere impiegati metodi come la privacy differenziale formalmente dimostrabile, che pu√≤ includere l‚Äôaggiunta di rumore di Laplace o gaussiano scalato alla sensibilit√† del set di dati.\n\nDwork, Cynthia, e Aaron Roth. 2013. ¬´The Algorithmic Foundations of Differential Privacy¬ª. Foundations and Trends¬Æ in Theoretical Computer Science 9 (3-4): 211‚Äì407. https://doi.org/10.1561/0400000042.\n\nAbadi, Martin, Andy Chu, Ian Goodfellow, H. Brendan McMahan, Ilya Mironov, Kunal Talwar, e Li Zhang. 2016. ¬´Deep Learning with Differential Privacy¬ª. In Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security, 308‚Äì18. CCS ‚Äô16. New York, NY, USA: ACM. https://doi.org/10.1145/2976749.2978318.\n\n\nRilevamento delle Anomalie\nIl rilevamento delle anomalie svolge un ruolo importante nell‚Äôidentificazione e nell‚Äôattenuazione di potenziali attacchi di avvelenamento dei dati. Ci√≤ pu√≤ essere ottenuto tramite analisi statistiche come la ‚ÄúPrincipal Component Analysis (PCA)‚Äù [analisi delle componenti principali] e il clustering, che aiutano a rilevare deviazioni nei dati di training aggregati. I metodi di serie temporali come i grafici Cumulative Sum (CUSUM) sono utili per identificare spostamenti indicativi di potenziale avvelenamento. Anche il confronto delle distribuzioni dei dati correnti con distribuzioni di dati pulite precedentemente visualizzate pu√≤ aiutare a segnalare anomalie. Inoltre, i batch sospetti di essere avvelenati dovrebbero essere rimossi dal processo di aggregazione degli aggiornamenti di training. Ad esempio, √® possibile condurre controlli a campione su sottoinsiemi di immagini di training sui dispositivi utilizzando hash photoDNA per identificare input avvelenati.\n\n\nValidazione dei Dati di Input\nInfine, la convalida dei dati di input √® essenziale per garantire l‚Äôintegrit√† e la validit√† dei dati di input prima che vengano immessi nel modello di training, proteggendo cos√¨ dai payload avversari. Misure di similarit√†, come la distanza del coseno, possono essere impiegate per catturare input che si discostano in modo significativo dalla distribuzione prevista. Gli input sospetti che potrebbero contenere payload avversari devono essere messi in quarantena e sanificati. Inoltre, l‚Äôaccesso del parser ai dati di training deve essere limitato solo ai percorsi di codice convalidati. Sfruttare le funzionalit√† di sicurezza hardware, come ARM Pointer Authentication, pu√≤ impedire la corruzione della memoria (ARM Limited, 2023). Un esempio di ci√≤ √® l‚Äôimplementazione di controlli di integrit√† degli input sui dati di training audio utilizzati dagli smart speaker prima dell‚Äôelaborazione da parte del modello di riconoscimento vocale (Z. Chen e Xu 2023).\n\nChen, Zhiyong, e Shugong Xu. 2023. ¬´Learning domain-heterogeneous speaker recognition systems with personalized continual federated learning¬ª. EURASIP Journal on Audio, Speech, and Music Processing 2023 (1): 33. https://doi.org/10.1186/s13636-023-00299-2.",
    "crumbs": [
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Apprendimento On-Device</span>"
    ]
  },
  {
    "objectID": "contents/core/ondevice_learning/ondevice_learning.it.html#framework-di-training-on-device",
    "href": "contents/core/ondevice_learning/ondevice_learning.it.html#framework-di-training-on-device",
    "title": "12¬† Apprendimento On-Device",
    "section": "12.7 Framework di Training On-Device",
    "text": "12.7 Framework di Training On-Device\nFramework di inferenza embedded come TF-Lite Micro (David et al. 2021), TVM (T. Chen et al. 2018) e MCUNet (Lin et al. 2020) forniscono un runtime snello per l‚Äôesecuzione di modelli di reti neurali su microcontrollori e altri dispositivi con risorse limitate. Tuttavia, non supportano l‚Äôaddestramento on-device. L‚Äôaddestramento richiede un proprio set di strumenti specializzati a causa dell‚Äôimpatto della quantizzazione sul calcolo del gradiente e dell‚Äôingombro di memoria della backpropagation (Lin et al. 2022).\n\nDavid, Robert, Jared Duke, Advait Jain, Vijay Janapa Reddi, Nat Jeffries, Jian Li, Nick Kreeger, et al. 2021. ¬´Tensorflow lite micro: Embedded machine learning for tinyml systems¬ª. Proceedings of Machine Learning and Systems 3: 800‚Äì811.\n\nChen, Tianqi, Thierry Moreau, Ziheng Jiang, Lianmin Zheng, Eddie Yan, Haichen Shen, Meghan Cowan, et al. 2018. ¬´TVM: An automated End-to-End optimizing compiler for deep learning¬ª. In 13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18), 578‚Äì94.\n\nLin, Ji, Wei-Ming Chen, Yujun Lin, John Cohn, Chuang Gan, e Song Han. 2020. ¬´MCUNet: Tiny Deep Learning on IoT Devices¬ª. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, a cura di Hugo Larochelle, Marc‚ÄôAurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, e Hsuan-Tien Lin. https://proceedings.neurips.cc/paper/2020/hash/86c51678350f656dcc7f490a43946ee5-Abstract.html.\n\nLin, Ji, Ligeng Zhu, Wei-Ming Chen, Wei-Chen Wang, Chuang Gan, e Song Han. 2022. ¬´On-device training under 256kb memory¬ª. Adv. Neur. In. 35: 22941‚Äì54.\nNegli ultimi anni, hanno iniziato a emergere una manciata di strumenti e framework che consentono l‚Äôaddestramento sul dispositivo. Tra questi Tiny Training Engine (Lin et al. 2022), TinyTL (Cai et al. 2020) e TinyTrain (Kwon et al. 2023).\n\n12.7.1 Tiny Training Engine\nTiny Training Engine (TTE) utilizza diverse tecniche per ottimizzare l‚Äôutilizzo della memoria e velocizzare il processo di training. Una panoramica del flusso di lavoro TTE √® mostrata in Figura¬†12.10. Innanzitutto, TTE scarica la differenziazione automatica in fase di compilazione anzich√© in fase di runtime, riducendo significativamente il sovraccarico durante il training. In secondo luogo, TTE esegue l‚Äôottimizzazione del grafo come la potatura e gli aggiornamenti sparsi per ridurre i requisiti di memoria e accelerare i calcoli.\n\n\n\n\n\n\nFigura¬†12.10: Flusso di lavoro di TTE.\n\n\n\nIn particolare, TTE segue quattro passaggi principali:\n\nDurante la fase di compilazione, TTE traccia il grafo di propagazione ‚Äúforward‚Äù e deriva il grafo ‚Äúbackward‚Äù corrispondente per la backpropagation. Ci√≤ consente alla differenziazione di avvenire in fase di compilazione anzich√© in fase di esecuzione.\nTTE elimina tutti i nodi che rappresentano pesi congelati dal grafo backward. I pesi congelati sono pesi che non vengono aggiornati durante l‚Äôaddestramento per ridurre l‚Äôimpatto di determinati neuroni. La potatura dei loro nodi consente di risparmiare memoria.\nTTE riordina gli operatori di discesa del gradiente per intercalarli con i calcoli del passaggio del backward. Questa pianificazione riduce al minimo le ‚Äúimpronte‚Äù [occupazione] di memoria.\nTTE utilizza la generazione di codice per compilare i grafi ‚Äúforward‚Äù e ‚Äúbackward‚Äù ottimizzati, che vengono poi distribuiti per l‚Äôaddestramento on-device.\n\n\n\n12.7.2 Tiny Transfer Learning\nTiny Transfer Learning (TinyTL) consente un training efficiente in termini di memoria sul dispositivo tramite una tecnica chiamata congelamento dei pesi. Durante il training, gran parte del collo di bottiglia della memoria deriva dall‚Äôarchiviazione delle attivazioni intermedie e dall‚Äôaggiornamento dei pesi nella rete neurale.\nPer ridurre questo sovraccarico di memoria, TinyTL congela la maggior parte dei pesi in modo che non debbano essere aggiornati durante il training. Ci√≤ elimina la necessit√† di archiviare le attivazioni intermedie per le parti congelate della rete. TinyTL ottimizza solo i termini di bias, che sono molto pi√π piccoli dei pesi. Una panoramica del flusso di lavoro TinyTL √® mostrata in Figura¬†12.11.\n\n\n\n\n\n\nFigura¬†12.11: Flusso di lavoro di TinyTL. In (a), l‚Äôapprendimento per trasferimento convenzionale ottimizza sia i pesi che i bias, richiedendo una grande quantit√† di memoria (mostrata in blu) per le mappe di attivazione durante la retropropagazione. In (b), TinyTL riduce le esigenze di memoria fissando i pesi e ottimizzando solo i bias, consentendo l‚Äôapprendimento per trasferimento su dispositivi pi√π piccoli. Infine, in (c), TinyTL aggiunge un componente di apprendimento residuo ‚Äúlite‚Äù per compensare i pesi fissi, utilizzando convoluzioni di gruppo efficienti ed evitando colli di bottiglia pesanti in termini di memoria, ottenendo un‚Äôelevata efficienza con una memoria minima. Fonte: Cai et al. (2020).)\n\n\nCai, Han, Chuang Gan, Ligeng Zhu, e Song Han 0003. 2020. ¬´TinyTL: Reduce Memory, Not Parameters for Efficient On-Device Learning.¬ª In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, a cura di Hugo Larochelle, Marc‚ÄôAurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, e Hsuan-Tien Lin. https://proceedings.neurips.cc/paper/2020/hash/81f7acabd411274fcf65ce2070ed568a-Abstract.html.\n\n\nI pesi di congelamento si applicano a layer completamente connessi, nonch√© a layer di normalizzazione e convoluzionali. Tuttavia, solo l‚Äôadattamento dei bias limita la capacit√† del modello di apprendere e adattarsi a nuovi dati.\nPer aumentare l‚Äôadattabilit√† senza molta memoria aggiuntiva, TinyTL utilizza un piccolo modello di apprendimento residuo. Questo affina le mappe delle feature intermedie per produrre output migliori, anche con pesi fissi. Il modello residuo introduce un overhead minimo, inferiore al 3,8% in pi√π rispetto al modello di base.\nCongelando la maggior parte dei pesi, TinyTL riduce significativamente l‚Äôutilizzo della memoria durante l‚Äôaddestramento on-device. Il modello residuo consente quindi di adattarsi e apprendere in modo efficace per l‚Äôattivit√†. L‚Äôapproccio combinato fornisce un addestramento on-device efficiente in termini di memoria con un impatto minimo sulla precisione del modello.\n\n\n12.7.3 Tiny Train\nTinyTrain riduce significativamente il tempo necessario per l‚Äôaddestramento sul dispositivo aggiornando selettivamente solo determinate parti del modello. Ci√≤ avviene utilizzando una tecnica chiamata aggiornamento sparso adattivo all‚Äôattivit√†, come mostrato in Figura¬†12.12.\n\n\n\n\n\n\nFigura¬†12.12: Flusso di lavoro di TinyTrain. Fonte: Kwon et al. (2023).\n\n\nKwon, Young D., Rui Li, Stylianos I. Venieris, Jagmohan Chauhan, Nicholas D. Lane, e Cecilia Mascolo. 2023. ¬´TinyTrain: Resource-Aware Task-Adaptive Sparse Training of DNNs at the Data-Scarce Edge¬ª. ArXiv preprint abs/2307.09988 (luglio). http://arxiv.org/abs/2307.09988v2.\n\n\nIn base ai dati utente, alla memoria e al calcolo disponibili sul dispositivo, TinyTrain sceglie dinamicamente quali layer della rete neurale aggiornare durante l‚Äôaddestramento. Questa selezione di layer √® ottimizzata per ridurre l‚Äôutilizzo di calcolo e memoria mantenendo un‚Äôelevata accuratezza.\nPi√π specificamente, TinyTrain esegue prima il pre-addestramento offline del modello. Durante il pre-addestramento, non solo addestra il modello sui dati dell‚Äôattivit√†, ma anche il meta-addestramento del modello. Meta-addestramento significa addestrare il modello sui metadati relativi al processo di addestramento stesso. Questo meta-addestramento migliora la capacit√† del modello di adattarsi in modo accurato anche quando sono disponibili dati limitati per l‚Äôattivit√† target.\nPoi, durante la fase di adattamento online, quando il modello viene personalizzato sul dispositivo, TinyTrain esegue aggiornamenti adattivi sparsi all‚Äôattivit√†. Utilizzando i criteri relativi alle capacit√† del dispositivo, seleziona solo determinati layer da aggiornare tramite backpropagation. I layer vengono scelti per bilanciare accuratezza, utilizzo della memoria e tempo di elaborazione.\nAggiornando in modo sparso i layer su misura per il dispositivo e l‚Äôattivit√†, TinyTrain riduce significativamente il tempo di addestramento sul dispositivo e l‚Äôutilizzo delle risorse. Il meta-training offline migliora anche l‚Äôaccuratezza quando si adatta a dati limitati. Insieme, questi metodi consentono un training on-device rapido, efficiente e accurato.\n\n\n12.7.4 Confronto\nTabella¬†12.3 riassume le principali somiglianze e differenze tra i diversi framework.\n\n\n\nTabella¬†12.3: Confronto di framework per l‚Äôottimizzazione del training on-device.\n\n\n\n\n\n\n\n\n\n\nFramework\nSomiglianze\nDifferenze\n\n\n\n\nTiny Training Engine\n\nAddestramento sul dispositivo\nOttimizza memoria e calcolo\nSfrutta potatura, sparsit√†, ecc.\n\n\nTraccia grafi forward & backward\nElimina i pesi congelati\nInterlaccia backprop e gradienti\nGenerazione di codice\n\n\n\nTinyTL\n\nAddestramento sul dispositivo\nOttimizza memoria e calcolo\nSfrutta congelamento, sparsit√†, ecc.\n\n\nCongela la maggior parte dei pesi\nAdatta solo i bias\nUtilizza il modello residuo\n\n\n\nTinyTrain\n\nAddestramento sul dispositivo\nOttimizza memoria e calcolo\nSfrutta sparsit√†, ecc.\n\n\nMeta-addestramento nel pre-addestramento\nAggiornamento sparse adattivo alle attivit√†\nAggiornamento selettivo dei layer",
    "crumbs": [
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Apprendimento On-Device</span>"
    ]
  },
  {
    "objectID": "contents/core/ondevice_learning/ondevice_learning.it.html#conclusione",
    "href": "contents/core/ondevice_learning/ondevice_learning.it.html#conclusione",
    "title": "12¬† Apprendimento On-Device",
    "section": "12.8 Conclusione",
    "text": "12.8 Conclusione\nIl concetto di apprendimento on-device [su dispositivo] √® sempre pi√π importante per aumentare l‚Äôusabilit√† e la scalabilit√† di TinyML. Questo capitolo ha esplorato le complessit√† dell‚Äôapprendimento on-device, esplorandone vantaggi e limiti, strategie di adattamento, algoritmi e tecniche chiave correlate, implicazioni di sicurezza e framework di training on-device esistenti ed emergenti.\nL‚Äôapprendimento su dispositivo √®, senza dubbio, un paradigma rivoluzionario che porta con s√© numerosi vantaggi per le distribuzioni ML embedded ed edge. Eseguendo il training direttamente sui dispositivi endpoint, si elimina la necessit√† di una connettivit√† cloud continua, rendendolo particolarmente adatto per applicazioni IoT ed edge computing. Presenta vantaggi quali maggiore privacy, facilit√† di conformit√† ed efficienza delle risorse. Allo stesso tempo, l‚Äôapprendimento su on-device deve affrontare limitazioni legate a vincoli hardware, dimensioni dei dati limitate e ridotta accuratezza e generalizzazione del modello.\nMeccanismi quali la ridotta complessit√† del modello, tecniche di ottimizzazione e compressione dei dati e metodi di apprendimento correlati quali apprendimento tramite trasferimento e apprendimento federato consentono ai modelli di adattarsi per apprendere ed evolversi in base a vincoli di risorse, fungendo cos√¨ da fondamento per un efficace ML sui dispositivi edge.\nLe problematiche critiche di sicurezza nell‚Äôapprendimento su dispositivo evidenziate in questo capitolo, che vanno dall‚Äôavvelenamento dei dati e dagli attacchi avversari ai rischi specifici introdotti dall‚Äôapprendimento on-device, devono essere affrontate in carichi di lavoro reali affinch√© l‚Äôapprendimento su dispositivo sia un paradigma praticabile. Strategie di mitigazione efficaci, quali convalida dei dati, crittografia, privacy differenziale, rilevamento delle anomalie e convalida dei dati di input, sono fondamentali per salvaguardare i sistemi di apprendimento on-device da queste minacce.\nL‚Äôemergere di framework di training specializzati on-device, come Tiny Training Engine, Tiny Transfer Learning e Tiny Train, offre strumenti pratici che consentono un training efficiente sui dispositivi. Questi framework impiegano varie tecniche per ottimizzare l‚Äôutilizzo della memoria, ridurre il sovraccarico computazionale e semplificare il processo di training on-device.\nIn conclusione, l‚Äôapprendimento on-device √® in prima linea in TinyML, promettendo un futuro in cui i modelli possono acquisire autonomamente conoscenze e adattarsi ad ambienti mutevoli su dispositivi edge. L‚Äôapplicazione dell‚Äôapprendimento on-device ha il potenziale per rivoluzionare vari ambiti, tra cui sanit√†, IoT industriale e citt√† intelligenti. Tuttavia, il potenziale trasformativo dell‚Äôapprendimento on-device deve essere bilanciato con misure di sicurezza robuste per proteggere da violazioni dei dati e minacce avversarie. L‚Äôadozione di framework di training on-device innovativi e l‚Äôimplementazione di protocolli di sicurezza rigorosi sono passaggi chiave per sbloccare il pieno potenziale dell‚Äôapprendimento su dispositivo. Man mano che questa tecnologia continua a evolversi, promette di rendere i nostri dispositivi pi√π intelligenti, pi√π reattivi e meglio integrati nella nostra vita quotidiana.",
    "crumbs": [
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Apprendimento On-Device</span>"
    ]
  },
  {
    "objectID": "contents/core/ondevice_learning/ondevice_learning.it.html#sec-on-device-learning-resource",
    "href": "contents/core/ondevice_learning/ondevice_learning.it.html#sec-on-device-learning-resource",
    "title": "12¬† Apprendimento On-Device",
    "section": "12.9 Risorse",
    "text": "12.9 Risorse\nEcco un elenco curato di risorse per supportare studenti e insegnanti nei loro percorsi di apprendimento e insegnamento. Lavoriamo continuamente per espandere questa raccolta e presto aggiungeremo nuovi esercizi.\n\n\n\n\n\n\nSlide\n\n\n\n\n\nQueste slide sono uno strumento prezioso per gli insegnanti per tenere lezioni e per gli studenti per rivedere il materiale al proprio ritmo. Incoraggiamo sia gli studenti che gli insegnanti a sfruttare queste slide per migliorare la loro comprensione e facilitare un trasferimento efficace delle conoscenze.\n\nIntro to TensorFlow Lite (TFLite).\nTFLite Optimization and Quantization.\nTFLite Quantization-Aware Training.\nTrasferimento dell‚ÄôApprendimento:\n\nTransfer Learning: with Visual Wake Words example.\nOn-device Training and Transfer Learning.\n\nAddestramento Distribuito:\n\nDistributed Training.\nDistributed Training.\n\nMonitoraggio Continuo:\n\nContinuous Evaluation Challenges for TinyML.\nFederated Learning Challenges.\nContinuous Monitoring with Federated ML.\nContinuous Monitoring Impact on MLOps.\n\n\n\n\n\n\n\n\n\n\n\nVideo\n\n\n\n\n\n\nVideo¬†12.1\nVideo¬†12.2\n\n\n\n\n\n\n\n\n\n\nEsercizi\n\n\n\n\n\nPer rafforzare i concetti trattati in questo capitolo, abbiamo curato una serie di esercizi che sfidano gli studenti ad applicare le proprie conoscenze e ad approfondire la propria comprensione.\n\nEsercizio¬†12.1\nEsercizio¬†12.2\nEsercizio¬†12.3",
    "crumbs": [
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Apprendimento On-Device</span>"
    ]
  },
  {
    "objectID": "contents/core/ops/ops.it.html",
    "href": "contents/core/ops/ops.it.html",
    "title": "13¬† Operazioni di ML",
    "section": "",
    "text": "13.1 Panoramica\nMachine Learning Operations (MLOps) √® un approccio sistematico che combina machine learning (ML), data science e ingegneria del software per automatizzare il ciclo di vita end-to-end di ML. Ci√≤ include tutto, dalla preparazione dei dati e dal training del modello alla distribuzione e alla manutenzione. MLOps garantisce che i modelli ML siano sviluppati, distribuiti e mantenuti in modo efficiente ed efficace.\nCominciamo prendendo un caso di esempio generale (ad esempio, ML non edge). Prendiamo in considerazione un‚Äôazienda di ‚Äúride sharing‚Äù che desidera distribuire un modello di machine learning per prevedere la domanda dei passeggeri in tempo reale. Il team di data science impiega mesi per sviluppare un modello, ma quando √® il momento di distribuirlo, si rende conto che deve essere compatibile con l‚Äôambiente di produzione del team di ingegneria. La distribuzione del modello richiede la ricostruzione da zero, il che comporta settimane di lavoro aggiuntivo. √à qui che entra in gioco MLOps.\nCon MLOps, protocolli e strumenti, il modello sviluppato dal team di data science pu√≤ essere distribuito e integrato senza problemi nell‚Äôambiente di produzione. In sostanza, MLOps elimina gli attriti durante lo sviluppo, la distribuzione e la manutenzione dei sistemi ML. Migliora la collaborazione tra i team tramite flussi di lavoro e interfacce definiti. MLOps accelera anche la velocit√† di iterazione consentendo la distribuzione continua per i modelli ML.\nPer l‚Äôazienda di ride sharing, implementare MLOps significa che il loro modello di previsione della domanda pu√≤ essere frequentemente riqualificato e distribuito in base ai nuovi dati in arrivo. Ci√≤ mantiene il modello accurato nonostante il cambiamento del comportamento del passeggero. MLOps consente inoltre all‚Äôazienda di sperimentare nuove tecniche di modellazione poich√© i modelli possono essere rapidamente testati e aggiornati.\nAltri vantaggi di MLOps includono il monitoraggio avanzato della discendenza del modello, la riproducibilit√† e l‚Äôauditing. La catalogazione dei flussi di lavoro ML e la standardizzazione degli artefatti, come il logging delle versioni del modello, il monitoraggio della discendenza dei dati e il confezionamento di modelli e parametri, consente una visione pi√π approfondita della provenienza del modello. La standardizzazione di questi artefatti facilita la tracciabilit√† di un modello fino alle sue origini, la replica del processo di sviluppo del modello e l‚Äôesame di come una versione del modello √® cambiata nel tempo. Ci√≤ facilita anche la conformit√† alle normative, che √® particolarmente critica in settori regolamentati come sanit√† e finanza, dove √® importante essere in grado di verificare e spiegare i modelli.\nLe principali organizzazioni adottano MLOps per aumentare la produttivit√†, aumentare la collaborazione e accelerare i risultati ML. Fornisce i framework, gli strumenti e le best practice per gestire efficacemente i sistemi ML durante il loro ciclo di vita. Ci√≤ si traduce in modelli pi√π performanti, tempi di realizzazione pi√π rapidi e un vantaggio competitivo duraturo. Mentre esploriamo ulteriormente MLOps, si consideri come l‚Äôimplementazione di queste pratiche pu√≤ aiutare ad affrontare le sfide ML embedded oggi e in futuro.",
    "crumbs": [
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Operazioni di ML</span>"
    ]
  },
  {
    "objectID": "contents/core/ops/ops.it.html#contesto-storico",
    "href": "contents/core/ops/ops.it.html#contesto-storico",
    "title": "13¬† Operazioni di ML",
    "section": "13.2 Contesto Storico",
    "text": "13.2 Contesto Storico\nMLOps affonda le sue radici in DevOps, un insieme di pratiche che combinano sviluppo software (Dev) e operazioni IT (Ops) per accorciare il ciclo di vita dello sviluppo e fornire una distribuzione ‚Äúcontinua‚Äù di software di alta qualit√†. I parallelismi tra MLOps e DevOps sono evidenti nella loro attenzione all‚Äôautomazione, alla collaborazione e al miglioramento continuo. In entrambi i casi, l‚Äôobiettivo √® quello di abbattere i ‚Äúsilos‚Äù tra i diversi team (sviluppatori, operazioni e, nel caso di MLOps, data scientist e ingegneri ML) e creare un processo pi√π snello ed efficiente. √à utile comprendere meglio la storia di questa evoluzione per comprendere MLOps nel contesto dei sistemi tradizionali.\n\n13.2.1 DevOps\nIl termine ‚ÄúDevOps‚Äù √® stato coniato per la prima volta nel 2009 da Patrick Debois, un consulente e professionista Agile. Debois ha organizzato la prima conferenza DevOpsDays a Ghent, in Belgio, nel 2009. La conferenza ha riunito professionisti dello sviluppo e delle operazioni per discutere di modi per migliorare la collaborazione e automatizzare i processi.\nDevOps ha le sue radici nel movimento Agile, iniziato nei primi anni 2000. Agile ha fornito le basi per un approccio pi√π collaborativo allo sviluppo software e ha enfatizzato le piccole release iterative. Tuttavia, Agile si concentra principalmente sulla collaborazione tra team di sviluppo. Man mano che le metodologie Agile diventavano pi√π popolari, le organizzazioni si sono rese conto della necessit√† di estendere questa collaborazione ai team operativi.\nLa natura isolata dei team di sviluppo e delle operazioni ha spesso portato a inefficienze, conflitti e ritardi nella distribuzione del software. Questa necessit√† di una migliore collaborazione e integrazione tra questi team ha portato al movimento DevOps. DevOps pu√≤ essere visto come un‚Äôestensione dei principi Agile, inclusi i team operativi.\nI principi chiave di DevOps includono collaborazione, automazione, integrazione continua, distribuzione e feedback. DevOps si concentra sull‚Äôautomazione dell‚Äôintera pipeline di distribuzione del software, dallo sviluppo alla distribuzione. Migliora la collaborazione tra i team di sviluppo e operativi, utilizzando strumenti come Jenkins, Docker e Kubernetes per semplificare il ciclo di vita dello sviluppo.\nMentre Agile e DevOps condividono principi comuni in materia di collaborazione e feedback, DevOps mira specificamente all‚Äôintegrazione di sviluppo e operazioni IT, espandendo Agile oltre i soli team di sviluppo. Introduce pratiche e strumenti per automatizzare la distribuzione del software e migliorare la velocit√† e la qualit√† delle release del software.\n\n\n13.2.2 MLOps\nMLOps, d‚Äôaltro canto, sta per Machine Learning Operations ed estende i principi di DevOps al ciclo di vita ML. MLOps automatizza e semplifica l‚Äôintero ciclo di vita dell‚Äôapprendimento automatico, dalla preparazione dei dati allo sviluppo del modello, fino all‚Äôimplementazione e al monitoraggio. L‚Äôobiettivo principale di MLOps √® facilitare la collaborazione tra data scientist, data engineer e operazioni IT e automatizzare la distribuzione, il monitoraggio e la gestione dei modelli ML. Alcuni fattori chiave hanno portato all‚Äôascesa di MLOps.\n\nData drift: La deriva dei dati degrada le prestazioni del modello nel tempo, motivando la necessit√† di rigorosi monitoraggi e procedure di riqualificazione automatizzate fornite da MLOps.\nRiproducibilit√†: La mancanza di riproducibilit√† negli esperimenti di machine learning ha motivato i sistemi MLOps a tracciare codice, dati e variabili di ambiente per abilitare flussi di lavoro ML riproducibili.\nSpiegabilit√†: La natura di ‚Äúscatola nera‚Äù e la mancanza di spiegabilit√† di modelli complessi hanno motivato la necessit√† di funzionalit√† MLOps per aumentare la trasparenza e la spiegabilit√† del modello.\nMonitoraggio: L‚Äôincapacit√† di monitorare in modo affidabile le prestazioni del modello dopo la distribuzione ha evidenziato la necessit√† di soluzioni MLOps con una solida strumentazione delle prestazioni del modello e avvisi.\nAttrito: L‚Äôattrito nel riaddestramento e nella distribuzione manuale dei modelli ha motivato la necessit√† di sistemi MLOps che automatizzano le pipeline di distribuzione dell‚Äôapprendimento automatico.\nOttimizzazione: La complessit√† della configurazione dell‚Äôinfrastruttura di apprendimento automatico ha motivato la necessit√† di piattaforme MLOps con un‚Äôinfrastruttura ML ottimizzata e pronta all‚Äôuso.\n\nSebbene DevOps e MLOps condividano l‚Äôobiettivo comune di automatizzare e semplificare i processi, differiscono significativamente in termini di attenzione e sfide. DevOps si occupa principalmente di sviluppo software e operazioni IT. Consente la collaborazione tra questi team e automatizza la distribuzione del software. Al contrario, MLOps si concentra sul ciclo di vita dell‚Äôapprendimento automatico. Affronta complessit√† aggiuntive come versioning dei dati, versioning dei modelli e monitoraggio dei modelli. MLOps richiede la collaborazione tra una gamma pi√π ampia di stakeholder, tra cui data scientist, data engineer e IT operations. Va oltre l‚Äôambito del DevOps tradizionale incorporando le sfide uniche della gestione dei modelli ML durante il loro ciclo di vita. Tabella¬†13.1 fornisce un confronto affiancato di DevOps e MLOps, evidenziandone le principali differenze e somiglianze.\n\n\n\nTabella¬†13.1: Confronto tra DevOps e MLOps.\n\n\n\n\n\n\n\n\n\n\nAspect\nDevOps\nMLOps\n\n\n\n\nObiettivo\nSemplificazione dei processi di sviluppo software e operativi\nOttimizzazione del ciclo di vita dei modelli di apprendimento automatico\n\n\nMetodologia\nIntegrazione continua e distribuzione continua (CI/CD) per lo sviluppo software\nSimile a CI/CD ma incentrato sui flussi di lavoro di apprendimento automatico\n\n\nStrumenti Principali\nControllo delle versioni (Git), strumenti CI/CD (Jenkins, Travis CI), gestione della configurazione (Ansible, Puppet)\nStrumenti di versioning dei dati, strumenti di training e deployment dei modelli, pipeline CI/CD su misura per ML\n\n\nProblemi Principali\nIntegrazione del codice, test, gestione delle release, automazione, infrastruttura come codice\nGestione dei dati, versioning dei modelli, monitoraggio degli esperimenti, deployment dei modelli, scalabilit√† dei flussi di lavoro ML\n\n\nRisultati Tipici\nRelease software pi√π rapide e affidabili, collaborazione migliorata tra team di sviluppo e operativi\nGestione e deployment efficienti dei modelli di apprendimento automatico, collaborazione migliorata tra data scientist e ingegneri\n\n\n\n\n\n\nScoprire di pi√π sui cicli di vita ML tramite un ‚Äúcase study‚Äù che presenta il riconoscimento vocale in Video¬†13.1.\n\n\n\n\n\n\nVideo¬†13.1: MLOps",
    "crumbs": [
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Operazioni di ML</span>"
    ]
  },
  {
    "objectID": "contents/core/ops/ops.it.html#componenti-chiave-di-mlops",
    "href": "contents/core/ops/ops.it.html#componenti-chiave-di-mlops",
    "title": "13¬† Operazioni di ML",
    "section": "13.3 Componenti Chiave di MLOps",
    "text": "13.3 Componenti Chiave di MLOps\nI componenti principali di MLOps formano un framework completo che supporta il ciclo di vita end-to-end dei modelli ML in produzione, dallo sviluppo iniziale all‚Äôimplementazione e alla gestione continua. In questa sezione, ci basiamo su argomenti come l‚Äôautomazione e il monitoraggio dei capitoli precedenti, integrandoli in un framework pi√π ampio e introducendo anche ulteriori pratiche chiave come la ‚Äúgovernance‚Äù. Ogni componente contribuisce a operazioni ML pi√π fluide e snelle, con strumenti popolari che aiutano i team ad affrontare attivit√† specifiche all‚Äôinterno di questo ecosistema. Insieme, questi elementi rendono MLOps un approccio solido alla gestione dei modelli ML e alla creazione di valore a lungo termine all‚Äôinterno delle organizzazioni.\nFigura¬†13.1 illustra lo stack completo del sistema MLOps. Mostra i vari layer coinvolti nelle operazioni di apprendimento automatico. In cima allo stack ci sono modelli/applicazioni ML, come BERT, seguiti da framework/piattaforme ML come PyTorch. Il livello MLOps principale, etichettato come Model Orchestration, comprende diversi componenti chiave: Data Management, CI/CD, Model Training, Model Evaluation, Deployment e Model Serving. Alla base del livello MLOps c‚Äô√® il livello Infrastructure, rappresentato da tecnologie come Kubernetes. Questo livello gestisce aspetti come Job Scheduling, Resource Management, Capacity Management e Monitoring, tra gli altri. A tenere tutto insieme c‚Äô√® il layer Hardware, che fornisce le risorse computazionali necessarie per le operazioni ML.\n\n\n\n\n\n\nFigura¬†13.1: Lo stack MLOps, inclusi Modelli ML, Framework, Orchestrazione del Modello, Infrastruttura e Hardware, illustra il flusso di lavoro end-to-end di MLOps.\n\n\n\nQuesto approccio a ‚Äúlayer‚Äù [strati] in Figura¬†13.1 dimostra come MLOps integra varie tecnologie e processi per facilitare lo sviluppo, la distribuzione e la gestione di modelli di apprendimento automatico in un ambiente di produzione. La figura illustra efficacemente le interdipendenze tra diversi componenti e come si uniscono per formare un ecosistema MLOps completo.\n\n13.3.1 Gestione dei Dati\nI dati nella loro forma grezza, siano essi raccolti da sensori, database, app o altri sistemi, spesso richiedono una preparazione significativa prima di poter essere utilizzati per l‚Äôaddestramento o l‚Äôinferenza. Problemi come formati incoerenti, valori mancanti e convenzioni di etichettatura in evoluzione possono portare a inefficienze e scarse prestazioni del modello se non affrontati sistematicamente. Solide pratiche di gestione dei dati garantiscono che i dati rimangano di alta qualit√†, tracciabili e facilmente accessibili durante l‚Äôintero ciclo di vita del ML, costituendo la base di sistemi di apprendimento automatico scalabili.\nUn aspetto chiave della gestione dei dati √® il controllo delle versioni. Strumenti come Git, GitHub e GitLab consentono ai team di tenere traccia delle modifiche ai set di dati, collaborare alla loro cura e ripristinare le versioni precedenti quando necessario. Oltre al versioning, l‚Äôannotazione e l‚Äôetichettatura dei set di dati sono fondamentali per le attivit√† di apprendimento supervisionato. Software come LabelStudio aiutano i team distribuiti a etichettare i dati in modo coerente su set di dati su larga scala, mantenendo al contempo l‚Äôaccesso alle versioni precedenti man mano che le convenzioni di etichettatura si evolvono. Queste pratiche non solo migliorano la collaborazione, ma garantiscono anche che i modelli vengano addestrati su dati affidabili e ben organizzati.\nUna volta preparati, i set di dati vengono in genere archiviati su soluzioni cloud scalabili come Amazon S3 o Google Cloud Storage. Questi servizi forniscono versioning, resilienza e controlli di accesso granulari, salvaguardando i dati sensibili e mantenendo al contempo la flessibilit√† per l‚Äôanalisi e la modellazione. Per semplificare la transizione dai dati grezzi ai formati pronti per l‚Äôanalisi, i team creano pipeline automatizzate utilizzando strumenti come Prefect, Apache Airflow e dbt. Queste pipeline automatizzano attivit√† come l‚Äôestrazione, la pulizia, la de-duplicazione e la trasformazione dei dati, riducendo il sovraccarico manuale e migliorando l‚Äôefficienza.\nAd esempio, una pipeline di dati potrebbe acquisire informazioni da database PostgreSQL, API REST e file CSV archiviati in S3, applicando trasformazioni per produrre set di dati puliti e aggregati. L‚Äôoutput pu√≤ essere memorizzato in archivi di funzionalit√† come Tecton o Feast, che forniscono un accesso a bassa latenza sia per il training che per le previsioni. In uno scenario di manutenzione predittiva industriale, i dati dei sensori potrebbero essere elaborati insieme ai record di manutenzione, con conseguenti set di dati arricchiti archiviati in Feast per consentire ai modelli di accedere alle informazioni pi√π recenti senza problemi.\nIntegrando il controllo delle versioni, gli strumenti di annotazione, le soluzioni di archiviazione e le pipeline automatizzate, la gestione dei dati diventa un abilitatore fondamentale per MLOps efficaci. Queste pratiche garantiscono che i dati non siano solo puliti e accessibili, ma anche costantemente allineati con le mutevoli esigenze del progetto, consentendo ai sistemi di machine learning di fornire prestazioni affidabili e scalabili negli ambienti di produzione.\nVideo¬†13.2 di seguito riporta una breve panoramica delle pipeline di dati.\n\n\n\n\n\n\nVideo¬†13.2: Pipeline di Dati\n\n\n\n\n\n\n\n\n13.3.2 Pipeline CI/CD\nLe pipeline di integrazione continua e distribuzione continua (CI/CD) automatizzano attivamente la progressione dei modelli ML dallo sviluppo iniziale alla distribuzione in produzione. Adattati per i sistemi ML, i principi CI/CD consentono ai team di distribuire rapidamente e in modo robusto nuovi modelli con errori manuali ridotti al minimo.\nLe pipeline CI/CD orchestrano i passaggi chiave, tra cui il controllo delle nuove modifiche al codice, la trasformazione dei dati, il training e la registrazione di nuovi modelli, i test di convalida, la containerizzazione, la distribuzione in ambienti come cluster di staging e la promozione in produzione. I team sfruttano le soluzioni CI/CD pi√π diffuse come Jenkins, CircleCI e GitHub Actions per eseguire queste pipeline MLOps, mentre Prefect, Metaflow e Kubeflow offrono opzioni incentrate su ML.\nFigura¬†13.2 illustra una pipeline CI/CD specificamente pensata per MLOps. Il processo inizia con un dataset e un repository di feature (a sinistra), che alimenta una fase di ingestione del dataset. Dopo l‚Äôingestione, i dati vengono sottoposti a convalida per garantirne la qualit√† prima di essere trasformati per l‚Äôaddestramento. Parallelamente, un trigger di riaddestramento pu√≤ avviare la pipeline in base a criteri specificati. I dati passano poi attraverso una fase di addestramento/ottimizzazione del modello all‚Äôinterno di un motore di elaborazione dati, seguita dalla valutazione e convalida del modello. Una volta convalidato, il modello viene registrato e archiviato in un repository di metadati e artefatti di apprendimento automatico. La fase finale prevede la distribuzione del modello addestrato nuovamente nel dataset e nel repository di feature, creando cos√¨ un processo ciclico per il miglioramento continuo e la distribuzione di modelli di apprendimento automatico.\n\n\n\n\n\n\nFigura¬†13.2: Diagramma CI/CD MLOps. Fonte: HarvardX.\n\n\n\nAd esempio, quando uno scienziato dei dati verifica i miglioramenti a un modello di classificazione delle immagini in un repository GitHub, questo attiva attivamente una pipeline CI/CD Jenkins. La pipeline riesegue le trasformazioni dei dati e l‚Äôaddestramento del modello sui dati pi√π recenti, monitorando gli esperimenti con MLflow. Dopo i test di validazione automatizzati, i team distribuiscono il contenitore del modello in un cluster di staging Kubernetes per un ulteriore controllo qualit√†. Una volta approvato, Jenkins facilita un rollout graduale del modello in produzione con distribuzioni canary per rilevare eventuali problemi. Se vengono rilevate anomalie, la pipeline consente ai team di tornare alla versione precedente del modello in modo fluido.\nLe pipeline CI/CD consentono ai team di iterare e distribuire rapidamente modelli ML collegando i diversi passaggi dallo sviluppo alla distribuzione con automazione continua. L‚Äôintegrazione di strumenti MLOps come MLflow migliora il packaging del modello, il controllo delle versioni e la tracciabilit√† della pipeline. CI/CD √® fondamentale per far progredire i modelli oltre i prototipi in sistemi aziendali sostenibili.\n\n\n13.3.3 Addestramento del Modello\nL‚Äôaddestramento del modello √® una fase critica in cui gli scienziati dei dati sperimentano varie architetture e algoritmi ML per ottimizzare i modelli che estraggono informazioni dai dati. MLOps introduce best practice e automazione per rendere questo processo iterativo pi√π efficiente e riproducibile. I moderni framework ML come TensorFlow, PyTorch e Keras forniscono componenti predefiniti che semplificano la progettazione di reti neurali e altre architetture di modelli. Questi strumenti consentono agli scienziati dei dati di concentrarsi sulla creazione di modelli ad alte prestazioni utilizzando moduli integrati per layer, attivazioni e funzioni di perdita.\nPer rendere il processo di addestramento efficiente e riproducibile, MLOps introduce best practice come il controllo delle versioni del codice di addestramento tramite Git e l‚Äôhosting in repository come GitHub. Gli ambienti riproducibili, spesso gestiti tramite strumenti interattivi come i notebook Jupyter, consentono ai team di raggruppare l‚Äôingestione dei dati, la pre-elaborazione, lo sviluppo del modello e la valutazione in un unico documento. Questi notebook non sono solo controllati per la versione, ma possono anche essere integrati in pipeline automatizzate per un riaddestramento continuo.\nL‚Äôautomazione svolge un ruolo significativo nella standardizzazione dei flussi di lavoro di addestramento. Funzionalit√† come ottimizzazione degli iperparametri, ricerca dell‚Äôarchitettura neurale e selezione automatica delle feature sono comunemente integrate nelle pipeline MLOps per iterare rapidamente e trovare configurazioni ottimali. Le pipeline CI/CD orchestrano i flussi di lavoro di addestramento automatizzando attivit√† come la pre-elaborazione dei dati, l‚Äôaddestramento del modello, la valutazione e la registrazione. Ad esempio, una pipeline Jenkins pu√≤ attivare uno script Python per riaddestrare un modello TensorFlow, convalidarne le prestazioni rispetto a metriche predefinite e distribuirlo se vengono superate delle soglie.\nI servizi di addestramento gestiti dal cloud hanno rivoluzionato l‚Äôaccessibilit√† di hardware ad alte prestazioni per i modelli di training. Questi servizi forniscono accesso on-demand all‚Äôinfrastruttura accelerata da GPU, rendendo l‚Äôaddestramento avanzato fattibile anche per piccoli team. A seconda del provider, gli sviluppatori possono gestire autonomamente il flusso di lavoro di training o affidarsi a opzioni completamente gestite come Vertex AI Fine Tuning, che pu√≤ perfezionare automaticamente un modello di base utilizzando un set di dati etichettato. Tuttavia, √® importante notare che la domanda di hardware GPU spesso supera l‚Äôofferta e la disponibilit√† pu√≤ variare in base alla regione o agli accordi contrattuali, ponendo potenziali colli di bottiglia per i team che si affidano ai servizi cloud.\nUn esempio di flusso di lavoro prevede che uno scienziato dei dati utilizzi un notebook PyTorch per sviluppare un modello CNN per la classificazione delle immagini. La libreria fastai fornisce API di alto livello per semplificare l‚Äôaddestramento delle CNN sui set di dati delle immagini. Il notebook addestra il modello sui dati campione, valuta le metriche di accuratezza e ottimizza gli iperparametri come la velocit√† di apprendimento e i layer per ottimizzare le prestazioni. Questo notebook riproducibile √® controllato dalla versione e integrato in una pipeline di riaddestramento.\nAutomatizzando e standardizzando l‚Äôaddestramento dei modelli, sfruttando i servizi cloud gestiti e integrando framework moderni, i team possono accelerare la sperimentazione e creare modelli di apprendimento automatico solidi e pronti per la produzione.\n\n\n13.3.4 Valutazione del Modello\nPrima di distribuire i modelli, i team eseguono una valutazione e dei test rigorosi per convalidare i benchmark delle prestazioni e la prontezza per il rilascio. MLOps fornisce le best practice per la convalida del modello, l‚Äôaudit e i metodi di test controllati per ridurre al minimo i rischi durante la distribuzione.\nIl processo di valutazione inizia con il test dei modelli rispetto ai set di dati di test che sono indipendenti dai dati di training ma provengono dalla stessa distribuzione dei dati di produzione. Metriche chiave come accuratezza, AUC, precisione, richiamo e punteggio F1 vengono calcolate per quantificare le prestazioni del modello. Il monitoraggio di queste metriche nel tempo aiuta i team a identificare tendenze e potenziali degradazioni nel comportamento del modello, in particolare quando i dati di valutazione provengono da flussi di produzione live. Ci√≤ √® fondamentale per rilevare il data drift [deriva dei dati], in cui le modifiche nelle distribuzioni dei dati di input possono erodere l‚Äôaccuratezza del modello.\nPer convalidare le prestazioni nel mondo reale, il test canary distribuisce il modello a un piccolo sottoinsieme di utenti. Questa distribuzione graduale consente ai team di monitorare le metriche in un ambiente live e di individuare potenziali problemi prima della distribuzione su vasta scala. Aumentando gradualmente il traffico verso il nuovo modello, i team possono valutare con sicurezza il suo impatto sull‚Äôesperienza dell‚Äôutente finale. Ad esempio, un rivenditore potrebbe testare un modello di raccomandazione personalizzato confrontando le sue metriche di accuratezza e diversit√† con i dati storici. Durante la fase di test, il team monitora le metriche delle prestazioni live e identifica un leggero calo dell‚Äôaccuratezza nell‚Äôarco di due settimane. Per garantire la stabilit√†, il modello viene inizialmente distribuito al 5% del traffico web, monitorato per potenziali problemi e distribuito ampiamente solo dopo aver dimostrato la sua solidit√† in produzione.\nI modelli ML distribuiti sul cloud traggono vantaggio dalla connettivit√† Internet costante e dalla capacit√† di registrare ogni richiesta e risposta. Ci√≤ rende possibile riprodurre o generare richieste sintetiche per confrontare diversi modelli e versioni. Alcuni provider offrono strumenti che automatizzano parti del processo di valutazione, come il monitoraggio degli esperimenti di iperparametri o il confronto delle esecuzioni del modello. Ad esempio, piattaforme come Weights and Biases semplificano questo processo automatizzando il monitoraggio degli esperimenti e generando artefatti dalle esecuzioni di training.\nL‚Äôautomazione dei processi di valutazione e test, combinata con un attento test canary, riduce i rischi di distribuzione. Mentre i processi di valutazione automatizzati rilevano molti problemi, la supervisione umana rimane essenziale per esaminare le prestazioni su specifici segmenti di dati e identificare sottili debolezze. Questa combinazione di rigorosa convalida pre-distribuzione e test nel mondo reale fornisce ai team sicurezza quando mettono i modelli in produzione.\n\n\n13.3.5 Distribuzione del Modello\nI team devono confezionare, testare e tracciare correttamente i modelli ML per distribuirli in modo affidabile in produzione. MLOps introduce framework e procedure per il versioning attivo, la distribuzione, il monitoraggio e l‚Äôaggiornamento dei modelli in modi sostenibili.\nUn approccio comune alla distribuzione prevede la containerizzazione dei modelli tramite strumenti come Docker, che impacchettano codice, librerie e dipendenze in unit√† standardizzate. I container garantiscono una portabilit√† fluida tra gli ambienti, rendendo la distribuzione coerente e prevedibile. Framework come TensorFlow Serving e BentoML aiutano a servire le previsioni dai modelli distribuiti tramite API ottimizzate per le prestazioni. Questi framework gestiscono il versioning, il ridimensionamento e il monitoraggio.\nPrima del rollout su larga scala, i team distribuiscono modelli aggiornati in ambienti di staging o QA per testare rigorosamente le prestazioni. Tecniche come le distribuzioni shadow o canary vengono utilizzate per convalidare i nuovi modelli in modo incrementale. Ad esempio, le distribuzioni canary indirizzano una piccola percentuale di traffico al nuovo modello monitorando attentamente le prestazioni. Se non si verificano problemi, il traffico verso il nuovo modello aumenta gradualmente. Procedure di rollback robuste sono essenziali per gestire problemi imprevisti, ripristinando i sistemi alla versione precedente del modello stabile per garantire un‚Äôinterruzione minima. L‚Äôintegrazione con pipeline CI/CD automatizza ulteriormente il processo di distribuzione e rollback, consentendo cicli di iterazione efficienti.\nPer mantenere la discendenza e la verificabilit√†, i team tengono traccia degli artefatti del modello, inclusi script, pesi, log e metriche, utilizzando strumenti come MLflow. I registri dei modelli, come il registro dei modelli di Vertex AI, fungono da repository centralizzati per l‚Äôarchiviazione e la gestione dei modelli addestrati. Questi registri non solo facilitano i confronti delle versioni, ma spesso includono anche l‚Äôaccesso ai modelli di base, che possono essere open source, proprietari o ibridi (ad esempio, LLAMA). La distribuzione di un modello dal registro a un endpoint di inferenza √® semplificata, gestendo il provisioning delle risorse, il peso dei download del modello e l‚Äôhosting.\nGli endpoint di inferenza in genere espongono il modello distribuito tramite API REST per previsioni in tempo reale. A seconda dei requisiti di prestazioni, i team possono configurare risorse, come acceleratori GPU, per soddisfare gli obiettivi di latenza e produttivit√†. Alcuni provider offrono anche opzioni flessibili come inferenza serverless o batch, eliminando la necessit√† di endpoint persistenti e consentendo distribuzioni scalabili ed economiche. Ad esempio, AWS SageMaker Inference supporta tali configurazioni.\nSfruttando questi strumenti e pratiche, i team possono distribuire modelli ML in modo resiliente, garantendo transizioni fluide tra le versioni, mantenendo la stabilit√† della produzione e ottimizzando le prestazioni in diversi casi d‚Äôuso.\n\n\n13.3.6 Model Serving\nDopo il ‚Äúdeployment‚Äù [distribuzione] del modello, ML-as-a-Service diventa un componente fondamentale nel ciclo di vita di MLOps. I servizi online come Facebook/Meta gestiscono decine di trilioni di query di inferenza al giorno (Wu et al. 2019). Il ‚Äúmodel serving‚Äù colma il divario tra i modelli sviluppati e le applicazioni ML o gli utenti finali, assicurando che i modelli distribuiti siano accessibili, performanti e scalabili negli ambienti di produzione.\n\nWu, Carole-Jean, David Brooks, Kevin Chen, Douglas Chen, Sy Choudhury, Marat Dukhan, Kim Hazelwood, et al. 2019. ¬´Machine Learning at Facebook: Understanding Inference at the Edge¬ª. In 2019 IEEE International Symposium on High Performance Computer Architecture (HPCA), 331‚Äì44. IEEE; IEEE. https://doi.org/10.1109/hpca.2019.00048.\nDiversi framework facilitano il model serving, tra cui TensorFlow Serving, NVIDIA Triton Inference Server e KServe (in precedenza KFServing). Questi strumenti forniscono interfacce standardizzate per la distribuzione di modelli distribuiti su varie piattaforme e gestiscono molte complessit√† dell‚Äôinferenza del modello su larga scala.\nIl model serving pu√≤ essere categorizzato in tre tipi principali:\n\nOnline Serving: Fornisce previsioni in tempo reale con bassa latenza, il che √® fondamentale per applicazioni come sistemi di raccomandazione o rilevamento frodi.\nOffline Serving: Elabora grandi batch di dati in modo asincrono, adatto per attivit√† come la generazione periodica di report.\nNear-Online Serving (semi-sincrono): Bilancia tra online e offline, offrendo risposte relativamente rapide per applicazioni meno sensibili al tempo come i chatbot.\n\nUna delle sfide principali per i sistemi di model serving √® operare secondo requisiti di prestazioni definiti da Service Level Agreement (SLA) e Service Level Objective (SLO). Gli SLA sono contratti formali che specificano i livelli di servizio previsti. Questi livelli di servizio si basano su parametri quali tempo di risposta, disponibilit√† e produttivit√†. Gli SLO sono obiettivi interni che i team si prefiggono di soddisfare o superare i propri SLA.\nPer il model serving ML, gli accordi e gli obiettivi SLA e SLO hanno un impatto diretto sull‚Äôesperienza utente, sull‚Äôaffidabilit√† del sistema e sui risultati aziendali. Pertanto, i team ottimizzano attentamente la propria piattaforma di servizio. I serving system ML impiegano varie tecniche per ottimizzare le prestazioni e l‚Äôutilizzo delle risorse, come le seguenti:\n\nPianificazione e batch delle richieste: Gestisce in modo efficiente le richieste di inferenza ML in arrivo, ottimizzando le prestazioni tramite strategie di accodamento e raggruppamento intelligenti. Sistemi come Clipper (Crankshaw et al. 2017) introducono il servizio di previsione online a bassa latenza con tecniche di caching e batch.\nSelezione e routing delle istanze del modello: Algoritmi intelligenti indirizzano le richieste alle versioni o alle istanze del modello appropriate. INFaaS (Romero et al. 2021) esplora questo aspetto generando varianti del modello e navigando in modo efficiente nello spazio di compromesso in base ai requisiti di prestazioni e accuratezza.\nBilanciamento del carico: Distribuisce i carichi di lavoro in modo uniforme su pi√π istanze di servizio. MArk (Model Ark) (C. Zhang et al. 2019) dimostra tecniche efficaci di bilanciamento del carico per sistemi di servizio ML.\nAutoscaling delle istanze del modello: Regola dinamicamente la capacit√† in base alla domanda. Sia INFaaS (Romero et al. 2021) che MArk (C. Zhang et al. 2019) incorporano funzionalit√† di autoscaling per gestire in modo efficiente le fluttuazioni del carico di lavoro.\nOrchestration del modello: Gestisce l‚Äôesecuzione del modello, abilitando l‚Äôelaborazione parallela e l‚Äôallocazione strategica delle risorse. AlpaServe (Z. Li et al. 2023) dimostra tecniche avanzate per la gestione di modelli di grandi dimensioni e scenari di servizio complessi.\nPrevisione del tempo di esecuzione: Sistemi come Clockwork (Gujarati et al. 2020) si concentrano sul servizio ad alte prestazioni prevedendo i tempi di esecuzione delle singole inferenze e utilizzando in modo efficiente gli acceleratori hardware.\n\n\nCrankshaw, Daniel, Xin Wang, Guilio Zhou, Michael J Franklin, Joseph E Gonzalez, e Ion Stoica. 2017. ¬´Clipper: A \\(\\{\\)Low-Latency\\(\\}\\) online prediction serving system¬ª. In 14th USENIX Symposium on Networked Systems Design and Implementation (NSDI 17), 613‚Äì27.\n\nRomero, Francisco, Qian Li 0027, Neeraja J. Yadwadkar, e Christos Kozyrakis. 2021. ¬´INFaaS: Automated Model-less Inference Serving.¬ª In 2021 USENIX Annual Technical Conference (USENIX ATC 21), 397‚Äì411. https://www.usenix.org/conference/atc21/presentation/romero.\n\nZhang, Chengliang, Minchen Yu, Wei Wang 0030, e Feng Yan 0001. 2019. ¬´MArk: Exploiting Cloud Services for Cost-Effective, SLO-Aware Machine Learning Inference Serving.¬ª In 2019 USENIX Annual Technical Conference (USENIX ATC 19), 1049‚Äì62. https://www.usenix.org/conference/atc19/presentation/zhang-chengliang.\n\nLi, Zhuohan, Lianmin Zheng, Yinmin Zhong, Vincent Liu, Ying Sheng, Xin Jin, Yanping Huang, et al. 2023. ¬´\\(\\{\\)AlpaServe\\(\\}\\): Statistical multiplexing with model parallelism for deep learning serving¬ª. In 17th USENIX Symposium on Operating Systems Design and Implementation (OSDI 23), 663‚Äì79.\n\nGujarati, Arpan, Reza Karimi, Safya Alzayat, Wei Hao, Antoine Kaufmann, Ymir Vigfusson, e Jonathan Mace. 2020. ¬´Serving DNNs like Clockwork: Performance Predictability from the Bottom Up.¬ª In 14th USENIX Symposium on Operating Systems Design and Implementation (OSDI 20), 443‚Äì62. https://www.usenix.org/conference/osdi20/presentation/gujarati.\nI serving system ML che eccellono in queste aree consentono alle organizzazioni di distribuire modelli che funzionano in modo affidabile sotto pressione. Il risultato sono applicazioni AI scalabili e reattive in grado di gestire le richieste del mondo reale e fornire valore in modo coerente.\n\n\n13.3.7 Gestione dell‚ÄôInfrastruttura\nI team MLOps sfruttano ampiamente gli strumenti ‚Äúinfrastructure as code (IaC)‚Äù e le solide architetture cloud per gestire attivamente le risorse necessarie per lo sviluppo, il training e la distribuzione dei sistemi ML.\nI team utilizzano strumenti IaC come Terraform, CloudFormation e Ansible per definire, fornire e aggiornare a livello di programmazione l‚Äôinfrastruttura in modo controllato dalla versione. Per MLOps, i team utilizzano ampiamente Terraform per avviare risorse su AWS, GCP e Azure.\nPer la creazione e il training dei modelli, i team forniscono dinamicamente risorse di elaborazione come server GPU, cluster di container, storage e database tramite Terraform in base alle esigenze degli scienziati dei dati. Il codice incapsula e preserva le definizioni dell‚Äôinfrastruttura.\nI container e gli orchestratori come Docker e Kubernetes consentono ai team di impacchettare modelli e distribuirli in modo affidabile in diversi ambienti. I contenitori possono essere attivati o disattivati automaticamente in base alla domanda.\nSfruttando l‚Äôelasticit√† del cloud, i team aumentano o diminuiscono le risorse per soddisfare i picchi nei carichi di lavoro come i lavori di ottimizzazione degli iperparametri o i picchi nelle richieste di previsione. Auto-scaling consente un‚Äôefficienza dei costi ottimizzata.\nL‚Äôinfrastruttura si estende su dispositivi on-premise, cloud e edge. Uno stack tecnologico robusto offre flessibilit√† e resilienza. Gli strumenti di monitoraggio consentono ai team di osservare l‚Äôutilizzo delle risorse.\nAd esempio, una configurazione Terraform pu√≤ distribuire un cluster GCP Kubernetes per ospitare modelli TensorFlow addestrati esposti come microservizi di previsione. Il cluster aumenta i pod per gestire un traffico maggiore. L‚Äôintegrazione CI/CD distribuisce senza problemi nuovi contenitori di modelli.\nLa gestione attenta dell‚Äôinfrastruttura tramite IaC e monitoraggio consente ai team di prevenire i colli di bottiglia nell‚Äôoperativit√† dei sistemi ML su larga scala.\n\n\n13.3.8 Monitoraggio\nI team MLOps mantengono attivamente un monitoraggio robusto per mantenere la visibilit√† nei modelli ML distribuiti in produzione. Il monitoraggio continuo fornisce informazioni sulle prestazioni del modello e del sistema in modo che i team possano rilevare e risolvere rapidamente i problemi per ridurre al minimo le interruzioni.\nI team monitorano attivamente gli aspetti chiave del modello, inclusa l‚Äôanalisi di campioni di previsioni live per tracciare metriche come accuratezza e matrice di confusione nel tempo.\nQuando monitorano le prestazioni, i team devono profilare i dati in arrivo per verificare la deriva del modello, un calo costante dell‚Äôaccuratezza del modello dopo l‚Äôimplementazione in produzione. La deriva del modello pu√≤ verificarsi in due modi: deriva del concetto e deriva dei dati. La deriva del concetto si riferisce a un cambiamento fondamentale osservato nella relazione tra i dati di input e quelli target. Ad esempio, con l‚Äôavanzare della pandemia di COVID-19, i siti di e-commerce e vendita al dettaglio hanno dovuto correggere le raccomandazioni del modello poich√© i dati di acquisto erano ampiamente distorti verso articoli come il disinfettante per le mani. La deriva dei dati descrive i cambiamenti nella distribuzione dei dati nel tempo. Ad esempio, gli algoritmi di riconoscimento delle immagini utilizzati nelle auto a guida autonoma devono tenere conto della stagionalit√† nell‚Äôosservazione dell‚Äôambiente circostante. I team monitorano anche le metriche delle prestazioni delle applicazioni come latenza ed errori per le integrazioni dei modelli.\nDa una prospettiva infrastrutturale, i team monitorano i problemi di capacit√† come elevato utilizzo di CPU, memoria e disco e interruzioni del sistema. Strumenti come Prometheus, Grafana ed Elastic consentono ai team di raccogliere, analizzare, interrogare e visualizzare attivamente diverse metriche di monitoraggio. Le dashboard rendono le dinamiche altamente visibili.\nI team configurano gli allarmi per le metriche di monitoraggio chiave come cali di accuratezza e guasti del sistema per consentire una risposta proattiva agli eventi che minacciano l‚Äôaffidabilit√†. Ad esempio, i cali di accuratezza del modello attivano avvisi per i team per esaminare potenziali deviazioni dei dati e riaddestrare i modelli utilizzando campioni di dati aggiornati e rappresentativi.\nDopo la distribuzione, il monitoraggio completo consente ai team di mantenere la fiducia nello stato del modello e del sistema. Consente ai team di rilevare e risolvere preventivamente le deviazioni tramite allarmi e dashboard basati sui dati. Il monitoraggio attivo √® essenziale per mantenere sistemi ML altamente disponibili e affidabili.\nGuardare il video qui sotto per saperne di pi√π sul monitoraggio.\n\n\n\n\n\n\nVideo¬†13.3: Monitoraggio del Modello\n\n\n\n\n\n\n\n\n13.3.9 Governance\nI team MLOps stabiliscono attivamente pratiche di governance appropriate come componente fondamentale. La governance fornisce una supervisione sui modelli ML per garantire che siano affidabili, etici e conformi. Senza governance, sussistono rischi significativi di modelli che si comportano in modi pericolosi o proibiti quando vengono distribuiti in applicazioni e processi aziendali.\nLa governance MLOps impiega tecniche per fornire trasparenza sulle previsioni, sulle prestazioni e sul comportamento del modello durante l‚Äôintero ciclo di vita ML. Metodi di spiegabilit√† come SHAP e LIME aiutano gli auditor a comprendere perch√© i modelli effettuano determinate previsioni evidenziando le caratteristiche di input influenti alla base delle decisioni. Bias detection analizza le prestazioni del modello in diversi gruppi demografici definiti da attributi come et√†, sesso ed etnia per rilevare eventuali distorsioni sistematiche. I team eseguono rigorose procedure di test su set di dati rappresentativi per convalidare le prestazioni del modello prima della distribuzione.\nUna volta in produzione, i team monitorano la concept drift [deriva del concetto] per determinare se le relazioni predittive cambiano nel tempo in modi che degradano l‚Äôaccuratezza del modello. I team analizzano anche i registri di produzione per scoprire pattern nei tipi di errori generati dai modelli. La documentazione sulla provenienza dei dati, le procedure di sviluppo e le metriche di valutazione fornisce ulteriore visibilit√†.\nPiattaforme come Watson OpenScale incorporano funzionalit√† di governance come il monitoraggio dei bias e la spiegabilit√† direttamente nella creazione di modelli, nei test e nel monitoraggio della produzione. Le aree di interesse principali della governance sono trasparenza, correttezza e conformit√†. Ci√≤ riduce al minimo i rischi che i modelli si comportino in modo errato o pericoloso quando integrati nei processi aziendali. L‚Äôintegrazione di pratiche di governance nei flussi di lavoro MLOps consente ai team di garantire un‚ÄôIA affidabile.\n\n\n13.3.10 Comunicazione e Collaborazione\nMLOps abbatte attivamente i ‚Äúsilos‚Äù e consente il libero flusso di informazioni e approfondimenti tra i team in tutte le fasi del ciclo di vita ML. Strumenti come MLflow, Weights & Biases e contesti di dati forniscono tracciabilit√† e visibilit√† per migliorare la collaborazione.\nI team utilizzano MLflow per sistematizzare il monitoraggio di esperimenti, versioni e artefatti del modello. Gli esperimenti possono essere loggati a livello di programmazione da notebook di data science e job di training. Il registro dei modelli fornisce un hub centrale per i team per archiviare modelli pronti per la produzione prima della distribuzione, con metadati come descrizioni, metriche, tag e discendenza. Le integrazioni con Github, GitLab facilitano i trigger per la modifica del codice.\n‚ÄúWeights & Biases‚Äù fornisce strumenti collaborativi su misura per i team ML. Gli scienziati dei dati registrano gli esperimenti, visualizzano metriche come curve di perdita e condividono approfondimenti sulla sperimentazione con i colleghi. Le dashboard di confronto evidenziano le differenze del modello. I team discutono dei progressi e dei passaggi successivi.\nLa definizione di contesti di dati condivisi, ovvero glossari, dizionari di dati e riferimenti di schemi, garantisce l‚Äôallineamento del significato e dell‚Äôutilizzo dei dati tra i ruoli. La documentazione aiuta a comprendere chi non ha accesso diretto ai dati.\nAd esempio, uno scienziato dei dati pu√≤ utilizzare ‚ÄúWeights & Biases‚Äù per analizzare un esperimento con un modello di rilevamento delle anomalie e condividere i risultati della valutazione con altri membri del team per discutere dei miglioramenti. Il modello finale pu√≤ quindi essere registrato con MLflow prima di essere consegnato per la distribuzione.\nL‚Äôabilitazione della trasparenza, della tracciabilit√† e della comunicazione tramite MLOps consente ai team di rimuovere i colli di bottiglia e accelerare la distribuzione di sistemi ML di impatto.\nVideo¬†13.4 affronta le sfide chiave nella distribuzione del modello, tra cui la deriva del concetto, la deriva del modello e i problemi di ingegneria del software.\n\n\n\n\n\n\nVideo¬†13.4: Sfide della Distribuzione",
    "crumbs": [
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Operazioni di ML</span>"
    ]
  },
  {
    "objectID": "contents/core/ops/ops.it.html#debito-tecnico-nascosto-nei-sistemi-ml",
    "href": "contents/core/ops/ops.it.html#debito-tecnico-nascosto-nei-sistemi-ml",
    "title": "13¬† Operazioni di ML",
    "section": "13.4 Debito Tecnico Nascosto nei Sistemi ML",
    "text": "13.4 Debito Tecnico Nascosto nei Sistemi ML\nIl debito tecnico [https://it.wikipedia.org/wiki/Debito_tecnico] √® sempre pi√π pressante per i sistemi di apprendimento automatico. Questa metafora, originariamente proposta negli anni ‚Äô90, paragona i costi a lungo termine dello sviluppo rapido del software al debito finanziario. Proprio come un debito finanziario alimenta una crescita vantaggiosa, un debito tecnico gestito con attenzione consente una rapida iterazione. Tuttavia, se non controllato, l‚Äôaccumulo di debito tecnico pu√≤ superare qualsiasi guadagno.\nFigura¬†13.3 illustra i vari componenti che contribuiscono al debito tecnico nascosto dei sistemi ML. Mostra la natura interconnessa di configurazione, raccolta dati ed estrazione di funzionalit√†, che √® fondamentale per la base di codice ML. Le dimensioni delle caselle indicano la proporzione dell‚Äôintero sistema rappresentata da ciascun componente. Nei sistemi ML industriali, il codice per l‚Äôalgoritmo del modello costituisce solo una piccola frazione (vedere la piccola casella nera al centro rispetto a tutte le altre caselle grandi). La complessit√† dei sistemi ML e la natura frenetica del settore rendono molto facile l‚Äôaccumulo di debito tecnico.\n\n\n\n\n\n\nFigura¬†13.3: Componenti del sistema ML. Fonte: Sambasivan et al. (2021)\n\n\n\n\n13.4.1 Erosione dei Confini del Modello\nA differenza del software tradizionale, ML non ha confini chiari tra i componenti, come si vede nel diagramma sopra. Questa erosione dell‚Äôastrazione crea intrecci che esacerbano il debito tecnico in diversi modi:\n\n\n13.4.2 Intreccio\nUn accoppiamento stretto tra i componenti del modello ML rende difficile isolare le modifiche. La modifica di una parte provoca effetti a catena imprevedibili in tutto il sistema. ‚ÄúChanging Anything Changes Everything (noto anche come CACE)‚Äù [Cambiare qualcosa cambia tutto] √® un fenomeno che si applica a qualsiasi modifica apportata al sistema. Le potenziali mitigazioni includono la scomposizione del problema quando possibile o il monitoraggio ravvicinato delle modifiche nel comportamento per contenerne l‚Äôimpatto.\n\n\n13.4.3 Cascate di Correzione\nFigura¬†13.4 illustra il concetto di cascate di correzione nel flusso di lavoro ML, dalla definizione del problema all‚Äôimplementazione del modello. Gli archi rappresentano le potenziali correzioni iterative necessarie in ogni fase del flusso di lavoro, con colori diversi corrispondenti a problemi distinti come l‚Äôinterazione con la fragilit√† del mondo fisico, competenze inadeguate nel dominio dell‚Äôapplicazione, sistemi di ricompensa in conflitto e scarsa documentazione inter-organizzativa.\nLe frecce rosse indicano l‚Äôimpatto delle cascate, che possono portare a revisioni significative nel processo di sviluppo del modello. Al contrario, la linea rossa tratteggiata rappresenta la misura drastica di abbandono del processo per riavviarlo. Questa immagine sottolinea la natura complessa e interconnessa dello sviluppo del sistema ML e l‚Äôimportanza di affrontare questi problemi all‚Äôinizio del ciclo di sviluppo per mitigare i loro effetti di amplificazione a valle.\n\n\n\n\n\n\nFigura¬†13.4: Diagramma di flusso delle cascate di correzione. Fonte: Sambasivan et al. (2021).\n\n\nSambasivan, Nithya, Shivani Kapania, Hannah Highfill, Diana Akrong, Praveen Paritosh, e Lora M Aroyo. 2021. ¬´‚ÄúEveryone wants to do the model work, not the data work‚Äù: Data Cascades in High-Stakes AI¬ª. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems. ACM. https://doi.org/10.1145/3411764.3445518.\n\n\nLa creazione di modelli in sequenza crea dipendenze rischiose in cui i modelli successivi si basano su quelli precedenti. Ad esempio, prendere un modello esistente e perfezionarlo per un nuovo caso d‚Äôuso sembra efficiente. Tuttavia, questo incorpora ipotesi dal modello originale che potrebbero eventualmente richiedere una correzione.\nDiversi fattori influenzano la decisione di creare modelli in sequenza o meno:\n\nDimensioni del dataset e tasso di crescita: Con set di dati statici e di piccole dimensioni, la messa a punto dei modelli esistenti ha spesso senso. Per set di dati di grandi dimensioni e in crescita, l‚Äôaddestramento di modelli personalizzati da zero consente una maggiore flessibilit√† per tenere conto dei nuovi dati.\nRisorse di elaborazione disponibili: La messa a punto richiede meno risorse rispetto all‚Äôaddestramento di modelli di grandi dimensioni da zero. Con risorse limitate, sfruttare i modelli esistenti potrebbe essere l‚Äôunico approccio fattibile.\n\nMentre la messa a punto dei modelli esistenti pu√≤ essere efficiente, la modifica dei componenti fondamentali in seguito diventa estremamente costosa a causa di questi effetti a cascata. Pertanto, si dovrebbe considerare attentamente l‚Äôintroduzione di nuove architetture di modelli, anche se ad alta intensit√† di risorse, per evitare cascate di correzioni in futuro. Questo approccio pu√≤ aiutare ad attenuare gli effetti di amplificazione dei problemi a valle e a ridurre il debito tecnico. Tuttavia, ci sono ancora scenari in cui la creazione di modelli sequenziali ha senso, il che richiede un attento equilibrio tra efficienza, flessibilit√† e manutenibilit√† a lungo termine nel processo di sviluppo ML.\n\n\n13.4.4 Consumatori Non Dichiarati\nUna volta che le previsioni del modello ML sono rese disponibili, molti sistemi downstream [derivati] potrebbero utilizzarle silenziosamente come input per un‚Äôulteriore elaborazione. Tuttavia, il modello originale non √® stato progettato per adattarsi a questo ampio riutilizzo. A causa dell‚Äôopacit√† intrinseca dei sistemi ML, diventa impossibile analizzare completamente l‚Äôimpatto degli output del modello come input altrove. Le modifiche al modello possono quindi avere conseguenze costose e pericolose interrompendo dipendenze non rilevate.\nI ‚Äúconsumatori‚Äù non dichiarati possono anche abilitare loop di feedback nascosti se i loro output influenzano indirettamente i dati di training del modello originale. Le mitigazioni includono la limitazione dell‚Äôaccesso alle previsioni, la definizione di contratti di servizio rigorosi e il monitoraggio di segnali di influenze non-modellate. Architettare sistemi ML per incapsulare e isolare i loro effetti limita i rischi di propagazione imprevista.\n\n\n13.4.5 Debito di Dipendenza dai Dati\nIl debito di dipendenza dei dati si riferisce a dipendenze di dati instabili e sottoutilizzate, che possono avere ripercussioni dannose e difficili da rilevare. Sebbene questo sia un fattore chiave del debito tecnologico per il software tradizionale, tali sistemi possono trarre vantaggio dall‚Äôuso di strumenti ampiamente disponibili per l‚Äôanalisi statica da parte di compilatori e linker per identificare dipendenze di questo tipo. I sistemi ML necessitano di strumenti simili.\nUna mitigazione per le dipendenze di dati instabili √® l‚Äôuso del versioning, che garantisce la stabilit√† degli input ma comporta il costo della gestione di pi√π set di dati e il potenziale della obsolescenza. Un‚Äôaltra mitigazione per le dipendenze di dati sottoutilizzate √® quella di condurre una valutazione esaustiva ‚Äúleave-one-feature-out‚Äù.\n\n\n13.4.6 Debito di Analisi dai Cicli di Feedback\nA differenza del software tradizionale, i sistemi ML possono cambiare il loro comportamento nel tempo, rendendo difficile l‚Äôanalisi pre-distribuzione. Questo debito si manifesta nei cicli di feedback, sia diretti che nascosti.\nI cicli di feedback diretti si verificano quando un modello influenza i suoi input futuri, ad esempio consigliando prodotti agli utenti che, a loro volta, modellano i dati di training futuri. I cicli nascosti sorgono indirettamente tra modelli, ad esempio due sistemi che interagiscono tramite ambienti del mondo reale. I cicli di feedback graduali sono particolarmente difficili da rilevare. Questi cicli portano al debito di analisi, ovvero l‚Äôincapacit√† di prevedere come un modello agir√† completamente dopo il rilascio. Essi compromettono la validazione pre-distribuzione consentendo un‚Äôautoinfluenza non modellata.\nUn attento monitoraggio e distribuzioni ‚Äúcanary‚Äù aiutano a rilevare il feedback. Tuttavia, permangono sfide fondamentali nella comprensione delle interazioni complesse del modello. Le scelte architettoniche che riducono l‚Äôintreccio e l‚Äôaccoppiamento mitigano l‚Äôeffetto composto del debito di analisi.\n\n\n13.4.7 Le Giungle di Pipeline\nI workflow [flussi di lavoro] ML spesso necessitano di interfacce pi√π standardizzate tra i componenti. Ci√≤ porta i team a ‚Äúincollare‚Äù gradualmente le pipeline con codice personalizzato. Ci√≤ che emerge sono ‚Äúgiungle di pipeline‚Äù, ovvero passaggi di pre-elaborazione aggrovigliati che sono fragili e resistono al cambiamento. Evitare modifiche a queste pipeline disordinate fa s√¨ che i team sperimentino attraverso prototipi alternativi. Presto, proliferano molteplici modi di fare. La necessit√† di astrazioni e interfacce impedisce quindi la condivisione, il riutilizzo e l‚Äôefficienza.\nIl debito tecnico si accumula man mano che le pipeline si solidificano in vincoli legacy. I team sprecano tempo nella gestione di codice idiosincratico anzich√© massimizzare le prestazioni del modello. Principi architettonici come modularit√† e incapsulamento sono necessari per stabilire interfacce pulite. Le astrazioni condivise consentono componenti intercambiabili, impediscono il lock-in e promuovono la diffusione delle ‚Äúbest practice‚Äù tra i team. Liberarsi dalle ‚Äúgiungle di pipeline‚Äù richiede in definitiva l‚Äôapplicazione di standard che impediscano l‚Äôaccumulo di debito di astrazione. I vantaggi delle interfacce e delle API che domano la complessit√† superano i costi di transizione.\n\n\n13.4.8 Debito di Configurazione\nI sistemi ML comportano una configurazione estesa di iperparametri, architetture e altri parametri di ottimizzazione. Tuttavia, la configurazione √® spesso un ripensamento, che necessita di pi√π rigore e test: aumentano le configurazioni ad hoc, amplificate dalle numerose ‚Äúmanopole‚Äù disponibili per l‚Äôottimizzazione di modelli ML complessi.\nQuesto accumulo di debito tecnico ha diverse conseguenze. Configurazioni fragili e obsolete portano a dipendenze nascoste e bug che causano guasti di produzione. La conoscenza sulle configurazioni ottimali √® isolata anzich√© condivisa, portando a un lavoro ridondante. Riprodurre e confrontare i risultati diventa difficile quando le configurazioni mancano di documentazione. I vincoli legacy si accumulano poich√© i team temono di modificare configurazioni poco comprese.\nPer affrontare il debito di configurazione √® necessario stabilire standard per documentare, testare, convalidare e archiviare centralmente le configurazioni. Investire in approcci pi√π automatizzati, come l‚Äôottimizzazione degli iperparametri e la ricerca dell‚Äôarchitettura, riduce la dipendenza dall‚Äôottimizzazione manuale. Una migliore igiene della configurazione rende il miglioramento iterativo pi√π gestibile impedendo alla complessit√† di aumentare all‚Äôinfinito. La chiave √® riconoscere la configurazione come parte integrante del ciclo di vita del sistema ML piuttosto che come un ripensamento ad hoc.\n\n\n13.4.9 Il Mondo che Cambia\nI sistemi ML operano in ambienti dinamici del mondo reale. Le soglie e le decisioni inizialmente efficaci diventano obsolete man mano che il mondo si evolve. Tuttavia, i vincoli legacy rendono difficile adattare i sistemi a popolazioni, pattern di utilizzo e altri fattori contestuali mutevoli.\nQuesto debito si manifesta in due modi principali. In primo luogo, le soglie preimpostate e le euristiche richiedono una rivalutazione e una messa a punto costanti man mano che i loro valori ottimali si spostano. In secondo luogo, la convalida dei sistemi tramite test statici di unit√† e integrazione fallisce quando input e comportamenti sono obiettivi in movimento.\nRispondere a un mondo in continua evoluzione in tempo reale con sistemi ML legacy √® impegnativo. Il debito tecnico si accumula man mano che le ipotesi decadono. La mancanza di architettura modulare e la capacit√† di aggiornare dinamicamente i componenti senza effetti collaterali esacerbano questi problemi.\nPer mitigare questo problema √® necessario integrare configurabilit√†, monitoraggio e aggiornabilit√† modulare. L‚Äôapprendimento online, in cui i modelli si adattano continuamente e solidi cicli di feedback alle pipeline di training, aiutano a sintonizzarsi automaticamente sul mondo. Tuttavia, anticipare e progettare il cambiamento √® essenziale per prevenire l‚Äôerosione delle prestazioni nel mondo reale nel tempo.\n\n\n13.4.10 Gestire il Debito Tecnico nelle Fasi Iniziali\n√à comprensibile che il debito tecnico si accumuli naturalmente nelle prime fasi di sviluppo del modello. Quando si punta a creare rapidamente modelli MVP, i team spesso hanno bisogno di informazioni pi√π complete su quali componenti raggiungeranno la scala o richiederanno modifiche. √à previsto un po‚Äô di lavoro differito.\nTuttavia, anche i sistemi iniziali frammentati dovrebbero seguire principi come ‚ÄúFlexible Foundations‚Äù per evitare di mettersi nei guai:\n\nIl codice modulare e le librerie riutilizzabili consentono di scambiare i componenti in un secondo momento\nL‚Äôaccoppiamento debole tra modelli, archivi dati e logica aziendale facilita il cambiamento\nI layer di astrazione nascondono i dettagli di implementazione che potrebbero cambiare nel tempo\nIl servizio di modelli containerizzati mantiene aperte le opzioni sui requisiti di distribuzione\n\nLe decisioni che sembrano ragionevoli al momento possono limitare seriamente la flessibilit√† futura. Ad esempio, incorporare la logica aziendale chiave nel codice modello anzich√© tenerla separata rende estremamente difficili le modifiche successive al modello.\nCon una progettazione ponderata, tuttavia, √® possibile creare rapidamente all‚Äôinizio mantenendo gradi di libert√† per migliorare. Man mano che il sistema matura, emergono prudenti punti di interruzione in cui l‚Äôintroduzione di nuove architetture in modo proattivo evita massicce rilavorazioni in futuro. In questo modo si bilanciano le urgenti tempistiche con la riduzione delle future cascate di correzione.\n\n\n13.4.11 Riepilogo\nSebbene il debito finanziario sia una buona metafora per comprendere i compromessi, differisce dalla misurabilit√† del debito tecnico. Il debito tecnico deve essere completamente monitorato e quantificato. Ci√≤ rende difficile per i team gestire i compromessi tra muoversi rapidamente e introdurre intrinsecamente pi√π debito rispetto al prendersi il tempo per ripagare tale debito.\nIl documento Hidden Technical Debt of Machine Learning Systems diffonde la consapevolezza delle sfumature del debito tecnologico specifico del sistema ML. Incoraggia un ulteriore sviluppo nell‚Äôampia area del ML manutenibile.",
    "crumbs": [
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Operazioni di ML</span>"
    ]
  },
  {
    "objectID": "contents/core/ops/ops.it.html#sec-roles-and_resp-ops",
    "href": "contents/core/ops/ops.it.html#sec-roles-and_resp-ops",
    "title": "13¬† Operazioni di ML",
    "section": "13.5 Ruoli e Responsabilit√†",
    "text": "13.5 Ruoli e Responsabilit√†\nData la vastit√† di MLOps, l‚Äôimplementazione di successo di sistemi ML richiede competenze diversificate e una stretta collaborazione tra persone con diverse aree di competenza. Mentre gli scienziati dei dati creano i modelli ML di base, √® necessario un lavoro di squadra interfunzionale per distribuire con successo questi modelli in ambienti di produzione e consentire loro di fornire un valore aziendale sostenibile.\nMLOps fornisce il framework e le pratiche per coordinare gli sforzi di vari ruoli coinvolti nello sviluppo, nella distribuzione e nell‚Äôesecuzione di sistemi MLG. Collegare i ‚Äúsilos‚Äù tradizionali tra i team di dati, ingegneria e operazioni √® fondamentale per il successo di MLOps. Abilitare una collaborazione senza soluzione di continuit√† attraverso il ciclo di vita dell‚Äôapprendimento automatico accelera la realizzazione dei vantaggi garantendo al contempo l‚Äôaffidabilit√† e le prestazioni a lungo termine dei modelli ML.\nEsamineremo alcuni ruoli chiave coinvolti in MLOps e le loro responsabilit√† principali. Comprendere l‚Äôampiezza delle competenze necessarie per rendere operativi i modelli ML guida l‚Äôassemblaggio dei team MLOps. Chiarisce inoltre come i flussi di lavoro tra i ruoli si adattano alla metodologia MLOps sovraordinata.\n\n13.5.1 Ingegneri dei Dati\nGli ingegneri dei dati sono responsabili della creazione e della manutenzione dell‚Äôinfrastruttura dati e delle pipeline che alimentano i dati nei modelli ML. Garantiscono che i dati vengano trasferiti senza problemi dai sistemi di origine agli ambienti di archiviazione, elaborazione e progettazione delle funzionalit√† necessari per lo sviluppo e la distribuzione dei modelli ML. Le loro principali responsabilit√† includono:\n\nMigrare dati grezzi da database, sensori e app ‚Äúon-prem‚Äù [in azienda], in data lake basati su cloud, come Amazon S3 o Google Cloud Storage. Ci√≤ fornisce un‚Äôarchiviazione economica e scalabile.\nCreare pipeline di dati con ‚Äúscheduler‚Äù [pianificatori] di flussi di lavoro come Apache Airflow, Prefect e dbt. Questi estraggono i dati dalle sorgenti, li trasformano e li convalidano, e li caricano direttamente in destinazioni come data warehouse, feature store o per l‚Äôaddestramento del modello.\nTrasformare dati grezzi e disordinati in set di dati strutturati e pronti per l‚Äôanalisi. Ci√≤ include la gestione di valori nulli o malformati, la deduplicazione, l‚Äôunione di origini dati disparate, l‚Äôaggregazione dei dati e la progettazione di nuove feature.\nManutenere componenti dell‚Äôinfrastruttura dati come data warehouse cloud (Snowflake, Redshift, BigQuery), data lake e sistemi di gestione dei metadati. Provisioning e ottimizzazione dei sistemi di elaborazione dati.\nFornire e ottimizzare sistemi di elaborazione dati per una gestione e un‚Äôanalisi dei dati efficiente e scalabile.\nDefinire i processi di versioning, backup e archiviazione dei dati per i set di dati e funzionalit√† ML e applicare policy di governance dei dati.\n\nAd esempio, un‚Äôazienda manifatturiera pu√≤ utilizzare pipeline Apache Airflow per estrarre dati dei sensori dai PLC in fabbrica e trasferirli in un data lake Amazon S3. Gli ingegneri dei dati elaborerebbero poi questi dati grezzi per filtrarli, pulirli e unirli ai metadati del prodotto. Questi output della pipeline verrebbero quindi caricati in un data warehouse Snowflake da cui √® possibile leggere le feature per l‚Äôaddestramento e la previsione del modello.\nIl team di ingegneria dei dati crea e sostiene la base dati per uno sviluppo e un funzionamento affidabili del modello. Il loro lavoro consente agli scienziati dei dati e agli ingegneri ML di concentrarsi sulla creazione, l‚Äôaddestramento e l‚Äôimplementazione di modelli ML su larga scala.\n\n\n13.5.2 Data Scientist\nIl lavoro dei ‚Äúdata scientist‚Äù [scienziato dei dati] √® concentrarsi sulla ricerca, sperimentazione, sviluppo e miglioramento continuo dei modelli ML. Sfruttano la loro competenza in statistica, modellazione e algoritmi per creare modelli ad alte prestazioni. Le loro principali responsabilit√† includono:\n\nCollaborare con team aziendali e di dati per identificare opportunit√† in cui ML pu√≤ aggiungere valore, inquadrare il problema e definire metriche di successo.\nEseguire analisi esplorative dei dati per comprendere le relazioni nei dati, ricavare informazioni e identificare funzionalit√† rilevanti per la modellazione.\nRicercare e sperimentare diversi algoritmi ML e architetture di modelli in base al problema e alle caratteristiche dei dati e sfruttare librerie come TensorFlow, PyTorch e Keras.\nMassimizzare le prestazioni, addestrare e perfezionare i modelli regolando gli iperparametri, regolando le architetture delle reti neurali, l‚Äôingegneria delle funzionalit√†, ecc.\nValutare le prestazioni del modello tramite metriche come accuratezza, AUC e punteggi F1 ed eseguire analisi degli errori per identificare aree di miglioramento.\nSviluppare nuove versioni del modello mediante l‚Äôintegrazione di nuovi dati, test di diversi approcci, ottimizzazione del comportamento del modello e mantenimento della documentazione e della discendenza per i modelli.\n\nAd esempio, uno scienziato dei dati pu√≤ sfruttare TensorFlow e TensorFlow Probability per sviluppare un modello di previsione della domanda per la pianificazione dell‚Äôinventario ala vendita al dettaglio. Itereranno su diversi modelli di sequenza come LSTM e sperimenteranno funzionalit√† derivate da dati di prodotto, vendite e stagionali. Il modello verr√† valutato in base a metriche di errore rispetto alla domanda effettiva prima dell‚Äôimplementazione. Lo scienziato dei dati monitora le prestazioni e riqualifica/migliora il modello man mano che arrivano nuovi dati.\nI data scientist guidano la creazione, il miglioramento e l‚Äôinnovazione del modello attraverso la loro competenza nelle tecniche di ML. Collaborano strettamente con altri ruoli per garantire che i modelli creino il massimo impatto aziendale.\n\n\n13.5.3 ML Engineer\nGli ‚Äúingegneri ML‚Äù consentono ai modelli sviluppati dagli scienziati dei dati di essere prodotti e distribuiti su larga scala. La loro competenza fa s√¨ che i modelli servano in modo affidabile alle previsioni nelle applicazioni e nei processi aziendali. Le loro principali responsabilit√† includono:\n\nPrendere modelli prototipo dagli scienziati dei dati e rafforzarli per gli ambienti di produzione tramite best practice di codifica.\nCreare API e microservizi per la distribuzione dei modelli utilizzando strumenti come Flask, FastAPI. Containerizzare i modelli con Docker.\nGestire le versioni dei modelli, sincronizzarli in produzione utilizzando pipeline CI/CD e implementare release canary, test A/B e procedure di rollback.\nOttimizzare le prestazioni dei modelli per elevata scalabilit√†, bassa latenza ed efficienza dei costi. Sfruttare compressione, quantizzazione e servizio multi-modello.\nMonitorare i modelli una volta in produzione e garantire affidabilit√† e precisione continue. Riqualificare periodicamente i modelli.\n\nAd esempio, un ingegnere ML pu√≤ prendere un modello di rilevamento delle frodi TensorFlow sviluppato da data scientist e containerizzarlo utilizzando TensorFlow Serving per una distribuzione scalabile. Il modello verrebbe integrato nella pipeline di elaborazione delle transazioni dell‚Äôazienda tramite API. L‚Äôingegnere ML implementa un registro dei modelli e una pipeline CI/CD utilizzando MLFlow e Jenkins per distribuire gli aggiornamenti del modello in modo affidabile. Gli ingegneri ML monitorano quindi il modello in esecuzione per prestazioni continue utilizzando strumenti come Prometheus e Grafana. Se l‚Äôaccuratezza del modello diminuisce, avviano la riqualificazione e la distribuzione di una nuova versione del modello.\nIl team di ingegneria ML consente ai modelli di data science di progredire senza problemi in sistemi di produzione sostenibili e robusti. La loro competenza nella creazione di sistemi modulari e monitorati offre un valore aziendale continuo.\n\n\n13.5.4 DevOps Engineer\nGli ‚Äúingegneri DevOps‚Äù abilitano MLOps creando e gestendo l‚Äôinfrastruttura sottostante per lo sviluppo, la distribuzione e il monitoraggio dei modelli ML. In quanto branca specializzata dell‚Äôingegneria del software, DevOps si concentra sulla creazione di pipeline di automazione, architettura cloud e framework operativi. Le loro principali responsabilit√† includono:\n\nApprovvigionare e gestire l‚Äôinfrastruttura cloud per i flussi di lavoro ML utilizzando strumenti IaC come Terraform, Docker e Kubernetes.\nSviluppare pipeline CI/CD per il riaddestramento, la convalida e la distribuzione del modello. Integrare strumenti ML nella pipeline, come MLflow e Kubeflow.\nMonitorare le prestazioni del modello e dell‚Äôinfrastruttura tramite strumenti come Prometheus, Grafana, stack ELK. Creare allarmi e dashboard.\nImplementare pratiche di governance relative allo sviluppo, al test e alla promozione del modello per consentire riproducibilit√† e tracciabilit√†.\nEmbedding dei modelli ML nelle applicazioni. Espongono i modelli tramite API e microservizi per l‚Äôintegrazione.\nOttimizzazione delle prestazioni e dei costi dell‚Äôinfrastruttura e sfruttamento dell‚Äôautoscaling, delle istanze spot e della disponibilit√† in tutte le regioni.\n\nAd esempio, un ingegnere DevOps esegue il provisioning di un cluster Kubernetes su AWS utilizzando Terraform per eseguire lavori di training ML e distribuzione online. L‚Äôingegnere crea una pipeline CI/CD in Jenkins, che attiva il riaddestramento del modello quando sono disponibili nuovi dati. Dopo il test automatizzato, il modello viene registrato con MLflow e distribuito nel cluster Kubernetes. L‚Äôingegnere monitora quindi lo stato del cluster, l‚Äôutilizzo delle risorse del contenitore e la latenza dell‚ÄôAPI utilizzando Prometheus e Grafana.\nIl team DevOps consente una rapida sperimentazione e distribuzioni affidabili per ML tramite competenze cloud, automazione e monitoraggio. Il loro lavoro massimizza l‚Äôimpatto del modello riducendo al minimo il debito tecnico.\n\n\n13.5.5 Project Manager\nI project manager svolgono un ruolo fondamentale in MLOps coordinando le attivit√† tra i team coinvolti nella distribuzione dei progetti ML. Aiutano a guidare l‚Äôallineamento, la ‚Äúaccountability‚Äù [affidabilit√†] ed accelerano i risultati. Le loro principali responsabilit√† includono:\n\nCollaborare con le parti interessate per definire obiettivi di progetto, metriche di successo, tempistiche e budget; delineare specifiche e ‚Äúscope‚Äù.\nCreare un piano di progetto che comprenda acquisizione dati, sviluppo modello, configurazione infrastrutturale, distribuzione e monitoraggio.\nCoordinare i lavori di progettazione, sviluppo e test tra ingegneri dei dati, scienziati dei dati, ingegneri ML e ruoli DevOps.\nMonitorare i progressi e le milestone, identificare gli ostacoli e risolverli tramite azioni correttive e gestire rischi e problemi.\nFacilitare la comunicazione tramite report di stato, riunioni, workshop e documentazione e consentire una collaborazione senza interruzioni.\nGuidare l‚Äôaderenza alle tempistiche e al budget e aumentare i superamenti o le carenze previsti per la mitigazione.\n\nAd esempio, un project manager creerebbe un piano di progetto per sviluppare e migliorare un modello di previsione dell‚Äôabbandono dei clienti. Coordinare data engineer che creano pipeline di dati, data scientist che sperimentano modelli, ML engineer che producono modelli e DevOps che impostano l‚Äôinfrastruttura di distribuzione. Il project manager monitora i progressi tramite milestone come preparazione del set di dati, prototipazione del modello, distribuzione e monitoraggio. Per attuare soluzioni preventive, evidenziano eventuali rischi, ritardi o problemi di budget.\nI project manager qualificati consentono ai team MLOps di lavorare in sinergia per fornire rapidamente il massimo valore aziendale dagli investimenti ML. La loro leadership e organizzazione si allineano con team diversi.",
    "crumbs": [
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Operazioni di ML</span>"
    ]
  },
  {
    "objectID": "contents/core/ops/ops.it.html#mlops-tradizionali-e-mlops-embedded",
    "href": "contents/core/ops/ops.it.html#mlops-tradizionali-e-mlops-embedded",
    "title": "13¬† Operazioni di ML",
    "section": "13.6 MLOps Tradizionali e MLOps Embedded",
    "text": "13.6 MLOps Tradizionali e MLOps Embedded\nSulla base della nostra discussione su L‚Äôapprendimento ‚ÄúOn-device‚Äù nel capitolo precedente, ora rivolgiamo la nostra attenzione al contesto pi√π ampio dei sistemi embedded in MLOps. I vincoli e i requisiti unici degli ambienti embedded hanno un impatto significativo sull‚Äôimplementazione di modelli e operazioni di apprendimento automatico. Come abbiamo discusso nei capitoli precedenti, i sistemi embedded introducono sfide uniche per MLOps a causa delle loro risorse limitate, della connettivit√† intermittente e della necessit√† di un calcolo efficiente e consapevole del consumo energetico. A differenza degli ambienti cloud con abbondanti capacit√† di calcolo e storage, i dispositivi embedded spesso operano con capacit√† di memoria, potenza e elaborazione limitate, il che richiede un‚Äôattenta ottimizzazione dei flussi di lavoro. Queste limitazioni influenzano tutti gli aspetti di MLOps, dall‚Äôimplementazione e raccolta dati al monitoraggio e agli aggiornamenti.\nNegli MLOps tradizionali, i modelli ML vengono in genere distribuiti in ambienti basati su cloud o server, con risorse abbondanti come potenza di calcolo e memoria. Questi ambienti facilitano il funzionamento regolare di modelli complessi che richiedono risorse di calcolo significative. Ad esempio, un modello di riconoscimento delle immagini basato su cloud potrebbe essere utilizzato da una piattaforma di social media per contrassegnare automaticamente le foto con etichette pertinenti. In questo caso, il modello pu√≤ sfruttare le vaste risorse disponibili nel cloud per elaborare in modo efficiente grandi quantit√† di dati.\nD‚Äôaltro canto, i MLOps embedded comportano la distribuzione di modelli ML su sistemi embedded, sistemi di calcolo specializzati progettati per eseguire funzioni specifiche all‚Äôinterno di sistemi pi√π grandi. I sistemi embedded sono in genere caratterizzati dalle loro risorse di calcolo e potenza limitate. Ad esempio, un modello ML potrebbe essere ‚Äúembedded‚Äù in un termostato intelligente per ottimizzare il riscaldamento e il raffreddamento in base alle preferenze e alle abitudini dell‚Äôutente. Il modello deve essere ottimizzato per funzionare in modo efficiente sull‚Äôhardware limitato del termostato senza comprometterne le prestazioni o la precisione.\nLa differenza fondamentale tra i MLOps tradizionali e quelli embedded risiede nei vincoli di risorse del sistema embedded. Mentre gli MLOps tradizionali possono sfruttare abbondanti risorse cloud o server, gli MLOps embedded devono fare i conti con le limitazioni hardware su cui viene distribuito il modello. Ci√≤ richiede un‚Äôattenta ottimizzazione e messa a punto del modello per garantire che possa fornire informazioni accurate e preziose entro i vincoli del sistema embedded.\nInoltre, gli MLOps embedded devono considerare le sfide uniche poste dall‚Äôintegrazione dei modelli ML con altri componenti del sistema embedded. Ad esempio, il modello deve essere compatibile con il software e l‚Äôhardware del sistema e deve essere in grado di interfacciarsi senza problemi con altri componenti, come sensori o attuatori. Ci√≤ richiede una profonda comprensione sia dei sistemi ML che di quelli integrati e una stretta collaborazione tra data scientist, ingegneri e altre parti interessate.\nQuindi, mentre gli MLOps tradizionali e gli MLOps embedded condividono l‚Äôobiettivo comune di distribuire e mantenere modelli ML in ambienti di produzione, le sfide uniche poste dai sistemi embedded richiedono un approccio specializzato. Gli MLOps embedded devono bilanciare attentamente la necessit√† di accuratezza e prestazioni del modello con i vincoli dell‚Äôhardware su cui viene distribuito il modello. Ci√≤ richiede una profonda comprensione sia dei sistemi ML che di quelli embedded e una stretta collaborazione tra i vari stakeholder per garantire l‚Äôintegrazione di successo dei modelli ML nei sistemi embedded.\nQuesta volta, raggrupperemo i sottoargomenti in categorie pi√π ampie per semplificare la struttura del nostro processo di pensiero su MLOps. Questa struttura aiuter√† a comprendere come i diversi aspetti di MLOps siano interconnessi e perch√© ciascuno sia importante per il funzionamento efficiente dei sistemi ML mentre discutiamo le sfide nel contesto dei sistemi embedded.\n\nGestione del Ciclo di Vita del Modello\n\nGestione dei Dati: Gestione dell‚Äôingestione dei dati, convalida e controllo delle versioni.\nAddestramento dei Modelli: Tecniche e pratiche per un addestramento dei modelli efficace e scalabile.\nValutazione dei Modelli: Strategie per testare e convalidare le prestazioni dei modelli.\nDistribuzione dei modelli: Approcci per la distribuzione dei modelli in ambienti di produzione.\n\nIntegrazione di Sviluppo e Operazioni\n\nPipeline CI/CD: Integrazione dei modelli ML in pipeline di integrazione e distribuzione continue.\nGestione dell‚Äôinfrastruttura: Impostazione e manutenzione dell‚Äôinfrastruttura necessaria per il training e la distribuzione dei modelli.\nComunicazione e Collaborazione: Garanzia di una comunicazione e collaborazione fluide tra data scientist, ingegneri ML e team operativi.\n\nEccellenza operativa\n\nMonitoraggio: Tecniche per il monitoraggio delle prestazioni dei modelli, della deriva dei dati e dello stato operativo.\nGovernance: Implementazione di policy per la verificabilit√†, la conformit√† e le considerazioni etiche dei modelli.\n\n\n\n13.6.1 Gestione del Ciclo di Vita del Modello\n\nGestione dei Dati\nNei tradizionali MLOps centralizzati, i dati vengono aggregati in grandi dataset e data lake, poi elaborati su server cloud o ‚Äúon-prem‚Äù [in sede]. Tuttavia, MLOps embedded si basa su dati decentralizzati da sensori locali sui dispositivi. I dispositivi raccolgono batch pi√π piccoli di dati incrementali, spesso rumorosi e non strutturati. Con vincoli di connettivit√†, questi dati non possono sempre essere trasmessi istantaneamente al cloud e devono essere memorizzati nella cache in modo intelligente ed elaborati all‚Äôedge.\nA causa della potenza di calcolo limitata sui dispositivi embedded, i dati si possono solo preelaborare e pulire in modo minimo prima della trasmissione. Il filtraggio e l‚Äôelaborazione anticipati avvengono nei gateway edge per ridurre i carichi di trasmissione. Mentre si sfrutta l‚Äôarchiviazione cloud, altre elaborazioni e archiviazioni avvengono all‚Äôedge per tenere conto della connettivit√† intermittente. I dispositivi identificano e trasmettono solo i sottoinsiemi di dati pi√π critici al cloud.\nAnche l‚Äôetichettatura richiede un accesso centralizzato ai dati, che richiede tecniche pi√π automatizzate come l‚Äôapprendimento federato, in cui i dispositivi etichettano in modo collaborativo i dati dei peer. Con i dispositivi edge personali, la privacy dei dati e le normative sono preoccupazioni critiche. La raccolta, la trasmissione e l‚Äôarchiviazione dei dati devono essere sicure e conformi.\nAd esempio, uno smartwatch pu√≤ raccogliere il conteggio dei passi giornalieri, la frequenza cardiaca e le coordinate GPS. Questi dati vengono memorizzati nella cache locale e trasmessi a un gateway edge quando √® disponibile il WiFi: il gateway elabora e filtra i dati prima di sincronizzare i sottoinsiemi rilevanti con la piattaforma cloud per riaddestrare i modelli.\n\n\nAddestramento del Modello\nNei tradizionali MLOps centralizzati, i modelli vengono addestrati utilizzando dati abbondanti tramite deep learning su server GPU cloud ad alta potenza. Tuttavia, glii MLOps embedded necessitano di maggiore supporto in termini di complessit√† del modello, disponibilit√† dei dati e risorse di elaborazione per l‚Äôaddestramento.\nIl volume di dati aggregati √® molto pi√π basso, spesso richiedendo tecniche come l‚Äôapprendimento federato tra dispositivi per creare set di addestramento. La natura specializzata dei dati edge limita anche i set di dati pubblici per il pre-addestramento. Per questioni di privacy, i campioni di dati devono essere strettamente controllati e resi anonimi ove possibile.\nInoltre, i modelli devono utilizzare architetture semplificate ottimizzate per hardware edge a bassa potenza. Date le limitazioni di elaborazione, le GPU di fascia alta sono inaccessibili per un deep learning intensivo. L‚Äôaddestramento sfrutta server edge e cluster a bassa potenza con approcci distribuiti per spartire il carico.\nIl ‚Äútransfer learning‚Äù emerge come una strategia cruciale per affrontare la scarsit√† di dati e l‚Äôirregolarit√† nell‚Äôapprendimento automatico, in particolare negli scenari di edge computing. Come illustrato in Figura¬†13.5, questo approccio prevede il pre-training di modelli su grandi set di dati pubblici e la loro successiva messa a punto su dati edge specifici del dominio. La figura rappresenta una rete neurale in cui gli strati iniziali (da \\(W_{A1}\\) a \\(W_{A4}\\)), responsabili dell‚Äôestrazione delle caratteristiche generali, sono congelati (indicati da una linea tratteggiata verde). Questi layer conservano la conoscenza delle attivit√† precedenti, accelerando l‚Äôapprendimento e riducendo i requisiti di risorse. Gli ultimi layer (da \\(W_{A5}\\) a \\(W_{A7}\\)), oltre la linea tratteggiata blu, sono ottimizzati per l‚Äôattivit√† specifica, concentrandosi sull‚Äôapprendimento delle feature specifiche dell‚Äôattivit√†.\n\n\n\n\n\n\nFigura¬†13.5: Trasferimento dell‚Äôapprendimento in MLOps. Fonte: HarvardX.\n\n\n\nQuesto metodo non solo mitiga la scarsit√† di dati, ma si adatta anche alla natura decentralizzata dei dati embedded. Inoltre, tecniche come l‚Äôapprendimento incrementale sul dispositivo possono personalizzare ulteriormente i modelli in base a casi d‚Äôuso specifici. La mancanza di dati ampiamente etichettati in molti domini motiva anche l‚Äôuso di tecniche semi-supervisionate, che completano l‚Äôapproccio di apprendimento per trasferimento. Sfruttando le conoscenze preesistenti e adattandole a compiti specializzati, l‚Äôapprendimento per trasferimento all‚Äôinterno di un framework MLOps consente ai modelli di ottenere prestazioni pi√π elevate con meno risorse, anche in ambienti con vincoli di dati.\nAd esempio, un assistente domestico intelligente pu√≤ pre-addestrare un modello di riconoscimento audio su clip YouTube pubbliche, il che aiuta a eseguire il bootstrap con conoscenze generali. Quindi trasferisce l‚Äôapprendimento a un piccolo campione di dati domestici per classificare elettrodomestici ed eventi personalizzati, specializzandosi nel modello. Il modello si trasforma in una rete neurale leggera ottimizzata per dispositivi abilitati al microfono in tutta la casa.\nPertanto, gli MLOps embedded affrontano sfide acute nella costruzione di set di dati di training, nella progettazione di modelli efficienti e nella distribuzione del calcolo per lo sviluppo del modello rispetto alle impostazioni tradizionali. Dati i vincoli embedded, √® necessario un attento adattamento, come l‚Äôapprendimento tramite trasferimento e il training distribuito, per addestrare i modelli.\n\n\nValutazione del Modello\nNei tradizionali MLOps centralizzati, i modelli vengono valutati principalmente utilizzando metriche di accuratezza e dataset di test di holdout. Tuttavia, gli MLOps embedded richiedono una valutazione pi√π olistica che tenga conto dei vincoli di sistema oltre all‚Äôaccuratezza.\nI modelli devono essere testati in anticipo e spesso su hardware edge distribuito che copre diverse configurazioni. Oltre all‚Äôaccuratezza, fattori come latenza, utilizzo della CPU, ingombro di memoria e consumo energetico sono criteri di valutazione critici. I modelli vengono selezionati in base a compromessi tra queste metriche per soddisfare i vincoli dei dispositivi edge.\nAnche la deriva dei dati deve essere monitorata, dove i modelli addestrati sui dati cloud degradano in accuratezza nel tempo sui dati edge locali. I dati embedded hanno spesso una maggiore variabilit√† rispetto ai set di addestramento centralizzati. Valutare i modelli su diversi campioni di dati edge operativi √® fondamentale. Ma a volte, ottenere i dati per monitorare la deriva pu√≤ essere difficile se questi dispositivi sono in circolazione e la comunicazione √® una barriera.\nIl monitoraggio continuo fornisce visibilit√† sulle prestazioni del mondo reale dopo l‚Äôimplementazione, rivelando colli di bottiglia non evidenziati durante i test. Ad esempio, un aggiornamento del modello di una smart camera potrebbe essere inizialmente testato su 100 telecamere e poi annullato se si osserva un calo della precisione, prima di essere esteso a tutte le 5000 telecamere.\n\n\nDistribuzione del Modello\nNegli MLOps tradizionali, le nuove versioni del modello vengono distribuite direttamente sui server tramite endpoint API. Tuttavia, i dispositivi embedded richiedono meccanismi di distribuzione ottimizzati per ricevere modelli aggiornati. Gli aggiornamenti over-the-air (OTA) forniscono un approccio standardizzato alla distribuzione wireless di nuove versioni di software o firmware ai dispositivi embedded. Invece dell‚Äôaccesso API diretto, i pacchetti OTA consentono la distribuzione remota di modelli e dipendenze come bundle pre-costruiti. In alternativa, l‚Äôapprendimento federato consente aggiornamenti del modello senza accesso diretto ai dati di training grezzi. Questo approccio decentralizzato ha il potenziale per un miglioramento continuo del modello, ma necessita di piattaforme MLOps robuste.\nLa distribuzione del modello si basa su interfacce fisiche come connessioni seriali USB o UART per dispositivi profondamente embedded privi di connettivit√†. Il packaging del modello segue ancora principi simili agli aggiornamenti OTA, ma il meccanismo di distribuzione √® adattato alle capacit√† dell‚Äôhardware edge. Inoltre, spesso vengono utilizzati protocolli OTA specializzati ottimizzati per reti IoT anzich√© protocolli WiFi o Bluetooth standard. I fattori chiave includono efficienza, affidabilit√†, sicurezza e telemetria, come il monitoraggio dei progressi, soluzioni come Mender. Io fornisce servizi OTA incentrati su embedded che gestiscono aggiornamenti differenziali tra flotte di dispositivi.\nFigura¬†13.6 presenta una panoramica di ‚ÄúModel Lifecycle Management‚Äù in un contesto MLOps, illustrando il flusso dallo sviluppo (in alto a sinistra) alla distribuzione e al monitoraggio (in basso a destra). Il processo inizia con lo sviluppo ML, in cui il codice e le configurazioni sono ‚Äúversion-controlled‚Äù. La gestione dei dati e dei modelli √® fondamentale per il processo, coinvolgendo set di dati e repository di funzionalit√†. Training continuo, conversione del modello e registro del modello sono fasi chiave nell‚Äôoperazionalizzazione della training. La distribuzione del modello include la fornitura del modello e la gestione dei log di fornitura. Sono in atto meccanismi di allarme per segnalare i problemi, che alimentano il monitoraggio continuo per garantire le prestazioni e l‚Äôaffidabilit√† del modello nel tempo. Questo approccio integrato garantisce che i modelli siano sviluppati e mantenuti in modo efficace durante tutto il loro ciclo di vita.\n\n\n\n\n\n\nFigura¬†13.6: Gestione del ciclo di vita del modello. Fonte: HarvardX.\n\n\n\n\n\n\n13.6.2 Integrazione di Sviluppo e Operazioni\n\nPipeline CI/CD\nNelle MLOps tradizionali, una solida infrastruttura CI/CD come Jenkins e Kubernetes consente l‚Äôautomazione della pipeline per la distribuzione di modelli su larga scala. Tuttavia, le MLOps embedded necessitano di questa infrastruttura centralizzata e di flussi di lavoro CI/CD pi√π personalizzati per i dispositivi edge.\nLa creazione di pipeline CI/CD deve tenere conto di un panorama frammentato di diverse versioni hardware, firmware e vincoli di connettivit√†. Non esiste una piattaforma standard per orchestrare le pipeline e il supporto degli strumenti √® pi√π limitato.\nI test devono coprire in anticipo questo ampio spettro di dispositivi embedded target, il che √® difficile senza un accesso centralizzato. Le aziende devono investire molto nell‚Äôacquisizione e nella gestione dell‚Äôinfrastruttura di test nell‚Äôecosistema embedded eterogeneo.\nGli aggiornamenti over-the-air richiedono la configurazione di server specializzati per distribuire in modo sicuro i bundle di modelli ai dispositivi sul campo. Anche le procedure di rollout e rollback devono essere attentamente personalizzate per particolari famiglie di dispositivi.\nCon gli strumenti CI/CD tradizionali meno applicabili, le MLOps embedded si affidano maggiormente a script personalizzati e integrazione. Le aziende adottano approcci diversi, dai framework open source alle soluzioni completamente interne. Una stretta integrazione tra sviluppatori, ingegneri edge e clienti finali stabilisce processi di rilascio affidabili.\nPertanto, gli MLOps embedded non possono sfruttare l‚Äôinfrastruttura cloud centralizzata per CI/CD. Le aziende combinano pipeline personalizzate, infrastruttura di test e distribuzione OTA per distribuire modelli su sistemi edge frammentati e disconnessi.\n\n\nGestione dell‚ÄôInfrastruttura\nNei tradizionali MLOps centralizzati, l‚Äôinfrastruttura comporta l‚Äôapprovvigionamento di server cloud, GPU e reti ad alta larghezza di banda per carichi di lavoro intensivi come l‚Äôaddestramento di modelli e la fornitura di previsioni su larga scala. Tuttavia, gli MLOps embedded richiedono un‚Äôinfrastruttura pi√π eterogenea che si estende su dispositivi edge, gateway e cloud.\nI dispositivi edge come i sensori catturano e preelaborano i dati localmente prima della trasmissione intermittente per evitare di sovraccaricare le reti: i gateway aggregano ed elaborano i dati dei dispositivi prima di inviare sottoinsiemi selezionati al cloud per l‚Äôaddestramento e l‚Äôanalisi. Il cloud fornisce gestione centralizzata ed elaborazione supplementare.\nQuesta infrastruttura necessita di una stretta integrazione e bilanciamento dei carichi di elaborazione e comunicazione. La larghezza di banda di rete √® limitata, il che richiede un attento filtraggio e compressione dei dati. Le capacit√† di elaborazione edge sono modeste rispetto al cloud, imponendo vincoli di ottimizzazione.\nLa gestione di aggiornamenti OTA sicuri su grandi flotte di dispositivi presenta sfide all‚Äôedge. I rollout devono essere incrementali e pronti per il rollback per una rapida mitigazione. Dato l‚Äôambiente decentralizzato, l‚Äôaggiornamento dell‚Äôinfrastruttura edge richiede coordinamento.\nAd esempio, un impianto industriale pu√≤ eseguire l‚Äôelaborazione di base del segnale sui sensori prima di inviare i dati a un gateway on-prem. Il gateway gestisce l‚Äôaggregazione dei dati, il monitoraggio dell‚Äôinfrastruttura e gli aggiornamenti OTA. Solo i dati curati vengono trasmessi al cloud per analisi avanzate e riaddestramento del modello.\nMLOps embedded richiede una gestione olistica dell‚Äôinfrastruttura distribuita che abbraccia edge vincolato, gateway e cloud centralizzato. I carichi di lavoro sono bilanciati tra i livelli tenendo conto delle sfide di connettivit√†, elaborazione e sicurezza.\n\n\nComunicazione e Collaborazione\nNelle MLOps tradizionali, la collaborazione tende a concentrarsi su data scientist, ingegneri ML e team DevOps. Tuttavia, le MLOps embedded richiedono un coordinamento interfunzionale pi√π stretto tra ruoli aggiuntivi per affrontare i vincoli di sistema.\nGli ingegneri edge ottimizzano le architetture dei modelli per gli ambienti hardware target. Forniscono feedback ai data scientist durante lo sviluppo in modo che i modelli si adattino anticipatamente alle capacit√† dei dispositivi. Analogamente, i team di prodotto definiscono i requisiti operativi informati dai contesti degli utenti finali.\nCon pi√π stakeholder nell‚Äôecosistema embedded, i canali di comunicazione devono facilitare la condivisione delle informazioni tra team centralizzati e remoti. Il monitoraggio dei problemi e la gestione dei progetti garantiscono l‚Äôallineamento.\nGli strumenti collaborativi ottimizzano i modelli per dispositivi specifici. I data scientist possono registrare i problemi replicati dai dispositivi sul campo in modo che i modelli siano specializzati in dati di nicchia. L‚Äôaccesso remoto ai dispositivi facilita il debug e la raccolta dati.\nAd esempio, i data scientist possono collaborare con i team sul campo che gestiscono flotte di turbine eoliche per recuperare campioni di dati operativi. Questi dati vengono utilizzati per specializzare i modelli rilevando anomalie specifiche per quella classe di turbine. Gli aggiornamenti dei modelli vengono testati in simulazioni e rivisti dagli ingegneri prima dell‚Äôimplementazione sul campo.\nGli MLOps embedded impongono un coordinamento continuo tra data scientist, ingegneri, clienti finali e altre parti interessate durante l‚Äôintero ciclo di vita del ML. Grazie a una stretta collaborazione, i modelli possono essere personalizzati e ottimizzati per i dispositivi edge mirati.\n\n\n\n13.6.3 Eccellenza operativa\n\nMonitoraggio\nIl monitoraggio MLOps tradizionale si concentra sul monitoraggio centralizzato dell‚Äôaccuratezza del modello, delle metriche delle prestazioni e della deriva dei dati. Tuttavia, MLOps embedded deve tenere conto del monitoraggio decentralizzato su diversi dispositivi e ambienti edge.\nI dispositivi edge richiedono una raccolta dati ottimizzata per trasmettere metriche di monitoraggio chiave senza sovraccaricare le reti. Le metriche aiutano a valutare le prestazioni del modello, i pattern di dati, l‚Äôutilizzo delle risorse e altri comportamenti sui dispositivi remoti.\nCon una connettivit√† limitata, vengono eseguite pi√π analisi all‚Äôedge prima di aggregare le informazioni centralmente. I gateway svolgono un ruolo chiave nel monitoraggio dello stato di salute della flotta e nel coordinamento degli aggiornamenti software. Gli indicatori confermati vengono infine propagati al cloud.\nUn‚Äôampia copertura dei dispositivi √® impegnativa ma critica. Possono sorgere problemi specifici per determinati tipi di dispositivi, quindi il monitoraggio deve coprire l‚Äôintero spettro. Le distribuzioni ‚Äúcanary‚Äù aiutano a testare i processi di monitoraggio prima del ridimensionamento.\nIl rilevamento delle anomalie identifica gli incidenti che richiedono il rollback dei modelli o la riqualificazione su nuovi dati. Tuttavia, l‚Äôinterpretazione degli allarmi richiede la comprensione dei contesti dei dispositivi univoci in base all‚Äôinput di ingegneri e clienti.\nAd esempio, una casa automobilistica pu√≤ monitorare i veicoli autonomi per gli indicatori di degradazione del modello utilizzando la memorizzazione nella cache, l‚Äôaggregazione e i flussi in tempo reale. Gli ingegneri valutano quando le anomalie identificate garantiscono gli aggiornamenti OTA per migliorare i modelli in base a fattori come la posizione e l‚Äôet√† del veicolo.\nIl monitoraggio MLOps embedded fornisce osservabilit√† nelle prestazioni del modello e del sistema in ambienti edge decentralizzati. Un‚Äôattenta raccolta, analisi e collaborazione dei dati fornisce informazioni significative per mantenere l‚Äôaffidabilit√†.\n\n\nGovernance\nNelle MLOps tradizionali, la governance si concentra sulla spiegabilit√† del modello, la correttezza e la conformit√† per i sistemi centralizzati. Tuttavia, le MLOps embedded devono anche affrontare le sfide di governance a livello di dispositivo relative alla privacy dei dati, alla sicurezza e alla protezione.\nCon i sensori che raccolgono dati personali e sensibili, la governance dei dati locali sui dispositivi √® fondamentale. I controlli di accesso ai dati, l‚Äôanonimizzazione e la memorizzazione nella cache crittografata aiutano ad affrontare i rischi per la privacy e la conformit√† come HIPAA e GDPR. Gli aggiornamenti devono mantenere patch e impostazioni di sicurezza.\nLa governance della sicurezza considera gli impatti fisici del comportamento difettoso del dispositivo. I guasti potrebbero causare condizioni non sicure in veicoli, fabbriche e sistemi critici. Ridondanza, sistemi di sicurezza e sistemi di allarme aiutano a mitigare i rischi.\nLa governance tradizionale, come il monitoraggio dei bias e la spiegabilit√† del modello, rimane imperativa ma √® pi√π difficile da implementare per l‚Äôintelligenza artificiale embedded. Anche dare un‚Äôocchiata ai modelli black-box su dispositivi a basso consumo pone delle sfide.\nAd esempio, un dispositivo medico pu√≤ cancellare i dati personali sul dispositivo prima della trasmissione. I rigidi protocolli di governance dei dati approvano gli aggiornamenti del modello. La spiegabilit√† del modello √® limitata, ma l‚Äôattenzione √® rivolta al rilevamento di comportamenti anomali. I sistemi di backup prevengono i guasti.\nLa governance MLOps embedded deve comprendere privacy, sicurezza, protezione, trasparenza ed etica. Sono necessarie tecniche specializzate e collaborazione di squadra per aiutare a stabilire fiducia e responsabilit√† all‚Äôinterno di ambienti decentralizzati.\n\n\n\n13.6.4 Confronto\nTabella¬†13.2 evidenzia le somiglianze e le differenze tra MLOps Tradizionali e MLOps Embedded sulla base di tutto ci√≤ che abbiamo imparato finora:\n\n\n\nTabella¬†13.2: Confronto tra le pratiche MLOps Tradizionali e quelle MLOps Embedded.\n\n\n\n\n\n\n\n\n\n\nArea\nMLOps Tradizionali\nMLOps Embedded\n\n\n\n\nGestione dei Dati\nGrandi set di dati, data lake, feature store\nAcquisizione dati sul dispositivo, edge caching ed elaborazione\n\n\nSviluppo del Modello\nSfrutta il deep learning, reti neurali complesse, addestramento GPU\nVincoli sulla complessit√† del modello, necessit√† di ottimizzazione\n\n\nDistribuzione\nCluster di server, distribuzione cloud, bassa latenza su larga scala\nDistribuzione OTA su dispositivi, connettivit√† intermittente\n\n\nMonitoraggio\nDashboard, log, allarmi per le prestazioni del modello cloud\nMonitoraggio sul dispositivo di previsioni, utilizzo delle risorse\n\n\nRiqualificazione\nRi-addestramento dei modelli su nuovi dati\nApprendimento federato da dispositivi, ri-addestramento edge\n\n\nInfrastruttura\nInfrastruttura cloud dinamica\nInfrastruttura edge/cloud eterogenea\n\n\nCollaborazione\nMonitoraggio degli esperimenti condivisi e registro dei modelli\nCollaborazione per l‚Äôottimizzazione specifica del dispositivo\n\n\n\n\n\n\nQuindi, mentre Embedded MLOps condivide i principi fondamentali di MLOps, si trova ad affrontare vincoli unici nell‚Äôadattare flussi di lavoro e infrastrutture specificamente per dispositivi edge con risorse limitate.\n\n\n13.6.5 Servizi MLOps Embedded\nNonostante la proliferazione di nuovi strumenti MLOps in risposta all‚Äôaumento della domanda, le sfide descritte in precedenza hanno limitato la disponibilit√† di tali strumenti negli ambienti dei sistemi embedded. Pi√π di recente, nuovi strumenti come Edge Impulse (Janapa Reddi et al. 2023) hanno reso il processo di sviluppo un po‚Äô pi√π semplice, come descritto di seguito.\n\nJanapa Reddi, Vijay, Alexander Elium, Shawn Hymel, David Tischler, Daniel Situnayake, Carl Ward, Louis Moreau, et al. 2023. ¬´Edge Impulse: An MLOps Platform for Tiny Machine Learning¬ª. Proceedings of Machine Learning and Systems 5.\n\nEdge Impulse\nEdge Impulse √® una piattaforma di sviluppo end-to-end per la creazione e l‚Äôimplementazione di modelli di apprendimento automatico su dispositivi edge come microcontrollori e piccoli processori. Rende l‚Äôapprendimento automatico embedded pi√π accessibile agli sviluppatori di software attraverso la sua interfaccia web di facile utilizzo e strumenti integrati per la raccolta dati, lo sviluppo di modelli, l‚Äôottimizzazione e l‚Äôimplementazione. Le sue funzionalit√† principali includono quanto segue:\n\nFlusso di lavoro intuitivo drag-and-drop per la creazione di modelli ML senza bisogno di codifica\nStrumenti per l‚Äôacquisizione, l‚Äôetichettatura, la visualizzazione e la preelaborazione dei dati dai sensori\nScelta di architetture di modelli, tra cui reti neurali e apprendimento non supervisionato\nTecniche di ottimizzazione dei modelli per bilanciare metriche delle prestazioni e vincoli hardware\nDistribuzione senza soluzione di continuit√† su dispositivi edge tramite compilazione, SDK e benchmark\nFunzionalit√† di collaborazione per team e integrazione con altre piattaforme\n\nEdge Impulse offre una soluzione completa per la creazione di intelligenza embedded e l‚Äôavanzamento dell‚Äôapprendimento automatico, in particolare per gli sviluppatori con competenze limitate in scienza dei dati. Questa piattaforma consente lo sviluppo di modelli ML specializzati che funzionano in modo efficiente in piccoli ambienti di elaborazione. Come illustrato in Figura¬†13.7, Edge Impulse facilita il percorso dalla raccolta dati alla distribuzione del modello, evidenziando la sua interfaccia intuitiva e gli strumenti che semplificano la creazione di soluzioni ML incorporate, rendendole cos√¨ accessibili a una gamma pi√π ampia di sviluppatori e applicazioni.\n\n\n\n\n\n\nFigura¬†13.7: Panoramica di Edge Impulse. Fonte: Edge Impulse\n\n\n\n\nInterfaccia utente\nEdge Impulse √® stato progettato con sette principi chiave: accessibilit√†, funzionalit√† end-to-end, un approccio incentrato sui dati, interattivit√†, estensibilit√†, orientamento al team e supporto della community. L‚Äôinterfaccia utente intuitiva, mostrata in Figura¬†13.8, guida gli sviluppatori di tutti i livelli di esperienza attraverso il caricamento dei dati, la selezione di un‚Äôarchitettura di modello, l‚Äôaddestramento del modello e la sua distribuzione su piattaforme hardware pertinenti. Va notato che, come qualsiasi strumento, Edge Impulse √® destinato ad assistere, non a sostituire, le considerazioni fondamentali come la determinazione se ML √® una soluzione appropriata o l‚Äôacquisizione delle competenze di dominio richieste per una determinata applicazione.\n\n\n\n\n\n\nFigura¬†13.8: Schermata dell‚Äôinterfaccia utente di Edge Impulse per la creazione di flussi di lavoro dai dati di input alle funzionalit√† di output.\n\n\n\nCi√≤ che rende Edge Impulse degno di nota √® il suo flusso di lavoro end-to-end completo ma intuitivo. Gli sviluppatori iniziano caricando i propri dati tramite l‚Äôinterfaccia utente grafica (GUI) o gli strumenti dell‚Äôinterfaccia a riga di comando (CLI), dopodich√© possono esaminare campioni grezzi e visualizzare la distribuzione dei dati nelle suddivisioni di addestramento e test. Successivamente, gli utenti possono scegliere tra vari ‚Äúblocchi‚Äù di pre-elaborazione per facilitare l‚Äôelaborazione del segnale digitale (DSP). Mentre vengono forniti valori di parametri predefiniti, gli utenti possono personalizzare i parametri in base alle proprie esigenze, osservando le considerazioni su memoria e la latenza visualizzate. Gli utenti possono scegliere facilmente la propria architettura di rete neurale, senza bisogno di alcun codice.\nGrazie all‚Äôeditor visivo della piattaforma, gli utenti possono personalizzare i componenti dell‚Äôarchitettura e i parametri specifici, assicurandosi al contempo che il modello sia ancora addestrabile. Gli utenti possono anche sfruttare algoritmi di apprendimento non supervisionato, come il clustering K-means e i Gaussian Mixture Model (GMM).\n\n\nOttimizzazioni\nPer adattarsi ai vincoli di risorse delle applicazioni TinyML, Edge Impulse fornisce una ‚Äúmatrice di confusione‚Äù che riassume le metriche chiave delle prestazioni, tra cui accuratezza per classe e punteggi F1. La piattaforma chiarisce i compromessi tra prestazioni del modello, dimensioni e latenza utilizzando simulazioni in Renode e benchmarking specifici del dispositivo. Per i casi di utilizzo dei dati in streaming, uno strumento di calibrazione delle prestazioni sfrutta un algoritmo genetico per trovare configurazioni di post-elaborazione ideali che bilanciano tassi di falsa accettazione e falso rifiuto. Sono disponibili tecniche come quantizzazione, ottimizzazione del codice e ottimizzazione specifica del dispositivo per i modelli. Per la distribuzione, i modelli possono essere compilati in formati appropriati per i dispositivi edge target. Gli SDK del firmware nativi consentono anche la raccolta diretta dei dati sui dispositivi.\nOltre a semplificare lo sviluppo, Edge Impulse ridimensiona il processo di modellazione stesso. Una funzionalit√† chiave √® EON Tuner, uno strumento di apprendimento automatico automatico (AutoML) che assiste gli utenti nell‚Äôottimizzazione degli iperparametri in base ai vincoli di sistema. Esegue una ricerca casuale per generare rapidamente configurazioni per l‚Äôelaborazione del segnale digitale e le fasi di training. I modelli risultanti vengono visualizzati affinch√© l‚Äôutente possa selezionarli in base a metriche di prestazioni, memoria e latenza pertinenti. Per i dati, l‚Äôapprendimento attivo facilita il training su un piccolo sottoinsieme etichettato, seguito dall‚Äôetichettatura manuale o automatica di nuovi campioni in base alla vicinanza alle classi esistenti. Ci√≤ espande l‚Äôefficienza dei dati.\n\n\nCasi d‚ÄôUso\nOltre all‚Äôaccessibilit√† della piattaforma stessa, il team di Edge Impulse ha ampliato la base di conoscenza dell‚Äôecosistema ML embedded. La piattaforma si presta ad ambienti accademici, essendo stata utilizzata in corsi online e workshop in loco a livello globale. Sono stati pubblicati numerosi casi di studio con casi d‚Äôuso di settore e di ricerca, in particolare Oura Ring, che utilizza ML per identificare i pattern del sonno. Il team ha reso i repository open source su GitHub, facilitando la crescita della comunit√†. Gli utenti possono anche rendere pubblici i progetti per condividere tecniche e scaricare librerie da condividere tramite Apache. L‚Äôaccesso a livello di organizzazione consente la collaborazione sui flussi di lavoro.\nNel complesso, Edge Impulse √® straordinariamente completo e integrabile per i flussi di lavoro degli sviluppatori. Piattaforme pi√π grandi come Google e Microsoft si concentrano maggiormente sul cloud rispetto ai sistemi embedded. I framework TinyMLOps come Neuton AI e Latent AI offrono alcune funzionalit√† ma non hanno le capacit√† end-to-end di Edge Impulse. TensorFlow Lite Micro √® il motore di inferenza standard grazie alla flessibilit√†, allo stato open source e all‚Äôintegrazione di TensorFlow, ma utilizza pi√π memoria e storage rispetto al compilatore EON di Edge Impulse. Altre piattaforme devono essere aggiornate, focalizzate sull‚Äôaspetto accademico o pi√π versatili. In sintesi, Edge Impulse semplifica e amplia l‚Äôapprendimento automatico embedded tramite una piattaforma accessibile e automatizzata.\n\n\n\nLimitazioni\nSebbene Edge Impulse fornisca una pipeline accessibile per ML embedded, permangono importanti limitazioni e rischi. Una sfida fondamentale √® la qualit√† e la disponibilit√† dei dati: i modelli sono validi solo quanto i dati utilizzati per addestrarli. Gli utenti devono disporre di campioni etichettati sufficienti che catturino l‚Äôampiezza delle condizioni operative previste e delle modalit√† di errore. Le anomalie e i valori anomali etichettati sono critici, ma richiedono molto tempo per essere raccolti e identificati. Dati insufficienti o distorti comportano scarse prestazioni del modello indipendentemente dalle capacit√† dello strumento.\nAnche il deploying su dispositivi a bassa potenza presenta sfide intrinseche. I modelli ottimizzati potrebbero comunque dover richiedere pi√π risorse per MCU a bassissimo consumo. Trovare il giusto equilibrio tra compressione e accuratezza richiede un po‚Äô di sperimentazione. Lo strumento semplifica, ma deve comunque eliminare la necessit√† di competenze di base in ML ed elaborazione del segnale. Gli ambienti embedded limitano anche il debug e l‚Äôinterpretabilit√† rispetto al cloud.\nSebbene siano ottenibili risultati impressionanti, gli utenti non dovrebbero considerare Edge Impulse come una soluzione ‚ÄúPush Button ML‚Äù. Un‚Äôattenta definizione dell‚Äôambito del progetto, la raccolta dati, la valutazione del modello e il test sono comunque essenziali. Come con qualsiasi strumento di sviluppo, si consigliano aspettative ragionevoli e diligenza nell‚Äôapplicazione. Tuttavia, Edge Impulse pu√≤ accelerare la prototipazione e l‚Äôimplementazione di ML embedded per gli sviluppatori disposti a investire lo sforzo di data science e ingegneria richiesto.\n\n\n\n\n\n\nEsercizio¬†13.1: Edge Impulse\n\n\n\n\n\nPronti a far salire di livello i vostri piccoli progetti di machine-learning? Combiniamo la potenza di Edge Impulse con le fantastiche visualizzazioni di Weights & Biases (WandB). In questo Colab, si imparer√† a monitorare i progressi del training del modello come un professionista! Si immagini di vedere fantastici grafici del modello che diventa pi√π intelligente, confrontando diverse versioni e assicurandovi che la vostra IA funzioni al meglio anche su dispositivi minuscoli.",
    "crumbs": [
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Operazioni di ML</span>"
    ]
  },
  {
    "objectID": "contents/core/ops/ops.it.html#casi-di-studio",
    "href": "contents/core/ops/ops.it.html#casi-di-studio",
    "title": "13¬† Operazioni di ML",
    "section": "13.7 Casi di Studio",
    "text": "13.7 Casi di Studio\n\n13.7.1 Oura Ring\nOura Ring √® un dispositivo indossabile che pu√≤ misurare l‚Äôattivit√†, il sonno e il recupero quando viene posizionato sul dito dell‚Äôutente. Utilizzando sensori per tracciare le metriche fisiologiche, il dispositivo utilizza ML embedded per prevedere le fasi del sonno. Per stabilire una base di legittimit√† nel settore, Oura ha condotto un esperimento di correlazione per valutare il successo del dispositivo nel prevedere le fasi del sonno rispetto a uno studio di base. Ci√≤ ha portato a una solida correlazione del 62% rispetto alla base di riferimento dell‚Äô82-83%. Pertanto, il team ha deciso di determinare come migliorare ulteriormente le proprie prestazioni.\nLa prima sfida √® stata ottenere dati migliori in termini sia di quantit√† che di qualit√†. Avrebbero potuto ospitare uno studio pi√π ampio per ottenere un set di dati pi√π completo, ma i dati sarebbero stati cos√¨ rumorosi e grandi che sarebbe stato difficile aggregarli, ripulirli e analizzarli. √à qui che entra in gioco Edge Impulse.\nAbbiamo condotto un massiccio studio sul sonno su 100 uomini e donne di et√† compresa tra 15 e 73 anni in tre continenti (Asia, Europa e Nord America). Oltre a indossare l‚ÄôOura Ring, i partecipanti erano tenuti a sottoporsi al test PSG [https://it.wikipedia.org/wiki/Polisonnografia] standard del settore, che ha fornito una ‚Äúetichetta‚Äù per questo set di dati. Con 440 notti di sonno da parte di 106 partecipanti, il set di dati ha totalizzato 3.444 ore di lunghezza tra dati Ring e PSG. Con Edge Impulse, Oura ha potuto caricare e consolidare facilmente i dati da diverse fonti in un bucket S3 privato. Sono stati anche in grado di impostare una Data Pipeline per unire campioni di dati in file individuali e preelaborare i dati senza dover eseguire lo ‚Äúscrubbing‚Äù [pulizia] manuale.\nCol tempo risparmiato nell‚Äôelaborazione dei dati grazie a Edge Impulse, il team Oura ha potuto concentrarsi sui driver chiave della propria previsione. Hanno estratto solo tre tipi di dati dei sensori: frequenza cardiaca, movimento e temperatura corporea. Dopo aver suddiviso i dati utilizzando la validazione incrociata a cinque livelli e classificato le fasi del sonno, il team ha ottenuto una correlazione del 79%, solo pochi punti percentuali in meno rispetto allo standard. Hanno prontamente distribuito due tipi di modelli di rilevamento del sonno: uno semplificato utilizzando solo l‚Äôaccelerometro dell‚Äôanello e uno pi√π completo sfruttando i segnali periferici mediati dal Autonomic Nervous System (ANS) [sistema nervoso autonomo] e le caratteristiche circadiane [ritmo cardiaco in 24 ore]. Con Edge Impulse, hanno in programma di condurre ulteriori analisi di diversi tipi di attivit√† e sfruttare la scalabilit√† della piattaforma per continuare a sperimentare con diverse fonti di dati e sottoinsiemi di caratteristiche estratte.\nMentre la maggior parte della ricerca in ambito ML si concentra su fasi dominate dal modello, come il training e la messa a punto, questo caso di studio sottolinea l‚Äôimportanza di un approccio olistico alle MLOps, in cui anche le fasi iniziali di aggregazione e pre-elaborazione dei dati hanno un impatto fondamentale sui risultati positivi.\n\n\n13.7.2 ClinAIOps\nDiamo un‚Äôocchiata a MLOps nel contesto del monitoraggio medico sanitario per comprendere meglio come MLOps ‚Äúmaturi‚Äù in un‚Äôimplementazione nel mondo reale. In particolare, prendiamo in considerazione il continuous therapeutic monitoring (CTM) [monitoraggio terapeutico continuo] abilitato da dispositivi e sensori indossabili. Il CTM cattura dati fisiologici dettagliati dai pazienti, offrendo l‚Äôopportunit√† di aggiustamenti pi√π frequenti e personalizzati ai trattamenti.\nI sensori indossabili abilitati per ML consentono un monitoraggio continuo fisiologico e dell‚Äôattivit√† al di fuori delle cliniche, aprendo possibilit√† per aggiustamenti terapeutici tempestivi e basati sui dati. Ad esempio, i biosensori indossabili per l‚Äôinsulina (Psoma e Kanthou 2023) e i sensori ECG da polso per il monitoraggio del glucosio (J. Li et al. 2021) possono automatizzare il dosaggio di insulina per il diabete, i sensori ECG e PPG da polso possono regolare gli anticoagulanti in base ai pattern di fibrillazione atriale (Attia et al. 2018; Guo et al. 2019), e gli accelerometri che tracciano l‚Äôandatura possono innescare cure preventive per la mobilit√† in declino negli anziani (Liu et al. 2022). La variet√† di segnali che ora possono essere catturati passivamente e continuamente consente la titolazione e l‚Äôottimizzazione della terapia su misura per le mutevoli esigenze di ogni paziente. Chiudendo il cerchio tra rilevamento fisiologico e risposta terapeutica con TinyML e apprendimento sul dispositivo, i dispositivi indossabili sono pronti a trasformare molte aree della medicina personalizzata.\n\nPsoma, Sotiria D., e Chryso Kanthou. 2023. ¬´Wearable Insulin Biosensors for Diabetes Management: Advances and Challenges¬ª. Biosensors 13 (7): 719. https://doi.org/10.3390/bios13070719.\n\nLi, Jingzhen, Igbe Tobore, Yuhang Liu, Abhishek Kandwal, Lei Wang, e Zedong Nie. 2021. ¬´Non-invasive Monitoring of Three Glucose Ranges Based On ECG By Using DBSCAN-CNN¬ª. IEEE Journal of Biomedical and Health Informatics 25 (9): 3340‚Äì50. https://doi.org/10.1109/jbhi.2021.3072628.\n\nAttia, Zachi I., Alan Sugrue, Samuel J. Asirvatham, Michael J. Ackerman, Suraj Kapa, Paul A. Friedman, e Peter A. Noseworthy. 2018. ¬´Noninvasive assessment of dofetilide plasma concentration using a deep learning (neural network) analysis of the surface electrocardiogram: A proof of concept study¬ª. PLOS ONE 13 (8): e0201059. https://doi.org/10.1371/journal.pone.0201059.\n\nGuo, Yutao, Hao Wang, Hui Zhang, Tong Liu, Zhaoguang Liang, Yunlong Xia, Li Yan, et al. 2019. ¬´Mobile Photoplethysmographic Technology to Detect Atrial Fibrillation¬ª. Journal of the American College of Cardiology 74 (19): 2365‚Äì75. https://doi.org/10.1016/j.jacc.2019.08.019.\n\nLiu, Yingcheng, Guo Zhang, Christopher G. Tarolli, Rumen Hristov, Stella Jensen-Roberts, Emma M. Waddell, Taylor L. Myers, et al. 2022. ¬´Monitoring gait at home with radio waves in Parkinson‚Äôs disease: A marker of severity, progression, and medication response¬ª. Science Translational Medicine 14 (663): eadc9669. https://doi.org/10.1126/scitranslmed.adc9669.\nIl ML √® molto promettente nell‚Äôanalisi dei dati CTM per fornire raccomandazioni basate sui dati per gli aggiustamenti della terapia. Ma semplicemente distribuire modelli di intelligenza artificiale in ‚Äúsilos‚Äù, senza integrarli correttamente nei flussi di lavoro clinici e nel processo decisionale, pu√≤ portare a una scarsa adozione o a risultati non ottimali. In altre parole, pensare solo a MLOps non √® sufficiente per renderli utili nella pratica. Questo studio dimostra che sono necessari framework per incorporare intelligenza artificiale e CTM nella pratica clinica reale senza soluzione di continuit√†.\nQuesto caso di studio analizza ‚ÄúClinAIOps‚Äù come modello per operazioni ML embedded in ambienti clinici complessi (Chen et al. 2023). Forniamo una panoramica del framework e del motivo per cui √® necessario, esaminiamo un esempio di applicazione e discutiamo le principali sfide di implementazione relative al monitoraggio del modello, all‚Äôintegrazione del flusso di lavoro e agli incentivi per gli stakeholder. L‚Äôanalisi di esempi concreti come ClinAIOps illumina principi cruciali e best practice per operazioni AI affidabili ed efficaci in molti domini.\nI framework MLOps tradizionali non sono sufficienti per integrare il monitoraggio terapeutico continuo (CTM) e l‚ÄôIA in contesti clinici per alcuni motivi chiave:\n\nMLOps si concentra sul ciclo di vita del modello ML: training, distribuzione, monitoraggio. Ma l‚Äôassistenza sanitaria implica il coordinamento di pi√π stakeholder umani, pazienti e medici, non solo modelli.\nMLOps automatizza il monitoraggio e la gestione dei sistemi IT. Tuttavia, l‚Äôottimizzazione della salute del paziente richiede cure personalizzate e supervisione umana, non solo automazione.\nCTM e l‚Äôerogazione dell‚Äôassistenza sanitaria sono sistemi sociotecnici complessi con molte parti mobili. MLOps non fornisce un framework per coordinare il processo decisionale umano e AI.\nLe considerazioni etiche relative all‚ÄôAI sanitaria richiedono giudizio umano, supervisione e responsabilit√†. I framework MLOps non hanno processi per la supervisione etica.\nI dati sanitari dei pazienti sono altamente sensibili e regolamentati. MLOps da solo non garantisce la gestione delle informazioni sanitarie protette secondo gli standard normativi e di privacy.\nLa convalida clinica dei piani di trattamento guidati dall‚ÄôAI √® essenziale per l‚Äôadozione da parte del provider. MLOps non incorpora la valutazione specifica del dominio delle raccomandazioni del modello.\nL‚Äôottimizzazione delle metriche sanitarie come i risultati dei pazienti richiede l‚Äôallineamento degli incentivi e dei flussi di lavoro delle parti interessate, che MLOps puramente incentrato sulla tecnologia trascura.\n\nPertanto, integrare efficacemente AI/ML e CTM nella pratica clinica richiede pi√π di semplici modelli e pipeline di dati; richiede il coordinamento di complessi processi decisionali collaborativi tra esseri umani e AI, che ClinAIOps affronta tramite i suoi cicli di feedback multi-stakeholder.\n\nCicli di Feedback\nIl framework ClinAIOps, mostrato in Figura¬†13.9, fornisce questi meccanismi attraverso tre cicli di feedback. I cicli sono utili per coordinare le informazioni dal monitoraggio fisiologico continuo, l‚Äôesperienza del medico e la guida dell‚ÄôIA tramite cicli di feedback, consentendo una medicina di precisione basata sui dati mantenendo al contempo la responsabilit√† umana. ClinAIOps fornisce un modello per un‚Äôefficace simbiosi uomo-IA nell‚Äôassistenza sanitaria: il paziente √® al centro, fornendo sfide e obiettivi sanitari che informano il regime terapeutico; il medico supervisiona questo regime, fornendo input per gli aggiustamenti basati sui dati di monitoraggio continuo e sui report sanitari del paziente; mentre gli sviluppatori di IA svolgono un ruolo cruciale creando sistemi che generano allarmi per gli aggiornamenti della terapia, che il medico quindi esamina.\nQuesti cicli di feedback, di cui parleremo di seguito, aiutano a mantenere la responsabilit√† e il controllo del medico sui piani di trattamento esaminando i suggerimenti dell‚ÄôIA prima che abbiano un impatto sui pazienti. Aiutano a personalizzare dinamicamente il comportamento e gli output del modello di IA in base allo stato di salute mutevole di ciascun paziente. Contribuiscono a migliorare l‚Äôaccuratezza del modello e l‚Äôutilit√† clinica nel tempo, imparando dalle risposte del medico e del paziente. Facilitano il processo decisionale condiviso e l‚Äôassistenza personalizzata durante le interazioni paziente-medico. Consentono una rapida ottimizzazione delle terapie in base a dati frequenti del paziente che i medici non possono analizzare manualmente.\n\n\n\n\n\n\nFigura¬†13.9: Ciclo ClinAIOps. Fonte: Chen et al. (2023).\n\n\n\n\nCiclo Paziente-IA\nIl ciclo paziente-IA consente un‚Äôottimizzazione frequente della terapia guidata dal monitoraggio fisiologico continuo. Ai pazienti vengono prescritti dispositivi indossabili come smartwatch o cerotti cutanei per raccogliere passivamente segnali sanitari rilevanti. Ad esempio, un paziente diabetico potrebbe avere un monitoraggio continuo del glucosio o un paziente con malattie cardiache potrebbe indossare un cerotto ECG. Un modello di IA analizza i flussi di dati sanitari longitudinali del paziente nel contesto delle sue cartelle cliniche elettroniche: diagnosi, esami di laboratorio, farmaci e dati demografici. Il modello di IA suggerisce modifiche al regime di trattamento su misura per quell‚Äôindividuo, come la modifica di una dose di farmaco o di un programma di somministrazione. Piccole modifiche entro un intervallo di sicurezza pre-approvato possono essere apportate dal paziente in modo indipendente, mentre le modifiche pi√π importanti vengono prima esaminate dal medico. Questo stretto feedback tra la fisiologia del paziente e la terapia guidata dall‚ÄôIA consente ottimizzazioni tempestive basate sui dati come raccomandazioni automatizzate sul dosaggio di insulina basate sui livelli di glucosio in tempo reale per i pazienti diabetici.\n\n\nCiclo Clinico-IA\nIl ciclo clinico-IA consente la supervisione clinica sulle raccomandazioni generate dall‚ÄôIA per garantire sicurezza e responsabilit√†. Il modello di IA fornisce al medico raccomandazioni terapeutiche e riepiloghi facilmente esaminabili dei dati rilevanti del paziente su cui si basano i suggerimenti. Ad esempio, un‚ÄôIA pu√≤ suggerire di ridurre la dose di farmaci per la pressione sanguigna di un paziente iperteso in base a letture costantemente basse. Il medico pu√≤ accettare, rifiutare o modificare le modifiche alla prescrizione proposte dall‚ÄôIA. Questo feedback del medico addestra e migliora ulteriormente il modello. Inoltre, il medico stabilisce i limiti per i tipi e l‚Äôentit√† delle modifiche al trattamento che l‚ÄôIA pu√≤ raccomandare autonomamente ai pazienti. Esaminando i suggerimenti dell‚ÄôIA, il medico mantiene l‚Äôautorit√† di trattamento finale in base al proprio giudizio clinico e alla propria responsabilit√†. Questo ciclo consente loro di supervisionare i casi dei pazienti con l‚Äôassistenza dell‚ÄôIA in modo efficiente.\n\n\nCiclo Paziente-Clinico\nInvece di una raccolta dati di routine, il medico pu√≤ concentrarsi sull‚Äôinterpretazione di pattern di dati di alto livello e sulla collaborazione con il paziente per stabilire obiettivi e priorit√† di salute. L‚Äôassistenza AI liberer√† anche il tempo dei medici, consentendo loro di concentrarsi maggiormente sull‚Äôascolto delle storie e delle preoccupazioni dei pazienti. Ad esempio, il medico pu√≤ discutere di cambiamenti di dieta ed esercizio fisico con un paziente diabetico per migliorare il controllo del glucosio in base ai dati di monitoraggio continuo. La frequenza degli appuntamenti pu√≤ anche essere regolata dinamicamente in base ai progressi del paziente anzich√© seguire un calendario fisso. Liberato dalla raccolta di dati di base, il medico pu√≤ fornire ‚Äúcoaching‚Äù e cure personalizzate a ciascun paziente informato dai suoi dati sanitari continui. La relazione paziente-medico diventa pi√π produttiva e personalizzata.\n\n\n\nEsempio di Ipertensione\nConsideriamo un esempio. Secondo i ‚ÄúCenters for Disease Control and Prevention‚Äù, quasi la met√† degli adulti soffre di ipertensione (48.1%, 119.9 milioni). L‚Äôipertensione pu√≤ essere gestita tramite ClinAIOps con l‚Äôaiuto di sensori indossabili utilizzando il seguente approccio:\n\nRaccolta Dati\nI dati raccolti includerebbero il monitoraggio continuo della pressione sanguigna tramite un dispositivo indossato al polso dotato di sensori per fotopletismografia (PPG) ed elettrocardiografia (ECG) per stimare la pressione sanguigna (Q. Zhang, Zhou, e Zeng 2017). Il dispositivo indossabile monitorerebbe anche l‚Äôattivit√† fisica del paziente tramite accelerometri embedded. Il paziente registrerebbe tutti i farmaci antipertensivi assunti, insieme all‚Äôora e alla dose. Verrebbero inoltre incorporati i dettagli demografici e la storia clinica del paziente dalla sua cartella clinica elettronica (EHR). Questi dati multimodali del mondo reale forniscono un contesto prezioso al modello di intelligenza artificiale per analizzare i pattern di pressione sanguigna del paziente, i livelli di attivit√†, l‚Äôaderenza ai farmaci e le risposte alla terapia.\n\nZhang, Qingxue, Dian Zhou, e Xuan Zeng. 2017. ¬´Highly wearable cuff-less blood pressure and heart rate monitoring with single-arm electrocardiogram and photoplethysmogram signals¬ª. BioMedical Engineering OnLine 16 (1): 23. https://doi.org/10.1186/s12938-017-0317-z.\n\n\nModello di Intelligenza Artificiale\nIl modello di intelligenza artificiale sul dispositivo analizzerebbe le tendenze continue della pressione sanguigna del paziente, i pattern circadiani, i livelli di attivit√† fisica, i comportamenti di aderenza ai farmaci e altri contesti. Utilizzerebbe ML per prevedere dosi ottimali di farmaci antipertensivi e tempi per controllare la pressione sanguigna dell‚Äôindividuo. Il modello invierebbe raccomandazioni di modifica del dosaggio direttamente al paziente per piccoli aggiustamenti o al medico revisore per l‚Äôapprovazione per modifiche pi√π significative. Osservando il feedback clinico sulle sue raccomandazioni e valutando i risultati ottenuti sulla pressione sanguigna nei pazienti, il modello di intelligenza artificiale potrebbe essere continuamente riqualificato per migliorarne le prestazioni. L‚Äôobiettivo √® una gestione della pressione sanguigna completamente personalizzata ottimizzata per le esigenze e le risposte di ciascun paziente.\n\n\nCiclo Paziente-IA\nNel ciclo Paziente-IA, il paziente iperteso riceverebbe notifiche sul suo dispositivo indossabile o sull‚Äôapp per smartphone collegata che raccomandano modifiche ai suoi farmaci antipertensivi. Per piccole modifiche della dose entro un intervallo di sicurezza predefinito, il paziente potrebbe implementare in modo indipendente la modifica suggerita dal modello di IA al suo regime. Tuttavia, il paziente deve ottenere l‚Äôapprovazione del medico prima di modificare il dosaggio per modifiche pi√π significative. Fornire raccomandazioni personalizzate e tempestive sui farmaci automatizza un elemento di autogestione dell‚Äôipertensione per il paziente. Pu√≤ migliorare la sua aderenza al regime e i risultati del trattamento. Il paziente √® autorizzato a sfruttare le informazioni dell‚ÄôIA per controllare meglio la sua pressione sanguigna.\n\n\nCiclo Clinico-IA\nNel ciclo Clinico-IA, il fornitore riceverebbe riepiloghi delle tendenze continue della pressione sanguigna del paziente e visualizzazioni dei suoi pattern di assunzione dei farmaci e dell‚Äôaderenza. Esaminano le modifiche al dosaggio antipertensivo suggerite dal modello AI e decidono se approvare, rifiutare o modificare le raccomandazioni prima che raggiungano il paziente. Il medico specifica anche i limiti di quanto l‚ÄôAI pu√≤ raccomandare in modo indipendente di modificare i dosaggi senza la supervisione del medico. Se la pressione sanguigna del paziente tende a livelli pericolosi, il sistema avvisa il medico in modo che possa intervenire tempestivamente e modificare i farmaci o richiedere una visita al pronto soccorso. Questo ciclo mantiene responsabilit√† e sicurezza consentendo al contempo al medico di sfruttare le intuizioni dell‚ÄôAI mantenendo il medico responsabile dell‚Äôapprovazione delle principali modifiche al trattamento.\n\n\nCiclo Paziente-Clinico\nNel ciclo Paziente-Clinico, mostrato in Figura¬†13.10, le visite di persona si concentrerebbero meno sulla raccolta di dati o sulle modifiche di base dei farmaci. Invece, il medico potrebbe interpretare tendenze e pattern di alto livello nei dati di monitoraggio continuo del paziente e avere discussioni mirate su dieta, esercizio fisico, gestione dello stress e altri cambiamenti nello stile di vita per migliorare il controllo della pressione sanguigna in modo olistico. La frequenza degli appuntamenti potrebbe essere ottimizzata dinamicamente in base alla stabilit√† del paziente anzich√© seguire un calendario fisso. Poich√© il medico non avrebbe bisogno di rivedere tutti i dati granulari, potrebbe concentrarsi sulla fornitura di cure e raccomandazioni personalizzate durante le visite. Con il monitoraggio continuo e l‚Äôottimizzazione assistita dall‚Äôintelligenza artificiale dei farmaci tra le visite, la relazione medico-paziente si concentra sugli obiettivi di benessere generale e diventa pi√π incisiva. Questo approccio proattivo e personalizzato basato sui dati pu√≤ aiutare a evitare complicazioni dell‚Äôipertensione come ictus, insufficienza cardiaca e altre minacce alla salute e al benessere del paziente.\n\n\n\n\n\n\nFigura¬†13.10: Ciclo interattivo ClinAIOps. Fonte: Chen et al. (2023).\n\n\nChen, Emma, Shvetank Prakash, Vijay Janapa Reddi, David Kim, e Pranav Rajpurkar. 2023. ¬´A framework for integrating artificial intelligence for clinical care with continuous therapeutic monitoring¬ª. Nature Biomedical Engineering, novembre. https://doi.org/10.1038/s41551-023-01115-0.\n\n\n\n\n\nMLOps vs.¬†ClinAIOps\nL‚Äôesempio dell‚Äôipertensione illustra bene perch√© i tradizionali MLOps sono insufficienti per molte applicazioni AI del mondo reale e perch√© sono invece necessari framework come ClinAIOps.\nCon l‚Äôipertensione, il semplice sviluppo e distribuzione di un modello ML per la regolazione dei farmaci avrebbe successo solo se considerasse il contesto clinico pi√π ampio. Il paziente, il medico e il sistema sanitario hanno preoccupazioni sulla definizione dell‚Äôadozione. Il modello AI non pu√≤ ottimizzare da solo i risultati della pressione sanguigna: richiede l‚Äôintegrazione con flussi di lavoro, comportamenti e incentivi.\n\nAlcune lacune chiave evidenziate dall‚Äôesempio in un approccio MLOps puro:\nIl modello stesso non avrebbe i dati dei pazienti del mondo reale su larga scala per raccomandare trattamenti in modo affidabile. ClinAIOps consente ci√≤ raccogliendo feedback da medici e pazienti tramite monitoraggio continuo.\nI medici si fiderebbero delle raccomandazioni del modello solo con trasparenza, spiegabilit√† e responsabilit√†. ClinAIOps mantiene il medico informato per creare fiducia.\nI pazienti hanno bisogno di coaching e motivazione personalizzati, non solo di notifiche AI. Il ciclo paziente-clinico di ClinAIOps facilita questo.\nL‚Äôaffidabilit√† dei sensori e l‚Äôaccuratezza dei dati sarebbero sufficienti solo con la supervisione clinica. ClinAIOps convalida le raccomandazioni.\nLa responsabilit√† per i risultati del trattamento deve essere chiarita solo con un modello ML. ClinAIOps mantiene la responsabilit√† umana.\nI sistemi sanitari dovrebbero dimostrare il valore per cambiare i flussi di lavoro. ClinAIOps allinea le parti interessate.\n\nIl caso dell‚Äôipertensione mostra chiaramente la necessit√† di guardare oltre il training e l‚Äôimplementazione di un modello ML performante per considerare l‚Äôintero sistema sociotecnico umano-IA. Questa √® la lacuna principale che ClinAIOps colma rispetto ai tradizionali MLOps. I tradizionali MLOps sono eccessivamente focalizzati sulla tecnologia per automatizzare lo sviluppo e l‚Äôimplementazione del modello ML, mentre ClinAIOps incorpora il contesto clinico e il coordinamento umano-IA attraverso cicli di feedback multi-stakeholder.\nTabella¬†13.3 li confronta. Questa tabella evidenzia come, quando si implementa MLOps, sia necessario considerare pi√π dei semplici modelli ML.\n\n\n\nTabella¬†13.3: Confronto tra operazioni MLOps e AI per uso clinico.\n\n\n\n\n\n\n\n\n\n\n\nMLOps tradizionali\nClinAIOps\n\n\n\n\nFocus\nSviluppo e distribuzione di modelli ML\nCoordinamento del processo decisionale umano e AI\n\n\nParti interessate\nData scientist, ingegneri IT\nPazienti, medici, sviluppatori AI\n\n\nCicli di feedback\nRiqualificazione del modello, monitoraggio\nPaziente-IA, clinico-IA, paziente-clinico\n\n\nObiettivo\nRendere operative le distribuzioni ML\nOttimizzare i risultati di salute del paziente\n\n\nProcessi\nPipeline e infrastruttura automatizzate\nIntegra flussi di lavoro clinici e supervisione\n\n\nConsiderazioni sui dati\nCreazione di set di dati di training\nPrivacy, etica, informazioni sanitarie protette\n\n\nValidazione del modello\nTest delle metriche delle prestazioni del modello\nValutazione clinica delle raccomandazioni\n\n\nImplementazione\nSi concentra sull‚Äôintegrazione tecnica\nAllinea gli incentivi degli stakeholder umani\n\n\n\n\n\n\n\n\nRiepilogo\nIn ambiti complessi come l‚Äôassistenza sanitaria, l‚Äôimplementazione di successo dell‚ÄôIA richiede di andare oltre un focus ristretto sul training e il deploying di modelli ML performanti. Come illustrato nell‚Äôesempio dell‚Äôipertensione, l‚Äôintegrazione dell‚ÄôIA nel mondo reale richiede il coordinamento di diverse parti interessate, l‚Äôallineamento degli incentivi, la convalida delle raccomandazioni e il mantenimento della responsabilit√†. Framework come ClinAIOps, che facilitano il processo decisionale collaborativo tra uomo e IA attraverso cicli di feedback integrati, sono necessari per affrontare queste sfide multiformi. Invece di automatizzare semplicemente le attivit√†, l‚ÄôIA deve aumentare le capacit√† umane e i flussi di lavoro clinici. Ci√≤ consente all‚ÄôIA di avere un impatto positivo sui risultati dei pazienti, sulla salute della popolazione e sull‚Äôefficienza dell‚Äôassistenza sanitaria.",
    "crumbs": [
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Operazioni di ML</span>"
    ]
  },
  {
    "objectID": "contents/core/ops/ops.it.html#conclusione",
    "href": "contents/core/ops/ops.it.html#conclusione",
    "title": "13¬† Operazioni di ML",
    "section": "13.8 Conclusione",
    "text": "13.8 Conclusione\nL‚ÄôML embedded √® pronto a trasformare molti settori abilitando le funzionalit√† AI direttamente su dispositivi edge come smartphone, sensori e hardware IoT. Tuttavia, lo sviluppo e l‚Äôimplementazione di modelli TinyML su sistemi embedded con risorse limitate pone sfide uniche rispetto ai tradizionali MLOps basati su cloud.\nQuesto capitolo ha fornito un‚Äôanalisi approfondita delle principali differenze tra MLOps tradizionali ed embedded nel ciclo di vita del modello, flussi di lavoro di sviluppo, gestione dell‚Äôinfrastruttura e pratiche operative. Abbiamo discusso di come fattori come connettivit√† intermittente, dati decentralizzati e computing limitato sul dispositivo richiedano tecniche innovative come apprendimento federato, inferenza sul dispositivo e ottimizzazione del modello. Pattern architettonici come apprendimento cross-device e infrastruttura edge-cloud gerarchica aiutano a mitigare i vincoli.\nAttraverso esempi concreti come Oura Ring e ClinAIOps, abbiamo dimostrato i principi applicati per MLOps embedded. I casi di studio hanno evidenziato considerazioni critiche che vanno oltre l‚Äôingegneria ML di base, come l‚Äôallineamento degli incentivi delle parti interessate, il mantenimento della responsabilit√† e il coordinamento del processo decisionale tra uomo e IA. Ci√≤ sottolinea la necessit√† di un approccio olistico che abbracci sia gli elementi tecnici che quelli umani.\nMentre gli MLOps embedded incontrano degli ostacoli, strumenti emergenti come Edge Impulse e lezioni dai pionieri aiutano ad accelerare l‚Äôinnovazione del TinyML. Una solida comprensione dei principi fondamentali degli MLOps adattati agli ambienti embedded consentir√† a pi√π organizzazioni di superare i vincoli e fornire capacit√† di intelligenza artificiale distribuita. Man mano che i framework e le best practice maturano, l‚Äôintegrazione fluida dell‚ÄôML nei dispositivi e nei processi edge trasformer√† i settori attraverso l‚Äôintelligenza localizzata.",
    "crumbs": [
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Operazioni di ML</span>"
    ]
  },
  {
    "objectID": "contents/core/ops/ops.it.html#sec-embedded-aiops-resource",
    "href": "contents/core/ops/ops.it.html#sec-embedded-aiops-resource",
    "title": "13¬† Operazioni di ML",
    "section": "13.9 Risorse",
    "text": "13.9 Risorse\nEcco un elenco curato di risorse per supportare studenti e insegnanti nei loro percorsi di apprendimento e insegnamento. Lavoriamo continuamente per espandere questa raccolta e presto aggiungeremo nuovi esercizi.\n\n\n\n\n\n\nSlide\n\n\n\n\n\nQueste slide sono uno strumento prezioso per gli insegnanti per tenere lezioni e per gli studenti per rivedere il materiale al proprio ritmo. Incoraggiamo sia gli studenti che gli insegnanti a sfruttare queste slide per migliorare la loro comprensione e facilitare un trasferimento efficace delle conoscenze.\n\nMLOps, DevOps, and AIOps.\nMLOps overview.\nTiny MLOps.\nMLOps: a use case.\nMLOps: Key Activities and Lifecycle.\nML Lifecycle.\nScaling TinyML: Challenges and Opportunities.\nOperazionalizzazione del Training:\n\nTraining Ops: CI/CD trigger.\nContinuous Integration.\nContinuous Deployment.\nProduction Deployment.\nProduction Deployment: Online Experimentation.\nTraining Ops Impact on MLOps.\n\nDeployment del Modello:\n\nScaling ML Into Production Deployment.\nContainers for Scaling ML Deployment.\nChallenges for Scaling TinyML Deployment: Part 1.\nChallenges for Scaling TinyML Deployment: Part 2.\nModel Deployment Impact on MLOps.\n\n\n\n\n\n\n\n\n\n\n\nVideo\n\n\n\n\n\n\nVideo¬†13.1\nVideo¬†13.2\nVideo¬†13.3\nVideo¬†13.4\n\n\n\n\n\n\n\n\n\n\nEsercizi\n\n\n\n\n\nPer rafforzare i concetti trattati in questo capitolo, abbiamo curato una serie di esercizi che sfidano gli studenti ad applicare le proprie conoscenze e ad approfondire la propria comprensione.\n\nEsercizio¬†13.1",
    "crumbs": [
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Operazioni di ML</span>"
    ]
  },
  {
    "objectID": "contents/core/privacy_security/privacy_security.it.html",
    "href": "contents/core/privacy_security/privacy_security.it.html",
    "title": "14¬† Sicurezza e Privacy",
    "section": "",
    "text": "14.1 Panoramica\nIl ‚ÄúMachine learning‚Äù [apprendimento automatico] si √® evoluto notevolmente dalle sue origini accademiche, in cui la privacy non era una preoccupazione primaria. Con la migrazione del ML in applicazioni commerciali e consumer, i dati sono diventati pi√π sensibili, comprendendo informazioni personali come comunicazioni, acquisti e dati sanitari. Questa esplosione di disponibilit√† di dati ha alimentato rapidi progressi nelle capacit√† del ML. Tuttavia, ha anche esposto nuovi rischi per la privacy, come dimostrato da incidenti come la fuga di dati di AOL nel 2006 e lo scandalo Cambridge Analytica.\nQuesti eventi hanno evidenziato la crescente necessit√† di affrontare la privacy nei sistemi ML. In questo capitolo, esploriamo insieme considerazioni sulla privacy e sulla sicurezza, poich√© sono intrinsecamente collegate nel ML. Ad esempio, una telecamera di sicurezza domestica basata su ML deve proteggere i flussi video da accessi non autorizzati e fornire protezioni della privacy per garantire che solo gli utenti previsti possano visualizzare il filmato. Una violazione della sicurezza o della privacy potrebbe esporre momenti privati degli utenti.\nI sistemi ML embedded come assistenti intelligenti e dispositivi indossabili sono onnipresenti ed elaborano dati intimi degli utenti. Tuttavia, i loro vincoli computazionali spesso impediscono protocolli di sicurezza pesanti. I progettisti devono bilanciare le esigenze di prestazioni con rigorosi standard di sicurezza e privacy adattati alle limitazioni dell‚Äôhardware embedded.\nQuesto capitolo fornisce conoscenze essenziali per affrontare il complesso panorama di privacy e sicurezza dell‚ÄôML embedded. Esploreremo le vulnerabilit√† e tratteremo varie tecniche che migliorano la privacy e la sicurezza all‚Äôinterno dei vincoli di risorse dei sistemi embedded.\nCi auguriamo che sviluppando una comprensione olistica dei rischi e delle misure di sicurezza, si acquisiranno i principi per sviluppare applicazioni ML embedded sicure ed etiche.",
    "crumbs": [
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Sicurezza e Privacy</span>"
    ]
  },
  {
    "objectID": "contents/core/privacy_security/privacy_security.it.html#terminologia",
    "href": "contents/core/privacy_security/privacy_security.it.html#terminologia",
    "title": "14¬† Sicurezza e Privacy",
    "section": "14.2 Terminologia",
    "text": "14.2 Terminologia\nIn questo capitolo parleremo insieme di sicurezza e privacy, quindi ci sono termini chiave su cui dobbiamo essere chiari. Poich√© questi termini sono concetti generali applicati in molti domini, vogliamo definire come si relazionano al contesto di questo capitolo e fornire esempi pertinenti per illustrare la loro applicazione.\n\nPrivacy: La capacit√† di controllare l‚Äôaccesso ai dati sensibili degli utenti raccolti ed elaborati da un sistema. Nel machine learning, ci√≤ implica garantire che le informazioni personali, come i dettagli finanziari o i dati biometrici, siano accessibili solo a individui autorizzati. Ad esempio, una telecamera di sicurezza domestica basata sull‚Äôapprendimento automatico potrebbe registrare filmati video e identificare i volti dei visitatori. Le preoccupazioni relative alla privacy riguardano chi pu√≤ accedere, visualizzare o condividere questi dati sensibili.\nSicurezza: La pratica di protezione dei sistemi di apprendimento automatico e dei loro dati da accessi non autorizzati, hacking, furti e uso improprio. Un sistema sicuro salvaguarda i propri dati e le proprie operazioni per garantire integrit√† e riservatezza. Ad esempio, nel contesto della telecamera di sicurezza domestica, le misure di sicurezza impediscono agli hacker di intercettare feed video in diretta o di manomettere i filmati archiviati e garantiscono che il modello stesso rimanga intatto.\nMinaccia: Si riferisce a qualsiasi potenziale pericolo, attore malintenzionato o evento dannoso che mira a sfruttare le debolezze di un sistema per comprometterne la sicurezza o la privacy. Una minaccia √® la forza o l‚Äôintento esterno che cerca di causare danni. Utilizzando l‚Äôesempio della telecamera di sicurezza domestica, una minaccia potrebbe coinvolgere un hacker che tenta di accedere a flussi live, rubare video archiviati o ingannare il sistema con falsi input per aggirare il riconoscimento facciale.\nVulnerabilit√†: Si riferisce a una debolezza, un difetto o una lacuna nel sistema che crea l‚Äôopportunit√† per una minaccia di avere successo. Le vulnerabilit√† sono i punti di esposizione che le minacce prendono di mira. Le vulnerabilit√† possono esistere in configurazioni hardware, software o di rete. Ad esempio, se la telecamera di sicurezza domestica si connette a Internet tramite una rete Wi-Fi non protetta, questa vulnerabilit√† potrebbe consentire agli aggressori di intercettare o manipolare i dati video.",
    "crumbs": [
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Sicurezza e Privacy</span>"
    ]
  },
  {
    "objectID": "contents/core/privacy_security/privacy_security.it.html#precedenti-storici",
    "href": "contents/core/privacy_security/privacy_security.it.html#precedenti-storici",
    "title": "14¬† Sicurezza e Privacy",
    "section": "14.3 Precedenti Storici",
    "text": "14.3 Precedenti Storici\nSebbene le specifiche della sicurezza hardware dell‚Äôapprendimento automatico possano essere distinte, il campo dei sistemi embedded ha una storia di incidenti di sicurezza che forniscono lezioni fondamentali per tutti i sistemi connessi, compresi quelli che utilizzano ML. Ecco analisi dettagliate di violazioni passate:\n\n14.3.1 Stuxnet\nNel 2010, qualcosa di inaspettato √® stato trovato su un computer in Iran: un virus informatico molto complicato che gli esperti non avevano mai visto prima. Stuxnet era un worm informatico dannoso che prendeva di mira i sistemi di controllo di supervisione e acquisizione dati (SCADA) ed era progettato per danneggiare il programma nucleare iraniano (Farwell e Rohozinski 2011). Stuxnet stava utilizzando quattro ‚Äúexploit zero-day‚Äù, attacchi che sfruttano debolezze segrete nel software di cui nessuno √® ancora a conoscenza. Ci√≤ ha reso Stuxnet molto subdolo e difficile da rilevare.\n\nFarwell, James P., e Rafal Rohozinski. 2011. ¬´Stuxnet and the Future of Cyber War¬ª. Survival 53 (1): 23‚Äì40. https://doi.org/10.1080/00396338.2011.555586.\nMa Stuxnet non √® stato progettato per rubare informazioni o spiare le persone. Il suo obiettivo era la distruzione fisica, sabotare le centrifughe della centrale nucleare iraniana di Natanz! Quindi come ha fatto il virus a raggiungere i computer della centrale di Natanz, che avrebbe dovuto essere disconnessa dal mondo esterno per motivi di sicurezza? Gli esperti pensano che qualcuno abbia inserito una chiavetta USB contenente Stuxnet nella rete interna di Natanz. Ci√≤ ha permesso al virus di ‚Äúsaltare‚Äù da un sistema esterno ai sistemi di controllo nucleare isolati e scatenare il caos.\nStuxnet era un malware incredibilmente avanzato creato dai governi nazionali per passare dal regno digitale alle infrastrutture del mondo reale. Ha preso di mira in modo specifico importanti macchine industriali, dove l‚Äôapprendimento automatico embedded √® altamente applicabile in un modo mai visto prima. Il virus ha lanciato un segnale di allarme su come i sofisticati attacchi informatici potrebbero ora distruggere fisicamente apparecchiature e strutture.\nQuesta violazione √® stata significativa a causa della sua sofisticatezza; Stuxnet ha preso di mira in modo specifico i ‚Äúprogrammable logic controllers (PLC)‚Äù utilizzati per automatizzare processi elettromeccanici come la velocit√† delle centrifughe per l‚Äôarricchimento dell‚Äôuranio. Il worm sfruttava le vulnerabilit√† del sistema operativo Windows per ottenere l‚Äôaccesso al software Siemens Step7 che controlla i PLC. Nonostante non sia un attacco diretto ai sistemi ML, Stuxnet √® rilevante per tutti i sistemi embedded in quanto mostra il potenziale degli attori a livello statale per progettare attacchi che collegano il mondo informatico e quello fisico con effetti devastanti. Figura¬†14.1 spiega Stuxnet in modo pi√π dettagliato.\n\n\n\n\n\n\nFigura¬†14.1: Spiegazione di Stuxnet. Fonte: IEEE Spectrum\n\n\n\n\n\n14.3.2 Hack della Jeep Cherokee\nL‚Äôhack della Jeep Cherokee √® stato un evento rivoluzionario che ha dimostrato i rischi insiti nelle automobili sempre pi√π connesse (Miller 2019). In una dimostrazione controllata, i ricercatori della sicurezza hanno sfruttato da remoto una vulnerabilit√† nel sistema di entertainment Uconnect, che aveva una connessione cellulare a Internet. Sono stati in grado di controllare il motore, la trasmissione e i freni del veicolo, allarmando l‚Äôindustria automobilistica e spingendola a riconoscere le gravi implicazioni per la sicurezza delle vulnerabilit√† informatiche nei veicoli. Video¬†14.1 di seguito √® riportato un breve documentario dell‚Äôattacco.\n\nMiller, Charlie. 2019. ¬´Lessons learned from hacking a car¬ª. IEEE Design &amp; Test 36 (6): 7‚Äì9. https://doi.org/10.1109/mdat.2018.2863106.\n\n\n\n\n\n\nVideo¬†14.1: Hack della Jeep Cherokee\n\n\n\n\n\n\nSebbene non si sia trattato di un attacco a un sistema ML in s√©, l‚Äôaffidamento dei veicoli moderni ai sistemi embedded per funzioni critiche per la sicurezza presenta parallelismi significativi con l‚Äôimplementazione di ML nei sistemi embedded, sottolineando la necessit√† di una sicurezza robusta a livello hardware.\n\n\n14.3.3 Botnet Mirai\nLa botnet Mirai ha coinvolto l‚Äôinfezione di dispositivi in rete come fotocamere digitali e lettori DVR (Antonakakis et al. 2017). Nell‚Äôottobre 2016, la botnet √® stata utilizzata per condurre uno dei pi√π grandi attacchi DDoS, interrompendo l‚Äôaccesso a Internet negli Stati Uniti. L‚Äôattacco √® stato possibile perch√© molti dispositivi utilizzavano nomi utente e password predefiniti, che sono stati facilmente sfruttati dal malware Mirai per controllare i dispositivi. Video¬†14.2 spiega come funziona la botnet Mirai.\n\nAntonakakis, Manos, Tim April, Michael Bailey, Matt Bernhard, Elie Bursztein, Jaime Cochran, Zakir Durumeric, et al. 2017. ¬´Understanding the mirai botnet¬ª. In 26th USENIX security symposium (USENIX Security 17), 1093‚Äì1110.\n\n\n\n\n\n\nVideo¬†14.2: Botnet Mirai\n\n\n\n\n\n\nSebbene i dispositivi non fossero basati su ML, l‚Äôincidente √® un duro promemoria di ci√≤ che pu√≤ accadere quando numerosi dispositivi embedded con scarsi controlli di sicurezza vengono collegati in rete, cosa che sta diventando sempre pi√π comune con la crescita dei dispositivi IoT basati su ML.\n\n\n14.3.4 Implicazioni\nQueste violazioni storiche dimostrano gli effetti a cascata delle vulnerabilit√† hardware nei sistemi embedded. Ogni incidente offre un precedente per comprendere i rischi e progettare protocolli di sicurezza migliori. Ad esempio, la botnet Mirai evidenzia l‚Äôimmenso potenziale distruttivo quando gli autori delle minacce possono ottenere il controllo su dispositivi in rete con sicurezza debole, una situazione che sta diventando sempre pi√π comune con i sistemi ML. Molti dispositivi ML attuali funzionano come dispositivi ‚Äúedge‚Äù pensati per raccogliere ed elaborare dati localmente prima di inviarli al cloud. Proprio come le telecamere e i DVR compromessi da Mirai, i dispositivi ML edge spesso si basano su hardware embedded come processori ARM ed eseguono sistemi operativi leggeri come Linux. Proteggere le credenziali del dispositivo √® fondamentale.\nAllo stesso modo, l‚Äôhacking della Jeep Cherokee √® stato un momento spartiacque per l‚Äôindustria automobilistica. Ha esposto gravi vulnerabilit√† nei crescenti sistemi di veicoli connessi in rete e la loro mancanza di isolamento dai sistemi di guida principali come freni e sterzo. In risposta, i produttori di automobili hanno investito molto in nuove misure di sicurezza informatica, anche se probabilmente permangono delle lacune.\nChrysler ha effettuato un richiamo per correggere il software vulnerabile Uconnect, che consentiva l‚Äôexploit remoto. Ci√≤ includeva l‚Äôaggiunta di protezioni a livello di rete per impedire l‚Äôaccesso esterno non autorizzato e la compartimentazione dei sistemi di bordo per limitare i movimenti laterali. Sono stati aggiunti ulteriori livelli di crittografia per i comandi inviati tramite il bus CAN all‚Äôinterno dei veicoli.\nL‚Äôincidente ha anche stimolato la creazione di nuovi standard e best practice per la sicurezza informatica. L‚ÄôAuto-ISAC √® stato istituito per consentire alle case automobilistiche di condividere informazioni e la NHTSA ha guidato i rischi di gestione. Sono state sviluppate nuove procedure di test e audit per valutare le vulnerabilit√† in modo proattivo. Gli effetti collaterali continuano a guidare il cambiamento nel settore automobilistico poich√© le auto diventano sempre pi√π definite dal software.\nSfortunatamente, i produttori spesso trascurano la sicurezza quando sviluppano nuovi dispositivi edge ML, utilizzando password predefinite, comunicazioni non crittografate, aggiornamenti firmware non protetti, ecc. Tali vulnerabilit√† potrebbero consentire agli aggressori di ottenere l‚Äôaccesso e controllare i dispositivi su larga scala infettandoli con malware. Con una botnet di dispositivi ML compromessi, gli aggressori potrebbero sfruttare la loro potenza di calcolo aggregata per attacchi DDoS su infrastrutture critiche.\nSebbene questi eventi non abbiano coinvolto direttamente hardware di machine learning, i principi degli attacchi si estendono ai sistemi ML, che spesso coinvolgono dispositivi embedded e architetture di rete simili. Poich√© l‚Äôhardware ML √® sempre pi√π integrato con il mondo fisico, proteggerlo da tali violazioni √® fondamentale. L‚Äôevoluzione delle misure di sicurezza in risposta a questi incidenti fornisce preziose informazioni sulla protezione dei sistemi ML attuali e futuri da vulnerabilit√† analoghe.\nLa natura distribuita dei dispositivi edge ML significa che le minacce possono propagarsi rapidamente attraverso le reti. E se i dispositivi vengono utilizzati per scopi critici come dispositivi medici, controlli industriali o veicoli a guida autonoma, il potenziale danno fisico dei bot ML armati potrebbe essere grave. Proprio come Mirai ha dimostrato il potenziale pericoloso dei dispositivi IoT scarsamente protetti, la prova del nove per la sicurezza dell‚Äôhardware ML sar√† quanto questi dispositivi siano vulnerabili o resilienti ad attacchi simili a worm. La posta in gioco aumenta man mano che il ML si diffonde in ambiti critici per la sicurezza, ponendo l‚Äôonere sui produttori e sugli operatori di sistema di incorporare le lezioni di Mirai.\nLa lezione √® l‚Äôimportanza di progettare per la sicurezza fin dall‚Äôinizio e di avere difese stratificate. Il caso Jeep evidenzia potenziali vulnerabilit√† per i sistemi ML in merito alle interfacce software esterne e all‚Äôisolamento tra sottosistemi. I produttori di dispositivi e piattaforme ML dovrebbero assumere un approccio proattivo e completo simile alla sicurezza piuttosto che lasciarlo come un ripensamento. Una risposta rapida e la diffusione delle best practice saranno cruciali man mano che le minacce si evolvono.",
    "crumbs": [
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Sicurezza e Privacy</span>"
    ]
  },
  {
    "objectID": "contents/core/privacy_security/privacy_security.it.html#minacce-alla-sicurezza-per-i-modelli-ml",
    "href": "contents/core/privacy_security/privacy_security.it.html#minacce-alla-sicurezza-per-i-modelli-ml",
    "title": "14¬† Sicurezza e Privacy",
    "section": "14.4 Minacce alla Sicurezza per i Modelli ML",
    "text": "14.4 Minacce alla Sicurezza per i Modelli ML\nI modelli ML affrontano rischi per la sicurezza che possono comprometterne l‚Äôintegrit√†, le prestazioni e l‚Äôaffidabilit√† se non affrontati adeguatamente. Tra queste, spiccano tre minacce principali: il furto di modelli, in cui gli avversari rubano parametri di modelli proprietari e i dati sensibili in essi contenuti; l‚Äôavvelenamento dei dati, che compromette i modelli manomettendo i dati di training; e gli attacchi avversari, progettati per ingannare i modelli e indurli a fare previsioni errate o indesiderate. Discuteremo ciascuna di queste minacce in dettaglio e forniremo esempi di casi di studio per illustrare le loro implicazioni nel mondo reale.\n\n14.4.1 Furto di Modelli\nIl furto di modelli si verifica quando un aggressore ottiene l‚Äôaccesso non autorizzato a un modello ML distribuito. La preoccupazione in questo caso √® il furto della struttura del modello e dei parametri addestrati, nonch√© dei dati proprietari in esso contenuti (Ateniese et al. 2015). Il furto di modelli √® una minaccia reale e crescente, come dimostrato da casi come quello dell‚Äôex ingegnere di Google Anthony Levandowski, che presumibilmente ha rubato i progetti di auto a guida autonoma di Waymo e ha fondato un‚Äôazienda concorrente. Oltre all‚Äôimpatto economico, il furto di modelli pu√≤ seriamente compromettere la privacy e consentire ulteriori attacchi.\n\nAteniese, Giuseppe, Luigi V. Mancini, Angelo Spognardi, Antonio Villani, Domenico Vitali, e Giovanni Felici. 2015. ¬´Hacking smart machines with smarter ones: How to extract meaningful data from machine learning classifiers¬ª. International Journal of Security and Networks 10 (3): 137. https://doi.org/10.1504/ijsn.2015.071829.\nAd esempio, si consideri un modello ML sviluppato per raccomandazioni personalizzate in un‚Äôapplicazione di e-commerce. Se un concorrente ruba questo modello, ottiene informazioni su analisi aziendali, preferenze dei clienti e persino segreti commerciali racchiusi nei dati del modello. Gli aggressori potrebbero sfruttare i modelli rubati per creare input pi√π efficaci per attacchi di ‚Äúinversione del modello‚Äù, deducendo dettagli privati sui dati di addestramento del modello. Un modello di raccomandazione di e-commerce clonato potrebbe rivelare i comportamenti di acquisto e i dati demografici dei clienti.\nPer comprendere gli attacchi di ‚Äúinversione del modello‚Äù, si consideri un sistema di riconoscimento facciale utilizzato per concedere l‚Äôaccesso a strutture protette. Il sistema viene addestrato su un set di dati di foto dei dipendenti. Un aggressore potrebbe dedurre le caratteristiche del set di dati originale osservando l‚Äôoutput del modello su vari input. Ad esempio, supponiamo che il livello di confidenza del modello per un particolare volto sia significativamente pi√π alto per un dato set di caratteristiche. In tal caso, un aggressore potrebbe dedurre che qualcuno con quelle caratteristiche √® probabile che sia nel set di dati di addestramento.\nLa metodologia di ‚Äúinversione del modello‚Äù in genere prevede i seguenti passaggi:\n\nAccesso agli Output del Modello: L‚Äôaggressore interroga il modello ML con dati di input e osserva gli output. Ci√≤ avviene spesso tramite un‚Äôinterfaccia legittima, come un‚ÄôAPI pubblica.\nAnalisi dei Confidence Score: Per ogni input, il modello fornisce un ‚Äúpunteggio di confidenza‚Äù che riflette quanto l‚Äôinput sia simile ai dati di training.\nReverse-Engineering: Analizzando i punteggi di confidenza o le probabilit√† di output, gli aggressori possono utilizzare tecniche di ottimizzazione per ricostruire ci√≤ che ritengono sia vicino ai dati di input originali.\n\nUn esempio storico di tale vulnerabilit√† esplorata √® stata la ricerca sugli attacchi di inversione contro il set di dati del premio Netflix degli Stati Uniti, in cui i ricercatori hanno dimostrato che era possibile conoscere le preferenze cinematografiche di un individuo, il che potrebbe portare a violazioni della privacy (Narayanan e Shmatikov 2006).\n\nNarayanan, Arvind, e Vitaly Shmatikov. 2006. ¬´How To Break Anonymity of the Netflix Prize Dataset¬ª. CoRR. http://arxiv.org/abs/cs/0610105.\nIl furto di modelli implica che potrebbe portare a perdite economiche, minare il vantaggio competitivo e violare la privacy degli utenti. C‚Äô√® anche il rischio di attacchi di inversione del modello, in cui un avversario potrebbe immettere vari dati nel modello rubato per dedurre informazioni sensibili sui dati di addestramento.\nIn base alla risorsa desiderata, gli attacchi con furto di modelli possono essere suddivisi in due categorie: propriet√† esatte del modello e comportamento approssimativo del modello.\n\nFurto di Propriet√† Esatte del Modello\nIn questi attacchi, l‚Äôobiettivo √® estrarre informazioni su metriche concrete, come i parametri appresi di una rete, gli iperparametri ottimizzati e l‚Äôarchitettura interna dei layer del modello (Oliynyk, Mayer, e Rauber 2023).\n\nParametri Appresi: Gli avversari mirano a rubare la conoscenza appresa di un modello (pesi e bias) per replicarla. Il furto di parametri √® generalmente utilizzato con altri attacchi, come il furto di architettura, che non hanno conoscenza dei parametri.\nIperparametri Ottimizzati: L‚Äôaddestramento √® costoso e l‚Äôidentificazione della configurazione ottimale degli iperparametri (come velocit√† di apprendimento e regolarizzazione) pu√≤ richiedere molto tempo e risorse. Di conseguenza, rubare gli iperparametri ottimizzati di un modello consente agli avversari di replicare il modello senza sostenere gli stessi costi di sviluppo.\nArchitettura del Modello: Questo attacco riguarda la progettazione e la struttura specifiche del modello, come strati, neuroni e pattern di connettivit√†. Oltre a ridurre i costi di training associati, questo furto rappresenta un grave rischio per la propriet√† intellettuale, potenzialmente compromettendo il vantaggio competitivo di un‚Äôazienda. Il furto di architettura pu√≤ essere ottenuto sfruttando attacchi side-channel (discussi pi√π avanti).\n\n\n\nFurto del Comportamento Approssimativo del Modello\nInvece di estrarre valori numerici esatti dei parametri del modello, questi attacchi mirano a riprodurre il comportamento del modello (previsioni ed efficacia), il processo decisionale e le caratteristiche di alto livello (Oliynyk, Mayer, e Rauber 2023). Queste tecniche mirano a ottenere risultati simili pur consentendo deviazioni interne nei parametri e nell‚Äôarchitettura. I tipi di furto di comportamento approssimativo includono l‚Äôottenimento dello stesso livello di efficacia e l‚Äôottenimento di coerenza di previsione.\n\nOliynyk, Daryna, Rudolf Mayer, e Andreas Rauber. 2023. ¬´I Know What You Trained Last Summer: A Survey on Stealing Machine Learning Models and Defences¬ª. ACM Computing Surveys 55 (14s): 1‚Äì41. https://doi.org/10.1145/3595292.\n\nLivello di efficacia: Gli aggressori mirano a replicare le capacit√† decisionali del modello piuttosto che concentrarsi sui valori precisi dei parametri. Ci√≤ avviene attraverso la comprensione del comportamento complessivo del modello. Consideriamo uno scenario in cui un aggressore desidera copiare il comportamento di un modello di classificazione delle immagini. Analizzando i limiti decisionali del modello, l‚Äôattacco ottimizza il suo modello per raggiungere un‚Äôefficacia paragonabile al modello originale. Ci√≤ potrebbe comportare l‚Äôanalisi di 1) la matrice di confusione per comprendere l‚Äôequilibrio delle metriche di previsione (vero positivo, vero negativo, falso positivo, falso negativo) e 2) altre metriche di prestazione, come punteggio F1 e precisione, per garantire che i due modelli siano comparabili.\nCoerenza della Previsione: L‚Äôattaccante cerca di allineare i pattern di previsione del proprio modello con quelli del modello target. Ci√≤ comporta l‚Äôabbinamento degli output di previsione (sia positivi che negativi) sullo stesso set di input e la garanzia della coerenza distributiva tra classi diverse. Ad esempio, prendiamo in considerazione un modello di elaborazione del linguaggio naturale (NLP) che genera un‚Äôanalisi del sentiment per le recensioni di film (etichettando le recensioni come positive, neutre o negative). L‚Äôattaccante cercher√† di mettere a punto il proprio modello per adattarlo alla previsione dei modelli originali sullo stesso set di recensioni di film. Ci√≤ include la garanzia che il modello commetta gli stessi errori (previsioni errate) commessi dal modello target.\n\n\n\nCaso di Studio: Il furto di Propriet√† Intellettuale di Tesla\nNel 2018, Tesla ha intentato una causa contro la startup di auto a guida autonoma Zoox, sostenendo che ex dipendenti avevano rubato dati riservati e segreti commerciali relativi al sistema di assistenza alla guida autonoma di Tesla.\nTesla ha affermato che diversi suoi ex dipendenti hanno sottratto oltre 10 GB di dati proprietari, tra cui modelli di ML e codice sorgente, prima di unirsi a Zoox. Ci√≤ avrebbe incluso uno dei modelli di riconoscimento delle immagini cruciali di Tesla per l‚Äôidentificazione degli oggetti.\nIl furto di questo modello proprietario sensibile potrebbe aiutare Zoox ad abbreviare anni di sviluppo ML e duplicare le capacit√† di Tesla. Tesla ha sostenuto che questo furto di propriet√† intellettuale ha causato notevoli danni finanziari e competitivi. C‚Äôerano anche preoccupazioni che potesse consentire attacchi di inversione del modello per dedurre dettagli privati sui dati di test di Tesla.\nI dipendenti di Zoox hanno negato di aver rubato informazioni proprietarie. Tuttavia, il caso evidenzia i rischi significativi del furto di modelli, che consente la clonazione di modelli commerciali, causando ripercussioni economiche e aprendo la porta a ulteriori violazioni della privacy dei dati.\n\n\n\n14.4.2 Avvelenamento dei Dati\nL‚Äôavvelenamento dei dati √® un attacco in cui i dati di training vengono manomessi, portando a un modello compromesso (Biggio, Nelson, e Laskov 2012). Gli aggressori possono modificare gli esempi di training esistenti, inserire nuovi dati dannosi o influenzare il processo di raccolta dati. I dati avvelenati vengono etichettati in modo tale da alterare il comportamento appreso del modello. Ci√≤ pu√≤ essere particolarmente dannoso nelle applicazioni in cui i modelli ML prendono decisioni automatizzate in base a pattern appresi. Oltre ai set di training, i test di avvelenamento e i dati di convalida possono consentire agli avversari di aumentare artificialmente le prestazioni del modello segnalate.\n\nBiggio, Battista, Blaine Nelson, e Pavel Laskov. 2012. ¬´Poisoning Attacks against Support Vector Machines.¬ª In Proceedings of the 29th International Conference on Machine Learning, ICML 2012, Edinburgh, Scotland, UK, June 26 - July 1, 2012. icml.cc / Omnipress. http://icml.cc/2012/papers/880.pdf.\nIl processo di solito prevede i seguenti passaggi:\n\nInjection: L‚Äôaggressore aggiunge esempi errati o fuorvianti al set di training. Questi esempi sono spesso progettati per sembrare normali a un‚Äôispezione superficiale, ma sono stati attentamente elaborati per interrompere il processo di apprendimento.\nTraining: Il modello ML si allena su questo set di dati manipolato e sviluppa comprensioni distorte dei pattern di dati.\nDeployment: Una volta distribuito il modello, l‚Äôaddestramento corrotto porta a un processo decisionale imperfetto o a vulnerabilit√† prevedibili che l‚Äôaggressore pu√≤ sfruttare.\n\nGli impatti dell‚Äôavvelenamento dei dati vanno oltre i semplici errori di classificazione o cali di accuratezza. Ad esempio, se dati errati o dannosi vengono introdotti nel set di addestramento di un sistema di riconoscimento dei segnali stradali, il modello potrebbe imparare a classificare erroneamente i segnali di stop come segnali di precedenza, il che pu√≤ avere pericolose conseguenze nel mondo reale, specialmente nei sistemi autonomi embedded come i veicoli autonomi.\nL‚Äôavvelenamento dei dati pu√≤ degradare l‚Äôaccuratezza di un modello, costringerlo a fare previsioni errate o farlo comportare in modo imprevedibile. In applicazioni critiche come l‚Äôassistenza sanitaria, tali alterazioni possono portare a significativi problemi di fiducia e sicurezza.\nEsistono sei categorie principali di avvelenamento dei dati (Oprea, Singhal, e Vassilev 2022):\n\nOprea, Alina, Anoop Singhal, e Apostol Vassilev. 2022. ¬´Poisoning Attacks Against Machine Learning: Can Machine Learning Be Trustworthy?¬ª Computer 55 (11): 94‚Äì99. https://doi.org/10.1109/mc.2022.3190787.\n\nAttacchi di Disponibilit√†: Questi attacchi cercano di compromettere la funzionalit√† complessiva di un modello. Fanno s√¨ che classifichi erroneamente la maggior parte dei campioni di test, rendendo il modello inutilizzabile per applicazioni pratiche. Un esempio √® il ‚Äúlabel flipping‚Äù, in cui le etichette di una classe specifica e mirata vengono sostituite con etichette di una classe diversa.\nAttacchi Mirati: A differenza degli attacchi alla disponibilit√†, gli attacchi mirati mirano a compromettere un numero limitato di campioni di test. Quindi, l‚Äôeffetto √® localizzato su un numero limitato di classi, mentre il modello mantiene lo stesso livello di accuratezza originale sulla maggior parte delle classi. La natura mirata dell‚Äôattacco richiede che l‚Äôaggressore conosca le classi del modello, rendendo pi√π difficile il rilevamento di questi attacchi.\nAttacchi Backdoor: In questi attacchi, un avversario prende di mira pattern specifici nei dati. L‚Äôaggressore introduce una backdoor (un trigger o pattern nascosto e dannoso) nei dati di training, ad esempio modificando determinate feature nei dati strutturati o un pattern di pixel in una posizione fissa. Ci√≤ fa s√¨ che il modello associ il pattern dannoso a etichette specifiche. Di conseguenza, quando il modello incontra campioni di test che contengono un pattern dannoso, fa false previsioni, evidenziando l‚Äôimportanza della cautela e della prevenzione nel ruolo dei professionisti della sicurezza dei dati.\nAttacchi di Sotto-popolazione: Gli aggressori scelgono selettivamente di compromettere un sottoinsieme dei campioni di test mantenendo l‚Äôaccuratezza sul resto dei campioni. Questi attacchi si possono pensare come una combinazione di attacchi di disponibilit√† e mirati: eseguire attacchi di disponibilit√† (degrado delle prestazioni) nell‚Äôambito di un sottoinsieme mirato. Sebbene gli attacchi di sottopopolazione possano sembrare molto simili agli attacchi mirati, i due presentano chiare differenze:\nScope: Mentre gli attacchi mirati prendono di mira un set selezionato di campioni, gli attacchi di sotto-popolazione prendono di mira una sotto-popolazione generale con rappresentazioni di caratteristiche simili. Ad esempio, in un attacco mirato, un attore inserisce immagini manipolate di un cartello di avvertimento ‚Äúrallentamenti‚Äù (con perturbazioni o pattern attentamente studiati), che fa s√¨ che un‚Äôauto autonoma non riconosca tale cartello e non rallenti. D‚Äôaltro canto, manipolare tutti i campioni di persone con accento britannico in modo che un modello di riconoscimento vocale classifichi erroneamente il discorso di una persona britannica √® un esempio di attacco di sotto-popolazione.\nConoscenza: Mentre gli attacchi mirati richiedono un alto grado di familiarit√† con i dati, gli attacchi alla sotto-popolazione richiedono una conoscenza meno approfondita per essere efficaci.\n\n\nCaso di Studio: Avvelenamento dei Sistemi di Moderazione dei Contenuti\nNel 2017, i ricercatori hanno dimostrato un attacco di avvelenamento dei data contro un modello di classificazione della tossicit√† popolare chiamato Perspective (Hosseini et al. 2017). Questo modello ML rileva commenti tossici online.\n\nHosseini, Hossein, Sreeram Kannan, Baosen Zhang, e Radha Poovendran. 2017. ¬´Deceiving Google‚Äôs Perspective API Built for Detecting Toxic Comments¬ª. ArXiv preprint abs/1702.08138 (febbraio). http://arxiv.org/abs/1702.08138v1.\nI ricercatori hanno aggiunto commenti tossici generati sinteticamente con lievi errori di ortografia e grammaticali ai dati di training del modello. Ci√≤ ha lentamente corrotto il modello, facendogli classificare erroneamente un numero crescente di input gravemente tossici come non tossici nel tempo.\nDopo il ri-addestramento sui dati avvelenati, il tasso di falsi negativi del modello √® aumentato dall‚Äô1,4% al 27%, consentendo ai commenti estremamente tossici di aggirare il rilevamento. I ricercatori hanno avvertito che questo furtivo ‚Äúdata poisoning‚Äù potrebbe consentire la diffusione di discorsi di odio, molestie e abusi se implementato contro sistemi di moderazione reali.\nQuesto caso evidenzia come l‚Äôavvelenamento dei dati possa degradare l‚Äôaccuratezza e l‚Äôaffidabilit√† del modello. Per le piattaforme di social media, un attacco di avvelenamento che compromette il rilevamento della tossicit√† potrebbe portare alla proliferazione di contenuti dannosi e alla sfiducia nei sistemi di moderazione ML. L‚Äôesempio dimostra perch√© proteggere l‚Äôintegrit√† dei dati di training e monitorare l‚Äôavvelenamento √® fondamentale in tutti i domini applicativi.\n\n\nCaso di Studio: Proteggere l‚ÄôArte Attraverso l‚ÄôAvvelenamento dei Dati\n√à interessante notare che gli attacchi di ‚Äúdata poisoning‚Äù non sono sempre dannosi (Shan et al. 2023). Nightshade, uno strumento sviluppato da un team guidato dal professor Ben Zhao presso l‚ÄôUniversit√† di Chicago, utilizza l‚Äôavvelenamento dei dati per aiutare gli artisti a proteggere la propria arte da scraping e violazioni del copyright da parte di modelli di IA generativa. Gli artisti possono utilizzare lo strumento per modificare le proprie immagini in modo sottile prima di caricarle online.\nSebbene queste modifiche siano impercettibili all‚Äôocchio umano, possono degradare significativamente le prestazioni dei modelli di intelligenza artificiale generativa quando integrate nei dati di addestramento. I modelli generativi possono essere manipolati per produrre output irrealistici o privi di senso. Ad esempio, con solo 300 immagini corrotte, i ricercatori dell‚ÄôUniversit√† di Chicago sono riusciti a ingannare l‚Äôultimo modello ‚ÄúStable Diffusion‚Äù per generare immagini di cani che assomigliano a felini o bovini quando richiesto per le automobili.\nCon l‚Äôaumento della quantit√† di immagini corrotte online, l‚Äôefficacia dei modelli addestrati su dati estratti diminuir√† esponenzialmente. Inizialmente, identificare i dati corrotti √® difficile e richiede un intervento manuale. Successivamente, la contaminazione si diffonde rapidamente ai concetti correlati, poich√© i modelli generativi stabiliscono connessioni tra le parole e le loro rappresentazioni visive. Di conseguenza, un‚Äôimmagine corrotta di un‚Äô‚Äúauto‚Äù potrebbe propagarsi in immagini generate collegate a termini come ‚Äúcamion‚Äù, ‚Äútreno‚Äù e ‚Äúautobus‚Äù.\nD‚Äôaltro canto, questo strumento pu√≤ essere utilizzato in modo dannoso e influenzare le applicazioni legittime del modello generativo. Ci√≤ dimostra la natura molto impegnativa e innovativa degli attacchi di apprendimento automatico.\nFigura¬†17.26 mostra gli effetti di diversi livelli di avvelenamento dei dati (50 campioni, 100 campioni e 300 campioni di immagini avvelenate) sulla generazione di immagini in varie categorie. Notare come le immagini iniziano a deformarsi e deviare dalla categoria desiderata. Ad esempio, dopo 300 campioni di avvelenamento, una richiesta di un‚Äôauto genera una mucca.\n\n\n\n\n\n\nFigura¬†14.2: Avvelenamento dei Dati. Fonte: Shan et al. (2023).\n\n\nShan, Shawn, Wenxin Ding, Josephine Passananti, Stanley Wu, Haitao Zheng, e Ben Y. Zhao. 2023. ¬´Nightshade: Prompt-Specific Poisoning Attacks on Text-to-Image Generative Models¬ª. ArXiv preprint abs/2310.13828 (ottobre). http://arxiv.org/abs/2310.13828v3.\n\n\n\n\n\n14.4.3 Attacchi Avversari\nGli attacchi avversari mirano a indurre i modelli a fare previsioni errate fornendo loro input ingannevoli appositamente creati (chiamati esempi avversari) (Parrish et al. 2023). Aggiungendo lievi perturbazioni ai dati di input, gli avversari possono ‚Äúhackerare‚Äù il riconoscimento di pattern di un modello e ingannarlo. Si tratta di tecniche sofisticate in cui lievi, spesso impercettibili alterazioni dei dati di input possono indurre un modello ML a fare una previsione errata.\n\nParrish, Alicia, Hannah Rose Kirk, Jessica Quaye, Charvi Rastogi, Max Bartolo, Oana Inel, Juan Ciro, et al. 2023. ¬´Adversarial Nibbler: A Data-Centric Challenge for Improving the Safety of Text-to-Image Models¬ª. ArXiv preprint abs/2305.14384 (maggio). http://arxiv.org/abs/2305.14384v1.\n\nRamesh, Aditya, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, e Ilya Sutskever. 2021. ¬´Zero-Shot Text-to-Image Generation¬ª. In Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, a cura di Marina Meila e Tong Zhang, 139:8821‚Äì31. Proceedings of Machine Learning Research. PMLR. http://proceedings.mlr.press/v139/ramesh21a.html.\n\nRombach, Robin, Andreas Blattmann, Dominik Lorenz, Patrick Esser, e Bjorn Ommer. 2022. ¬´High-Resolution Image Synthesis with Latent Diffusion Models¬ª. In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 10674‚Äì85. IEEE. https://doi.org/10.1109/cvpr52688.2022.01042.\n√à possibile generare prompt che portano a immagini non sicure in modelli testo-immagine come DALLE (Ramesh et al. 2021) o Stable Diffusion (Rombach et al. 2022). Ad esempio, alterando i valori dei pixel di un‚Äôimmagine, gli aggressori possono ingannare un sistema di riconoscimento facciale facendogli identificare un volto come una persona diversa.\nGli attacchi avversari sfruttano il modo in cui i modelli ML apprendono e prendono decisioni durante l‚Äôinferenza. Questi modelli funzionano sul principio di riconoscimento di pattern nei dati. Un avversario crea input dannosi con perturbazioni per fuorviare il riconoscimento di pattern del modello, essenzialmente ‚Äúhackerando‚Äù le percezioni del modello.\nGli attacchi avversari rientrano in diversi scenari:\n\nAttacchi Whitebox: L‚Äôattaccante ha una conoscenza completa del funzionamento interno del modello target, inclusi i dati di addestramento, i parametri e l‚Äôarchitettura. Questo ampio accesso facilita lo sfruttamento delle vulnerabilit√† del modello. L‚Äôattaccante pu√≤ sfruttare debolezze specifiche e sottili per costruire esempi avversari altamente efficaci.\nAttacchi Blackbox: A differenza degli attacchi Whitebox, in quelli Blackbox l‚Äôattaccante ha poca o nessuna conoscenza del modello target. L‚Äôattore avversario deve osservare attentamente il comportamento di output del modello per eseguire l‚Äôattacco.\nAttacchi Greybox: Questi attacchi occupano uno spettro tra gli attacchi Blackbox e Whitebox. L‚Äôavversario possiede una conoscenza parziale della struttura interna del modello target. Ad esempio, l‚Äôattaccante potrebbe conoscere i dati di training ma non avere informazioni sull‚Äôarchitettura o sui parametri del modello. In scenari pratici, la maggior parte degli attacchi rientra in questa zona grigia.\n\nIl panorama dei modelli di apprendimento automatico √® complesso e ampio, soprattutto data la loro relativamente recente integrazione nelle applicazioni commerciali. Questa rapida adozione, sebbene trasformativa, ha portato alla luce numerose vulnerabilit√† all‚Äôinterno di questi modelli. Di conseguenza, sono emersi vari metodi di attacco avversari, ognuno dei quali sfrutta strategicamente diversi aspetti di vari modelli. Di seguito, evidenziamo un sottoinsieme di questi metodi, che mostra la natura multiforme degli attacchi avversari sui modelli di apprendimento automatico:\n\nGenerative Adversarial Network (GAN): La natura avversaria delle GAN, in cui un generatore e un discriminatore competono, si allinea perfettamente con la creazione di attacchi avversari (Goodfellow et al. 2020). Sfruttando questo framework, la rete del generatore viene addestrata per produrre input che sfruttano le debolezze di un modello target, causandone una classificazione errata. Questo processo dinamico e competitivo rende le GAN particolarmente efficaci nel creare esempi avversari sofisticati e diversificati, sottolineando la loro adattabilit√† nell‚Äôattaccare i modelli di apprendimento automatico.\nTransfer Learning Adversarial Attacks: Questi attacchi prendono di mira gli estrattori di feature nei modelli di apprendimento per trasferimento introducendo perturbazioni che manipolano le loro rappresentazioni apprese. Gli estrattori di feature, pre-addestrati per identificare modelli generali, sono ottimizzati per attivit√† specifiche nei modelli downstream [a valle]. Gli avversari sfruttano questo trasferimento creando input che distorcono gli output dell‚Äôestrattore di feature, causando classificazioni errate a valle. Gli ‚Äúattacchi headless‚Äù esemplificano questa strategia, in cui gli avversari si concentrano sull‚Äôestrattore di feature senza richiedere l‚Äôaccesso alla classificazione principale o ai dati di addestramento. Ci√≤ evidenzia una vulnerabilit√† critica nelle pipeline di apprendimento per trasferimento, poich√© i componenti fondamentali di molti modelli possono essere sfruttati. Rafforzare le difese √® essenziale, data la diffusa dipendenza dai modelli pre-addestrati (Abdelkader et al. 2020).\n\n\nGoodfellow, Ian, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, e Yoshua Bengio. 2020. ¬´Generative adversarial networks¬ª. Communications of the ACM 63 (11): 139‚Äì44. https://doi.org/10.1145/3422622.\n\nAbdelkader, Ahmed, Michael J. Curry, Liam Fowl, Tom Goldstein, Avi Schwarzschild, Manli Shu, Christoph Studer, e Chen Zhu. 2020. ¬´Headless Horseman: Adversarial Attacks on Transfer Learning Models¬ª. In ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 3087‚Äì91. IEEE. https://doi.org/10.1109/icassp40776.2020.9053181.\n\nCaso di Studio: Inganno dei Modelli di Rilevamento dei Segnali Stradali\nNel 2017, i ricercatori hanno condotto esperimenti posizionando piccoli adesivi bianchi e neri sui segnali di stop (Eykholt et al. 2017). Quando visti da un occhio umano normale, gli adesivi non oscuravano il segnale n√© ne impedivano l‚Äôinterpretazione. Tuttavia, quando le immagini degli adesivi dei segnali di stop venivano inserite nei modelli ML standard di classificazione dei segnali stradali, venivano classificati erroneamente come segnali di limite di velocit√† nell‚Äô85% dei casi.\n\nEykholt, Kevin, Ivan Evtimov, Earlence Fernandes, Bo Li, Amir Rahmati, Chaowei Xiao, Atul Prakash, Tadayoshi Kohno, e Dawn Song. 2017. ¬´Robust Physical-World Attacks on Deep Learning Models¬ª. ArXiv preprint abs/1707.08945 (luglio). http://arxiv.org/abs/1707.08945v5.\nQuesta dimostrazione ha mostrato come semplici adesivi avversari potrebbero ingannare i sistemi ML facendogli interpretare male i segnali stradali critici. Se implementati in modo realistico, questi attacchi potrebbero mettere a repentaglio la sicurezza pubblica, inducendo i veicoli autonomi a interpretare male i segnali di stop come limiti di velocit√†. I ricercatori hanno avvertito che ci√≤ potrebbe potenzialmente causare pericolosi semplici rallentamenti o accelerazioni negli incroci.\nQuesto caso di studio fornisce un‚Äôillustrazione concreta di come gli esempi avversari sfruttano i meccanismi di riconoscimento di pattern dei modelli ML. Alterando in modo sottile i dati di input, gli aggressori possono indurre previsioni errate e rappresentare rischi significativi per applicazioni critiche per la sicurezza come le auto a guida autonoma. La semplicit√† dell‚Äôattacco dimostra come anche cambiamenti minori e impercettibili possano sviare i modelli. Di conseguenza, gli sviluppatori devono implementare difese robuste contro tali minacce.",
    "crumbs": [
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Sicurezza e Privacy</span>"
    ]
  },
  {
    "objectID": "contents/core/privacy_security/privacy_security.it.html#minacce-alla-sicurezza-per-lhardware-ml",
    "href": "contents/core/privacy_security/privacy_security.it.html#minacce-alla-sicurezza-per-lhardware-ml",
    "title": "14¬† Sicurezza e Privacy",
    "section": "14.5 Minacce alla Sicurezza Per l‚ÄôHardware ML",
    "text": "14.5 Minacce alla Sicurezza Per l‚ÄôHardware ML\nL‚Äôhardware di apprendimento automatico embedded svolge un ruolo fondamentale nel potenziamento delle moderne applicazioni di IA, ma √® sempre pi√π esposto a una vasta gamma di minacce alla sicurezza. Queste vulnerabilit√† possono derivare da difetti nella progettazione hardware, manomissioni fisiche o persino dai complessi percorsi delle catene di fornitura globali. Per affrontare questi rischi √® necessaria una comprensione completa dei vari modi in cui l‚Äôintegrit√† hardware pu√≤ essere compromessa. Come riassunto in Tabella¬†14.1, questa sezione esplora le categorie chiave delle minacce hardware, offrendo approfondimenti sulle loro origini, metodi e implicazioni per i sistemi di ML.\n\n\n\nTabella¬†14.1: Tipi di minaccia alla sicurezza hardware.\n\n\n\n\n\n\n\n\n\n\nTipo di minaccia\nDescrizione\nRilevanza per la sicurezza hardware ML\n\n\n\n\nBug Hardware\nDifetti intrinseci nelle progettazioni hardware che possono compromettere l‚Äôintegrit√† del sistema.\nFondamento della vulnerabilit√† hardware.\n\n\nAttacchi Fisici\nSfruttamento diretto dell‚Äôhardware tramite accesso fisico o manipolazione.\nModello di minaccia basilare e palese.\n\n\nAttacchi di Injection di Guasti\nInduzione di guasti per causare errori nel funzionamento dell‚Äôhardware, portando a potenziali crash di sistema.\nManipolazione sistematica che porta al guasto.\n\n\nAttacchi a Canale Laterale\nSfruttamento di informazioni sul funzionamento dell‚Äôhardware per estrarre dati sensibili.\nAttacco indiretto tramite osservazione ambientale.\n\n\nInterfacce con Perdite\nVulnerabilit√† derivanti da interfacce che espongono i dati in modo involontario.\nEsposizione dei dati tramite canali di comunicazione.\n\n\nHardware Contraffatto\nUtilizzo di componenti hardware non autorizzati che potrebbero presentare falle di sicurezza.\nProblemi di vulnerabilit√† aggravati.\n\n\nRischi della Catena di Fornitura\nRischi introdotti durante il ciclo di vita dell‚Äôhardware, dalla produzione alla distribuzione.\nSfide di sicurezza cumulative e multiformi.\n\n\n\n\n\n\n\n14.5.1 Bug Hardware\nL‚Äôhardware non √® immune al problema pervasivo di difetti di progettazione o bug. Gli aggressori possono sfruttare queste vulnerabilit√† per accedere, manipolare o estrarre dati sensibili, violando la riservatezza e l‚Äôintegrit√† da cui dipendono utenti e servizi. Un esempio di tali vulnerabilit√† √® venuto alla luce con la scoperta di Meltdown e Spectre, due vulnerabilit√† hardware che sfruttano vulnerabilit√† critiche nei processori moderni. Questi bug consentono agli aggressori di aggirare la barriera hardware che separa le applicazioni, consentendo a un programma dannoso di leggere la memoria di altri programmi e del sistema operativo.\nMeltdown (Kocher et al. 2019a) e Spectre (Kocher et al. 2019b) funzionano sfruttando le ottimizzazioni nelle CPU moderne che consentono loro di eseguire istruzioni speculative fuori ordine prima che i controlli di validit√† siano stati completati. Ci√≤ rivela dati che dovrebbero essere inaccessibili, che l‚Äôattacco cattura tramite canali laterali come le cache. La complessit√† tecnica dimostra la difficolt√† di eliminare le vulnerabilit√† anche con una validazione estesa.\n\n‚Äî‚Äî‚Äî, et al. 2019a. ¬´Spectre Attacks: Exploiting Speculative Execution¬ª. In 2019 IEEE Symposium on Security and Privacy (SP), 1‚Äì19. IEEE. https://doi.org/10.1109/sp.2019.00002.\n\nKocher, Paul, Jann Horn, Anders Fogh, Daniel Genkin, Daniel Gruss, Werner Haas, Mike Hamburg, et al. 2019b. ¬´Spectre Attacks: Exploiting Speculative Execution¬ª. In 2019 IEEE Symposium on Security and Privacy (SP), 1‚Äì19. IEEE. https://doi.org/10.1109/sp.2019.00002.\nSe un sistema ML elabora dati sensibili, come informazioni personali degli utenti o analisi aziendali proprietarie, Meltdown e Spectre rappresentano un pericolo reale e presente per la sicurezza dei dati. Consideriamo il caso di una scheda acceleratrice ML progettata per velocizzare i processi di apprendimento automatico, come quelli di cui abbiamo parlato nel capitolo Accelerazione IA. Questi acceleratori lavorano con la CPU per gestire calcoli complessi, spesso correlati all‚Äôanalisi dei dati, al riconoscimento delle immagini e all‚Äôelaborazione del linguaggio naturale. Se una scheda acceleratrice di questo tipo presenta una vulnerabilit√† simile a Meltdown o Spectre, potrebbe far trapelare i dati che elabora. Un aggressore potrebbe sfruttare questa falla non solo per sottrarre dati, ma anche per ottenere informazioni sul funzionamento del modello ML, incluso potenzialmente il reverse engineering del modello stesso (tornando quindi al problema del furto di modelli.\nUno scenario reale in cui ci√≤ potrebbe essere devastante sarebbe nel settore sanitario. I sistemi ML elaborano regolarmente dati altamente sensibili dei pazienti per aiutare a diagnosticare, pianificare il trattamento e prevedere i risultati. Un bug nell‚Äôhardware del sistema potrebbe portare alla divulgazione non autorizzata di informazioni sanitarie personali, violando la privacy del paziente e contravvenendo a rigidi standard normativi come l‚ÄôHealth Insurance Portability and Accountability Act (HIPAA)\nLe vulnerabilit√† Meltdown e Spectre sono un duro promemoria del fatto che la sicurezza hardware non consiste solo nel prevenire l‚Äôaccesso fisico non autorizzato, ma anche nel garantire che l‚Äôarchitettura dell‚Äôhardware non diventi un canale per l‚Äôesposizione dei dati. Difetti di progettazione hardware simili emergono regolarmente in CPU, acceleratori, memoria, bus e altri componenti. Ci√≤ richiede continue mitigazioni retroattive e compromessi sulle prestazioni nei sistemi distribuiti. Soluzioni proattive come le architetture di elaborazione confidenziale potrebbero mitigare intere classi di vulnerabilit√† attraverso una progettazione hardware fondamentalmente pi√π sicura. Contrastare i bug hardware richiede rigore in ogni fase di progettazione, validazione e distribuzione.\n\n\n14.5.2 Attacchi Fisici\nLa manomissione fisica si riferisce alla manipolazione diretta e non autorizzata di risorse informatiche fisiche per minare l‚Äôintegrit√† dei sistemi di apprendimento automatico. √à un attacco particolarmente insidioso perch√© aggira le tradizionali misure di sicurezza informatica, che spesso si concentrano pi√π sulle vulnerabilit√† del software che sulle minacce hardware.\nLa manomissione fisica pu√≤ assumere molte forme, da quelle relativamente semplici, come l‚Äôinserimento di un dispositivo USB caricato con software dannoso in un server, a quelle altamente sofisticate, come l‚Äôinclusione di un Trojan hardware durante il processo di produzione di un microchip (discusso pi√π avanti in dettaglio nella sezione Supply Chain). I sistemi ML sono suscettibili a questo attacco perch√© si basano sull‚Äôaccuratezza e l‚Äôintegrit√† del loro hardware per elaborare e analizzare correttamente grandi quantit√† di dati.\nSi consideri un drone alimentato da ML utilizzato per la mappatura geografica. Il funzionamento del drone si basa su una serie di sistemi di bordo, tra cui un modulo di navigazione che elabora gli input da vari sensori per determinare il suo percorso. Se un aggressore ottiene l‚Äôaccesso fisico a questo drone, potrebbe sostituire il modulo di navigazione originale con uno compromesso che include una backdoor. Questo modulo manipolato potrebbe quindi alterare la traiettoria di volo del drone per condurre la sorveglianza su aree riservate o persino contrabbandare merci di contrabbando volando su rotte non rilevate.\nUn altro esempio √® la manomissione fisica degli scanner biometrici utilizzati per il controllo degli accessi in strutture sicure. Introducendo un sensore modificato che trasmette dati biometrici a un ricevitore non autorizzato, un aggressore pu√≤ accedere ai dati di identificazione personale per autenticare gli individui.\nEsistono diversi modi in cui la manomissione fisica pu√≤ verificarsi nell‚Äôhardware ML:\n\nManipolazione dei sensori: Si consideri un veicolo autonomo dotato di telecamere e LiDAR per la percezione ambientale. Un malintenzionato potrebbe manipolare deliberatamente l‚Äôallineamento fisico di questi sensori per creare zone di occlusione o distorcere le misure della distanza. Ci√≤ potrebbe compromettere le capacit√† di rilevamento degli oggetti e potenzialmente mettere in pericolo gli occupanti del veicolo.\nTrojan hardware: Le modifiche dannose ai circuiti possono introdurre trojan progettati per attivarsi in base a specifiche condizioni di input. Ad esempio, un chip acceleratore ML potrebbe funzionare come previsto fino a quando non incontra un trigger predeterminato, dopodich√© si comporta in modo irregolare.\nManomissione della memoria: L‚Äôesposizione fisica e la manipolazione dei chip di memoria potrebbero consentire l‚Äôestrazione di parametri del modello ML crittografati. Le tecniche di iniezione di guasti possono anche corrompere i dati del modello per degradare l‚Äôaccuratezza.\nIntroduzione di backdoor: Ottenendo l‚Äôaccesso fisico ai server, un avversario potrebbe utilizzare keylogger hardware per catturare password e creare account backdoor per l‚Äôaccesso persistente. Questi potrebbero poi essere utilizzati per esfiltrare dati di training ML nel tempo.\nAttacchi alla supply chain: Manipolare componenti hardware di terze parti o compromettere i canali di produzione e spedizione crea vulnerabilit√† sistemiche difficili da rilevare e correggere.\n\n\n\n14.5.3 Attacchi di Fault-injection\nIntroducendo intenzionalmente guasti nell‚Äôhardware ML, gli aggressori possono indurre errori nel processo di elaborazione, portando a output non corretti. Questa manipolazione compromette l‚Äôintegrit√† delle operazioni ML e pu√≤ fungere da vettore per ulteriori sfruttamenti, come il reverse engineering del sistema o il bypass del protocollo di sicurezza. L‚Äôiniezione di guasti comporta l‚Äôinterruzione deliberata delle operazioni di elaborazione standard in un sistema tramite interferenze esterne (Joye e Tunstall 2012). Attivando con precisione gli errori di elaborazione, gli avversari possono alterare l‚Äôesecuzione del programma in modi che degradano l‚Äôaffidabilit√† o trapelano informazioni sensibili.\n\nJoye, Marc, e Michael Tunstall. 2012. Fault Analysis in Cryptography. Springer Berlin Heidelberg. https://doi.org/10.1007/978-3-642-29656-7.\n\nBarenghi, Alessandro, Guido M. Bertoni, Luca Breveglieri, Mauro Pellicioli, e Gerardo Pelosi. 2010. ¬´Low voltage fault attacks to AES¬ª. In 2010 IEEE International Symposium on Hardware-Oriented Security and Trust (HOST), 7‚Äì12. IEEE; IEEE. https://doi.org/10.1109/hst.2010.5513121.\n\nHutter, Michael, Jorn-Marc Schmidt, e Thomas Plos. 2009. ¬´Contact-based fault injections and power analysis on RFID tags¬ª. In 2009 European Conference on Circuit Theory and Design, 409‚Äì12. IEEE; IEEE. https://doi.org/10.1109/ecctd.2009.5275012.\n\nAmiel, Frederic, Christophe Clavier, e Michael Tunstall. 2006. ¬´Fault Analysis of DPA-Resistant Algorithms¬ª. In Fault Diagnosis and Tolerance in Cryptography, 223‚Äì36. Springer; Springer Berlin Heidelberg. https://doi.org/10.1007/11889700\\_20.\n\nAgrawal, Dakshi, Selcuk Baktir, Deniz Karakoyunlu, Pankaj Rohatgi, e Berk Sunar. 2007. ¬´Trojan Detection using IC Fingerprinting¬ª. In 2007 IEEE Symposium on Security and Privacy (SP ‚Äô07), 296‚Äì310. Springer; IEEE. https://doi.org/10.1109/sp.2007.36.\n\nSkorobogatov, Sergei. 2009. ¬´Local heating attacks on Flash memory devices¬ª. In 2009 IEEE International Workshop on Hardware-Oriented Security and Trust, 1‚Äì6. IEEE; IEEE. https://doi.org/10.1109/hst.2009.5225028.\n\nSkorobogatov, Sergei P., e Ross J. Anderson. 2002. ¬´Optical Fault Induction Attacks.¬ª In Cryptographic Hardware and Embedded Systems-CHES 2002: 4th International Workshop Redwood Shores, CA, USA, August 13‚Äì15, 2002 Revised Papers 4, 2‚Äì12. Springer. https://doi.org/10.1007/3-540-36400-5\\_2.\nPer l‚Äôiniezione di guasti possono essere utilizzate varie tecniche di manomissione fisica. Bassa tensione (Barenghi et al. 2010), picchi di potenza (Hutter, Schmidt, e Plos 2009), anomalie di clock (Amiel, Clavier, e Tunstall 2006), impulsi elettromagnetici (Agrawal et al. 2007), aumento della temperatura (S. Skorobogatov 2009) e colpi laser (S. P. Skorobogatov e Anderson 2002) sono comuni vettori di attacco hardware. Sono programmati con precisione per indurre guasti come bit invertiti o istruzioni saltate durante operazioni critiche.\nPer i sistemi ML, le conseguenze includono una precisione del modello compromessa, negazione del servizio, estrazione di dati di training privati o parametri del modello e reverse engineering delle architetture del modello. Gli aggressori potrebbero utilizzare l‚Äôiniezione di guasti per forzare classificazioni errate, interrompere sistemi autonomi o rubare propriet√† intellettuale.\nAd esempio, Breier et al. (2018) ha iniettato con successo un ‚Äúfault attack‚Äù in una rete neurale profonda distribuita su un microcontrollore. Hanno utilizzato un laser per riscaldare transistor specifici, costringendoli a cambiare stato. In un caso, hanno utilizzato questo metodo per attaccare una funzione di attivazione ReLU, con il risultato che la funzione emetteva sempre un valore di 0, indipendentemente dall‚Äôinput. Nel codice assembly mostrato in Figura¬†14.3, l‚Äôattacco ha fatto s√¨ che il programma in esecuzione saltasse sempre l‚Äôistruzione jmp end alla riga 6. Ci√≤ significa che HiddenLayerOutput[i] √® sempre impostato su 0, sovrascrivendo eventuali valori scritti su di esso nelle righe 4 e 5. Di conseguenza, i neuroni mirati vengono resi inattivi, con conseguenti classificazioni errate.\n\n\n\n\n\n\nFigura¬†14.3: Iniezione di errore dimostrata con codice assembly. Fonte: Breier et al. (2018).\n\n\nBreier, Jakub, Xiaolu Hou, Dirmanto Jap, Lei Ma, Shivam Bhasin, e Yang Liu. 2018. ¬´DeepLaser: Practical Fault Attack on Deep Neural Networks¬ª. ArXiv preprint abs/1806.05859 (giugno). http://arxiv.org/abs/1806.05859v2.\n\n\nLa strategia di un aggressore potrebbe essere quella di dedurre informazioni sulle funzioni di attivazione tramite attacchi side-channel (discussi in seguito). Quindi, l‚Äôaggressore potrebbe tentare di colpire pi√π calcoli di funzioni di attivazione iniettando casualmente guasti nei livelli il pi√π vicino possibile al livello di output, aumentando la probabilit√† e l‚Äôimpatto dell‚Äôattacco.\nI dispositivi embedded sono particolarmente vulnerabili a causa di un limitato rafforzamento fisico e di vincoli di risorse che limitano le difese di runtime robuste. Senza un packaging antimanomissione, l‚Äôaccesso dell‚Äôaggressore ai bus di sistema e alla memoria consente di infierire guasti precisi. Anche i modelli ML embedded leggeri mancano di ridondanza per bypassare gli errori.\nQuesti attacchi possono essere particolarmente insidiosi perch√© aggirano le tradizionali misure di sicurezza basate su software, spesso non tenendo conto delle interruzioni fisiche. Inoltre, poich√© i sistemi ML si basano in larga misura sull‚Äôaccuratezza e l‚Äôaffidabilit√† del loro hardware per attivit√† come il riconoscimento di pattern, il processo decisionale e le risposte automatiche, qualsiasi compromesso nel loro funzionamento dovuto all‚Äôiniezione di guasti pu√≤ avere conseguenze gravi e di vasta portata.\nPer mitigare i rischi di iniezione di guasti √® necessario un approccio multi-layer. Il rafforzamento fisico tramite custodie antimanomissione e offuscamento del design aiuta a ridurre l‚Äôaccesso. Il rilevamento di leggere anomalie pu√≤ identificare input di sensori insoliti o output di modelli errati (Hsiao et al. 2023). Le memorie con correzione degli errori riducono al minimo le interruzioni, mentre la crittografia dei dati salvaguarda le informazioni. Le tecniche emergenti di watermarking dei modelli tracciano i parametri rubati.\n\nHsiao, Yu-Shun, Zishen Wan, Tianyu Jia, Radhika Ghosal, Abdulrahman Mahmoud, Arijit Raychowdhury, David Brooks, Gu-Yeon Wei, e Vijay Janapa Reddi. 2023. ¬´MAVFI: An End-to-End Fault Analysis Framework with Anomaly Detection and Recovery for Micro Aerial Vehicles¬ª. In 2023 Design, Automation &amp; Test in Europe Conference &amp; Exhibition (DATE), 1‚Äì6. IEEE; IEEE. https://doi.org/10.23919/date56975.2023.10137246.\nTuttavia, bilanciare protezioni robuste con i limiti di dimensioni e potenza ristretti dei sistemi embedded rimane una sfida. I limiti della crittografia e la mancanza di coprocessori sicuri su hardware embedded sensibile ai costi limitano le opzioni. In definitiva, la resilienza all‚Äôiniezione di guasti richiede una prospettiva multi-layer che abbraccia i layer di progettazione elettrica, firmware, software e fisica.\n\n\n14.5.4 Attacchi a canale laterale\nGli attacchi side-channel costituiscono una classe di violazioni della sicurezza che sfruttano informazioni rivelate inavvertitamente tramite l‚Äôimplementazione fisica dei sistemi informatici. Contrariamente agli attacchi diretti che prendono di mira vulnerabilit√† software o di rete, questi attacchi sfruttano le caratteristiche hardware intrinseche del sistema per estrarre informazioni sensibili.\nLa premessa fondamentale di un attacco side-channel √® che il funzionamento di un dispositivo pu√≤ rivelare inavvertitamente informazioni. Tali fughe possono provenire da varie fonti, tra cui l‚Äôenergia elettrica consumata da un dispositivo (Kocher, Jaffe, e Jun 1999), i campi elettromagnetici che emette (Gandolfi, Mourtel, e Olivier 2001), il tempo necessario per elaborare determinate operazioni o persino i suoni che produce. Ogni canale pu√≤ intravedere indirettamente i processi interni del sistema, rivelando informazioni che possono compromettere la sicurezza.\n\nKocher, Paul, Joshua Jaffe, e Benjamin Jun. 1999. ¬´Differential Power Analysis¬ª. In Advances in Cryptology ‚Äî CRYPTO‚Äô 99, 388‚Äì97. Springer; Springer Berlin Heidelberg. https://doi.org/10.1007/3-540-48405-1\\_25.\n\nGandolfi, Karine, Christophe Mourtel, e Francis Olivier. 2001. ¬´Electromagnetic Analysis: Concrete Results¬ª. In Cryptographic Hardware and Embedded Systems ‚Äî CHES 2001, 251‚Äì61. Springer; Springer Berlin Heidelberg. https://doi.org/10.1007/3-540-44709-1\\_21.\n\nKocher, Paul, Joshua Jaffe, Benjamin Jun, e Pankaj Rohatgi. 2011. ¬´Introduction to differential power analysis¬ª. Journal of Cryptographic Engineering 1 (1): 5‚Äì27. https://doi.org/10.1007/s13389-011-0006-y.\nSi consideri un sistema di apprendimento automatico che esegue transazioni crittografate. Gli algoritmi di crittografia sono progettati per proteggere i dati, ma richiedono un lavoro computazionale per crittografare e de-crittografare le informazioni. Uno standard di crittografia ampiamente utilizzato √® l‚ÄôAdvanced Encryption Standard (AES), che crittografa i dati per impedire l‚Äôaccesso non autorizzato. Tuttavia, gli aggressori possono analizzare i modelli di consumo energetico di un dispositivo che esegue la crittografia per dedurre informazioni sensibili, come la chiave crittografica. Con metodi statistici sofisticati, piccole variazioni nel consumo energetico durante il processo di crittografia possono essere correlate ai dati in fase di elaborazione, rivelando infine la chiave. Alcune tecniche di attacco di analisi differenziale sono ‚ÄúDifferential Power Analysis (DPA)‚Äù (Kocher et al. 2011), ‚ÄúDifferential Electromagnetic Analysis (DEMA)‚Äù e ‚ÄúCorrelation Power Analysis (CPA)‚Äù.\nUn aggressore che tenta di violare la crittografia AES potrebbe raccogliere tracce di potenza o elettromagnetiche (registrazioni di consumi o emissioni di energia) dal dispositivo mentre esegue la crittografia. Analizzando queste tracce con tecniche statistiche, l‚Äôaggressore potrebbe identificare correlazioni tra le tracce e il testo in chiaro (testo originale non crittografato) o il testo cifrato (testo crittografato). Queste correlazioni potrebbero quindi essere utilizzate per dedurre singoli bit della chiave AES e, alla fine, ricostruire l‚Äôintera chiave. Gli attacchi di analisi differenziale sono particolarmente pericolosi perch√© sono economici, efficaci e non intrusivi, consentendo agli aggressori di aggirare le misure di sicurezza algoritmiche e a livello hardware. Anche le compromissioni tramite questi attacchi sono difficili da rilevare, poich√© non alterano fisicamente il dispositivo n√© violano l‚Äôalgoritmo di crittografia stesso.\nDi seguito, una visualizzazione semplificata illustra come l‚Äôanalisi dei pattern di consumo energetico del dispositivo di crittografia pu√≤ aiutare a estrarre informazioni sulle operazioni dell‚Äôalgoritmo e, a sua volta, sui dati segreti. L‚Äôesempio mostra un dispositivo che accetta una password di 5 byte come input. La password immessa in questo scenario √® 0x61, 0x52, 0x77, 0x6A, 0x73, che rappresenta la password corretta. I modelli di consumo energetico durante l‚Äôautenticazione forniscono informazioni su come funziona l‚Äôalgoritmo.\nIn Figura¬†14.4, la forma d‚Äôonda rossa rappresenta le linee di dati seriali mentre il bootloader riceve i dati della password in blocchi (ad esempio 0x61, 0x52, 0x77, 0x6A, 0x73). Ciascun segmento etichettato (ad esempio, ‚ÄúData: 61‚Äù) corrisponde a un byte della password elaborata dall‚Äôalgoritmo di crittografia. Il grafico blu mostra il consumo energetico del dispositivo di crittografia mentre elabora ogni byte. Quando viene inserita la password corretta, il dispositivo elabora tutti i 5 byte con successo e il grafico della tensione blu mostra modelli coerenti in tutto. Questo grafico fornisce una linea di base per comprendere come appare il consumo energetico del dispositivo quando viene inserita una password corretta. Nelle figure successive, si vedr√† come cambia il profilo energetico con password errate, aiutando a individuare le differenze nel comportamento del dispositivo quando l‚Äôautenticazione fallisce.\n\n\n\n\n\n\nFigura¬†14.4: Profilo del consumo energetico del dispositivo durante le normali operazioni con una password valida di 5 byte (0x61, 0x52, 0x77, 0x6A, 0x73). La linea rossa rappresenta i dati seriali ricevuti dal bootloader, che in questa figura riceve i byte corretti. Notare come la linea blu, che rappresenta il consumo energetico durante l‚Äôautenticazione, corrisponda alla ricezione e alla verifica dei byte. Nelle figure successive, questo profilo di consumo energetico blu cambier√†. Fonte: Colin O‚ÄôFlynn.\n\n\n\nQuando viene inserita una password errata, il grafico dell‚Äôanalisi della potenza √® mostrato in Figura¬†14.5. I primi tre byte della password sono corretti (ad esempio 0x61, 0x52, 0x77). Di conseguenza, i pattern di tensione sono molto simili o identici tra i due grafici, fino al quarto byte incluso. Dopo aver elaborato il quarto byte (0x42), il dispositivo rileva una mancata corrispondenza con la password corretta e interrompe l‚Äôulteriore elaborazione. Ci√≤ determina un cambiamento evidente nel pattern di alimentazione, mostrato dal salto improvviso nella linea blu all‚Äôaumentare della tensione.\n\n\n\n\n\n\nFigura¬†14.5: Profilo di consumo energetico del dispositivo quando viene inserita una password errata di 5 byte (0x61, 0x52, 0x77, 0x42, 0x42). La linea rossa rappresenta i dati seriali ricevuti dal bootloader, che mostrano i byte di input in fase di elaborazione. I primi tre byte (0x61, 0x52, 0x77) sono corretti e corrispondono alla password prevista, come indicato dalla linea blu coerente del consumo energetico. Tuttavia, durante l‚Äôelaborazione del quarto byte (0x42), viene rilevata una mancata corrispondenza. Il bootloader interrompe l‚Äôulteriore elaborazione, con conseguente salto evidente nella linea blu del consumo energetico, poich√© il dispositivo interrompe l‚Äôautenticazione ed entra in uno stato di errore. Fonte: Colin O‚ÄôFlynn.\n\n\n\nLa Figura¬†14.6 mostra un altro esempio, ma in cui la password √® completamente errata (0x30, 0x30, 0x30, 0x30, 0x30), a differenza dell‚Äôesempio precedente con i primi tre byte corretti. Qui, il dispositivo identifica la mancata corrispondenza subito dopo l‚Äôelaborazione del primo byte e interrompe l‚Äôulteriore elaborazione. Ci√≤ si riflette nel profilo di consumo energetico, in cui la linea blu mostra un brusco salto dopo il primo byte, indicando la conclusione anticipata dell‚Äôautenticazione del dispositivo.\n\n\n\n\n\n\nFigura¬†14.6: Profilo di consumo energetico del dispositivo quando viene inserita una password completamente errata (0x30, 0x30, 0x30, 0x30, 0x30). La linea blu mostra un brusco salto dopo l‚Äôelaborazione del primo byte, indicando che il dispositivo ha interrotto il processo di autenticazione. Fonte: Colin O‚ÄôFlynn.\n\n\n\nL‚Äôesempio sopra dimostra come le informazioni sul processo di crittografia e sulla chiave segreta possono essere dedotte analizzando diversi input e varianti di test di forza bruta di ogni byte della password, in pratica ‚Äúintercettando‚Äù le operazioni del dispositivo. Per una spiegazione pi√π dettagliata, guardare Video¬†14.3 di seguito.\n\n\n\n\n\n\nVideo¬†14.3: Power Attack\n\n\n\n\n\n\nUn altro esempio √® un sistema ML per il riconoscimento vocale, che elabora i comandi vocali per eseguire azioni. Misurando la latenza del sistema per rispondere ai comandi o la potenza utilizzata durante l‚Äôelaborazione, un aggressore potrebbe dedurre quali comandi vengono elaborati e quindi apprendere i pattern operativi del sistema. Ancora pi√π sottilmente, il suono emesso dalla ventola o dal disco rigido di un computer potrebbe cambiare in risposta al carico di lavoro, che un microfono sensibile potrebbe captare e analizzare per determinare che tipo di operazioni vengono eseguite.\nIn scenari reali, gli attacchi side-channel hanno effettivamente estratto chiavi di crittografia e compromesso comunicazioni sicure. Uno dei primi casi registrati di un simile attacco si √® verificato negli anni ‚Äô60, quando l‚Äôagenzia di intelligence britannica MI5 ha affrontato la sfida di decifrare comunicazioni crittografate dall‚Äôambasciata egiziana a Londra. I loro sforzi di decifrazione dei codici sono stati inizialmente ostacolati dalle limitazioni computazionali dell‚Äôepoca, fino a quando un‚Äôingegnosa osservazione dell‚Äôagente MI5 Peter Wright ha alterato il corso dell‚Äôoperazione.\nL‚Äôagente dell‚ÄôMI5 Peter Wright propose di usare un microfono per catturare le sottili firme acustiche emesse dalla macchina di cifratura del rotore dell‚Äôambasciata durante la crittografia (Burnet e Thomas 1989). I distinti clic meccanici dei rotori mentre gli operatori li configuravano quotidianamente facevano trapelare informazioni critiche sulle impostazioni iniziali. Questo semplice ‚Äúcanale laterale‚Äù del suono ha permesso all‚ÄôMI5 di ridurre drasticamente la complessit√† della decifrazione dei messaggi. Questo primo attacco di ‚Äúperdita‚Äù acustica evidenzia che gli attacchi a canale laterale non sono semplicemente una novit√† dell‚Äôera digitale, ma una continuazione di antichi principi di crittoanalisi. L‚Äôidea che dove c‚Äô√® un segnale, c‚Äô√® un‚Äôopportunit√† di intercettazione rimane fondamentale. Dai clic meccanici alle fluttuazioni elettriche e oltre, i canali laterali consentono agli avversari di estrarre segreti indirettamente attraverso un‚Äôattenta analisi del segnale.\n\nBurnet, David, e Richard Thomas. 1989. ¬´Spycatcher: The Commodification of Truth¬ª. Journal of Law and Society 16 (2): 210. https://doi.org/10.2307/1410360.\n\nAsonov, D., e R. Agrawal. s.d. ¬´Keyboard acoustic emanations¬ª. In IEEE Symposium on Security and Privacy, 2004. Proceedings. 2004, 3‚Äì11. IEEE; IEEE. https://doi.org/10.1109/secpri.2004.1301311.\n\nGnad, Dennis R. E., Fabian Oboril, e Mehdi B. Tahoori. 2017. ¬´Voltage drop-based fault attacks on FPGAs using valid bitstreams¬ª. In 2017 27th International Conference on Field Programmable Logic and Applications (FPL), 1‚Äì7. IEEE; IEEE. https://doi.org/10.23919/fpl.2017.8056840.\n\nZhao, Mark, e G. Edward Suh. 2018. ¬´FPGA-Based Remote Power Side-Channel Attacks¬ª. In 2018 IEEE Symposium on Security and Privacy (SP), 229‚Äì44. IEEE; IEEE. https://doi.org/10.1109/sp.2018.00049.\nOggi, la crittoanalisi acustica si √® evoluta in attacchi come l‚Äôintercettazione della tastiera (Asonov e Agrawal, s.d.). I canali laterali elettrici spaziano dall‚Äôanalisi della potenza su hardware crittografico (Gnad, Oboril, e Tahoori 2017) alle fluttuazioni di tensione (Zhao e Suh 2018) su acceleratori di machine learning. Anche tempistiche, emissioni elettromagnetiche e persino impronte di calore possono essere sfruttate. Nuovi e inaspettati canali laterali emergono spesso man mano che l‚Äôinformatica diventa pi√π interconnessa e miniaturizzata.\nProprio come la ‚Äúperdita‚Äù acustica analogica dell‚ÄôMI5 ha trasformato la loro decifrazione dei codici, i moderni attacchi ai canali laterali aggirano i confini tradizionali della difesa informatica. Comprendere lo spirito creativo e la persistenza storica degli exploit dei canali laterali √® una conoscenza fondamentale per sviluppatori e difensori che cercano di proteggere in modo completo i moderni sistemi di apprendimento automatico dalle minacce digitali e fisiche.\n\n\n14.5.5 Interfacce con Perdite\nLe interfacce ‚Äúleaky‚Äù nei sistemi embedded sono spesso backdoor trascurate che possono trasformarsi in significative vulnerabilit√† di sicurezza. Sebbene progettate per scopi legittimi come comunicazione, manutenzione o debug, queste interfacce possono inavvertitamente fornire agli aggressori una finestra attraverso la quale estrarre informazioni sensibili o iniettare dati dannosi.\nUn‚Äôinterfaccia diventa ‚Äúleaky‚Äù quando espone pi√π informazioni del dovuto, spesso a causa della mancanza di rigorosi controlli di accesso o di una schermatura inadeguata dei dati trasmessi. Ecco alcuni esempi concreti di problemi di interfaccia leaky che causano problemi di sicurezza in dispositivi IoT ed embedded:\n\nBaby Monitor: Molti baby monitor abilitati al WiFi hanno interfacce non protette per l‚Äôaccesso remoto. Ci√≤ ha consentito agli aggressori di ottenere feed audio e video in tempo reale dalle case delle persone, rappresentando una grave violazione della privacy.\nPacemaker: Sono state scoperte vulnerabilit√† dell‚Äôinterfaccia in alcuni pacemaker che potrebbero consentire agli aggressori di manipolare le funzioni cardiache se sfruttate. Ci√≤ presenta uno scenario potenzialmente letale.\nLampadine Smart: Un ricercatore ha scoperto di poter accedere a dati non crittografati da lampadine intelligenti tramite un‚Äôinterfaccia di debug, comprese le credenziali WiFi, consentendogli di accedere alla rete connessa (Greengard 2021).\nAuto Smart: Se non protetta, la porta di diagnostica OBD-II ha dimostrato di fornire un vettore di attacco ai sistemi automobilistici. Gli aggressori potrebbero usarlo per controllare i freni e altri componenti (Miller e Valasek 2015).\n\n\nGreengard, Samuel. 2021. The Internet of Things. The MIT Press. https://doi.org/10.7551/mitpress/13937.001.0001.\n\nMiller, Charlie, e Chris Valasek. 2015. ¬´Remote exploitation of an unaltered passenger vehicle¬ª. Black Hat USA 2015 (S 91): 1‚Äì91.\nSebbene quanto sopra non sia direttamente collegato al ML, si consideri l‚Äôesempio di un sistema di casa intelligente con un componente ML embedded che controlla la sicurezza domestica in base a pattern di comportamento che apprende nel tempo. Il sistema include un‚Äôinterfaccia di manutenzione accessibile tramite la rete locale per aggiornamenti software e controlli di sistema. Se questa interfaccia non richiede un‚Äôautenticazione forte o i dati trasmessi tramite essa non sono crittografati, un aggressore sulla stessa rete potrebbe ottenere l‚Äôaccesso. Potrebbero quindi intercettare le routine quotidiane del proprietario di casa o riprogrammare le impostazioni di sicurezza manipolando il firmware.\nTali ‚Äúfughe‚Äù rappresentano un problema di privacy e un potenziale punto di ingresso per exploit pi√π dannosi. L‚Äôesposizione di dati di training, parametri del modello o output ML da una ‚Äúfuga‚Äù potrebbe aiutare gli avversari a costruire esempi avversari o a sottoporre a reverse engineering i modelli. L‚Äôaccesso tramite un‚Äôinterfaccia con ‚Äúperdite‚Äù potrebbe anche essere utilizzato per modificare il firmware di un dispositivo embedded, caricandolo con codice dannoso che potrebbe spegnerlo, intercettare dati o utilizzarlo in attacchi botnet.\nPer mitigare questi rischi, √® necessario un approccio multi-strato, che comprenda controlli tecnici quali autenticazione, crittografia, rilevamento delle anomalie, policy e processi come inventari di interfaccia, controlli di accesso, auditing e pratiche di sviluppo sicure. Disattivare le interfacce non necessarie e compartimentare i rischi tramite un modello zero-trust fornisce una protezione aggiuntiva.\nCome progettisti di sistemi ML embedded, dovremmo valutare le interfacce nelle prime fasi dello sviluppo e monitorarle continuamente dopo l‚Äôimplementazione come parte di un ciclo di vita della sicurezza end-to-end. Comprendere e proteggere le interfacce √® fondamentale per garantire la sicurezza complessiva del ML embedded.\n\n\n14.5.6 Hardware Contraffatto\nI sistemi ML sono affidabili solo quanto l‚Äôhardware sottostante. In un‚Äôepoca in cui i componenti hardware sono beni di consumo globali, l‚Äôaumento di hardware contraffatti o clonati rappresenta una sfida significativa. L‚Äôhardware contraffatto comprende tutti i componenti che sono riproduzioni non autorizzate di parti originali. I componenti contraffatti si infiltrano nei sistemi ML attraverso complesse catene di fornitura che si estendono oltre i confini e coinvolgono numerose fasi dalla produzione alla consegna.\nAnche una sola mancanza di integrit√† nella catena di fornitura pu√≤ comportare l‚Äôinserimento di parti contraffatte, progettate per imitare fedelmente le funzioni e l‚Äôaspetto dell‚Äôhardware originale. Ad esempio, un sistema di riconoscimento facciale per il controllo degli accessi ad alta sicurezza potrebbe essere compromesso se dotato di processori contraffatti. Questi processori potrebbero non riuscire a elaborare e verificare accuratamente i dati biometrici, consentendo potenzialmente a persone non autorizzate di accedere ad aree riservate.\nLa sfida con l‚Äôhardware contraffatto √® multiforme. Compromette la qualit√† e l‚Äôaffidabilit√† dei sistemi ML, poich√© questi componenti potrebbero degradarsi pi√π rapidamente o funzionare in modo imprevedibile a causa di una produzione scadente. Anche i rischi per la sicurezza sono profondi; l‚Äôhardware contraffatto pu√≤ contenere vulnerabilit√† pronte per essere sfruttate da malintenzionati. Ad esempio, un router di rete clonato in un data center ML potrebbe includere una backdoor nascosta, consentendo l‚Äôintercettazione dei dati o l‚Äôintrusione nella rete senza essere rilevati.\nInoltre, l‚Äôhardware contraffatto comporta rischi legali e di conformit√†. Le aziende che utilizzano inavvertitamente parti contraffatte nei loro sistemi ML possono affrontare gravi ripercussioni legali, tra cui multe e sanzioni per il mancato rispetto delle normative e degli standard del settore. Ci√≤ √® particolarmente vero per i settori in cui √® obbligatoria la conformit√† a specifiche normative sulla sicurezza e sulla privacy, come l‚Äôassistenza sanitaria e la finanza.\nLe pressioni economiche per ridurre i costi aggravano il problema dell‚Äôhardware contraffatto e costringono le aziende ad approvvigionarsi da fornitori a basso costo, privi di rigorosi processi di verifica. Questa economia pu√≤ introdurre inavvertitamente parti contraffatte in sistemi altrimenti sicuri. Inoltre, rilevare queste contraffazioni √® intrinsecamente complicato poich√© vengono create per passare per componenti originali, il che spesso richiede attrezzature e competenze sofisticate per essere identificate.\nNel campo dell‚Äôapprendimento automatico, dove decisioni in tempo reale e calcoli complessi sono la norma, le implicazioni di un guasto hardware possono essere scomode e potenzialmente pericolose. √à fondamentale che le parti interessate siano pienamente consapevoli di questi rischi. Le sfide poste dall‚Äôhardware contraffatto richiedono una comprensione completa delle attuali minacce all‚Äôintegrit√† del sistema di apprendimento automatico. Ci√≤ sottolinea la necessit√† di una gestione proattiva e informata del ciclo di vita dell‚Äôhardware all‚Äôinterno di questi sistemi avanzati.\n\n\n14.5.7 Rischi della Catena di Fornitura\nLa minaccia dell‚Äôhardware contraffatto √® strettamente legata alle vulnerabilit√† pi√π ampie della supply chain [catena di fornitura]. Le supply chain globalizzate e interconnesse creano molteplici opportunit√† per componenti compromessi di infiltrarsi nel ciclo di vita di un prodotto. Le supply chain coinvolgono numerose entit√†, dalla progettazione alla produzione, all‚Äôassemblaggio, alla distribuzione e all‚Äôintegrazione. Una mancanza di trasparenza e supervisione di ogni partner rende difficile la verifica dell‚Äôintegrit√† a ogni passaggio. Le lacune in qualsiasi punto della catena possono consentire l‚Äôinserimento di parti contraffatte.\nAd esempio, un produttore su contratto potrebbe ricevere e includere inconsapevolmente rifiuti elettronici riciclati contenenti contraffazioni pericolose. Un distributore inaffidabile potrebbe introdurre di nascosto componenti clonati. Le minacce interne a qualsiasi fornitore potrebbero deliberatamente mescolare contraffazioni in spedizioni legittime.\nUna volta che le contraffazioni entrano nel flusso di fornitura, passano rapidamente attraverso pi√π mani prima di finire nei sistemi ML in cui il rilevamento √® difficile. Le contraffazioni avanzate come parti ricondizionate o cloni con esterni riconfezionati possono mascherarsi da componenti autentici, superando l‚Äôispezione visiva.\nPer identificare i falsi, spesso √® richiesta una profilazione tecnica completa tramite micrografia, screening a raggi X, analisi forense dei componenti e test funzionali. Tuttavia, un‚Äôanalisi cos√¨ costosa non √® pratica per gli acquisti su larga scala.\nStrategie come audit della supply chain, screening dei fornitori, convalida della provenienza dei componenti e aggiunta di protezioni antimanomissione possono aiutare a mitigare i rischi. Tuttavia, date le sfide globali alla sicurezza della supply chain, un approccio zero-trust √® prudente. Progettare sistemi ML per utilizzare controlli ridondanti, fail-safe e monitoraggio continuo del runtime fornisce resilienza contro i compromessi dei componenti.\nUna rigorosa convalida delle sorgenti hardware abbinata ad architetture di sistema fault-tolerant offre la difesa pi√π solida contro i rischi pervasivi di supply chain globali contorte e opache.\n\n\n14.5.8 Caso di Studio: Una Chiamata di Risveglio per la Sicurezza Hardware\nNel 2018, Bloomberg Businessweek ha pubblicato una storia allarmante che ha attirato molta attenzione nel mondo della tecnologia. L‚Äôarticolo sosteneva che Supermicro aveva segretamente impiantato minuscoli chip spia nell‚Äôhardware del server. I giornalisti hanno affermato che gli hacker statali cinesi che lavoravano con Supermicro potevano infilare questi minuscoli chip nelle schede madri durante la produzione. I minuscoli chip avrebbero presumibilmente dato agli hacker un accesso backdoor ai server utilizzati da oltre 30 grandi aziende, tra cui Apple e Amazon.\nSe fosse vero, ci√≤ consentirebbe agli hacker di spiare dati privati o persino manomettere i sistemi. Tuttavia, dopo aver indagato, Apple e Amazon non hanno trovato prove dell‚Äôesistenza di tale hardware Supermicro hackerato. Altri esperti hanno messo in dubbio l‚Äôaccuratezza dell‚Äôarticolo di Bloomberg.\nSe la storia sia del tutto accurata o meno non √® una nostra preoccupazione da un punto di vista pedagogico. Tuttavia, questo incidente ha attirato l‚Äôattenzione sui rischi delle catene di fornitura globali per l‚Äôhardware prodotto principalmente in Cina. Quando le aziende esternalizzano e acquistano componenti hardware da fornitori in tutto il mondo, √® necessario che vi sia maggiore visibilit√† nel processo. In questa complessa pipeline globale, si teme che hardware contraffatti o manomessi possano essere introdotti da qualche parte lungo il percorso senza che le aziende tecnologiche se ne accorgano. Le aziende che si affidano troppo a singoli produttori o distributori creano rischi. Ad esempio, a causa dell‚Äôeccessiva dipendenza da TSMC per la produzione di semiconduttori, gli Stati Uniti hanno investito 50 miliardi di dollari nel CHIPS Act.\nMan mano che l‚Äôapprendimento automatico si sposta in sistemi pi√π critici, √® fondamentale verificare l‚Äôintegrit√† dell‚Äôhardware dalla progettazione alla produzione e alla consegna. La backdoor Supermicro segnalata ha dimostrato che per la sicurezza dell‚Äôapprendimento automatico non possiamo dare per scontate le catene di fornitura e la produzione globali. Dobbiamo ispezionare e convalidare l‚Äôhardware a ogni collegamento della catena.",
    "crumbs": [
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Sicurezza e Privacy</span>"
    ]
  },
  {
    "objectID": "contents/core/privacy_security/privacy_security.it.html#sicurezza-hardware-del-ml-embedded",
    "href": "contents/core/privacy_security/privacy_security.it.html#sicurezza-hardware-del-ml-embedded",
    "title": "14¬† Sicurezza e Privacy",
    "section": "14.6 Sicurezza Hardware del ML Embedded",
    "text": "14.6 Sicurezza Hardware del ML Embedded\n\n14.6.1 Trusted Execution Environments\n\nInformazioni su TEE\nUn Trusted Execution Environment (TEE) √® un‚Äôarea protetta all‚Äôinterno di un processore host che garantisce l‚Äôesecuzione sicura del codice e la protezione dei dati sensibili. Isolando le attivit√† critiche dal sistema operativo, i TEE resistono agli attacchi software e hardware, fornendo un ambiente protetto per la gestione di calcoli sensibili.\n\n\nVantaggi\nI TEE sono particolarmente preziosi in scenari in cui devono essere elaborati dati sensibili o in cui l‚Äôintegrit√† delle operazioni di un sistema √® critica. Nel contesto dell‚Äôhardware ML, i TEE assicurano che gli algoritmi e i dati ML siano protetti da manomissioni e ‚Äúperdite‚Äù. Ci√≤ √® essenziale perch√© i modelli ML elaborano spesso informazioni private, segreti commerciali o dati che potrebbero essere sfruttati se esposti.\nAd esempio, un TEE pu√≤ proteggere i parametri del modello ML dall‚Äôestrazione da parte di software dannosi sullo stesso dispositivo. Questa protezione √® fondamentale per la privacy e il mantenimento dell‚Äôintegrit√† del sistema ML, assicurando che i modelli funzionino come previsto e non forniscano output distorti a causa di parametri manipolati. Secure Enclave di Apple, presente in iPhone e iPad, √® una forma di TEE che fornisce un ambiente isolato per proteggere i dati sensibili degli utenti e le operazioni crittografiche.\nI Trusted Execution Environment (TEE) sono essenziali per i settori che richiedono elevati livelli di sicurezza, tra cui telecomunicazioni, finanza, sanit√† e automotive. I TEE proteggono l‚Äôintegrit√† delle reti 5G nelle telecomunicazioni e supportano applicazioni critiche. Nella finanza, proteggono i pagamenti mobili e i processi di autenticazione. L‚Äôassistenza sanitaria si affida ai TEE per salvaguardare i dati sensibili dei pazienti, mentre il settore automobilistico dipende da loro per la sicurezza e l‚Äôaffidabilit√† dei sistemi autonomi. In tutti i settori, i TEE garantiscono la riservatezza e l‚Äôintegrit√† dei dati e delle operazioni.\nNei sistemi ML, i TEE possono:\n\nEseguire in modo sicuro l‚Äôaddestramento e l‚Äôinferenza del modello, assicurando che i risultati del calcolo rimangano riservati.\nProteggere la riservatezza dei dati di input, come le informazioni biometriche, utilizzati per l‚Äôidentificazione personale o per attivit√† di classificazione sensibili.\nProteggere i modelli ML impedendo il reverse engineering, che pu√≤ proteggere le informazioni proprietarie e mantenere un vantaggio competitivo.\nAbilitare aggiornamenti sicuri ai modelli ML, assicurando che gli aggiornamenti provengano da una fonte attendibile e non siano stati manomessi durante il transito.\nRafforzare la sicurezza della rete salvaguardando la trasmissione dei dati tra componenti ML distribuiti tramite crittografia ed elaborazione sicura in-TEE.\n\nL‚Äôimportanza dei TEE nella sicurezza hardware ML deriva dalla loro capacit√† di proteggere da minacce esterne e interne, tra cui le seguenti:\n\nSoftware Dannoso: I TEE possono impedire al malware ad alto privilegio di accedere alle aree sensibili del sistema ML.\nManomissione Fisica: Integrandosi con le misure di sicurezza hardware, i TEE possono proteggere dalla manomissione fisica che tenta di aggirare la sicurezza del software.\nAttacchi Side-Channel: Sebbene non siano impenetrabili, i TEE possono mitigare specifici attacchi side-channel controllando l‚Äôaccesso a operazioni sensibili e modelli di dati.\nMinacce di Rete: I TEE migliorano la sicurezza della rete salvaguardando la trasmissione dei dati tra componenti ML distribuiti tramite crittografia ed elaborazione in-TEE sicura. Ci√≤ impedisce efficacemente gli attacchi ‚Äúman-in-the-middle‚Äù e garantisce che i dati vengano trasmessi tramite canali attendibili.\n\n\n\nMeccanica\nI fondamenti dei TEE contengono quattro parti principali:\n\nEsecuzione Isolata: Il codice all‚Äôinterno di un TEE viene eseguito in un ambiente separato dal sistema operativo host del dispositivo host. Questo isolamento protegge il codice dall‚Äôaccesso non autorizzato da parte di altre applicazioni.\nArchiviazione Sicura: I TEE possono archiviare in modo sicuro chiavi crittografiche, token di autenticazione e dati sensibili, impedendo alle applicazioni normali di accedervi al di fuori del TEE.\nProtezione dell‚ÄôIntegrit√†: I TEE possono verificare l‚Äôintegrit√† del codice e dei dati, assicurando che non siano stati alterati prima dell‚Äôesecuzione o durante l‚Äôarchiviazione.\nCrittografia dei Dati: I dati gestiti all‚Äôinterno di un TEE possono essere crittografati, rendendoli illeggibili per entit√† senza le chiavi appropriate, che sono anch‚Äôesse gestite all‚Äôinterno del TEE.\n\nEcco alcuni esempi di TEE che forniscono sicurezza basata su hardware per applicazioni sensibili:\n\nARMTrustZone: Questa tecnologia crea ambienti di esecuzione sicuri e normali isolati tramite controlli hardware e implementati in molti chipset mobili. mobile chipsets.\nIntelSGX: Le estensioni Software Guard di Intel forniscono un‚Äôenclave per l‚Äôesecuzione del codice che protegge da varie minacce basate sul software, prendendo di mira in modo specifico le vulnerabilit√† del livello del sistema operativo. Vengono utilizzate per salvaguardare i carichi di lavoro nel cloud.\nQualcomm Secure Execution Environment: Un sandbox hardware su chipset Qualcomm per app di pagamento e autenticazione mobili.\nApple SecureEnclave: Un TEE per la gestione dei dati biometrici e delle chiavi crittografiche su iPhone e iPad, che facilita i pagamenti mobili sicuri.\n\nFigura¬†14.7 √® un diagramma che mostra un‚Äôenclave sicura isolata dal processore host per fornire un ulteriore livello di sicurezza. L‚Äôenclave sicura ha una ROM di avvio per stabilire una ‚Äúroot‚Äù hardware di attendibilit√†, un motore AES per operazioni crittografiche efficienti e sicure e memoria protetta. Ha anche un meccanismo per memorizzare le informazioni in modo sicuro su un archivio collegato separato da quello flash NAND utilizzato dal processore applicativo e dal sistema operativo. La NAND flash √® un tipo di storage non volatile utilizzato in dispositivi come SSD, smartphone e tablet per conservare i dati anche quando sono spenti. Isolando i dati sensibili dallo storage NAND a cui accede il sistema principale, questo design garantisce che i dati degli utenti rimangano protetti anche se il kernel del processore applicativo √® compromesso.\n\n\n\n\n\n\nFigura¬†14.7: Enclave sicura System-on-chip. Fonte: Apple.\n\n\n\n\n\nCompromessi\nSebbene i ‚ÄúTrusted Execution Environment‚Äù offrano notevoli vantaggi in termini di sicurezza, la loro implementazione comporta dei compromessi. Diversi fattori influenzano se un sistema include un TEE:\nCosto: L‚Äôimplementazione dei TEE comporta costi aggiuntivi. Ci sono costi diretti per l‚Äôhardware e costi indiretti associati allo sviluppo e alla manutenzione di software sicuro per i TEE. Questi costi potrebbero essere giustificabili solo per alcuni dispositivi, in particolare prodotti a basso margine.\nComplessit√†: I TEE aggiungono complessit√† alla progettazione e allo sviluppo del sistema. L‚Äôintegrazione di un TEE con sistemi esistenti richiede una sostanziale ri-progettazione dello stack hardware e software, che pu√≤ rappresentare un ostacolo, in particolare per i sistemi legacy.\nPerformance Overhead: I TEE possono introdurre un overhead di prestazioni dovuto ai passaggi aggiuntivi coinvolti nella crittografia e nella verifica dei dati, che potrebbero rallentare le applicazioni sensibili al tempo.\nSfide di Sviluppo: Lo sviluppo per i TEE richiede conoscenze specialistiche e spesso deve rispettare rigidi protocolli di sviluppo. Ci√≤ pu√≤ estendere i tempi di sviluppo e complicare i processi di debug e test.\nScalabilit√† e Flessibilit√†: I TEE, a causa della loro natura protetta, possono imporre limitazioni di scalabilit√† e flessibilit√†. L‚Äôaggiornamento dei componenti protetti o il ridimensionamento del sistema per pi√π utenti o dati pu√≤ essere pi√π impegnativo quando tutto deve passare attraverso un ambiente protetto e chiuso.\nConsumo Energetico: L‚Äôelaborazione aumentata richiesta per la crittografia, la decrittografia e i controlli di integrit√† possono portare a un maggiore consumo energetico, una preoccupazione significativa per i dispositivi alimentati a batteria.\nDomanda del Mercato: Non tutti i mercati o le applicazioni richiedono il livello di sicurezza fornito dai TEE. Per molte applicazioni consumer, il rischio percepito potrebbe essere abbastanza basso da indurre i produttori a scegliere di non includere TEE nei loro progetti.\nCertificazione e Garanzia di Sicurezza: I sistemi con TEE potrebbero aver bisogno di rigorose certificazioni di sicurezza con enti come Common Criteria (CC) o European Union Agency for Cybersecurity (ENISA), che possono essere lunghe e costose. Alcune organizzazioni potrebbero scegliere di astenersi dall‚Äôimplementare TEE per evitare questi ostacoli.\nDispositivi con risorse limitate: I dispositivi con potenza di elaborazione, memoria o archiviazione limitate potrebbero supportare solo TEE senza compromettere la loro funzionalit√† primaria.\n\n\n\n14.6.2 Avvio Sicuro\n\nInformazioni\nSecure Boot √® uno standard di sicurezza fondamentale che garantisce che un dispositivo si avvii solo tramite software ritenuto attendibile dal produttore del dispositivo. Durante l‚Äôavvio, il firmware controlla la firma digitale di ogni componente software di avvio, inclusi bootloader, kernel e sistema operativo di base. Questo processo verifica che il software non sia stato alterato o manomesso. Se una firma non supera la verifica, il processo di avvio viene interrotto per impedire l‚Äôesecuzione di codice non autorizzato che potrebbe compromettere l‚Äôintegrit√† della sicurezza del sistema.\n\n\nVantaggi\nL‚Äôintegrit√† di un sistema ML embedded √® fondamentale fin dal momento dell‚Äôaccensione. Qualsiasi compromissione nel processo di avvio pu√≤ portare all‚Äôesecuzione di software dannoso prima che il sistema operativo e le applicazioni ML inizino, con conseguenti operazioni ML manipolate, accesso ai dati non autorizzato o riutilizzo del dispositivo per attivit√† dannose come botnet o crypto-mining.\nSecure Boot offre protezioni vitali per l‚Äôhardware ML embedded tramite i seguenti meccanismi critici:\n\nProtezione dei Dati ML: Garantire che i dati utilizzati dai modelli ML, che possono includere informazioni private o sensibili, non siano esposti a manomissioni o furti durante il processo di boot [avvio].\nProtezione dell‚ÄôIntegrit√† del Modello: Mantenere l‚Äôintegrit√† dei modelli ML √® fondamentale, poich√© la loro manomissione potrebbe portare a risultati errati o dannosi.\nAggiornamenti Sicuri del Modello: Abilitare aggiornamenti sicuri per modelli e algoritmi ML, assicurando che gli aggiornamenti siano autenticati e non siano stati alterati.\n\n\n\nMeccanica\nSecure Boot funziona con i TEE per migliorare ulteriormente la sicurezza del sistema. Figura¬†14.8 illustra un diagramma di flusso di un sistema embedded affidabile. Nella fase di validazione iniziale, Secure Boot verifica che il codice in esecuzione nel TEE sia la versione corretta e non manomessa autorizzata dal produttore del dispositivo. Controllando le firme digitali del firmware e di altri componenti critici del sistema, Secure Boot impedisce modifiche non autorizzate che potrebbero compromettere le capacit√† di sicurezza del TEE. Ci√≤ stabilisce una base di fiducia su cui il TEE pu√≤ eseguire in modo sicuro operazioni sensibili come la gestione delle chiavi crittografiche e l‚Äôelaborazione sicura dei dati. Applicando questi livelli di sicurezza, Secure Boot consente operazioni dei dispositivi sicure e resilienti anche negli ambienti con risorse pi√π limitate.\n\n\n\n\n\n\nFigura¬†14.8: Flusso di Secure Boot. Fonte: R. V. e A. (2018).\n\n\nR. V., Rashmi, e Karthikeyan A. 2018. ¬´Secure boot of Embedded Applications - A Review¬ª. In 2018 Second International Conference on Electronics, Communication and Aerospace Technology (ICECA), 291‚Äì98. IEEE. https://doi.org/10.1109/iceca.2018.8474730.\n\n\n\n\nCaso di Studio: Face ID di Apple\nUn esempio concreto dell‚Äôapplicazione di Secure Boot pu√≤ essere osservato nella tecnologia Face ID di Apple, che utilizza algoritmi di apprendimento automatico avanzati per abilitare il riconoscimento facciale su iPhone e iPad. Face ID si basa su una sofisticata integrazione di sensori e software per mappare con precisione la geometria del volto di un utente. Affinch√© Face ID funzioni in modo sicuro e protegga i dati biometrici degli utenti, le operazioni del dispositivo devono essere affidabili fin dall‚Äôinizializzazione. √à qui che Secure Boot svolge un ruolo fondamentale. Di seguito viene descritto come funziona Secure Boot insieme a Face ID:\n\nVerifica Iniziale: All‚Äôavvio di un iPhone, il processo Secure Boot inizia all‚Äôinterno di Secure Enclave, un coprocessore specializzato progettato per aggiungere un ulteriore livello di sicurezza. Secure Enclave gestisce i dati biometrici, come le impronte digitali per Touch ID e i dati di riconoscimento facciale per Face ID. Durante il processo di avvio, il sistema verifica rigorosamente che Apple abbia firmato digitalmente il firmware di Secure Enclave, garantendone l‚Äôautenticit√†. Questa fase di verifica assicura che il firmware utilizzato per elaborare i dati biometrici rimanga sicuro e senza essere compromesso.\nControlli di sicurezza continui: Dopo l‚Äôinizializzazione e la convalida del sistema da parte di Secure Boot, Secure Enclave comunica con il processore centrale del dispositivo per mantenere una catena di avvio sicura. Durante questo processo, le firme digitali del kernel iOS e di altri componenti di avvio critici vengono meticolosamente verificate per garantirne l‚Äôintegrit√† prima di procedere. Questo modello di ‚Äúcatena di fiducia‚Äù impedisce efficacemente modifiche non autorizzate al bootloader e al sistema operativo, salvaguardando la sicurezza complessiva del dispositivo.\nElaborazione dei Dati del Viso: Una volta completata la sequenza di avvio sicura, Secure Enclave interagisce in modo sicuro con gli algoritmi di apprendimento automatico che alimentano Face ID. Il riconoscimento facciale prevede la proiezione e l‚Äôanalisi di oltre 30.000 punti invisibili per creare una mappa di profondit√† del volto dell‚Äôutente e un‚Äôimmagine a infrarossi. Questi dati vengono convertiti in una rappresentazione matematica e confrontati in modo sicuro con i dati del volto registrati e archiviati in Secure Enclave.\nSecure Enclave e Protezione dei Dati: Secure Enclave √® progettato con precisione per proteggere i dati sensibili e gestire le operazioni crittografiche che salvaguardano tali dati. Anche in caso di kernel del sistema operativo compromesso, i dati del volto elaborati tramite Face ID rimangono inaccessibili ad applicazioni non autorizzate o ad aggressori esterni. √à importante sottolineare che i dati di Face ID non vengono mai trasmessi dal dispositivo e non vengono archiviati su iCloud o altri server esterni.\nAggiornamenti Firmware: Apple rilascia frequentemente aggiornamenti per risolvere le vulnerabilit√† della sicurezza e migliorare la funzionalit√† del sistema. Secure Boot garantisce che tutti gli aggiornamenti del firmware siano autenticati, consentendo l‚Äôinstallazione solo di quelli firmati da Apple. Questo processo aiuta a preservare l‚Äôintegrit√† e la sicurezza del sistema di Face ID nel tempo.\n\nIntegrando Secure Boot con hardware dedicato come Secure Enclave, Apple offre solide garanzie di sicurezza per operazioni critiche come il riconoscimento facciale.\n\n\nSfide\nNonostante i suoi vantaggi, l‚Äôimplementazione di Secure Boot presenta diverse sfide, in particolare in distribuzioni complesse e su larga scala: Complessit√† della Gestione delle Chiavi: Generare, archiviare, distribuire, ruotare e revocare chiavi crittografiche in modo dimostrabilmente sicuro √® particolarmente impegnativo ma fondamentale per mantenere la catena di fiducia. Qualsiasi compromissione delle chiavi paralizza le protezioni. Le grandi aziende che gestiscono moltitudini di chiavi di dispositivi affrontano particolari sfide di scala.\nSovraccarico di Prestazioni: Il controllo delle firme crittografiche durante il Boot pu√≤ aggiungere 50-100ms o pi√π per componente verificato. Questo ritardo pu√≤ essere proibitivo per applicazioni sensibili al tempo o con risorse limitate. Tuttavia, gli impatti sulle prestazioni possono essere ridotti tramite parallelizzazione e accelerazione hardware.\nSigning Burden: [Onere della firma] Gli sviluppatori devono garantire diligentemente che tutti i componenti software coinvolti nel processo di avvio, ovvero bootloader, firmware, kernel del sistema operativo, driver, applicazioni, ecc., siano firmati correttamente da chiavi attendibili. L‚Äôaccettazione della firma del codice di terze parti rimane un problema.\nVerifica Crittografica: Gli algoritmi e i protocolli sicuri devono convalidare la legittimit√† di chiavi e firme, evitare manomissioni o bypass e supportare la revoca. L‚Äôaccettazione di chiavi dubbie mina la fiducia.\nVincoli di Personalizzazione: Le architetture Secure Boot bloccate dal fornitore limitano il controllo dell‚Äôutente e l‚Äôaggiornabilit√†. I bootloader open source come u-boot e coreboot abilitano la sicurezza supportando al contempo la personalizzazione.\nStandard Scalabili: Standard emergenti come Device Identifier Composition Engine (DICE) e IDevID promettono di fornire e gestire in modo sicuro identit√† e chiavi dei dispositivi su larga scala in tutti gli ecosistemi.\nL‚Äôadozione di Secure Boot richiede di seguire le best practice di sicurezza relative alla gestione delle chiavi, alla convalida della crittografia, agli aggiornamenti firmati e al controllo degli accessi. Secure Boot fornisce una solida base per creare integrit√† e affidabilit√† dei dispositivi se implementato con cura.\n\n\n\n14.6.3 Moduli di Sicurezza Hardware\n\nHSM\nUn ‚ÄúHardware Security Module (HSM)‚Äù √® un dispositivo fisico che gestisce le chiavi digitali per un‚Äôautenticazione avanzata e fornisce l‚Äôelaborazione crittografica. Questi moduli sono progettati per essere resistenti alle manomissioni e fornire un ambiente sicuro per l‚Äôesecuzione di operazioni crittografiche. Gli HSM possono essere dispositivi standalone, schede plug-in o circuiti integrati su un altro dispositivo.\nGli HSM sono fondamentali per varie applicazioni sensibili alla sicurezza perch√© offrono un‚Äôenclave rafforzata e sicura per l‚Äôarchiviazione delle chiavi crittografiche e l‚Äôesecuzione di funzioni crittografiche. Sono particolarmente importanti per garantire la sicurezza delle transazioni, le verifiche dell‚Äôidentit√† e la crittografia dei dati.\n\n\nVantaggi\nGli HSM forniscono diverse funzionalit√† utili per la sicurezza dei sistemi ML:\nProtezione dei Dati Sensibili: Nelle applicazioni di apprendimento automatico, i modelli spesso elaborano dati sensibili che possono essere proprietari o personali. Gli HSM proteggono le chiavi di crittografia utilizzate per proteggere questi dati, sia a riposo che in transito, dall‚Äôesposizione o dal furto.\nGaranzia dell‚ÄôIntegrit√† del Modello: L‚Äôintegrit√† dei modelli ML √® fondamentale per il loro funzionamento affidabile. Gli HSM possono gestire in modo sicuro i processi di firma e verifica per software e firmware ML, assicurando che parti non autorizzate non abbiano alterato i modelli.\nAddestramento e Aggiornamenti Sicuri del Modello: L‚Äôaddestramento e l‚Äôaggiornamento dei modelli ML comportano l‚Äôelaborazione di dati potenzialmente sensibili. Gli HSM garantiscono che questi processi vengano condotti all‚Äôinterno di un confine crittografico sicuro, proteggendo dall‚Äôesposizione dei dati di addestramento e dagli aggiornamenti non autorizzati del modello.\n\n\nCompromessi\nGli HSM comportano diversi compromessi per l‚ÄôML embedded. Questi compromessi sono simili ai TEE, ma per completezza, li discuteremo anche qui attraverso la lente dell‚ÄôHSM.\nCosto: Gli HSM sono dispositivi specializzati che possono essere costosi da procurare e implementare, aumentando il costo complessivo di un progetto ML. Questo pu√≤ essere un fattore significativo per i sistemi embedded, dove i vincoli di costo sono spesso pi√π rigidi.\nSovraccarico di Prestazioni: Sebbene sicure, le operazioni crittografiche eseguite dagli HSM possono introdurre latenza. Qualsiasi ritardo aggiunto pu√≤ essere critico nelle applicazioni ML embedded ad alte prestazioni in cui l‚Äôinferenza deve avvenire in tempo reale, come nei veicoli autonomi o nei dispositivi di traduzione.\nSpazio Fisico: I sistemi embedded sono spesso limitati dallo spazio fisico e l‚Äôaggiunta di un HSM pu√≤ essere difficile in ambienti con vincoli rigidi. Ci√≤ √® particolarmente vero per l‚Äôelettronica di consumo e la tecnologia indossabile, dove le dimensioni e il fattore di forma sono considerazioni chiave.\nConsumo Energetico: Gli HSM richiedono energia per funzionare, il che pu√≤ rappresentare uno svantaggio per i dispositivi a batteria con una lunga durata della batteria. L‚Äôelaborazione sicura e le operazioni crittografiche possono scaricare la batteria pi√π velocemente, un compromesso significativo per le applicazioni ML embedded mobili o remote.\nComplessit√† nell‚ÄôIntegrazione: L‚Äôintegrazione degli HSM nei sistemi hardware esistenti aggiunge complessit√†. Spesso sono necessarie conoscenze specialistiche per gestire la comunicazione sicura tra l‚ÄôHSM e il processore del sistema e sviluppare software in grado di interfacciarsi con l‚ÄôHSM.\nScalabilit√†: Il ridimensionamento di una soluzione ML che utilizza gli HSM pu√≤ essere impegnativo. Gestire una flotta di HSM e garantire l‚Äôuniformit√† nelle pratiche di sicurezza tra i dispositivi pu√≤ diventare complesso e costoso quando aumentano le dimensioni della distribuzione, soprattutto quando si ha a che fare con sistemi embedded in cui la comunicazione √® costosa.\nComplessit√† Operativa: Gli HSM possono rendere pi√π complesso l‚Äôaggiornamento del firmware e dei modelli ML. Ogni aggiornamento deve essere firmato e possibilmente crittografato, il che aggiunge passaggi al processo di aggiornamento e potrebbe richiedere meccanismi sicuri per la gestione delle chiavi e la distribuzione degli aggiornamenti.\nSviluppo e Manutenzione: La natura sicura degli HSM implica che solo personale limitato abbia accesso all‚ÄôHSM per scopi di sviluppo e manutenzione. Ci√≤ pu√≤ rallentare il processo di sviluppo e rendere pi√π difficile la manutenzione di routine.\nCertificazione e Conformit√†: Garantire che un HSM soddisfi specifici standard di settore e requisiti di conformit√† pu√≤ aumentare i tempi e i costi di sviluppo. Ci√≤ potrebbe comportare l‚Äôesecuzione di rigorosi processi di certificazione e audit.\n\n\n\n14.6.4 Physical Unclonable Functions (PUF)\n\nInformazioni\nLe ‚ÄúPhysical Unclonable Function (PUF)‚Äù [funzioni fisiche non clonabili] forniscono un mezzo intrinseco all‚Äôhardware per la generazione di chiavi crittografiche e l‚Äôautenticazione del dispositivo sfruttando la variabilit√† di produzione intrinseca nei componenti semiconduttori. Durante la fabbricazione, fattori fisici casuali come variazioni di drogaggio, ruvidit√† del bordo della linea e spessore dielettrico determinano differenze microscopiche tra i semiconduttori, anche quando prodotti dalle stesse maschere. Questi creano variazioni di temporizzazione e potenza rilevabili che agiscono come una ‚Äúimpronta digitale‚Äù unica per ogni chip. Le PUF sfruttano questo fenomeno incorporando circuiti integrati per amplificare piccole differenze di temporizzazione o potenza in uscite digitali misurabili.\nQuando stimolato con uno stimolo in input, il circuito PUF produce una risposta di output basata sulle caratteristiche fisiche intrinseche del dispositivo. A causa della loro unicit√† fisica, lo stesso stimolo produrr√† una risposta diversa su altri dispositivi. Questo meccanismo di stimolo-risposta pu√≤ essere utilizzato per generare chiavi in modo sicuro e identificatori legati all‚Äôhardware specifico, eseguire l‚Äôautenticazione del dispositivo o archiviare in modo sicuro i segreti. Ad esempio, una chiave derivata da un PUF funzioner√† solo su quel dispositivo e non potr√† essere clonata o estratta nemmeno con accesso fisico o reverse engineering completo (Gao, Al-Sarawi, e Abbott 2020).\n\n\nVantaggi\nLa generazione di chiavi PUF evita l‚Äôarchiviazione esterna delle chiavi, che rischia di essere esposta. Fornisce inoltre una base per altre primitive di sicurezza hardware come Secure Boot. Le sfide di implementazione includono la gestione di affidabilit√† ed entropia variabili tra diverse PUF, sensibilit√† alle condizioni ambientali e suscettibilit√† agli attacchi di modellazione di apprendimento automatico. Se progettate con cura, le PUF consentono applicazioni promettenti nella protezione IP, nel trusted computing e nell‚Äôanticontraffazione.\n\n\nUtilit√†\nI modelli di apprendimento automatico stanno rapidamente diventando una parte fondamentale della funzionalit√† per molti dispositivi embedded, come smartphone, assistenti domestici intelligenti e droni autonomi. Tuttavia, proteggere l‚Äôapprendimento automatico su hardware embedded con risorse limitate pu√≤ essere difficile. Ed √® qui che i PUF si rivelano particolarmente utili. Diamo un‚Äôocchiata ad alcuni esempi di come le PUF possono essere utili.\nLe PUF forniscono un modo per generare impronte digitali e chiavi crittografiche univoche legate alle caratteristiche fisiche di ciascun chip sul dispositivo. Facciamo un esempio. Abbiamo un drone con telecamera intelligente che usa ML embedded per tracciare gli oggetti. Un PUF integrato nel processore del drone potrebbe creare una chiave specifica del dispositivo per crittografare il modello ML prima di caricarlo sul drone. In questo modo, anche se un aggressore in qualche modo hackerasse il drone e provasse a rubare il modello, non sarebbe in grado di usarlo su un altro dispositivo!\nLa stessa chiave PUF potrebbe anche creare una filigrana digitale embedded nel modello ML. Se quel modello dovesse mai trapelare e essere pubblicato online da qualcuno che cercasse di piratarlo, la filigrana potrebbe aiutare a dimostrare che proviene dal drone rubato e non dall‚Äôaggressore. Inoltre, si immagini che la telecamera del drone si colleghi al cloud per scaricare parte della sua elaborazione ML. Il PUF pu√≤ autenticare che la telecamera √® legittima prima che il cloud esegua l‚Äôinferenza su video sensibili. Il cloud potrebbe verificare che il drone non sia stato manomesso fisicamente controllando che le risposte PUF non siano cambiate.\nLe PUF consentono tutta questa sicurezza attraverso la casualit√† intrinseca del loro comportamento di stimolo-risposta e il binding hardware. Senza dover memorizzare le chiavi esternamente, le PUF sono ideali per proteggere l‚ÄôML embedded con risorse limitate. Pertanto, offrono un vantaggio unico rispetto ad altri meccanismi.\n\n\nMeccanica\nIl principio di funzionamento alla base dei PUF, illustrato in Figura¬†14.9, comporta la generazione di una coppia ‚Äústimolo-risposta‚Äù, in cui un input specifico (lo stimolo) al circuito PUF determina un output (la risposta) che √® determinato dalle propriet√† fisiche uniche di quel circuito. Questo processo pu√≤ essere paragonato a un meccanismo di impronte digitali per dispositivi elettronici. I dispositivi che utilizzano ML per elaborare i dati dei sensori possono utilizzare i PUF per proteggere la comunicazione tra dispositivi e impedire l‚Äôesecuzione di modelli ML su hardware contraffatto.\nFigura¬†14.9 illustra una panoramica delle basi dei PUF: a) PUF pu√≤ essere pensato come un‚Äôimpronta digitale unica per ogni pezzo di hardware; b) un PUF Ottico √® uno speciale token di plastica che viene illuminato, creando un pattern a macchie unico che viene poi registrato; c) in un APUF (Arbiter PUF), i bit di stimolo selezionano percorsi diversi e un giudice decide quale √® pi√π veloce, dando una risposta di ‚Äò1‚Äô o ‚Äò0‚Äô; d) in un PUF SRAM, la risposta √® determinata dalla mancata corrispondenza nella tensione di soglia dei transistor, dove determinate condizioni portano a una risposta preferita di ‚Äò1‚Äô. Ognuno di questi metodi utilizza caratteristiche specifiche dell‚Äôhardware per creare un identificatore univoco.\n\n\n\n\n\n\nFigura¬†14.9: Nozioni di base sui PUF. Fonte: Gao, Al-Sarawi, e Abbott (2020).\n\n\nGao, Yansong, Said F. Al-Sarawi, e Derek Abbott. 2020. ¬´Physical unclonable functions¬ª. Nature Electronics 3 (2): 81‚Äì91. https://doi.org/10.1038/s41928-020-0372-5.\n\n\n\n\nSfide\nCi sono alcune sfide con i PUF. La risposta PUF pu√≤ essere sensibile alle condizioni ambientali, come fluttuazioni di temperatura e tensione, portando a un comportamento incoerente di cui si deve tenere conto nella progettazione. Inoltre, poich√© i PUF possono generare molte coppie stimolo-risposta uniche, gestire e garantire la coerenza di queste coppie per tutta la durata del dispositivo pu√≤ essere impegnativo. Ultimo ma non meno importante, l‚Äôintegrazione della tecnologia PUF pu√≤ aumentare il costo di produzione complessivo di un dispositivo, sebbene possa far risparmiare sui costi di gestione delle chiavi durante il ciclo di vita del dispositivo.",
    "crumbs": [
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Sicurezza e Privacy</span>"
    ]
  },
  {
    "objectID": "contents/core/privacy_security/privacy_security.it.html#problemi-di-privacy-nella-gestione-dei-dati",
    "href": "contents/core/privacy_security/privacy_security.it.html#problemi-di-privacy-nella-gestione-dei-dati",
    "title": "14¬† Sicurezza e Privacy",
    "section": "14.7 Problemi di Privacy nella Gestione dei Dati",
    "text": "14.7 Problemi di Privacy nella Gestione dei Dati\nLa gestione sicura ed etica dei dati personali e sensibili √® fondamentale poich√© l‚Äôapprendimento automatico permea dispositivi come smartphone, dispositivi indossabili ed elettrodomestici intelligenti. Per l‚Äôhardware medico, la gestione sicura ed etica dei dati √® ulteriormente richiesta dalla legge tramite l‚ÄôHealth Insurance Portability and Accountability Act (HIPAA). Questi sistemi ML embedded presentano rischi unici per la privacy, data la loro intima vicinanza alla vita degli utenti.\n\n14.7.1 Tipi di Dati Sensibili\nI dispositivi ML embedded come quelli indossabili, assistenti domestici intelligenti e veicoli autonomi elaborano spesso dati altamente personali che richiedono un‚Äôattenta gestione per preservare la privacy dell‚Äôutente e prevenirne l‚Äôuso improprio. Esempi specifici includono referti medici e piani di trattamento elaborati da dispositivi indossabili per la salute, conversazioni private costantemente acquisite da assistenti domestici intelligenti e abitudini di guida dettagliate raccolte da auto connesse. La compromissione di tali dati sensibili pu√≤ portare a gravi conseguenze come furto di identit√†, manipolazione emotiva, umiliazione pubblica ed abuso di sorveglianza di massa.\nI dati sensibili assumono molte forme: registri strutturati come elenchi di contatti e contenuti non strutturati come flussi audio e video conversazionali. In ambito medico, le ‚Äúprotected health information (PHI)‚Äù [informazioni sanitarie protette] vengono raccolte dai medici durante ogni interazione e sono fortemente regolamentate da rigide linee guida HIPAA. Anche al di fuori degli ambienti medici, i dati sensibili possono comunque essere raccolti sotto forma di Personally Identifiable Information (PII) [Informazioni di identificazione personale], definite come ‚Äúqualsiasi rappresentazione di informazioni che consenta di dedurre ragionevolmente l‚Äôidentit√† di un individuo a cui si applicano le informazioni con mezzi diretti o indiretti‚Äù. Esempi di PII includono indirizzi e-mail, numeri di previdenza sociale e numeri di telefono, tra gli altri campi. Le PII vengono raccolte in ambito medico e in altri contesti (applicazioni finanziarie, ecc.) e sono fortemente regolamentate dalle politiche del Dipartimento del Lavoro.\nAnche gli output dei modelli derivati potrebbero far trapelare indirettamente dettagli sugli individui. Oltre ai dati personali, anche algoritmi e set di dati proprietari garantiscono la protezione della riservatezza. Nella sezione Data Engineering, abbiamo trattato diversi argomenti in dettaglio.\nTecniche come la de-identificazione, l‚Äôaggregazione, l‚Äôanonimizzazione e la federazione possono aiutare a trasformare i dati sensibili in forme meno rischiose mantenendo al contempo l‚Äôutilit√† analitica. Tuttavia, controlli diligenti su accesso, crittografia, auditing, consenso, minimizzazione e pratiche di conformit√† sono ancora essenziali durante tutto il ciclo di vita dei dati. Regolamenti come GDPR categorizzano diverse classi di dati sensibili e prescrivono responsabilit√† in merito alla loro gestione etica. Standard come NIST 800-53 forniscono rigorose linee guida per il controllo della sicurezza per la protezione della riservatezza. Con la crescente dipendenza dal ML embedded, comprendere i rischi dei dati sensibili √® fondamentale.\n\n\n14.7.2 Regolamenti Applicabili\nMolte applicazioni ML embedded gestiscono dati sensibili degli utenti in base alle normative HIPAA, GDPR e CCPA. Comprendere le protezioni imposte da queste leggi √® fondamentale per creare sistemi conformi.\n\nLa norma sulla privacy HIPAA stabilisce che i fornitori di assistenza che svolgono determinate attivit√† regolano la privacy e la sicurezza dei dati medici negli Stati Uniti, con severe sanzioni per le violazioni. Tutti i dispositivi ML embedded correlati alla salute, come dispositivi indossabili diagnostici o robot di assistenza, dovrebbero implementare controlli come audit trail, controlli di accesso e crittografia prescritti da HIPAA.\nIl GDPR impone trasparenza, limiti di conservazione e diritti degli utenti sui dati dei cittadini dell‚ÄôUE, anche quando elaborati da aziende al di fuori dell‚ÄôUE. I sistemi per la casa intelligente che catturano conversazioni familiari o pattern di posizione dovrebbero essere conformi al GDPR. I requisiti chiave includono la minimizzazione dei dati, la crittografia e meccanismi per il consenso e la cancellazione.\nIl CCPA, che si applica in California, protegge la privacy dei dati dei consumatori tramite disposizioni quali divulgazioni obbligatorie e diritti di opt-out: i gadget IoT come gli smart speaker e i fitness tracker utilizzati dai californiani rientrano probabilmente nel suo ambito.\nIl CCPA √® stato il primo insieme di regolamenti specifici per stato in merito alle preoccupazioni sulla privacy. Dopo il CCPA, regolamenti simili sono stati emanati anche in altri 10 stati, con alcuni stati che hanno proposto progetti di legge per la protezione della privacy dei dati dei consumatori.\n\nInoltre, quando pertinenti all‚Äôapplicazione, le norme specifiche del settore disciplinano telematica, servizi finanziari, servizi di pubblica utilit√†, ecc. Le best practice come ‚ÄúPrivacy by design‚Äù, valutazioni di impatto e mantenimento di audit trail aiutano a incorporare la conformit√† se non √® gi√† richiesta dalla legge. Date le sanzioni potenzialmente costose, √® consigliabile consultare team legali/di conformit√† quando si sviluppano sistemi ML embedded regolamentati.\n\n\n14.7.3 De-identificazione\nSe i dati medici vengono completamente de-identificati, le linee guida HIPAA non si applicano direttamente e ci sono molte meno normative. Tuttavia, i dati medici devono essere de-identificati utilizzando metodi HIPAA (metodi Safe Harbor o metodi Expert Determination) affinch√© le linee guida HIPAA non siano pi√π applicabili.\n\nMetodi Safe Harbor\nI metodi Safe Harbor sono pi√π comunemente utilizzati per de-identificare le informazioni sanitarie protette a causa delle risorse limitate necessarie rispetto ai metodi Expert Determination. La de-identificazione Safe Harbor richiede la pulizia dei set di dati di tutti i dati che rientrano in una delle 18 categorie. Le seguenti categorie sono elencate come informazioni sensibili in base allo standard Safe Harbor:\n\nNome, Localizzatore geografico, Data di nascita, Numero di telefono, Indirizzo e-mail, Indirizzi, Numeri di previdenza sociale, Numeri di cartella clinica, Numeri di beneficiari sanitari, Identificatori di dispositivi e Numeri di serie, Numeri di certificati/patenti (Certificato di nascita, Patente di guida, ecc.), Numeri di conto, Identificatori di veicoli, URL di siti Web, Foto a pieno facciale e Immagini comparabili, Identificatori biometrici, Qualsiasi altro identificatore univoco\n\nPer la maggior parte di queste categorie, tutti i dati devono essere rimossi indipendentemente dalle circostanze. Per altre categorie, tra cui informazioni geografiche e data di nascita, i dati possono essere rimossi parzialmente quanto basta per rendere le informazioni difficili da re-identificare. Ad esempio, se un codice postale √® abbastanza grande, le prime 3 cifre possono rimanere poich√© ci sono abbastanza persone nell‚Äôarea geografica da rendere difficile la re-identificazione. Le date di nascita devono essere ripulite da tutti gli elementi tranne l‚Äôanno di nascita e tutte le et√† superiori a 89 anni devono essere aggregate in una categoria 90+.\n\n\nMetodi di Determinazione degli Esperti\nI metodi Safe Harbor funzionano per diversi casi di de-identificazione dei dati medici, sebbene in alcuni casi sia ancora possibile la re-identificazione. Ad esempio, supponiamo che si raccolgano dati su un paziente in una citt√† urbana con un grande codice postale, ma √® stata documentata una malattia rara di cui soffre, una malattia che colpisce solo 25 persone in tutta la citt√†. Dati i dati geografici associati all‚Äôanno di nascita, √® altamente possibile che qualcuno possa re-identificare questo individuo, il che rappresenta una violazione della privacy estremamente dannosa.\nIn casi unici come questi, sono preferiti metodi di de-identificazione dei dati di determinazione esperta. La de-identificazione di determinazione esperta richiede una ‚Äúpersona con conoscenza ed esperienza appropriate di principi e metodi statistici e scientifici generalmente accettati per rendere le informazioni non identificabili individualmente‚Äù per valutare un set di dati e determinare se il rischio di re-identificazione dei dati individuali in un dato set di dati in combinazione con dati disponibili al pubblico (registri di voto, ecc.), √® estremamente ridotto.\nLa de-identificazione tramite Expert Determination √® comprensibilmente pi√π difficile da completare rispetto alla de-identificazione tramite Safe Harbor, a causa del costo e della fattibilit√† dell‚Äôaccesso a un esperto per verificare la probabilit√† di re-identificazione di un set di dati. Tuttavia, in molti casi, √® richiesta una Expert Determination per garantire che la re-identificazione dei dati sia estremamente improbabile.\n\n\n\n14.7.4 Riduzione al Minimo dei Dati\nLa riduzione al minimo dei dati comporta la raccolta, la conservazione e l‚Äôelaborazione solo dei dati utente necessari per ridurre i rischi per la privacy derivanti dai sistemi ML embedded. Si inizia limitando i tipi di dati e le istanze raccolte al minimo indispensabile per la funzionalit√† di base del sistema. Ad esempio, un modello di rilevamento degli oggetti raccoglie solo le immagini necessarie per quella specifica attivit√† di visione artificiale. Allo stesso modo, un assistente vocale limiterebbe l‚Äôacquisizione audio a specifici comandi vocali anzich√© registrare in modo persistente i suoni ambientali.\nOve possibile, i dati temporanei che risiedono brevemente nella memoria senza archiviazione persistente forniscono un‚Äôulteriore riduzione al minimo. Dovrebbe essere stabilita una chiara base giuridica, come il consenso dell‚Äôutente, per la raccolta e la conservazione. Il sandboxing e i controlli di accesso impediscono l‚Äôuso non autorizzato oltre le attivit√† previste. I periodi di conservazione dovrebbero essere definiti in base allo scopo, con procedure di eliminazione sicura che rimuovono i dati scaduti.\nLa riduzione al minimo dei dati pu√≤ essere suddivisa in 3 categorie:\n\n‚ÄúI dati devono essere adeguati rispetto allo scopo perseguito‚Äù. L‚Äôomissione di dati pu√≤ limitare l‚Äôaccuratezza dei modelli addestrati sui dati e qualsiasi utilit√† generale di un set di dati. La minimizzazione dei dati richiede che una quantit√† minima di dati venga raccolta dagli utenti durante la creazione di un set di dati che aggiunge valore ad altri.\nI dati raccolti dagli utenti devono essere rilevanti allo scopo della raccolta dati.\nI dati degli utenti dovrebbero essere limitati ai soli dati necessari per soddisfare lo scopo della raccolta dati iniziale. Se √® possibile ottenere risultati altrettanto solidi e accurati da un set di dati pi√π piccolo, non dovrebbero essere raccolti dati aggiuntivi oltre questo set di dati pi√π piccolo.\n\nTecniche emergenti come la privacy differenziale, l‚Äôaddestramento federato e la generazione di dati sintetici consentono informazioni utili derivate da dati utente meno grezzi. L‚Äôesecuzione di mappature del flusso di dati e valutazioni di impatto aiutano a identificare le opportunit√† per ridurre al minimo l‚Äôutilizzo di dati grezzi.\nMetodologie come Privacy by Design (Cavoukian 2009) considerano tale minimizzazione all‚Äôinizio dell‚Äôarchitettura del sistema. Anche normative come il GDPR impongono principi di minimizzazione dei dati. Con un approccio multilayer nei regni legale, tecnico e di processo, la minimizzazione dei dati limita i rischi nei prodotti ML embedded.\n\nCavoukian, Ann. 2009. ¬´Privacy by design¬ª. Office of the Information and Privacy Commissioner.\n\nCaso di Studio: Minimizzazione dei Dati Basata sulle Prestazioni\nLa minimizzazione dei dati basata sulle prestazioni (Biega et al. 2020) si concentra sull‚Äôespansione della terza categoria di minimizzazione dei dati menzionata sopra, ovvero la limitazione. Definisce specificamente la robustezza dei risultati del modello su un dato set di dati tramite determinate metriche delle prestazioni, in modo che i dati non debbano essere raccolti ulteriormente se non migliorano significativamente le prestazioni. Le metriche delle prestazioni possono essere divise in due categorie:\n\nBiega, Asia J., Peter Potash, Hal Daum√©, Fernando Diaz, e Mich√®le Finck. 2020. ¬´Operationalizing the Legal Principle of Data Minimization for Personalization¬ª. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval, a cura di Jimmy Huang, Yi Chang, Xueqi Cheng, Jaap Kamps, Vanessa Murdock, Ji-Rong Wen, e Yiqun Liu, 399‚Äì408. ACM. https://doi.org/10.1145/3397271.3401034.\n\nPrestazioni di minimizzazione dei dati globali: Soddisfatte se un set di dati riduce al minimo la quantit√† di dati per utente mentre le sue prestazioni medie su tutti i dati sono paragonabili alle prestazioni medie del set di dati originale non minimizzato.\nPrestazioni di minimizzazione dei dati per utente: Soddisfatte se un set di dati riduce al minimo la quantit√† di dati per utente mentre le prestazioni minime dei dati utente individuali sono paragonabili a quelle dei dati utente individuali nel set di dati originale non minimizzato.\n\nLa riduzione al minimo dei dati basata sulle prestazioni pu√≤ essere sfruttata in impostazioni di apprendimento automatico, inclusi algoritmi di raccomandazione di film e impostazioni di e-commerce.\nLa riduzione al minimo dei dati globali √® molto pi√π fattibile della riduzione al minimo dei dati per utente, data la differenza molto pi√π significativa nelle perdite per utente tra i set di dati ridotti al minimo e quelli originali.\n\n\n\n14.7.5 Consenso e Trasparenza\nUn consenso e una trasparenza significativi sono fondamentali quando si raccolgono dati utente per prodotti ML embedded come smart speaker, dispositivi indossabili e veicoli autonomi. Quando viene configurato per la prima volta. Idealmente, il dispositivo dovrebbe spiegare chiaramente quali tipi di dati vengono raccolti, per quali scopi, come vengono elaborati e le policy di conservazione. Ad esempio, uno smart speaker potrebbe raccogliere campioni vocali per addestrare il riconoscimento vocale e profili vocali personalizzati. Durante l‚Äôuso, promemoria e opzioni della dashboard forniscono una trasparenza continua su come vengono gestiti i dati, come riepiloghi settimanali di frammenti vocali acquisiti. Le opzioni di controllo consentono di revocare o limitare il consenso, come disattivare l‚Äôarchiviazione dei profili vocali.\nI flussi di consenso dovrebbero fornire controlli granulari che vadano oltre le semplici scelte binarie s√¨/no. Ad esempio, gli utenti potrebbero acconsentire selettivamente a determinati utilizzi dei dati, come l‚Äôaddestramento al riconoscimento vocale, ma non alla personalizzazione. I focus group e i test di usabilit√† con gli utenti target modellano le interfacce di consenso e la formulazione delle policy sulla privacy per ottimizzare la comprensione e il controllo. Il rispetto dei diritti degli utenti, come l‚Äôeliminazione e la rettifica dei dati, dimostra affidabilit√†. Un gergo legale vago ostacola la trasparenza. Regolamenti come GDPR e CCPA rafforzano i requisiti di consenso. Un consenso ponderato e la trasparenza forniscono agli utenti l‚Äôagenzia sui propri dati, creando al contempo fiducia nei prodotti ML incorporati attraverso una comunicazione e un controllo aperti.\n\n\n14.7.6 Problemi di Privacy nell‚ÄôApprendimento Automatico\n\nIA Generativa\nSono aumentate anche le preoccupazioni sulla privacy e sulla sicurezza con l‚Äôuso pubblico di modelli di intelligenza artificiale generativa, tra cui GPT4 di OpenAI e altri LLM. ChatGPT, in particolare, √® stato discusso pi√π di recente in merito alla privacy, date tutte le informazioni personali raccolte dagli utenti di ChatGPT. Nel giugno 2023 √® stata intentata una class action contro ChatGPT a causa del timore che fosse stato addestrato su informazioni mediche e personali proprietarie senza le dovute autorizzazioni o consensi. Come risultato di queste preoccupazioni sulla privacy, molte aziende hanno proibito ai propri dipendenti di accedere a ChatGPT e di caricare informazioni aziendali private sul chatbot. Inoltre, ChatGPT √® suscettibile al ‚Äúprompt injection‚Äù e altri attacchi alla sicurezza che potrebbero compromettere la privacy dei dati proprietari su cui √® stato addestrato.\n\nCaso di Studio: Aggiramento delle Misure di Sicurezza di ChatGPT\nMentre ChatGPT ha istituito delle protezioni per impedire alle persone di accedere a informazioni private ed eticamente discutibili, diversi individui sono riusciti a bypassare queste protezioni tramite ‚Äúprompt injection‚Äù e altri attacchi alla sicurezza. Come dimostrato in Figura¬†14.10, gli utenti possono bypassare le protezioni di ChatGPT per imitare il tono di una ‚Äúnonna defunta‚Äù per imparare come bypassare un firewall per applicazioni Web (Gupta et al. 2023).\n\n\n\n\n\n\nFigura¬†14.10: Gioco di ruolo della nonna per bypassare le restrizioni di sicurezza. Fonte: Gupta et al. (2023).\n\n\n\nInoltre, gli utenti hanno anche utilizzato con successo la psicologia inversa per manipolare ChatGPT e accedere a informazioni inizialmente proibite dal modello. In Figura¬†14.11, a un utente viene inizialmente impedito di scoprire siti Web di pirateria tramite ChatGPT, ma pu√≤ bypassare queste restrizioni utilizzando la psicologia inversa.\n\n\n\n\n\n\nFigura¬†14.11: Psicologia inversa per bypassare le restrizioni di sicurezza. Fonte: Gupta et al. (2023).\n\n\nGupta, Maanak, Charankumar Akiri, Kshitiz Aryal, Eli Parker, e Lopamudra Praharaj. 2023. ¬´From ChatGPT to ThreatGPT: Impact of Generative AI in Cybersecurity and Privacy¬ª. IEEE Access 11: 80218‚Äì45. https://doi.org/10.1109/access.2023.3300381.\n\n\nLa facilit√† con cui gli attacchi alla sicurezza possono manipolare ChatGPT √® preoccupante, date le informazioni private su cui √® stato addestrato senza consenso. Ulteriori ricerche sulla privacy dei dati in LLM e sull‚Äôintelligenza artificiale generativa dovrebbero concentrarsi sull‚Äôimpedire al modello di essere cos√¨ ingenuo da indurre attacchi di ‚Äúinjection‚Äù.\n\n\n\nCancellazione dei Dati\nMolte delle normative precedentemente menzionate, incluso il GDPR, includono una clausola sul ‚Äúdiritto all‚Äôoblio‚Äù. Questa clausola afferma essenzialmente che ‚Äúl‚Äôinteressato ha il diritto di ottenere dal titolare del trattamento la cancellazione dei dati personali che lo riguardano senza indebito ritardo‚Äù. Tuttavia, in diversi casi, anche se i dati dell‚Äôutente sono stati cancellati da una piattaforma, i dati vengono cancellati solo parzialmente se un modello di apprendimento automatico √® stato addestrato su questi dati per scopi separati. Attraverso metodi simili agli attacchi di inferenza di appartenenza, altri individui possono ancora dedurre i dati di addestramento su cui √® stato addestrato un modello, anche se la presenza dei dati √® stata esplicitamente rimossa online.\nUn approccio per affrontare i problemi di privacy con i dati di addestramento dell‚Äôapprendimento automatico √® stato attraverso metodi di privacy differenziali. Ad esempio, aggiungendo rumore laplaciano nel set di addestramento, un modello pu√≤ essere robusto agli attacchi di inferenza di appartenenza, impedendo il recupero dei dati eliminati. Un altro approccio per impedire che i dati eliminati vengano dedotti da attacchi alla sicurezza √® semplicemente riaddestrare il modello da zero sui dati rimanenti. Poich√© questo processo √® dispendioso in termini di tempo e di elaborazione dati, altri ricercatori hanno tentato di affrontare le preoccupazioni sulla privacy relative all‚Äôinferenza dei dati di training del modello tramite un processo chiamato ‚Äúmachine unlearning‚Äù, in cui un modello itera attivamente su se stesso per rimuovere l‚Äôinfluenza dei dati ‚Äúdimenticati‚Äù su cui potrebbe essere stato addestrato, come menzionato di seguito.",
    "crumbs": [
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Sicurezza e Privacy</span>"
    ]
  },
  {
    "objectID": "contents/core/privacy_security/privacy_security.it.html#tecniche-ml-per-la-tutela-della-privacy",
    "href": "contents/core/privacy_security/privacy_security.it.html#tecniche-ml-per-la-tutela-della-privacy",
    "title": "14¬† Sicurezza e Privacy",
    "section": "14.8 Tecniche ML per la Tutela della Privacy",
    "text": "14.8 Tecniche ML per la Tutela della Privacy\nSono state sviluppate molte tecniche per preservare la privacy, ognuna delle quali affronta diversi aspetti e sfide per la sicurezza dei dati. Questi metodi possono essere ampiamente categorizzati in diverse aree chiave: Differential Privacy, che si concentra sulla privacy statistica negli output dei dati; Federated Learning, che enfatizza l‚Äôelaborazione decentralizzata dei dati; Homomorphic Encryption e Secure Multi-party Computation (SMC), entrambi abilitanti calcoli sicuri su dati crittografati o privati; Data Anonymization e Data Masking and Obfuscation, che alterano i dati per proteggere le identit√† individuali; Private Set Intersection e Zero-Knowledge Proofs, che facilitano confronti e convalide di dati sicuri; Decentralized Identifiers (DID) per identit√† digitali auto-sovrane; Privacy-Preserving Record Linkage (PPRL), che collega i dati tra le fonti senza esposizione; Synthetic Data Generation, che crea set di dati artificiali per analisi sicure; e Adversarial Learning Techniques, che migliora la resistenza dei dati o dei modelli agli attacchi alla privacy.\nData l‚Äôampia gamma di queste tecniche, non √® possibile approfondire ciascuna di esse in un singolo corso o discussione, e tanto meno che qualcuno possa conoscerle tutte nei loro gloriosi dettagli. Pertanto, esploreremo alcune tecniche specifiche in modo relativamente dettagliato, fornendo una comprensione pi√π approfondita dei loro principi, applicazioni e delle sfide uniche per la privacy che affrontano nell‚Äôapprendimento automatico. Questo approccio mirato ci fornir√† una comprensione pi√π completa e pratica dei principali metodi di tutela della privacy nei moderni sistemi ML.\n\n14.8.1 Privacy Differenziale\n\nIdea Centrale\nLa ‚ÄúDifferential Privacy‚Äù Privacy Differenziale √® un framework per quantificare e gestire la privacy degli individui in un set di dati (Dwork et al. 2006). Fornisce una garanzia matematica che la privacy degli individui nel set di dati non verr√† compromessa, indipendentemente da qualsiasi conoscenza aggiuntiva che un aggressore potrebbe possedere. L‚Äôidea fondamentale della privacy differenziale √® che il risultato di qualsiasi analisi (come una query statistica) dovrebbe essere essenzialmente lo stesso, indipendentemente dal fatto che i dati di un individuo siano inclusi nel set di dati o meno. Ci√≤ significa che osservando il risultato dell‚Äôanalisi, non √® possibile determinare se i dati di un individuo siano stati utilizzati nel calcolo.\n\nDwork, Cynthia, Frank McSherry, Kobbi Nissim, e Adam Smith. 2006. ¬´Calibrating Noise to Sensitivity in Private Data Analysis¬ª. In Theory of Cryptography, a cura di Shai Halevi e Tal Rabin, 265‚Äì84. Berlin, Heidelberg: Springer Berlin Heidelberg. https://doi.org/10.1007/11681878\\_14.\nAd esempio, supponiamo che un database contenga cartelle cliniche di 10 pazienti. Vogliamo pubblicare statistiche sulla prevalenza del diabete in questo campione senza rivelare le condizioni di un paziente. Per fare ci√≤, potremmo aggiungere una piccola quantit√† di rumore casuale al conteggio reale prima di pubblicarlo. Se il numero reale di pazienti diabetici √® 6, potremmo aggiungere rumore da una distribuzione di Laplace per ottenere casualmente 5, 6 o 7, ciascuno con una certa probabilit√†. Un osservatore ora non pu√≤ dire se un singolo paziente ha il diabete basandosi solo sull‚Äôoutput rumoroso. Il risultato della query √® simile a se i dati di ogni paziente sono inclusi o esclusi. Questa √® la privacy differenziale. Pi√π formalmente, un algoritmo randomizzato soddisfa la privacy differenziale Œµ se, per qualsiasi database vicino D e D π che differisce solo per una voce, la probabilit√† di qualsiasi risultato cambia al massimo di un fattore Œµ. Un Œµ inferiore fornisce maggiori garanzie di privacy.\nIl meccanismo di Laplace √® uno dei metodi pi√π semplici e comunemente utilizzati per ottenere la privacy differenziale. Comporta l‚Äôaggiunta di rumore che segue una distribuzione di Laplace ai dati o ai risultati delle query. A parte il Meccanismo di Laplace, il principio generale di aggiunta di rumore √® fondamentale per la Privacy differenziale. L‚Äôidea √® di aggiungere rumore casuale ai dati o ai risultati di una query. Il rumore √® calibrato per garantire la necessaria garanzia di privacy mantenendo i dati utili.\nMentre la distribuzione di Laplace √® comune, possono essere utilizzate anche altre distribuzioni come quella gaussiana. Il rumore di Laplace √® utilizzato per la privacy differenziale rigorosa Œµ per query a bassa sensibilit√†. Al contrario, le distribuzioni gaussiane possono essere utilizzate quando la privacy non √® garantita, nota come privacy differenziale (œµ, ùõø). In questa versione rilassata della privacy differenziale, epsilon e delta definiscono la quantit√† di privacy garantita quando si rilasciano informazioni o un modello correlato a un set di dati. Epsilon stabilisce un limite su quanta informazione pu√≤ essere appresa sui dati in base all‚Äôoutput. Allo stesso tempo, delta consente una piccola probabilit√† che la garanzia della privacy venga violata. La scelta tra Laplace, gaussiana e altre distribuzioni dipender√† dai requisiti specifici della query e del set di dati e dal compromesso tra Privacy e accuratezza.\nPer illustrare il compromesso tra Privacy e accuratezza nella Privacy differenziale (\\(\\epsilon\\), \\(\\delta\\)), i seguenti grafici in Figura¬†14.12 mostrano i risultati sull‚Äôaccuratezza per diversi livelli di rumore sul set di dati MNIST, un ampio set di dati di cifre scritte a mano (Abadi et al. 2016). Il valore delta (linea nera; asse y destro) indica il livello di rilassamento della privacy (un valore elevato indica che la Privacy √® meno rigorosa). Man mano che la Privacy diventa pi√π rilassata, aumenta l‚Äôaccuratezza del modello.\n\n\n\n\n\n\nFigura¬†14.12: Compromesso tra privacy e accuratezza. Fonte: Abadi et al. (2016).\n\n\nAbadi, Martin, Andy Chu, Ian Goodfellow, H. Brendan McMahan, Ilya Mironov, Kunal Talwar, e Li Zhang. 2016. ¬´Deep Learning with Differential Privacy¬ª. In Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security, 308‚Äì18. CCS ‚Äô16. New York, NY, USA: ACM. https://doi.org/10.1145/2976749.2978318.\n\n\nI punti chiave da ricordare sulla privacy differenziale sono i seguenti:\n\nAggiunta di Rumore: La tecnica fondamentale nella Privacy differenziale √® l‚Äôaggiunta di rumore casuale controllato ai dati o ai risultati delle query. Questo rumore maschera il contributo dei singoli dati.\nAtto di Bilanciamento: C‚Äô√® un equilibrio tra Privacy e accuratezza. Pi√π rumore (œµ inferiore) nei dati significa maggiore Privacy ma minore accuratezza nei risultati del modello.\nUniversalit√†: La privacy differenziale non si basa su ipotesi su ci√≤ che sa un aggressore.s. Ci√≤ lo rende robusto contro gli attacchi di re-identificazione, in cui un aggressore cerca di scoprire dati individuali.\nApplicabilit√†: Pu√≤ essere applicato a vari tipi di dati e query, rendendolo uno strumento versatile per l‚Äôanalisi dei dati che preserva la privacy.\n\n\n\nCompromessi\nCi sono diversi compromessi da fare con la Privacy differenziale, come nel caso di qualsiasi algoritmo. Ma concentriamoci sui compromessi specifici computazionali, poich√© ci interessano i sistemi ML. Ci sono alcune considerazioni e compromessi computazionali chiave quando si implementa la privacy differenziale in un sistema di apprendimento automatico:\nGenerazione di Rumore: L‚Äôimplementazione della privacy differenziale introduce diversi compromessi computazionali importanti rispetto alle tecniche di apprendimento automatico standard. Una considerazione importante √® la necessit√† di generare in modo sicuro rumore casuale da distribuzioni come Laplace o Gaussiana che vengono aggiunte ai risultati delle query e agli output del modello. La generazione di numeri casuali crittografici di alta qualit√† pu√≤ essere computazionalmente costosa.\nAnalisi di Sensibilit√†: Un altro requisito fondamentale √® il monitoraggio rigoroso della sensibilit√† degli algoritmi sottostanti ai singoli punti dati che vengono aggiunti o rimossi. Questa analisi di sensibilit√† globale √® necessaria per calibrare correttamente i livelli di rumore. Tuttavia, l‚Äôanalisi della sensibilit√† del caso peggiore pu√≤ aumentare sostanzialmente la complessit√† computazionale per complesse procedure di addestramento del modello e pipeline di dati.\nGestione del budget per la privacy: La gestione del budget per la perdita della privacy su pi√π query e iterazioni di apprendimento √® un altro sovraccarico contabile. Il sistema deve tenere traccia dei costi cumulativi per la privacy e comporli per spiegare le garanzie di privacy complessive. Ci√≤ aggiunge un onere computazionale che va oltre la semplice esecuzione di query o modelli di addestramento.\nCompromessi tra batch e online: Per i sistemi di apprendimento online con query continue ad alto volume, gli algoritmi differenzialmente privati richiedono nuovi meccanismi per mantenere l‚Äôutilit√† e prevenire troppe perdite di privacy accumulate poich√© ogni query pu√≤ potenzialmente alterare il budget per la privacy. L‚Äôelaborazione offline in batch √® pi√π semplice da una prospettiva computazionale poich√© elabora i dati in grandi batch, dove ogni batch viene trattato come una singola query. I dati sparsi ad alta dimensionalit√† aumentano anche le difficolt√† dell‚Äôanalisi di sensibilit√†.\nAddestramento distribuito: Quando si addestrano modelli utilizzando approcci distribuiti o federati, sono necessari nuovi protocolli crittografici per tracciare e limitare le ‚Äúfughe‚Äù di privacy tra i nodi. Il calcolo multi-parti sicuro con dati crittografati per la Privacy differenziale aggiunge un carico computazionale sostanziale.\nMentre la Privacy differenziale fornisce solide garanzie formali di privacy, la sua implementazione rigorosa richiede aggiunte e modifiche alla pipeline di apprendimento automatico a un costo computazionale. La gestione di queste spese generali preservando l‚Äôaccuratezza del modello rimane un‚Äôarea di ricerca attiva.\n\n\nCaso di Studio: Privacy Differenziale in Apple\nL‚Äôimplementazione della Privacy differenziale da parte di Apple in iOS e MacOS fornisce un importante esempio concreto di come la Privacy differenziale pu√≤ essere distribuita su larga scala. Apple voleva raccogliere statistiche aggregate sull‚Äôutilizzo nel proprio ecosistema per migliorare prodotti e servizi, ma mirava a farlo senza compromettere la privacy dei singoli utenti.\nPer raggiungere questo obiettivo, ha implementato tecniche di privacy differenziale direttamente sui dispositivi degli utenti per rendere anonimi i punti dati prima di inviarli ai server Apple. In particolare, Apple utilizza il meccanismo di Laplace per iniettare rumore casuale attentamente calibrato. Ad esempio, supponiamo che la cronologia delle posizioni di un utente contenga [Lavoro, Casa, Lavoro, Palestra, Lavoro, Casa]. In tal caso, la versione privata differenziale potrebbe sostituire le posizioni esatte con un campione rumoroso come [Palestra, Casa, Lavoro, Lavoro, Casa, Lavoro].\nApple regola la distribuzione del rumore di Laplace per fornire un elevato livello di privacy preservando al contempo l‚Äôutilit√† delle statistiche aggregate. L‚Äôaumento dei livelli di rumore fornisce maggiori garanzie di privacy (valori Œµ inferiori nella terminologia DP) ma pu√≤ ridurre l‚Äôutilit√† dei dati. Gli ingegneri della privacy di Apple hanno ottimizzato empiricamente questo compromesso in base ai loro obiettivi di prodotto.\nApple ottiene statistiche aggregate ad alta fedelt√† aggregando centinaia di milioni di punti dati rumorosi dai dispositivi. Ad esempio, possono analizzare le funzionalit√† delle nuove app iOS mascherando i comportamenti delle app di qualsiasi utente. Il calcolo sul dispositivo evita di inviare dati grezzi ai server Apple.\nIl sistema utilizza la generazione di numeri casuali sicuri basata su hardware per campionare in modo efficiente dalla distribuzione di Laplace sui dispositivi. Apple ha anche dovuto ottimizzare i suoi algoritmi e pipeline differenzialmente privati per operare sotto i vincoli computazionali dell‚Äôhardware degli utenti.\nNumerosi audit di terze parti hanno verificato che il sistema Apple fornisce rigorose protezioni differenziali della privacy in linea con le loro politiche dichiarate. Naturalmente, le ipotesi sulla composizione nel tempo e sui potenziali rischi di reidentificazione sono ancora valide. L‚Äôimplementazione di Apple mostra come la privacy differenziale pu√≤ essere realizzata in grandi prodotti del mondo reale quando supportata da sufficienti risorse ingegneristiche.\n\n\n\n\n\n\nEsercizio¬†14.1: Privacy Differenziale - Privacy TensorFlow\n\n\n\n\n\nVolete addestrare un modello ML senza compromettere i segreti di nessuno? La Privacy differenziale √® come un superpotere per i dati! In questo Colab, useremo TensorFlow Privacy per aggiungere rumore speciale durante l‚Äôaddestramento. Ci√≤ rende molto pi√π difficile per chiunque determinare se sono stati utilizzati i dati di una singola persona, anche se hanno modi furtivi per sbirciare il modello.\n\n\n\n\n\n\n\n14.8.2 Il Federated Learning\n\nIdea Centrale\nIl ‚ÄúFederated Learning (FL)‚Äù √® un tipo di apprendimento automatico in cui un modello viene creato e distribuito su pi√π dispositivi o server mantenendo localizzati i dati di training. √à stato precedentemente discusso nel capitolo Ottimizzazioni del modello. Tuttavia, lo riepilogheremo brevemente qui per completarlo e concentrarci su cose che riguardano questo capitolo.\nFL addestra modelli di apprendimento automatico su reti decentralizzate di dispositivi o sistemi, mantenendo tutti i dati di addestramento localizzati. Figura¬†14.13 illustra questo processo: ogni dispositivo partecipante sfrutta i propri dati locali per calcolare gli aggiornamenti del modello, che vengono poi aggregati per creare un modello globale migliorato. Tuttavia, i dati di training grezzi non vengono mai condivisi, trasferiti o compilati direttamente. Questo approccio di tutela della privacy consente lo sviluppo congiunto di modelli ML senza centralizzare i dati di training potenzialmente sensibili in un unico posto.\n\n\n\n\n\n\nFigura¬†14.13: Ciclo di Vita dell‚ÄôApprendimento Federato. Fonte: Jin et al. (2020).\n\n\nJin, Yilun, Xiguang Wei, Yang Liu, e Qiang Yang. 2020. ¬´Towards Utilizing Unlabeled Data in Federated Learning: A Survey and Prospective¬ª. arXiv preprint arXiv:2002.11545, febbraio. http://arxiv.org/abs/2002.11545v2.\n\n\nUno degli algoritmi di aggregazione di modelli pi√π comuni √® Federated Averaging (FedAvg), in cui il modello globale viene creato calcolando la media di tutti i parametri dai parametri locali. Mentre FedAvg funziona bene con dati indipendenti e distribuiti in modo identico (IID), algoritmi alternativi come Federated Proximal (FedProx) sono fondamentali nelle applicazioni del mondo reale in cui i dati sono spesso non IID. FedProx √® progettato per il processo FL quando c‚Äô√® una significativa eterogeneit√† negli aggiornamenti client a causa di diverse distribuzioni di dati tra dispositivi, capacit√† di calcolo o quantit√† variabili di dati.\nLasciando i dati grezzi distribuiti e scambiando solo aggiornamenti temporanei del modello, l‚Äôapprendimento federato fornisce un‚Äôalternativa pi√π sicura e che migliora la privacy alle tradizionali pipeline di apprendimento automatico centralizzate. Ci√≤ consente alle organizzazioni e agli utenti di trarre vantaggio in modo collaborativo da modelli condivisi mantenendo al contempo il controllo e la propriet√† sui dati sensibili. La natura decentralizzata di FL lo rende anche robusto per singoli punti di errore.\nSi immagini un gruppo di ospedali che desidera collaborare a uno studio per prevedere i risultati dei pazienti in base ai loro sintomi. Tuttavia, non possono condividere i dati dei pazienti a causa di problemi di privacy e normative come HIPAA. Ecco come il Federated Learning pu√≤ aiutare.\n\nAddestramento Locale: Ogni ospedale addestra un modello di apprendimento automatico sui dati dei pazienti. Questo addestramento avviene localmente, il che significa che i dati non lasciano mai i server dell‚Äôospedale.\nCondivisione del Modello: Dopo l‚Äôaddestramento, ogni ospedale invia solo il modello (in particolare, i suoi parametri o pesi) a un server centrale. Non invia alcun dato del paziente.\nModelli di Aggregazione: Il server centrale aggrega questi modelli da tutti gli ospedali in un singolo modello pi√π robusto. Questo processo in genere comporta la media dei parametri del modello.\nVantaggio: Il risultato √® un modello di apprendimento automatico che ha appreso da un‚Äôampia gamma di dati dei pazienti senza condividere dati sensibili o rimuoverli dalla loro posizione originale.\n\n\n\nCompromessi\nCi sono diversi aspetti correlati alle prestazioni del sistema di FL nei sistemi di apprendimento automatico. Sarebbe saggio comprendere questi compromessi perch√© non esiste un ‚Äúpranzo gratis‚Äù per preservare la privacy tramite FL (Li et al. 2020).\n\nLi, Tian, Anit Kumar Sahu, Ameet Talwalkar, e Virginia Smith. 2020. ¬´Federated Learning: Challenges, Methods, and Future Directions¬ª. IEEE Signal Processing Magazine 37 (3): 50‚Äì60. https://doi.org/10.1109/msp.2020.2975749.\nSovraccarico di Comunicazione e Vincoli di Rete: Nel FL, una delle sfide pi√π significative √® la gestione del sovraccarico di comunicazione. Ci√≤ comporta la frequente trasmissione di aggiornamenti del modello tra un server centrale e numerosi dispositivi client, che pu√≤ richiedere molta larghezza di banda. Il numero totale di round di comunicazione e la dimensione dei messaggi trasmessi per round devono essere ridotti per ridurre ulteriormente la comunicazione. Ci√≤ pu√≤ comportare un traffico di rete sostanziale, soprattutto in scenari con molti partecipanti. Inoltre, la latenza diventa un fattore critico: il tempo impiegato per inviare, aggregare e ridistribuire questi aggiornamenti pu√≤ causare ritardi. Ci√≤ influisce sul tempo di training complessivo e ha un impatto sulla reattivit√† del sistema e sulle capacit√† in tempo reale. Gestire questa comunicazione riducendo al minimo l‚Äôutilizzo della larghezza di banda e la latenza √® fondamentale per implementare FL.\nCarico di Calcolo sui Dispositivi Locali: FL si basa su dispositivi client (come smartphone o dispositivi IoT, che sono particolarmente importanti in TinyML) per l‚Äôaddestramento del modello, che spesso hanno una potenza di calcolo e una durata della batteria limitate. L‚Äôesecuzione di algoritmi complessi di apprendimento automatico in locale pu√≤ mettere a dura prova queste risorse, portando a potenziali problemi di prestazioni. Inoltre, le capacit√† di questi dispositivi possono variare in modo significativo, con conseguenti contributi non uniformi al processo di addestramento del modello. Alcuni dispositivi elaborano gli aggiornamenti in modo pi√π rapido ed efficiente di altri, portando a disparit√† nel processo di apprendimento. Bilanciare il carico computazionale per garantire una partecipazione e un‚Äôefficienza coerenti su tutti i dispositivi √® una sfida fondamentale in FL.\nEfficienza dell‚ÄôAddestramento del Modello: La natura decentralizzata di FL pu√≤ influire sull‚Äôefficienza dell‚Äôaddestramento del modello. Raggiungere la convergenza, in cui il modello non migliora pi√π in modo significativo, pu√≤ essere pi√π lento in FL rispetto ai metodi di addestramento centralizzati. Ci√≤ √® particolarmente vero nei casi in cui i dati sono non IID (non indipendenti e distribuiti in modo identico) tra i dispositivi. Inoltre, gli algoritmi utilizzati per aggregare gli aggiornamenti del modello svolgono un ruolo fondamentale nel processo di addestramento. La loro efficienza influisce direttamente sulla velocit√† e l‚Äôefficacia dell‚Äôapprendimento. Sviluppare e implementare algoritmi in grado di gestire le complessit√† di FL garantendo al contempo una convergenza tempestiva √® essenziale per le prestazioni del sistema.\nSfide di Scalabilit√†: La scalabilit√† √® una preoccupazione significativa in FL, soprattutto con l‚Äôaumento del numero di dispositivi partecipanti. La gestione e il coordinamento degli aggiornamenti del modello da molti dispositivi aggiungono complessit√† e possono mettere a dura prova il sistema. √à fondamentale garantire che l‚Äôarchitettura del sistema possa gestire in modo efficiente questo carico aumentato senza degradare le prestazioni. Ci√≤ implica non solo la gestione degli aspetti computazionali e di comunicazione, ma anche il mantenimento della qualit√† e della coerenza del modello man mano che aumenta la scala dell‚Äôoperazione. Una sfida fondamentale √® la progettazione di sistemi FL che si adattino in modo efficace mantenendo le prestazioni.\nSincronizzazione e Coerenza dei Dati: Garantire la sincronizzazione dei dati e mantenere la coerenza del modello su tutti i dispositivi partecipanti in FL √® una sfida. Mantenere tutti i dispositivi sincronizzati con l‚Äôultima versione del modello pu√≤ essere difficile in ambienti con connettivit√† intermittente o dispositivi che vanno offline periodicamente. Inoltre, √® fondamentale mantenere la coerenza nel modello addestrato, soprattutto quando si ha a che fare con un‚Äôampia gamma di dispositivi con diverse distribuzioni dei dati e frequenze di aggiornamento. Ci√≤ richiede sofisticate strategie di sincronizzazione e aggregazione per garantire che il modello finale rifletta accuratamente gli addestramenti da tutti i dispositivi.\nConsumo Energetico: Il consumo energetico dei dispositivi client in FL √® un fattore critico, in particolare per i dispositivi alimentati a batteria come smartphone e altri dispositivi TinyML/IoT. Le richieste di elaborazione dei modelli di training a livello locale possono portare a un notevole consumo della batteria, il che potrebbe scoraggiare la partecipazione continua al processo FL. √à essenziale bilanciare i requisiti di elaborazione dei modelli di training con l‚Äôefficienza energetica. Ci√≤ comporta l‚Äôottimizzazione di algoritmi e processi di training per ridurre il consumo energetico e ottenere risultati di addestramento efficaci. Garantire un funzionamento efficiente dal punto di vista energetico √® fondamentale per l‚Äôaccettazione da parte dell‚Äôutente e la sostenibilit√† dei sistemi FL.\n\n\nCaso di Studio: Addestramento Federato per Set di Dati Sanitari Collaborativi\nNel settore sanitario e farmaceutico, le organizzazioni spesso detengono grandi quantit√† di dati preziosi, ma condividerli direttamente √® irto di sfide. Normative severe come GDPR e HIPAA, nonch√© preoccupazioni sulla protezione della propriet√† intellettuale, rendono quasi impossibile combinare set di dati tra aziende. Tuttavia, la collaborazione rimane essenziale per settori in evoluzione come la scoperta di farmaci e l‚Äôassistenza ai pazienti. L‚Äôaddestramento federato offre una soluzione unica consentendo alle aziende di addestrare in modo collaborativo modelli di apprendimento automatico senza mai condividere i propri dati grezzi. Questo approccio garantisce che ogni organizzazione mantenga il pieno controllo dei propri dati, continuando a beneficiare delle intuizioni collettive del gruppo.\nIl progetto MELLODDY, un‚Äôiniziativa fondamentale in Europa, esemplifica come l‚Äôaddestramento federato possa superare queste barriere (Heyndrickx et al. 2023). MELLODDY ha riunito dieci aziende farmaceutiche per creare la pi√π grande libreria di composti chimici condivisa mai assemblata, che comprende oltre 21 milioni di molecole e 2,6 miliardi di dati sperimentali. Nonostante lavorassero con dati sensibili e proprietari, le aziende hanno collaborato in modo sicuro per migliorare i modelli predittivi per lo sviluppo dei farmaci.\n\nHeyndrickx, Wouter, Lewis Mervin, Tobias Morawietz, No√© Sturm, Lukas Friedrich, Adam Zalewski, Anastasia Pentina, et al. 2023. ¬´Melloddy: Cross-pharma federated learning at unprecedented scale unlocks benefits in qsar without compromising proprietary information¬ª. Journal of chemical information and modeling 64 (7): 2331‚Äì44. https://pubs.acs.org/doi/10.1021/acs.jcim.3c00799.\nI risultati sono stati notevoli. Mettendo in comune le informazioni tramite l‚Äôaddestramento federato, ogni azienda ha migliorato significativamente la propria capacit√† di identificare promettenti farmaci candidati. L‚Äôaccuratezza predittiva √® migliorata mentre i modelli hanno anche acquisito una pi√π ampia applicabilit√† a diversi set di dati. MELLODDY ha dimostrato che l‚Äôaddestramento federato non solo preserva la privacy, ma sblocca anche nuove opportunit√† di innovazione consentendo una collaborazione su larga scala basata sui dati. Questo approccio evidenzia un futuro in cui le aziende possono lavorare insieme per risolvere problemi complessi senza sacrificare la sicurezza o la propriet√† dei dati.\n\n\n\n14.8.3 Machine Unlearning\n\nIdea Centrale\nIl ‚ÄúMachine unlearning‚Äù √® un processo abbastanza nuovo che descrive come l‚Äôinfluenza di un sottoinsieme di dati di training pu√≤ essere rimossa dal modello. Sono stati utilizzati diversi metodi per eseguire l‚Äôunlearning automatico e rimuovere l‚Äôinfluenza di un sottoinsieme di dati di training dal modello finale. Un approccio di base potrebbe consistere semplicemente nel perfezionare il modello per pi√π epoche solo sui dati che dovrebbero essere ricordati per ridurre l‚Äôinfluenza dei dati ‚Äúdimenticati‚Äù dal modello. Poich√© questo approccio non rimuove esplicitamente l‚Äôinfluenza dei dati che dovrebbero essere cancellati, sono ancora possibili attacchi di inferenza di appartenenza, quindi i ricercatori hanno adottato altri approcci per disimparare i dati da un modello in modo esplicito. Un tipo di approccio adottato dai ricercatori include l‚Äôadeguamento della funzione di perdita del modello per trattare le perdite del ‚Äúset di dimenticanza esplicito‚Äù (dati da disimparare) e del ‚Äúset di conservazione‚Äù (dati rimanenti che dovrebbero ancora essere ricordati) in modo diverso (Tarun et al. 2022; Khan e Swaroop 2021). Figura¬†14.14 illustra alcune delle applicazioni di Machine-unlearning.\n\nTarun, Ayush K, Vikram S Chundawat, Murari Mandal, e Mohan Kankanhalli. 2022. ¬´Deep Regression Unlearning¬ª. ArXiv preprint abs/2210.08196 (ottobre). http://arxiv.org/abs/2210.08196v2.\n\nKhan, Mohammad Emtiyaz, e Siddharth Swaroop. 2021. ¬´Knowledge-Adaptation Priors¬ª. In Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, a cura di Marc‚ÄôAurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, e Jennifer Wortman Vaughan, 19757‚Äì70. https://proceedings.neurips.cc/paper/2021/hash/a4380923dd651c195b1631af7c829187-Abstract.html.\n\n\n\n\n\n\nFigura¬†14.14: Applicazioni di Machine Unlearning. Fonte: BBVA OpenMind\n\n\n\n\n\nCaso di Studio: L‚ÄôEsperimento di Harry Potter\nAlcuni ricercatori hanno dimostrato un esempio concreto di approcci di disapprendimento automatico applicati ai modelli di machine learning SOTA attraverso l‚Äôaddestramento di un LLM, LLaMA2-7b, per disimparare qualsiasi riferimento a Harry Potter (Eldan e Russinovich 2023). Sebbene questo modello abbia richiesto 184K ore di GPU per il pre-addestramento, √® bastata solo 1 ora di GPU di messa a punto per cancellare la capacit√† del modello di generare o richiamare contenuti correlati a Harry Potter senza compromettere in modo evidente l‚Äôaccuratezza della generazione di contenuti non correlati a Harry Potter. Figura¬†14.15 mostra come l‚Äôoutput del modello cambia prima (colonna Llama-7b-chat-hf) e dopo (colonna Llama-b messa a punto) che si √® verificato il disapprendimento.\n\n\n\n\n\n\nFigura¬†14.15: Llama disapprendimento di Harry Potter. Fonte: Eldan e Russinovich (2023).\n\n\nEldan, Ronen, e Mark Russinovich. 2023. ¬´Who‚Äôs Harry Potter? Approximate Unlearning in LLMs¬ª. ArXiv preprint abs/2310.02238 (ottobre). http://arxiv.org/abs/2310.02238v2.\n\n\n\n\nAltri Utilizzi\n\nRimozione di dati avversari\n√à stato precedentemente dimostrato che i modelli di deep learning sono vulnerabili ad attacchi avversari, in cui l‚Äôaggressore genera dati avversari simili ai dati di training originali, in cui un essere umano non riesce a distinguere tra i dati reali e quelli fabbricati. I dati avversari fanno s√¨ che il modello emetta previsioni errate, il che potrebbe avere conseguenze negative in varie applicazioni, tra cui le previsioni di diagnosi sanitaria. Il disapprendimento automatico √® stato utilizzato per disimparare l‚Äôinfluenza dei dati avversari, impedendo cos√¨ che queste previsioni errate si verifichino e causino danni.\n\n\n\n\n14.8.4 Crittografia Omomorfica\n\nIdea Centrale\nLa crittografia omomorfica √® una forma di crittografia che consente di eseguire calcoli su testo cifrato, generando un risultato crittografato che, una volta decrittografato, corrisponde al risultato delle operazioni eseguite sul testo in chiaro. Ad esempio, moltiplicando due numeri crittografati con crittografia omomorfica si ottiene un prodotto crittografato che decrittografa il prodotto effettivo dei due numeri. Ci√≤ significa che i dati possono essere elaborati in forma crittografata e solo l‚Äôoutput risultante deve essere decrittografato, migliorando significativamente la sicurezza dei dati, in particolare per le informazioni sensibili.\nLa crittografia omomorfica consente calcoli esternalizzati su dati crittografati senza esporre i dati stessi esternamente per eseguire le operazioni. Tuttavia, solo determinati calcoli come addizione e moltiplicazione sono supportati negli schemi parzialmente omomorfici. La ‚ÄúFully Homomorphic Encryption (FHE)‚Äù [crittografia completamente omomorfica], in grado di gestire qualsiasi calcolo, √® ancora pi√π complessa. Il numero di possibili operazioni √® limitato prima che l‚Äôaccumulo di rumore corrompa il testo cifrato.\nPer utilizzare la crittografia omomorfica su diverse entit√†, le chiavi pubbliche generate con cura devono essere scambiate per le operazioni su dati crittografati separatamente. Questa tecnica di crittografia avanzata consente paradigmi di calcolo sicuri precedentemente impossibili, ma richiede competenze specifiche per essere implementata correttamente nei sistemi del mondo reale.\n\n\nVantaggi\nLa crittografia omomorfica consente l‚Äôaddestramento del modello di apprendimento automatico e l‚Äôinferenza sui dati crittografati, assicurando che gli input sensibili e i valori intermedi rimangano riservati. Ci√≤ √® fondamentale in ambito sanitario, finanziario, genetico e altri domini, che si affidano sempre di pi√π al ML per analizzare set di dati sensibili e regolamentati contenenti miliardi di dati personali.\nLa crittografia omomorfica ostacola attacchi come l‚Äôestrazione del modello e l‚Äôinferenza dell‚Äôappartenenza che potrebbero esporre dati privati utilizzati nei flussi di lavoro ML. Fornisce un‚Äôalternativa ai TEE che utilizzano enclave hardware per l‚Äôelaborazione riservata. Tuttavia, gli schemi attuali hanno elevati overhead computazionali e limitazioni algoritmiche che limitano le applicazioni del mondo reale.\nLa crittografia omomorfica realizza la visione decennale di elaborazione multi-parti sicura consentendo l‚Äôelaborazione su testi cifrati. Concepiti negli anni ‚Äô70, i primi sistemi crittografici completamente omomorfici sono emersi nel 2009, consentendo elaborazioni arbitrarie. La ricerca in corso sta rendendo queste tecniche pi√π efficienti e pratiche.\nLa crittografia omomorfica mostra grandi promesse nell‚Äôabilitare l‚Äôapprendimento automatico che preserva la privacy in base alle normative emergenti sui dati. Tuttavia, dati i vincoli, si dovrebbe valutare attentamente la sua applicabilit√† rispetto ad altri approcci di elaborazione confidenziale. Esistono ampie risorse per esplorare la crittografia omomorfica e monitorare i progressi nell‚Äôattenuare le barriere all‚Äôadozione.\n\n\nMeccanica\n\nCrittografia dei Dati: Prima che i dati vengano elaborati o inviati a un modello ML, vengono crittografati utilizzando uno schema di crittografia omomorfica e una chiave pubblica. Ad esempio, la crittografia dei numeri \\(x\\) e \\(y\\) genera i testi cifrati \\(E(x)\\) e \\(E(y)\\).\nCalcolo sul Testo Cifrato: L‚Äôalgoritmo ML elabora direttamente i dati crittografati. Ad esempio, la moltiplicazione dei testi cifrati \\(E(x)\\) e \\(E(y)\\) genera \\(E(xy)\\). √à possibile eseguire anche un training del modello pi√π complesso sui testi cifrati.\nCrittografia del Risultato: Il risultato \\(E(xy)\\) rimane crittografato e pu√≤ essere decrittografato solo da qualcuno con la chiave privata corrispondente per rivelare il prodotto effettivo \\(xy\\).\n\nSolo le parti autorizzate con la chiave privata possono decrittografare gli output finali, proteggendo lo stato intermedio. Tuttavia, il rumore si accumula con ogni operazione, impedendo ulteriori calcoli senza decrittazione.\nOltre all‚Äôassistenza sanitaria, la crittografia omomorfica consente il calcolo riservato per applicazioni come il rilevamento di frodi finanziarie, analisi assicurative, ricerca genetica e altro ancora. Offre un‚Äôalternativa a tecniche come il calcolo multipartitico e i TEE. La ricerca in corso migliora l‚Äôefficienza e le capacit√†.\nStrumenti come HElib, SEAL e TensorFlow HE forniscono librerie per esplorare l‚Äôimplementazione della crittografia omomorfica in pipeline di apprendimento automatico nel mondo reale.\n\n\nCompromessi\nPer molte applicazioni in tempo reale ed embedded, la crittografia completamente omomorfica rimane poco pratica per i seguenti motivi.\nSovraccarico Computazionale: La crittografia omomorfica impone sovraccarichi computazionali molto elevati, spesso con conseguenti rallentamenti di oltre 100 volte per le applicazioni ML del mondo reale. Ci√≤ la rende poco pratica per molti utilizzi sensibili al tempo o con risorse limitate. L‚Äôhardware ottimizzato e la parallelizzazione possono alleviare, ma non eliminare, questo problema.\nComplessit√† di Implementazione Gli algoritmi sofisticati richiedono una profonda competenza in crittografia per essere implementati correttamente. Sfumature come la compatibilit√† del formato con modelli ML in virgola mobile e la gestione scalabile delle chiavi pongono ostacoli. Questa complessit√† ostacola l‚Äôadozione pratica diffusa.\nLimitazioni Algoritmiche: Gli schemi attuali limitano le funzioni e la profondit√† dei calcoli supportati, limitando i modelli e i volumi di dati che possono essere elaborati. La ricerca in corso sta spingendo questi limiti, ma permangono delle restrizioni.\nAccelerazione Hardware: La crittografia omomorfica richiede hardware specializzato, come processori sicuri o coprocessori con TEE, che aggiungono costi di progettazione e infrastruttura.\nProgetti Ibridi: Anzich√© crittografare interi flussi di lavoro, l‚Äôapplicazione selettiva della crittografia omomorfica a sotto-componenti critici pu√≤ ottenere protezione riducendo al minimo i costi generali.\n\n\n\n\n\n\nEsercizio¬†14.2: Crittografia Omomorfica\n\n\n\n\n\nLa potenza del calcolo crittografato viene sbloccata tramite la crittografia omomorfica, un approccio trasformativo in cui i calcoli vengono eseguiti direttamente sui dati crittografati, garantendo la tutela della privacy durante tutto il processo. Questo Colab esplora i principi del calcolo su numeri crittografati senza esporre i dati sottostanti. Si immagini uno scenario in cui un modello di apprendimento automatico viene addestrato su dati a cui non √® possibile accedere direttamente: tale √® la forza della crittografia omomorfica.\n\n\n\n\n\n\n\n14.8.5 Secure Multiparty Communication\n\nIdea Centrale\nLa Multi-Party Communication (MPC) consente a pi√π parti di calcolare congiuntamente una funzione sui propri input, garantendo al contempo la riservatezza degli input di ciascuna parte. Ad esempio, due organizzazioni possono collaborare all‚Äôaddestramento di un modello di apprendimento automatico combinando set di dati senza rivelare reciprocamente informazioni sensibili. I protocolli MPC sono essenziali laddove le normative sulla privacy e sulla riservatezza limitano la condivisione diretta dei dati, come nel settore sanitario o finanziario.\nMPC divide il calcolo in parti che ogni partecipante esegue in modo indipendente utilizzando i propri dati privati. Questi risultati vengono quindi combinati per rivelare solo l‚Äôoutput finale, preservando la privacy dei valori intermedi. Vengono utilizzate tecniche crittografiche per garantire che i risultati parziali rimangano privati in modo dimostrabile.\nPrendiamo un semplice esempio di protocollo MPC. Uno dei protocolli MPC pi√π basilari √® l‚Äôaddizione sicura di due numeri. Ogni parte suddivide il suo input in quote casuali che vengono distribuite segretamente. Si scambiano le quote e calcolano localmente la somma delle quote, che ricostruisce la somma finale senza rivelare i singoli input. Ad esempio, se Alice ha input x e Bob ha input y:\n\nAlice genera \\(x_1\\) casuale e imposta \\(x_2 = x - x_1\\)\nBob genera \\(y_1\\) casuale e imposta \\(y_2 = y - y_1\\)\nAlice invia \\(x_1\\) a Bob, Bob invia \\(y_1\\) ad Alice (mantenendo segreti \\(x_2\\) e \\(y_2\\))\nAlice calcola \\(x_2 + y_1 = s_1\\), Bob calcola \\(x_1 + y_2 = s_2\\)\n\\(s_1 + s_2 = x + y\\) √® la somma finale, senza rivelare \\(x\\) o \\(y\\).\n\nGli input individuali di Alice e Bob (\\(x\\) e \\(y\\)) rimangono privati e ciascuna parte rivela solo un numero associato ai propri input originali. Grazie ai risultati casuali, non viene rivelata alcuna informazione sui numeri originali.\nConfronto Sicuro: Un‚Äôaltra operazione di base √® un confronto sicuro di due numeri, per determinare quale √® maggiore dell‚Äôaltro. Questo pu√≤ essere fatto usando tecniche come i ‚ÄúYao‚Äôs Garbled Circuits‚Äù [circuiti distorti di Yao] (https://it.wikipedia.org/wiki/Andrew_Chi-Chih_Yao), dove il circuito di confronto √® crittografato per consentire una valutazione congiunta degli input senza trapelare.\nMoltiplicazione Sicura di Matrici: Le operazioni di matrice come la moltiplicazione sono essenziali per l‚Äôapprendimento automatico. Le tecniche MPC (Multiparty Communication) come la condivisione segreta additiva possono essere usate per dividere le matrici in quote casuali, calcolare i prodotti sulle quote e quindi ricostruire il risultato.\nAddestramento Sicuro del Modello: Gli algoritmi di addestramento dell‚Äôapprendimento automatico distribuito come la media federata possono essere resi sicuri usando MPC. Gli aggiornamenti del modello calcolati su dati partizionati in ogni nodo vengono condivisi segretamente tra i nodi e aggregati per addestrare il modello globale senza esporre aggiornamenti individuali.\nL‚Äôidea fondamentale alla base dei protocolli MPC √® quella di dividere il calcolo in passaggi che possono essere eseguiti congiuntamente senza rivelare dati sensibili intermedi. Ci√≤ si ottiene combinando tecniche crittografiche come la condivisione segreta, la crittografia omomorfica, il trasferimento inconsapevole e i circuiti garbled [distorti]. I protocolli MPC consentono il calcolo collaborativo di dati sensibili fornendo al contempo garanzie di privacy dimostrabili. Questa capacit√† di preservazione della privacy √® essenziale per molte applicazioni di apprendimento automatico odierne che coinvolgono pi√π parti che non possono condividere direttamente i propri dati grezzi.\nGli approcci principali utilizzati in MPC includono:\n\nCrittografia omomorfica: La crittografia speciale consente di eseguire calcoli su dati crittografati senza decrittografarli.\nCondivisione segreta: I dati privati vengono suddivisi in quote casuali distribuite a ciascuna parte. I calcoli vengono eseguiti localmente sulle quote e infine ricostruiti.\nTrasferimento inconsapevole: Un protocollo in cui un ricevitore ottiene un sottoinsieme di dati da un mittente, ma il mittente non sa quali dati specifici sono stati trasferiti.\nCircuiti Garbled: La funzione da calcolare √® rappresentata come un circuito booleano crittografato (‚Äúdistorto‚Äù) per consentire una valutazione congiunta senza rivelare gli input.\n\n\n\nCompromessi\nSebbene i protocolli MPC forniscano solide garanzie di privacy, hanno un costo computazionale elevato rispetto ai calcoli semplici. Ogni operazione sicura, come addizione, moltiplicazione, confronto, ecc., richiede pi√π ordini di elaborazione rispetto all‚Äôoperazione equivalente non crittografata. Questo overhead deriva dalle tecniche crittografiche sottostanti:\n\nNella crittografia parzialmente omomorfica, ogni calcolo su testi cifrati richiede costose operazioni a chiave pubblica. La crittografia completamente omomorfica ha overhead ancora pi√π elevati.\nLa condivisione segreta divide i dati in pi√π porzioni, quindi anche le operazioni di base richiedono la manipolazione di molte porzioni.\nIl trasferimento inconsapevole e i circuiti distorti aggiungono mascheramento e crittografia per nascondere i pattern di accesso ai dati e i flussi di esecuzione.\nI sistemi MPC richiedono un‚Äôampia comunicazione e interazione tra le parti per elaborare congiuntamente quote/testi cifrati.\n\nDi conseguenza, i protocolli MPC possono rallentare i calcoli di 3-4 ordini di grandezza rispetto alle implementazioni semplici. Ci√≤ diventa proibitivo per grandi set di dati e modelli. Pertanto, l‚Äôaddestramento di modelli di apprendimento automatico su dati crittografati tramite MPC rimane oggi irrealizzabile per dimensioni di set di dati realistiche a causa del sovraccarico. Sono necessarie ottimizzazioni e approssimazioni intelligenti per rendere pratico l‚ÄôMPC.\nLa ricerca in corso sull‚ÄôMPC colma questo divario di efficienza attraverso progressi crittografici, nuovi algoritmi, hardware affidabile come le enclave SGX e sfruttando acceleratori come GPU/TPU. Tuttavia, nel prossimo futuro, sar√† necessario un certo grado di approssimazione e compromesso sulle prestazioni per scalare MPC in modo da soddisfare le esigenze dei sistemi di apprendimento automatico del mondo reale.\n\n\n\n14.8.6 Generazione di Dati Sintetici\n\nIdea Centrale\nLa generazione di dati sintetici √® emersa come un importante approccio di apprendimento automatico per la tutela della privacy che consente di sviluppare e testare modelli senza esporre dati utente reali. L‚Äôidea chiave √® quella di addestrare modelli generativi su set di dati del mondo reale e quindi campionare da questi modelli per sintetizzare dati artificiali che statisticamente corrispondono alla distribuzione dei dati originali ma non contengono informazioni effettive dell‚Äôutente. Ad esempio, tecniche come GAN, VAE e aumento dei dati possono essere utilizzate per produrre dati sintetici che imitano set di dati reali preservando al contempo la privacy. Le simulazioni sono anche comunemente impiegate in scenari in cui i dati sintetici devono rappresentare sistemi complessi, come nella ricerca scientifica o nella pianificazione urbana.\nLa sfida principale della sintesi dei dati √® garantire che gli avversari non possano re-identificare il set di dati originale. Un approccio semplice per ottenere dati sintetici √® aggiungere rumore al set di dati originale, che rischia comunque di far trapelare la privacy. Quando il rumore viene aggiunto ai dati nel contesto della privacy differenziale, vengono utilizzati meccanismi sofisticati basati sulla sensibilit√† dei dati per calibrare la quantit√† e la distribuzione del rumore. Attraverso questi limiti matematicamente rigorosi, la privacy differenziale garantisce generalmente la privacy a un certo livello, che √® l‚Äôobiettivo primario di questa tecnica. Oltre a preservare la privacy, i dati sintetici contrastano molteplici problemi di disponibilit√† dei dati, come set di dati sbilanciati, set di dati scarsi e rilevamento di anomalie.\nI ricercatori possono condividere liberamente questi dati sintetici e collaborare alla modellazione senza rivelare informazioni mediche private. I dati sintetici ben costruiti proteggono la privacy e, al tempo stesso, sono utili per lo sviluppo di modelli accurati. Le tecniche chiave per impedire la ricostruzione dei dati originali includono l‚Äôaggiunta di rumore di privacy differenziale durante l‚Äôaddestramento, l‚Äôapplicazione di vincoli di plausibilit√† e l‚Äôutilizzo di pi√π modelli generativi diversi.\n\n\nVantaggi\nSebbene i dati sintetici possano essere necessari a causa di rischi per la privacy o la conformit√†, sono ampiamente utilizzati nei modelli di apprendimento automatico quando i dati disponibili sono di scarsa qualit√†, scarsi o inaccessibili. I dati sintetici offrono uno sviluppo pi√π efficiente ed efficace semplificando i processi di addestramento, test e distribuzione dei modelli robusti. Consentono ai ricercatori di condividere i modelli pi√π ampiamente senza violare le leggi e le normative sulla privacy. La collaborazione tra gli utenti dello stesso set di dati sar√† facilitata, il che aiuter√† ad ampliare le capacit√† e i progressi nella ricerca ML.\nEsistono diverse motivazioni per l‚Äôutilizzo di dati sintetici nell‚Äôapprendimento automatico:\n\nPrivacy e Conformit√†: I dati sintetici evitano di esporre informazioni personali, consentendo una condivisione e una collaborazione pi√π aperte. Ci√≤ √® importante quando si lavora con set di dati sensibili come cartelle cliniche o informazioni finanziarie.\nScarsit√† di dati: Quando non sono disponibili dati reali sufficienti, i dati sintetici possono aumentare i set di dati di addestramento. Ci√≤ migliora l‚Äôaccuratezza del modello quando i dati limitati rappresentano un collo di bottiglia.\nTest del modello: I dati sintetici forniscono sandbox protetti dalla privacy per testare le prestazioni del modello, risolvere i problemi e monitorare i bias.\nEtichettatura dei dati: I dati di training etichettati di alta qualit√† sono spesso scarsi e costosi. I dati sintetici possono aiutare a generare automaticamente esempi etichettati.\n\n\n\nCompromessi\nSebbene i dati sintetici cerchino di rimuovere qualsiasi prova del set di dati originale, la perdita della privacy rappresenta comunque un rischio, poich√© i dati sintetici imitano i dati originali. Le informazioni statistiche e la distribuzione sono simili, se non uguali, tra i dati originali e sintetici. Ricampionando dalla distribuzione, gli avversari potrebbero comunque essere in grado di recuperare i campioni di addestramento originali. A causa dei loro processi di apprendimento e complessit√† intrinseci, le reti neurali potrebbero rivelare accidentalmente informazioni sensibili sui dati di addestramento originali.\nUna sfida fondamentale con i dati sintetici √® il potenziale divario tra le distribuzioni dei dati sintetici e quelli del mondo reale. Nonostante i progressi nelle tecniche di modellazione generativa, i dati sintetici potrebbero catturare solo parzialmente la complessit√†, la diversit√† e i pattern sfumati dei dati reali. Ci√≤ pu√≤ limitare l‚Äôutilit√† dei dati sintetici per l‚Äôaddestramento robusto di modelli di apprendimento automatico. Valutare rigorosamente la qualit√† dei dati sintetici tramite metodi avversari e confrontare le prestazioni del modello con i benchmark dei dati reali aiuta a valutare e migliorare la fedelt√†. Tuttavia, intrinsecamente, i dati sintetici rimangono un‚Äôapprossimazione.\nUn‚Äôaltra preoccupazione critica sono i rischi per la privacy dei dati sintetici. I modelli generativi possono far trapelare informazioni identificabili sugli individui nei dati di training, il che potrebbe consentire la ricostruzione di informazioni private. Gli attacchi avversari emergenti dimostrano le sfide nel prevenire la perdita di identit√† dalle pipeline di generazione di dati sintetici. Tecniche come la privacy differenziale possono aiutare a salvaguardare la privacy, ma comportano compromessi nell‚Äôutilit√† dei dati. Esiste una tensione intrinseca tra la produzione di dati sintetici validi e la protezione completa dei dati di training sensibili, che deve essere bilanciata.\nUlteriori insidie dei dati sintetici includono distorsioni amplificate, etichettature errate, sovraccarico computazionale per l‚Äôaddestramento di modelli generativi, costi di archiviazione e mancata contabilizzazione di nuovi dati fuori distribuzione. Sebbene questi siano secondari rispetto al divario sintetico-reale e ai rischi per la privacy, rimangono considerazioni importanti quando si valuta l‚Äôidoneit√† dei dati sintetici per particolari attivit√† di apprendimento automatico. Come con qualsiasi tecnica, i vantaggi dei dati sintetici comportano compromessi e limitazioni intrinseche che richiedono strategie di mitigazione ponderate.\n\n\n\n14.8.7 Riepilogo\nSebbene tutte le tecniche di cui abbiamo discusso finora mirino a consentire un apprendimento automatico che salvaguardi la privacy, esse implicano meccanismi e compromessi distinti. Fattori come vincoli computazionali, ipotesi di fiducia richieste, modelli di minaccia e caratteristiche dei dati aiutano a guidare il processo di selezione per un caso d‚Äôuso particolare. Tuttavia, trovare il giusto equilibrio tra privacy, accuratezza ed efficienza richiede sperimentazione e valutazione empirica per molte applicazioni. Tabella¬†14.2 √® una tabella di confronto delle principali tecniche di apprendimento automatico che salvaguardano la privacy e dei loro pro e contro:\n\n\n\nTabella¬†14.2: Confronto di tecniche per l‚Äôapprendimento automatico che tutela la privacy.\n\n\n\n\n\n\n\n\n\n\nTecnica\nPro\nContro\n\n\n\n\nPrivacy Differenziale\n\nForti garanzie formali di privacy\nRobusto per attacchi dati ausiliari\nVersatile per molti tipi di dati e analisi\n\n\nPerdita di accuratezza dovuta all‚Äôaggiunta di rumore\nOverhead computazionale per analisi di sensibilit√† e generazione di rumore\n\n\n\nAddestramento Federato\n\nConsente l‚Äôapprendimento collaborativo senza condividere dati grezzi\nI dati rimangono decentralizzati migliorando la sicurezza\nNessuna necessit√† di elaborazione crittografata\n\n\nOverhead di comunicazione aumentato\nConvergenza del modello potenzialmente pi√π lenta\nCapacit√† di dispositivi client non uniformi\n\n\n\nMachine Unlearning\n\nConsente la rimozione selettiva dell‚Äôinfluenza dei dati dai modelli\nUtile per la conformit√† alle normative sulla privacy\nImpedisce la conservazione involontaria di dati avversari o obsoleti\n\n\nPu√≤ degradare le prestazioni del modello su attivit√† correlate\nComplessit√† di implementazione in modelli su larga scala\nRischio di unlearning incompleto o inefficace\n\n\n\nCrittografia Omomorfica\n\nConsente il calcolo su dati crittografati\nPreviene l‚Äôesposizione allo stato intermedio\n\n\nCosti di calcolo estremamente elevati\nImplementazioni crittografiche complesse\nRestrizioni sui tipi di funzione\n\n\n\nCalcolo Multi-Parte Sicuro\n\nConsente il calcolo congiunto su dati sensibili\nFornisce garanzie di privacy crittografica\nProtocolli flessibili per varie funzioni\n\n\nCosti di calcolo molto elevati\nComplessit√† di implementazione\nVincoli algoritmici sulla profondit√† della funzione\n\n\n\nGenerazione di Dati Sintetici\n\nConsente la condivisione dei dati senza perdite\nAttenua i problemi di scarsit√† di dati\n\n\nDivario sintetico-reale nelle distribuzioni\nPotenziale per la ricostruzione di dati privati\nBias e problemi di etichettatura",
    "crumbs": [
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Sicurezza e Privacy</span>"
    ]
  },
  {
    "objectID": "contents/core/privacy_security/privacy_security.it.html#conclusione",
    "href": "contents/core/privacy_security/privacy_security.it.html#conclusione",
    "title": "14¬† Sicurezza e Privacy",
    "section": "14.9 Conclusione",
    "text": "14.9 Conclusione\nLa sicurezza hardware del machine learning √® fondamentale poich√© i sistemi ML embedded vengono sempre pi√π implementati in domini critici per la sicurezza come dispositivi medici, controlli industriali e veicoli autonomi. Abbiamo esplorato varie minacce che spaziano da bug hardware, attacchi fisici, canali laterali, rischi della supply chain, ecc. Difese come TEE, Secure Boot, PUF e moduli di sicurezza hardware forniscono una protezione multi-livello su misura per dispositivi embedded con risorse limitate.\nTuttavia, una vigilanza continua √® essenziale per tracciare i vettori di attacco emergenti e affrontare potenziali vulnerabilit√† tramite pratiche di ingegneria sicure durante l‚Äôintero ciclo di vita dell‚Äôhardware. Man mano che ML e ML embedded si diffondono, il mantenimento di rigorose basi di sicurezza che corrispondano al ritmo accelerato di innovazione del settore rimane imperativo.",
    "crumbs": [
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Sicurezza e Privacy</span>"
    ]
  },
  {
    "objectID": "contents/core/privacy_security/privacy_security.it.html#sec-security-and-privacy-resource",
    "href": "contents/core/privacy_security/privacy_security.it.html#sec-security-and-privacy-resource",
    "title": "14¬† Sicurezza e Privacy",
    "section": "14.10 Risorse",
    "text": "14.10 Risorse\nEcco un elenco curato di risorse per supportare studenti e insegnanti nei loro percorsi di apprendimento e insegnamento. Lavoriamo continuamente per espandere questa raccolta e presto aggiungeremo nuovi esercizi.\n\n\n\n\n\n\nSlide\n\n\n\n\n\nQueste slide sono uno strumento prezioso per gli insegnanti per tenere lezioni e per gli studenti per rivedere il materiale secondo il proprio ritmo. Incoraggiamo studenti e docenti a sfruttare queste slide per migliorare la loro comprensione e facilitare un trasferimento efficace delle conoscenze.\n\nSecurity.\nPrivacy.\nMonitoring after Deployment.\n\n\n\n\n\n\n\n\n\n\nVideo\n\n\n\n\n\n\nVideo¬†14.1\nVideo¬†14.2\nVideo¬†14.3\n\n\n\n\n\n\n\n\n\n\nEsercizi\n\n\n\n\n\nPer rafforzare i concetti trattati in questo capitolo, abbiamo curato una serie di esercizi che sfidano gli studenti ad applicare le proprie conoscenze e ad approfondire la propria comprensione.\n\nEsercizio¬†14.1\nEsercizio¬†14.2",
    "crumbs": [
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Sicurezza e Privacy</span>"
    ]
  },
  {
    "objectID": "contents/core/responsible_ai/responsible_ai.it.html",
    "href": "contents/core/responsible_ai/responsible_ai.it.html",
    "title": "15¬† IA Responsabile",
    "section": "",
    "text": "15.1 Panoramica\nI modelli di apprendimento automatico sono sempre pi√π utilizzati per automatizzare le decisioni in ambiti sociali ad alto rischio come sanit√†, giustizia penale e occupazione. Tuttavia, senza un‚Äôattenzione deliberata, questi algoritmi possono perpetuare pregiudizi, violare la privacy o causare altri danni. Ad esempio, un modello di approvazione di prestiti addestrato esclusivamente su dati provenienti da quartieri ad alto reddito potrebbe svantaggiare i richiedenti provenienti da aree a basso reddito. Ci√≤ motiva la necessit√† di un apprendimento automatico responsabile, ovvero la creazione di modelli equi, responsabili, trasparenti ed etici.\nDiversi principi fondamentali sono alla base di un apprendimento automatico responsabile. L‚Äôequit√† garantisce che i modelli non discriminino in base a genere, razza, et√† e altri attributi. La spiegabilit√† consente agli esseri umani di interpretare i comportamenti del modello e migliorare la trasparenza. Le tecniche di robustezza e sicurezza prevengono vulnerabilit√† come gli esempi avversari. Test e convalide rigorosi aiutano a ridurre le debolezze indesiderate del modello o gli effetti collaterali.\nL‚Äôimplementazione di un apprendimento automatico responsabile presenta sfide sia tecniche che etiche. Gli sviluppatori devono confrontarsi con la definizione matematica dell‚Äôequit√†, bilanciando obiettivi concorrenti come accuratezza e interpretabilit√† e assicurando dati di training di qualit√†. Le organizzazioni devono anche allineare incentivi, politiche e cultura per sostenere l‚ÄôIA etica.\nQuesto capitolo fornir√† gli strumenti per valutare criticamente i sistemi di IA e contribuire allo sviluppo di applicazioni di apprendimento automatico utili ed etiche, coprendo le basi, i metodi e le implicazioni nel mondo reale dell‚ÄôML responsabile. I principi dell‚ÄôML responsabile discussi sono conoscenze cruciali poich√© gli algoritmi mediano pi√π aspetti della societ√† umana.",
    "crumbs": [
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>IA Responsabile</span>"
    ]
  },
  {
    "objectID": "contents/core/responsible_ai/responsible_ai.it.html#terminologia",
    "href": "contents/core/responsible_ai/responsible_ai.it.html#terminologia",
    "title": "15¬† IA Responsabile",
    "section": "15.2 Terminologia",
    "text": "15.2 Terminologia\nL‚ÄôIA responsabile riguarda lo sviluppo di un‚ÄôIA che abbia un impatto positivo sulla societ√† in base all‚Äôetica e ai valori umani. Non esiste una definizione universalmente accettata di ‚ÄúIA responsabile‚Äù, ma ecco un riassunto di come viene comunemente descritta. L‚ÄôIA responsabile si riferisce alla progettazione, allo sviluppo e all‚Äôimplementazione di sistemi di intelligenza artificiale in modo etico e socialmente utile. L‚Äôobiettivo principale √® creare un‚ÄôIA affidabile, imparziale, equa, trasparente, responsabile e sicura. Sebbene non esista una definizione canonica, si ritiene generalmente che l‚ÄôIA responsabile comprenda principi quali:\n\nEquit√†: Evitare pregiudizi, discriminazioni e potenziali danni a determinati gruppi o popolazioni\nSpiegabilit√†: Consentire agli esseri umani di comprendere e interpretare il modo in cui i modelli di IA prendono decisioni\nTrasparenza: Comunicare apertamente come funzionano, sono costruiti e valutati i sistemi di IA\nResponsabilit√†: Avere processi per determinare responsabilit√† e obblighi per guasti o impatti negativi dell‚ÄôIA\nRobustezza: Garantire che i sistemi di IA siano sicuri, affidabili e si comportino come previsto\nPrivacy: Proteggere i dati sensibili degli utenti e rispettare le leggi e l‚Äôetica sulla privacy\n\nMettere in pratica questi principi implica tecniche tecniche, politiche aziendali, quadri di governance e filosofia morale. Sono inoltre in corso dibattiti sulla definizione di concetti ambigui come l‚Äôequit√† e sulla determinazione di come bilanciare obiettivi in competizione.",
    "crumbs": [
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>IA Responsabile</span>"
    ]
  },
  {
    "objectID": "contents/core/responsible_ai/responsible_ai.it.html#principi-e-concetti",
    "href": "contents/core/responsible_ai/responsible_ai.it.html#principi-e-concetti",
    "title": "15¬† IA Responsabile",
    "section": "15.3 Principi e Concetti",
    "text": "15.3 Principi e Concetti\n\n15.3.1 Trasparenza e Spiegabilit√†\nI modelli di apprendimento automatico sono spesso criticati come misteriose ‚Äúscatole nere‚Äù, sistemi opachi in cui non √® chiaro come siano arrivati a particolari previsioni o decisioni. Ad esempio, un sistema di intelligenza artificiale chiamato COMPAS utilizzato per valutare il rischio di recidiva criminale negli Stati Uniti si √® rivelato razzialmente discriminatorio nei confronti degli imputati neri. Tuttavia, l‚Äôopacit√† dell‚Äôalgoritmo ha reso difficile comprendere e risolvere il problema. Questa mancanza di trasparenza pu√≤ nascondere pregiudizi, errori e carenze.\nSpiegare i comportamenti del modello aiuta a generare fiducia da parte del pubblico e degli esperti del settore e consente di identificare i problemi da affrontare. Le tecniche di interpretabilit√† svolgono un ruolo chiave in questo processo. Ad esempio, LIME (Local Interpretable Model-Agnostic Explanations) evidenzia come le singole funzionalit√† di input contribuiscano a una previsione specifica, mentre i valori Shapley quantificano il contributo di ciascuna funzionalit√† all‚Äôoutput di un modello in base alla teoria dei giochi cooperativi. Le ‚Äúmappe di salienza‚Äù, comunemente utilizzate nei modelli basati su immagini, evidenziano visivamente le aree di un‚Äôimmagine che hanno maggiormente influenzato la decisione del modello. Questi strumenti consentono agli utenti di comprendere la logica del modello.\nOltre ai vantaggi pratici, la trasparenza √® sempre pi√π richiesta dalla legge. Regolamenti come l‚Äô‚ÄúEuropean Union‚Äôs General Data Protection Regulation (GDPR)‚Äù dell‚ÄôUnione Europea impongono alle organizzazioni di fornire spiegazioni per determinate decisioni automatizzate, soprattutto quando hanno un impatto significativo sugli individui. Ci√≤ rende la spiegabilit√† non solo una buona pratica, ma una necessit√† legale in alcuni contesti. Insieme, trasparenza e spiegabilit√† costituiscono pilastri fondamentali per la creazione di sistemi di IA responsabili e affidabili.\n\n\n15.3.2 Equit√†, Bias [pregiudizi] e Discriminazione\nI modelli di ML addestrati su dati storicamente distorti spesso perpetuano e amplificano tali pregiudizi. √à stato dimostrato che gli algoritmi sanitari svantaggiano i pazienti neri sottostimandone le esigenze (Obermeyer et al. 2019). Il riconoscimento facciale deve essere pi√π accurato per le donne e le persone di colore. Tale discriminazione algoritmica pu√≤ avere un impatto negativo profondo sulla vita delle persone.\n\nObermeyer, Ziad, Brian Powers, Christine Vogeli, e Sendhil Mullainathan. 2019. ¬´Dissecting racial bias in an algorithm used to manage the health of populations¬ª. Science 366 (6464): 447‚Äì53. https://doi.org/10.1126/science.aax2342.\nEsistono anche diverse prospettive filosofiche sull‚Äôequit√†, ad esempio, √® pi√π giusto trattare tutti gli individui allo stesso modo o cercare di ottenere risultati uguali per i gruppi? Garantire l‚Äôequit√† richiede di rilevare e mitigare in modo proattivo i pregiudizi nei dati e nei modelli. Tuttavia, raggiungere l‚Äôequit√† perfetta √® tremendamente difficile a causa di definizioni matematiche e prospettive etiche contrastanti. Tuttavia, promuovere l‚Äôequit√† algoritmica e la non discriminazione √® una responsabilit√† fondamentale nello sviluppo dell‚Äôintelligenza artificiale.\n\n\n15.3.3 Privacy e Governance dei Dati\nMantenere la privacy degli individui √® un obbligo etico e un requisito legale per le organizzazioni che implementano sistemi di intelligenza artificiale. Regolamentazioni come il GDPR dell‚ÄôUE impongono protezioni e diritti sulla privacy dei dati, come la possibilit√† di accedere ed eliminare i propri dati.\nTuttavia, massimizzare l‚Äôutilit√† e l‚Äôaccuratezza dei dati per i modelli di addestramento pu√≤ entrare in conflitto con la tutela della privacy: la modellazione della progressione della malattia potrebbe trarre vantaggio dall‚Äôaccesso ai genomi completi dei pazienti, ma la condivisione di tali dati viola ampiamente la privacy.\nUna governance dei dati responsabile implica l‚Äôanonimizzazione attenta dei dati, il controllo dell‚Äôaccesso tramite crittografia, l‚Äôottenimento del consenso informato degli interessati e la raccolta dei dati minimi necessari. Rispettare la privacy √® difficile ma fondamentale man mano che le capacit√† e l‚Äôadozione dell‚Äôintelligenza artificiale si espandono.\n\n\n15.3.4 Sicurezza e Robustezza\nMettere in funzione i sistemi di intelligenza artificiale nel mondo reale richiede di garantire che siano sicuri, affidabili e robusti, soprattutto per gli scenari di interazione umana. Le auto a guida autonoma di Uber e Tesla sono state coinvolte in incidenti mortali a causa di comportamenti non sicuri.\nGli attacchi avversari che alterano in modo sottile i dati di input possono anche ingannare i modelli ML e causare guasti pericolosi se i sistemi non sono resistenti. I deepfake rappresentano un‚Äôaltra area di minaccia emergente.\nVideo¬†15.1 √® un video deepfake di Barack Obama che √® diventato virale qualche anno fa.\n\n\n\n\n\n\nVideo¬†15.1: Fake Obama\n\n\n\n\n\n\nLa promozione della sicurezza richiede test approfonditi, analisi dei rischi, supervisione umana e progettazione di sistemi che combinano pi√π modelli deboli per evitare singoli punti di errore. Rigorosi meccanismi di sicurezza sono essenziali per l‚Äôimplementazione responsabile di un‚ÄôIA efficiente.\n\n\n15.3.5 Responsabilit√† e Governance\nQuando i sistemi di IA alla fine falliscono o producono risultati dannosi, devono esistere meccanismi per affrontare i problemi risultanti, risarcire le parti interessate e assegnare la responsabilit√†. Sia le politiche di responsabilit√† aziendale che le normative governative sono indispensabili per una governance responsabile dell‚ÄôIA. Ad esempio, l‚ÄôArtificial Intelligence Video Interview Act dell‚ÄôIllinois richiede alle aziende di divulgare e ottenere il consenso per l‚Äôanalisi video dell‚ÄôIA, promuovendo la responsabilit√†.\nSenza una chiara responsabilit√†, anche i danni causati involontariamente potrebbero rimanere irrisolti, alimentando ulteriormente l‚Äôindignazione e la sfiducia pubblica. I comitati di vigilanza, le valutazioni di impatto, i processi di risoluzione dei reclami e gli audit indipendenti promuovono lo sviluppo e l‚Äôimplementazione responsabili.",
    "crumbs": [
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>IA Responsabile</span>"
    ]
  },
  {
    "objectID": "contents/core/responsible_ai/responsible_ai.it.html#cloud-edge-e-tiny-ml",
    "href": "contents/core/responsible_ai/responsible_ai.it.html#cloud-edge-e-tiny-ml",
    "title": "15¬† IA Responsabile",
    "section": "15.4 Cloud, Edge e Tiny ML",
    "text": "15.4 Cloud, Edge e Tiny ML\nSebbene questi principi siano ampiamente applicabili a tutti i sistemi di intelligenza artificiale, alcune considerazioni di IA responsabile sono uniche o pronunciate quando si ha a che fare con l‚Äôapprendimento automatico su dispositivi embedded rispetto alla modellazione tradizionale basata su server. Pertanto, presentiamo una tassonomia di alto livello che confronta le considerazioni di intelligenza artificiale responsabile nei sistemi cloud, edge e TinyML.\n\n15.4.1 Spiegabilit√†\nPer l‚Äôapprendimento automatico basato su cloud, le tecniche di spiegabilit√† possono sfruttare risorse di elaborazione significative, consentendo metodi complessi come valori SHAP o approcci basati sul campionamento per interpretare i comportamenti del modello. Ad esempio, il toolkit InterpretML di Microsoft fornisce tecniche di spiegabilit√† su misura per gli ambienti cloud.\nTuttavia, l‚Äôedge ML opera su dispositivi con risorse limitate, richiedendo metodi di spiegabilit√† pi√π leggeri che possono essere eseguiti localmente senza latenza eccessiva. Tecniche come LIME (Ribeiro, Singh, e Guestrin 2016) approssimano le spiegazioni del modello utilizzando modelli lineari o alberi decisionali per evitare calcoli costosi, il che le rende ideali per dispositivi con risorse limitate. Tuttavia, LIME richiede l‚Äôaddestramento di centinaia o persino migliaia di modelli per generare buone spiegazioni, il che √® spesso irrealizzabile dati i vincoli dell‚Äôedge computing. Al contrario, i metodi basati sulla salienza sono spesso molto pi√π rapidi nella pratica, richiedendo solo un singolo passaggio in avanti attraverso la rete per stimare l‚Äôimportanza delle funzionalit√†. Questa maggiore efficienza rende tali metodi pi√π adatti ai dispositivi edge con risorse di elaborazione limitate, in cui le spiegazioni a bassa latenza sono fondamentali.\nDate le ridotte capacit√† hardware, i sistemi embedded pongono le sfide pi√π significative per la spiegabilit√†. Modelli pi√π compatti e dati limitati semplificano la trasparenza intrinseca del modello. Spiegare le decisioni potrebbe non essere fattibile su microcontrollori di grandi dimensioni e con potenza ottimizzata. Il programma Transparent Computing della DARPA cerca di sviluppare una spiegabilit√† con costi di gestione estremamente bassi, in particolare per i dispositivi TinyML come sensori e dispositivi indossabili.\n\n\n15.4.2 Equit√†\nPer il machine learning nel cloud, vasti set di dati e potenza di calcolo consentono di rilevare pregiudizi su grandi popolazioni eterogenee e di mitigarli tramite tecniche come la riponderazione dei campioni di dati. Tuttavia, i pregiudizi possono emergere dagli ampi dati comportamentali utilizzati per addestrare i modelli cloud. Il framework Fairness Flow di Amazon aiuta a valutare l‚Äôequit√† del ML cloud.\nEdge ML si basa su dati limitati sul dispositivo, rendendo pi√π difficile l‚Äôanalisi dei pregiudizi tra gruppi diversi. Tuttavia, i dispositivi edge interagiscono strettamente con gli individui, offrendo un‚Äôopportunit√† di adattamento locale per l‚Äôequit√†. Federated Learning di Google distribuisce l‚Äôaddestramento del modello tra i dispositivi per incorporare le differenze individuali.\nTinyML pone sfide uniche per l‚Äôequit√† con hardware specializzato altamente disperso e dati di addestramento minimi. I test sui pregiudizi sono difficili su dispositivi diversi. La raccolta di dati rappresentativi da molti dispositivi per mitigare i pregiudizi presenta ostacoli di scala e privacy. Gli sforzi di Assured Neuro Symbolic Learning and Reasoning (ANSR) di DARPA sono orientati allo sviluppo di tecniche di equit√† dati i vincoli hardware estremi.\n\n\n15.4.3 Privacy\nPer il cloud ML, grandi quantit√† di dati utente sono concentrate nel cloud, creando rischi di esposizione tramite violazioni. Le tecniche di privacy differenziali aggiungono rumore ai dati cloud per preservare la privacy. Rigidi controlli di accesso e crittografia proteggono i dati cloud a riposo e in transito.\nEdge ML sposta l‚Äôelaborazione dei dati sui dispositivi utente, riducendo la raccolta di dati aggregati ma aumentando la potenziale sensibilit√† poich√© i dati personali risiedono sul dispositivo. Apple utilizza ML on-device e privacy differenziale per addestrare modelli riducendo al minimo la condivisione dei dati. L‚Äôanonimizzazione dei dati e le enclave sicure proteggono i dati on-device.\nTinyML distribuisce i dati su molti dispositivi con risorse limitate, rendendo improbabili le violazioni centralizzate e rendendo difficile l‚Äôanonimizzazione su larga scala. La minimizzazione dei dati e l‚Äôutilizzo di dispositivi edge come intermediari aiutano la privacy di TinyML.\nQuindi, mentre il cloud ML deve proteggere dati centralizzati espansivi, l‚Äôedge ML protegge i dati sensibili on-device e TinyML mira a una condivisione minima dei dati distribuiti a causa dei vincoli. Mentre la privacy √® fondamentale in tutto, le tecniche devono adattarsi all‚Äôambiente. La comprensione delle sfumature consente di selezionare approcci appropriati per la tutela della privacy.\n\n\n15.4.4 Sicurezza\nI principali rischi per la sicurezza del cloud ML includono hacking dei modelli, avvelenamento dei dati e malware che interrompono i servizi cloud. Le tecniche di robustezza come l‚Äôaddestramento avversario, il rilevamento delle anomalie e i modelli diversificati mirano a rafforzare il cloud ML contro gli attacchi. La ridondanza pu√≤ aiutare a prevenire singoli punti di errore.\nEdge ML e TinyML interagiscono con il mondo fisico, quindi l‚Äôaffidabilit√† e la convalida della sicurezza sono fondamentali. Piattaforme di test rigorose come Foretellix generano sinteticamente scenari edge per convalidare la sicurezza. La sicurezza di TinyML √® amplificata da dispositivi autonomi con supervisione limitata. La sicurezza di TinyML spesso si basa sul coordinamento collettivo: sciami di droni mantengono la sicurezza tramite ridondanza. Anche le barriere di controllo fisiche limitano i comportamenti non sicuri dei dispositivi TinyML.\nLe considerazioni sulla sicurezza variano notevolmente tra i domini, riflettendo le loro sfide uniche. Cloud ML si concentra sulla protezione da hacking e violazioni dei dati, Edge ML enfatizza l‚Äôaffidabilit√† grazie alle sue interazioni fisiche con l‚Äôambiente e TinyML spesso si basa sul coordinamento distribuito per mantenere la sicurezza nei sistemi autonomi. Riconoscere queste sfumature √® essenziale per applicare le tecniche di sicurezza appropriate a ciascun dominio.\n\n\n15.4.5 Responsabilit√†\nLa responsabilit√† di Cloud ML si concentra su pratiche aziendali come comitati AI responsabili, carte etiche e processi per affrontare incidenti dannosi. Audit di terze parti e supervisione governativa esterna promuovono la responsabilit√† di Cloud ML.\nLa responsabilit√† di Edge ML √® pi√π complessa con dispositivi distribuiti e frammentazione della supply chain. Le aziende sono responsabili dei dispositivi, ma i componenti provengono da vari fornitori. Gli standard di settore aiutano a coordinare la responsabilit√† di Edge ML tra le parti interessate.\nCon TinyML, i meccanismi di responsabilit√† devono essere tracciati attraverso lunghe e complesse supply chain di circuiti integrati, sensori e altro hardware. Gli schemi di certificazione TinyML aiutano a tracciare la provenienza dei componenti. Le associazioni di categoria dovrebbero idealmente promuovere la responsabilit√† condivisa per TinyML etico.\n\n\n15.4.6 Governance\nLe organizzazioni istituiscono una governance interna per il cloud ML, come comitati etici, audit e gestione del rischio del modello. Anche la governance esterna svolge un ruolo significativo nel garantire responsabilit√† ed equit√†. Abbiamo gi√† introdotto il General Data Protection Regulation (GDPR), che stabilisce requisiti rigorosi per la protezione dei dati e la trasparenza. Tuttavia, non √® l‚Äôunico quadro che guida pratiche di IA responsabili. L‚ÄôAI Bill of Rights stabilisce principi per un uso etico dell‚ÄôIA negli Stati Uniti e il California Consumer Protection Act (CCPA) si concentra sulla salvaguardia della privacy dei dati dei consumatori in California. Gli audit di terze parti rafforzano ulteriormente la governance del ML nel cloud fornendo una supervisione esterna.\nEdge ML √® pi√π decentralizzato e richiede un‚Äôautogovernance responsabile da parte di sviluppatori e aziende che distribuiscono modelli localmente. Le associazioni di settore coordinano la governance tra i fornitori di edge ML e il software aperto aiuta ad allineare gli incentivi per l‚Äôedge ML etico.\nL‚Äôestrema decentralizzazione e complessit√† rendono la governance esterna impraticabile con TinyML. TinyML si basa su protocolli e standard per l‚Äôautogovernance integrati nella progettazione del modello e nell‚Äôhardware. La crittografia consente l‚Äôaffidabilit√† dimostrabile dei dispositivi TinyML.\n\n\n15.4.7 Riepilogo\nTabella¬†15.1 riassume come i principi di intelligenza artificiale responsabile si manifestino in modo diverso nelle architetture cloud, edge e TinyML e come le considerazioni fondamentali si leghino alle loro capacit√† e limitazioni uniche. I vincoli e i compromessi di ogni ambiente modellano il modo in cui affrontiamo la trasparenza, la responsabilit√†, la governance e altri pilastri dell‚Äôintelligenza artificiale responsabile.\n\n\n\nTabella¬†15.1: Confronto dei principi chiave di Cloud ML, Edge ML e TinyML.\n\n\n\n\n\n\n\n\n\n\n\nPrincipio\nCloud ML\nEdge ML\nTinyML\n\n\n\n\nSpiegabilit√†\nSupporta modelli e metodi complessi come SHAP e approcci di campionamento\nRichiede metodi leggeri e a bassa latenza come le mappe di salienza\nGravemente limitato a causa dell‚Äôhardware vincolato\n\n\nEquit√†\nGrandi set di dati consentono il rilevamento e l‚Äôattenuazione dei bias\nI bias localizzati sono pi√π difficili da rilevare ma consentono regolazioni sul dispositivo\nI dati minimi limitano l‚Äôanalisi e l‚Äôattenuazione dei bias\n\n\nPrivacy\nI dati centralizzati sono a rischio di violazioni ma possono sfruttare una crittografia avanzata e la privacy differenziale\nI dati personali sensibili sul dispositivo richiedono protezioni sul dispositivo\nI dati distribuiti riducono i rischi centralizzati ma pongono sfide per l‚Äôanonimizzazione\n\n\nSicurezza\nVulnerabile all‚Äôhacking e agli attacchi su larga scala\nLe interazioni nel mondo reale rendono fondamentale l‚Äôaffidabilit√†\nRichiede meccanismi di sicurezza distribuiti a causa dell‚Äôautonomia\n\n\nResponsabilit√†\nLe policy e gli audit aziendali garantiscono la responsabilit√†\nLe catene di fornitura frammentate complicano la responsabilit√†\nTracciabilit√† richiesta su lunghe e complesse catene hardware\n\n\nGovernance\nSupervisione esterna e normative come GDPR o CCPA sono fattibili\nRichiede autogoverno da parte di sviluppatori e stakeholder\nSi basa su protocolli integrati e garanzie crittografiche",
    "crumbs": [
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>IA Responsabile</span>"
    ]
  },
  {
    "objectID": "contents/core/responsible_ai/responsible_ai.it.html#aspetti-tecnici",
    "href": "contents/core/responsible_ai/responsible_ai.it.html#aspetti-tecnici",
    "title": "15¬† IA Responsabile",
    "section": "15.5 Aspetti Tecnici",
    "text": "15.5 Aspetti Tecnici\n\n15.5.1 Rilevamento e Mitigazione dei Pregiudizi\nI modelli di apprendimento automatico, come qualsiasi sistema complesso, possono talvolta presentare ‚Äúbias‚Äù [distorsioni] nelle loro previsioni. Queste distorsioni possono manifestarsi in prestazioni insufficienti per gruppi specifici o in decisioni che limitano inavvertitamente l‚Äôaccesso a determinate opportunit√† o risorse (Buolamwini e Gebru 2018). Comprendere e affrontare queste distorsioni √® fondamentale, soprattutto perch√© i sistemi di apprendimento automatico sono sempre pi√π utilizzati in settori sensibili come prestiti, assistenza sanitaria e giustizia penale.\nPer valutare e affrontare questi problemi, l‚Äôequit√† nell‚Äôapprendimento automatico viene in genere valutata analizzando gli ‚Äúattributi del sottogruppo‚Äù, che sono caratteristiche non correlate all‚Äôattivit√† di previsione, come posizione geografica, fascia d‚Äôet√†, livello di reddito, razza, genere o religione. Ad esempio, in un modello di previsione di inadempienza del prestito, i sottogruppi potrebbero includere razza, genere o religione. Quando i modelli vengono addestrati con l‚Äôunico obiettivo di massimizzare l‚Äôaccuratezza, potrebbero trascurare le differenze di performance tra questi sottogruppi, con conseguenti potenziali risultati distorti o incoerenti.\nQuesto concetto √® illustrato in Figura¬†15.1, che visualizza le performance di un modello di apprendimento automatico che prevede il rimborso del prestito per due sottogruppi, Sottogruppo A (blu) e Sottogruppo B (rosso). Ogni individuo nel set di dati √® rappresentato da un simbolo: i pi√π (+) indicano gli individui che rimborseranno i loro prestiti (veri positivi), mentre i cerchi (O) indicano gli individui che saranno inadempienti sui loro prestiti (veri negativi). L‚Äôobiettivo del modello √® classificare correttamente questi individui in rimborsatori e inadempienti.\n\n\n\n\n\n\nFigura¬†15.1: Illustra il compromesso nell‚Äôimpostazione delle soglie di classificazione per due sottogruppi (A e B) in un modello di rimborso del prestito. I pi√π (+) rappresentano i veri positivi (rimborsatori) e i cerchi (O) rappresentano i veri negativi (inadempienti). Soglie diverse (75% per B e 81,25% per A) massimizzano l‚Äôaccuratezza del sottogruppo ma rivelano problemi di equit√†.\n\n\n\nPer valutare le prestazioni, vengono mostrate due linee tratteggiate, che rappresentano le soglie alle quali il modello raggiunge un‚Äôaccuratezza accettabile per ciascun sottogruppo. Per il Sottogruppo A, la soglia deve essere impostata all‚Äô81,25% di accuratezza (la seconda linea tratteggiata) per classificare correttamente tutti i rimborsatori (pi√π). Tuttavia, l‚Äôutilizzo di questa stessa soglia per il Sottogruppo B comporterebbe classificazioni errate, poich√© alcuni rimborsatori nel Sottogruppo B scenderebbero erroneamente al di sotto di questa soglia e verrebbero classificati come inadempienti. Per il Sottogruppo B, √® necessaria una soglia inferiore del 75% di accuratezza (la prima linea tratteggiata) per classificare correttamente i suoi rimborsatori. Tuttavia, l‚Äôapplicazione di questa soglia inferiore al Sottogruppo A comporterebbe classificazioni errate per quel gruppo. Ci√≤ illustra come il modello funzioni in modo diseguale nei due sottogruppi, con ciascuno che richiede una soglia diversa per massimizzare i propri tassi di veri positivi.\nLa disparit√† nelle soglie richieste evidenzia la sfida di raggiungere l‚Äôequit√† nelle previsioni del modello. Se le classificazioni positive portano all‚Äôapprovazione dei prestiti, gli individui nel Sottogruppo B sarebbero svantaggiati a meno che la soglia non venga regolata specificamente per il loro sottogruppo. Tuttavia, la regolazione delle soglie introduce compromessi tra accuratezza e correttezza a livello di gruppo, dimostrando la tensione intrinseca nell‚Äôottimizzazione per questi obiettivi nei sistemi di apprendimento automatico.\nPertanto, la letteratura sull‚Äôequit√† ha proposto tre principali metriche di equit√† per quantificare quanto sia equo un modello su un set di dati (Hardt, Price, e Srebro 2016). Dato un modello \\(h\\) e un set di dati \\(D\\) costituito da campioni \\((x, y, s)\\), dove \\(x\\) sono le caratteristiche dei dati, \\(y\\) √® l‚Äôetichetta e \\(s\\) √® l‚Äôattributo del sottogruppo, e supponiamo che ci siano semplicemente due sottogruppi \\(a\\) e \\(b\\), possiamo definire quanto segue:\n\nParit√† Demografica chiede quanto √® accurato un modello per ogni sottogruppo. In altre parole, \\(P(h(X) = Y \\mid S = a) = P(h(X) = Y \\mid S = b)\\).\nQuote Equalizzate chiede quanto √® preciso un modello su campioni positivi e negativi per ogni sottogruppo. \\(P(h(X) = y \\mid S = a, Y = y) = P(h(X) = y \\mid S = b, Y = y)\\).\nUguaglianza di Opportunit√† √® un caso speciale di probabilit√† equalizzate che chiede solo quanto √® preciso un modello su campioni positivi. Ci√≤ √® rilevante in casi come l‚Äôallocazione delle risorse, in cui ci preoccupiamo di come le etichette positive (vale a dire, allocate in base alle risorse) siano distribuite tra i gruppi. Ad esempio, ci preoccupiamo che una proporzione uguale di prestiti venga concessa sia agli uomini che alle donne. \\(P(h(X) = 1 \\mid S = a, Y = 1) = P(h(X) = 1 \\mid S = b, Y = 1)\\).\n\nNota: Queste definizioni spesso adottano una visione ristretta quando si considerano confronti binari tra due sottogruppi. Un altro filone di ricerca di apprendimento automatico equo incentrato su multi-calibrazione e multi-accuratezza considera le interazioni tra un numero arbitrario di identit√†, riconoscendo l‚Äôintersezionalit√† intrinseca delle identit√† individuali nel mondo reale (H√©bert-Johnson et al. 2018).\n\nH√©bert-Johnson, √örsula, Michael P. Kim, Omer Reingold, e Guy N. Rothblum. 2018. ¬´Multicalibration: Calibration for the (Computationally-Identifiable) Masses¬ª. In Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsm√§ssan, Stockholm, Sweden, July 10-15, 2018, a cura di Jennifer G. Dy e Andreas Krause, 80:1944‚Äì53. Proceedings of Machine Learning Research. PMLR. http://proceedings.mlr.press/v80/hebert-johnson18a.html.\n\nIl Contesto √® Importante\nPrima di prendere qualsiasi decisione tecnica per sviluppare un algoritmo ML imparziale, dobbiamo comprendere il contesto che circonda il nostro modello. Ecco alcune delle domande chiave su cui riflettere:\n\nPer chi prender√† decisioni questo modello?\nChi √® rappresentato nei dati di training?\nChi √® rappresentato e chi manca al tavolo di ingegneri, progettisti e manager?\nChe tipo di impatti duraturi potrebbe avere questo modello? Ad esempio, avr√† un impatto sulla sicurezza finanziaria di un individuo su scala generazionale, come la determinazione delle ammissioni al college o l‚Äôammissione di un prestito per una casa?\nQuali pregiudizi storici e sistematici sono presenti in questo contesto e sono presenti nei dati di training da cui il modello generalizzer√†?\n\nComprendere il background sociale, etico e storico di un sistema √® fondamentale per prevenire danni e dovrebbe informare le decisioni durante tutto il ciclo di sviluppo del modello. Dopo aver compreso il contesto, si possono prendere varie decisioni tecniche per rimuovere i pregiudizi. Innanzitutto, si deve decidere quale metrica di equit√† √® il criterio pi√π appropriato per l‚Äôottimizzazione. Successivamente, ci sono generalmente tre aree principali in cui si pu√≤ intervenire per eliminare i pregiudizi di un sistema ML.\nInnanzitutto, la preelaborazione √® quando si bilancia un set di dati per garantire una rappresentazione equa o addirittura si aumenta il peso su determinati gruppi sottorappresentati per garantire che il modello funzioni bene. In secondo luogo, nell‚Äôelaborazione si tenta di modificare il processo di training di un sistema ML per garantire che dia priorit√† all‚Äôequit√†. Questo pu√≤ essere semplice come aggiungere un regolarizzatore di equit√† (Lowy et al. 2021) al training di un insieme di modelli e campionarli in un modo specifico (Agarwal et al. 2018).\n\nLowy, Andrew, Rakesh Pavan, Sina Baharlouei, Meisam Razaviyayn, e Ahmad Beirami. 2021. ¬´Fermi: Fair empirical risk minimization via exponential R√©nyi mutual information¬ª.\n\nAgarwal, Alekh, Alina Beygelzimer, Miroslav Dudƒ±ÃÅk, John Langford, e Hanna M. Wallach. 2018. ¬´A Reductions Approach to Fair Classification¬ª. In Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsm√§ssan, Stockholm, Sweden, July 10-15, 2018, a cura di Jennifer G. Dy e Andreas Krause, 80:60‚Äì69. Proceedings of Machine Learning Research. PMLR. http://proceedings.mlr.press/v80/agarwal18a.html.\n\nAlghamdi, Wael, Hsiang Hsu, Haewon Jeong, Hao Wang, Peter Michalak, Shahab Asoodeh, e Flavio Calmon. 2022. ¬´Beyond Adult and COMPAS: Fair multi-class prediction via information projection¬ª. Adv. Neur. In. 35: 38747‚Äì60.\n\nHardt, Moritz, Eric Price, e Nati Srebro. 2016. ¬´Equality of Opportunity in Supervised Learning¬ª. In Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, a cura di Daniel D. Lee, Masashi Sugiyama, Ulrike von Luxburg, Isabelle Guyon, e Roman Garnett, 3315‚Äì23. https://proceedings.neurips.cc/paper/2016/hash/9d2682367c3935defcb1f9e247a97c0d-Abstract.html.\nInfine, la post-elaborazione degrada un modello dopo il fatto, prendendo un modello addestrato e modificandone le previsioni in un modo specifico per garantire che l‚Äôequit√† venga preservata (Alghamdi et al. 2022; Hardt, Price, e Srebro 2016). La post-elaborazione si basa sulle fasi di pre-elaborazione e in-elaborazione offrendo un‚Äôaltra opportunit√† per affrontare i problemi di bias [pregiudizi] e equit√† nel modello dopo che √® gi√† stato addestrato.\nIl processo in tre fasi di pre-elaborazione, in-elaborazione e post-elaborazione fornisce un framework per intervenire in diverse fasi dello sviluppo del modello per mitigare i problemi relativi a pregiudizi ed equit√†. Mentre la pre-elaborazione e l‚Äôin-elaborazione si concentrano sui dati e sul training, la post-elaborazione consente di apportare modifiche dopo che il modello √® stato completamente formato. Insieme, questi tre approcci offrono molteplici opportunit√† per rilevare e rimuovere pregiudizi ingiusti.\n\n\nDistribuzione Ponderata\nL‚Äôampiezza delle definizioni di equit√† e degli interventi di debiasing esistenti sottolinea la necessit√† di una valutazione ponderata prima di distribuire sistemi ML. Come ricercatori e sviluppatori ML, lo sviluppo responsabile del modello richiede di istruirci in modo proattivo sul contesto del mondo reale, consultare esperti del settore e utenti finali e concentrarci sulla prevenzione dei danni.\nInvece di vedere le considerazioni sull‚Äôequit√† come una casella da spuntare, dobbiamo impegnarci profondamente con le implicazioni sociali uniche e i compromessi etici attorno a ogni modello che costruiamo. Ogni scelta tecnica su set di dati, architetture di modelli, metriche di valutazione e vincoli di distribuzione incorpora valori. Ampliando la nostra prospettiva oltre le metriche tecniche ristrette, valutando attentamente i compromessi e ascoltando le voci interessate, possiamo lavorare per garantire che i nostri sistemi espandano le opportunit√† anzich√© codificare i pregiudizi.\nLa strada da seguire non risiede in una checklist di ‚Äúdebiasing‚Äù arbitraria, ma nell‚Äôimpegno a comprendere e sostenere la nostra responsabilit√† etica a ogni passo. Questo impegno inizia con l‚Äôeducazione proattiva di noi stessi e la consultazione degli altri, piuttosto che limitarci a seguire i movimenti di una checklist di equit√†. Richiede un profondo impegno nei compromessi etici nelle nostre scelte tecniche, la valutazione degli impatti su diversi gruppi e l‚Äôascolto delle voci maggiormente interessate.\nIn definitiva, i sistemi di intelligenza artificiale responsabili ed etici non derivano dal ‚Äúdebiasing‚Äù delle caselle di controllo, ma dal rispetto del nostro dovere di valutare i danni, ampliare le prospettive, comprendere i compromessi e garantire di offrire opportunit√† a tutti i gruppi. Questa responsabilit√† etica dovrebbe guidare ogni passo.\nIl collegamento tra i paragrafi √® che il primo stabilisce la necessit√† di una valutazione ponderata delle questioni di equit√† piuttosto che di un approccio basato su caselle di controllo. Il secondo paragrafo si sofferma poi su come si presenta in pratica questa valutazione ponderata, ovvero impegnarsi con i compromessi, valutare gli impatti sui gruppi e ascoltare le voci interessate. Infine, l‚Äôultimo paragrafo fa riferimento all‚Äôevitare una ‚Äúchecklist di debiasing arbitraria‚Äù e impegnarsi nella responsabilit√† etica attraverso la valutazione, la comprensione dei compromessi e l‚Äôofferta di opportunit√†.\n\n\n\n15.5.2 Preservare la Privacy\nIncidenti recenti hanno fatto luce su come i modelli di intelligenza artificiale possano memorizzare dati sensibili degli utenti in modi che violano la privacy. Ippolito et al. (2023) dimostra che i modelli linguistici tendono a memorizzare i dati di addestramento e possono persino riprodurre esempi di addestramento specifici. Questi rischi sono amplificati con sistemi ML personalizzati distribuiti in ambienti intimi come case o dispositivi indossabili. Prendiamo in considerazione uno smart speaker che usa le nostre conversazioni per migliorare la qualit√† del servizio per gli utenti che apprezzano tali miglioramenti. Sebbene potenzialmente vantaggioso, questo crea anche rischi per la privacy, poich√© i malintenzionati potrebbero tentare di estrarre ci√≤ che lo speaker ‚Äúricorda‚Äù. Il problema si estende oltre i modelli linguistici. Figura¬†15.2 mostra come i modelli di diffusione possono memorizzare e generare esempi di training individuali (Nicolas Carlini et al. 2023), dimostrando ulteriormente i potenziali rischi per la privacy associati ai sistemi di intelligenza artificiale che apprendono dai dati degli utenti.\n\nCarlini, Nicolas, Jamie Hayes, Milad Nasr, Matthew Jagielski, Vikash Sehwag, Florian Tramer, Borja Balle, Daphne Ippolito, e Eric Wallace. 2023. ¬´Extracting training data from diffusion models¬ª. In 32nd USENIX Security Symposium (USENIX Security 23), 5253‚Äì70.\n\n\n\n\n\n\nFigura¬†15.2: Modelli di diffusione che memorizzano campioni dai dati di training. Fonte: Ippolito et al. (2023).\n\n\nIppolito, Daphne, Florian Tramer, Milad Nasr, Chiyuan Zhang, Matthew Jagielski, Katherine Lee, Christopher Choquette Choo, e Nicholas Carlini. 2023. ¬´Preventing Generation of Verbatim Memorization in Language Models Gives a False Sense of Privacy¬ª. In Proceedings of the 16th International Natural Language Generation Conference, 5253‚Äì70. Association for Computational Linguistics. https://doi.org/10.18653/v1/2023.inlg-main.3.\n\n\nMan mano che l‚Äôintelligenza artificiale si integra sempre di pi√π nella nostra vita quotidiana, sta diventando sempre pi√π importante che le preoccupazioni sulla privacy e le solide misure di sicurezza per proteggere le informazioni degli utenti siano sviluppate con occhio critico. La sfida sta nel bilanciare i vantaggi dell‚Äôintelligenza artificiale personalizzata con il diritto fondamentale alla privacy.\nGli avversari possono usare queste capacit√† di memorizzazione e addestrare modelli per rilevare se specifici dati di addestramento hanno influenzato un modello target. Ad esempio, gli attacchi di inferenza di appartenenza addestrano un modello secondario che impara a rilevare un cambiamento negli output del modello target quando si effettuano inferenze sui dati su cui √® stato addestrato rispetto a quelli su cui non √® stato addestrato (Shokri et al. 2017).\n\nShokri, Reza, Marco Stronati, Congzheng Song, e Vitaly Shmatikov. 2017. ¬´Membership Inference Attacks Against Machine Learning Models¬ª. In 2017 IEEE Symposium on Security and Privacy (SP), 3‚Äì18. IEEE; IEEE. https://doi.org/10.1109/sp.2017.41.\n\nAbadi, Martin, Andy Chu, Ian Goodfellow, H. Brendan McMahan, Ilya Mironov, Kunal Talwar, e Li Zhang. 2016. ¬´Deep Learning with Differential Privacy¬ª. In Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security, 308‚Äì18. CCS ‚Äô16. New York, NY, USA: ACM. https://doi.org/10.1145/2976749.2978318.\nI dispositivi ML sono particolarmente vulnerabili perch√© sono spesso personalizzati sui dati degli utenti e vengono distribuiti in contesti ancora pi√π intimi come la casa. Le tecniche di apprendimento automatico privato si sono evolute per stabilire misure di sicurezza contro gli avversari, come menzionato nel capitolo Sicurezza e Privacy per combattere questi problemi di privacy. Metodi come la privacy differenziale aggiungono rumore matematico durante l‚Äôaddestramento per oscurare l‚Äôinfluenza dei singoli punti dati sul modello. Tecniche popolari come DP-SGD (Abadi et al. 2016) tagliano anche i gradienti per limitare ci√≤ che il modello trapeler√† sui dati. Tuttavia, gli utenti dovrebbero anche avere la possibilit√† di eliminare l‚Äôimpatto dei propri dati in un secondo momento.\n\n\n15.5.3 Machine Unlearning\nCon dispositivi ML personalizzati per singoli utenti e poi distribuiti su edge remoti senza connettivit√†, sorge una sfida: come possono i modelli ‚Äúdimenticare‚Äù in modo reattivo i dati dopo la distribuzione? Se gli utenti richiedono che i loro dati vengano rimossi da un modello personalizzato, la mancanza di connettivit√† rende impossibile la riqualificazione. Pertanto, un‚Äôefficiente dimenticanza dei dati sul dispositivo √® necessaria, ma pone degli ostacoli.\nGli approcci iniziali di disapprendimento hanno incontrato delle limitazioni in questo contesto. Date le limitazioni delle risorse, recuperare modelli da zero sul dispositivo per dimenticare i dati si rivela inefficiente o addirittura impossibile. La riqualificazione completa richiede anche di conservare tutti i dati di training originali sul dispositivo, il che comporta dei rischi per la sicurezza e la privacy. Le comuni tecniche di ‚Äúmachine unlearning‚Äù [disapprendimento automatico] (Bourtoule et al. 2021) per sistemi ML embedded remoti non riescono a consentire la rimozione dei dati reattiva e sicura.\n\nBourtoule, Lucas, Varun Chandrasekaran, Christopher A. Choquette-Choo, Hengrui Jia, Adelin Travers, Baiwu Zhang, David Lie, e Nicolas Papernot. 2021. ¬´Machine Unlearning¬ª. In 2021 IEEE Symposium on Security and Privacy (SP), 141‚Äì59. IEEE; IEEE. https://doi.org/10.1109/sp40001.2021.00019.\nTuttavia, metodi pi√π recenti sembrano promettenti nel modificare i modelli in modo da dimenticare approssimativamente i dati senza doverli riqualificare completamente. Sebbene la perdita di accuratezza derivante dall‚Äôevitare ricostruzioni complete sia modesta, garantire la privacy dei dati dovrebbe comunque essere la priorit√† quando si gestiscono eticamente le informazioni sensibili degli utenti. Anche una minima esposizione a dati privati pu√≤ violare la fiducia degli utenti. Poich√© i sistemi ML diventano profondamente personalizzati, efficienza e privacy devono essere abilitate fin dall‚Äôinizio, non ripensamenti.\nLe normative globali sulla privacy, come il consolidato GDPR nell‚ÄôUnione Europea, il CCPA in California e le proposte pi√π recenti come il CPPA del Canada e l‚ÄôAPPI del Giappone, sottolineano il diritto di eliminare i dati personali. Queste politiche, insieme a incidenti di IA di alto profilo come la memorizzazione dei dati degli artisti da parte di Stable Diffusion, hanno evidenziato l‚Äôimperativo etico per i modelli di consentire agli utenti di eliminare i propri dati anche dopo l‚Äôaddestramento.\nIl diritto di rimuovere i dati nasce da preoccupazioni sulla privacy relative alle aziende o agli avversari che abusano delle informazioni sensibili degli utenti. L‚Äôunlearning automatico si riferisce alla rimozione dell‚Äôinfluenza di punti specifici da un modello gi√† addestrato. Ingenuamente, ci√≤ comporta una riqualificazione completa senza i dati eliminati. Tuttavia, i vincoli di connettivit√† spesso rendono la riqualificazione non fattibile per i sistemi ML personalizzati e distribuiti su edge remoti. Se uno smart speaker impara da conversazioni domestiche private, √® importante mantenere l‚Äôaccesso per eliminare tali dati.\nSebbene limitati, i metodi si stanno evolvendo per consentire approssimazioni efficienti della riqualificazione per l‚Äôunlearning. Modificando il tempo di inferenza dei modelli, possono imitare i dati ‚Äúdimenticati‚Äù senza accesso completo ai dati di addestramento. Tuttavia, la maggior parte delle tecniche attuali √® limitata a modelli semplici, ha ancora costi di risorse e scambia una certa accuratezza. Sebbene i metodi si stiano evolvendo, consentire una rimozione efficiente dei dati e rispettare la privacy degli utenti rimane fondamentale per una distribuzione TinyML responsabile.\n\n\n15.5.4 Esempi Avversari e Robustezza\nI modelli di apprendimento automatico, in particolare le reti neurali profonde, hanno un tallone d‚ÄôAchille ben documentato: spesso si rompono quando vengono apportate anche piccole perturbazioni ai loro input (Szegedy et al. 2014). Questa sorprendente fragilit√† evidenzia un importante divario di robustezza che minaccia l‚Äôimplementazione nel mondo reale in domini ad alto rischio. Apre anche la porta ad attacchi avversari progettati per ingannare deliberatamente i modelli.\nI modelli di apprendimento automatico possono mostrare una sorprendente fragilit√†: piccole modifiche agli input possono causare malfunzionamenti scioccanti, anche nelle reti neurali profonde all‚Äôavanguardia (Szegedy et al. 2014). Questa imprevedibilit√† sui dati fuori campione sottolinea le lacune nella generalizzazione e nella robustezza del modello. Data la crescente ubiquit√† dell‚Äôapprendimento automatico, consente anche minacce avversarie che sfruttano i punti ciechi dei modelli.\nLe reti neurali profonde dimostrano una doppia natura quasi paradossale: competenza umana nelle distribuzioni di training abbinata a un‚Äôestrema fragilit√† alle piccole perturbazioni di input (Szegedy et al. 2014). Questa lacuna di vulnerabilit√† avversaria ne evidenzia altre nelle procedure ML standard e minacce all‚Äôaffidabilit√† nel mondo reale. Allo stesso tempo, pu√≤ essere sfruttata: gli aggressori possono trovare punti di rottura del modello che gli umani non percepirebbero.\n\nSzegedy, Christian, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian J. Goodfellow, e Rob Fergus. 2014. ¬´Intriguing properties of neural networks¬ª. In 2nd International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings, a cura di Yoshua Bengio e Yann LeCun. http://arxiv.org/abs/1312.6199.\nFigura¬†15.3 include un esempio di una piccola perturbazione insignificante che modifica una previsione del modello. Questa fragilit√† ha impatti nel mondo reale: la mancanza di robustezza mina la fiducia nell‚Äôimplementazione di modelli per applicazioni ad alto rischio come auto a guida autonoma o diagnosi mediche. Inoltre, la vulnerabilit√† porta a minacce alla sicurezza: gli aggressori possono creare deliberatamente esempi avversari che sono percettivamente indistinguibili dai dati normali ma causano errori del modello.\n\n\n\n\n\n\nFigura¬†15.3: Effetto della perturbazione sulla previsione. Fonte: Microsoft.\n\n\n\nAd esempio, lavori passati mostrano attacchi riusciti che ingannano i modelli per attivit√† come il rilevamento NSFW (Bhagoji et al. 2018), il blocco degli annunci (Tram√®r et al. 2019) e il riconoscimento vocale (Nicholas Carlini et al. 2016). Sebbene gli errori in questi domini rappresentino gi√† dei rischi per la sicurezza, il problema si estende oltre la sicurezza IT. Di recente, la robustezza avversaria √® stata proposta come metrica di prestazioni aggiuntiva approssimando il comportamento del caso peggiore.\n\nBhagoji, Arjun Nitin, Warren He, Bo Li, e Dawn Song. 2018. ¬´Practical black-box attacks on deep neural networks using efficient query mechanisms¬ª. In Proceedings of the European conference on computer vision (ECCV), 154‚Äì69.\n\nTram√®r, Florian, Pascal Dupr√©, Gili Rusak, Giancarlo Pellegrino, e Dan Boneh. 2019. ¬´AdVersarial: Perceptual Ad Blocking meets Adversarial Machine Learning¬ª. In Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security, 2005‚Äì21. ACM. https://doi.org/10.1145/3319535.3354222.\n\nCarlini, Nicholas, Pratyush Mishra, Tavish Vaidya, Yuankai Zhang, Micah Sherr, Clay Shields, David Wagner, e Wenchao Zhou. 2016. ¬´Hidden voice commands¬ª. In 25th USENIX security symposium (USENIX security 16), 513‚Äì30.\nLa sorprendente fragilit√† del modello evidenziata sopra mette in dubbio l‚Äôaffidabilit√† nel mondo reale e apre la porta alla manipolazione avversaria. Questa crescente vulnerabilit√† sottolinea diverse esigenze. In primo luogo, le valutazioni della robustezza morale sono essenziali per quantificare le vulnerabilit√† del modello prima dell‚Äôimplementazione. L‚Äôapprossimazione del comportamento del caso peggiore fa emergere punti ciechi.\nIn secondo luogo, devono essere sviluppate difese efficaci in tutti i domini per colmare queste lacune di robustezza. Con la sicurezza in gioco, gli sviluppatori non possono ignorare la minaccia di attacchi che sfruttano le debolezze del modello. Inoltre, non possiamo permetterci guasti indotti dalla fragilit√† per applicazioni critiche per la sicurezza come veicoli a guida autonoma e diagnosi mediche. Sono in gioco delle vite.\nInfine, la comunit√† di ricerca continua a mobilitarsi rapidamente in risposta. L‚Äôinteresse per l‚Äôapprendimento automatico avversario √® esploso poich√© gli attacchi rivelano la necessit√† di colmare il divario di robustezza tra dati sintetici e dati del mondo reale. Le conferenze ora comunemente presentano difese per proteggere e stabilizzare i modelli. La comunit√† riconosce che la fragilit√† del modello √® un problema critico che deve essere affrontato tramite test di robustezza, sviluppo di difese e ricerca continua. Evidenziando i punti ciechi e rispondendo con difese basate su principi, possiamo lavorare per garantire affidabilit√† e sicurezza per i sistemi di apprendimento automatico, specialmente in domini ad alto rischio.\n\n\n15.5.5 Creazione di Modelli Interpretabili\nPoich√© i modelli vengono distribuiti pi√π frequentemente in contesti ad alto rischio, professionisti, sviluppatori, utenti finali a valle e una regolamentazione crescente hanno evidenziato la necessit√† di spiegabilit√† nell‚Äôapprendimento automatico. L‚Äôobiettivo di molti metodi di interpretabilit√† e spiegabilit√† √® fornire ai professionisti maggiori informazioni sul comportamento complessivo dei modelli o sul comportamento dato un input specifico. Ci√≤ consente agli utenti di decidere se l‚Äôoutput o la previsione di un modello sono affidabili o meno.\nTale analisi pu√≤ aiutare gli sviluppatori a eseguire il debug dei modelli e migliorare le prestazioni evidenziando distorsioni, correlazioni spurie e modalit√† di errore dei modelli. Nei casi in cui i modelli possono superare le prestazioni umane in un‚Äôattivit√†, l‚Äôinterpretabilit√† pu√≤ aiutare utenti e ricercatori a comprendere meglio le relazioni nei loro dati e pattern precedentemente sconosciuti.\nEsistono molte classi di metodi di spiegabilit√†/interpretabilit√†, tra cui la spiegabilit√† post hoc, l‚Äôinterpretabilit√† intrinseca e l‚Äôinterpretabilit√† meccanicistica. Questi metodi mirano a rendere pi√π comprensibili i modelli di apprendimento automatico complessi e a garantire che gli utenti possano fidarsi delle previsioni del modello, soprattutto in contesti critici. Fornendo trasparenza nel comportamento del modello, le tecniche di spiegabilit√† sono uno strumento importante per sviluppare sistemi di intelligenza artificiale sicuri, equi e affidabili.\n\nSpiegabilit√† Post Hoc\nI metodi di spiegabilit√† ‚Äúpost hoc‚Äù in genere spiegano il comportamento di output di un modello black-box su un input specifico. metodi pi√π diffusi includono spiegazioni controfattuali, metodi di attribuzione delle caratteristiche e spiegazioni basate sui concetti.\nSpiegazioni controfattuali, spesso chiamate anche ricorso algoritmico, ‚ÄúSe X non si fosse verificato, Y non si sarebbe verificato‚Äù (Wachter, Mittelstadt, e Russell 2017). Ad esempio, si consideri una persona che richiede un prestito bancario la cui richiesta viene respinta da un modello. Potrebbe chiedere alla propria banca un ricorso o come modificare per essere idonea a un prestito. Una spiegazione controfattuale indicherebbe loro quali caratteristiche devono modificare e di quanto, in modo che la previsione del modello cambi.\n\nWachter, Sandra, Brent Mittelstadt, e Chris Russell. 2017. ¬´Counterfactual Explanations Without Opening the Black Box: Automated Decisions and the GDPR¬ª. SSRN Electronic Journal 31: 841. https://doi.org/10.2139/ssrn.3063289.\n\nSelvaraju, Ramprasaath R., Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, e Dhruv Batra. 2017. ¬´Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization¬ª. In 2017 IEEE International Conference on Computer Vision (ICCV), 618‚Äì26. IEEE. https://doi.org/10.1109/iccv.2017.74.\n\nSmilkov, Daniel, Nikhil Thorat, Been Kim, Fernanda Vi√©gas, e Martin Wattenberg. 2017. ¬´Smoothgrad: Removing noise by adding noise¬ª. ArXiv preprint abs/1706.03825. https://arxiv.org/abs/1706.03825.\n\nRibeiro, Marco Tulio, Sameer Singh, e Carlos Guestrin. 2016. ¬´‚Äù Why should i trust you?‚Äù Explaining the predictions of any classifier¬ª. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, 1135‚Äì44.\n\nLundberg, Scott M., e Su-In Lee. 2017. ¬´A Unified Approach to Interpreting Model Predictions¬ª. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, a cura di Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, e Roman Garnett, 4765‚Äì74. https://proceedings.neurips.cc/paper/2017/hash/8a20a8621978632d76c43dfd28b67767-Abstract.html.\nI metodi di attribuzione delle caratteristiche evidenziano le caratteristiche di input che sono importanti o necessarie per una particolare previsione. Per un modello di visione artificiale, ci√≤ significherebbe evidenziare i singoli pixel che hanno contribuito maggiormente all‚Äôetichetta prevista dell‚Äôimmagine. Si noti che questi metodi non spiegano in che modo quei pixel/caratteristiche influenzano la previsione, ma solo che lo fanno. I metodi comuni includono gradienti di input, GradCAM (Selvaraju et al. 2017), SmoothGrad (Smilkov et al. 2017), LIME (Ribeiro, Singh, e Guestrin 2016) e SHAP (Lundberg e Lee 2017).\nFornendo esempi di modifiche alle caratteristiche di input che altererebbero una previsione (controfattuali) o indicando le caratteristiche pi√π influenti per una data previsione (attribuzione), queste tecniche di spiegazione post hoc fanno luce sul comportamento del modello per input individuali. Questa trasparenza granulare aiuta gli utenti a determinare se possono fidarsi e agire su output di modelli specifici.\nLe spiegazioni basate sui concetti mirano a spiegare il comportamento del modello e gli output utilizzando un set predefinito di concetti semantici (ad esempio, il modello riconosce la classe di scena ‚Äúcamera da letto‚Äù in base alla presenza dei concetti ‚Äúletto‚Äù e ‚Äúcuscino‚Äù). Lavori recenti mostrano che gli utenti spesso preferiscono queste spiegazioni a quelle basate sull‚Äôattribuzione e sugli esempi perch√© ‚Äúassomigliano al ragionamento e alle spiegazioni umane‚Äù (Vikram V. Ramaswamy et al. 2023b). I metodi di spiegazione basati sui concetti pi√π diffusi includono TCAV (Cai et al. 2019), Network Dissection (Bau et al. 2017) e decomposizione della base interpretabile (Zhou et al. 2018).\n\nRamaswamy, Vikram V, Sunnie SY Kim, Ruth Fong, e Olga Russakovsky. 2023b. ¬´UFO: A unified method for controlling Understandability and Faithfulness Objectives in concept-based explanations for CNNs¬ª. ArXiv preprint abs/2303.15632. https://arxiv.org/abs/2303.15632.\n\nCai, Carrie J., Emily Reif, Narayan Hegde, Jason Hipp, Been Kim, Daniel Smilkov, Martin Wattenberg, et al. 2019. ¬´Human-Centered Tools for Coping with Imperfect Algorithms During Medical Decision-Making¬ª. In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems, a cura di Jennifer G. Dy e Andreas Krause, 80:2673‚Äì82. Proceedings of Machine Learning Research. ACM. https://doi.org/10.1145/3290605.3300234.\n\nBau, David, Bolei Zhou, Aditya Khosla, Aude Oliva, e Antonio Torralba. 2017. ¬´Network Dissection: Quantifying Interpretability of Deep Visual Representations¬ª. In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 3319‚Äì27. IEEE. https://doi.org/10.1109/cvpr.2017.354.\n\nZhou, Bolei, Yiyou Sun, David Bau, e Antonio Torralba. 2018. ¬´Interpretable basis decomposition for visual explanation¬ª. In Proceedings of the European Conference on Computer Vision (ECCV), 119‚Äì34.\n\nRamaswamy, Vikram V., Sunnie S. Y. Kim, Ruth Fong, e Olga Russakovsky. 2023a. ¬´Overlooked Factors in Concept-Based Explanations: Dataset Choice, Concept Learnability, and Human Capability¬ª. In 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 10932‚Äì41. IEEE. https://doi.org/10.1109/cvpr52729.2023.01052.\nSi noti che questi metodi sono estremamente sensibili alla dimensione e alla qualit√† del set di concetti e c‚Äô√® un compromesso tra la loro accuratezza e fedelt√† e la loro interpretabilit√† o comprensibilit√† per gli esseri umani (Vikram V. Ramaswamy et al. 2023a). Tuttavia, mappando le previsioni del modello su concetti comprensibili per gli esseri umani, le spiegazioni basate sui concetti possono fornire trasparenza nel ragionamento alla base degli output del modello.\n\n\nInterpretabilit√† Intrinseca\nI modelli intrinsecamente interpretabili sono costruiti in modo tale che le loro spiegazioni siano parte dell‚Äôarchitettura del modello e siano quindi naturalmente fedeli, il che a volte li rende preferibili alle spiegazioni post-hoc applicate ai modelli black-box, specialmente in domini ad alto rischio in cui la trasparenza √® fondamentale (Rudin 2019). Spesso, questi modelli sono vincolati in modo che le relazioni tra le caratteristiche di input e le previsioni siano facili da seguire per gli esseri umani (modelli lineari, alberi decisionali, set di decisioni, modelli k-NN) o obbediscano alla conoscenza strutturale del dominio, come la monotonicit√† (Gupta et al. 2016), la causalit√† o l‚Äôadditivit√† (Lou et al. 2013; Beck e Jackman 1998).\n\nRudin, Cynthia. 2019. ¬´Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead¬ª. Nature Machine Intelligence 1 (5): 206‚Äì15. https://doi.org/10.1038/s42256-019-0048-x.\n\nGupta, Maya, Andrew Cotter, Jan Pfeifer, Konstantin Voevodski, Kevin Canini, Alexander Mangylov, Wojciech Moczydlowski, e Alexander Van Esbroeck. 2016. ¬´Monotonic calibrated interpolated look-up tables¬ª. The Journal of Machine Learning Research 17 (1): 3790‚Äì3836.\n\nLou, Yin, Rich Caruana, Johannes Gehrke, e Giles Hooker. 2013. ¬´Accurate intelligible models with pairwise interactions¬ª. In Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining, a cura di Inderjit S. Dhillon, Yehuda Koren, Rayid Ghani, Ted E. Senator, Paul Bradley, Rajesh Parekh, Jingrui He, Robert L. Grossman, e Ramasamy Uthurusamy, 623‚Äì31. ACM. https://doi.org/10.1145/2487575.2487579.\n\nBeck, Nathaniel, e Simon Jackman. 1998. ¬´Beyond Linearity by Default: Generalized Additive Models¬ª. Am. J. Polit. Sci. 42 (2): 596. https://doi.org/10.2307/2991772.\n\nKoh, Pang Wei, Thao Nguyen, Yew Siang Tang, Stephen Mussmann, Emma Pierson, Been Kim, e Percy Liang. 2020. ¬´Concept Bottleneck Models¬ª. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, 119:5338‚Äì48. Proceedings of Machine Learning Research. PMLR. http://proceedings.mlr.press/v119/koh20a.html.\n\nChen, Chaofan, Oscar Li, Daniel Tao, Alina Barnett, Cynthia Rudin, e Jonathan Su. 2019. ¬´This Looks Like That: Deep Learning for Interpretable Image Recognition¬ª. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, a cura di Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d‚ÄôAlch√©-Buc, Emily B. Fox, e Roman Garnett, 8928‚Äì39. https://proceedings.neurips.cc/paper/2019/hash/adf7ee2dcf142b0e11888e72b43fcb75-Abstract.html.\nTuttavia, lavori pi√π recenti hanno allentato le restrizioni sui modelli intrinsecamente interpretabili, utilizzando modelli black-box per l‚Äôestrazione delle caratteristiche e un modello intrinsecamente interpretabile pi√π semplice per la classificazione, consentendo spiegazioni fedeli che collegano le caratteristiche di alto livello alla previsione. Ad esempio, i Concept Bottleneck Models (Koh et al. 2020) prevedono un set di concetti c che viene passato in un classificatore lineare. I ProtoPNets (Chen et al. 2019) sezionano gli input in combinazioni lineari di somiglianze con parti prototipiche del set di training.\n\n\nInterpretabilit√† Meccanicistica\nI metodi di interpretabilit√† meccanicistica cercano di effettuare il reverse engineering delle reti neurali, spesso paragonandoli a come si potrebbe effettuare quello di un binario compilato o a come i neuroscienziati tentano di decodificare la funzione di singoli neuroni e circuiti nel cervello. La maggior parte delle ricerche sull‚Äôinterpretabilit√† meccanicistica vede i modelli come un grafo computazionale (Geiger et al. 2021) e i circuiti sono sottografi con funzionalit√† distinte (Wang e Zhan 2019). Gli attuali approcci all‚Äôestrazione di circuiti dalle reti neurali e alla comprensione della loro funzionalit√† si basano sull‚Äôispezione manuale umana delle visualizzazioni prodotte dai circuiti (Olah et al. 2020).\n\nGeiger, Atticus, Hanson Lu, Thomas Icard, e Christopher Potts. 2021. ¬´Causal Abstractions of Neural Networks¬ª. In Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, a cura di Marc‚ÄôAurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, e Jennifer Wortman Vaughan, 9574‚Äì86. https://proceedings.neurips.cc/paper/2021/hash/4f5c422f4d49a5a807eda27434231040-Abstract.html.\n\nWang, LingFeng, e YaQing Zhan. 2019. ¬´A conceptual peer review model for arXiv and other preprint databases¬ª. Learn. Publ. 32 (3): 213‚Äì19. https://doi.org/10.1002/leap.1229.\n\nOlah, Chris, Nick Cammarata, Ludwig Schubert, Gabriel Goh, Michael Petrov, e Shan Carter. 2020. ¬´Zoom In: An Introduction to Circuits¬ª. Distill 5 (3): e00024‚Äì001. https://doi.org/10.23915/distill.00024.001.\n\nDavarzani, Samaneh, David Saucier, Purva Talegaonkar, Erin Parker, Alana Turner, Carver Middleton, Will Carroll, et al. 2023. ¬´Closing the Wearable Gap: Footankle kinematic modeling via deep learning models based on a smart sock wearable¬ª. Wearable Technologies 4. https://doi.org/10.1017/wtc.2023.3.\nIn alternativa, alcuni approcci creano autoencoder sparsi che incoraggiano i neuroni a codificare caratteristiche interpretabili districate (Davarzani et al. 2023). Questo campo √® molto pi√π nuovo rispetto alle aree esistenti in spiegabilit√† e interpretabilit√† e, in quanto tale, la maggior parte dei lavori √® generalmente esplorativa piuttosto che orientata alla soluzione.\nCi sono molti problemi nell‚Äôinterpretabilit√† meccanicistica, tra cui la polisemanticit√† di neuroni e circuiti, l‚Äôinconveniente e la soggettivit√† dell‚Äôetichettatura umana e lo spazio di ricerca esponenziale per l‚Äôidentificazione dei circuiti in grandi modelli con miliardi o trilioni di neuroni.\n\n\nSfide e Considerazioni\nMan mano che i metodi per interpretare e spiegare i modelli progrediscono, √® importante notare che gli esseri umani si fidano troppo e abusano degli strumenti di interpretabilit√† (Kaur et al. 2020) e che la fiducia di un utente in un modello dovuta a una spiegazione pu√≤ essere indipendente dalla correttezza delle spiegazioni (Lakkaraju e Bastani 2020). Pertanto, √® necessario che oltre a valutare la fedelt√†/correttezza delle spiegazioni, i ricercatori debbano anche garantire che i metodi di interpretabilit√† siano sviluppati e implementati tenendo a mente un utente specifico e che vengano eseguiti studi sugli utenti per valutarne l‚Äôefficacia e l‚Äôutilit√† nella pratica.\n\nKaur, Harmanpreet, Harsha Nori, Samuel Jenkins, Rich Caruana, Hanna Wallach, e Jennifer Wortman Vaughan. 2020. ¬´Interpreting Interpretability: Understanding Data Scientists‚Äô Use of Interpretability Tools for Machine Learning¬ª. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems, a cura di Regina Bernhaupt, Florian ‚ÄôFloyd‚ÄôMueller, David Verweij, Josh Andres, Joanna McGrenere, Andy Cockburn, Ignacio Avellino, et al., 1‚Äì14. ACM. https://doi.org/10.1145/3313831.3376219.\n\nLakkaraju, Himabindu, e Osbert Bastani. 2020. ¬´‚ÄùHow do I fool you?‚Äù: Manipulating User Trust via Misleading Black Box Explanations¬ª. In Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society, 79‚Äì85. ACM. https://doi.org/10.1145/3375627.3375833.\nInoltre, le spiegazioni devono essere adattate alle competenze dell‚Äôutente, all‚Äôattivit√† per cui stanno utilizzando la spiegazione e alla corrispondente quantit√† minima di informazioni richieste affinch√© la spiegazione sia utile per prevenire il sovraccarico di informazioni.\nMentre interpretabilit√†/spiegabilit√† sono aree popolari nella ricerca sull‚Äôapprendimento automatico, pochissimi lavori studiano la loro intersezione con TinyML ed edge computing. Dato che un‚Äôapplicazione significativa di TinyML √® l‚Äôassistenza sanitaria, che spesso richiede elevata trasparenza e interpretabilit√†, le tecniche esistenti devono essere testate per scalabilit√† ed efficienza relativamente ai dispositivi edge. Molti metodi si basano su passaggi aggiuntivi ‚Äúforward‚Äù e ‚Äúbackward‚Äù e alcuni richiedono persino un training approfondito nei modelli proxy, che non sono fattibili su microcontrollori con risorse limitate.\nDetto questo, i metodi di spiegabilit√† possono essere molto utili nello sviluppo di modelli per dispositivi edge, in quanto possono fornire informazioni su come i dati di input e i modelli possono essere compressi e su come le rappresentazioni possono cambiare dopo la compressione. Inoltre, molti modelli interpretabili sono spesso pi√π piccoli delle loro controparti black-box, il che potrebbe essere utile per le applicazioni TinyML.\n\n\n\n15.5.6 Monitoraggio delle Prestazioni del Modello\nMentre gli sviluppatori possono addestrare modelli che sembrano avversarialmente robusti, equi e interpretabili prima della distribuzione, √® fondamentale che sia gli utenti sia i proprietari del modello ne continuino a monitorare le prestazioni e l‚Äôaffidabilit√† durante l‚Äôintero ciclo di vita. I dati cambiano frequentemente nella pratica, il che pu√≤ spesso comportare cambiamenti nella distribuzione. Questi cambiamenti nella distribuzione possono avere un impatto profondo sulle prestazioni predittive ‚Äúvanilla‚Äù del modello e sulla sua affidabilit√† (equit√†, robustezza e interpretabilit√†) nei dati del mondo reale.\nInoltre, le definizioni di equit√† cambiano frequentemente nel tempo, come ci√≤ che la societ√† considera un attributo protetto, e anche le competenze degli utenti che chiedono spiegazioni possono cambiare.\nPer garantire che i modelli rimangano aggiornati con tali cambiamenti nel mondo reale, gli sviluppatori devono valutare continuamente i loro modelli su dati e standard attuali e rappresentativi e aggiornare i modelli quando necessario.",
    "crumbs": [
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>IA Responsabile</span>"
    ]
  },
  {
    "objectID": "contents/core/responsible_ai/responsible_ai.it.html#sfide-di-implementazione",
    "href": "contents/core/responsible_ai/responsible_ai.it.html#sfide-di-implementazione",
    "title": "15¬† IA Responsabile",
    "section": "15.6 Sfide di Implementazione",
    "text": "15.6 Sfide di Implementazione\n\n15.6.1 Strutture Organizzative e Culturali\nSebbene innovazione e regolamentazione siano spesso viste come interessi contrapposti, molti paesi hanno ritenuto necessario fornire supervisione man mano che i sistemi di intelligenza artificiale si espandono in pi√π settori. Come mostrato in Figura¬†15.4, questa supervisione √® diventata cruciale poich√© questi sistemi continuano a permeare vari settori e ad avere un impatto sulla vita delle persone. Ulteriori discussioni su questo argomento sono disponibili in Human-Centered AI, Capitolo 22 ‚ÄúGovernment Interventions and Regulations‚Äù.\n\n\n\n\n\n\nFigura¬†15.4: Come vari gruppi influenzano l‚ÄôAI incentrata sull‚Äôuomo. Fonte: Shneiderman (2020).\n\n\nShneiderman, Ben. 2020. ¬´Bridging the Gap Between Ethics and Practice: Guidelines for Reliable, Safe, and Trustworthy Human-centered AI Systems¬ª. ACM Trans. Interact. Intell. Syst. 10 (4): 1‚Äì31. https://doi.org/10.1145/3419764.\n\n\nIn questo capitolo abbiamo trattato diverse politiche chiave volte a guidare lo sviluppo e l‚Äôimplementazione dell‚ÄôIA responsabile. Di seguito √® riportato un riepilogo di queste politiche, insieme ad altri framework degni di nota che riflettono una spinta globale per la trasparenza nei sistemi di IA:\n\nIl General Data Protection Regulation (GDPR) dell‚ÄôUnione Europea impone misure di trasparenza e protezione dei dati per i sistemi di IA che gestiscono dati personali.\nL‚ÄôAI Bill of Rights delinea i principi per un utilizzo etico dell‚ÄôIA negli Stati Uniti, sottolineando correttezza, privacy e responsabilit√†.\nIl California Consumer Privacy Act (CCPA) protegge i dati dei consumatori e ritiene le organizzazioni responsabili per l‚Äôuso improprio dei dati.\nIl Responsible Use of Artificial Intelligence del Canada delinea le migliori pratiche per l‚Äôimplementazione etica dell‚ÄôIA.\nL‚ÄôAct on the Protection of Personal Information (APPI) del Giappone stabilisce linee guida per la gestione dei dati personali nei sistemi di IA.\nLa proposta canadese del Consumer Privacy Protection Act (CPPA) mira a rafforzare la protezione della privacy negli ecosistemi digitali.\nIl White Paper on Artificial Intelligence: A European Approach to Excellence and Trust della Commissione Europea sottolinea lo sviluppo etico dell‚ÄôIA insieme all‚Äôinnovazione.\nL‚ÄôInformation Commissioner‚Äôs Office del Regno Unito e la Guidance on Explaining AI Decisions dell‚ÄôAlan Turing Institute forniscono raccomandazioni per aumentare la trasparenza dell‚ÄôIA.\n\nQueste politiche evidenziano uno sforzo globale in corso per bilanciare innovazione e responsabilit√† e garantire che i sistemi di IA siano sviluppati e distribuiti in modo responsabile.\n\n\n15.6.2 Ottenere Dati di Qualit√† e Rappresentativi\nCome discusso nel capitolo Data Engineering, la progettazione responsabile dell‚ÄôIA deve avvenire in tutte le fasi della pipeline, inclusa la raccolta dei dati. Ci√≤ solleva la domanda: cosa significa che i dati siano di alta qualit√† e rappresentativi? Consideriamo i seguenti scenari che ostacolano la rappresentativit√† dei dati:\n\nSquilibrio dei Sottogruppi\nQuesto √® probabilmente ci√≤ che viene in mente quando si sente parlare di ‚Äúdati rappresentativi‚Äù. Lo squilibrio dei sottogruppi significa che il set di dati contiene relativamente pi√π dati da un sottogruppo rispetto a un altro. Questo squilibrio pu√≤ influire negativamente sul modello ML a valle, facendolo sovradimensionare per un sottogruppo di persone e con prestazioni scadenti per un altro.\nUn esempio di conseguenza dello squilibrio dei sottogruppi √® la discriminazione razziale nella tecnologia di riconoscimento facciale (Buolamwini e Gebru 2018); gli algoritmi commerciali di riconoscimento facciale hanno tassi di errore fino al 34% peggiori sulle donne dalla pelle scura rispetto agli uomini dalla pelle chiara.\n\nBuolamwini, Joy, e Timnit Gebru. 2018. ¬´Gender shades: Intersectional accuracy disparities in commercial gender classification¬ª. In Conference on fairness, accountability and transparency, 77‚Äì91. PMLR.\nSi noti che lo squilibrio dei dati √® reciproco e i sottogruppi possono anche essere sovrarappresentati in modo dannoso nel set di dati. Ad esempio, l‚ÄôAllegheny Family Screening Tool (AFST) prevede la probabilit√† che un bambino venga alla fine allontanato da una casa. L‚ÄôAFST produce punteggi sproporzionati per diversi sottogruppi, uno dei motivi √® che √® basato su dati storicamente distorti, provenienti da sistemi legali penali minorili e per adulti, agenzie di assistenza pubblica e agenzie e programmi di salute comportamentale.\n\n\nQuantificazione dei Risultati Target\nCi√≤ si verifica in applicazioni in cui l‚Äôetichetta di verit√† di base non pu√≤ essere misurata o √® difficile da rappresentare in una singola quantit√†. Ad esempio, un modello ML in un‚Äôapplicazione mobile per il benessere potrebbe voler prevedere i livelli di stress individuali. Le vere etichette di stress sono impossibili da ottenere direttamente e devono essere dedotte da altri segnali biologici, come la variabilit√† della frequenza cardiaca e i dati auto-riportati dall‚Äôutente. In queste situazioni, il rumore √® incorporato nei dati per progettazione, rendendo questo un compito ML impegnativo.\n\n\nSpostamento della Distribuzione\nI dati potrebbero non rappresentare pi√π un compito se un evento esterno importante causa un drastico cambiamento della fonte dati. Il modo pi√π comune di pensare alle ‚Äúdistribution shift‚Äù [spostamenti della distribuzione] √® rispetto al tempo; ad esempio, i dati sulle abitudini di acquisto dei consumatori raccolti prima del Covid potrebbero non essere pi√π presenti nel comportamento dei consumatori oggi.\nIl trasferimento provoca un‚Äôaltra forma di spostamento della distribuzione. Ad esempio, quando si applica un sistema di triage addestrato sui dati di un ospedale a un altro, potrebbe verificarsi uno spostamento nella distribuzione se i due ospedali sono molto diversi.\n\n\nRaccolta Dati\nUna soluzione ragionevole per molti dei problemi di cui sopra con dati non rappresentativi o di bassa qualit√† √® raccoglierne di pi√π; possiamo raccogliere pi√π dati mirati a un sottogruppo sottorappresentato o dall‚Äôospedale target a cui il nostro modello potrebbe essere trasferito. Tuttavia, per alcune ragioni, raccogliere pi√π dati √® una soluzione inappropriata o non fattibile per il compito da svolgere.\n\nLa raccolta dati pu√≤ essere dannosa. Questo √® il paradosso dell‚Äôesposizione, la situazione in cui coloro che traggono un guadagno significativo dalla raccolta dei propri dati sono anche coloro che sono messi a rischio dal processo di raccolta (D‚Äôignazio e Klein (2023), Capitolo 4). Ad esempio, raccogliere pi√π dati su individui non binari pu√≤ essere importante per garantire l‚Äôequit√† dell‚Äôapplicazione ML, ma li espone anche a rischi, a seconda di chi raccoglie i dati e di come (se i dati sono facilmente identificabili, contengono contenuti sensibili, ecc.).\nLa raccolta dati pu√≤ essere costosa. In alcuni ambiti, come l‚Äôassistenza sanitaria, ottenere dati pu√≤ essere costoso in termini di tempo e denaro.\nRaccolta dati distorta. Le cartelle cliniche elettroniche sono un‚Äôenorme fonte di dati per le applicazioni sanitarie basate su ML. A parte i problemi di rappresentazione dei sottogruppi, i dati stessi possono essere raccolti in modo distorto. Ad esempio, il linguaggio negativo (‚Äúnon aderente‚Äù, ‚Äúnon disposto‚Äù) √® utilizzato in modo sproporzionato sui pazienti neri (Himmelstein, Bates, e Zhou 2022).\n\n\nD‚Äôignazio, Catherine, e Lauren F Klein. 2023. Data feminism. MIT press.\n\nHimmelstein, Gracie, David Bates, e Li Zhou. 2022. ¬´Examination of Stigmatizing Language in the Electronic Health Record¬ª. JAMA Network Open 5 (1): e2144967. https://doi.org/10.1001/jamanetworkopen.2021.44967.\nConcludiamo con diverse strategie aggiuntive per mantenere la qualit√† dei dati. Innanzitutto, √® fondamentale promuovere una comprensione pi√π approfondita dei dati. Ci√≤ pu√≤ essere ottenuto tramite l‚Äôimplementazione di etichette e misure standardizzate della qualit√† dei dati, come nel Data Nutrition Project. Collaborare con le organizzazioni responsabili della raccolta dei dati aiuta a garantire che i dati vengano interpretati correttamente. In secondo luogo, √® importante impiegare strumenti efficaci per l‚Äôesplorazione dei dati. Le tecniche di visualizzazione e le analisi statistiche possono rivelare problemi con i dati. Infine, stabilire un ciclo di feedback all‚Äôinterno della pipeline ML √® essenziale per comprendere le implicazioni reali dei dati. Le metriche, come le misure di equit√†, ci consentono di definire la ‚Äúqualit√† dei dati‚Äù nel contesto dell‚Äôapplicazione downstream; il miglioramento dell‚Äôequit√† pu√≤ migliorare direttamente la qualit√† delle previsioni che gli utenti finali ricevono.\n\n\n\n15.6.3 Bilanciamento di Accuratezza e Altri Obiettivi\nI modelli di apprendimento automatico vengono spesso valutati solo in base all‚Äôaccuratezza, ma questa singola metrica non riesce a catturare completamente le prestazioni del modello e i compromessi per i sistemi di intelligenza artificiale responsabili. Altre dimensioni etiche, come correttezza, robustezza, interpretabilit√† e privacy, possono competere con la pura accuratezza predittiva durante lo sviluppo del modello. Ad esempio, modelli intrinsecamente interpretabili come piccoli alberi decisionali o classificatori lineari con funzionalit√† semplificate barattano intenzionalmente una certa accuratezza per la trasparenza nel comportamento del modello e nelle previsioni. Mentre questi modelli semplificati raggiungono una minore accuratezza non catturando tutta la complessit√† nel set di dati, una migliore interpretabilit√† crea fiducia consentendo l‚Äôanalisi diretta da parte di professionisti umani.\nInoltre, alcune tecniche pensate per migliorare la robustezza avversaria, come esempi di training avversario o riduzione della dimensionalit√†, possono degradare l‚Äôaccuratezza dei dati di convalida puliti. In applicazioni sensibili come l‚Äôassistenza sanitaria, concentrarsi strettamente sull‚Äôaccuratezza all‚Äôavanguardia comporta rischi etici se consente ai modelli di fare pi√π affidamento su correlazioni spurie che introducono distorsioni o utilizzano ragionamenti opachi. Pertanto, gli obiettivi di prestazione appropriati dipendono in larga misura dal contesto socio-tecnico.\nMetodologie come Value Sensitive Design forniscono framework per valutare formalmente le priorit√† di vari stakeholder all‚Äôinterno del sistema di distribuzione nel mondo reale. Ci√≤ spiega le tensioni tra valori quali accuratezza, interpretabilit√† ed equit√†, che possono quindi orientare decisioni di compromesso responsabili. Per un sistema di diagnosi medica, raggiungere la massima accuratezza potrebbe non essere l‚Äôobiettivo unico: migliorare la trasparenza per creare fiducia nei professionisti o ridurre i pregiudizi verso i gruppi minoritari potrebbe giustificare piccole perdite di accuratezza. L‚Äôanalisi del contesto socio-tecnico √® fondamentale per stabilire questi obiettivi.\nAdottando una visione olistica, possiamo bilanciare responsabilmente l‚Äôaccuratezza con altri obiettivi etici per il successo del modello. Il monitoraggio continuo delle prestazioni lungo pi√π dimensioni √® fondamentale man mano che il sistema si evolve dopo la distribuzione.",
    "crumbs": [
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>IA Responsabile</span>"
    ]
  },
  {
    "objectID": "contents/core/responsible_ai/responsible_ai.it.html#considerazioni-etiche-nella-progettazione-dellia",
    "href": "contents/core/responsible_ai/responsible_ai.it.html#considerazioni-etiche-nella-progettazione-dellia",
    "title": "15¬† IA Responsabile",
    "section": "15.7 Considerazioni Etiche Nella Progettazione dell‚ÄôIA",
    "text": "15.7 Considerazioni Etiche Nella Progettazione dell‚ÄôIA\nDobbiamo discutere almeno di alcune delle numerose questioni etiche in gioco nella progettazione e nell‚Äôapplicazione di sistemi di intelligenza artificiale e di diversi framework per affrontare tali questioni, tra cui quelle relative alla sicurezza dell‚Äôintelligenza artificiale, all‚Äôinterazione uomo-computer (HCI) e alla scienza, tecnologia e societ√† (STS).\n\n15.7.1 Sicurezza dell‚ÄôIntelligenza Artificiale e Allineamento dei Valori\nNel 1960, Norbert Weiner scrisse: ‚Äú‚Äôse utilizziamo, per raggiungere i nostri scopi, un‚Äôagenzia meccanica con il cui funzionamento non possiamo interferire efficacemente‚Ä¶ faremmo meglio ad essere abbastanza sicuri che lo scopo attribuito alla macchina sia lo scopo che desideriamo‚Äù (Wiener 1960).\n\nWiener, Norbert. 1960. ¬´Some Moral and Technical Consequences of Automation: As machines learn they may develop unforeseen strategies at rates that baffle their programmers.¬ª Science 131 (3410): 1355‚Äì58. https://doi.org/10.1126/science.131.3410.1355.\n\nRussell, Stuart. 2021. ¬´Human-compatible artificial intelligence¬ª. Human-like machine intelligence, 3‚Äì23.\nNegli ultimi anni, poich√© le capacit√† dei modelli di deep learning hanno raggiunto, e talvolta persino superato, le capacit√† umane, la questione della creazione di sistemi di intelligenza artificiale che agiscano in accordo con le intenzioni umane invece di perseguire obiettivi non intenzionali o indesiderati √® diventata fonte di preoccupazione (Russell 2021). Nel campo della sicurezza dell‚ÄôIA, un obiettivo particolare riguarda ‚Äúl‚Äôallineamento dei valori‚Äù, ovvero il problema di come codificare lo scopo ‚Äúgiusto‚Äù nelle macchine Intelligenza artificiale compatibile con gli esseri umani. L‚Äôattuale ricerca sull‚ÄôIA presuppone che conosciamo gli obiettivi che vogliamo raggiungere e ‚Äústudia la capacit√† di raggiungere gli obiettivi, non la progettazione di tali obiettivi‚Äù.\nTuttavia, i complessi contesti di distribuzione nel mondo reale rendono difficile definire esplicitamente ‚Äúlo scopo giusto‚Äù per le macchine, richiedendo quadri per l‚Äôimpostazione di obiettivi responsabili ed etici. Metodologie come Value Sensitive Design forniscono meccanismi formali per far emergere le tensioni tra i valori e le priorit√† delle parti interessate.\nAdottando una visione socio-tecnica olistica, possiamo garantire meglio che i sistemi intelligenti perseguano obiettivi che si allineano con ampie intenzioni umane anzich√© massimizzare metriche ristrette come la sola accuratezza. Raggiungere questo obiettivo nella pratica rimane una questione di ricerca aperta e critica man mano che le capacit√† dell‚ÄôIA avanzano rapidamente.\nL‚Äôassenza di questo allineamento pu√≤ portare a diversi problemi di sicurezza dell‚ÄôIA, come documentato in una variet√† di modelli di deep learning. Una caratteristica comune dei sistemi che ottimizzano per un obiettivo √® che le variabili non direttamente incluse nell‚Äôobiettivo possono essere impostate su valori estremi per aiutare a ottimizzare per quell‚Äôobiettivo, portando a problemi caratterizzati come gioco di specifiche, hacking di ricompensa, ecc., nel ‚Äúreinforcement learning (RL)‚Äù [apprendimento per rinforzo].\nNegli ultimi anni, un‚Äôimplementazione particolarmente popolare di RL √® stata quella dei modelli pre-addestrati utilizzando apprendimento auto-supervisionato e ‚ÄúReinforcement Learning From Human Feedback (RLHF)‚Äù [apprendimento per rinforzo fine-tuned da feedback umano] (Christiano et al. 2017). Ngo 2022 (Ngo, Chan, e Mindermann 2022) sostiene che premiando i modelli per apparire innocui ed etici e massimizzando al contempo i risultati utili, RLHF potrebbe incoraggiare l‚Äôemergere di tre propriet√† problematiche: hacking della ricompensa consapevole della situazione, in cui le politiche sfruttano la fallibilit√† umana per ottenere un‚Äôelevata ricompensa, obiettivi rappresentati internamente non allineati che si generalizzano oltre la distribuzione di messa a punto RLHF e strategie di ricerca del potere.\n\nChristiano, Paul F., Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, e Dario Amodei. 2017. ¬´Deep Reinforcement Learning from Human Preferences¬ª. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, a cura di Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, e Roman Garnett, 4299‚Äì4307. https://proceedings.neurips.cc/paper/2017/hash/d5e2c0adad503c91f91df240d0cd4e49-Abstract.html.\n\nNgo, Richard, Lawrence Chan, e S√∂ren Mindermann. 2022. ¬´The alignment problem from a deep learning perspective¬ª. ArXiv preprint abs/2209.00626. https://arxiv.org/abs/2209.00626.\n\nVan Noorden, Richard. 2016. ¬´ArXiv preprint server plans multimillion-dollar overhaul¬ª. Nature 534 (7609): 602‚Äì2. https://doi.org/10.1038/534602a.\nAllo stesso modo, Van Noorden (2016) delinea sei problemi concreti per la sicurezza dell‚ÄôIA, tra cui evitare effetti collaterali negativi, evitare hacking della ricompensa, supervisione scalabile per aspetti dell‚Äôobiettivo che sono troppo costosi per essere valutati frequentemente durante il training, strategie di esplorazione sicure che incoraggiano la creativit√† prevenendo al contempo i danni e robustezza allo spostamento distributivo in ambienti di test invisibili.\n\n\n15.7.2 Sistemi Autonomi e Controllo [e Fiducia]\nLe conseguenze dei sistemi autonomi che agiscono indipendentemente dalla supervisione umana e spesso al di fuori del giudizio umano sono state ampiamente documentate in diversi settori e casi d‚Äôuso. Pi√π di recente, il Dipartimento dei veicoli a motore della California ha sospeso i permessi di distribuzione e collaudo di Cruise per i suoi veicoli autonomi, citando ‚Äúrischi irragionevoli per la sicurezza pubblica‚Äù. Uno di questi incidenti si √® verificato quando un veicolo ha colpito un pedone che stava attraversando le strisce pedonali dopo che il semaforo era diventato verde e al veicolo √® stato permesso di procedere. Nel 2018, un pedone che attraversava la strada con la sua bicicletta √® morto quando un‚Äôauto Uber a guida autonoma, che operava in modalit√† autonoma, non √® riuscita a classificare accuratamente il suo corpo in movimento come un oggetto da evitare.\nAnche i sistemi autonomi oltre ai veicoli a guida autonoma sono suscettibili a tali problemi, con conseguenze potenzialmente pi√π gravi, poich√© i droni alimentati da remoto stanno gi√† rimodellando la guerra. Sebbene tali incidenti sollevino importanti questioni etiche su chi dovrebbe essere ritenuto responsabile quando questi sistemi falliscono, evidenziano anche le sfide tecniche nel dare il pieno controllo di attivit√† complesse e reali alle macchine.\nIn sostanza, c‚Äô√® una tensione tra autonomia umana e delle macchine. Le discipline ingegneristiche e informatiche hanno teso a concentrarsi sull‚Äôautonomia delle macchine. Ad esempio, a partire dal 2019, una ricerca della parola ‚Äúautonomia‚Äù nella Digital Library dell‚ÄôAssociation for Computing Machinery (ACM) rivela che dei 100 articoli pi√π citati, il 90% riguarda l‚Äôautonomia delle macchine (Calvo et al. 2020). Nel tentativo di costruire sistemi a beneficio dell‚Äôumanit√†, queste discipline hanno assunto, senza dubbio, l‚Äôaumento della produttivit√†, dell‚Äôefficienza e dell‚Äôautomazione come strategie primarie per il beneficio dell‚Äôumanit√†.\n\nMcCarthy, John. 1981. ¬´Epistemological Problems Of Artificial Intelligence¬ª. In Readings in Artificial Intelligence, 459‚Äì65. Elsevier. https://doi.org/10.1016/b978-0-934613-03-3.50035-0.\nQuesti obiettivi pongono l‚Äôautomazione delle macchine in prima linea, spesso a spese dell‚Äôuomo. Questo approccio soffre di sfide intrinseche, come notato fin dai primi giorni dell‚ÄôIA attraverso il ‚ÄúFrame problem‚Äù [specifica degli effetti] e il ‚Äúqualification problem‚Äù [qualificazione delle precondizioni] (cfr. http://www.diag.uniroma1.it/~nardi/Didattica/RC/lezioni/sitcalc-1.pdf), che formalizza l‚Äôosservazione che √® impossibile specificare tutte le precondizioni necessarie per il successo di un‚Äôazione nel mondo reale (McCarthy 1981).\nQueste limitazioni logiche hanno dato origine ad approcci matematici come la ‚ÄúResponsibility-sensitive safety (RSS)‚Äù [sicurezza sensibile alla responsabilit√†] (Shalev-Shwartz, Shammah, e Shashua 2017), che mira a scomporre l‚Äôobiettivo finale di un sistema di guida automatizzato (vale a dire la sicurezza) in condizioni concrete e verificabili che possono essere rigorosamente formulate in termini matematici. L‚Äôobiettivo dell‚ÄôRSS √® che tali norme di sicurezza garantiscano la sicurezza del ‚ÄúAutomated Driving System (ADS)‚Äù [sistema di guida autonoma] nella rigorosa forma di dimostrazione matematica. Tuttavia, tali approcci tendono a utilizzare l‚Äôautomazione per affrontare i problemi dell‚Äôautomazione e sono suscettibili a molti degli stessi problemi.\n\nShalev-Shwartz, Shai, Shaked Shammah, e Amnon Shashua. 2017. ¬´On a formal model of safe and scalable self-driving cars¬ª. ArXiv preprint abs/1708.06374. https://arxiv.org/abs/1708.06374.\n\nFriedman, Batya. 1996. ¬´Value-sensitive design¬ª. Interactions 3 (6): 16‚Äì23. https://doi.org/10.1145/242485.242493.\n\nPeters, Dorian, Rafael A. Calvo, e Richard M. Ryan. 2018. ¬´Designing for Motivation, Engagement and Wellbeing in Digital Experience¬ª. Front. Psychol. 9 (maggio): 797. https://doi.org/10.3389/fpsyg.2018.00797.\n\nRyan, Richard M., e Edward L. Deci. 2000. ¬´Self-determination theory and the facilitation of intrinsic motivation, social development, and well-being.¬ª Am. Psychol. 55 (1): 68‚Äì78. https://doi.org/10.1037/0003-066x.55.1.68.\nUn altro approccio per combattere questi problemi √® concentrarsi sulla progettazione ‚Äúhuman-centered‚Äù di sistemi interattivi che incorporano il controllo umano. Il design sensibile al valore (Friedman 1996) ha descritto tre fattori di progettazione chiave per un‚Äôinterfaccia utente che hanno un impatto sull‚Äôautonomia, tra cui capacit√† del sistema, complessit√†, rappresentazione errata e fluidit√†. Un modello pi√π recente, chiamato METUX (A Model for Motivation, Engagement, and Thriving in the User Experience), sfrutta le intuizioni della ‚ÄúSelf-determination Theory (SDT)‚Äù in psicologia per identificare sei sfere distinte dell‚Äôesperienza tecnologica che contribuiscono ai sistemi di progettazione che promuovono il benessere e la prosperit√† umana (Peters, Calvo, e Ryan 2018). SDT definisce l‚Äôautonomia come agire in base ai propri obiettivi e valori, il che √® distinto dall‚Äôuso dell‚Äôautonomia come semplice sinonimo di indipendenza o di controllo (Ryan e Deci 2000).\nCalvo et al. (2020) elabora METUX e le sue sei ‚Äúsfere di esperienza tecnologica‚Äù nel contesto dei sistemi di raccomandazione AI. Propongono queste sfere (Adozione, Interfaccia, Attivit√†, Comportamento, Vita e Societ√†) come un modo per organizzare il pensiero e la valutazione della progettazione tecnologica al fine di catturare in modo appropriato gli impatti contraddittori e a valle sull‚Äôautonomia umana quando interagisce con i sistemi AI.\n\nCalvo, Rafael A, Dorian Peters, Karina Vold, e Richard M Ryan. 2020. ¬´Supporting human autonomy in AI systems: A framework for ethical enquiry¬ª. Ethics of digital well-being: A multidisciplinary approach, 31‚Äì54.\n\n\n15.7.3 Impatti Economici su Posti di Lavoro, Competenze, Salari\nUna delle principali preoccupazioni dell‚Äôattuale ascesa delle tecnologie AI √® la disoccupazione diffusa. Con l‚Äôespansione delle capacit√† dei sistemi AI, molti temono che queste tecnologie causeranno una perdita assoluta di posti di lavoro, poich√© sostituiranno i lavoratori attuali e supereranno ruoli occupazionali alternativi in tutti i settori. Tuttavia, il cambiamento dei panorami economici per mano dell‚Äôautomazione non √® una novit√† e, storicamente, si √® scoperto che riflette pattern di spostamento piuttosto che di sostituzione (Shneiderman 2022)‚ÄîCapitolo 4. In particolare, l‚Äôautomazione di solito riduce i costi e aumenta la qualit√†, aumentando notevolmente l‚Äôaccesso e la domanda. La necessit√† di servire questi mercati in crescita spinge la produzione, creando nuovi posti di lavoro.\n\n‚Äî‚Äî‚Äî. 2022. Human-centered AI. Oxford University Press.\nInoltre, gli studi hanno scoperto che i tentativi di raggiungere un‚Äôautomazione ‚Äúlights-out‚Äù, ovvero un‚Äôautomazione produttiva e flessibile con un numero minimo di lavoratori umani, non hanno avuto successo. I tentativi di farlo hanno portato a quella che la task force del MIT Work of the Future ha definito ‚Äúautomazione a somma zero‚Äù, in cui la flessibilit√† dei processi viene sacrificata per aumentare la produttivit√†.\nAl contrario, la task force propone un approccio di ‚Äúautomazione a somma positiva‚Äù in cui la flessibilit√† viene aumentata progettando una tecnologia che incorpora strategicamente gli esseri umani dove sono molto necessari, rendendo pi√π facile per i dipendenti della linea addestrare e correggere i robot, utilizzando un approccio bottom-up per identificare quali attivit√† dovrebbero essere automatizzate; e scegliendo le giuste metriche per misurare il successo (vedi Work of the Future del MIT).\nTuttavia, l‚Äôottimismo delle prospettive di alto livello non esclude danni individuali, specialmente per coloro le cui competenze e lavori saranno resi obsoleti dall‚Äôautomazione. La pressione pubblica e legislativa, cos√¨ come gli sforzi di responsabilit√† sociale delle aziende, dovranno essere diretti alla creazione di politiche che condividano i vantaggi dell‚Äôautomazione con i lavoratori e si traducano in salari minimi e benefici pi√π elevati.\n\n\n15.7.4 Comunicazione Scientifica e Alfabetizzazione IA\nUn sondaggio del 1993 sulle convinzioni di 3000 adulti nordamericani sulla ‚Äúmacchina pensante elettronica‚Äù ha rivelato due prospettive principali del primo computer: la prospettiva dello ‚Äústrumento utile dell‚Äôuomo‚Äù e la prospettiva della ‚Äúmacchina pensante fantastica‚Äù. Gli atteggiamenti che contribuiscono alla visione della ‚Äúmacchina pensante fantastica‚Äù in questo e altri studi hanno rivelato una caratterizzazione dei computer come ‚Äúcervelli intelligenti, pi√π intelligenti delle persone, illimitati, veloci, misteriosi e spaventosi‚Äù (Martin 1993). Questi timori evidenziano una componente facilmente trascurata dell‚ÄôIA responsabile, specialmente in mezzo alla corsa alla commercializzazione di tali tecnologie: la comunicazione scientifica che comunica accuratamente le capacit√† e le limitazioni di questi sistemi, fornendo al contempo trasparenza sui limiti della conoscenza degli esperti su questi sistemi.\n\nMartin, C. Dianne. 1993. ¬´The myth of the awesome thinking machine¬ª. Commun. ACM 36 (4): 120‚Äì33. https://doi.org/10.1145/255950.153587.\n\nHandlin, Oscar. 1965. ¬´Science and technology in popular culture¬ª. Daedalus-us., 156‚Äì70.\nMan mano che le capacit√† dei sistemi di IA si espandono oltre la comprensione della maggior parte delle persone, c‚Äô√® una tendenza naturale a presumere i tipi di mondi apocalittici dipinti dai nostri media. Ci√≤ √® dovuto in parte all‚Äôapparente difficolt√† di assimilare informazioni scientifiche, persino in culture tecnologicamente avanzate, che porta i prodotti della scienza a essere percepiti come magia, ‚Äúcomprensibili solo in termini di ci√≤ che hanno fatto, non di come hanno funzionato‚Äù (Handlin 1965).\nMentre le aziende tecnologiche dovrebbero essere ritenute responsabili per aver limitato le affermazioni grandiose e non essere cadute in cicli di clamore, la ricerca che studia la comunicazione scientifica, in particolare per quanto riguarda l‚Äôintelligenza artificiale (generativa), sar√† utile anche per tracciare e correggere la comprensione pubblica di queste tecnologie. Un‚Äôanalisi del database accademico Scopus ha scoperto che tale ricerca √® scarsa, con solo una manciata di articoli che menzionano sia ‚Äúcomunicazione scientifica‚Äù che ‚Äúintelligenza artificiale‚Äù (Sch√§fer 2023).\n\nSch√§fer, Mike S. 2023. ¬´The Notorious GPT: Science communication in the age of artificial intelligence¬ª. Journal of Science Communication 22 (02): Y02. https://doi.org/10.22323/2.22020402.\n\nLindgren, Simon. 2023. Handbook of Critical Studies of Artificial Intelligence. Edward Elgar Publishing.\n\nNg, Davy Tsz Kit, Jac Ka Lok Leung, Kai Wah Samuel Chu, e Maggie Shen Qiao. 2021. ¬´AI literacy: Definition, teaching, evaluation and ethical issues¬ª. Proceedings of the Association for Information Science and Technology 58 (1): 504‚Äì9.\nLa ricerca che espone le prospettive, i ‚Äúframe‚Äù e le immagini del futuro promosse da istituzioni accademiche, aziende tecnologiche, stakeholder, enti regolatori, giornalisti, ONG e altri aiuter√† anche a identificare potenziali lacune nell‚Äôalfabetizzazione AI tra gli adulti (Lindgren 2023). Una maggiore attenzione all‚Äôalfabetizzazione AI da parte di tutti gli stakeholder sar√† importante per aiutare le persone le cui competenze sono rese obsolete dall‚Äôautomazione AI (Ng et al. 2021).\n‚ÄúMa anche coloro che non acquisiscono mai quella comprensione hanno bisogno di rassicurazioni sul fatto che esista una connessione tra gli obiettivi della scienza e il loro benessere e, soprattutto, che lo scienziato non sia un uomo completamente a parte, ma uno che condivide parte del loro valore.‚Äù (Handlin, 1965)",
    "crumbs": [
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>IA Responsabile</span>"
    ]
  },
  {
    "objectID": "contents/core/responsible_ai/responsible_ai.it.html#conclusione",
    "href": "contents/core/responsible_ai/responsible_ai.it.html#conclusione",
    "title": "15¬† IA Responsabile",
    "section": "15.8 Conclusione",
    "text": "15.8 Conclusione\nUn‚Äôintelligenza artificiale responsabile √® fondamentale poich√© i sistemi di apprendimento automatico esercitano una crescente influenza nei settori sanitario, lavorativo, finanziario e della giustizia penale. Mentre l‚Äôintelligenza artificiale promette immensi benefici, i modelli progettati in modo sconsiderato rischiano di perpetrare danni attraverso pregiudizi, violazioni della privacy, comportamenti indesiderati e altre insidie.\nMantenere i principi di equit√†, spiegabilit√†, responsabilit√†, sicurezza e trasparenza consente lo sviluppo di un‚Äôintelligenza artificiale etica allineata ai valori umani. Tuttavia, l‚Äôimplementazione di questi principi comporta il superamento di complesse sfide tecniche e sociali relative al rilevamento di pregiudizi nei set di dati, alla scelta di appropriati compromessi nei modelli, alla protezione di dati di training di qualit√† e altro ancora. Framework come la progettazione sensibile al valore guidano il bilanciamento dell‚Äôaccuratezza rispetto ad altri obiettivi in base alle esigenze delle parti interessate.\nGuardando al futuro, il progresso dell‚Äôintelligenza artificiale responsabile richiede una ricerca continua e l‚Äôimpegno del settore. Sono necessari benchmark pi√π standardizzati per confrontare pregiudizi e robustezza dei modelli. Man mano che il TinyML personalizzato si espande, abilitare una trasparenza efficiente e il controllo dell‚Äôutente per i dispositivi edge giustifica l‚Äôattenzione. Le strutture e le politiche di incentivazione riviste devono incoraggiare uno sviluppo deliberato ed etico prima di un‚Äôimplementazione sconsiderata. L‚Äôistruzione sulla cultura dell‚Äôintelligenza artificiale e sui suoi limiti contribuir√† ulteriormente alla comprensione pubblica.\nI metodi responsabili sottolineano che, mentre l‚Äôapprendimento automatico offre un potenziale immenso, un‚Äôapplicazione sconsiderata rischia di avere conseguenze negative. La collaborazione interdisciplinare e la progettazione incentrata sull‚Äôuomo sono essenziali affinch√© l‚Äôintelligenza artificiale possa promuovere un ampio beneficio sociale. Il percorso da seguire non risiede in una checklist arbitraria, ma in un impegno costante per comprendere e sostenere la nostra responsabilit√† etica a ogni passo. Intraprendendo un‚Äôazione coscienziosa, la comunit√† dell‚Äôapprendimento automatico pu√≤ guidare l‚Äôintelligenza artificiale verso l‚Äôemancipazione di tutte le persone in modo equo e sicuro.",
    "crumbs": [
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>IA Responsabile</span>"
    ]
  },
  {
    "objectID": "contents/core/responsible_ai/responsible_ai.it.html#sec-responsible-ai-resource",
    "href": "contents/core/responsible_ai/responsible_ai.it.html#sec-responsible-ai-resource",
    "title": "15¬† IA Responsabile",
    "section": "15.9 Risorse",
    "text": "15.9 Risorse\nEcco un elenco curato di risorse per supportare studenti e insegnanti nei loro percorsi di apprendimento e insegnamento. Stiamo lavorando costantemente per espandere questa raccolta e presto aggiungeremo nuovi esercizi.\n\n\n\n\n\n\nSlide\n\n\n\n\n\nQueste slide sono uno strumento prezioso per gli insegnanti per tenere lezioni e per gli studenti per rivedere il materiale secondo il proprio ritmo. Incoraggiamo studenti e docenti a sfruttare queste slide per migliorare la loro comprensione e facilitare un trasferimento efficace delle conoscenze.\n\nWhat am I building? What is the goal?\nWho is the audience?\nWhat are the consequences?\nResponsible Data Collection.\n\n\n\n\n\n\n\n\n\n\nVideo\n\n\n\n\n\n\nVideo¬†15.1\n\n\n\n\n\n\n\n\n\n\nEsercizi\n\n\n\n\n\nPer rafforzare i concetti trattati in questo capitolo, abbiamo curato una serie di esercizi che sfidano gli studenti ad applicare le proprie conoscenze e ad approfondire la propria comprensione.\n\nProssimamente.",
    "crumbs": [
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>IA Responsabile</span>"
    ]
  },
  {
    "objectID": "contents/core/sustainable_ai/sustainable_ai.it.html",
    "href": "contents/core/sustainable_ai/sustainable_ai.it.html",
    "title": "16¬† IA Sostenibile",
    "section": "",
    "text": "16.1 Panoramica\nI rapidi progressi nell‚Äôintelligenza artificiale (IA) e nel machine learning (ML) [apprendimento automatico] hanno portato a molte applicazioni e ottimizzazioni utili per l‚Äôefficienza delle prestazioni. Tuttavia, la notevole crescita dell‚ÄôIA ha un costo significativo ma spesso trascurato: il suo impatto ambientale. Il rapporto pi√π recente pubblicato dall‚ÄôIPCC, l‚Äôorganismo internazionale che guida le valutazioni scientifiche del cambiamento climatico e dei suoi impatti, ha sottolineato l‚Äôimportanza urgente di affrontare il cambiamento climatico. Senza sforzi immediati per ridurre le emissioni globali di \\(\\textrm{CO}_2\\) di almeno il 43 percento prima del 2030, supereremo il riscaldamento globale di 1,5 gradi Celsius (Winkler et al. 2022). Ci√≤ potrebbe avviare cicli di feedback positivi, spingendo le temperature ancora pi√π in alto. Accanto alle questioni ambientali, le Nazioni Unite hanno riconosciuto 17 Sustainable Development Goals (SDG) [Obiettivi di sviluppo sostenibile], in cui l‚ÄôIA pu√≤ svolgere un ruolo importante e, viceversa, possono svolgere un ruolo importante nello sviluppo di sistemi di IA. Poich√© il campo continua a espandersi, considerare la sostenibilit√† √® fondamentale.\nI sistemi di intelligenza artificiale, in particolare i grandi modelli linguistici come GPT-3 e i modelli di visione artificiale come DALL-E 2, richiedono enormi quantit√† di risorse computazionali per l‚Äôaddestramento. Ad esempio, si stima che GPT-3 consumi 1.300 megawattora di elettricit√†, pari a 1.450 famiglie medie statunitensi in un mese intero (Maslej et al. 2023), o in altre parole, consuma abbastanza energia da rifornire una famiglia media statunitense per 120 anni! Questa immensa richiesta di energia deriva principalmente da data center affamati di energia con server che eseguono calcoli intensivi per addestrare queste complesse reti neurali per giorni o settimane.\nLe stime attuali indicano che le emissioni di carbonio prodotte dallo sviluppo di un singolo modello di intelligenza artificiale sofisticato possono eguagliare le emissioni nell‚Äôarco di vita di cinque veicoli standard a benzina (Strubell, Ganesh, e McCallum 2019). Una parte significativa dell‚Äôelettricit√† attualmente consumata dai data center √® generata da fonti non rinnovabili come carbone e gas naturale, con il risultato che i data center contribuiscono a circa l‚Äô1% delle emissioni totali di carbonio a livello mondiale. Ci√≤ √® paragonabile alle emissioni dell‚Äôintero settore delle compagnie aeree. Questa immensa impronta di carbonio dimostra l‚Äôurgente necessit√† di passare a fonti di energia rinnovabili come l‚Äôenergia solare ed eolica per gestire lo sviluppo dell‚Äôintelligenza artificiale.\nInoltre, anche i sistemi di intelligenza artificiale su piccola scala distribuiti su dispositivi edge come parte di TinyML hanno impatti ambientali che non dovrebbero essere ignorati (Prakash, Stewart, et al. 2023). L‚Äôhardware specializzato richiesto per l‚Äôintelligenza artificiale ha un impatto ambientale dovuto all‚Äôestrazione e alla produzione di risorse naturali. GPU, CPU e chip come le TPU dipendono da metalli delle terre rare la cui estrazione e lavorazione generano un notevole inquinamento. Anche la produzione di questi componenti ha le sue richieste energetiche. Inoltre, la raccolta, l‚Äôarchiviazione e la preelaborazione dei dati utilizzati per addestrare modelli sia su piccola che su larga scala comportano costi ambientali, esacerbando ulteriormente le implicazioni di sostenibilit√† dei sistemi ML.\nPertanto, mentre l‚Äôintelligenza artificiale promette innovazioni in molti campi, per sostenere il progresso √® necessario affrontare le sfide della sostenibilit√†. L‚Äôintelligenza artificiale pu√≤ continuare a progredire in modo responsabile ottimizzando l‚Äôefficienza dei modelli, esplorando hardware specializzato alternativo e fonti di energia rinnovabile per i data center e monitorando il suo impatto ambientale complessivo.",
    "crumbs": [
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>IA Sostenibile</span>"
    ]
  },
  {
    "objectID": "contents/core/sustainable_ai/sustainable_ai.it.html#panoramica",
    "href": "contents/core/sustainable_ai/sustainable_ai.it.html#panoramica",
    "title": "16¬† IA Sostenibile",
    "section": "",
    "text": "Winkler, Harald, Franck Lecocq, Hans Lofgren, Maria Virginia Vilari√±o, Sivan Kartha, e Joana Portugal-Pereira. 2022. ¬´Examples of shifting development pathways: Lessons on how to enable broader, deeper, and faster climate action¬ª. Climate Action 1 (1). https://doi.org/10.1007/s44168-022-00026-1.\n\nMaslej, Nestor, Loredana Fattorini, Erik Brynjolfsson, John Etchemendy, Katrina Ligett, Terah Lyons, James Manyika, et al. 2023. ¬´Artificial intelligence index report 2023¬ª. ArXiv preprint abs/2310.03715. https://arxiv.org/abs/2310.03715.\n\n\n\nPrakash, Shvetank, Matthew Stewart, Colby Banbury, Mark Mazumder, Pete Warden, Brian Plancher, e Vijay Janapa Reddi. 2023. ¬´Is TinyML Sustainable? Assessing the Environmental Impacts of Machine Learning on Microcontrollers¬ª. ArXiv preprint. https://arxiv.org/abs/2301.11899.",
    "crumbs": [
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>IA Sostenibile</span>"
    ]
  },
  {
    "objectID": "contents/core/sustainable_ai/sustainable_ai.it.html#social-and-ethical-responsibility",
    "href": "contents/core/sustainable_ai/sustainable_ai.it.html#social-and-ethical-responsibility",
    "title": "16¬† IA Sostenibile",
    "section": "16.2 Responsabilit√† Sociale ed Etica",
    "text": "16.2 Responsabilit√† Sociale ed Etica\nL‚Äôimpatto ambientale dell‚ÄôIA non √® solo una questione tecnica, ma anche etica e sociale. Man mano che l‚ÄôIA diventa sempre pi√π integrata nelle nostre vite e nei nostri settori, la sua sostenibilit√† diventa sempre pi√π critica.\n\n16.2.1 Considerazioni Etiche\nLa portata dell‚Äôimpatto ambientale dell‚ÄôIA solleva profonde questioni etiche sulle responsabilit√† degli sviluppatori e delle aziende di IA nel ridurre al minimo le emissioni di carbonio e l‚Äôuso di energia. In quanto creatori di sistemi e tecnologie di IA che possono avere impatti globali di vasta portata, gli sviluppatori hanno l‚Äôobbligo etico di integrare consapevolmente la tutela ambientale nel loro processo di progettazione, anche se la sostenibilit√† avviene a scapito di alcuni guadagni di efficienza.\nC‚Äô√® una chiara e attuale necessit√† per noi di avere conversazioni aperte e oneste sui compromessi ambientali dell‚ÄôIA all‚Äôinizio del ciclo di vita dello sviluppo. I ricercatori dovrebbero sentirsi autorizzati a esprimere preoccupazioni se le priorit√† organizzative non sono allineate con gli obiettivi etici, come nel caso della lettera aperta per sospendere i giganteschi esperimenti di IA.\nInoltre, c‚Äô√® una crescente necessit√† per le aziende di IA di esaminare attentamente i loro contributi al cambiamento climatico e al danno ambientale. Le grandi aziende tecnologiche sono responsabili dell‚Äôinfrastruttura cloud, delle richieste di energia dei data center e dell‚Äôestrazione delle risorse necessarie per alimentare l‚ÄôIA odierna. La leadership dovrebbe valutare se i valori e le politiche organizzative promuovano la sostenibilit√†, dalla produzione di hardware alle pipeline di training dei modelli.\nInoltre, potrebbe essere necessaria pi√π di un‚Äôautoregolamentazione volontaria: i governi potrebbero dover introdurre nuove normative volte a standard e pratiche di intelligenza artificiale sostenibili se speriamo di frenare l‚Äôesplosione energetica prevista di modelli sempre pi√π grandi. Le metriche segnalate come l‚Äôutilizzo del computer, l‚Äôimpronta di carbonio e i parametri di riferimento dell‚Äôefficienza potrebbero responsabilizzare le organizzazioni.\nAttraverso principi etici, politiche aziendali e regole pubbliche, i tecnici e le aziende di intelligenza artificiale hanno un profondo dovere nei confronti del nostro pianeta per garantire l‚Äôavanzamento responsabile e sostenibile della tecnologia in grado di trasformare radicalmente la societ√† moderna. Dobbiamo alle generazioni future di fare le cose per bene. Figura¬†16.1 delinea alcune preoccupazioni e sfide etiche che l‚ÄôIA deve affrontare.\n\n\n\n\n\n\nFigura¬†16.1: Sfide etiche nello sviluppo dell‚ÄôIA. Fonte: COE\n\n\n\n\n\n16.2.2 Sostenibilit√† a Lungo Termine\nLa massiccia espansione prevista dell‚ÄôIA solleva urgenti preoccupazioni sulla sua sostenibilit√† a lungo termine. Poich√© il software e le applicazioni di IA aumentano rapidamente in complessit√† e utilizzo in tutti i settori, la domanda di potenza di calcolo e infrastrutture salir√† alle stelle in modo esponenziale nei prossimi anni.\nPer mettere in prospettiva la portata della crescita prevista, la capacit√† di calcolo totale richiesta per l‚Äôaddestramento dei modelli di IA ha visto un sorprendente aumento di 350.000 volte dal 2012 al 2019 (R. Schwartz et al. 2020). I ricercatori prevedono una crescita di oltre un ordine di grandezza ogni anno, man mano che vengono sviluppati assistenti IA personalizzati, tecnologia autonoma, strumenti di medicina di precisione e altro ancora. Le tendenze sono simili per i sistemi ML embedded, con una stima di 2,5 miliardi di dispositivi edge abilitati all‚ÄôIA distribuiti entro il 2030.\nLa gestione di questo livello di espansione richiede innovazioni incentrate su software e hardware in termini di efficienza e integrazione rinnovabile da parte di ingegneri e scienziati dell‚ÄôIA. Dal lato software, nuove tecniche di ottimizzazione dei modelli, distillazione, potatura, numeri a bassa precisione, condivisione delle conoscenze tra sistemi e altre aree devono diventare best practice diffuse per frenare le esigenze energetiche. Ad esempio, realizzare anche una domanda di elaborazione ridotta del 50% per raddoppio della capacit√† avrebbe un impatto enorme sull‚Äôenergia totale.\nDal lato dell‚Äôinfrastruttura hardware, a causa dei crescenti costi di trasferimento dati, archiviazione, raffreddamento e spazio, continuare con l‚Äôattuale modello di server farm centralizzato nei data center √® probabilmente irrealizzabile a lungo termine (Lannelongue, Grealey, e Inouye 2021). Esplorare opzioni di elaborazione decentralizzate alternative attorno a ‚Äúedge AI‚Äù su dispositivi locali o all‚Äôinterno di reti di telecomunicazioni pu√≤ alleviare le pressioni di ridimensionamento sui data center iper-scalabili ad alto consumo energetico. Allo stesso modo, il passaggio a fonti di energia rinnovabile ibride e a zero emissioni di carbonio che alimentano i principali data center dei provider cloud in tutto il mondo sar√† essenziale.\n\nLannelongue, Loƒ±Ãàc, Jason Grealey, e Michael Inouye. 2021. ¬´Green Algorithms: Quantifying the Carbon Footprint of Computation¬ª. Adv. Sci. 8 (12): 2100707. https://doi.org/10.1002/advs.202100707.\n\n\n16.2.3 IA per il Bene Ambientale\nSebbene molta attenzione sia rivolta alle sfide di sostenibilit√† dell‚ÄôIA, queste potenti tecnologie forniscono soluzioni uniche per combattere il cambiamento climatico e guidare il progresso ambientale. Ad esempio, l‚Äôapprendimento automatico pu√≤ ottimizzare continuamente le reti elettriche intelligenti per migliorare l‚Äôintegrazione delle energie rinnovabili e l‚Äôefficienza della distribuzione dell‚Äôelettricit√† attraverso le reti (Zhang, Han, e Deng 2018). I modelli possono acquisire lo stato in tempo reale di una rete elettrica e le previsioni meteorologiche per allocare e spostare le fonti rispondendo alla domanda e all‚Äôofferta.\n\nZhang, Dongxia, Xiaoqing Han, e Chunyu Deng. 2018. ¬´Review on the research and practice of deep learning and reinforcement learning in smart grids¬ª. CSEE Journal of Power and Energy Systems 4 (3): 362‚Äì70. https://doi.org/10.17775/cseejpes.2018.00520.\n\nLam, Remi, Alvaro Sanchez-Gonzalez, Matthew Willson, Peter Wirnsberger, Meire Fortunato, Ferran Alet, Suman Ravuri, et al. 2023. ¬´Learning skillful medium-range global weather forecasting¬ª. Science 382 (6677): 1416‚Äì21. https://doi.org/10.1126/science.adi2336.\n\nKurth, Thorsten, Shashank Subramanian, Peter Harrington, Jaideep Pathak, Morteza Mardani, David Hall, Andrea Miele, Karthik Kashinath, e Anima Anandkumar. 2023. ¬´FourCastNet: Accelerating Global High-Resolution Weather Forecasting Using Adaptive Fourier Neural Operators¬ª. In Proceedings of the Platform for Advanced Scientific Computing Conference, 1‚Äì11. ACM. https://doi.org/10.1145/3592979.3593412.\nLe reti neurali ottimizzate si sono anche dimostrate notevolmente efficaci nelle previsioni meteorologiche di prossima generazione (Lam et al. 2023) e nella modellazione climatica (Kurth et al. 2023). Possono analizzare rapidamente enormi volumi di dati climatici per potenziare la preparazione agli eventi estremi e la pianificazione delle risorse per uragani, inondazioni, siccit√† e altro ancora. I ricercatori del clima hanno raggiunto un‚Äôaccuratezza all‚Äôavanguardia del percorso delle tempeste combinando simulazioni di IA con modelli numerici tradizionali.\nL‚Äôintelligenza artificiale consente inoltre un migliore monitoraggio della biodiversit√† (Silvestro et al. 2022), della fauna selvatica (D. Schwartz et al. 2021), degli ecosistemi e della deforestazione illegale tramite droni e satelliti. Gli algoritmi di visione artificiale possono automatizzare le stime della popolazione delle specie e le valutazioni della salute dell‚Äôhabitat su vaste regioni non monitorate. Queste capacit√† forniscono agli ambientalisti potenti strumenti per combattere il bracconaggio (Bondi et al. 2018), ridurre i rischi di estinzione delle specie e comprendere i cambiamenti ecologici.\n\nSilvestro, Daniele, Stefano Goria, Thomas Sterner, e Alexandre Antonelli. 2022. ¬´Improving biodiversity protection through artificial intelligence¬ª. Nature Sustainability 5 (5): 415‚Äì24. https://doi.org/10.1038/s41893-022-00851-6.\n\nSchwartz, Daniel, Jonathan Michael Gomes Selman, Peter Wrege, e Andreas Paepcke. 2021. ¬´Deployment of Embedded Edge-AI for Wildlife Monitoring in Remote Regions¬ª. In 2021 20th IEEE International Conference on Machine Learning and Applications (ICMLA), 1035‚Äì42. IEEE; IEEE. https://doi.org/10.1109/icmla52953.2021.00170.\n\nBondi, Elizabeth, Ashish Kapoor, Debadeepta Dey, James Piavis, Shital Shah, Robert Hannaford, Arvind Iyer, Lucas Joppa, e Milind Tambe. 2018. ¬´Near Real-Time Detection of Poachers from Drones in AirSim¬ª. In Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, a cura di J√©r√¥me Lang, 5814‚Äì16. International Joint Conferences on Artificial Intelligence Organization. https://doi.org/10.24963/ijcai.2018/847.\nInvestimenti mirati in applicazioni di intelligenza artificiale per la sostenibilit√† ambientale, la condivisione di dati intersettoriali e l‚Äôaccessibilit√† dei modelli possono accelerare notevolmente le soluzioni a urgenti problemi ecologici. L‚Äôenfasi sull‚Äôintelligenza artificiale per il bene sociale indirizza l‚Äôinnovazione in direzioni pi√π pulite, guidando queste tecnologie che modellano il mondo verso uno sviluppo etico e responsabile.\n\n\n16.2.4 Caso di Studio: L‚ÄôIA di DeepMind per l‚ÄôEfficienza Energetica Basata sull‚ÄôIA\nI data center di Google sono fondamentali per alimentare prodotti come Search, Gmail e YouTube, utilizzati quotidianamente da miliardi di persone. Tuttavia, mantenere attive e funzionanti le vaste server farm richiede molta energia, in particolare per i sistemi di raffreddamento essenziali. Google si impegna costantemente per migliorare l‚Äôefficienza in tutte le operazioni. Tuttavia, i progressi si stavano rivelando difficili solo con i metodi tradizionali, considerando le complesse dinamiche personalizzate coinvolte. Questa sfida ha spinto una svolta nell‚Äôapprendimento automatico, producendo potenziali risparmi.\nDopo oltre un decennio di ottimizzazione della progettazione dei data center, invenzione di hardware di elaborazione a basso consumo energetico e protezione di fonti di energia rinnovabili, Google ha portato gli scienziati di DeepMind a sbloccare ulteriori progressi. Gli esperti di intelligenza artificiale hanno affrontato fattori complessi che circondano il funzionamento degli apparati di raffreddamento industriali. Apparecchiature come pompe e refrigeratori interagiscono in modo non lineare, mentre cambiano anche le condizioni meteorologiche esterne e le variabili architettoniche interne. Catturare questa complessit√† ha confuso le rigide formule ingegneristiche e l‚Äôintuizione umana.\nIl team DeepMind ha sfruttato i dati storici estesi dei sensori di Google che descrivono temperature, consumi energetici e altri attributi come input di training. Hanno creato un sistema flessibile basato su reti neurali per modellare le relazioni e prevedere configurazioni ottimali, riducendo al minimo la ‚Äúpower usage effectiveness (PUE)‚Äù [efficacia dell‚Äôutilizzo di energia] (Barroso, H√∂lzle, e Ranganathan 2019); PUE √® la misura standard per valutare l‚Äôefficienza con cui un data center utilizza l‚Äôenergia, che fornisce la percentuale di energia totale consumata dalla struttura divisa per l‚Äôenergia utilizzata direttamente per le operazioni di elaborazione. Quando testato in tempo reale, il sistema AI ha prodotto notevoli guadagni rispetto alle innovazioni precedenti, riducendo l‚Äôenergia di raffreddamento del 40% per un calo del 15% nel PUE totale, un nuovo record del sito. Il framework generalizzabile ha appreso rapidamente le dinamiche di raffreddamento in condizioni mutevoli che le regole statiche non potevano eguagliare. Questa svolta evidenzia il ruolo crescente dell‚ÄôAI nella trasformazione della tecnologia moderna e nell‚Äôabilitazione di un futuro sostenibile.\n\nBarroso, Luiz Andr√©, Urs H√∂lzle, e Parthasarathy Ranganathan. 2019. The Datacenter as a Computer: Designing Warehouse-Scale Machines. Springer International Publishing. https://doi.org/10.1007/978-3-031-01761-2.",
    "crumbs": [
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>IA Sostenibile</span>"
    ]
  },
  {
    "objectID": "contents/core/sustainable_ai/sustainable_ai.it.html#energy-consumption",
    "href": "contents/core/sustainable_ai/sustainable_ai.it.html#energy-consumption",
    "title": "16¬† IA Sostenibile",
    "section": "16.3 Consumo Energetico",
    "text": "16.3 Consumo Energetico\n\n16.3.1 Comprendere le Esigenze Energetiche\nComprendere le esigenze energetiche per il training e il funzionamento dei modelli di intelligenza artificiale √® fondamentale nel campo in rapida evoluzione dell‚Äôintelligenza artificiale. Con l‚Äôintelligenza artificiale che sta entrando in uso diffuso in molti nuovi campi (Bohr e Memarzadeh 2020; Sudhakar, Sze, e Karaman 2023), si prevede che la domanda di dispositivi e data center abilitati all‚Äôintelligenza artificiale esploder√†. Questa comprensione ci aiuta a capire perch√© l‚Äôintelligenza artificiale, in particolare il deep learning, √® spesso etichettata come ad alta intensit√† energetica.\n\nBohr, Adam, e Kaveh Memarzadeh. 2020. ¬´The rise of artificial intelligence in healthcare applications¬ª. In Artificial Intelligence in Healthcare, 25‚Äì60. Elsevier. https://doi.org/10.1016/b978-0-12-818438-7.00002-2.\n\nRequisiti Energetici per il Training dell‚ÄôIntelligenza Artificiale\nIl training di sistemi di intelligenza artificiale complessi come i grandi modelli di deep learning pu√≤ richiedere livelli sorprendentemente elevati di potenza di calcolo, con profonde implicazioni energetiche. Consideriamo il modello linguistico all‚Äôavanguardia di OpenAI GPT-3 come un esempio lampante. Questo sistema spinge i confini della generazione di testo attraverso algoritmi formati su enormi set di dati. Tuttavia, l‚Äôenergia consumata da GPT-3 per un singolo ciclo di addestramento potrebbe rivaleggiare con l‚Äôutilizzo mensile di un‚Äôintera cittadina. Negli ultimi anni, questi modelli di intelligenza artificiale generativa hanno guadagnato sempre pi√π popolarit√†, portando a un numero maggiore di modelli addestrati. Oltre all‚Äôaumento del numero di modelli, aumenter√† anche il numero di parametri in questi modelli. La ricerca mostra che l‚Äôaumento delle dimensioni del modello (numero di parametri), delle dimensioni del set di dati e del calcolo utilizzato per l‚Äôaddestramento migliora le prestazioni in modo fluido senza segni di saturazione (Kaplan et al. 2020). Notare come, in Figura¬†16.2, il ‚Äútest loss‚Äù diminuisce man mano che ciascuno dei 3 aumenta.\n\n\n\n\n\n\nFigura¬†16.2: Le prestazioni migliorano con il calcolo, il set di dati e le dimensioni del modello. Fonte: Kaplan et al. (2020).\n\n\nKaplan, Jared, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, e Dario Amodei. 2020. ¬´Scaling Laws for Neural Language Models¬ª. ArXiv preprint abs/2001.08361. https://arxiv.org/abs/2001.08361.\n\n\nCosa determina requisiti cos√¨ immensi? Durante l‚Äôaddestramento, modelli come GPT-3 apprendono le proprie capacit√† elaborando continuamente enormi volumi di dati per regolare i parametri interni. La capacit√† di elaborazione che consente i rapidi progressi dell‚ÄôIA contribuisce anche all‚Äôaumento del consumo di energia, soprattutto quando i set di dati e i modelli aumentano a dismisura. GPT-3 evidenzia una traiettoria costante nel campo in cui ogni balzo nella sofisticazione dell‚ÄôIA risale a una potenza di calcolo e risorse sempre pi√π sostanziali. Il suo predecessore, GPT-2, richiedeva un addestramento 10 volte inferiore per calcolare solo 1,5 miliardi di parametri, una differenza ora ridotta da grandezze in quanto GPT-3 comprende 175 miliardi di parametri. Mantenere questa traiettoria verso un‚Äôintelligenza artificiale sempre pi√π capace solleva sfide future in termini di fornitura di energia e infrastrutture.\n\n\nUso Operativo dell‚ÄôEnergia\nLo sviluppo e l‚Äôaddestramento di modelli di intelligenza artificiale richiedono un‚Äôenorme quantit√† di dati, potenza di calcolo ed energia. Tuttavia, l‚Äôimplementazione e il funzionamento di tali modelli comportano anche significativi costi ricorrenti di risorse nel tempo. I sistemi di intelligenza artificiale sono ora integrati in vari settori e applicazioni e stanno entrando nella vita quotidiana di una fascia demografica in crescita. Il loro impatto cumulativo sull‚Äôenergia operativa e sulle infrastrutture potrebbe eclissare l‚Äôaddestramento iniziale del modello.\nQuesto concetto si riflette nella domanda di hardware di addestramento e inferenza nei data center e nell‚Äôedge. L‚Äôinferenza si riferisce all‚Äôuso di un modello addestrato per fare previsioni o decisioni su dati del mondo reale. Secondo una recente analisi McKinsey, la necessit√† di sistemi avanzati per addestrare modelli sempre pi√π grandi sta crescendo rapidamente.\nTuttavia, i calcoli di inferenza costituiscono gi√† una parte dominante e crescente dei carichi di lavoro totali dell‚Äôintelligenza artificiale, come mostrato in Figura¬†16.3. L‚Äôesecuzione di inferenze in tempo reale con modelli addestrati, sia per la classificazione delle immagini, il riconoscimento vocale o l‚Äôanalisi predittiva, richiede invariabilmente hardware di elaborazione come server e chip. Tuttavia, persino un modello che gestisce migliaia di richieste di riconoscimento facciale o query in linguaggio naturale ogni giorno √® messo in ombra da piattaforme enormi come Meta. Dove l‚Äôinferenza su milioni di foto e video condivisi sui social media, i requisiti energetici dell‚Äôinfrastruttura continuano a crescere.\n\n\n\n\n\n\nFigura¬†16.3: Dimensioni del mercato per l‚Äôhardware di inferenza e training. Fonte: McKinsey.\n\n\n\nGli algoritmi che alimentano assistenti intelligenti abilitati all‚Äôintelligenza artificiale, magazzini automatizzati, veicoli a guida autonoma, assistenza sanitaria personalizzata e altro hanno un‚Äôimpronta energetica individuale marginale. Tuttavia, la proliferazione prevista di queste tecnologie potrebbe aggiungere centinaia di milioni di endpoint che eseguono algoritmi di intelligenza artificiale ininterrottamente, causando un aumento della scala dei loro requisiti energetici collettivi. Gli attuali guadagni di efficienza hanno bisogno di aiuto per controbilanciare questa crescita netta.\nSi prevede che l‚Äôintelligenza artificiale registrer√† un tasso di crescita annuale del 37,3% tra il 2023 e il 2030. Tuttavia, applicando lo stesso tasso di crescita al computing operativo, entro il 2030 il fabbisogno energetico annuale dell‚ÄôIA potrebbe moltiplicarsi fino a 1.000 volte. Quindi, mentre l‚Äôottimizzazione del modello affronta un aspetto, l‚Äôinnovazione responsabile deve anche considerare i costi totali del ciclo di vita su scale di distribuzione globali che erano inimmaginabili solo anni fa, ma che ora pongono sfide infrastrutturali e di sostenibilit√†.\n\n\n\n16.3.2 Data Center e il Loro Impatto\nCon l‚Äôaumento della domanda di servizi di IA, l‚Äôimpatto dei data center sul consumo energetico dei sistemi di IA sta diventando sempre pi√π importante. Sebbene queste strutture siano fondamentali per il progresso e la distribuzione dell‚ÄôIA, contribuiscono in modo significativo al suo impatto energetico.\n\nScala\nI data center sono i cavalli da tiro essenziali che consentono le recenti richieste di elaborazione dei sistemi di IA avanzati. Ad esempio, i principali provider come Meta gestiscono enormi data center che si estendono fino alle dimensioni di pi√π campi da calcio, ospitando centinaia di migliaia di server ad alta capacit√† ottimizzati per l‚Äôelaborazione parallela e la produttivit√† dei dati.\nQueste enormi strutture forniscono l‚Äôinfrastruttura per addestrare reti neurali complesse su vasti set di dati. Ad esempio, sulla base di informazioni trapelate, il modello linguistico GPT-4 di OpenAI √® stato addestrato su data center Azure con oltre 25.000 GPU Nvidia A100, utilizzate ininterrottamente per oltre 90-100 giorni.\nInoltre, l‚Äôinferenza in tempo reale per applicazioni AI consumer su larga scala √® resa possibile solo sfruttando le server farm all‚Äôinterno dei data center. Servizi come Alexa, Siri e Google Assistant elaborano miliardi di richieste vocali al mese da parte di utenti in tutto il mondo, basandosi sul data center computing per una risposta a bassa latenza. In futuro, l‚Äôespansione di casi d‚Äôuso all‚Äôavanguardia come veicoli a guida autonoma, diagnosi di medicina di precisione e modelli di previsione climatica accurati richieder√† risorse di calcolo significative da ottenere attingendo a vaste risorse di cloud computing on-demand dai data center. Alcune applicazioni emergenti, come le auto autonome, hanno rigidi vincoli di latenza e larghezza di banda. Sar√† necessario collocare la potenza di calcolo a livello di data center sull‚Äôedge anzich√© sul cloud.\nI prototipi di ricerca del MIT hanno mostrato camion e auto con hardware di bordo che eseguono l‚Äôelaborazione AI in tempo reale dei dati dei sensori equivalenti a piccoli data center (Sudhakar, Sze, e Karaman 2023). Questi innovativi ‚Äúdata center su ruote‚Äù dimostrano come veicoli come i camion a guida autonoma potrebbero aver bisogno di un calcolo su scala di data center embedded a bordo per ottenere una latenza di sistema di millisecondi per la navigazione, sebbene probabilmente ancora integrato dalla connettivit√† wireless 5G a data center cloud pi√π potenti.\n\nSudhakar, Soumya, Vivienne Sze, e Sertac Karaman. 2023. ¬´Data Centers on Wheels: Emissions From Computing Onboard Autonomous Vehicles¬ª. IEEE Micro 43 (1): 29‚Äì39. https://doi.org/10.1109/mm.2022.3219803.\nLa larghezza di banda, lo storage e le capacit√† di elaborazione richieste per abilitare questa futura tecnologia su larga scala dipenderanno in larga misura dai progressi nell‚Äôinfrastruttura dei data center e dalle innovazioni algoritmiche dell‚Äôintelligenza artificiale.\n\n\nDomanda di Energia\nLa domanda di energia dei data center pu√≤ essere approssimativamente suddivisa in 4 componenti: infrastruttura, rete, storage e server. In Figura¬†16.4, vediamo che l‚Äôinfrastruttura dati (che include raffreddamento, illuminazione e controlli) e i server utilizzano la maggior parte del budget energetico totale dei data center negli Stati Uniti (Shehabi et al. 2016). Questa sezione suddivide la domanda di energia per i server e l‚Äôinfrastruttura. Per quest‚Äôultima, l‚Äôattenzione √® rivolta ai sistemi di raffreddamento, poich√© il raffreddamento √® il fattore dominante nel consumo energetico nell‚Äôinfrastruttura.\n\nShehabi, Arman, Sarah Smith, Dale Sartor, Richard Brown, Magnus Herrlin, Jonathan Koomey, Eric Masanet, Nathaniel Horner, In√™s Azevedo, e William Lintner. 2016. ¬´United states data center energy usage report¬ª.\n\n\n\n\n\n\nFigura¬†16.4: Consumo energetico dei data center negli Stati Uniti. Fonte: International Energy Agency (IEA).\n\n\n\n\nServer\nL‚Äôaumento del consumo energetico dei data center deriva principalmente dalla crescita esponenziale dei requisiti di elaborazione AI. Le macchine NVIDIA DGX H100 ottimizzate per il deep learning possono assorbire fino a 10,2 kW al picco. I principali provider gestiscono data center con centinaia o migliaia di questi nodi DGX ad alto consumo energetico collegati in rete per addestrare i pi√π recenti modelli AI. Ad esempio, il supercomputer sviluppato per OpenAI √® un singolo sistema con oltre 285.000 core CPU, 10.000 GPU e 400 gigabit al secondo di connettivit√† di rete per ogni server GPU.\nI calcoli intensivi necessari per l‚Äôintera flotta densamente popolata di una struttura e l‚Äôhardware di supporto comportano che i data center assorbano decine di megawatt 24 ore su 24. Nel complesso, gli algoritmi AI avanzati continuano ad aumentare il consumo energetico dei data center man mano che vengono distribuiti pi√π nodi DGX per tenere il passo con la crescita prevista della domanda di risorse di elaborazione AI nei prossimi anni.\n\n\nSistemi di Raffreddamento\nPer mantenere i server robusti alimentati al massimo della capacit√† e freschi, i data center richiedono una capacit√† di raffreddamento enorme per contrastare il calore prodotto da server densamente stipati, apparecchiature di rete e altro hardware che eseguono carichi di lavoro intensivi di elaborazione senza sosta. Con grandi data center che contengono migliaia di rack di server che operano a pieno regime, sono necessarie torri di raffreddamento e refrigeratori su scala industriale, che utilizzano energia pari al 30-40% dell‚Äôimpronta elettrica totale del data center (Dayarathna, Wen, e Fan 2016). Di conseguenza, le aziende sono alla ricerca di metodi di raffreddamento alternativi. Ad esempio, il data center di Microsoft in Irlanda sfrutta un fiordo vicino per scambiare calore utilizzando oltre mezzo milione di galloni [1.9 milioni di litri] di acqua di mare al giorno.\nRiconoscendo l‚Äôimportanza del raffreddamento efficiente dal punto di vista energetico, sono state introdotte innovazioni volte a ridurre questa domanda di energia. Tecniche come il raffreddamento gratuito, che utilizza fonti di aria o acqua esterne quando le condizioni sono favorevoli, e l‚Äôuso dell‚Äôintelligenza artificiale per ottimizzare i sistemi di raffreddamento sono esempi di come il settore si adatta. Queste innovazioni riducono il consumo energetico, i costi operativi e diminuiscono l‚Äôimpatto ambientale. Tuttavia, gli aumenti esponenziali della complessit√† del modello AI continuano a richiedere pi√π server e hardware di accelerazione che operano a un utilizzo pi√π elevato, il che si traduce in una maggiore generazione di calore e in un‚Äôenergia sempre maggiore utilizzata esclusivamente per il raffreddamento.\n\n\n\nL‚Äôimpatto Ambientale\nL‚Äôimpatto ambientale dei data center non √® causato solo dal consumo energetico diretto del data center stesso (Siddik, Shehabi, e Marston 2021). Il funzionamento del data center comporta la fornitura di acqua trattata al data center e lo scarico delle acque reflue dal data center. Gli impianti idrici e di trattamento delle acque reflue sono i principali consumatori di elettricit√†.\n\nSiddik, Md Abu Bakar, Arman Shehabi, e Landon Marston. 2021. ¬´The environmental footprint of data centers in the United States¬ª. Environ. Res. Lett. 16 (6): 064017. https://doi.org/10.1088/1748-9326/abfba1.\n\nDavis, Jacqueline, Daniel Bizo, Andy Lawrence, Owen Rogers, e Max Smolaks. 2022. ¬´Uptime Institute Global Data Center Survey 2022¬ª. Uptime Institute.\nOltre al consumo di elettricit√†, ci sono molti altri aspetti dell‚Äôimpatto ambientale di questi data center. Il consumo di acqua dei data center pu√≤ portare a problemi di scarsit√† idrica, maggiori esigenze di trattamento delle acque e adeguate infrastrutture di scarico delle acque reflue. Inoltre, le materie prime necessarie per la costruzione e la trasmissione di rete hanno un impatto considerevole sull‚Äôambiente e i componenti nei data center devono essere aggiornati e sottoposti a manutenzione. Laddove quasi il 50 percento dei server √® stato aggiornato entro 3 anni di utilizzo, i cicli di aggiornamento hanno dimostrato di rallentare (Davis et al. 2022). Tuttavia, ci√≤ genera notevoli rifiuti elettronici, che possono essere difficili da riciclare.\n\n\n\n16.3.3 Ottimizzazione Energetica\nIn definitiva, misurare e comprendere il consumo energetico dell‚ÄôIA facilita l‚Äôottimizzazione del consumo energetico.\nUn modo per ridurre il consumo energetico di una data quantit√† di lavoro computazionale √® eseguirlo su hardware pi√π efficiente dal punto di vista energetico. Ad esempio, i chip TPU possono essere pi√π efficienti dal punto di vista energetico rispetto alle CPU quando si tratta di eseguire grandi calcoli tensoriali per l‚ÄôIA, poich√© le TPU possono eseguire tali calcoli molto pi√π velocemente senza consumare molta pi√π energia delle CPU. Un altro modo √® quello di creare sistemi software consapevoli del consumo energetico e delle caratteristiche dell‚Äôapplicazione. Buoni esempi sono lavori di sistema come Zeus (You, Chung, e Chowdhury 2023) e Perseus (Chung et al. 2023), entrambi caratterizzati dal compromesso tra tempo di calcolo e consumo energetico a vari livelli di un sistema di addestramento ML per ottenere una riduzione energetica senza rallentamento end-to-end. In realt√†, costruire sia hardware che software a basso consumo energetico e combinarne i vantaggi dovrebbe essere promettente, insieme a framework open source (ad esempio, Zeus) che facilitano gli sforzi della comunit√†.\n\nYou, Jie, Jae-Won Chung, e Mosharaf Chowdhury. 2023. ¬´Zeus: Understanding and Optimizing GPU Energy Consumption of DNN Training¬ª. In 20th USENIX Symposium on Networked Systems Design and Implementation (NSDI 23), 119‚Äì39. Boston, MA: USENIX Association. https://www.usenix.org/conference/nsdi23/presentation/you.\n\nChung, Jae-Won, Yile Gu, Insu Jang, Luoxi Meng, Nikhil Bansal, e Mosharaf Chowdhury. 2023. ¬´Perseus: Removing Energy Bloat from Large Model Training¬ª. ArXiv preprint abs/2312.06902. https://arxiv.org/abs/2312.06902.",
    "crumbs": [
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>IA Sostenibile</span>"
    ]
  },
  {
    "objectID": "contents/core/sustainable_ai/sustainable_ai.it.html#carbon-footprint",
    "href": "contents/core/sustainable_ai/sustainable_ai.it.html#carbon-footprint",
    "title": "16¬† IA Sostenibile",
    "section": "16.4 Impronta di Carbonio",
    "text": "16.4 Impronta di Carbonio\nI data center consumano enormi quantit√† di elettricit√† e, senza l‚Äôaccesso a fonti di energia rinnovabile, questa richiesta pu√≤ avere un impatto ambientale notevole. Molte strutture dipendono fortemente da fonti di energia non rinnovabili come carbone e gas naturale. Ad esempio, si stima che i data center producano fino al 2% delle emissioni globali totali di \\(\\textrm{CO}_2\\), il che sta colmando il divario con il settore aereo. Come accennato nelle sezioni precedenti, le richieste di elaborazione dell‚Äôintelligenza artificiale sono destinate ad aumentare. Le emissioni di questa ondata sono triplici. In primo luogo, si prevede che i data center aumenteranno di dimensioni (Liu et al. 2020). In secondo luogo, le emissioni durante il training sono destinate ad aumentare in modo significativo (Patterson et al. 2022). In terzo luogo, le chiamate di inferenza a questi modelli sono destinate ad aumentare drasticamente.\n\nLiu, Yanan, Xiaoxia Wei, Jinyu Xiao, Zhijie Liu, Yang Xu, e Yun Tian. 2020. ¬´Energy consumption and emission mitigation prediction based on data center traffic and PUE for global data centers¬ª. Global Energy Interconnection 3 (3): 272‚Äì82. https://doi.org/10.1016/j.gloei.2020.07.008.\nSenza azioni, questa crescita esponenziale della domanda rischia di aumentare ulteriormente l‚Äôimpronta di carbonio dei data center a livelli insostenibili. I principali fornitori hanno promesso la neutralit√† carbonica e impegnato fondi per garantire energia pulita, ma i progressi rimangono incrementali rispetto ai piani di espansione complessivi del settore. Politiche di decarbonizzazione della rete pi√π radicali e investimenti in energia rinnovabile potrebbero rivelarsi essenziali per contrastare l‚Äôimpatto climatico dell‚Äôondata imminente di nuovi data center volti a supportare la prossima generazione di IA.\n\n16.4.1 Definizione e Significato\nIl concetto di ‚Äúimpronta di carbonio‚Äù √® emerso come una metrica chiave. Questo termine si riferisce alla quantit√† totale di gas serra, in particolare anidride carbonica, emessi direttamente o indirettamente da un individuo, un‚Äôorganizzazione, un evento o un prodotto. Queste emissioni contribuiscono in modo significativo all‚Äôeffetto serra, accelerando il riscaldamento globale e il cambiamento climatico. L‚Äôimpronta di carbonio √® misurata in termini di equivalenti di anidride carbonica (\\(\\textrm{CO}_2\\)e), consentendo un resoconto completo che include vari gas serra e il loro relativo impatto ambientale. Esempi di ci√≤ applicato ad attivit√† di ML su larga scala sono mostrati in Figura¬†16.5.\n\n\n\n\n\n\nFigura¬†16.5: Impronta di carbonio delle attivit√† di ML su larga scala. Fonte: Wu et al. (2022).\n\n\n\nConsiderare l‚Äôimpronta di carbonio √® particolarmente importante nel rapido progresso dell‚ÄôIA e nella sua integrazione in vari settori, mettendone in evidenza l‚Äôimpatto ambientale. I sistemi di IA, in particolare quelli che comportano calcoli intensivi come il deep learning e l‚Äôelaborazione di dati su larga scala, sono noti per le loro notevoli richieste di energia. Questa energia, spesso ricavata dalle reti elettriche, potrebbe ancora basarsi prevalentemente sui combustibili fossili, il che comporta significative emissioni di gas serra.\nPrendiamo ad esempio l‚Äôaddestramento di grandi modelli di IA come GPT-3 o complesse reti neurali. Questi processi richiedono un‚Äôimmensa potenza di calcolo, in genere fornita dai data center. Il consumo energetico associato al funzionamento di questi centri, in particolare per attivit√† ad alta intensit√†, comporta notevoli emissioni di gas serra. Gli studi hanno evidenziato che l‚Äôaddestramento di un singolo modello di intelligenza artificiale pu√≤ generare emissioni di carbonio paragonabili a quelle delle emissioni di pi√π auto nel corso della loro vita, facendo luce sul costo ambientale dello sviluppo di tecnologie di intelligenza artificiale avanzate (Dayarathna, Wen, e Fan 2016). Figura¬†16.6 mostra un confronto tra le impronte di carbonio pi√π basse e pi√π alte, a partire da un volo di andata e ritorno tra New York e San Francisco, la vita media umana all‚Äôanno, la vita media americana all‚Äôanno, un‚Äôauto statunitense incluso il carburante nel corso della vita e un modello Transformer con ricerca di architettura neurale, che ha l‚Äôimpronta pi√π alta.\n\n\n\n\n\n\nFigura¬†16.6: Impronta di carbonio del modello NLP in libbre di \\(\\textrm{CO}_2\\) equivalente. Fonte: Dayarathna, Wen, e Fan (2016).\n\n\nDayarathna, Miyuru, Yonggang Wen, e Rui Fan. 2016. ¬´Data Center Energy Consumption Modeling: A Survey¬ª. IEEE Communications Surveys &amp; Tutorials 18 (1): 732‚Äì94. https://doi.org/10.1109/comst.2015.2481183.\n\n\nInoltre, l‚Äôimpronta di carbonio dell‚ÄôIA si estende oltre la fase operativa. L‚Äôintero ciclo di vita dei sistemi di IA, inclusa la produzione di hardware di elaborazione, l‚Äôenergia utilizzata nei data center per il raffreddamento e la manutenzione e lo smaltimento dei rifiuti elettronici, contribuisce alla loro impronta di carbonio complessiva. Abbiamo discusso alcuni di questi aspetti in precedenza e discuteremo degli aspetti relativi ai rifiuti pi√π avanti in questo capitolo.\n\n\n16.4.2 La Necessit√† di Consapevolezza e Azione\nComprendere l‚Äôimpronta di carbonio dei sistemi di IA √® fondamentale per diversi motivi. In primo luogo, √® un passo avanti verso la mitigazione degli impatti del cambiamento climatico. Man mano che l‚Äôintelligenza artificiale continua a crescere e a permeare diversi aspetti delle nostre vite, il suo contributo alle emissioni globali di carbonio diventa una preoccupazione significativa. La consapevolezza di queste emissioni pu√≤ informare le decisioni prese da sviluppatori, aziende, decisori politici e persino ingegneri e scienziati ML come noi per garantire un equilibrio tra innovazione tecnologica e responsabilit√† ambientale.\nInoltre, questa comprensione stimola la spinta verso la ‚ÄòGreen AI‚Äô (R. Schwartz et al. 2020). Questo approccio si concentra sullo sviluppo di tecnologie di intelligenza artificiale efficienti, potenti e sostenibili dal punto di vista ambientale. Incoraggia l‚Äôesplorazione di algoritmi ad alta efficienza energetica, l‚Äôutilizzo di fonti di energia rinnovabili nei data center e l‚Äôadozione di pratiche che riducano l‚Äôimpatto ambientale complessivo dell‚Äôintelligenza artificiale.\nIn sostanza, l‚Äôimpronta di carbonio √® una considerazione essenziale nello sviluppo e nell‚Äôapplicazione delle tecnologie di intelligenza artificiale. Man mano che l‚Äôintelligenza artificiale si evolve e le sue applicazioni diventano pi√π diffuse, la gestione della sua impronta di carbonio √® fondamentale per garantire che questo progresso tecnologico sia in linea con gli obiettivi pi√π ampi di sostenibilit√† ambientale.\n\n\n16.4.3 Stima dell‚ÄôImpronta di Carbonio dell‚ÄôIA\nStimare l‚Äôimpronta di carbonio dei sistemi di IA √® fondamentale per comprendere il loro impatto ambientale. Ci√≤ comporta l‚Äôanalisi dei vari elementi che contribuiscono alle emissioni durante il ciclo di vita delle tecnologie di intelligenza artificiale e l‚Äôimpiego di metodologie specifiche per quantificare accuratamente tali emissioni. Sono stati proposti molti metodi diversi per quantificare le emissioni di carbonio dell‚Äôapprendimento automatico.\nL‚Äôimpronta di carbonio dell‚Äôintelligenza artificiale comprende diversi elementi chiave, ognuno dei quali contribuisce all‚Äôimpatto ambientale complessivo. Innanzitutto, l‚Äôenergia viene consumata durante le fasi di addestramento e operative del modello di intelligenza artificiale. La fonte di questa energia influenza pesantemente le emissioni di carbonio. Una volta addestrati, questi modelli, a seconda della loro applicazione e scala, continuano a consumare elettricit√† durante il funzionamento. Oltre alle considerazioni energetiche, anche l‚Äôhardware utilizzato stressa l‚Äôambiente.\nL‚Äôimpronta di carbonio varia in modo significativo in base alle fonti di energia utilizzate. La composizione delle fonti che forniscono l‚Äôenergia utilizzata nella rete varia ampiamente a seconda della regione geografica e persino del momento in un singolo giorno. Ad esempio, negli Stati Uniti, circa il 60 percento dell‚Äôapprovvigionamento energetico totale √® ancora coperto da combustibili fossili. Le fonti di energia nucleare e rinnovabili coprono il restante 40 percento. Queste frazioni non sono costanti durante il giorno. Poich√© la produzione di energia rinnovabile solitamente si basa su fattori ambientali, come la radiazione solare e i campi di pressione, non forniscono una fonte di energia costante.\nLa variabilit√† della produzione di energia rinnovabile √® stata una sfida continua nell‚Äôuso diffuso di queste fonti. Guardando Figura¬†16.7, che mostra i dati per la rete europea, vediamo che dovrebbe essere in grado di produrre la quantit√† di energia richiesta durante il giorno. Mentre l‚Äôenergia solare raggiunge il picco a met√† giornata, quella eolica ha due picchi distinti, al mattino e alla sera. Attualmente, facciamo affidamento su metodi di produzione di energia basati su combustibili fossili e carbone per supplire alla mancanza di energia nei periodi in cui le energie rinnovabili non soddisfano il fabbisogno.\n√à necessaria l‚Äôinnovazione nelle soluzioni di accumulo di energia per consentire un uso costante di fonti di energia rinnovabile. Il carico energetico di base √® attualmente soddisfatto dall‚Äôenergia nucleare. Questa fonte energetica costante non produce direttamente emissioni di carbonio, ma deve essere pi√π rapida per adattarsi alla variabilit√† delle fonti energetiche rinnovabili. Le aziende tecnologiche come Microsoft hanno mostrato interesse per le fonti di energia nucleare per alimentare i loro data center. Poich√© la domanda dei data center √® pi√π costante rispetto alla domanda delle normali famiglie, l‚Äôenergia nucleare potrebbe essere utilizzata come fonte energetica dominante.\n\n\n\n\n\n\nFigura¬†16.7: Fonti di energia e capacit√† di generazione. Fonte: Energy Charts.\n\n\n\nInoltre, la produzione e lo smaltimento dell‚Äôhardware AI aumentano l‚Äôimpronta di carbonio. La produzione di dispositivi informatici specializzati, come GPU e CPU, richiede molta energia e risorse. Questa fase spesso si basa su fonti energetiche che contribuiscono alle emissioni di gas serra. Il processo di produzione dell‚Äôindustria elettronica √® stato identificato come una delle otto grandi catene di fornitura responsabili di oltre il 50 percento delle emissioni globali (Challenge 2021). Inoltre, lo smaltimento a fine vita di questo hardware, che pu√≤ portare a rifiuti elettronici, ha anche implicazioni ambientali. Come accennato, i server hanno un ciclo di aggiornamento di circa 3-5 anni. Di questi rifiuti elettronici, attualmente solo il 17,4 percento viene raccolto e riciclato correttamente. Le emissioni di carbonio di questi rifiuti elettronici hanno mostrato un aumento di oltre il 50 percento tra il 2014 e il 2020 (Singh e Ogunseitan 2022).\n\nChallenge, WEF Net-Zero. 2021. ¬´The Supply Chain Opportunity¬ª. In World Economic Forum: Geneva, Switzerland.\n\nSingh, Narendra, e Oladele A. Ogunseitan. 2022. ¬´Disentangling the worldwide web of e-waste and climate change co-benefits¬ª. Circular Economy 1 (2): 100011. https://doi.org/10.1016/j.cec.2022.100011.\nCome √® chiaro da quanto sopra, √® necessaria un‚Äôadeguata analisi del ciclo di vita per descrivere tutti gli aspetti rilevanti delle emissioni causate dall‚ÄôIA. Un altro metodo √® la contabilit√† del carbonio, che valuta la quantit√† di emissioni di anidride carbonica direttamente e indirettamente associate alle operazioni di IA. Questa misura utilizza in genere equivalenti di \\(\\textrm{CO}_2\\), consentendo un modo standardizzato di segnalare e valutare le emissioni.\n\n\n\n\n\n\nEsercizio¬†16.1: Impronta di Carbonio dell‚ÄôIA\n\n\n\n\n\nSapevate che i modelli di IA all‚Äôavanguardia che potreste utilizzare hanno un impatto ambientale? Questo esercizio approfondir√† l‚Äô‚Äúimpronta di carbonio‚Äù di un sistema di IA. Imparerete come le richieste energetiche dei data center, il training dei grandi modelli di IA e persino la produzione di hardware contribuiscono alle emissioni di gas serra. Discuteremo perch√© √® fondamentale essere consapevoli di questo impatto e impareremo metodi per stimare l‚Äôimpronta di carbonio dei progetti di IA. Prepariamoci ad esplorare l‚Äôintersezione tra IA e sostenibilit√† ambientale!",
    "crumbs": [
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>IA Sostenibile</span>"
    ]
  },
  {
    "objectID": "contents/core/sustainable_ai/sustainable_ai.it.html#beyond-carbon-footprint",
    "href": "contents/core/sustainable_ai/sustainable_ai.it.html#beyond-carbon-footprint",
    "title": "16¬† IA Sostenibile",
    "section": "16.5 Oltre l‚ÄôImpronta di Carbonio",
    "text": "16.5 Oltre l‚ÄôImpronta di Carbonio\nL‚Äôattuale attenzione alla riduzione delle emissioni di carbonio e del consumo energetico dei sistemi di intelligenza artificiale affronta un aspetto cruciale della sostenibilit√†. Tuttavia, la produzione di semiconduttori e hardware che consentono l‚Äôintelligenza artificiale comporta anche gravi impatti ambientali che ricevono relativamente meno attenzione pubblica. Costruire e gestire un impianto di fabbricazione di semiconduttori all‚Äôavanguardia, o ‚Äúfab‚Äù, ha notevoli requisiti di risorse e sottoprodotti inquinanti che vanno oltre un‚Äôampia impronta di carbonio.\nAd esempio, una fabbrica all‚Äôavanguardia che produce chip come quelli a 5 nm potrebbe richiedere fino a quattro milioni di galloni di acqua pura al giorno. Questo consumo di acqua si avvicina a ci√≤ che una citt√† di mezzo milione di persone richiederebbe per tutte le esigenze. L‚Äôapprovvigionamento di questa risorsa pone costantemente un‚Äôenorme pressione sulle falde acquifere e sui bacini idrici locali, soprattutto nelle regioni gi√† sottoposte a stress idrico che ospitano molti centri di produzione ad alta tecnologia.\nInoltre, oltre 250 sostanze chimiche pericolose uniche vengono utilizzate in varie fasi della produzione di semiconduttori all‚Äôinterno delle fab (Mills e Le Hunte 1997). Tra queste, solventi volatili come acido solforico, acido nitrico e acido fluoridrico, insieme ad arsina, fosfina e altre sostanze altamente tossiche. Per impedire lo scarico di queste sostanze chimiche sono necessari ampi controlli di sicurezza e infrastrutture di trattamento delle acque reflue per evitare la contaminazione del suolo e rischi per le comunit√† circostanti. Qualsiasi manipolazione chimica impropria o fuoriuscita imprevista comporta conseguenze disastrose.\n\nMills, Andrew, e Stephen Le Hunte. 1997. ¬´An overview of semiconductor photocatalysis¬ª. J. Photochem. Photobiol., A 108 (1): 1‚Äì35. https://doi.org/10.1016/s1010-6030(97)00118-4.\nOltre al consumo di acqua e ai rischi chimici, le operazioni di fabbricazione dipendono anche dall‚Äôapprovvigionamento di metalli rari, generano tonnellate di rifiuti pericolosi e possono ostacolare la biodiversit√† locale. Questa sezione analizzer√† questi impatti critici ma meno discussi. Con vigilanza e investimenti nella sicurezza, i danni derivanti dalla produzione di semiconduttori possono essere contenuti pur consentendo il progresso tecnologico. Tuttavia, ignorare questi problemi esternalizzati aggraver√† i danni ecologici e i rischi per la salute nel lungo periodo.\n\n16.5.1 Utilizzo e Stress Idrico\nLa fabbricazione di semiconduttori √® un processo che richiede un consumo di acqua incredibilmente elevato. In base a un articolo del 2009, un tipico wafer di silicio da 300 mm richiede 8.328 litri di acqua, di cui 5.678 litri sono acqua ultrapura (Cope 2009). Mentre fabbriche moderne come quelle menzionate in precedenza possono utilizzare diversi milioni di galloni di acqua pura al giorno, si prevede che l‚Äôultima fabbrica di TSMC in Arizona ne consumer√† ancora di pi√π, 8,9 milioni di galloni al giorno, pari a quasi il 3 percento dell‚Äôattuale produzione idrica della citt√†. Per mettere le cose in prospettiva, Intel e Quantis hanno scoperto che oltre il 97% del loro consumo diretto di acqua √® attribuito alle operazioni di produzione di semiconduttori all‚Äôinterno dei loro stabilimenti di fabbricazione (Cooper et al. 2011).\n\nCope, Gord. 2009. ¬´Pure water, semiconductors and the recession¬ª. Global Water Intelligence 10 (10).\n\nCooper, Tom, Suzanne Fallender, Joyann Pafumi, Jon Dettling, Sebastien Humbert, e Lindsay Lessard. 2011. ¬´A semiconductor company‚Äôs examination of its water footprint approach¬ª. In Proceedings of the 2011 IEEE International Symposium on Sustainable Systems and Technology, 1‚Äì6. IEEE; IEEE. https://doi.org/10.1109/issst.2011.5936865.\n\n\n\n\n\n\nFigura¬†16.8: Impronta Idrica Giornaliera dei Data Center in confronto ad altri utilizzi idrici. Fonte: Raffreddamento del Data Center di Google\n\n\n\nPer mettere in prospettiva questi numeri, si consideri un data center di Google, che utilizza circa 450.000 galloni di acqua al giorno. Ci√≤ equivale ad irrigare 17 acri di erba o a produrre 160 paia di jeans di cotone, il che dimostra l‚Äôenorme richiesta di acqua delle tecnologie avanzate.\nQuest‚Äôacqua viene ripetutamente utilizzata per rimuovere i contaminanti nelle fasi di pulizia e funge anche da refrigerante e fluido vettore nei processi di ossidazione termica, deposizione chimica e planarizzazione chimico-meccanica. Nei mesi estivi di punta, ci√≤ equivale approssimativamente al consumo giornaliero di acqua di una citt√† con una popolazione di mezzo milione di persone.\nNonostante si trovi in regioni con acqua a sufficienza, l‚Äôuso intensivo pu√≤ depauperare gravemente le falde acquifere e i bacini di drenaggio locali. Ad esempio, la citt√† di Hsinchu a Taiwan ha subito affondamenti delle falde acquifere e intrusioni di acqua marina nelle falde acquifere a causa dell‚Äôeccessivo pompaggio per soddisfare le richieste di approvvigionamento idrico della fabbrica della Taiwan Semiconductor Manufacturing Company (TSMC). Nelle aree interne con scarsit√† d‚Äôacqua come l‚ÄôArizona, sono necessari massicci apporti di acqua per supportare le fabbriche nonostante i bacini gi√† esistenti.\nLo scarico di acqua dalle fabbriche rischia di contaminare l‚Äôambiente oltre all‚Äôesaurimento se non trattato correttamente. Sebbene gran parte dello scarico venga riciclato all‚Äôinterno della fabbrica, i sistemi di purificazione filtrano comunque metalli, acidi e altri contaminanti che possono inquinare fiumi e laghi se non gestiti con cautela (Prakash, Callahan, et al. 2023). Questi fattori rendono essenziale la gestione dell‚Äôuso dell‚Äôacqua quando si mitigano impatti pi√π ampi sulla sostenibilit√†.\n\n\n16.5.2 Uso di Sostanze Chimiche Pericolose\nLa moderna fabbricazione di semiconduttori comporta la lavorazione di molte sostanze chimiche altamente pericolose in condizioni estreme di calore e pressione (Kim et al. 2018). Le principali sostanze chimiche utilizzate includono:\n\nKim, Sunju, Chungsik Yoon, Seunghon Ham, Jihoon Park, Ohun Kwon, Donguk Park, Sangjun Choi, Seungwon Kim, Kwonchul Ha, e Won Kim. 2018. ¬´Chemical use in the semiconductor manufacturing industry¬ª. Int. J. Occup. Env. Heal. 24 (3-4): 109‚Äì18. https://doi.org/10.1080/10773525.2018.1519957.\n\nAcidi forti: Gli acidi fluoridrico, solforico, nitrico e cloridrico corrodono rapidamente gli ossidi e altri contaminanti superficiali, ma presentano anche pericoli di tossicit√†. Le fab possono utilizzare migliaia di tonnellate di questi acidi all‚Äôanno e l‚Äôesposizione accidentale pu√≤ essere fatale per i lavoratori.\nSolventi: Solventi chiave come xilene, metanolo e metilisobutilchetone (MIBK) gestiscono i fotoresist dissolvibili, ma hanno effetti negativi sulla salute come irritazione della pelle/degli occhi ed effetti narcotici se maneggiati in modo improprio. Creano anche rischi di esplosione e inquinamento atmosferico.\nGas tossici: Le miscele di gas contenenti arsina (AsH3), fosfina (PH3), diborano (B2H6), germano (GeH4), ecc., sono alcune delle sostanze chimiche pi√π letali utilizzate nelle fasi di doping e deposizione di vapore. Esposizioni minime possono causare avvelenamento, danni ai tessuti e persino la morte senza un trattamento rapido.\nComposti clorurati: Le vecchie formulazioni di planarizzazione chimico-meccanica incorporavano percloroetilene, tricloroetilene e altri solventi clorurati, che da allora sono stati vietati a causa dei loro effetti cancerogeni e dell‚Äôimpatto sullo strato di ozono. Tuttavia, il loro rilascio precedente minaccia ancora le falde acquifere circostanti.\n\nProtocolli di gestione rigorosi, dispositivi di protezione per i lavoratori, ventilazione, sistemi di filtraggio/lavaggio, serbatoi di contenimento secondari e meccanismi di smaltimento specializzati sono essenziali laddove queste sostanze chimiche vengono utilizzate per ridurre al minimo i pericoli per la salute, le esplosioni, l‚Äôaria e le fuoriuscite ambientali (Wald e Jones 1987). Ma occasionalmente si verificano ancora errori umani e guasti alle apparecchiature, evidenziando perch√© la riduzione delle intensit√† chimiche di fabbricazione √® uno sforzo di sostenibilit√† continuo.\n\nWald, Peter H., e Jeffrey R. Jones. 1987. ¬´Semiconductor manufacturing: An introduction to processes and hazards¬ª. Am. J. Ind. Med. 11 (2): 203‚Äì21. https://doi.org/10.1002/ajim.4700110209.\n\n\n16.5.3 Esaurimento delle Risorse\nSebbene il silicio costituisca la base, sulla Terra c‚Äô√® una scorta pressoch√© infinita di silicio. Infatti, il silicio √® il secondo elemento pi√π abbondante trovato nella crosta terrestre, rappresentando il 27,7% della massa totale della crosta. Solo l‚Äôossigeno supera il silicio in abbondanza all‚Äôinterno della crosta. Pertanto, il silicio non √® necessario da considerare per l‚Äôesaurimento delle risorse. Tuttavia, i vari metalli e materiali speciali che consentono il processo di fabbricazione dei circuiti integrati e forniscono propriet√† specifiche devono ancora essere scoperti. Mantenere le scorte di queste risorse √® fondamentale, ma √® minacciato dalla disponibilit√† finita e dalle influenze geopolitiche (Nakano 2021).\n\nNakano, Jane. 2021. The geopolitics of critical minerals supply chains. JSTOR.\n\nChen, H.-W. 2006. ¬´Gallium, Indium, and Arsenic Pollution of Groundwater from a Semiconductor Manufacturing Area of Taiwan¬ª. B. Environ. Contam. Tox. 77 (2): 289‚Äì96. https://doi.org/10.1007/s00128-006-1062-3.\nGallio, indio e arsenico sono ingredienti vitali nella formazione di semiconduttori composti ultra-efficienti nei chip ad altissima velocit√† adatti per applicazioni 5G e AI (Chen 2006). Tuttavia, questi elementi rari hanno depositi naturali relativamente scarsi che si stanno esaurendo. Lo United States Geological Survey ha inserito l‚Äôindio nella sua lista delle materie prime a rischio pi√π critiche, stimando una fornitura globale sostenibile per meno di 15 anni alla crescita attuale della domanda (Davies 2011).\nL‚Äôelio √® richiesto in grandi volumi per le fabbriche di nuova generazione per consentire un raffreddamento preciso dei wafer durante il funzionamento. Ma la relativa rarit√† dell‚Äôelio e il fatto che una volta rilasciato nell‚Äôatmosfera, fuoriesce rapidamente dalla Terra rendono il mantenimento delle scorte di elio estremamente impegnativo a lungo termine (Davies 2011). Secondo le US National Academies, in questo mercato scarsamente scambiato si stanno gi√† verificando notevoli aumenti dei prezzi e shock dell‚Äôofferta.\n\nJha, A. R. 2014. Rare Earth Materials: Properties and Applications. CRC Press. https://doi.org/10.1201/b17045.\nAltri rischi includono il controllo della Cina sul 90% degli elementi delle terre rare fondamentali per la produzione di materiali semiconduttori (Jha 2014). Qualsiasi problema nella catena di fornitura o controversia commerciale pu√≤ portare a catastrofiche carenze di materie prime, data la mancanza di alternative attuali. Insieme alle carenze di elio, risolvere la disponibilit√† limitata e lo squilibrio geografico nell‚Äôaccesso agli ingredienti essenziali rimane una priorit√† del settore per la sostenibilit√†.\n\n\n16.5.4 Generazione di Rifiuti Pericolosi\nLe fabbriche di semiconduttori generano tonnellate di rifiuti pericolosi ogni anno come sottoprodotti dei vari processi chimici (Grossman 2007). I principali flussi di rifiuti includono:\n\nGrossman, Elizabeth. 2007. High tech trash: Digital devices, hidden toxics, and human health. Island press.\n\nRifiuti gassosi: I sistemi di ventilazione delle fab catturano gas nocivi come arsina, fosfina e germano e li filtrano per evitare l‚Äôesposizione dei lavoratori. Tuttavia, ci√≤ produce quantit√† significative di gas condensato pericoloso che necessita di un trattamento specializzato.\nCOV: I composti organici volatili come xilene, acetone e metanolo sono ampiamente utilizzati come solventi fotoresistenti e vengono evaporati come emissioni durante la cottura, l‚Äôincisione e lo stripping. I COV pongono problemi di tossicit√† e richiedono sistemi di lavaggio per impedirne il rilascio.\nAcidi esausti: Acidi forti come acido solforico, acido fluoridrico e acido nitrico si esauriscono nelle fasi di pulizia e incisione, trasformandosi in una zuppa corrosiva e tossica che pu√≤ reagire pericolosamente, rilasciando calore e fumi se mescolata.\nFanghi: Il trattamento delle acque degli effluenti scaricati contiene metalli pesanti concentrati, residui acidi e contaminanti chimici. I sistemi di filtropressa separano questi fanghi pericolosi.\nTorta di filtrazione: I sistemi di filtrazione gassosa generano torte appiccicose di diverse tonnellate di composti assorbiti pericolosi che richiedono contenimento.\n\nSenza adeguate procedure di movimentazione, serbatoi di stoccaggio, materiali di imballaggio e contenimento secondario, lo smaltimento improprio di uno qualsiasi di questi flussi di rifiuti pu√≤ causare pericolose fuoriuscite, esplosioni e rilasci nell‚Äôambiente. Gli enormi volumi significano che anche le fabbriche ben gestite producono tonnellate di rifiuti pericolosi anno dopo anno, che richiedono un trattamento esteso.\n\n\n16.5.5 Impatti sulla Biodiversit√†\n\nInterruzione e Frammentazione dell‚ÄôHabitat\nLe fabbriche di semiconduttori necessitano di ampie aree contigue per ospitare camere bianche, strutture di supporto, stoccaggio di sostanze chimiche, trattamento dei rifiuti e infrastrutture ausiliarie. Lo sviluppo di questi vasti spazi edificati smantella inevitabilmente gli habitat esistenti, danneggiando biomi sensibili che potrebbero aver impiegato decenni per svilupparsi. Ad esempio, la costruzione di un nuovo modulo di fabbricazione potrebbe radere al suolo gli ecosistemi forestali locali da cui specie come gufi maculati e alci dipendono per sopravvivere. La rimozione totale di tali habitat minaccia gravemente le popolazioni di animali selvatici che dipendono da quei terreni.\nInoltre, condutture, canali idrici, sistemi di scarico dell‚Äôaria e dei rifiuti, strade di accesso, torri di trasmissione e altre infrastrutture di supporto frammentano gli habitat indisturbati rimanenti. Gli animali che si spostano quotidianamente per cibo, acqua e deposizione delle uova possono vedere i loro pattern di migrazione bloccati da queste barriere umane fisiche che dividono in due i corridoi precedentemente naturali.\n\n\nDisturbi della Vita Acquatica\nCon le fabbriche di semiconduttori che consumano milioni di galloni di acqua ultra pura ogni giorno, accedere e scaricare tali volumi rischia di alterare l‚Äôidoneit√† degli ambienti acquatici circostanti che ospitano pesci, piante acquatiche, anfibi e altre specie. Se la fabbrica attinge alle falde acquifere come fonte di approvvigionamento primaria, un prelievo eccessivo a tassi insostenibili pu√≤ impoverire i laghi o portare all‚Äôessiccazione dei corsi d‚Äôacqua man mano che i livelli dell‚Äôacqua scendono (Davies 2011).\n\nDavies, Emma. 2011. ¬´Endangered elements: Critical thinking¬ª. https://www.rsc.org/images/Endangered\\%20Elements\\%20-\\%20Critical\\%20Thinking\\_tcm18-196054.pdf.\n\nLeRoy Poff, N, MM Brinson, e JW Day. 2002. ¬´Aquatic ecosystems & Global climate change¬ª. Pew Center on Global Climate Change.\n\nTill, Aaron, Andrew L. Rypel, Andrew Bray, e Samuel B. Fey. 2019. ¬´Fish die-offs are concurrent with thermal extremes in north temperate lakes¬ª. Nat. Clim. Change 9 (8): 637‚Äì41. https://doi.org/10.1038/s41558-019-0520-y.\nInoltre, lo scarico di acque reflue a temperature pi√π elevate per raffreddare le apparecchiature di fabbricazione pu√≤ modificare le condizioni del fiume a valle attraverso l‚Äôinquinamento termico. Le variazioni di temperatura oltre le soglie per cui si sono evolute le specie autoctone possono interrompere i cicli riproduttivi. L‚Äôacqua pi√π calda contiene anche meno ossigeno disciolto, fondamentale per sostenere la vita di piante e animali acquatici (LeRoy Poff, Brinson, e Day 2002). In combinazione con tracce di contaminanti residui che sfuggono ai sistemi di filtrazione, l‚Äôacqua scaricata pu√≤ trasformare cumulativamente gli ambienti rendendoli molto meno abitabili per gli organismi sensibili (Till et al. 2019).\n\n\nEmissioni Chimiche e Aeree\nMentre le moderne fabbriche di semiconduttori mirano a contenere gli scarichi di aria e sostanze chimiche attraverso sistemi di filtraggio estesi, alcuni livelli di emissioni spesso persistono, aumentando i rischi per la flora e la fauna vicine. Gli inquinanti atmosferici possono essere trasportati sottovento, tra cui composti organici volatili (COV), composti di ossido di azoto (NOx), particolato proveniente da scarichi operativi delle fabbriche ed emissioni di carburante delle centrali elettriche.\nPoich√© i contaminanti permeano i terreni e le fonti d‚Äôacqua locali, la fauna selvatica che ingerisce cibo e acqua contaminati ingerisce sostanze tossiche, che la ricerca dimostra possono ostacolare la funzione cellulare, i tassi di riproduzione e la longevit√†, avvelenando lentamente gli ecosistemi (Hsu et al. 2016).\n\nHsu, Liang-Ching, Ching-Yi Huang, Yen-Hsun Chuang, Ho-Wen Chen, Ya-Ting Chan, Heng Yi Teah, Tsan-Yao Chen, Chiung-Fen Chang, Yu-Ting Liu, e Yu-Min Tzou. 2016. ¬´Accumulation of heavy metals and trace elements in fluvial sediments received effluents from traditional and semiconductor industries¬ª. Scientific Reports 6 (1): 34250. https://doi.org/10.1038/srep34250.\nAllo stesso modo, le fuoriuscite accidentali di sostanze chimiche e la gestione impropria dei rifiuti, che rilasciano acidi e metalli pesanti nel terreno, possono influire notevolmente sulla capacit√† di ritenzione e lisciviazione. La flora, come le vulnerabili orchidee autoctone adattate a substrati poveri di nutrienti, pu√≤ subire morie quando viene a contatto con sostanze chimiche di deflusso estranee che alterano il pH e la permeabilit√† del terreno. Un‚Äôanalisi ha scoperto che una singola fuoriuscita di 500 galloni di acido nitrico ha portato all‚Äôestinzione regionale di una rara specie di muschio nell‚Äôanno successivo, quando l‚Äôeffluente acido ha raggiunto gli habitat forestali vicini. Tali eventi di contaminazione innescano reazioni a catena attraverso la rete interconnessa della vita. Pertanto, protocolli rigorosi sono essenziali per evitare scarichi e deflussi pericolosi.",
    "crumbs": [
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>IA Sostenibile</span>"
    ]
  },
  {
    "objectID": "contents/core/sustainable_ai/sustainable_ai.it.html#life-cycle-analysis",
    "href": "contents/core/sustainable_ai/sustainable_ai.it.html#life-cycle-analysis",
    "title": "16¬† IA Sostenibile",
    "section": "16.6 Analisi del Ciclo di Vita",
    "text": "16.6 Analisi del Ciclo di Vita\nPer comprendere l‚Äôimpatto ambientale olistico dei sistemi di intelligenza artificiale √® necessario un approccio completo che consideri l‚Äôintero ciclo di vita di queste tecnologie. Il ‚ÄúLife Cycle Analysis (LCA)‚Äù analisi del ciclo di vita si riferisce a un quadro metodologico utilizzato per quantificare gli impatti ambientali in tutte le fasi del ciclo di vita di un prodotto o sistema, dall‚Äôestrazione delle materie prime allo smaltimento a fine vita. L‚Äôapplicazione dell‚ÄôLCA ai sistemi di intelligenza artificiale pu√≤ aiutare a identificare le aree prioritarie da prendere di mira per ridurre l‚Äôimpatto ambientale complessivo.\n\n\n\n\n\n\nFigura¬†16.9: L‚Äôanalisi del Ciclo di vita del Sistema AI √® suddivisa in quattro fasi chiave: Progettazione, Produzione, Utilizzo, Smaltimento.\n\n\n\n\n16.6.1 Fasi del Ciclo di Vita di un Sistema di IA\nIl ciclo di vita di un sistema di intelligenza artificiale pu√≤ essere suddiviso in quattro fasi chiave:\n\nFase di Progettazione: Include l‚Äôenergia e le risorse utilizzate nella ricerca e nello sviluppo delle tecnologie di intelligenza artificiale. Comprende le risorse computazionali utilizzate per lo sviluppo e il test degli algoritmi che contribuiscono alle emissioni di carbonio.\nFase di Produzione: Questa fase prevede la produzione di componenti hardware come schede grafiche, processori e altri dispositivi di elaborazione necessari per l‚Äôesecuzione degli algoritmi di intelligenza artificiale. La produzione di questi componenti spesso comporta un notevole consumo di energia per l‚Äôestrazione dei materiali, l‚Äôelaborazione e le emissioni di gas serra.\nFase di Utilizzo: La fase successiva pi√π dispendiosa in termini di energia riguarda l‚Äôuso operativo dei sistemi di intelligenza artificiale. Include l‚Äôelettricit√† consumata nei data center per l‚Äôaddestramento e l‚Äôesecuzione delle reti neurali e l‚Äôalimentazione delle applicazioni per gli utenti finali. Questa √® probabilmente una delle fasi con il pi√π alto consumo di carbonio.\nFase di Smaltimento: Questa fase finale riguarda gli aspetti di fine vita dei sistemi di intelligenza artificiale, tra cui il riciclaggio e lo smaltimento dei rifiuti elettronici generati da hardware obsoleto o non funzionante oltre la sua durata utile.\n\n\n\n16.6.2 Impatto Ambientale in Ogni Fase\nProgettazione e Produzione\nL‚Äôimpatto ambientale durante queste fasi iniziali di vita include emissioni derivanti dall‚Äôuso di energia e dall‚Äôesaurimento delle risorse derivanti dall‚Äôestrazione di materiali per la produzione di hardware. Al centro dell‚Äôhardware AI ci sono semiconduttori, principalmente silicio, utilizzati per realizzare i circuiti integrati nei processori e nei chip di memoria. Questa produzione di hardware si basa su metalli come il rame per i cablaggi, l‚Äôalluminio per gli involucri e varie plastiche e compositi per altri componenti. Utilizza anche terre rare e leghe specializzate, elementi come neodimio, terbio e ittrio, utilizzati in piccole ma vitali quantit√†. Ad esempio, la creazione di GPU si basa su rame e alluminio. Allo stesso tempo, i chip utilizzano terre rare, che √® il processo di estrazione che pu√≤ generare notevoli emissioni di carbonio e danni all‚Äôecosistema.\nFase di Utilizzo\nL‚ÄôAI calcola la maggior parte delle emissioni nel ciclo di vita a causa del continuo elevato consumo di energia, in particolare per il training e l‚Äôesecuzione di modelli. Ci√≤ include emissioni dirette e indirette derivanti dall‚Äôuso di elettricit√† e dalla generazione di energia di rete non rinnovabile. Gli studi stimano che l‚Äôaddestramento di modelli complessi pu√≤ avere un‚Äôimpronta di carbonio paragonabile alle emissioni fino a cinque auto nel corso della loro vita.\nFase di Smaltimento\nGli impatti della fase di smaltimento includono l‚Äôinquinamento dell‚Äôaria e dell‚Äôacqua dovuto a materiali tossici nei dispositivi, le sfide associate al riciclaggio di componenti elettronici complessi e la contaminazione in caso di gestione impropria. I composti nocivi derivanti dalla combustione dei rifiuti elettronici vengono rilasciati nell‚Äôatmosfera. Allo stesso tempo, la perdita di piombo, mercurio e altri materiali dalle discariche comporta rischi di contaminazione del suolo e delle falde acquifere se non adeguatamente controllata. L‚Äôimplementazione di un efficace riciclaggio dei componenti elettronici √® fondamentale.\n\n\n\n\n\n\nEsercizio¬†16.2: Monitoraggio delle Emissioni ML\n\n\n\n\n\nIn questo esercizio, esploreremo l‚Äôimpatto ambientale dell‚Äôaddestramento di modelli di machine learning. Utilizzeremo CodeCarbon per monitorare le emissioni, apprenderemo l‚Äôanalisi del ‚ÄúLife Cycle Analysis (LCA)‚Äù per comprendere l‚Äôimpronta di carbonio dell‚ÄôIA ed esploreremo strategie per rendere lo sviluppo del modello ML pi√π rispettoso dell‚Äôambiente. Alla fine, si sar√† in grado di monitorare le emissioni di carbonio dei modelli e iniziare a implementare pratiche pi√π ecologiche nei progetti.",
    "crumbs": [
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>IA Sostenibile</span>"
    ]
  },
  {
    "objectID": "contents/core/sustainable_ai/sustainable_ai.it.html#challenges-in-lca",
    "href": "contents/core/sustainable_ai/sustainable_ai.it.html#challenges-in-lca",
    "title": "16¬† IA Sostenibile",
    "section": "16.7 Sfide nell‚ÄôLCA",
    "text": "16.7 Sfide nell‚ÄôLCA\n\n16.7.1 Mancanza di Coerenza e Standard\nUna delle principali sfide che l‚Äôanalisi del ‚Äúlife cycle analysis (LCA)‚Äù [ciclo di vita ] deve affrontare per i sistemi di intelligenza artificiale √® la necessit√† di standard e framework metodologici coerenti. A differenza di categorie di prodotti come i materiali da costruzione, che hanno sviluppato standard internazionali per LCA tramite ISO 14040, non esistono linee guida stabilite per analizzare l‚Äôimpatto ambientale di tecnologie informatiche complesse come l‚Äôintelligenza artificiale.\nQuesta assenza di uniformit√† significa che i ricercatori fanno ipotesi diverse e scelte metodologiche variabili. Ad esempio, uno studio del 2021 dell‚ÄôUniversit√† del Massachusetts Amherst (Strubell, Ganesh, e McCallum 2019) ha analizzato le emissioni del ciclo di vita di diversi modelli di elaborazione del linguaggio naturale, ma ha preso in considerazione solo l‚Äôutilizzo delle risorse computazionali per il training e ha omesso gli impatti sulla produzione di hardware. Uno studio pi√π completo del 2020 condotto dai ricercatori della Stanford University ha incluso stime delle emissioni derivanti dalla produzione di server, processori e altri componenti pertinenti, seguendo uno standard LCA allineato a ISO per l‚Äôhardware dei computer. Tuttavia, queste scelte divergenti nei confini del sistema e negli approcci contabili riducono la robustezza e impediscono confronti tra risultati simili.\nFramework e protocolli standardizzati su misura per gli aspetti unici dei sistemi di intelligenza artificiale e rapidi cicli di aggiornamento fornirebbero maggiore coerenza. Ci√≤ potrebbe consentire a ricercatori e sviluppatori di comprendere i punti critici ambientali, confrontare le opzioni tecnologiche e monitorare con precisione i progressi nelle iniziative di sostenibilit√† nel campo dell‚Äôintelligenza artificiale. Gruppi industriali e organismi di normazione internazionali come IEEE o ACM dovrebbero dare priorit√† all‚Äôaffrontare questa lacuna metodologica.\n\n\n16.7.2 Lacune nei Dati\nUn‚Äôaltra sfida fondamentale per una valutazione completa del ciclo di vita dei sistemi di intelligenza artificiale sono le lacune sostanziali nei dati, in particolare per quanto riguarda gli impatti sulla catena di fornitura a monte e i flussi di rifiuti elettronici a valle. La maggior parte degli studi esistenti si concentra strettamente sulle emissioni della fase di apprendimento o di utilizzo derivanti dalle richieste di potenza di calcolo, che tralasciano una parte significativa delle emissioni nel corso della vita (Gupta et al. 2022).\nAd esempio, esistono pochi dati pubblici delle aziende che quantificano l‚Äôuso di energia e le emissioni derivanti dalla produzione di componenti hardware specializzati che abilitano l‚Äôintelligenza artificiale, tra cui GPU di fascia alta, chip ASIC, unit√† a stato solido e altro. Spesso i ricercatori si affidano a fonti secondarie o medie generiche del settore per approssimare gli impatti sulla produzione. Analogamente, in media, c‚Äô√® una trasparenza limitata sul destino a valle una volta che i sistemi di intelligenza artificiale vengono scartati dopo 4-5 anni di durata utile.\nMentre i livelli di generazione di rifiuti elettronici possono essere stimati, i dettagli sulle perdite di materiali pericolosi, sui tassi di riciclaggio e sui metodi di smaltimento per i componenti complessi sono estremamente incerti senza una migliore documentazione aziendale o requisiti di reporting normativi.\nLa necessit√† di dati dettagliati sul consumo di risorse computazionali per l‚Äôaddestramento di diversi tipi di modelli rende difficili calcoli affidabili delle emissioni per parametro o per query anche per la fase di utilizzo. Esistono tentativi di creare inventari del ciclo di vita che stimino il fabbisogno energetico medio per le attivit√† chiave dell‚Äôintelligenza artificiale (Henderson et al. 2020; Anthony, Kanding, e Selvan 2020), ma la variabilit√† tra configurazioni hardware, algoritmi e incertezza dei dati di input rimane estremamente elevata. Inoltre, i dati sull‚Äôintensit√† di carbonio in tempo reale, fondamentali per tracciare con precisione l‚Äôimpronta di carbonio operativa, devono essere migliorati in molte posizioni geografiche, rendendo gli strumenti esistenti per le emissioni di carbonio operative mere approssimazioni basate sui valori di intensit√† di carbonio media annuale.\n\nHenderson, Peter, Jieru Hu, Joshua Romoff, Emma Brunskill, Dan Jurafsky, e Joelle Pineau. 2020. ¬´Towards the systematic reporting of the energy and carbon footprints of machine learning¬ª. The Journal of Machine Learning Research 21 (1): 10039‚Äì81.\n\nAnthony, Lasse F. Wolff, Benjamin Kanding, e Raghavendra Selvan. 2020. ICML Workshop on Challenges in Deploying and monitoring Machine Learning Systems.\nLa sfida √® che strumenti come CodeCarbon e ML \\(\\textrm{CO}_2\\) sono, nella migliore delle ipotesi, solo approcci ad hoc, nonostante le loro buone intenzioni. Colmare le lacune reali dei dati con divulgazioni pi√π rigorose sulla sostenibilit√† aziendale e una rendicontazione obbligatoria dell‚Äôimpatto ambientale sar√† fondamentale per comprendere e gestire gli impatti climatici complessivi dell‚ÄôIA.\n\n\n16.7.3 Rapido Ritmo di Evoluzione\nL‚Äôevoluzione estremamente rapida dei sistemi di intelligenza artificiale pone ulteriori sfide nel mantenere aggiornate le valutazioni del ciclo di vita e nel tenere conto degli ultimi progressi hardware e software. Gli algoritmi di base, i chip specializzati, i framework e l‚Äôinfrastruttura tecnica alla base dell‚Äôintelligenza artificiale hanno tutti fatto progressi eccezionalmente rapidi, con nuovi sviluppi che hanno rapidamente reso obsoleti i sistemi precedenti.\nAd esempio, nel deep learning, le nuove architetture di reti neurali che raggiungono prestazioni significativamente migliori su benchmark chiave o nuovi hardware ottimizzati come i chip TPU di Google possono cambiare completamente un modello ‚Äúmedio‚Äù in meno di un anno. Questi rapidi cambiamenti rendono rapidamente obsoleti gli studi LCA una tantum per il monitoraggio accurato delle emissioni derivanti dalla progettazione, esecuzione o smaltimento dell‚Äôintelligenza artificiale pi√π recente.\nTuttavia, le risorse e l‚Äôaccesso necessari per aggiornare continuamente gli LCA devono essere migliorati. Rifare frequentemente il lavoro, inventari del ciclo di vita ad alta intensit√† di dati e modelli di impatto per rimanere aggiornati con lo stato dell‚Äôarte dell‚Äôintelligenza artificiale √® probabilmente irrealizzabile per molti ricercatori e organizzazioni. Tuttavia, analisi aggiornate potrebbero rilevare hotspot ambientali man mano che algoritmi e chip di silicio continuano a evolversi rapidamente.\nCi√≤ presenta difficolt√† nel bilanciare la precisione dinamica attraverso una valutazione continua con vincoli pragmatici. Alcuni ricercatori hanno proposto metriche proxy semplificate come il monitoraggio delle generazioni hardware nel tempo o l‚Äôutilizzo di benchmark rappresentativi come un set oscillante di paletti per confronti relativi, sebbene la granularit√† possa essere sacrificata. Nel complesso, la sfida del cambiamento rapido richieder√† soluzioni metodologiche innovative per evitare di sottostimare gli oneri ambientali in evoluzione dell‚ÄôIA.\n\n\n16.7.4 Complessit√† della Catena di Fornitura\nInfine, le complesse e spesso opache catene di fornitura associate alla produzione dell‚Äôampia gamma di componenti hardware specializzati che abilitano l‚Äôintelligenza artificiale pongono sfide per la modellazione completa del ciclo di vita. Lo ‚Äústato-dell‚Äôarte‚Äù dellIA si basa su progressi all‚Äôavanguardia nell‚Äôelaborazione di chip, schede grafiche, archiviazione dati, apparecchiature di rete e altro ancora. Tuttavia, tracciare le emissioni e l‚Äôuso delle risorse attraverso le reti a livelli di fornitori globalizzati per tutti questi componenti √® estremamente difficile.\nAd esempio, le unit√† di elaborazione grafica NVIDIA dominano gran parte dell‚Äôhardware di elaborazione dell‚Äôintelligenza artificiale, ma l‚Äôazienda si affida a diversi fornitori discreti in Asia e oltre per produrre GPU. Molte aziende a ogni livello di fornitore scelgono di mantenere privati i dati ambientali a livello di stabilimento, il che potrebbe abilitare completamente LCA robuste. Ottenere la trasparenza end-to-end su pi√π livelli di fornitori in aree geografiche diverse con protocolli di divulgazione e normative variabili pone barriere nonostante sia fondamentale per la definizione completa dei confini. Ci√≤ diventa ancora pi√π complesso quando si tenta di modellare acceleratori hardware emergenti come le ‚Äútensor processing units (TPU)‚Äù [unit√† di elaborazione tensoriale], le cui reti di produzione devono ancora essere rese pubbliche.\nSenza la volont√† dei giganti della tecnologia di richiedere e consolidare la divulgazione dei dati sull‚Äôimpatto ambientale da tutte le loro catene di fornitura di elettronica globali, rimarr√† una notevole incertezza sulla quantificazione dell‚Äôimpronta del ciclo di vita completo dell‚Äôabilitazione hardware AI. Una maggiore visibilit√† della catena di fornitura abbinata a quadri di reporting sulla sostenibilit√† standardizzati che affrontino specificamente gli input complessi dell‚ÄôAI promettono di arricchire gli LCA e dare priorit√† alle riduzioni dell‚Äôimpatto ambientale.",
    "crumbs": [
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>IA Sostenibile</span>"
    ]
  },
  {
    "objectID": "contents/core/sustainable_ai/sustainable_ai.it.html#sustainable-design-and-development",
    "href": "contents/core/sustainable_ai/sustainable_ai.it.html#sustainable-design-and-development",
    "title": "16¬† IA Sostenibile",
    "section": "16.8 Progettazione e Sviluppo Sostenibili",
    "text": "16.8 Progettazione e Sviluppo Sostenibili\n\n16.8.1 Principi di Sostenibilit√†\nMan mano che l‚Äôimpatto dell‚ÄôIA sull‚Äôambiente diventa sempre pi√π evidente, l‚Äôattenzione alla progettazione e allo sviluppo sostenibili nell‚ÄôIA sta acquisendo importanza. Ci√≤ comporta l‚Äôincorporazione di principi di sostenibilit√† nella progettazione dell‚ÄôIA, lo sviluppo di modelli a risparmio energetico e l‚Äôintegrazione di queste considerazioni in tutta la pipeline di sviluppo dell‚ÄôIA. C‚Äô√® una crescente necessit√† di considerare le implicazioni di sostenibilit√† e sviluppare principi per guidare l‚Äôinnovazione responsabile. Di seguito √® riportato un set di principi fondamentali. I principi fluiscono dalle fondamenta concettuali all‚Äôesecuzione pratica ai fattori di supporto all‚Äôimplementazione; i principi forniscono una prospettiva del ciclo completo sull‚Äôincorporamento della sostenibilit√† nella progettazione e nello sviluppo dell‚ÄôIA.\nLifecycle Thinking: Incoraggiare i progettisti a considerare l‚Äôintero ciclo di vita dei sistemi di IA, dalla raccolta e preelaborazione dei dati allo sviluppo del modello, al training, all‚Äôimplementazione e al monitoraggio. L‚Äôobiettivo √® garantire che la sostenibilit√† sia presa in considerazione in ogni fase. Ci√≤ include l‚Äôutilizzo di hardware a risparmio energetico, la priorit√† alle fonti di energia rinnovabili e la pianificazione del riutilizzo o del riciclaggio di modelli dismessi.\nA Prova di Futuro: Progettare sistemi di intelligenza artificiale che anticipino esigenze e cambiamenti futuri pu√≤ migliorare la sostenibilit√†. Ci√≤ pu√≤ comportare la creazione di modelli adattabili tramite apprendimento per trasferimento e architetture modulari. Include anche la capacit√† di pianificazione per aumenti previsti di scala operativa e volumi di dati.\nEfficienza e Minimalismo: Questo principio si concentra sulla creazione di modelli di intelligenza artificiale che raggiungano i risultati desiderati con il minimo utilizzo di risorse possibile. Comporta la semplificazione di modelli e algoritmi per ridurre i requisiti computazionali. Tecniche specifiche includono la potatura di parametri ridondanti, la quantizzazione e la compressione di modelli e la progettazione di architetture di modelli efficienti, come quelle discusse nel capitolo Ottimizzazioni.\nIntegrazione del Lifecycle Assessment (LCA): [Valutazione del Ciclo di Vita ] L‚Äôanalisi degli impatti ambientali durante lo sviluppo e l‚Äôimplementazione dei cicli di vita evidenzia le pratiche non sostenibili in anticipo. I team possono quindi apportare modifiche anzich√© scoprire i problemi in ritardo, quando sono pi√π difficili da affrontare. L‚Äôintegrazione di questa analisi nel flusso di progettazione standard evita di creare problemi ereditati di sostenibilit√†.\nAllineamento degli Incentivi: Gli incentivi economici e politici dovrebbero promuovere e premiare lo sviluppo sostenibile dell‚ÄôIA. Questi possono includere sovvenzioni governative, iniziative aziendali, standard di settore e mandati accademici per la sostenibilit√†. Gli incentivi allineati consentono alla sostenibilit√† di essere inglobata nella cultura dell‚ÄôIA.\nMetriche e Obiettivi di Sostenibilit√†: √à importante stabilire metriche chiaramente definite che misurino fattori di sostenibilit√† come l‚Äôuso del carbonio e l‚Äôefficienza energetica. Stabilire obiettivi chiari per queste metriche fornisce linee guida concrete per i team per sviluppare sistemi di IA responsabili. Il monitoraggio delle prestazioni sulle metriche nel tempo mostra i progressi verso gli obiettivi di sostenibilit√† prefissati.\nEquit√†, Trasparenza e Responsabilit√†: I sistemi di IA sostenibili dovrebbero essere equi, trasparenti e responsabili. I modelli dovrebbero essere imparziali, con processi di sviluppo trasparenti e meccanismi per l‚Äôaudit e la risoluzione dei problemi. Ci√≤ crea fiducia nel pubblico e consente l‚Äôidentificazione di pratiche non sostenibili.\nCollaborazione Interdisciplinare: I ricercatori di intelligenza artificiale che collaborano con scienziati e ingegneri ambientali possono dare vita a sistemi innovativi ad alte prestazioni ma rispettosi dell‚Äôambiente. L‚Äôunione di competenze provenienti da diversi campi fin dall‚Äôinizio dei progetti consente di incorporare il pensiero sostenibile nel processo di progettazione dell‚Äôintelligenza artificiale.\nIstruzione e Consapevolezza: Workshop, programmi di formazione e programmi di studio che riguardano la sostenibilit√† dell‚Äôintelligenza artificiale accrescono la consapevolezza tra la prossima generazione di professionisti. Ci√≤ fornisce agli studenti le conoscenze per sviluppare un‚Äôintelligenza artificiale che riduca al minimo gli impatti negativi sulla societ√† e sull‚Äôambiente. Inculcare questi valori fin dall‚Äôinizio plasma i professionisti e le culture aziendali di domani.",
    "crumbs": [
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>IA Sostenibile</span>"
    ]
  },
  {
    "objectID": "contents/core/sustainable_ai/sustainable_ai.it.html#green-ai-infrastructure",
    "href": "contents/core/sustainable_ai/sustainable_ai.it.html#green-ai-infrastructure",
    "title": "16¬† IA Sostenibile",
    "section": "16.9 Infrastruttura di IA Green",
    "text": "16.9 Infrastruttura di IA Green\nGreen AI rappresenta un approccio trasformativo all‚ÄôIA che incorpora la sostenibilit√† ambientale come principio fondamentale nella progettazione e nel ciclo di vita del sistema di IA (R. Schwartz et al. 2020). Questo cambiamento √® guidato dalla crescente consapevolezza dell‚Äôimpatto ecologico e dell‚Äôimpronta di carbonio significativa delle tecnologie di IA, in particolare il processo di elaborazione intensiva di modelli di ML complessi.\n\nSchwartz, Roy, Jesse Dodge, Noah A. Smith, e Oren Etzioni. 2020. ¬´Green AI¬ª. Commun. ACM 63 (12): 54‚Äì63. https://doi.org/10.1145/3381831.\nL‚Äôessenza di Green AI risiede nel suo impegno ad allineare il progresso dell‚ÄôIA con gli obiettivi di sostenibilit√† in termini di efficienza energetica, utilizzo di energia rinnovabile e riduzione dei rifiuti. L‚Äôintroduzione degli ideali di Green AI riflette la crescente responsabilit√† nel settore tecnologico verso la tutela ambientale e le pratiche tecnologiche etiche. Va oltre le ottimizzazioni tecniche verso una valutazione olistica del ciclo di vita su come i sistemi di IA influenzano le metriche di sostenibilit√†. Stabilire nuovi standard per un‚ÄôIA ecologicamente consapevole apre la strada alla coesistenza armoniosa di progresso tecnologico e salute planetaria.\n\n16.9.1 Sistemi di IA a Risparmio Energetico\nL‚Äôefficienza energetica nei sistemi di intelligenza artificiale √® un pilastro della Green AI, che mira a ridurre le richieste di energia tradizionalmente associate allo sviluppo e alle operazioni di intelligenza artificiale. Questo passaggio verso pratiche di intelligenza artificiale attente al risparmio energetico √® fondamentale per affrontare le preoccupazioni ambientali sollevate dal campo in rapida espansione dell‚Äôintelligenza artificiale. Concentrandosi sull‚Äôefficienza energetica, i sistemi di intelligenza artificiale possono diventare pi√π sostenibili, riducendo il loro impatto ambientale e aprendo la strada a un loro utilizzo pi√π responsabile.\nCome abbiamo discusso in precedenza, l‚Äôaddestramento e il funzionamento dei modelli di intelligenza artificiale, in particolare quelli su larga scala, sono noti per il loro elevato consumo energetico, che deriva dall‚Äôarchitettura del modello ad alta intensit√† di calcolo e dall‚Äôaffidamento a grandi quantit√† di dati di addestramento. Ad esempio, si stima che l‚Äôaddestramento di un grande modello di rete neurale all‚Äôavanguardia possa avere un‚Äôimpronta di carbonio di 284 tonnellate, equivalente alle emissioni di 5 auto nel corso della loro vita (Strubell, Ganesh, e McCallum 2019).\n\nStrubell, Emma, Ananya Ganesh, e Andrew McCallum. 2019. ¬´Energy and Policy Considerations for Deep Learning in NLP¬ª. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, 3645‚Äì50. Florence, Italy: Association for Computational Linguistics. https://doi.org/10.18653/v1/p19-1355.\nPer affrontare le enormi richieste di energia, ricercatori e sviluppatori stanno esplorando attivamente metodi per ottimizzare i sistemi di intelligenza artificiale per una migliore efficienza energetica mantenendo al contempo l‚Äôaccuratezza e le prestazioni del modello. Ci√≤ include tecniche come quelle che abbiamo discusso nei capitoli sulle ottimizzazioni del modello, sull‚Äôintelligenza artificiale efficiente e sull‚Äôaccelerazione hardware:\n\nDistillazione della conoscenza per trasferire la conoscenza da grandi modelli di intelligenza artificiale a versioni in miniatura\nApprocci di quantizzazione e potatura che riducono le complessit√† computazionali e spaziali\nNumeri a bassa precisione: riduzione della precisione matematica senza influire sulla qualit√† del modello\nHardware specializzato come TPU, chip neuromorfici ottimizzati esplicitamente per un‚Äôelaborazione efficiente dell‚Äôintelligenza artificiale\n\nUn esempio √® il lavoro di Intel su Q8BERT: quantizzazione del modello di linguaggio BERT con interi a 8 bit, che porta a una riduzione di 4 volte delle dimensioni del modello con una perdita di accuratezza minima (Zafrir et al. 2019). La spinta verso un‚Äôintelligenza artificiale efficiente dal punto di vista energetico non √® solo uno sforzo tecnico: ha implicazioni tangibili nel mondo reale. Sistemi pi√π performanti riducono i costi operativi e l‚Äôimpatto ambientale dell‚Äôintelligenza artificiale, rendendola accessibile per un‚Äôampia distribuzione su dispositivi mobili ed edge. Apre inoltre la strada alla democratizzazione dell‚ÄôIA e mitiga i pregiudizi ingiusti che possono emergere da un accesso non uniforme alle risorse informatiche tra regioni e comunit√†. Perseguire un‚ÄôIA efficiente dal punto di vista energetico √® quindi fondamentale per creare un futuro equo e sostenibile con l‚ÄôIA.\n\nZafrir, Ofir, Guy Boudoukh, Peter Izsak, e Moshe Wasserblat. 2019. ¬´Q8BERT: Quantized 8Bit BERT¬ª. In 2019 Fifth Workshop on Energy Efficient Machine Learning and Cognitive Computing - NeurIPS Edition (EMC2-NIPS), 36‚Äì39. IEEE; IEEE. https://doi.org/10.1109/emc2-nips53020.2019.00016.\n\n\n16.9.2 Infrastruttura di IA Sostenibile\nL‚Äôinfrastruttura AI sostenibile include i framework fisici e tecnologici che supportano i sistemi AI, concentrandosi sulla sostenibilit√† ambientale. Ci√≤ implica la progettazione e la gestione dell‚Äôinfrastruttura AI per ridurre al minimo l‚Äôimpatto ecologico, conservare le risorse e ridurre le emissioni di carbonio. L‚Äôobiettivo √® creare un ecosistema sostenibile per l‚ÄôAI che si allinei con obiettivi ambientali pi√π ampi.\nI data center green sono fondamentali per l‚Äôinfrastruttura AI sostenibile, ottimizzati per l‚Äôefficienza energetica e spesso alimentati da fonti di energia rinnovabili. Questi data center impiegano tecnologie di raffreddamento avanzate (Ebrahimi, Jones, e Fleischer 2014), design di server a risparmio energetico (Uddin e Rahman 2012) e sistemi di gestione intelligenti (Buyya, Beloglazov, e Abawajy 2010) per ridurre il consumo di energia. Il passaggio a un‚Äôinfrastruttura informatica ecologica implica anche l‚Äôadozione di hardware a basso consumo energetico, come processori ottimizzati per l‚ÄôIA che offrono prestazioni elevate con requisiti energetici ridotti, di cui abbiamo parlato nel capitolo Accelerazione dell‚ÄôIA. Questi sforzi riducono collettivamente l‚Äôimpronta di carbonio delle operazioni di intelligenza artificiale su larga scala.\n\nEbrahimi, Khosrow, Gerard F. Jones, e Amy S. Fleischer. 2014. ¬´A review of data center cooling technology, operating conditions and the corresponding low-grade waste heat recovery opportunities¬ª. Renewable Sustainable Energy Rev. 31 (marzo): 622‚Äì38. https://doi.org/10.1016/j.rser.2013.12.007.\n\nUddin, Mueen, e Azizah Abdul Rahman. 2012. ¬´Energy efficiency and low carbon enabler green IT framework for data centers considering green metrics¬ª. Renewable Sustainable Energy Rev. 16 (6): 4078‚Äì94. https://doi.org/10.1016/j.rser.2012.03.014.\n\nBuyya, Rajkumar, Anton Beloglazov, e Jemal Abawajy. 2010. ¬´Energy-Efficient Management of Data Center Resources for Cloud Computing: A Vision, Architectural Elements, and Open Challenges¬ª. https://arxiv.org/abs/1006.0308.\n\nChua, L. 1971. ¬´Memristor-The missing circuit element¬ª. #IEEE_J_CT# 18 (5): 507‚Äì19. https://doi.org/10.1109/tct.1971.1083337.\nL‚Äôintegrazione di fonti di energia rinnovabili, come energia solare, eolica e idroelettrica, nell‚Äôinfrastruttura di intelligenza artificiale √® importante per la sostenibilit√† ambientale (Chua 1971). Molte aziende tecnologiche e istituti di ricerca stanno investendo in progetti di energia rinnovabile per alimentare i loro data center. Ci√≤ non solo aiuta a rendere le operazioni di intelligenza artificiale carbon neutral, ma promuove anche un‚Äôadozione pi√π ampia di energia pulita. L‚Äôutilizzo di fonti di energia rinnovabili mostra chiaramente l‚Äôimpegno per la responsabilit√† ambientale nel settore dell‚Äôintelligenza artificiale.\nLa sostenibilit√† si estende anche ai materiali e all‚Äôhardware utilizzati nella creazione di sistemi di intelligenza artificiale. Ci√≤ implica la scelta di materiali ecocompatibili, l‚Äôadozione di pratiche di riciclaggio e la garanzia di uno smaltimento responsabile dei rifiuti elettronici. Sono in corso sforzi per sviluppare componenti hardware pi√π sostenibili, tra cui chip a risparmio energetico progettati per attivit√† specifiche del dominio (come gli acceleratori di IA) e materiali ecocompatibili nella produzione di dispositivi (Cenci et al. 2021; Irimia-Vladu 2014). Anche il ciclo di vita di questi componenti √® un punto focale, con iniziative volte a estendere la durata di vita dell‚Äôhardware e a promuovere il riciclaggio e il riutilizzo.\n\nCenci, Marcelo Pilotto, Tatiana Scarazzato, Daniel Dotto Munchen, Paula Cristina Dartora, Hugo Marcelo Veit, Andrea Moura Bernardes, e Pablo R. Dias. 2021. ¬´Eco-Friendly ElectronicsA Comprehensive Review¬ª. Adv. Mater. Technol. 7 (2): 2001263. https://doi.org/10.1002/admt.202001263.\n\nIrimia-Vladu, Mihai. 2014. ¬´‚ÄúGreen‚Äù electronics: Biodegradable and biocompatible materials and devices for sustainable future¬ª. Chem. Soc. Rev. 43 (2): 588‚Äì610. https://doi.org/10.1039/c3cs60235d.\nSebbene si stiano facendo progressi nell‚Äôinfrastruttura di IA sostenibile, permangono delle sfide, come gli elevati costi della tecnologia verde e la necessit√† di standard globali nelle pratiche sostenibili. Le direzioni future includono un‚Äôadozione pi√π diffusa di energia verde, ulteriori innovazioni nell‚Äôhardware a risparmio energetico e la collaborazione internazionale su politiche di IA sostenibile. Perseguire un‚Äôinfrastruttura di IA sostenibile non √® solo uno sforzo tecnico, ma un approccio olistico che comprende aspetti ambientali, economici e sociali, assicurando che l‚ÄôIA avanzi in armonia con la salute del nostro pianeta.\n\n\n16.9.3 Framework e Strumenti\nL‚Äôaccesso ai framework e agli strumenti giusti √® essenziale per implementare in modo efficace le pratiche di intelligenza artificiale verde. Queste risorse sono progettate per aiutare sviluppatori e ricercatori a creare sistemi di IA pi√π efficienti dal punto di vista energetico e rispettosi dell‚Äôambiente. Vanno da librerie software ottimizzate per un basso consumo energetico a piattaforme che facilitano lo sviluppo di applicazioni di IA sostenibili.\nDiverse librerie software e ambienti di sviluppo sono specificamente pensati per l‚Äôintelligenza artificiale verde. Questi strumenti spesso includono funzionalit√† per ottimizzare i modelli di IA per ridurre il loro carico computazionale e, di conseguenza, il loro consumo energetico. Ad esempio, le librerie in PyTorch e TensorFlow che supportano la potatura del modello, la quantizzazione e le architetture di reti neurali efficienti consentono agli sviluppatori di creare sistemi di intelligenza artificiale che richiedono meno potenza di elaborazione ed energia. Inoltre, comunit√† open source come la Green Software Foundation stanno creando una metrica centralizzata dell‚Äôintensit√† di carbonio e sviluppando software per un‚Äôinformatica attenta alle emissioni di carbonio.\nGli strumenti di monitoraggio dell‚Äôenergia sono fondamentali per l‚Äôintelligenza artificiale verde, poich√© consentono agli sviluppatori di misurare e analizzare il consumo energetico dei loro sistemi. Figura¬†16.10 √® uno screenshot di una dashboard del consumo energetico fornita dalla piattaforma di servizi cloud di Microsoft. Fornendo informazioni dettagliate su dove e come viene utilizzata l‚Äôenergia, questi strumenti consentono agli sviluppatori di prendere decisioni informate sull‚Äôottimizzazione dei loro modelli per una migliore efficienza energetica. Ci√≤ pu√≤ comportare modifiche nella progettazione dell‚Äôalgoritmo, nella selezione dell‚Äôhardware, nella selezione del software di cloud computing o nei parametri operativi.\n\n\n\n\n\n\nFigura¬†16.10: Dashboard del consumo energetico di Microsoft Azure. Fonte: Will Buchanan.\n\n\n\nCon la crescente integrazione di fonti di energia rinnovabile nelle operazioni di IA, i framework che facilitano questo processo stanno diventando sempre pi√π importanti. Questi framework aiutano a gestire l‚Äôapprovvigionamento energetico da fonti rinnovabili come l‚Äôenergia solare o eolica, assicurando che i sistemi di IA possano funzionare in modo efficiente con input energetici fluttuanti.\nOltre all‚Äôefficienza energetica, gli strumenti di valutazione della sostenibilit√† aiutano a valutare l‚Äôimpatto ambientale pi√π ampio dei sistemi di IA. Questi strumenti possono analizzare fattori come l‚Äôimpronta di carbonio delle operazioni di IA, l‚Äôimpatto del ciclo di vita dei componenti hardware (Gupta et al. 2022) e la sostenibilit√† complessiva dei progetti di IA (Prakash, Callahan, et al. 2023).\n\nGupta, Udit, Mariam Elgamal, Gage Hills, Gu-Yeon Wei, Hsien-Hsin S. Lee, David Brooks, e Carole-Jean Wu. 2022. ¬´Act: designing sustainable computer systems with an architectural carbon modeling tool¬ª. In Proceedings of the 49th Annual International Symposium on Computer Architecture, 784‚Äì99. ACM. https://doi.org/10.1145/3470496.3527408.\n\nPrakash, Shvetank, Tim Callahan, Joseph Bushagour, Colby Banbury, Alan V. Green, Pete Warden, Tim Ansell, e Vijay Janapa Reddi. 2023. ¬´CFU Playground: Full-stack Open-Source Framework for Tiny Machine Learning (TinyML) Acceleration on FPGAs¬ª. In 2023 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS). Vol. abs/2201.01863. IEEE. https://doi.org/10.1109/ispass57527.2023.00024.\nLa disponibilit√† e lo sviluppo continuo di framework e strumenti di IA Green sono fondamentali per promuovere pratiche di sostenibili. Fornendo le risorse necessarie a sviluppatori e ricercatori, questi strumenti facilitano la creazione di sistemi pi√π rispettosi dell‚Äôambiente e incoraggiano un pi√π ampio cambiamento verso la sostenibilit√† nella comunit√† tecnologica. Man mano che l‚ÄôIA Green continua a evolversi, questi framework e strumenti svolgeranno un ruolo fondamentale nel dare forma a un futuro pi√π sostenibile per l‚ÄôIA. As Green AI continues to evolve, these frameworks and tools will play a vital role in shaping a more sustainable future for AI.\n\n\n16.9.4 Benchmark e Classifiche\nBenchmark e classifiche sono importanti per guidare i progressi nell‚ÄôIA Green, poich√© forniscono modi standardizzati per misurare e confrontare diversi metodi. Benchmark ben progettati che catturano metriche rilevanti su efficienza energetica, emissioni di carbonio e altri fattori di sostenibilit√† consentono alla comunit√† di monitorare i progressi in modo equo e significativo.\nEsistono ampi benchmark per tracciare le prestazioni del modello di IA, come quelli discussi nel capitolo Benchmarking. Tuttavia, esiste una chiara e urgente necessit√† di ulteriori benchmark standardizzati incentrati su parametri di sostenibilit√† come efficienza energetica, emissioni di carbonio e impatto ecologico complessivo. La comprensione dei costi ambientali dell‚ÄôIA deve attualmente essere migliorata da una mancanza di trasparenza e misura standardizzata attorno a questi fattori.\nSforzi emergenti come ML.ENERGY Leaderboard, che fornisce risultati di benchmarking delle prestazioni e del consumo energetico per la generazione di testo di modelli linguistici di grandi dimensioni (LLM), aiutano a migliorare la comprensione del costo energetico dell‚Äôimplementazione GenAI.\nCome con qualsiasi benchmark, quelli di IA Green devono rappresentare scenari di utilizzo e carichi di lavoro realistici. I benchmark che si concentrano strettamente su metriche facilmente manipolabili possono portare a guadagni a breve termine, ma non riescono a riflettere gli ambienti di produzione effettivi in cui sono necessarie misure di efficienza e sostenibilit√† pi√π olistiche. La comunit√† dovrebbe continuare ad espandere i benchmark per coprire diversi casi d‚Äôuso.\nUn‚Äôadozione pi√π ampia di suite di benchmark comuni da parte degli operatori del settore accelerer√† l‚Äôinnovazione nell‚ÄôIA Green consentendo un confronto pi√π semplice delle tecniche tra le organizzazioni. I benchmark condivisi abbassano la barriera per dimostrare i vantaggi di sostenibilit√† di nuovi strumenti e best practice. uttavia, quando si progettano benchmark per l‚Äôintero settore, √® necessario prestare attenzione a questioni come propriet√† intellettuale, privacy e sensibilit√† commerciale. Le iniziative per sviluppare set di dati di riferimento aperti per la valutazione dell‚ÄôIA Green possono aiutare a promuovere una partecipazione pi√π ampia.\nMan mano che i metodi e l‚Äôinfrastruttura per l‚ÄôIA Green continuano a maturare, la comunit√† deve rivedere la progettazione dei benchmark per garantire che le suite esistenti catturino bene nuove tecniche e scenari. Monitorare il panorama in evoluzione attraverso aggiornamenti e revisioni regolari dei benchmark sar√† importante per mantenere confronti rappresentativi nel tempo. Gli sforzi della comunit√† per la cura dei benchmark possono consentire suite di benchmark sostenibili che resistano alla prova del tempo. Suite di benchmark complete di propriet√† di comunit√† di ricerca o terze parti neutrali come MLCommons possono incoraggiare una pi√π ampia partecipazione e standardizzazione.",
    "crumbs": [
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>IA Sostenibile</span>"
    ]
  },
  {
    "objectID": "contents/core/sustainable_ai/sustainable_ai.it.html#case-study-google-4ms",
    "href": "contents/core/sustainable_ai/sustainable_ai.it.html#case-study-google-4ms",
    "title": "16¬† IA Sostenibile",
    "section": "16.10 Caso di Studio: 4M di Google",
    "text": "16.10 Caso di Studio: 4M di Google\nNegli ultimi dieci anni, l‚Äôintelligenza artificiale √® passata rapidamente dalla ricerca accademica ai sistemi di produzione su larga scala che alimentano numerosi prodotti e servizi Google. Poich√© i modelli e i carichi di lavoro dell‚ÄôIA sono cresciuti esponenzialmente in termini di dimensioni e richieste di elaborazione, sono emerse preoccupazioni circa il loro consumo energetico e l‚Äôimpatto ambientale. Alcuni ricercatori hanno previsto una crescita incontrollata dell‚Äôappetito energetico del ML che potrebbe superare le efficienze ottenute da algoritmi e hardware migliorati (Thompson et al. 2021).\n\nThompson, Neil C., Kristjan Greenewald, Keeheon Lee, e Gabriel F. Manso. 2021. ¬´Deep Learning‚Äôs Diminishing Returns: The Cost of Improvement is Becoming Unsustainable¬ª. IEEE Spectr. 58 (10): 50‚Äì55. https://doi.org/10.1109/mspec.2021.9563954.\nTuttavia, i dati di produzione di Google rivelano una storia diversa: l‚ÄôIA rappresenta un costante 10-15% del consumo energetico totale dell‚Äôazienda dal 2019 al 2021. Questo caso di studio analizza come Google ha applicato un approccio sistematico sfruttando quattro best practice, quelle che definiscono le ‚Äú4 M‚Äù: ‚ÄúModel efficiency‚Äù, ‚ÄúMachine optimization‚Äù, ‚ÄúMechanization through cloud computing‚Äù e ‚ÄúMapping to green locations‚Äù [efficienza del modello, ottimizzazione delle macchine, meccanizzazione tramite cloud computing e mappatura di luoghi Green], per piegare la curva delle emissioni dai carichi di lavoro dell‚ÄôIA.\nLa portata dell‚Äôutilizzo dell‚ÄôIA da parte di Google lo rende un caso di studio ideale. Solo nel 2021, l‚Äôazienda ha addestrato modelli come il GLam da 1,2 trilioni di parametri. Analizzare come l‚Äôapplicazione dell‚ÄôIA √® stata abbinata a rapidi guadagni di efficienza in questo ambiente ci aiuta a fornire un modello logico che il pi√π ampio campo dell‚ÄôIA seguir√†.\nPubblicando in modo trasparente statistiche dettagliate sull‚Äôuso dell‚Äôenergia, adottando tassi di acquisto di cloud senza emissioni di carbonio e fonti rinnovabili e altro ancora, insieme alle sue innovazioni tecniche, Google ha consentito ai ricercatori esterni di misurare i progressi in modo accurato. Il loro studio nell‚ÄôACM CACM (Patterson et al. 2022) evidenzia come l‚Äôapproccio multiforme dell‚Äôazienda dimostri che le previsioni di consumo energetico dell‚ÄôIA incontrollabili possono essere superate concentrando gli sforzi ingegneristici su modelli di sviluppo sostenibile. Il ritmo dei miglioramenti suggerisce anche che i guadagni di efficienza dell‚ÄôML sono appena iniziati.\n\nPatterson, David, Joseph Gonzalez, Urs Holzle, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild, David R. So, Maud Texier, e Jeff Dean. 2022. ¬´The Carbon Footprint of Machine Learning Training Will Plateau, Then Shrink¬ª. Computer 55 (7): 18‚Äì28. https://doi.org/10.1109/mc.2022.3148714.\n\n16.10.1 Le 4M Best Practice di Google\nPer ridurre le emissioni derivanti dai carichi di lavoro IA in rapida espansione, gli ingegneri di Google hanno sistematicamente identificato quattro aree di best practice, denominate ‚Äú4 M‚Äù, in cui le ottimizzazioni potrebbero sommarsi per ridurre l‚Äôimpatto ambientale del ML:\n\nModello: La selezione di architetture di modelli di intelligenza artificiale efficienti pu√≤ ridurre i calcoli di 5-10 volte senza alcuna perdita di qualit√†. Google ha svolto ricerche approfondite sullo sviluppo di modelli ‚Äúsparsi‚Äù e sulla ricerca di architetture neurali per creare modelli pi√π efficienti come Evolved Transformer e Primer.\nMacchina: L‚Äôutilizzo di hardware ottimizzato per l‚ÄôIA rispetto ai sistemi generici migliora le prestazioni per watt di 2-5 volte. Le Tensor Processing Unit (TPU) di Google hanno portato a un‚Äôefficienza di carbonio 5-13 volte migliore rispetto alle GPU non ottimizzate per il ML.\nMeccanizzazione: Sfruttando i sistemi di cloud computing progettati per un utilizzo elevato rispetto ai tradizionali data center on-premise, i costi energetici si riducono di 1,4-2 volte. Google cita l‚Äôefficacia dell‚Äôutilizzo energetico del suo data center come superiore alle medie del settore.\nMappa: La scelta di ubicazioni per data center dotate di elettricit√† a basse emissioni di carbonio riduce le emissioni lorde di altre 5-10 volte. Google fornisce mappe in tempo reale che evidenziano la percentuale di energia rinnovabile utilizzata dalle sue strutture.\n\nInsieme, queste pratiche hanno creato drastici guadagni di efficienza composti. Ad esempio, l‚Äôottimizzazione del modello Transformer AI su TPU in una sede di data center sostenibile ha ridotto il consumo di energia dell‚Äô83x. Ha ridotto le emissioni di \\(\\textrm{CO}_2\\) di un fattore di 747.\n\n\n16.10.2 Risultati Significativi\nNonostante la crescita esponenziale nell‚Äôadozione dell‚ÄôIA nei prodotti e nei servizi, gli sforzi di Google per migliorare l‚Äôefficienza del carbonio del ML hanno prodotto guadagni misurabili, contribuendo a limitare l‚Äôappetito energetico complessivo. Un punto dati chiave che evidenzia questo progresso √® che i carichi di lavoro dell‚Äôintelligenza artificiale sono rimasti stabili al 10%-15% del consumo energetico totale dell‚Äôazienda dal 2019 al 2021. Man mano che l‚ÄôIA √® diventata parte integrante di pi√π offerte Google, i cicli di elaborazione complessivi dedicati ad essa sono cresciuti in modo sostanziale. Tuttavia, l‚Äôefficienza negli algoritmi, nell‚Äôhardware specializzato, nella progettazione dei data center e nella geografia flessibile ha consentito alla sostenibilit√† di tenere il passo, con l‚ÄôIA che rappresenta solo una frazione dell‚Äôelettricit√† totale del data center in anni di espansione.\nAltri casi di studio sottolineano come un focus ingegneristico sui pattern di sviluppo dell‚Äôintelligenza artificiale sostenibile abbia consentito rapidi miglioramenti della qualit√† di pari passo con i guadagni ambientali. Ad esempio, il modello di elaborazione del linguaggio naturale GPT-3 √® stato considerato all‚Äôavanguardia a met√† del 2020. Tuttavia, il suo successore GLaM ha migliorato la precisione riducendo al contempo le esigenze di elaborazione del training e utilizzando energia pi√π pulita nei data center, riducendo le emissioni di CO2 di un fattore 14 in soli 18 mesi di evoluzione del modello.\nAnalogamente, Google ha scoperto che le precedenti speculazioni pubblicate non hanno colto nel segno sull‚Äôappetito energetico del ML per fattori da 100 a 100.000X a causa della mancanza di metriche del mondo reale. Tracciando in modo trasparente l‚Äôimpatto dell‚Äôottimizzazione, Google sperava di motivare l‚Äôefficienza evitando al contempo estrapolazioni sovrastimate sul pedaggio ambientale del ML.\nQuesti casi di studio basati sui dati mostrano come aziende come Google stiano indirizzando i progressi dell‚ÄôIA verso traiettorie sostenibili e migliorando l‚Äôefficienza per superare la crescita dell‚Äôadozione. Con ulteriori sforzi in termini di analisi del ciclo di vita, ottimizzazione dell‚Äôinferenza ed espansione delle energie rinnovabili, le aziende possono puntare ad accelerare i progressi, dimostrando che il potenziale pulito del ML √® stato appena sbloccato dagli attuali guadagni.\n\n\n16.10.3 Ulteriori Miglioramenti\nSebbene Google abbia compiuto progressi misurabili nel limitare l‚Äôimpatto ambientale delle sue operazioni di intelligenza artificiale, l‚Äôazienda riconosce che ulteriori guadagni in termini di efficienza saranno essenziali per un‚Äôinnovazione responsabile, data la continua espansione della tecnologia.\nUn‚Äôarea di attenzione √® mostrare come i progressi siano spesso erroneamente considerati come un aumento dell‚Äôinsostenibilit√† informatica, come la ricerca di architettura neurale (NAS) per trovare modelli ottimizzati, che stimolano risparmi a valle, superando i costi iniziali. Nonostante spenda pi√π energia nella scoperta di modelli piuttosto che nell‚Äôingegneria manuale, la NAS riduce le emissioni nel corso del ciclo di vita producendo progetti efficienti richiamabili su innumerevoli applicazioni.\nInoltre, l‚Äôanalisi rivela che concentrare gli sforzi di sostenibilit√† sull‚Äôottimizzazione lato server e data center ha senso, dato il consumo energetico dominante rispetto ai dispositivi consumer. Sebbene Google riduca gli impatti dell‚Äôinferenza su processori come i telefoni cellulari, la priorit√† √® il miglioramento dei cicli di training e dell‚Äôapprovvigionamento di energie rinnovabili per data center per ottenere il massimo effetto.\nA tal fine, i progressi di Google nel mettere in comune strutture cloud progettate in modo inefficiente evidenziano il valore della scala e della centralizzazione. Con l‚Äôallontanamento dei carichi di lavoro dai server locali inefficienti, la priorit√† data dai giganti di Internet alle energie rinnovabili (con Google e Meta che hanno raggiunto il 100% di energie rinnovabili rispettivamente dal 2017 e dal 2020) sblocca tagli alle emissioni complessive.\nInsieme, questi sforzi sottolineano che, sebbene non sia possibile adagiarsi sugli allori, l‚Äôapproccio multiforme di Google dimostra che i miglioramenti dell‚Äôefficienza dell‚ÄôIA stanno solo accelerando. Le iniziative intersettoriali relative alla valutazione del ciclo di vita, ai pattern di sviluppo attenti alle emissioni di carbonio, alla trasparenza e all‚Äôabbinamento della crescente domanda di IA con la fornitura di energia elettrica pulita aprono la strada a un‚Äôulteriore flessione della curva man mano che l‚Äôadozione aumenta. I risultati dell‚Äôazienda spingono il settore pi√π ampio a replicare queste attivit√† di sostenibilit√† integrate.",
    "crumbs": [
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>IA Sostenibile</span>"
    ]
  },
  {
    "objectID": "contents/core/sustainable_ai/sustainable_ai.it.html#embedded-ai-internet-of-trash",
    "href": "contents/core/sustainable_ai/sustainable_ai.it.html#embedded-ai-internet-of-trash",
    "title": "16¬† IA Sostenibile",
    "section": "16.11 IA Embedded - Internet of Trash",
    "text": "16.11 IA Embedded - Internet of Trash\nSebbene molta attenzione sia stata rivolta a rendere pi√π sostenibili gli immensi data center che alimentano l‚ÄôIA, una preoccupazione altrettanto urgente √® lo spostamento delle capacit√† dell‚ÄôIA in dispositivi edge e endpoint intelligenti. L‚ÄôIA edge/embedded consente una reattivit√† quasi in tempo reale senza dipendenze dalla connettivit√†. Riduce inoltre le esigenze di larghezza di banda di trasmissione. Tuttavia, l‚Äôaumento di dispositivi minuscoli comporta altri rischi.\nI minuscoli computer, microcontrollori e ASIC personalizzati che alimentano l‚Äôintelligenza edge affrontano limitazioni di dimensioni, costi e potenza che escludono le GPU di fascia alta utilizzate nei data center. Invece, richiedono algoritmi ottimizzati e circuiti estremamente compatti ed efficienti dal punto di vista energetico per funzionare senza problemi. Tuttavia, l‚Äôingegneria per questi fattori di forma microscopici apre rischi in termini di obsolescenza programmata, smaltibilit√† e spreco. Figura¬†16.11 mostra che si prevede che il numero di dispositivi IoT raggiunger√† i 30 miliardi di dispositivi connessi entro il 2030.\n\n\n\n\n\n\nFigura¬†16.11: Numero di dispositivi connessi all‚ÄôInternet of Things (IoT) in tutto il mondo dal 2019 al 2023. Fonte: Statista.\n\n\n\nLa gestione del fine vita dei gadget connessi a Internet dotati di sensori e intelligenza artificiale rimane un problema spesso trascurato durante la progettazione. Tuttavia, questi prodotti permeano beni di consumo, veicoli, infrastrutture pubbliche, apparecchiature industriali e altro ancora.\n\n16.11.1 Rifiuti Elettronici\nI rifiuti elettronici, o ‚Äúe-waste‚Äù, si riferiscono ad apparecchiature elettriche e componenti scartati che entrano nel flusso dei rifiuti. Ci√≤ include dispositivi che devono essere collegati, hanno una batteria o circuiti elettrici. Con la crescente adozione di dispositivi intelligenti e sensori connessi a Internet, i volumi di e-waste aumentano rapidamente ogni anno. Questi gadget in proliferazione contengono metalli pesanti tossici come piombo, mercurio e cadmio che diventano pericoli per l‚Äôambiente e la salute se smaltiti in modo improprio.\nLa quantit√† di rifiuti elettronici prodotti sta crescendo a un ritmo allarmante. Oggi, ne produciamo gi√† 50 milioni di tonnellate all‚Äôanno. Entro il 2030, si prevede che tale cifra salir√† a ben 75 milioni di tonnellate, poich√© il consumo di elettronica di consumo continua ad accelerare. La produzione globale di e-waste raggiunger√† i 120 milioni di tonnellate all‚Äôanno entro il 2050 (Un e Forum 2019). La produzione in forte crescita e i brevi cicli di vita dei nostri gadget alimentano questa crisi, dagli smartphone e tablet ai dispositivi connessi a Internet e agli elettrodomestici.\nI paesi in via di sviluppo sono i pi√π colpiti, in quanto necessitano di pi√π infrastrutture per elaborare in modo sicuro i dispositivi elettronici obsoleti. Nel 2019, i tassi di riciclaggio formali dei rifiuti elettronici nei paesi pi√π poveri variavano dal 13% al 23%. Il resto finisce per essere scaricato illegalmente, bruciato o smantellato in modo grossolano, rilasciando materiali tossici nell‚Äôambiente e danneggiando i lavoratori e le comunit√† locali. Chiaramente, c‚Äô√® ancora molto da fare per costruire una capacit√† globale per una gestione etica e sostenibile dei rifiuti elettronici, altrimenti rischiamo danni irreversibili.\nIl pericolo √® che la manipolazione grossolana dei dispositivi elettronici per spogliarli delle parti di valore esponga i lavoratori e le comunit√† emarginati a nocive plastiche/metalli bruciati. L‚Äôavvelenamento da piombo presenta rischi particolarmente elevati per lo sviluppo infantile se ingerito o inalato. Nel complesso, solo circa il 20% dei rifiuti elettronici prodotti √® stato raccolto utilizzando metodi ecologicamente corretti, secondo le stime delle Nazioni Unite (Un e Forum 2019). Quindi sono urgentemente necessarie soluzioni per una gestione responsabile del ciclo di vita per contenere lo smaltimento non sicuro, dato che il volume aumenta vertiginosamente.\n\nUn, e World Economic Forum. 2019. A New Circular Vision for Electronics, Time for a Global Reboot. PACE - Platform for Accelerating the Circular Economy. https://www3.weforum.org/docs/WEF\\_A\\_New\\_Circular\\_Vision\\_for\\_Electronics.pdf.\n\n\n16.11.2 Elettronica Monouso\nI costi in rapida diminuzione dei microcontrollori, delle piccole batterie ricaricabili e dell‚Äôhardware compatto di comunicazione hanno consentito l‚Äôintegrazione di sistemi di sensori intelligenti nei beni di consumo di uso quotidiano. Questi dispositivi Internet-of-Things (IoT) monitorano le condizioni del prodotto, le interazioni degli utenti e i fattori ambientali per consentire reattivit√† in tempo reale, personalizzazione e decisioni aziendali basate sui dati nel mercato connesso in evoluzione.\nTuttavia, questi dispositivi elettronici integrati affrontano poca supervisione o pianificazione per gestire in modo sostenibile il loro eventuale smaltimento una volta che i prodotti spesso rivestiti in plastica vengono scartati dopo breve tempo. I sensori IoT ora risiedono comunemente in articoli monouso come bottiglie d‚Äôacqua, imballaggi per alimenti, flaconi di farmaci e contenitori per cosmetici che finiscono prevalentemente nei flussi di rifiuti delle discariche dopo poche settimane o mesi di utilizzo da parte dei consumatori.\nIl problema si accelera poich√© sempre pi√π produttori si affrettano a integrare chip mobili, fonti di alimentazione, moduli Bluetooth e altri moderni circuiti integrati in silicio, che costano meno di 1 dollaro USA, in vari prodotti senza protocolli per il riciclaggio, la sostituzione delle batterie o la riutilizzabilit√† dei componenti. Nonostante le loro piccole dimensioni individuali, i volumi di questi dispositivi e il peso dei rifiuti nel corso della loro vita incombono. A differenza della regolamentazione di dispositivi elettronici pi√π grandi, esistono pochi vincoli normativi sui requisiti dei materiali o sulla tossicit√† di piccoli gadget monouso.\nPur offrendo praticit√† durante il lavoro, la combinazione insostenibile di difficile recupero e limitati meccanismi di guasto sicuri fa s√¨ che i dispositivi connessi monouso contribuiscano a quote sproporzionate di futuri volumi di rifiuti elettronici che necessitano di urgente attenzione.\n\n\n16.11.3 Obsolescenza Programmata\nL‚Äôobsolescenza programmata si riferisce alla strategia di progettazione intenzionale di produzione di prodotti con durate di vita artificialmente limitate che diventano rapidamente non funzionali o obsoleti. Ci√≤ stimola cicli di acquisto di sostituzione pi√π rapidi poich√© i consumatori scoprono che i dispositivi non soddisfano pi√π le loro esigenze nel giro di pochi anni. Tuttavia, l‚Äôelettronica progettata per l‚Äôobsolescenza prematura contribuisce a volumi di rifiuti elettronici insostenibili.\nAd esempio, incollare batterie e componenti di smartphone insieme ostacola la riparabilit√† rispetto ad assemblaggi modulari e accessibili. L‚Äôimplementazione di aggiornamenti software che rallentano deliberatamente le prestazioni del sistema crea la percezione che valga la pena aggiornare i dispositivi prodotti solo diversi anni prima.\nAllo stesso modo, le introduzioni alla moda di nuove generazioni di prodotti con aggiunte di funzionalit√† minori ma esclusive fanno sembrare rapidamente datate le versioni precedenti. Queste tattiche costringono ad acquistare nuovi gadget (ad esempio, iPhone) molto prima della fine della loro operativit√†. Se moltiplicati per categorie di elettronica in rapida evoluzione, miliardi di articoli appena indossati vengono scartati ogni anno.\nL‚Äôobsolescenza programmata intensifica quindi l‚Äôutilizzo delle risorse e la creazione di rifiuti nella produzione di prodotti senza alcuna intenzione di lunga durata. Ci√≤ contraddice i principi di sostenibilit√† in materia di durata, riutilizzo e conservazione dei materiali. Mentre stimola vendite e guadagni continui per i produttori nel breve termine, la strategia esternalizza costi ambientali e tossine su comunit√† prive di un‚Äôadeguata infrastruttura di elaborazione dei rifiuti elettronici.\nLe politiche e l‚Äôazione dei consumatori sono fondamentali per contrastare i design dei gadget che sono inutilmente monouso per default. Le aziende dovrebbero anche investire in programmi di gestione dei prodotti che supportino il riutilizzo e il recupero responsabili.\nConsideriamo l‚Äôesempio del mondo reale. Apple √® stata attenzionata nel corso degli anni per aver presumibilmente coinvolto nell‚Äôobsolescenza programmata per incoraggiare i clienti ad acquistare nuovi modelli di iPhone. L‚Äôazienda avrebbe progettato i suoi telefoni in modo che le prestazioni si degradino nel tempo o che le funzionalit√† esistenti diventino incompatibili con i nuovi sistemi operativi, il che, secondo i critici, √® finalizzato a stimolare cicli di aggiornamento pi√π rapidi. Nel 2020, Apple ha pagato una multa di 25 milioni di euro per risolvere un caso in Francia in cui le autorit√† di regolamentazione hanno ritenuto l‚Äôazienda colpevole di aver rallentato intenzionalmente i vecchi iPhone senza informare chiaramente i clienti tramite aggiornamenti di iOS.\nNon essendo trasparente sulle modifiche alla gestione dell‚Äôalimentazione che hanno ridotto le prestazioni del dispositivo, Apple ha partecipato ad attivit√† ingannevoli che hanno ridotto la durata del prodotto per aumentare le vendite. L‚Äôazienda ha affermato che √® stato fatto per ‚Äúsmussare‚Äù i picchi che potrebbero causare improvvisamente lo spegnimento delle vecchie batterie. Tuttavia, questo esempio evidenzia i rischi legali legati all‚Äôimpiego dell‚Äôobsolescenza programmata e alla mancata comunicazione corretta di quando le modifiche alle funzionalit√† influiscono sull‚Äôusabilit√† del dispositivo nel tempo: persino marchi leader come Apple possono avere problemi se percepiti come coloro che accorciano intenzionalmente i cicli di vita del prodotto.",
    "crumbs": [
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>IA Sostenibile</span>"
    ]
  },
  {
    "objectID": "contents/core/sustainable_ai/sustainable_ai.it.html#policy-and-regulatory-considerations",
    "href": "contents/core/sustainable_ai/sustainable_ai.it.html#policy-and-regulatory-considerations",
    "title": "16¬† IA Sostenibile",
    "section": "16.12 Considerazioni Normative e Politiche",
    "text": "16.12 Considerazioni Normative e Politiche\n\n16.12.1 Mandati di Misura e Rendicontazione\nUn meccanismo politico sempre pi√π rilevante per i sistemi di IA √® rappresentato dai requisiti di misurazione e rendicontazione relativi al consumo energetico e alle emissioni di carbonio. Misurazioni obbligatorie, audit, divulgazioni e metodologie pi√π rigorose allineate alle metriche di sostenibilit√† possono aiutare a colmare le lacune informative che ostacolano le ottimizzazioni dell‚Äôefficienza.\nAllo stesso tempo, le politiche nazionali o regionali richiedono alle aziende di una certa dimensione di utilizzare l‚ÄôIA nei loro prodotti o sistemi back-end per segnalare il consumo energetico o le emissioni associate ai principali carichi di lavoro di IA. Organizzazioni come la Partnership on AI, IEEE e NIST potrebbero contribuire a definire metodologie standardizzate. Proposte pi√π complesse implicano la definizione di modalit√† coerenti per misurare la complessit√† computazionale, il PUE del data center, l‚Äôintensit√† di carbonio dell‚Äôapprovvigionamento energetico e le efficienze ottenute tramite hardware specifico per l‚ÄôIA.\nAnche gli obblighi di rendicontazione per gli utenti del settore pubblico che acquistano servizi di IA, ad esempio tramite una proposta di legge in Europa, potrebbero aumentare la trasparenza. Tuttavia, gli enti regolatori devono bilanciare l‚Äôulteriore onere di misurazione che tali mandati impongono alle organizzazioni con le continue riduzioni di carbonio derivanti dall‚Äôincorporazione di pattern di sviluppo consapevoli della sostenibilit√†.\nPer essere pi√π costruttivi, qualsiasi politica di misurazione e rendicontazione dovrebbe concentrarsi sull‚Äôabilitazione di un continuo perfezionamento piuttosto che su restrizioni o limiti semplicistici. Man mano che i progressi dell‚ÄôIA si sviluppano rapidamente, agili barriere di sicurezza di governance che incorporano considerazioni sulla sostenibilit√† in normali metriche di valutazione possono motivare un cambiamento positivo. Tuttavia, una prescrizione eccessiva rischia di limitare l‚Äôinnovazione se i requisiti diventano obsoleti. La politica di efficienza dell‚ÄôIA accelera i progressi in tutto il settore combinando flessibilit√† con appropriate barriere di sicurezza di trasparenza.\n\n\n16.12.2 Meccanismi di Restrizione\nOltre agli obblighi di segnalazione, i politici dispongono di diversi meccanismi di restrizione che potrebbero modellare direttamente il modo in cui i sistemi di IA vengono sviluppati e implementati per ridurre le emissioni:\nLimiti sulle Emissioni delle Elaborazioni: La proposta di legge sull‚Äôintelligenza artificiale della Commissione europea adotta un approccio orizzontale che potrebbe consentire di stabilire limiti per l‚Äôintera economia sul volume di potenza di elaborazione disponibile per l‚Äôaddestramento dei modelli di intelligenza artificiale. Come i sistemi di scambio delle emissioni, i limiti mirano a disincentivare indirettamente l‚Äôelaborazione estensiva rispetto alla sostenibilit√†. Tuttavia, la qualit√† del modello potrebbe essere migliorata per fornire pi√π percorsi per l‚Äôacquisizione di capacit√† aggiuntiva.\nCondizionamento dell‚ÄôAccesso alle Risorse Pubbliche: Alcuni esperti hanno proposto incentivi come consentire l‚Äôaccesso solo a set di dati pubblici o potenza di elaborazione per lo sviluppo di modelli fondamentalmente efficienti piuttosto che architetture stravaganti. Ad esempio, il consorzio di benchmarking MLCommons fondato da importanti aziende tecnologiche potrebbe integrare formalmente l‚Äôefficienza nelle sue metriche di classifica standardizzate, tuttavia, l‚Äôaccesso condizionato rischia di limitare l‚Äôinnovazione.\nMeccanismi Finanziari: Analogamente alle tasse sul carbonio sulle industrie inquinanti, le tariffe applicate per unit√† di consumo di elaborazione correlato all‚ÄôIA potrebbero scoraggiare un inutile ridimensionamento del modello, finanziando al contempo innovazioni di efficienza. I crediti d‚Äôimposta potrebbero in alternativa premiare le organizzazioni pioniere di tecniche di IA pi√π accurate ma compatte. Tuttavia, gli strumenti finanziari richiedono un‚Äôattenta calibrazione tra generazione di entrate ed equit√† e non penalizzare eccessivamente gli usi produttivi dell‚ÄôIA.\nDivieti Tecnologici: Se la misurazione fissasse costantemente le emissioni estreme su applicazioni specifiche dell‚ÄôIA senza percorsi di bonifica, i divieti assoluti rappresentano uno strumento di ultima istanza per i decisori politici. Tuttavia, dato il duplice uso dell‚ÄôIA, definire implementazioni dannose e benefiche risulta complesso, rendendo necessaria una valutazione di impatto olistica prima di concludere che non esiste alcun valore redentivo. Vietare tecnologie promettenti rischia di avere conseguenze indesiderate e richiede cautela.\n\n\n16.12.3 Incentivi Governativi\n√à una pratica comune per i governi fornire incentivi fiscali o di altro tipo a consumatori o aziende quando contribuiscono a pratiche tecnologiche pi√π sostenibili. Tali incentivi esistono gi√† negli Stati Uniti per l‚Äôadozione di pannelli solari o edifici a risparmio energetico. Per quanto ne sappiamo, non esistono ancora incentivi fiscali per pratiche di sviluppo specifiche per l‚ÄôIA.\nUn altro potenziale programma di incentivi che sta iniziando a essere esplorato √® l‚Äôutilizzo di sovvenzioni governative per finanziare progetti di IA Green. Ad esempio, in Spagna, sono stati stanziati 300 milioni di euro per finanziare specificamente progetti di IA e sostenibilit√†. Gli incentivi governativi sono una strada promettente per incoraggiare pratiche di comportamento aziendale e dei consumatori sostenibili, ma √® necessaria un‚Äôattenta riflessione per determinare come tali incentivi si adatteranno alle richieste del mercato (Cohen, Lobel, e Perakis 2016).\n\nCohen, Maxime C., Ruben Lobel, e Georgia Perakis. 2016. ¬´The Impact of Demand Uncertainty on Consumer Subsidies for Green Technology Adoption¬ª. Manage. Sci. 62 (5): 1235‚Äì58. https://doi.org/10.1287/mnsc.2015.2173.\n\n\n16.12.4 Autoregolamentazione\nComplementari alle potenziali azioni governative, i meccanismi di autogoverno volontario consentono alla comunit√† dell‚ÄôIA di perseguire obiettivi di sostenibilit√† senza interventi dall‚Äôalto:\nImpegni per le Energie Rinnovabili: Grandi professionisti dell‚ÄôIA come Google, Microsoft, Amazon e Meta si sono impegnati ad acquistare abbastanza elettricit√† rinnovabile per soddisfare il 100% delle loro richieste energetiche. Questi impegni sbloccano tagli alle emissioni composti man mano che aumenta la potenza di calcolo. La formalizzazione di tali programmi incentiva le regioni dei data center verdi. Tuttavia, ci sono critiche sul fatto che questi impegni siano sufficienti (Monyei e Jenkins 2018).\n\nMonyei, Chukwuka G., e Kirsten E. H. Jenkins. 2018. ¬´Electrons have no identity: Setting right misrepresentations in Google and Apple‚Äôs clean energy purchasing¬ª. Energy Research &amp; Social Science 46 (dicembre): 48‚Äì51. https://doi.org/10.1016/j.erss.2018.06.015.\nPrezzi Interni del Carbonio: Alcune organizzazioni utilizzano prezzi ombra sulle emissioni di carbonio per rappresentare i costi ambientali nelle decisioni di allocazione del capitale tra progetti di IA. Se modellati in modo efficace, gli oneri teorici sulle impronte di carbonio dello sviluppo indirizzano i finanziamenti verso innovazioni efficienti piuttosto che solo verso guadagni di accuratezza.\nChecklist per lo Sviluppo dell‚ÄôEfficienza: Gruppi come AI Sustainability Coalition suggeriscono modelli di checklist volontari che evidenziano le scelte di progettazione del modello, le configurazioni hardware e altri fattori che gli architetti possono regolare per applicazione per limitare le emissioni. Le organizzazioni possono guidare il cambiamento radicando la sostenibilit√† come metrica di successo primaria insieme a precisione e costi.\nAuditing Indipendente: Anche in assenza di mandati di divulgazione pubblica, le aziende specializzate in audit di sostenibilit√† tecnologica aiutano gli sviluppatori di IA a identificare gli sprechi, creare roadmap di efficienza e confrontare i progressi tramite revisioni imparziali. Strutturare tali audit in procedure di governance interna o nel processo di approvvigionamento espande la responsabilit√†.\n\n\n16.12.5 Considerazioni Globali\nMentre misurazione, restrizioni, incentivi e autoregolamentazione rappresentano potenziali meccanismi politici per promuovere la sostenibilit√† dell‚ÄôIA, la frammentazione tra i regimi nazionali rischia di avere conseguenze indesiderate. Come per altri domini di politica tecnologica, la divergenza tra regioni deve essere gestita attentamente.\nAd esempio, a causa di preoccupazioni sulla privacy dei dati regionali, OpenAI ha impedito agli utenti europei di accedere al suo chatbot virale ChatGPT. Ci√≤ √® avvenuto dopo che la proposta di legge sull‚ÄôIA dell‚ÄôUE ha segnalato un approccio precauzionale, consentendo alla CE di vietare determinati usi dell‚ÄôIA ad alto rischio e di imporre regole di trasparenza che creano incertezza per il rilascio di nuovi modelli. Tuttavia, sarebbe saggio mettere in guardia contro l‚Äôazione del regolatore in quanto potrebbe inavvertitamente limitare l‚Äôinnovazione europea se i regimi con una regolamentazione pi√π leggera attraggono pi√π spesa e talenti per la ricerca sull‚ÄôIA nel settore privato. Trovare un terreno comune √® fondamentale.\nI principi dell‚ÄôOCSE sull‚ÄôIA e i quadri delle Nazioni Unite sottolineano principi universalmente concordati che tutte le politiche nazionali dovrebbero sostenere: trasparenza, responsabilit√†, mitigazione dei ‚Äúbias‚Äù [pregiudizi] e altro ancora. Incorporare in modo costruttivo la sostenibilit√† come principio fondamentale per un‚ÄôIA responsabile all‚Äôinterno di linee guida internazionali pu√≤ motivare un‚Äôazione unitaria senza sacrificare la flessibilit√† tra sistemi legali divergenti. Evitare dinamiche di corsa al ribasso dipende da una cooperazione multilaterale illuminata.",
    "crumbs": [
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>IA Sostenibile</span>"
    ]
  },
  {
    "objectID": "contents/core/sustainable_ai/sustainable_ai.it.html#public-perception-and-engagement",
    "href": "contents/core/sustainable_ai/sustainable_ai.it.html#public-perception-and-engagement",
    "title": "16¬† IA Sostenibile",
    "section": "16.13 Percezione e Coinvolgimento del Pubblico",
    "text": "16.13 Percezione e Coinvolgimento del Pubblico\nMentre l‚Äôattenzione della societ√† e gli sforzi politici volti alla sostenibilit√† ambientale aumentano in tutto il mondo, cresce l‚Äôentusiasmo per l‚Äôutilizzo dell‚Äôintelligenza artificiale per aiutare ad affrontare le sfide ecologiche. Tuttavia, la comprensione e gli atteggiamenti del pubblico nei confronti del ruolo dei sistemi di IA nei contesti di sostenibilit√† devono ancora essere chiariti e sono offuscati da idee sbagliate. Da un lato, le persone sperano che algoritmi avanzati possano fornire nuove soluzioni per l‚Äôenergia verde, il consumo responsabile, i percorsi di decarbonizzazione e la conservazione dell‚Äôecosistema. Dall‚Äôaltro, i timori sui rischi dell‚ÄôIA incontrollata si insinuano anche nel dominio ambientale e minano il discorso costruttivo. Inoltre, una mancanza di consapevolezza pubblica su questioni chiave come la trasparenza nello sviluppo di strumenti di IA incentrati sulla sostenibilit√† e potenziali pregiudizi nei dati o nella modellazione minacciano anche di limitare la partecipazione inclusiva e degradare la fiducia del pubblico.\nAffrontare priorit√† complesse e interdisciplinari come la sostenibilit√† ambientale richiede un coinvolgimento pubblico informato e sfumato e progressi responsabili nell‚Äôinnovazione dell‚ÄôIA. Il percorso da seguire richiede sforzi collaborativi attenti ed equi tra esperti in ML, climatologia, politica ambientale, scienze sociali e comunicazione. Mappare il panorama delle percezioni pubbliche, identificare le insidie e tracciare strategie per coltivare sistemi di IA comprensibili, accessibili e affidabili che puntino a priorit√† ecologiche condivise si riveler√† essenziale per realizzare obiettivi di sostenibilit√†. Questo terreno complesso giustifica un esame approfondito delle dinamiche socio-tecniche coinvolte.\n\n16.13.1 Consapevolezza dell‚ÄôIA\nA maggio 2022, il Pew Research Center ha intervistato 5.101 adulti statunitensi, scoprendo che il 60% aveva sentito o letto ‚Äúun po‚Äô‚Äù sull‚ÄôIA mentre il 27% ne aveva sentito ‚Äúmolto‚Äù, il che indica un discreto riconoscimento generale, ma probabilmente una comprensione limitata di dettagli o applicazioni. Tuttavia, tra coloro che hanno una certa familiarit√† con l‚ÄôIA, emergono preoccupazioni riguardo ai rischi di uso improprio dei dati personali secondo i termini concordati. Ciononostante, il 62% ritiene che l‚ÄôIA potrebbe semplificare la vita moderna se applicata in modo responsabile. Tuttavia, una comprensione specifica dei contesti di sostenibilit√† deve ancora essere migliorata.\nGli studi che tentano di categorizzare i ‚Äúsentiment‚Äù del discorso online rilevano una divisione quasi equa tra ottimismo e cautela riguardo all‚Äôimplementazione dell‚ÄôIA per obiettivi di sostenibilit√†. I fattori che guidano la positivit√† includono le speranze di una migliore previsione dei cambiamenti ecologici utilizzando modelli di ML. La negativit√† nasce da una mancanza di fiducia negli algoritmi auto-supervisionati che evitano conseguenze indesiderate dovute a impatti umani imprevedibili su sistemi naturali complessi durante l‚Äôaddestramento.\nLa convinzione pubblica pi√π diffusa rimane che, mentre l‚ÄôIA ha il potenziale per accelerare le soluzioni su questioni come la riduzione delle emissioni e la protezione della fauna selvatica, una salvaguardia inadeguata intorno a pregiudizi dei dati, punti ciechi etici e considerazioni sulla privacy potrebbero essere rischi pi√π apprezzati se perseguiti con noncuranza, soprattutto su larga scala. Ci√≤ porta a esitazione intorno al supporto incondizionato senza prove di uno sviluppo deliberato e guidato democraticamente.\n\n\n16.13.2 Messaggistica\nGli sforzi ottimistici stanno evidenziando la promessa di sostenibilit√† dell‚ÄôIA e sottolineano il potenziale del ML avanzato per accelerare radicalmente gli effetti di decarbonizzazione da reti intelligenti, app personalizzate di tracciamento del carbonio, ottimizzazioni automatizzate dell‚Äôefficienza degli edifici e analisi predittive che guidano gli sforzi di conservazione mirati. Una modellazione in tempo reale pi√π completa di complessi cambiamenti climatici ed ecologici utilizzando algoritmi auto-miglioranti offre speranza per mitigare le perdite di biodiversit√† ed evitare gli scenari peggiori.\nTuttavia, prospettive cautelative, come i Principi di IA di Asilomar, mettono in dubbio se l‚ÄôIA stessa potrebbe esacerbare le sfide della sostenibilit√† se vincolata in modo improprio. Le crescenti richieste di energia dei sistemi di elaborazione su larga scala e il training sempre pi√π massiccio del modello di rete neurale sono in conflitto con le ambizioni di energia pulita. La mancanza di diversit√† negli input di dati o nelle priorit√† degli sviluppatori potrebbe sminuire le urgenti considerazioni di giustizia ambientale. L‚Äôimpegno pubblico scettico a breve termine probabilmente dipende dalla necessit√† di salvaguardie percepibili contro i sistemi di intelligenza artificiale incontrollati che impazziscono nei processi ecologici fondamentali.\nIn sostanza, i ‚Äúframing‚Äù polarizzati promuovono l‚Äôintelligenza artificiale come uno strumento indispensabile per la risoluzione dei problemi di sostenibilit√†, se indirizzata compassionevolmente verso le persone e il pianeta, oppure presentano l‚ÄôIA come un amplificatore dei danni esistenti che dominano insidiosamente aspetti nascosti dei sistemi naturali centrali per tutta la vita. Superare tali impasse richiede di bilanciare discussioni oneste sui compromessi con visioni condivise per un progresso tecnologico equo e democraticamente governato che mira al ripristino.\n\n\n16.13.3 Partecipazione Equa\nGarantire una partecipazione e un accesso equi dovrebbe costituire la pietra angolare di qualsiasi iniziativa di sostenibilit√† con il potenziale per importanti impatti sociali. Questo principio si applica ugualmente ai sistemi di IA che mirano a obiettivi ambientali. Tuttavia, voci comunemente escluse come le comunit√† in prima linea, rurali o indigene e le generazioni future non presenti per il consenso potrebbero subire conseguenze sproporzionate dalle trasformazioni tecnologiche. Ad esempio, la Partnership on AI ha lanciato eventi espressamente mirati al contributo delle comunit√† emarginate sull‚Äôimplementazione responsabile dell‚Äôintelligenza artificiale.\nGarantire un accesso e una partecipazione equi dovrebbe costituire la pietra angolare di qualsiasi iniziativa di sostenibilit√† con il potenziale per importanti impatti sociali, che si tratti di intelligenza artificiale o altro. Tuttavia, l‚Äôimpegno inclusivo nell‚Äôintelligenza artificiale ambientale si basa in parte sulla disponibilit√† e sulla comprensione delle risorse informatiche fondamentali. Come sottolinea il recente rapporto OCSE sulla capacit√† di calcolo IA nazionale (Oecd 2023), molti paesi attualmente non dispongono di dati o piani strategici che mappino le esigenze per l‚Äôinfrastruttura richiesta per alimentare i sistemi di IA. Questo punto cieco politico potrebbe limitare gli obiettivi economici ed esacerbare le barriere all‚Äôingresso per le popolazioni emarginate. Il loro progetto sollecita lo sviluppo di strategie nazionali per la capacit√† di calcolo AI lungo dimensioni di capacit√†, accessibilit√†, pipeline di innovazione e resilienza per ancorare l‚Äôinnovazione. L‚Äôarchiviazione dei dati di base deve essere migliorata e le piattaforme di sviluppo dei modelli o l‚Äôhardware specializzato potrebbero inavvertitamente concentrare i progressi dell‚ÄôAI nelle mani di gruppi selezionati. Pertanto, la pianificazione di un‚Äôespansione equilibrata delle risorse di calcolo AI fondamentali tramite iniziative politiche si collega direttamente alle speranze di una risoluzione dei problemi di sostenibilit√† democratizzata utilizzando strumenti ML equi e trasparenti.\n\nOecd. 2023. ¬´A blueprint for building national compute capacity for artificial intelligence¬ª. 350. Organisation for Economic Co-Operation; Development (OECD). https://doi.org/10.1787/876367e3-en.\nL‚Äôidea chiave √® che la partecipazione equa nei sistemi di intelligenza artificiale che affrontano le sfide ambientali si basa in parte sulla garanzia che la capacit√† di elaborazione e l‚Äôinfrastruttura di base siano corrette, il che richiede una pianificazione politica proattiva da una prospettiva nazionale.\n\n\n16.13.4 Trasparenza\nMentre le agenzie del settore pubblico e le aziende private si affrettano ad adottare strumenti di IA per aiutare ad affrontare le urgenti sfide ambientali, le richieste di trasparenza sullo sviluppo e la funzionalit√† di questi sistemi hanno iniziato ad amplificarsi. Le funzionalit√† di ML spiegabili e interpretabili diventano sempre pi√π cruciali per creare fiducia nei modelli emergenti che mirano a guidare le conseguenti politiche di sostenibilit√†. Iniziative come il Montreal Carbon Pledge hanno riunito i leader della tecnologia per impegnarsi a pubblicare valutazioni di impatto prima di lanciare sistemi ambientali, come promesso di seguito:\n\n‚ÄúCome investitori istituzionali, dobbiamo agire nel migliore interesse a lungo termine dei nostri beneficiari. In questo ruolo fiduciario, i rischi di investimento a lungo termine sono associati alle emissioni di gas serra, ai cambiamenti climatici e alla regolamentazione del carbonio. Misurare la nostra impronta di carbonio √® fondamentale per comprendere meglio, quantificare e gestire gli impatti, i rischi e le opportunit√† correlati al carbonio e ai cambiamenti climatici nei nostri investimenti. Pertanto, come primo passo, ci impegniamo a misurare e divulgare annualmente l‚Äôimpronta di carbonio dei nostri investimenti per utilizzare queste informazioni per sviluppare una strategia di coinvolgimento e identificare e stabilire obiettivi di riduzione dell‚Äôimpronta di carbonio.‚Äù ‚Äì Montr√©al Carbon Pledge\n\nAbbiamo bisogno di un impegno simile per la sostenibilit√† e la responsabilit√† dell‚ÄôIA. L‚Äôaccettazione diffusa e l‚Äôimpatto delle soluzioni di sostenibilit√† dell‚ÄôIA dipenderanno in parte dalla comunicazione deliberata di schemi di convalida, metriche e livelli di giudizio umano applicati prima dell‚Äôimplementazione in tempo reale. Lavori come i Principi per l‚ÄôIA spiegabile del NIST possono aiutare a promuovere la trasparenza nei sistemi di IA. Il National Institute of Standards and Technology (NIST) ha pubblicato un influente set di linee guida denominato ‚ÄúPrinciples for Explainable AI‚Äù [Principi per l‚ÄôIA spiegabile] (Phillips et al. 2020). Questo framework articola le best practice per la progettazione, la valutazione e l‚Äôimplementazione di sistemi di IA responsabili con funzionalit√† trasparenti e interpretabili che creano comprensione e fiducia fondamentali per l‚Äôutente.\n\nPhillips, P Jonathon, Carina A Hahn, Peter C Fontana, David A Broniatowski, e Mark A Przybocki. 2020. ¬´Four principles of explainable artificial intelligence¬ª. Gaithersburg, Maryland 18.\nDelinea quattro principi fondamentali: in primo luogo, i sistemi di IA dovrebbero fornire spiegazioni contestualmente rilevanti che giustifichino il ragionamento alla base dei loro output alle parti interessate appropriate. In secondo luogo, queste spiegazioni di IA devono comunicare informazioni in modo significativo per il livello di comprensione appropriato del loro pubblico target. Il successivo √® il principio di accuratezza, che stabilisce che le spiegazioni dovrebbero riflettere fedelmente il processo effettivo e la logica che informano i meccanismi interni di un modello di IA per generare output o raccomandazioni dati in base agli input. Infine, un principio di limiti di conoscenza obbliga le spiegazioni a chiarire i confini di un modello di IA nel catturare l‚Äôintera ampiezza della complessit√†, della varianza e delle incertezze del mondo reale all‚Äôinterno di uno spazio problematico.\nNel complesso, questi principi NIST offrono ai professionisti e agli adottanti dell‚ÄôIA una guida su considerazioni chiave sulla trasparenza, essenziali per sviluppare soluzioni accessibili che diano priorit√† all‚Äôautonomia e alla fiducia dell‚Äôutente piuttosto che semplicemente massimizzare le sole metriche di accuratezza predittiva. Man mano che l‚ÄôIA avanza rapidamente in contesti sociali sensibili come sanit√†, finanza, occupazione e oltre, tali linee guida di progettazione incentrate sull‚Äôuomo continueranno a crescere in importanza per ancorare l‚Äôinnovazione agli interessi pubblici.\nCi√≤ si applica anche al dominio della capacit√† ambientale. Un‚Äôinnovazione dell‚ÄôIA responsabile e guidata democraticamente che mira a priorit√† ecologiche condivise dipende dal mantenimento della vigilanza pubblica, della comprensione e della supervisione su sistemi altrimenti opachi che assumono ruoli di primo piano nelle decisioni della societ√†. Dare priorit√† a progetti di algoritmi spiegabili e pratiche di trasparenza radicale secondo standard globali pu√≤ aiutare a sostenere la fiducia collettiva che questi strumenti migliorino piuttosto che mettere a repentaglio le speranze per un futuro guidato.",
    "crumbs": [
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>IA Sostenibile</span>"
    ]
  },
  {
    "objectID": "contents/core/sustainable_ai/sustainable_ai.it.html#direzioni-e-sfide-future",
    "href": "contents/core/sustainable_ai/sustainable_ai.it.html#direzioni-e-sfide-future",
    "title": "16¬† IA Sostenibile",
    "section": "16.14 Direzioni e Sfide Future",
    "text": "16.14 Direzioni e Sfide Future\nGuardando al futuro, il ruolo dell‚ÄôIA nella sostenibilit√† ambientale √® destinato a crescere in modo ancora pi√π significativo. Il potenziale dell‚ÄôIA per guidare i progressi nell‚Äôenergia rinnovabile, nella modellazione climatica, negli sforzi di conservazione e altro √® immenso. Tuttavia, √® una moneta a due facce, poich√© dobbiamo superare diverse sfide e indirizzare i nostri sforzi verso uno sviluppo dell‚ÄôIA sostenibile e responsabile.\n\n16.14.1 Direzioni Future\nUna delle direzioni chiave del futuro √® lo sviluppo di modelli e algoritmi di intelligenza artificiale pi√π efficienti dal punto di vista energetico. Ci√≤ implica una ricerca e innovazione continue in aree come il ‚Äúpruning‚Äù [potatura] dei modelli, la quantizzazione e l‚Äôuso di numeri a bassa precisione, nonch√© lo sviluppo dell‚Äôhardware per consentire la piena redditivit√† di queste innovazioni. Inoltre, esaminiamo paradigmi di elaborazione alternativi che non si basano su architetture von-Neumann. Ulteriori informazioni su questo argomento sono disponibili nel capitolo sull‚Äôaccelerazione hardware. L‚Äôobiettivo √® creare sistemi di IA che offrano prestazioni elevate riducendo al minimo il consumo di energia e le emissioni di carbonio.\nUn‚Äôaltra direzione importante √® l‚Äôintegrazione di fonti di energia rinnovabili nell‚Äôinfrastruttura di IA. Poich√© i data center continuano a contribuire in modo significativo all‚Äôimpronta di carbonio dell‚ÄôIA, la transizione verso fonti di energia rinnovabili come l‚Äôenergia solare ed eolica √® fondamentale. Gli sviluppi nell‚Äôaccumulo di energia sostenibile a lungo termine, come Ambri, uno spin-off del MIT, potrebbero consentire questa transizione. Ci√≤ richiede investimenti e collaborazioni significativi tra aziende tecnologiche, fornitori di energia e politici.\n\n\n16.14.2 Sfide\nNonostante queste promettenti direzioni, devono essere affrontate diverse sfide. Una delle sfide principali √® la necessit√† di standard e metodologie coerenti per misurare e segnalare l‚Äôimpatto ambientale dell‚ÄôIA. Questi metodi devono catturare la complessit√† dei cicli di vita dei modelli di IA e dell‚Äôhardware di sistema. Inoltre, sono necessarie infrastrutture IA e hardware di sistema efficienti e sostenibili dal punto di vista ambientale. Ci√≤ √® costituito da tre componenti:\n\nMassimizzare l‚Äôutilizzo delle risorse di acceleratore e sistema.\nProlungare la durata di vita delle infrastrutture IA.\nProgettare hardware di sistema tenendo presente l‚Äôimpatto ambientale.\n\nDal lato software, dovremmo bilanciare la sperimentazione e il conseguente costo di training. Tecniche come la ricerca dell‚Äôarchitettura neurale e l‚Äôottimizzazione degli iperparametri possono essere utilizzate per l‚Äôesplorazione dello spazio di progettazione. Tuttavia, queste sono spesso molto dispendiose in termini di risorse. Una sperimentazione efficiente pu√≤ ridurre significativamente l‚Äôimpatto ambientale. Successivamente, dovrebbero essere esplorati metodi per ridurre gli sforzi di training sprecati.\nPer migliorare la qualit√† del modello, spesso ridimensioniamo il set di dati. Tuttavia, le maggiori risorse di sistema richieste per l‚Äôarchiviazione e l‚Äôingestione dei dati causate da questa scalabilit√† hanno un impatto ambientale significativo (Wu et al. 2022). √à importante comprendere a fondo la velocit√† con cui i dati perdono il loro valore predittivo e ideare strategie di campionamento dei dati.\n\nWu, Carole-Jean, Ramya Raghavendra, Udit Gupta, Bilge Acun, Newsha Ardalani, Kiwan Maeng, Gloria Chang, et al. 2022. ¬´Sustainable ai: Environmental implications, challenges and opportunities¬ª. Proceedings of Machine Learning and Systems 4: 795‚Äì813.\nAnche le lacune nei dati rappresentano una sfida significativa. Senza aziende e governi che condividono apertamente dati dettagliati e accurati sul consumo di energia, sulle emissioni di carbonio e su altri impatti ambientali, non √® facile sviluppare strategie efficaci per un‚ÄôIA sostenibile.\nInfine, il ritmo rapido dello sviluppo dell‚ÄôIA richiede un approccio agile alla politica imposta a questi sistemi. La politica dovrebbe garantire uno sviluppo sostenibile senza limitare l‚Äôinnovazione. Ci√≤ richiede che esperti in tutti i settori dell‚ÄôIA, delle scienze ambientali, dell‚Äôenergia e della politica lavorino insieme per raggiungere un futuro sostenibile.",
    "crumbs": [
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>IA Sostenibile</span>"
    ]
  },
  {
    "objectID": "contents/core/sustainable_ai/sustainable_ai.it.html#conclusione",
    "href": "contents/core/sustainable_ai/sustainable_ai.it.html#conclusione",
    "title": "16¬† IA Sostenibile",
    "section": "16.15 Conclusione",
    "text": "16.15 Conclusione\nDobbiamo affrontare le considerazioni sulla sostenibilit√† man mano che l‚Äôintelligenza artificiale si espande rapidamente nei settori e nella societ√†. L‚Äôintelligenza artificiale promette innovazioni rivoluzionarie, ma il suo impatto ambientale minaccia la sua crescita diffusa. Questo capitolo analizza molteplici aspetti, dall‚Äôenergia e dalle emissioni agli impatti sui rifiuti e sulla biodiversit√†, che gli sviluppatori di intelligenza artificiale/apprendimento automatico devono valutare quando creano sistemi di IA responsabili.\nFondamentalmente, abbiamo bisogno di elevare la sostenibilit√† a priorit√† di progettazione primaria piuttosto che a un ripensamento. Tecniche come modelli ad alta efficienza energetica, data center alimentati da fonti rinnovabili e programmi di riciclaggio dell‚Äôhardware offrono soluzioni, ma l‚Äôimpegno olistico rimane fondamentale. Abbiamo bisogno di standard in materia di trasparenza, contabilit√† del carbonio e divulgazioni della catena di fornitura per integrare i guadagni tecnici. Tuttavia, esempi come le pratiche di efficienza 4M di Google contenenti l‚Äôuso di energia ML evidenziano che possiamo far progredire l‚Äôintelligenza artificiale di pari passo con gli obiettivi ambientali con uno sforzo concertato. Raggiungiamo questo equilibrio armonioso facendo collaborare ricercatori, aziende, regolatori e utenti in tutti i domini. L‚Äôobiettivo non √® soluzioni perfette, ma un miglioramento continuo mentre integriamo l‚ÄôIA in nuovi settori.",
    "crumbs": [
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>IA Sostenibile</span>"
    ]
  },
  {
    "objectID": "contents/core/sustainable_ai/sustainable_ai.it.html#sec-sustainable-ai-resource",
    "href": "contents/core/sustainable_ai/sustainable_ai.it.html#sec-sustainable-ai-resource",
    "title": "16¬† IA Sostenibile",
    "section": "16.16 Risorse",
    "text": "16.16 Risorse\nEcco un elenco curato di risorse per supportare studenti e insegnanti nei loro percorsi di apprendimento e insegnamento. Lavoriamo continuamente per espandere questa raccolta e presto aggiungeremo nuovi esercizi.\n\n\n\n\n\n\nSlide\n\n\n\n\n\nQueste slide sono uno strumento prezioso per gli insegnanti per tenere lezioni e per gli studenti per rivedere il materiale secondo il proprio ritmo. Incoraggiamo studenti e docenti a sfruttare queste slide per migliorare la loro comprensione e facilitare un trasferimento efficace delle conoscenze.\n\nTransparency and Sustainability.\nSustainability of TinyML.\nModel Cards for Transparency.\n\n\n\n\n\n\n\n\n\n\nVideo\n\n\n\n\n\n\nProssimamente.\n\n\n\n\n\n\n\n\n\n\nEsercizi\n\n\n\n\n\nPer rafforzare i concetti trattati in questo capitolo, abbiamo curato una serie di esercizi che sfidano gli studenti ad applicare le proprie conoscenze e ad approfondire la propria comprensione.\n\nEsercizio¬†16.1\nEsercizio¬†16.2",
    "crumbs": [
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>IA Sostenibile</span>"
    ]
  },
  {
    "objectID": "contents/core/robust_ai/robust_ai.it.html",
    "href": "contents/core/robust_ai/robust_ai.it.html",
    "title": "17¬† IA Robusta",
    "section": "",
    "text": "17.1 Panoramica\nPer IA robusta si intende la capacit√† di un sistema di mantenere le proprie prestazioni e affidabilit√† anche in presenza di errori. Un sistema di apprendimento automatico robusto √® progettato per essere tollerante ai guasti e resiliente agli errori, in grado di funzionare efficacemente anche in condizioni avverse.\nMan mano che i sistemi ML diventano sempre pi√π integrati in vari aspetti della nostra vita, dai servizi basati su cloud ai dispositivi edge e ai sistemi embedded, l‚Äôimpatto dei guasti hardware e software sulle loro prestazioni e affidabilit√† diventa pi√π significativo. In futuro, man mano che i sistemi ML diventano pi√π complessi e vengono implementati in applicazioni ancora pi√π critiche, la necessit√† di progetti robusti e tolleranti ai guasti sar√† fondamentale.\nSi prevede che i sistemi ML svolgeranno ruoli cruciali nei veicoli autonomi, nelle citt√† intelligenti, nell‚Äôassistenza sanitaria e nei domini dell‚Äôautomazione industriale. In questi domini, le conseguenze dei guasti hardware o software possono essere gravi, potenzialmente causa di perdita di vite umane, danni economici o danni ambientali.\nI ricercatori e gli ingegneri devono concentrarsi sullo sviluppo di tecniche avanzate per il rilevamento, l‚Äôisolamento e il ripristino dei guasti per mitigare questi rischi e garantire il funzionamento affidabile dei futuri sistemi ML.\nQuesto capitolo si concentrer√† in modo specifico su tre categorie principali di guasti ed errori che possono influire sulla robustezza dei sistemi ML: guasti hardware, guasti software ed errori umani.\nLe sfide e gli approcci specifici per ottenere la robustezza possono variare a seconda della scala e dei vincoli del sistema ML. I sistemi di cloud computing o data center su larga scala possono concentrarsi sulla tolleranza ai guasti e sulla resilienza tramite ridondanza, elaborazione distribuita e tecniche avanzate di rilevamento e correzione degli errori. Al contrario, i dispositivi edge con risorse limitate o i sistemi embedded affrontano sfide uniche a causa della potenza di calcolo, della memoria e delle risorse energetiche limitate.\nIndipendentemente dalla scala e dai vincoli, le caratteristiche chiave di un sistema ML robusto includono tolleranza ai guasti, resilienza agli errori e mantenimento delle prestazioni. Comprendendo e affrontando le sfide multiformi alla robustezza, possiamo sviluppare sistemi ML affidabili e sicuri in grado di navigare nelle complessit√† degli ambienti del mondo reale.\nQuesto capitolo non riguarda solo l‚Äôesplorazione di strumenti, framework e tecniche dei sistemi ML per rilevare e mitigare guasti, attacchi e cambiamenti durante la distribuzione. Si tratta di sottolineare il ruolo cruciale di ognuno di nel dare priorit√† alla resilienza durante tutto il ciclo di vita dello sviluppo dell‚ÄôIA, dalla raccolta dati e dall‚Äôaddestramento del modello all‚Äôimplementazione e al monitoraggio. Affrontando in modo proattivo le sfide alla robustezza, possiamo sbloccare il pieno potenziale delle tecnologie ML garantendone al contempo un‚Äôimplementazione sicura, affidabile e responsabile nelle applicazioni del mondo reale.\nMentre l‚ÄôIA continua a plasmare il nostro futuro, il potenziale delle tecnologie ML √® immenso. Ma √® solo quando creiamo sistemi resilienti in grado di resistere alle sfide del mondo reale che possiamo davvero sfruttare questo potenziale. Questo √® un fattore determinante per il successo e l‚Äôimpatto sociale di questa tecnologia trasformativa ed √® alla nostra portata.",
    "crumbs": [
      "<span class='chapter-number'>17</span>¬† <span class='chapter-title'>IA Robusta</span>"
    ]
  },
  {
    "objectID": "contents/core/robust_ai/robust_ai.it.html#panoramica",
    "href": "contents/core/robust_ai/robust_ai.it.html#panoramica",
    "title": "17¬† IA Robusta",
    "section": "",
    "text": "Guasti Hardware: Guasti transitori, permanenti e intermittenti possono influire sui componenti hardware di un sistema ML, corrompendo i calcoli e degradando le prestazioni.\nRobustezza del Modello: I modelli ML possono essere vulnerabili ad attacchi avversari, avvelenamento dei dati e cambiamenti di distribuzione, che possono indurre classificazioni errate mirate, alterare il comportamento appreso del modello o compromettere l‚Äôintegrit√† e l‚Äôaffidabilit√† del sistema.\nGuasti software: Bug, difetti di progettazione ed errori di implementazione nei componenti software, come algoritmi, librerie e framework, possono propagare errori e introdurre vulnerabilit√†.",
    "crumbs": [
      "<span class='chapter-number'>17</span>¬† <span class='chapter-title'>IA Robusta</span>"
    ]
  },
  {
    "objectID": "contents/core/robust_ai/robust_ai.it.html#esempi-del-mondo-reale",
    "href": "contents/core/robust_ai/robust_ai.it.html#esempi-del-mondo-reale",
    "title": "17¬† IA Robusta",
    "section": "17.2 Esempi del mondo reale",
    "text": "17.2 Esempi del mondo reale\nEcco alcuni esempi reali di casi in cui guasti nell‚Äôhardware o nel software hanno causato problemi importanti nei sistemi ML in ambienti cloud, edge ed embedded:\n\n17.2.1 Cloud\nNel febbraio 2017, Amazon Web Services (AWS) ha subito un‚Äôinterruzione significativa a causa di un errore umano durante la manutenzione. Un tecnico ha inserito inavvertitamente un comando errato, causando la disconnessione di molti server. Questa interruzione ha interrotto molti servizi AWS, tra cui l‚Äôassistente basato sull‚Äôintelligenza artificiale di Amazon, Alexa. Di conseguenza, i dispositivi basati su Alexa, come Amazon Echo e prodotti di terze parti che utilizzano Alexa Voice Service, non hanno potuto rispondere alle richieste degli utenti per diverse ore. Questo incidente evidenzia il potenziale impatto degli errori umani sui sistemi ML basati su cloud e la necessit√† di procedure di manutenzione robuste e meccanismi di sicurezza.\nIn un altro esempio (Vangal et al. 2021), Facebook ha riscontrato un problema di ‚Äúsilent data corruption (SDC)‚Äù [corruzione silenziosa dei dati] all‚Äôinterno della sua infrastruttura di query distribuita, come mostrato in Figura¬†17.1. L‚Äôinfrastruttura di Facebook include un sistema di query che preleva ed esegue query SQL e simili a SQL su pi√π set di dati utilizzando framework come Presto, Hive e Spark. Una delle applicazioni che ha utilizzato questa infrastruttura di query √® stata un‚Äôapplicazione di compressione per ridurre l‚Äôingombro degli archivi dati. In questa applicazione di compressione, i file venivano compressi quando non venivano letti e decompressi quando veniva effettuata una richiesta di lettura. Prima della decompressione, la dimensione del file veniva controllata per assicurarsi che fosse maggiore di zero, indicando un file compresso valido con contenuti.\n\nVangal, Sriram, Somnath Paul, Steven Hsu, Amit Agarwal, Saurabh Kumar, Ram Krishnamurthy, Harish Krishnamurthy, James Tschanz, Vivek De, e Chris H. Kim. 2021. ¬´Wide-Range Many-Core SoC Design in Scaled CMOS: Challenges and Opportunities¬ª. IEEE Trans. Very Large Scale Integr. VLSI Syst. 29 (5): 843‚Äì56. https://doi.org/10.1109/tvlsi.2021.3061649.\n\n\n\n\n\n\nFigura¬†17.1: Corruzione silenzioso dei dati nelle applicazioni di database. Fonte: Facebook\n\n\n\nTuttavia, in un caso, quando la dimensione del file veniva calcolata per un file valido di dimensioni diverse da zero, l‚Äôalgoritmo di decompressione ha richiamato una funzione di potenza dalla libreria Scala. Inaspettatamente, la funzione Scala ha restituito un valore di dimensione zero per il file nonostante avesse una dimensione decompressa nota diversa da zero. Di conseguenza, la decompressione non √® stata eseguita e il file non √® stato scritto nel database di output. Questo problema si √® manifestato sporadicamente, con alcune occorrenze dello stesso calcolo della dimensione del file che restituivano il valore corretto diverso da zero.\nL‚Äôimpatto di questa corruzione silenziosa dei dati √® stato significativo, portando a file mancanti e dati errati nel database di output. L‚Äôapplicazione che si basava sui file decompressi ha fallito a causa delle incongruenze dei dati. Nel caso di studio presentato nel documento, l‚Äôinfrastruttura di Facebook, che consiste in centinaia di migliaia di server che gestiscono miliardi di richieste al giorno dalla loro enorme base di utenti, ha riscontrato un problema di corruzione silenziosa dei dati. Il sistema interessato elaborava query utente, caricamenti di immagini e contenuti multimediali, che richiedevano un‚Äôesecuzione rapida, affidabile e sicura.\nQuesto caso di studio illustra come la corruzione silenziosa dei dati pu√≤ propagarsi attraverso pi√π strati di uno stack applicativo, causando perdita di dati e guasti delle applicazioni in un sistema distribuito su larga scala. La natura intermittente del problema e la mancanza di messaggi di errore espliciti lo hanno reso particolarmente difficile da diagnosticare e risolvere. Ma questo non √® limitato solo a Meta, anche altre aziende come Google che gestiscono ipercomputer IA affrontano questi problemi. Figura¬†17.2 Jeff Dean, Chief Scientist presso Google DeepMind e Google Research, parla degli SDC e del loro impatto sui sistemi di apprendimento automatico.\n\n\n\n\n\n\nFigura¬†17.2: Gli errori ‚ÄúSilent data corruption (SDC)‚Äù sono un problema importante per gli ipercomputer di IA. Fonte: Jeff Dean at MLSys 2024, Keynote (Google)\n\n\n\n\n\n17.2.2 Edge\nPer quanto riguarda esempi di guasti ed errori nei sistemi edge ML, un‚Äôarea che ha ricevuto notevole attenzione √® il dominio delle auto a guida autonoma. I veicoli a guida autonoma si basano in larga misura su algoritmi di apprendimento automatico per la percezione, il processo decisionale e il controllo, rendendoli particolarmente sensibili all‚Äôimpatto di guasti hardware e software. Negli ultimi anni, diversi incidenti di alto profilo che hanno coinvolto veicoli autonomi hanno evidenziato le sfide e i rischi associati all‚Äôimplementazione di questi sistemi in ambienti reali.\nA maggio 2016, si √® verificato un incidente mortale quando una Tesla Model S con pilota automatico si √® schiantata contro un autoarticolato bianco che attraversava l‚Äôautostrada. Il sistema Autopilot, che si basava su algoritmi di visione artificiale e apprendimento automatico, non √® riuscito a riconoscere il rimorchio bianco sullo sfondo di un cielo luminoso. Il conducente, che secondo quanto riferito stava guardando un film al momento dell‚Äôincidente, non √® intervenuto in tempo e il veicolo √® entrato in collisione con il rimorchio a tutta velocit√†. Questo incidente ha sollevato preoccupazioni sui limiti dei sistemi di percezione basati sull‚Äôintelligenza artificiale e sulla necessit√† di solidi meccanismi di sicurezza nei veicoli autonomi. Ha inoltre evidenziato l‚Äôimportanza della consapevolezza del conducente e la necessit√† di linee guida chiare sull‚Äôuso delle funzionalit√† di guida semi-autonoma, come mostrato in Figura¬†17.3.\n\n\n\n\n\n\nFigura¬†17.3: Tesla nell‚Äôincidente mortale in California era in modalit√† Autopilot. Fonte: BBC News\n\n\n\nA marzo 2018, un veicolo di prova a guida autonoma di Uber ha investito e ucciso un pedone che attraversava la strada a Tempe, in Arizona. L‚Äôincidente √® stato causato da un difetto software nel sistema di riconoscimento degli oggetti del veicolo, che non √® riuscito a identificare i pedoni in modo appropriato per evitarli come ostacoli. L‚Äôautista di sicurezza, che avrebbe dovuto monitorare il funzionamento del veicolo e intervenire se necessario, √® stato trovato distratto durante l‚Äôincidente. Questo incidente ha portato ad un‚Äôampia revisione del programma di guida autonoma di Uber e ha sollevato dubbi sulla prontezza della tecnologia dei veicoli autonomi per le strade pubbliche. Ha inoltre sottolineato la necessit√† di rigorosi test, convalide e misure di sicurezza nello sviluppo e nell‚Äôimplementazione di sistemi di guida autonoma basati sull‚Äôintelligenza artificiale.\nNel 2021, Tesla ha dovuto affrontare un controllo pi√π rigoroso a seguito di diversi incidenti che hanno coinvolto veicoli in modalit√† Autopilot. Alcuni di questi incidenti sono stati attribuiti a problemi con la capacit√† del sistema Autopilot di rilevare e rispondere a determinate situazioni stradali, come veicoli di emergenza fermi o ostacoli sulla strada. Ad esempio, nell‚Äôaprile 2021, una Tesla Model S si √® schiantata contro un albero in Texas, uccidendo due passeggeri. I primi rapporti suggerivano che nessuno si trovasse al posto di guida al momento dell‚Äôincidente, sollevando interrogativi sull‚Äôuso e il potenziale uso improprio delle funzionalit√† Autopilot. Questi incidenti evidenziano le sfide in corso nello sviluppo di sistemi di guida autonoma affidabili e robusti e la necessit√† di normative chiare e di istruzione dei consumatori in merito alle capacit√† e ai limiti di queste tecnologie.\n\n\n17.2.3 Embedded\nI sistemi embedded, che spesso operano in ambienti con risorse limitate e applicazioni critiche per la sicurezza, hanno da tempo dovuto affrontare sfide legate a guasti hardware e software. Poich√© le tecnologie di IA e apprendimento automatico sono sempre pi√π integrate in questi sistemi, il potenziale di guasti ed errori assume nuove dimensioni, con l‚Äôaggiunta di complessit√† degli algoritmi di IA e la natura critica delle applicazioni in cui vengono distribuiti.\nConsideriamo alcuni esempi, a partire dall‚Äôesplorazione dello spazio. La missione Mars Polar Lander della NASA nel 1999 ha subito un guasto catastrofico a causa di un errore software nel sistema di rilevamento dell‚Äôatterraggio (Figura¬†17.4). Il software di bordo della navicella spaziale ha interpretato erroneamente il rumore proveniente dall‚Äôapertura delle sue gambe di atterraggio come un segnale di atterraggio sulla superficie marziana. Di conseguenza, la navicella ha spento prematuramente i suoi motori, causando lo schianto sulla superficie. Questo incidente evidenzia l‚Äôimportanza critica di una progettazione software solida e di test approfonditi nei sistemi embedded, in particolare quelli che operano in ambienti remoti e ostili. Poich√© le capacit√† di IA sono integrate nelle future missioni spaziali, garantire l‚Äôaffidabilit√† e la tolleranza ai guasti di questi sistemi sar√† fondamentale per il successo della missione.\n\n\n\n\n\n\nFigura¬†17.4: La missione fallita della NASA Mars Polar Lander nel 1999 √® costata oltre $200M. Fonte: SlashGear\n\n\n\nTornando sulla Terra, nel 2015, un Boeing 787 Dreamliner ha subito un arresto elettrico completo durante un volo a causa di un bug del software nelle sue unit√† di controllo del generatore. Questo incidente sottolinea come i guasti software possano avere gravi conseguenze nei sistemi integrati complessi come quelli degli aeromobili. Poich√© le tecnologie di IA sono sempre pi√π applicate all‚Äôaviazione, come nei sistemi di volo autonomi e nella manutenzione predittiva, garantire la robustezza e l‚Äôaffidabilit√† di questi sistemi sar√† fondamentale per la sicurezza dei passeggeri.\n\n‚ÄúSe le quattro unit√† di controllo del generatore principale (associate ai generatori montati sul motore) fossero accese contemporaneamente, dopo 248 giorni di alimentazione continua, tutte e quattro le GCU entrerebbero in modalit√† fail-safe contemporaneamente, con conseguente perdita di tutta l‚Äôalimentazione elettrica CA indipendentemente dalla fase di volo.‚Äù ‚Äì Direttiva della Federal Aviation Administration (2015)\n\nPoich√© le capacit√† di IA si integrano sempre di pi√π nei sistemi embedded, il potenziale di guasti ed errori diventa pi√π complesso e grave. Si immagini un pacemaker intelligente che ha un improvviso problema tecnico. Un paziente potrebbe morire a causa di tale effetto. Pertanto, gli algoritmi AI, come quelli utilizzati per la percezione, il processo decisionale e il controllo, introducono nuove fonti di potenziali guasti, come problemi relativi ai dati, incertezze del modello e comportamenti inaspettati nei casi limite. Inoltre, la natura opaca di alcuni modelli di IA pu√≤ rendere difficile identificare e diagnosticare i guasti quando si verificano.",
    "crumbs": [
      "<span class='chapter-number'>17</span>¬† <span class='chapter-title'>IA Robusta</span>"
    ]
  },
  {
    "objectID": "contents/core/robust_ai/robust_ai.it.html#guasti-hardware",
    "href": "contents/core/robust_ai/robust_ai.it.html#guasti-hardware",
    "title": "17¬† IA Robusta",
    "section": "17.3 Guasti Hardware",
    "text": "17.3 Guasti Hardware\nI guasti hardware rappresentano una sfida significativa nei sistemi informatici, inclusi i sistemi tradizionali e ML. Questi guasti si verificano quando componenti fisici, come processori, moduli di memoria, dispositivi di archiviazione o interconnessioni, funzionano male o si comportano in modo anomalo. I guasti hardware possono causare calcoli errati, danneggiamento dei dati, crash del sistema o guasti completi del sistema, compromettendo l‚Äôintegrit√† e l‚Äôaffidabilit√† dei calcoli eseguiti (Jha et al. 2019). Un guasto completo del sistema si riferisce a una situazione in cui l‚Äôintero sistema informatico diventa non reattivo o inutilizzabile a causa di un malfunzionamento hardware critico. Questo tipo di guasto √® il pi√π grave, poich√© rende il sistema inutilizzabile e pu√≤ portare alla perdita o al danneggiamento dei dati, richiedendo un intervento manuale per riparare o sostituire i componenti difettosi.\nComprendere la tassonomia dei guasti hardware √® essenziale per chiunque lavori con sistemi informatici, in particolare nel contesto dei sistemi ML. I sistemi ML si basano su architetture hardware complesse e calcoli su larga scala per addestrare e distribuire modelli che apprendono dai dati e fanno previsioni o decisioni intelligenti. Tuttavia, i guasti hardware possono introdurre errori e incongruenze nella pipeline MLOps, influenzando l‚Äôaccuratezza, la robustezza e l‚Äôaffidabilit√† dei modelli addestrati (G. Li et al. 2017).\nConoscere i diversi tipi di guasti hardware, i loro meccanismi e il loro potenziale impatto sul comportamento del sistema √® fondamentale per sviluppare strategie efficaci per rilevarli, mitigarli e ripristinarli. Questa conoscenza √® necessaria per progettare sistemi di elaborazione tolleranti ai guasti, implementare algoritmi ML robusti e garantire l‚Äôaffidabilit√† complessiva delle applicazioni basate su ML.\nLe sezioni seguenti esploreranno le tre categorie principali di guasti hardware: transitori, permanenti e intermittenti. Discuteremo le loro definizioni, caratteristiche, cause, meccanismi ed esempi di come si manifestano nei sistemi di elaborazione. Tratteremo anche tecniche di rilevamento e mitigazione specifiche per ogni tipo di guasto.\n\nGuasti Transitori: I guasti transitori sono temporanei e non ricorrenti. Sono spesso causati da fattori esterni come raggi cosmici, interferenze elettromagnetiche o fluttuazioni di potenza. Un esempio comune di guasto transitorio √® un bit flip, in cui un singolo bit in una posizione di memoria o registro cambia il suo valore in modo imprevisto. I guasti transitori possono causare calcoli errati o corruzione dei dati, ma non causano danni permanenti all‚Äôhardware.\nGuasti permanenti: I guasti permanenti, chiamati anche errori hard, sono irreversibili e persistono nel tempo. Sono in genere causati da difetti fisici o usura dei componenti hardware. Esempi di guasti permanenti includono guasti bloccati, in cui un bit o un segnale √® impostato in modo permanente su un valore specifico (ad esempio, sempre 0 o sempre 1) e guasti del dispositivo, come un processore malfunzionante o un modulo di memoria danneggiato. I guasti permanenti possono causare un guasto completo del sistema o un significativo degrado delle prestazioni.\nGuasti Intermittenti: I guasti intermittenti sono guasti ricorrenti che compaiono e scompaiono in modo intermittente. Condizioni hardware instabili, come connessioni allentate, componenti obsoleti o difetti di fabbricazione, spesso ne sono la causa. I guasti intermittenti possono essere difficili da diagnosticare e riprodurre perch√© possono verificarsi sporadicamente e in condizioni specifiche. Esempi includono cortocircuiti intermittenti o problemi di resistenza dei contatti. I guasti intermittenti possono portare a un comportamento imprevedibile del sistema e a errori intermittenti.\n\nAlla fine di questa discussione, i lettori avranno una solida comprensione della tassonomia dei guasti e della sua rilevanza per i sistemi di elaborazione e ML tradizionali. Questa base li aiuter√† a prendere decisioni informate durante la progettazione, l‚Äôimplementazione e la distribuzione di soluzioni tolleranti ai guasti, migliorando l‚Äôaffidabilit√† e la credibilit√† dei loro sistemi di elaborazione e delle applicazioni ML.\n\n17.3.1 Guasti Transitori\nI guasti transitori nell‚Äôhardware possono manifestarsi in varie forme, ciascuna con le sue caratteristiche e cause uniche. Questi guasti sono di natura temporanea e non causano danni permanenti ai componenti hardware.\n\nDefinizione e Caratteristiche\nAlcuni dei tipi comuni di guasti transitori includono Single Event Upset (SEU) causati da radiazioni ionizzanti, fluttuazioni di tensione (Reddi e Gupta 2013) dovute a rumore dell‚Äôalimentatore o interferenze elettromagnetiche, ‚ÄúElectromagnetic Interference (EMI)‚Äù indotte da campi elettromagnetici esterni, ‚ÄúElectrostatic Discharge (ESD)‚Äù risultanti da un improvviso flusso di elettricit√† statica, diafonia causata da accoppiamento di segnali involontari, rimbalzo di massa innescato dalla commutazione simultanea di pi√π uscite, violazioni di temporizzazione dovute a violazioni dei vincoli di temporizzazione del segnale ed errori soft nella logica combinatoria che influenzano l‚Äôuscita dei circuiti logici (Mukherjee, Emer, e Reinhardt 2005). Comprendere questi diversi tipi di guasti transitori √® fondamentale per progettare sistemi hardware robusti e resilienti che possano mitigarne l‚Äôimpatto e garantire un funzionamento affidabile.\n\nReddi, Vijay Janapa, e Meeta Sharma Gupta. 2013. Resilient Architecture Design for Voltage Variation. Springer International Publishing. https://doi.org/10.1007/978-3-031-01739-1.\n\nMukherjee, S. S., J. Emer, e S. K. Reinhardt. 2005. ¬´The Soft Error Problem: An Architectural Perspective¬ª. In 11th International Symposium on High-Performance Computer Architecture, 243‚Äì47. IEEE; IEEE. https://doi.org/10.1109/hpca.2005.37.\nTutti questi guasti transitori sono caratterizzati dalla loro breve durata e dalla loro natura non permanente. Non persistono n√© lasciano alcun impatto duraturo sull‚Äôhardware. Tuttavia, possono comunque portare a calcoli errati, corruzione dei dati o comportamento scorretto del sistema se non gestiti correttamente.\n\n\n\nCause di Guasti Transitori\nI guasti transitori possono essere attribuiti a vari fattori esterni. Una causa comune sono i raggi cosmici, particelle ad alta energia provenienti dallo spazio. Quando queste particelle colpiscono aree sensibili dell‚Äôhardware, come celle di memoria o transistor, possono indurre disturbi di carica che alterano i dati memorizzati o trasmessi. Ci√≤ √® illustrato in Figura¬†17.5. Un‚Äôaltra causa di guasti transitori √® l‚Äôelectromagnetic interference (EMI) [interferenza elettromagnetica] da dispositivi vicini o fluttuazioni di potenza. L‚ÄôEMI pu√≤ accoppiarsi con i circuiti e causare picchi di tensione o glitch che interrompono temporaneamente il normale funzionamento dell‚Äôhardware.\n\n\n\n\n\n\nFigura¬†17.5: Meccanismo di Occorrenza di Guasti Transitori Hardware. Fonte: NTT\n\n\n\n\n\nMeccanismi di Guasti Transitori\nI guasti transitori possono manifestarsi attraverso meccanismi diversi a seconda del componente hardware interessato. Nei dispositivi di memoria come DRAM o SRAM, i guasti transitori spesso portano a inversioni di bit, in cui un singolo bit cambia il suo valore da 0 a 1 o viceversa. Ci√≤ pu√≤ corrompere i dati o le istruzioni archiviati. Nei circuiti logici, i guasti transitori possono causare glitch o picchi di tensione che si propagano attraverso la logica combinatoria, con conseguenti output o segnali di controllo errati. I guasti transitori possono anche influenzare i canali di comunicazione, causando errori di bit o perdite di pacchetti durante la trasmissione dei dati.\n\n\nImpatto sui Sistemi ML\nUn esempio comune di guasto transitorio √® un‚Äôinversione di bit nella memoria principale. Se una struttura dati importante o un‚Äôistruzione critica viene archiviata nella posizione di memoria interessata, pu√≤ portare a calcoli errati o a un comportamento errato del programma. Se si verifica un guasto transitorio nella memoria che archivia i pesi o i gradienti del modello. Ad esempio, un bit flip nella memoria che memorizza un contatore di loop pu√≤ causare l‚Äôesecuzione indefinita del loop o la sua terminazione prematura. Errori transitori nei registri di controllo o nei bit di flag possono alterare il flusso di esecuzione del programma, causando salti imprevisti o decisioni di diramazione errate. Nei sistemi di comunicazione, gli errori transitori possono danneggiare i pacchetti di dati trasmessi, causando ritrasmissioni o perdita di dati.\nNei sistemi ML, gli errori transitori possono avere implicazioni significative durante la fase di training (He et al. 2023). Il training ML comporta calcoli iterativi e aggiornamenti dei parametri del modello basati su grandi set di dati. Se si verifica un errore transitorio nella memoria dei pesi o dei gradienti del modello, pu√≤ causare aggiornamenti errati e compromettere la convergenza e l‚Äôaccuratezza del processo di training. Figura¬†17.6 mostra un esempio concreto tratto dalla flotta di produzione di Google, in cui un‚Äôanomalia SDC ha causato una differenza significativa nella norma del gradiente.\n\n\n\n\n\n\nFigura¬†17.6: SDC nella fase di training ML determina anomalie nella norma del gradiente. Fonte: Jeff Dean, MLSys 2024 Keynote (Google)\n\n\n\nAd esempio, un‚Äôinversione di bit nella matrice dei pesi di una rete neurale pu√≤ far s√¨ che il modello apprenda pattern o associazioni errati, con conseguente peggioramento delle prestazioni (Wan et al. 2021). Errori transitori nella pipeline dei dati, come la corruzione dei campioni di training o delle etichette, possono anche introdurre rumore e influire sulla qualit√† del modello appreso.\n\nWan, Zishen, Aqeel Anwar, Yu-Shun Hsiao, Tianyu Jia, Vijay Janapa Reddi, e Arijit Raychowdhury. 2021. ¬´Analyzing and Improving Fault Tolerance of Learning-Based Navigation Systems¬ª. In 2021 58th ACM/IEEE Design Automation Conference (DAC), 841‚Äì46. IEEE; IEEE. https://doi.org/10.1109/dac18074.2021.9586116.\nDurante la fase di inferenza, gli errori transitori possono influire sull‚Äôaffidabilit√† e l‚Äôattendibilit√† delle previsioni ML. Se si verifica un errore transitorio nella memoria dei parametri del modello addestrato o nel calcolo dei risultati dell‚Äôinferenza, pu√≤ portare a previsioni errate o incoerenti. Ad esempio, un‚Äôinversione di bit nei valori di attivazione di una rete neurale pu√≤ alterare l‚Äôoutput finale di classificazione o regressione (Mahmoud et al. 2020).\nNelle applicazioni ‚Äúsafety-critical‚Äù, come i veicoli autonomi o la diagnosi medica, i guasti transitori durante l‚Äôinferenza possono avere gravi conseguenze, portando a decisioni o azioni errate (G. Li et al. 2017; Jha et al. 2019). Garantire la resilienza dei sistemi ML contro i guasti transitori √® fondamentale per mantenere l‚Äôintegrit√† e l‚Äôaffidabilit√† delle previsioni.\n\nLi, Guanpeng, Siva Kumar Sastry Hari, Michael Sullivan, Timothy Tsai, Karthik Pattabiraman, Joel Emer, e Stephen W. Keckler. 2017. ¬´Understanding error propagation in deep learning neural network (DNN) accelerators and applications¬ª. In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, 1‚Äì12. ACM. https://doi.org/10.1145/3126908.3126964.\n\nCourbariaux, Matthieu, Itay Hubara, Daniel Soudry, Ran El-Yaniv, e Yoshua Bengio. 2016. ¬´Binarized neural networks: Training deep neural networks with weights and activations constrained to+ 1 or-1¬ª. arXiv preprint arXiv:1602.02830.\n\nAygun, Sercan, Ece Olcay Gunes, e Christophe De Vleeschouwer. 2021. ¬´Efficient and robust bitstream processing in binarised neural networks¬ª. Electron. Lett. 57 (5): 219‚Äì22. https://doi.org/10.1049/ell2.12045.\nAll‚Äôaltro estremo, in ambienti con risorse limitate come TinyML, le ‚ÄúBinarized Neural Networks [BNNs]‚Äù [reti neurali binarizzate] (Courbariaux et al. 2016) sono emerse come una soluzione promettente. Le BNN rappresentano pesi di rete in precisione a bit singolo, offrendo efficienza computazionale e tempi di inferenza pi√π rapidi. Tuttavia, questa rappresentazione binaria rende le BNN fragili agli errori di inversione di bit sui pesi della rete. Ad esempio, lavori precedenti (Aygun, Gunes, e De Vleeschouwer 2021) hanno dimostrato che un‚Äôarchitettura BNN a due strati nascosti per un‚Äôattivit√† semplice come la classificazione MNIST subisce un degrado delle prestazioni dal 98% di accuratezza del test al 70% quando vengono inseriti errori soft di inversione di bit casuali tramite pesi del modello con una probabilit√† del 10%.\nPer affrontare tali problemi √® necessario considerare tecniche di training ‚Äúflip-aware‚Äù o sfruttare paradigmi di elaborazione emergenti (ad esempio, elaborazione stocastica) per migliorare la tolleranza ai guasti e la robustezza, di cui parleremo in Sezione 17.3.4. Le direzioni di ricerca future mirano a sviluppare architetture ibride, nuove funzioni di attivazione e funzioni di perdita su misura per colmare il divario di accuratezza rispetto ai modelli a precisione completa mantenendo al contempo la loro efficienza computazionale.\n\n\n\n17.3.2 Guasti Permanenti\nI guasti permanenti sono difetti hardware che persistono e causano danni irreversibili ai componenti interessati. Questi guasti sono caratterizzati dalla loro natura persistente e richiedono la riparazione o la sostituzione dell‚Äôhardware difettoso per ripristinare la normale funzionalit√† del sistema.\n\nDefinizione e Caratteristiche\nI guasti permanenti sono difetti hardware che causano malfunzionamenti persistenti e irreversibili nei componenti interessati. Il componente difettoso rimane non operativo finch√© un guasto permanente non viene riparato o sostituito. Questi guasti sono caratterizzati dalla loro natura coerente e riproducibile, il che significa che il comportamento difettoso viene osservato ogni volta che il componente interessato viene utilizzato. I guasti permanenti possono avere un impatto su vari componenti hardware, come processori, moduli di memoria, dispositivi di archiviazione o interconnessioni, causando crash del sistema, danneggiamento dei dati o guasto completo del sistema.\nUn esempio notevole di guasto permanente √® il bug Intel FDIV, scoperto nel 1994. Il bug FDIV era un difetto in alcune unit√† di divisione a virgola mobile (FDIV) dei processori Intel Pentium. Il bug causava risultati errati per specifiche operazioni di divisione, portando a calcoli imprecisi.\nIl bug FDIV si √® verificato a causa di un errore nella tabella di ricerca utilizzata dall‚Äôunit√† di divisione. In rari casi, il processore recuperava un valore errato dalla tabella di ricerca, con un risultato leggermente meno preciso del previsto. Ad esempio, Figura¬†17.7 mostra una frazione 4195835/3145727 tracciata su un processore Pentium con l‚Äôerrore permanente FDIV. Le regioni triangolari sono quelle in cui si sono verificati calcoli errati. Idealmente, tutti i valori corretti verrebbero arrotondati a 1,3338, ma i risultati errati mostrano 1,3337, indicando un errore nella quinta cifra.\nSebbene l‚Äôerrore fosse piccolo, poteva accumularsi su molte operazioni di divisione, portando a significative imprecisioni nei calcoli matematici. L‚Äôimpatto del bug FDIV era significativo, soprattutto per le applicazioni che si basavano in modo massiccio sulla divisione precisa in virgola mobile, come simulazioni scientifiche, calcoli finanziari e progettazione assistita da computer. Il bug ha portato a risultati errati, che potrebbero avere gravi conseguenze in settori come la finanza o l‚Äôingegneria.\n\n\n\n\n\n\nFigura¬†17.7: Processore Intel Pentium con errore permanente FDIV. Le regioni triangolari sono quelle in cui si sono verificati calcoli errati. Fonte: Byte Magazine\n\n\n\nIl bug Intel FDIV √® un monito per il potenziale impatto di guasti permanenti sui sistemi ML. Nel contesto del ML, guasti permanenti nei componenti hardware possono portare a calcoli errati, influenzando l‚Äôaccuratezza e l‚Äôaffidabilit√† dei modelli. Ad esempio, se un sistema ML si basa su un processore con un‚Äôunit√† a virgola mobile difettosa, simile al bug Intel FDIV, potrebbe introdurre errori nei calcoli eseguiti durante l‚Äôaddestramento o l‚Äôinferenza.\nQuesti errori possono propagarsi attraverso il modello, portando a previsioni imprecise o apprendimento distorto. Nelle applicazioni in cui il ML viene utilizzato per attivit√† critiche, come la guida autonoma, la diagnosi medica o le previsioni finanziarie, le conseguenze di calcoli errati dovuti a guasti permanenti possono essere gravi.\n√à fondamentale che i professionisti del ML siano consapevoli del potenziale impatto dei guasti permanenti e incorporino tecniche di tolleranza ai guasti, come ridondanza hardware, meccanismi di rilevamento e correzione degli errori e progettazione di algoritmi robusti, per mitigare i rischi associati a questi guasti. Inoltre, test approfonditi e convalida dei componenti hardware ML possono aiutare a identificare e risolvere i guasti permanenti prima che influiscano sulle prestazioni e l‚Äôaffidabilit√† del sistema.\n\n\nCause dei Guasti Permanenti\nI guasti permanenti possono derivare da diverse cause, tra cui difetti di fabbricazione e meccanismi di usura. I difetti di fabbricazione sono difetti intrinseci introdotti durante il processo di fabbricazione dei componenti hardware. Questi difetti includono incisione impropria, doping non corretto o contaminazione, che portano a componenti non funzionali o parzialmente funzionali.\nD‚Äôaltro canto, i meccanismi di usura si verificano nel tempo man mano che i componenti hardware sono sottoposti a un uso prolungato e a stress. Fattori come elettromigrazione, rottura dell‚Äôossido o stress termico possono causare una graduale degradazione dei componenti, portando infine a guasti permanenti.\n\n\nMeccanismi dei Guasti Permanenti\nI guasti permanenti possono manifestarsi attraverso vari meccanismi, a seconda della natura e della posizione del guasto. Gli ‚ÄúStuck-at fault‚Äù [guasti bloccati] (Seong et al. 2010) sono guasti permanenti comuni in cui un segnale o una cella di memoria rimane fissata a un valore particolare (0 o 1) indipendentemente dagli input, come illustrato in Figura¬†17.8.\n\nSeong, Nak Hee, Dong Hyuk Woo, Vijayalakshmi Srinivasan, Jude A. Rivers, e Hsien-Hsin S. Lee. 2010. ¬´SAFER: Stuck-at-fault Error Recovery for Memories¬ª. In 2010 43rd Annual IEEE/ACM International Symposium on Microarchitecture, 115‚Äì24. IEEE; IEEE. https://doi.org/10.1109/micro.2010.46.\n\n\n\n\n\n\nFigura¬†17.8: Modello di Guasto Bloccato nei Circuiti Digitali. Fonte: Accendo Reliability\n\n\n\nI guasti bloccati possono verificarsi in porte logiche, celle di memoria o interconnessioni, causando calcoli errati o corruzione dei dati. Un altro meccanismo sono i guasti del dispositivo, in cui un componente, come un transistor o una cella di memoria, cessa completamente di funzionare. Ci√≤ pu√≤ essere dovuto a difetti di fabbricazione o grave usura. I guasti di ‚Äúbridging‚Äù si verificano quando due o pi√π linee di segnale sono collegate involontariamente, causando cortocircuiti o un comportamento logico errato.\nOltre ai guasti stuck-at, ci sono diversi altri tipi di guasti permanenti che possono influenzare i circuiti digitali e che possono avere un impatto su un sistema ML. I guasti di ritardo possono causare il superamento del limite specificato del ritardo di propagazione di un segnale, portando a violazioni di temporizzazione. I guasti di interconnessione, come guasti aperti (fili rotti), guasti resistivi (resistenza aumentata) o guasti capacitivi (capacit√† aumentata), possono causare problemi di integrit√† del segnale o violazioni di temporizzazione. Le celle di memoria possono anche subire vari guasti, tra cui guasti di transizione (impossibilit√† di cambiare stato), guasti di accoppiamento (interferenza tra celle adiacenti) e guasti sensibili al pattern di vicinato (guasti che dipendono dai valori delle celle vicine). Altri guasti permanenti possono verificarsi nella rete di alimentazione o nella rete di distribuzione del clock, influenzando la funzionalit√† e la temporizzazione del circuito.\n\n\nImpatto sui Sistemi ML\nI guasti permanenti possono influire gravemente sul comportamento e l‚Äôaffidabilit√† dei sistemi di elaborazione. Ad esempio, un guasto nell‚Äôunit√† logica aritmetica (ALU) di un processore pu√≤ causare calcoli errati, portando a risultati errati o crash del sistema. Un guasto permanente in un modulo di memoria, in una specifica cella di memoria, pu√≤ danneggiare i dati archiviati, causando la perdita di dati o un comportamento errato del programma. Nei dispositivi di archiviazione, guasti permanenti come settori danneggiati o guasti del dispositivo possono causare l‚Äôinaccessibilit√† dei dati o la perdita completa delle informazioni archiviate. I guasti permanenti di interconnessione possono interrompere i canali di comunicazione, causando il danneggiamento dei dati o il blocco del sistema.\nI guasti permanenti possono influire significativamente sui sistemi ML durante le fasi di addestramento e inferenza. Durante l‚Äôaddestramento, guasti permanenti nelle unit√† di elaborazione o nella memoria possono causare calcoli errati, con conseguenti modelli danneggiati o non ottimali (He et al. 2023). Inoltre, i guasti nei dispositivi di archiviazione possono corrompere i dati di training o i parametri del modello archiviati, causando la perdita di dati o incongruenze del modello (He et al. 2023).\n\nZhang, Jeff Jun, Tianyu Gu, Kanad Basu, e Siddharth Garg. 2018. ¬´Analyzing and mitigating the impact of permanent faults on a systolic array based neural network accelerator¬ª. In 2018 IEEE 36th VLSI Test Symposium (VTS), 1‚Äì6. IEEE; IEEE. https://doi.org/10.1109/vts.2018.8368656.\nDurante l‚Äôinferenza, i guasti permanenti possono influire sull‚Äôaffidabilit√† e la correttezza delle previsioni ML. I guasti nelle unit√† di elaborazione possono produrre risultati errati o causare guasti del sistema, mentre i guasti nella memoria che archivia i parametri del modello possono portare all‚Äôutilizzo di modelli corrotti o obsoleti per l‚Äôinferenza (J. J. Zhang et al. 2018).\nPer mitigare l‚Äôimpatto dei guasti permanenti nei sistemi ML, devono essere impiegate tecniche di tolleranza ai guasti sia a livello hardware che software. La ridondanza hardware, come la duplicazione di componenti critici o l‚Äôutilizzo di codici di correzione degli errori (Kim, Sullivan, e Erez 2015), pu√≤ aiutare a rilevare e ripristinare i guasti permanenti. Le tecniche software, come i meccanismi di checkpoint e riavvio (Egwutuoha et al. 2013), possono consentire al sistema di recuperare da guasti permanenti tornando a uno stato salvato in precedenza. Il monitoraggio, il test e la manutenzione regolari dei sistemi ML possono aiutare a identificare e sostituire i componenti difettosi prima che causino interruzioni significative.\n\nKim, Jungrae, Michael Sullivan, e Mattan Erez. 2015. ¬´Bamboo ECC: Strong, safe, and flexible codes for reliable computer memory¬ª. In 2015 IEEE 21st International Symposium on High Performance Computer Architecture (HPCA), 101‚Äì12. IEEE; IEEE. https://doi.org/10.1109/hpca.2015.7056025.\n\nEgwutuoha, Ifeanyi P., David Levy, Bran Selic, e Shiping Chen. 2013. ¬´A survey of fault tolerance mechanisms and checkpoint/restart implementations for high performance computing systems¬ª. The Journal of Supercomputing 65 (3): 1302‚Äì26. https://doi.org/10.1007/s11227-013-0884-0.\nProgettare sistemi ML tenendo a mente la tolleranza ai guasti √® fondamentale per garantirne l‚Äôaffidabilit√† e la robustezza in presenza di guasti permanenti. Ci√≤ pu√≤ comportare l‚Äôincorporazione di ridondanza, meccanismi di rilevamento e correzione degli errori e strategie di sicurezza nell‚Äôarchitettura del sistema. Affrontando in modo proattivo le sfide poste dai guasti permanenti, i sistemi ML possono mantenere la loro integrit√†, accuratezza e affidabilit√†, anche di fronte a guasti hardware.\n\n\n\n17.3.3 Guasti Intermittenti\nI guasti intermittenti sono guasti hardware che si verificano sporadicamente e in modo imprevedibile in un sistema. Un esempio √® illustrato in Figura¬†17.9, dove le crepe nel materiale possono introdurre una maggiore resistenza [elettrica] nei circuiti. Questi guasti sono particolarmente difficili da rilevare e diagnosticare perch√© compaiono e scompaiono in modo intermittente, rendendo difficile riprodurre e isolare la causa principale. I guasti intermittenti possono causare instabilit√† del sistema, corruzione dei dati e degrado delle prestazioni.\n\n\n\n\n\n\nFigura¬†17.9: Maggiore resistenza dovuta a un guasto intermittente, ovvero una crepa tra la protuberanza di rame e la saldatura del package. Fonte: Constantinescu\n\n\n\n\nDefinizione e Caratteristiche\nI guasti intermittenti sono caratterizzati dalla loro natura sporadica e non deterministica. Si verificano in modo irregolare e possono apparire e scomparire spontaneamente, con durate e frequenze variabili. Questi guasti non si manifestano in modo coerente ogni volta che viene utilizzato il componente interessato, il che li rende pi√π difficili da rilevare rispetto ai guasti permanenti. I guasti intermittenti possono interessare vari componenti hardware, tra cui processori, moduli di memoria, dispositivi di archiviazione o interconnessioni. Possono causare errori transitori, danneggiamento dei dati o comportamento imprevisto del sistema.\nI guasti intermittenti possono avere un impatto significativo sul comportamento e l‚Äôaffidabilit√† dei sistemi di elaborazione (Rashid, Pattabiraman, e Gopalakrishnan 2015). Ad esempio, un guasto intermittente nella logica di controllo di un processore pu√≤ causare un flusso di programma irregolare, portando a calcoli errati o blocchi del sistema. I guasti intermittenti nei moduli di memoria possono danneggiare i valori dei dati, con conseguente esecuzione errata del programma o incoerenze nei dati. Nei dispositivi di archiviazione, i guasti intermittenti possono causare errori di lettura/scrittura o perdita di dati. Errori intermittenti nei canali di comunicazione possono causare corruzione dei dati, perdita di pacchetti o problemi di connettivit√† intermittenti. Questi errori possono causare crash del sistema, problemi di integrit√† dei dati o degrado delle prestazioni, a seconda della gravit√† e della frequenza degli errori intermittenti.\n\n‚Äî‚Äî‚Äî. 2015. ¬´Characterizing the Impact of Intermittent Hardware Faults on Programs¬ª. IEEE Trans. Reliab. 64 (1): 297‚Äì310. https://doi.org/10.1109/tr.2014.2363152.\n\n\nCause degli Errori Intermittenti\nI guasti intermittenti possono derivare da diverse cause, sia interne che esterne, ai componenti hardware (Constantinescu 2008). Una causa comune √® l‚Äôinvecchiamento e l‚Äôusura dei componenti. Man mano che i dispositivi elettronici invecchiano, diventano pi√π suscettibili a guasti intermittenti dovuti a meccanismi di degradazione come elettromigrazione, rottura dell‚Äôossido o affaticamento dei giunti di saldatura.\n\nConstantinescu, Cristian. 2008. ¬´Intermittent faults and effects on reliability of integrated circuits¬ª. In 2008 Annual Reliability and Maintainability Symposium, 370‚Äì74. IEEE; IEEE. https://doi.org/10.1109/rams.2008.4925824.\nAnche difetti di fabbricazione o variazioni di processo possono causare guasti intermittenti, in cui componenti marginali o borderline possono presentare guasti sporadici in condizioni specifiche, come mostrato in Figura¬†17.10.\n\n\n\n\n\n\nFigura¬†17.10: Guasto intermittente indotto da residui in un chip DRAM. Fonte: Hynix Semiconductor\n\n\n\nFattori ambientali, come fluttuazioni di temperatura, umidit√† o vibrazioni, possono innescare guasti intermittenti alterando le caratteristiche elettriche dei componenti. Collegamenti allentati o degradati, come quelli nei connettori o nei circuiti stampati, possono causare guasti intermittenti.\n\n\nMeccanismi dei Guasti Intermittenti\nI guasti intermittenti possono manifestarsi attraverso vari meccanismi, a seconda della causa sottostante e del componente interessato. Un meccanismo √® il circuito aperto o cortocircuito intermittente, in cui un percorso o una connessione del segnale viene temporaneamente interrotto o cortocircuitato, causando un comportamento irregolare. Un altro meccanismo √® il guasto di ritardo intermittente (J. Zhang et al. 2018), in cui la temporizzazione dei segnali o i ritardi di propagazione diventano incoerenti, causando problemi di sincronizzazione o calcoli errati. I guasti intermittenti possono manifestarsi come bit flip [inversioni] transitori o errori soft nelle celle di memoria o nei registri, causando corruzione dei dati o esecuzione errata del programma.\n\nZhang, Jeff, Kartheek Rangineni, Zahra Ghodsi, e Siddharth Garg. 2018. ¬´ThUnderVolt: Enabling Aggressive Voltage Underscaling and Timing Error Resilience for Energy Efficient Deep Learning Accelerators¬ª. In 2018 55th ACM/ESDA/IEEE Design Automation Conference (DAC), 1‚Äì6. IEEE. https://doi.org/10.1109/dac.2018.8465918.\n\n\nImpatto sui Sistemi ML\nNel contesto dei sistemi ML, i guasti intermittenti possono introdurre sfide significative e avere un impatto sull‚Äôaffidabilit√† e le prestazioni del sistema. Durante la fase di addestramento, i guasti intermittenti nelle unit√† di elaborazione o nella memoria possono portare a incongruenze nei calcoli, con conseguenti gradienti e aggiornamenti del peso errati o rumorosi. Ci√≤ pu√≤ influire sulla convergenza e l‚Äôaccuratezza del processo di addestramento, portando a modelli sub-ottimali o instabili. Errori intermittenti di archiviazione o recupero dei dati possono corrompere i dati di training, introducendo rumore o errori che degradano la qualit√† dei modelli addestrati (He et al. 2023).\n\nHe, Yi, Mike Hutton, Steven Chan, Robert De Gruijl, Rama Govindaraju, Nishant Patil, e Yanjing Li. 2023. ¬´Understanding and Mitigating Hardware Failures in Deep Learning Training Systems¬ª. In Proceedings of the 50th Annual International Symposium on Computer Architecture, 1‚Äì16. IEEE; ACM. https://doi.org/10.1145/3579371.3589105.\nDurante la fase di inferenza, gli errori intermittenti possono influire sull‚Äôaffidabilit√† e la coerenza delle previsioni ML. Gli errori nelle unit√† di elaborazione o nella memoria possono causare calcoli errati o corruzione dei dati, portando a previsioni errate o incoerenti. Gli errori intermittenti nella pipeline dei dati possono introdurre rumore o errori nei dati di input, influenzando l‚Äôaccuratezza e la robustezza delle previsioni. Nelle applicazioni safety-critical, come veicoli autonomi o sistemi di diagnosi medica, gli errori intermittenti possono avere gravi conseguenze, portando a decisioni o azioni errate che compromettono la sicurezza e l‚Äôaffidabilit√†.\nPer mitigare l‚Äôimpatto degli errori intermittenti nei sistemi ML √® necessario un approccio poliedrico (Rashid, Pattabiraman, e Gopalakrishnan 2012). A livello hardware, tecniche come pratiche di progettazione robuste, selezione dei componenti e controllo ambientale possono aiutare a ridurre il verificarsi di guasti intermittenti. Meccanismi di ridondanza e correzione degli errori possono essere impiegati per rilevare e ripristinare guasti intermittenti. A livello software, monitoraggio del runtime, rilevamento delle anomalie e tecniche di tolleranza ai guasti possono essere incorporate nella pipeline ML. Ci√≤ pu√≤ includere tecniche come convalida dei dati, rilevamento di valori anomali, assemblaggio di modelli o adattamento del modello di runtime per gestire con eleganza i guasti intermittenti.\n\nRashid, Layali, Karthik Pattabiraman, e Sathish Gopalakrishnan. 2012. ¬´Intermittent Hardware Errors Recovery: Modeling and Evaluation¬ª. In 2012 Ninth International Conference on Quantitative Evaluation of Systems, 220‚Äì29. IEEE; IEEE. https://doi.org/10.1109/qest.2012.37.\nProgettare sistemi ML resilienti ai guasti intermittenti √® fondamentale per garantirne affidabilit√† e robustezza. Ci√≤ comporta l‚Äôincorporazione di tecniche di tolleranza ai guasti, monitoraggio del runtime e meccanismi adattivi nell‚Äôarchitettura del sistema. Affrontando in modo proattivo le sfide dei guasti intermittenti, i sistemi ML possono mantenere la loro accuratezza, coerenza e affidabilit√†, anche in caso di guasti hardware sporadici. Test, monitoraggio e manutenzione regolari dei sistemi ML possono aiutare a identificare e mitigare i guasti intermittenti prima che causino interruzioni significative o un degrado delle prestazioni.\n\n\n\n17.3.4 Rilevamento e Mitigazione\nQuesta sezione esplora varie tecniche di rilevamento degli errori, inclusi approcci a livello hardware e software, e discute strategie di mitigazione efficaci per migliorare la resilienza dei sistemi ML. Inoltre, esamineremo le considerazioni sulla progettazione di sistemi ML resilienti, presenteremo casi di studio ed esempi e metteremo in evidenza le future direzioni di ricerca nei sistemi ML tolleranti agli errori.\n\nTecniche di Rilevamento degli Errori\nLe tecniche di rilevamento degli errori sono importanti per identificare e localizzare gli errori hardware nei sistemi ML. Queste tecniche possono essere ampiamente categorizzate in approcci a livello hardware e software, ognuno dei quali offre capacit√† e vantaggi unici.\n\nRilevamento degli errori a livello hardware\nLe tecniche di rilevamento degli errori a livello hardware sono implementate a livello fisico del sistema e mirano a identificare gli errori nei componenti hardware sottostanti. Esistono diverse tecniche hardware, ma in generale, possiamo raggruppare questi diversi meccanismi nelle seguenti categorie.\nBuilt-in self-test (BIST) mechanisms: BIST √® una tecnica potente per rilevare guasti nei componenti hardware (Bushnell e Agrawal 2002). Comporta l‚Äôincorporazione di circuiti hardware aggiuntivi nel sistema per l‚Äôautotest e il rilevamento dei guasti. BIST pu√≤ essere applicato a vari componenti, come processori, moduli di memoria o circuiti integrati specifici per applicazione (ASIC). Ad esempio, BIST pu√≤ essere implementato in un processore utilizzando catene di scansione, che sono percorsi dedicati che consentono l‚Äôaccesso ai registri interni e alla logica per scopi di test.\n\nBushnell, Michael L, e Vishwani D Agrawal. 2002. ¬´Built-in self-test¬ª. Essentials of electronic testing for digital, memory and mixed-signal VLSI circuits, 489‚Äì548.\nDurante il processo BIST, vengono applicati pattern di test predefiniti ai circuiti interni del processore e le risposte vengono confrontate con i valori previsti. Eventuali discrepanze indicano la presenza di guasti. I processori Xeon di Intel, ad esempio, includono meccanismi BIST per testare i core della CPU, la memoria cache e altri componenti critici durante l‚Äôavvio del sistema.\nCodici di rilevamento degli errori: I codici di rilevamento degli errori sono ampiamente utilizzati per rilevare errori di archiviazione e trasmissione dei dati (Hamming 1950). Questi codici aggiungono bit ridondanti ai dati originali, consentendo il rilevamento di errori di bit. Esempio: I controlli di parit√† sono una forma semplice di codice di rilevamento degli errori mostrato in Figura¬†17.11. In uno schema di parit√† a bit singolo, un bit extra viene aggiunto a ogni parola di dati, rendendo il numero di 1 nella parola pari (parit√† pari) o dispari (parit√† dispari).\n\nHamming, R. W. 1950. ¬´Error Detecting and Error Correcting Codes¬ª. Bell Syst. Tech. J. 29 (2): 147‚Äì60. https://doi.org/10.1002/j.1538-7305.1950.tb00463.x.\n\n\n\n\n\n\nFigura¬†17.11: Esempio di bit di parit√†. Fonte: Computer Hope\n\n\n\nQuando si leggono i dati, la parit√† viene controllata e, se non corrisponde al valore previsto, viene rilevato un errore. Codici di rilevamento degli errori pi√π avanzati, come i ‚Äúcyclic redundancy checks (CRC)‚Äù [controlli di ridondanza ciclica], calcolano un checksum in base ai dati e lo aggiungono al messaggio. Il checksum viene ricalcolato all‚Äôestremit√† ricevente e confrontato con il checksum trasmesso per rilevare gli errori. I moduli di memoria con ‚ÄúError-correcting code (ECC)‚Äù [codice di correzione degli errori], comunemente utilizzati nei server e nei sistemi critici, impiegano codici avanzati di rilevamento e correzione degli errori per rilevare e correggere errori a bit singolo o multi-bit nella memoria.\nRidondanza hardware e meccanismi di voto: La ridondanza hardware implica la duplicazione dei componenti critici e il confronto dei loro output per rilevare e mascherare i guasti (Sheaffer, Luebke, e Skadron 2007). I meccanismi di voto, come la ‚Äútriple modular redundancy (TMR)‚Äù [ridondanza modulare tripla], impiegano pi√π istanze di un componente e confrontano i loro output per identificare e mascherare comportamenti difettosi (Arifeen, Hassan, e Lee 2020).\n\nSheaffer, Jeremy W, David P Luebke, e Kevin Skadron. 2007. ¬´A hardware redundancy and recovery mechanism for reliable scientific computation on graphics processors¬ª. In Graphics Hardware, 2007:55‚Äì64. Citeseer.\n\nArifeen, Tooba, Abdus Sami Hassan, e Jeong-A Lee. 2020. ¬´Approximate Triple Modular Redundancy: A Survey¬ª. #IEEE_O_ACC# 8: 139851‚Äì67. https://doi.org/10.1109/access.2020.3012673.\n\nYeh, Y. C. 1996. ¬´Triple-triple redundant 777 primary flight computer¬ª. In 1996 IEEE Aerospace Applications Conference. Proceedings, 1:293‚Äì307. IEEE; IEEE. https://doi.org/10.1109/aero.1996.495891.\nIn un sistema TMR, tre istanze identiche di un componente hardware, come un processore o un sensore, eseguono lo stesso calcolo in parallelo. Gli output di queste istanze vengono immessi in un circuito di voto, che confronta i risultati e seleziona il valore di maggioranza come output finale. Se una delle istanze produce un risultato non corretto a causa di un guasto, il meccanismo di voto maschera l‚Äôerrore e mantiene l‚Äôoutput corretto. Il TMR √® comunemente utilizzato nei sistemi aerospaziali e aeronautici, dove l‚Äôelevata affidabilit√† √® fondamentale. Ad esempio, l‚Äôaereo Boeing 777 impiega il TMR nel suo sistema di computer di volo primario per garantire la disponibilit√† e la correttezza delle funzioni di controllo del volo (Yeh 1996).\nI computer a guida autonoma di Tesla impiegano un‚Äôarchitettura hardware ridondante per garantire la sicurezza e l‚Äôaffidabilit√† delle funzioni critiche, come percezione, processo decisionale e controllo del veicolo, come mostrato in Figura¬†17.12. Un componente chiave di questa architettura √® l‚Äôutilizzo della ‚Äúdual modular redundancy (DMR)‚Äù [ridondanza modulare duale] nei sistemi di computer di bordo dell‚Äôauto.\n\n\n\n\n\n\nFigura¬†17.12: Computer Tesla a guida autonoma completa con SoC duali ridondanti. Fonte: Tesla\n\n\n\nNell‚Äôimplementazione DMR di Tesla, due unit√† hardware identiche, spesso chiamate ‚Äúcomputer ridondanti‚Äù o ‚Äúunit√† di controllo ridondanti‚Äù, eseguono gli stessi calcoli in parallelo (Bannon et al. 2019). Ogni unit√† elabora in modo indipendente i dati dei sensori, esegue algoritmi di percezione e decisionali e genera comandi di controllo per gli attuatori del veicolo (ad esempio, sterzo, accelerazione e frenata).\n\nBannon, Pete, Ganesh Venkataramanan, Debjit Das Sarma, e Emil Talpes. 2019. ¬´Computer and Redundancy Solution for the Full Self-Driving Computer¬ª. In 2019 IEEE Hot Chips 31 Symposium (HCS), 1‚Äì22. IEEE Computer Society; IEEE. https://doi.org/10.1109/hotchips.2019.8875645.\nGli output di queste due unit√† ridondanti vengono costantemente confrontati per rilevare eventuali discrepanze o guasti. Se gli output corrispondono, il sistema presuppone che entrambe le unit√† funzionino correttamente e i comandi di controllo vengono inviati agli attuatori del veicolo. Tuttavia, se c‚Äô√® una mancata corrispondenza tra gli output, il sistema identifica un potenziale guasto in una delle unit√† e adotta le misure appropriate per garantire un funzionamento sicuro.\nIl sistema pu√≤ impiegare meccanismi aggiuntivi per determinare quale unit√† √® difettosa in una mancata corrispondenza. Ci√≤ pu√≤ comportare l‚Äôutilizzo di algoritmi diagnostici, il confronto degli output con i dati di altri sensori o sottosistemi o l‚Äôanalisi della coerenza degli output nel tempo. Una volta identificata l‚Äôunit√† difettosa, il sistema pu√≤ isolarla e continuare a funzionare utilizzando l‚Äôoutput dell‚Äôunit√† non difettosa.\nIl DMR nel computer di guida autonoma di Tesla fornisce un ulteriore livello di sicurezza e tolleranza ai guasti. Avendo due unit√† indipendenti che eseguono gli stessi calcoli, il sistema pu√≤ rilevare e mitigare i guasti che possono verificarsi in una delle unit√†. Questa ridondanza aiuta a prevenire singoli punti di guasto e garantisce che le funzioni critiche rimangano operative nonostante i guasti hardware.\nInoltre, Tesla incorpora anche meccanismi di ridondanza aggiuntivi oltre al DMR. Ad esempio, utilizzano alimentatori ridondanti, sistemi di sterzo e frenata e diverse suite di sensori (ad esempio, telecamere, radar e sensori a ultrasuoni) per fornire pi√π livelli di tolleranza ai guasti. Queste ridondanze contribuiscono collettivamente alla sicurezza e all‚Äôaffidabilit√† complessive del sistema di guida autonoma.\n√à importante notare che mentre DMR fornisce rilevamento guasti e un certo livello di tolleranza ai guasti, TMR pu√≤ fornire un diverso livello di mascheramento dei guasti. In DMR, se entrambe le unit√† subiscono guasti simultanei o il guasto influisce sul meccanismo di confronto, il sistema potrebbe non essere in grado di identificare il guasto. Pertanto, gli SDC di Tesla si basano su una combinazione di DMR e altri meccanismi di ridondanza per raggiungere un elevato livello di tolleranza ai guasti.\nL‚Äôuso di DMR nel computer a guida autonoma di Tesla evidenzia l‚Äôimportanza della ridondanza hardware nelle applicazioni critiche per la sicurezza. Utilizzando unit√† di elaborazione ridondanti e confrontando i loro output, il sistema pu√≤ rilevare e mitigare i guasti, migliorando la sicurezza e l‚Äôaffidabilit√† complessive della funzionalit√† di guida autonoma.\nGoogle utilizza ‚Äúhot spare‚Äù ridondanti per gestire i problemi SDC nei suoi data center, migliorando cos√¨ l‚Äôaffidabilit√† delle funzioni critiche. Come illustrato in Figura¬†17.13, durante la normale fase di addestramento, pi√π ‚Äúworker‚Äù di training sincroni funzionano in modo impeccabile. Tuttavia, se un worker diventa difettoso e causa SDC, un verificatore SDC identifica automaticamente i problemi. Dopo aver rilevato l‚ÄôSDC, il verificatore SDC sposta il training su un hot spare e invia la macchina difettosa per la riparazione. Questa ridondanza salvaguarda la continuit√† e l‚Äôaffidabilit√† del training ML, riducendo al minimo i tempi di inattivit√† e preservando l‚Äôintegrit√† dei dati.\n\n\n\n\n\n\nFigura¬†17.13: Google impiega ‚Äúcore hot spare‚Äù per gestire in modo trasparente gli SDC nel data center. Fonte: Jeff Dean, MLSys 2024 Keynote (Google)\n\n\n\nWatchdog timer: I watchdog timer sono componenti hardware che monitorano l‚Äôesecuzione di attivit√† o processi critici (Pont e Ong 2002). Sono comunemente utilizzati per rilevare e ripristinare guasti software o hardware che causano la mancata risposta di un sistema o il suo blocco in un ciclo infinito. In un sistema embedded, un watchdog timer pu√≤ essere configurato per monitorare l‚Äôesecuzione del loop principale, come illustrato in Figura¬†17.14. Il software reimposta periodicamente il watchdog timer per indicare che funziona correttamente. Supponiamo che il software non riesca a reimpostare il timer entro un limite di tempo specificato (periodo di timeout). In tal caso, il watchdog timer presuppone che il sistema abbia riscontrato un guasto e attiva un‚Äôazione di ripristino predefinita, come il reset del sistema o il passaggio a un componente di backup. I watchdog timer sono ampiamente utilizzati nell‚Äôelettronica automobilistica, nei sistemi di controllo industriale e in altre applicazioni critiche per la sicurezza per garantire il rilevamento e il ripristino tempestivi dai guasti.\n\nPont, Michael J, e Royan HL Ong. 2002. ¬´Using watchdog timers to improve the reliability of single-processor embedded systems: Seven new patterns and a case study¬ª. In Proceedings of the First Nordic Conference on Pattern Languages of Programs, 159‚Äì200. Citeseer.\n\n\n\n\n\n\nFigura¬†17.14: Esempio di watchdog timer nel rilevamento di guasti MCU. Fonte: Ablic\n\n\n\n\n\nRilevamento guasti a livello software\nLe tecniche di rilevamento degli errori a livello software si basano su algoritmi software e meccanismi di monitoraggio per identificare gli errori di sistema. Queste tecniche possono essere implementate a vari livelli dello stack software, tra cui il sistema operativo, il middleware o il livello dell‚Äôapplicazione.\nMonitoraggio del runtime e rilevamento delle anomalie: Il monitoraggio del runtime comporta l‚Äôosservazione continua del comportamento del sistema e dei suoi componenti durante l‚Äôesecuzione (Francalanza et al. 2017). Aiuta a rilevare anomalie, errori o comportamenti imprevisti che potrebbero indicare la presenza di errori. Ad esempio, si consideri un sistema di classificazione delle immagini basato su ML distribuito in un‚Äôauto a guida autonoma. Il monitoraggio del runtime pu√≤ essere implementato per tracciare le prestazioni e il comportamento del modello di classificazione (Mahmoud et al. 2021).\n\nFrancalanza, Adrian, Luca Aceto, Antonis Achilleos, Duncan Paul Attard, Ian Cassar, Dario Della Monica, e Anna Ing√≥lfsd√≥ttir. 2017. ¬´A foundation for runtime monitoring¬ª. In International Conference on Runtime Verification, 8‚Äì29. Springer.\n\nMahmoud, Abdulrahman, Siva Kumar Sastry Hari, Christopher W. Fletcher, Sarita V. Adve, Charbel Sakr, Naresh Shanbhag, Pavlo Molchanov, Michael B. Sullivan, Timothy Tsai, e Stephen W. Keckler. 2021. ¬´Optimizing Selective Protection for CNN Resilience¬ª. In 2021 IEEE 32nd International Symposium on Software Reliability Engineering (ISSRE), 127‚Äì38. IEEE. https://doi.org/10.1109/issre52982.2021.00025.\n\nChandola, Varun, Arindam Banerjee, e Vipin Kumar. 2009. ¬´Anomaly detection: A survey¬ª. ACM Comput. Surv. 41 (3): 1‚Äì58. https://doi.org/10.1145/1541880.1541882.\nGli algoritmi di rilevamento delle anomalie possono essere applicati alle previsioni del modello o alle attivazioni di livelli intermedi, come il rilevamento statistico di valori anomali o approcci basati sull‚Äôapprendimento automatico (ad esempio, One-Class SVM o Autoencoders) (Chandola, Banerjee, e Kumar 2009). Figura¬†17.15 mostra un esempio di rilevamento delle anomalie. Supponiamo che il sistema di monitoraggio rilevi una deviazione significativa dai pattern previsti, come un calo improvviso dell‚Äôaccuratezza della classificazione o campioni fuori distribuzione. In tal caso, pu√≤ generare un ‚Äúalert‚Äù che indica un potenziale errore nel modello o nella pipeline dei dati di input. Questo rilevamento precoce consente di applicare strategie di intervento tempestivo e di mitigazione degli errori.\n\n\n\n\n\n\nFigura¬†17.15: Esempi di rilevamento delle anomalie. (a) Rilevamento delle anomalie completamente supervisionato, (b) rilevamento delle anomalie solo normali, (c, d, e) rilevamento delle anomalie semi-supervisionato, (f) rilevamento delle anomalie non supervisionato. Fonte: Google\n\n\n\nControlli di coerenza e convalida dei dati: I controlli di coerenza e le tecniche di convalida dei dati garantiscono l‚Äôintegrit√† e la correttezza dei dati in diverse fasi di elaborazione in un sistema ML (Lindholm et al. 2019). Questi controlli aiutano a rilevare danneggiamenti dei dati, incongruenze o errori che potrebbero propagarsi e influenzare il comportamento del sistema. Esempio: In un sistema ML distribuito in cui pi√π nodi collaborano per addestrare un modello, √® possibile implementare controlli di coerenza per convalidare l‚Äôintegrit√† dei parametri condivisi del modello. Ogni nodo pu√≤ calcolare un checksum o un hash dei parametri del modello prima e dopo l‚Äôiterazione di addestramento, come mostrato in Figura¬†17.15. Eventuali incongruenze o danneggiamenti dei dati possono essere rilevati confrontando i checksum tra i nodi. Inoltre, √® possibile applicare controlli di intervallo ai dati di input e agli output del modello per garantire che rientrino nei limiti previsti. Ad esempio, se il sistema di percezione di un veicolo autonomo rileva un oggetto con dimensioni o velocit√† non realistiche, pu√≤ indicare un errore nei dati del sensore o negli algoritmi di percezione (Wan et al. 2023).\n\nLindholm, Andreas, Dave Zachariah, Petre Stoica, e Thomas B. Schon. 2019. ¬´Data Consistency Approach to Model Validation¬ª. #IEEE_O_ACC# 7: 59788‚Äì96. https://doi.org/10.1109/access.2019.2915109.\n\nWan, Zishen, Yiming Gan, Bo Yu, S Liu, A Raychowdhury, e Y Zhu. 2023. ¬´Vpp: The vulnerability-proportional protection paradigm towards reliable autonomous machines¬ª. In Proceedings of the 5th International Workshop on Domain Specific System Architecture (DOSSA), 1‚Äì6.\n\nKawazoe Aguilera, Marcos, Wei Chen, e Sam Toueg. 1997. ¬´Heartbeat: A timeout-free failure detector for quiescent reliable communication¬ª. In Distributed Algorithms: 11th International Workshop, WDAG‚Äô97 Saarbr√ºcken, Germany, September 2426, 1997 Proceedings 11, 126‚Äì40. Springer.\nMeccanismi di heartbeat e timeout: I meccanismi di heartbeat e timeout sono comunemente utilizzati per rilevare errori nei sistemi distribuiti e garantire la vitalit√† e la reattivit√† dei componenti (Kawazoe Aguilera, Chen, e Toueg 1997). Sono molto simili ai timer watchdog presenti nell‚Äôhardware. Ad esempio, in un sistema ML distribuito, in cui pi√π nodi collaborano per eseguire attivit√† quali pre-elaborazione dei dati, training del modello o inferenza, √® possibile implementare meccanismi heartbeat per monitorare lo stato e la disponibilit√† di ciascun nodo. Ogni nodo invia periodicamente un messaggio heartbeat a un coordinatore centrale o ai suoi nodi peer, indicando il suo stato e la sua disponibilit√†. Supponiamo che un nodo non riesca a inviare un heartbeat entro un periodo di timeout specificato, come mostrato in Figura¬†17.16. In tal caso, viene considerato difettoso e possono essere intraprese azioni appropriate, come la ridistribuzione del carico di lavoro o l‚Äôavvio di un meccanismo di ‚Äúfailover‚Äù. I timeout possono anche essere utilizzati per rilevare e gestire componenti bloccati o non reattivi. Ad esempio, se un processo di caricamento dati supera una soglia di timeout predefinita, potrebbe indicare un errore nella pipeline dati e il sistema pu√≤ adottare misure correttive.\n\n\n\n\n\n\nFigura¬†17.16: Messaggi heartbeat nei sistemi distribuiti. Fonte: GeeksforGeeks\n\n\n\n\nTecniche di ‚ÄúSoftware-implemented fault tolerance (SIFT)‚Äù: Le tecniche SIFT introducono meccanismi di ridondanza e rilevamento degli errori a livello software per migliorare l‚Äôaffidabilit√† e la tolleranza agli errori del sistema (Reis et al. 2005). Esempio: La programmazione N-version √® una tecnica SIFT in cui pi√π versioni di componenti software funzionalmente equivalenti vengono sviluppate in modo indipendente da team diversi. Questo pu√≤ essere applicato a componenti critici come il motore di inferenza del modello in un sistema ML. Pi√π versioni del motore di inferenza possono essere eseguite in parallelo e i loro output possono essere confrontati per coerenza. √à considerato il risultato corretto se la maggior parte delle versioni produce lo stesso output. Se c‚Äô√® una discrepanza, indica un potenziale errore in una o pi√π versioni e possono essere attivati meccanismi di gestione degli errori appropriati. Un altro esempio √® l‚Äôutilizzo di codici di correzione degli errori basati su software, come i codici Reed-Solomon (Plank 1997), per rilevare e correggere errori nell‚Äôarchiviazione o nella trasmissione dei dati, come mostrato in Figura¬†17.17. Questi codici aggiungono ridondanza ai dati, consentendo di rilevare e correggere determinati errori e migliorare la tolleranza agli errori del sistema.\n\nReis, G. A., J. Chang, N. Vachharajani, R. Rangan, e D. I. August. 2005. ¬´SWIFT: Software Implemented Fault Tolerance¬ª. In International Symposium on Code Generation and Optimization, 243‚Äì54. IEEE; IEEE. https://doi.org/10.1109/cgo.2005.34.\n\nPlank, James S. 1997. ¬´A tutorial on ReedSolomon coding for fault-tolerance in RAID-like systems¬ª. Software: Practice and Experience 27 (9): 995‚Äì1012.\n\n\n\n\n\n\nFigura¬†17.17: Rappresentazione a n bit dei codici Reed-Solomon. Fonte: GeeksforGeeks\n\n\n\n\n\n\n\n\n\nEsercizio¬†17.1: Rilevamento delle Anomalie\n\n\n\n\n\nIn questo Colab, si svolge il ruolo di un detective di guasti IA! Si costruir√† un rilevatore di anomalie basato su autoencoder per individuare gli errori nei dati sulla salute cardiaca. Si scopre come identificare i malfunzionamenti nei sistemi ML, un‚Äôabilit√† fondamentale per creare un‚ÄôIA affidabile. Utilizzeremo Keras Tuner per mettere a punto l‚Äôautoencoder per un rilevamento di guasti di prim‚Äôordine. Questa esperienza si collega direttamente al capitolo Robust AI, dimostrando l‚Äôimportanza del rilevamento di guasti in applicazioni reali come l‚Äôassistenza sanitaria e i sistemi autonomi. Preparatevi a rafforzare l‚Äôaffidabilit√† delle creazioni IA!\n\n\n\n\n\n\n\n\n17.3.5 Riepilogo\nTabella¬†17.1 fornisce un‚Äôanalisi comparativa estesa di guasti transitori, permanenti e intermittenti. Descrive le caratteristiche o dimensioni primarie che distinguono questi tipi di guasti. Qui, riassumiamo le dimensioni rilevanti che abbiamo esaminato ed esploriamo le sfumature che differenziano i guasti transitori, permanenti e intermittenti in modo pi√π dettagliato.\n\n\n\nTabella¬†17.1: Confronto tra guasti transitori, permanenti e intermittenti.\n\n\n\n\n\n\n\n\n\n\n\nDimensione\nGuasti Transitori\nGuasti Permanenti\nGuasti intermittenti\n\n\n\n\nDurata\nDi breve durata, temporaneo\nPersistente, rimane fino alla riparazione o alla sostituzione\nSporadica, appare e scompare in modo intermittente\n\n\nPersistenza\nScompare dopo che la condizione di errore √® passata\n√à costantemente presente finch√© non viene affrontato\nSi ripete in modo irregolare, non sempre presente\n\n\nCause\nFattori esterni (ad esempio, interferenza elettromagnetica raggi cosmici)\nDifetti hardware, danni fisici, usura\nCondizioni hardware instabili, connessioni allentate, componenti obsoleti\n\n\nManifestazione\nBit flip, glitch, danneggiamento temporaneo dei dati\nErrori bloccati, componenti rotti, guasti completi del dispositivo\nBit flip occasionali, problemi di segnale intermittenti, malfunzionamenti sporadici\n\n\nImpatto sui Sistemi ML\nIntroduce errori temporanei o rumore nei calcoli\nCausa errori o guasti costanti, che influiscono sull‚Äôaffidabilit√†\nPorta a errori sporadici e imprevedibili, difficili da diagnosticare e mitigare\n\n\nRilevamento\nCodici di rilevamento degli errori, confronto con i valori previsti\nAutotest integrati, codici di rilevamento degli errori, controlli di coerenza\nMonitoraggio delle anomalie, analisi di pattern di errore e correlazioni\n\n\nMitigazione\nCodici di correzione degli errori, ridondanza, checkpoint e riavvio\nRiparazione o sostituzione hardware, ridondanza dei componenti, meccanismi di failover\nProgettazione robusta, controllo ambientale, monitoraggio del runtime, tecniche di tolleranza agli errori",
    "crumbs": [
      "<span class='chapter-number'>17</span>¬† <span class='chapter-title'>IA Robusta</span>"
    ]
  },
  {
    "objectID": "contents/core/robust_ai/robust_ai.it.html#robustezza-del-modello-ml",
    "href": "contents/core/robust_ai/robust_ai.it.html#robustezza-del-modello-ml",
    "title": "17¬† IA Robusta",
    "section": "17.4 Robustezza del Modello ML",
    "text": "17.4 Robustezza del Modello ML\n\n17.4.1 Attacchi Avversari\n\nDefinizione e Caratteristiche\nGli attacchi avversari mirano a indurre i modelli a fare previsioni errate fornendo loro input ingannevoli appositamente creati (chiamati esempi avversari) (Parrish et al. 2023). Aggiungendo lievi perturbazioni ai dati di input, gli avversari possono ‚Äúhackerare‚Äù il riconoscimento di pattern di un modello e ingannarlo. Si tratta di tecniche sofisticate in cui piccole, spesso impercettibili modifiche ai dati di input possono indurre un modello ML a fare una previsione errata, come mostrato in Figura¬†17.18.\n\nParrish, Alicia, Hannah Rose Kirk, Jessica Quaye, Charvi Rastogi, Max Bartolo, Oana Inel, Juan Ciro, et al. 2023. ¬´Adversarial Nibbler: A Data-Centric Challenge for Improving the Safety of Text-to-Image Models¬ª. ArXiv preprint abs/2305.14384. https://arxiv.org/abs/2305.14384.\n\n\n\n\n\n\nFigura¬†17.18: Un piccolo rumore avversario aggiunto all‚Äôimmagine originale pu√≤ far s√¨ che la rete neurale classifichi l‚Äôimmagine come un Guacamole anzich√© come un gatto egiziano. Fonte: Sutanto\n\n\n\n√à possibile generare prompt che portano a immagini non sicure in modelli testo-immagine come DALLE (Ramesh et al. 2021) o Stable Diffusion (Rombach et al. 2022). Ad esempio, alterando i valori dei pixel di un‚Äôimmagine, gli aggressori possono ingannare un sistema di riconoscimento facciale facendogli identificare un volto come una persona diversa.\n\nRamesh, Aditya, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, e Ilya Sutskever. 2021. ¬´Zero-Shot Text-to-Image Generation¬ª. In Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, a cura di Marina Meila e Tong Zhang, 139:8821‚Äì31. Proceedings of Machine Learning Research. PMLR. http://proceedings.mlr.press/v139/ramesh21a.html.\n\nRombach, Robin, Andreas Blattmann, Dominik Lorenz, Patrick Esser, e Bjorn Ommer. 2022. ¬´High-Resolution Image Synthesis with Latent Diffusion Models¬ª. In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). IEEE. https://doi.org/10.1109/cvpr52688.2022.01042.\nGli attacchi avversari sfruttano il modo in cui i modelli ML apprendono e prendono decisioni durante l‚Äôinferenza. Questi modelli funzionano sul principio di riconoscimento di pattern nei dati. Un avversario crea input speciali con perturbazioni per confondere il riconoscimento degli pattern del modello, in pratica ‚Äúhackerando‚Äù le percezioni del modello.\nGli attacchi avversari rientrano in diversi scenari:\n\nAttacchi Whitebox: L‚Äôattaccante conosce perfettamente il funzionamento interno del modello target, inclusi i dati di training, i parametri e l‚Äôarchitettura (Ye e Hamidi 2021). Questo accesso completo crea condizioni favorevoli per gli aggressori per sfruttare le vulnerabilit√† del modello. L‚Äôattaccante pu√≤ usare debolezze specifiche e sottili per creare esempi avversari efficaci.\nAttacchi Blackbox: A differenza degli attacchi White-box, i Black-box implicano che l‚Äôattaccante abbia poca o nessuna conoscenza del modello target (Guo et al. 2019). Per eseguire l‚Äôattacco, l‚Äôattore avversario deve osservare attentamente il comportamento dell‚Äôoutput del modello.\nAttacchi Greybox: Si collocano tra gli attacchi Blackbox e Whitebox. L‚Äôattaccante ha solo una conoscenza parziale della progettazione interna del modello target (Xu et al. 2021). Ad esempio, l‚Äôattaccante potrebbe avere conoscenza dei dati di training ma non dell‚Äôarchitettura o dei parametri. Nel mondo reale, gli attacchi pratici rientrano solitamente nelle categorie black-box o grey-box.\n\n\nYe, Linfeng, e Shayan Mohajer Hamidi. 2021. ¬´Thundernna: A white box adversarial attack¬ª. arXiv preprint arXiv:2111.12305.\n\nGuo, Chuan, Jacob Gardner, Yurong You, Andrew Gordon Wilson, e Kilian Weinberger. 2019. ¬´Simple black-box adversarial attacks¬ª. In International conference on machine learning, 2484‚Äì93. PMLR.\n\nXu, Ying, Xu Zhong, Antonio Jimeno Yepes, e Jey Han Lau. 2021. ¬´Grey-box adversarial attack and defence for sentiment classification¬ª. arXiv preprint arXiv:2103.11576.\nIl panorama dei modelli di apprendimento automatico √® complesso e ampio, soprattutto data la loro relativamente recente integrazione nelle applicazioni commerciali. Questa rapida adozione, sebbene trasformativa, ha portato alla luce numerose vulnerabilit√† all‚Äôinterno di questi modelli. Di conseguenza, sono emersi vari metodi di attacco avversari, ognuno dei quali sfrutta strategicamente diversi aspetti di vari modelli. Di seguito, evidenziamo un sottoinsieme di questi metodi, che mostra la natura multiforme degli attacchi avversari sui modelli di apprendimento automatico:\n\nLe Generative Adversarial Network (GAN) sono modelli di deep learning costituiti da due reti in competizione tra loro: un generatore e un discriminatore (Goodfellow et al. 2020). Il generatore cerca di sintetizzare dati realistici mentre il discriminatore valuta se sono reali o falsi. Le GAN possono essere utilizzate per creare esempi avversari. La rete del generatore √® addestrata per produrre input che il modello target classifica erroneamente. Queste immagini generate da GAN possono quindi attaccare un classificatore target o un modello di rilevamento. Il generatore e il modello target sono impegnati in un processo competitivo, con il generatore che migliora continuamente la sua capacit√† di creare esempi ingannevoli e il modello target che aumenta la sua resistenza a tali esempi. Le reti GAN forniscono un potente framework per la creazione di input avversari complessi e diversificati, dimostrando l‚Äôadattabilit√† dei modelli generativi nel panorama avversario.\nI Transfer Learning Adversarial Attacks [attacchi avversari di apprendimento di trasferimento] sfruttano la conoscenza trasferita da un modello pre-addestrato a un modello target, creando esempi avversari che possono ingannare entrambi i modelli. Questi attacchi rappresentano una preoccupazione crescente, in particolare quando gli avversari hanno conoscenza dell‚Äôestrattore di feature ma non hanno accesso alla testa di classificazione (la parte o il layer responsabile della creazione delle classificazioni finali). Denominate ‚Äúattacchi headless‚Äù, queste strategie avversarie trasferibili sfruttano le capacit√† espressive degli estrattori di feature per creare perturbazioni, senza tenere conto dello spazio delle etichette o dei dati di addestramento. L‚Äôesistenza di tali attacchi sottolinea l‚Äôimportanza di sviluppare difese robuste per le applicazioni di apprendimento tramite trasferimento, soprattutto perch√© i modelli pre-addestrati sono comunemente utilizzati (Abdelkader et al. 2020).\n\n\nGoodfellow, Ian, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, e Yoshua Bengio. 2020. ¬´Generative adversarial networks¬ª. Commun. ACM 63 (11): 139‚Äì44. https://doi.org/10.1145/3422622.\n\nAbdelkader, Ahmed, Michael J. Curry, Liam Fowl, Tom Goldstein, Avi Schwarzschild, Manli Shu, Christoph Studer, e Chen Zhu. 2020. ¬´Headless Horseman: Adversarial Attacks on Transfer Learning Models¬ª. In ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 3087‚Äì91. IEEE. https://doi.org/10.1109/icassp40776.2020.9053181.\n\n\nMeccanismi degli Attacchi Avversari\nAttacchi Basati sul Gradiente\nUna categoria importante di attacchi avversari √® quella degli attacchi basati sul gradiente. Questi attacchi sfruttano i gradienti della funzione di perdita del modello ML per creare esempi avversari. Il Fast Gradient Sign Method (FGSM) √® una tecnica ben nota in questa categoria. FGSM perturba i dati di input aggiungendo un piccolo rumore nella direzione del gradiente, con l‚Äôobiettivo di massimizzare l‚Äôerrore di previsione del modello. FGSM pu√≤ generare rapidamente esempi avversari, come mostrato in Figura¬†17.19, eseguendo un singolo passaggio nella direzione del gradiente.\n\n\n\n\n\n\nFigura¬†17.19: Attacchi Basati sul Gradiente. Fonte: Ivezic\n\n\n\nUn‚Äôaltra variante, l‚Äôattacco ‚ÄúProjected Gradient Descent (PGD)‚Äù, estende FGSM applicando iterativamente la fase di aggiornamento del gradiente, consentendo esempi avversari pi√π raffinati e potenti. L‚Äôattacco ‚ÄúJacobian-based Saliency Map (JSMA)‚Äù √® un altro approccio basato sul gradiente che identifica le caratteristiche di input pi√π influenti e le perturba per creare esempi avversari.\nAttacchi Basati sull‚ÄôOttimizzazione\nQuesti attacchi formulano la generazione di esempi avversari come un problema di ottimizzazione. L‚Äôattacco Carlini e Wagner (C&W) √® un esempio importante in questa categoria. Trova la perturbazione pi√π piccola che pu√≤ causare una classificazione errata mantenendo la somiglianza percettiva con l‚Äôinput originale. L‚Äôattacco C&W impiega un processo di ottimizzazione iterativo per ridurre al minimo la perturbazione massimizzando al contempo l‚Äôerrore di previsione del modello.\nUn altro approccio basato sull‚Äôottimizzazione √® l‚ÄôElastic Net Attack to DNNs (EAD), che incorpora la regolarizzazione elastica della rete per generare esempi avversari con perturbazioni sparse.\nAttacchi Basati sul Trasferimento\nGli attacchi basati sul trasferimento sfruttano la propriet√† di trasferibilit√† degli esempi avversari. La trasferibilit√† si riferisce al fenomeno per cui gli esempi avversari creati per un modello ML possono spesso ingannare altri modelli, anche se hanno architetture diverse o sono stati addestrati su set di dati diversi. Ci√≤ consente agli aggressori di generare esempi avversari utilizzando un modello surrogato e quindi trasferirli al modello target senza richiedere l‚Äôaccesso diretto ai suoi parametri o gradienti. Gli attacchi basati sul trasferimento evidenziano la generalizzazione delle vulnerabilit√† avversarie su diversi modelli e il potenziale per attacchi black-box.\nAttacchi nel Mondo Fisico\nGli attacchi nel mondo fisico portano gli esempi avversari nel regno degli scenari del mondo reale. Questi attacchi comportano la creazione di oggetti fisici o manipolazioni che possono ingannare i modelli ML quando vengono catturati da sensori o telecamere. Le patch avversarie, ad esempio, sono piccole patch progettate con cura che possono essere posizionate sugli oggetti per ingannare i modelli di rilevamento o classificazione degli oggetti. Quando vengono applicate a oggetti del mondo reale, queste patch possono causare una classificazione errata dei modelli o il mancato rilevamento accurato degli oggetti. Gli oggetti avversari, come sculture stampate in 3D o segnali stradali modificati, possono anche essere creati per ingannare i sistemi ML in ambienti fisici.\nRiepilogo\nTabella¬†17.2 una panoramica concisa delle diverse categorie di attacchi avversari, tra cui attacchi basati su gradiente (FGSM, PGD, JSMA), attacchi basati sull‚Äôottimizzazione (C&W, EAD), attacchi basati sul trasferimento e attacchi nel mondo fisico (patch e oggetti avversari). Ogni attacco viene brevemente descritto, evidenziandone le caratteristiche e i meccanismi principali.\n\n\n\nTabella¬†17.2: Diversi tipi di attacco sui modelli ML.\n\n\n\n\n\n\n\n\n\n\nCategoria di attacco\nNome attacco\nDescrizione\n\n\n\n\nBasato sul gradiente\nFast Gradient Sign Method (FGSM) Projected Gradient Descent (PGD) Jacobian-based Saliency Map Attack (JSMA)\nPerturba i dati di input aggiungendo un piccolo rumore nella direzione del gradiente per massimizzare l‚Äôerrore di previsione. Estende FGSM applicando iterativamente il passaggio di aggiornamento del gradiente per esempi avversari pi√π raffinati. Identifica le caratteristiche di input influenti e le perturba per creare esempi avversari.\n\n\nBasato sull‚Äôottimizzazione\nCarlini and Wagner (C&W) Attack Elastic Net Attack to DNNs (EAD)\nTrova la perturbazione pi√π piccola che causa una classificazione errata mantenendo la somiglianza percettiva. Incorpora la regolarizzazione elastica della rete per generare esempi avversari con perturbazioni sparse.\n\n\nBasato sul trasferimento\nTransferability-based Attacks\nSfrutta la trasferibilit√† di esempi avversari su modelli diversi, consentendo attacchi black-box.\n\n\nMondo fisico\nAdversarial Patches Adversarial Objects\nPiccole patch attentamente progettate, posizionate sugli oggetti per ingannare i modelli di rilevamento o classificazione degli oggetti. Oggetti fisici (ad esempio, sculture stampate in 3D, segnali stradali modificati) creati per ingannare i sistemi ML in scenari del mondo reale.\n\n\n\n\n\n\nI meccanismi degli attacchi avversari rivelano l‚Äôintricata interazione tra i limiti decisionali del modello ML, i dati di input e gli obiettivi dell‚Äôattaccante. Manipolando attentamente i dati di input, gli aggressori possono sfruttare le sensibilit√† e i punti ciechi del modello, portando a previsioni errate. Il successo degli attacchi avversari evidenzia la necessit√† di una comprensione pi√π approfondita delle propriet√† di robustezza e generalizzazione dei modelli ML.\nLa difesa dagli attacchi avversari richiede un approccio multiforme. L‚Äôaddestramento avversario √® una strategia di difesa comune in cui i modelli vengono addestrati su esempi avversari per migliorare la robustezza. Esporre il modello a esempi avversari durante l‚Äôaddestramento gli insegna a classificarli correttamente e a diventare pi√π resiliente agli attacchi. La distillazione difensiva, la preelaborazione degli input e i metodi di ensemble sono altre tecniche che possono aiutare a mitigare l‚Äôimpatto degli attacchi avversari.\nMan mano che l‚Äôapprendimento automatico avversario si evolve, i ricercatori esplorano nuovi meccanismi di attacco e sviluppano difese pi√π sofisticate. La corsa agli armamenti tra aggressori e difensori spinge la necessit√† di innovazione e vigilanza costanti nel proteggere i sistemi ML dalle minacce avversarie. Comprendere i meccanismi degli attacchi avversari √® fondamentale per sviluppare modelli ML robusti e affidabili in grado di resistere al panorama in continua evoluzione degli esempi avversari.\n\n\nImpatto sui Sistemi ML\nGli attacchi avversari sui sistemi di apprendimento automatico sono emersi come una preoccupazione significativa negli ultimi anni, evidenziando le potenziali vulnerabilit√† e i rischi associati all‚Äôadozione diffusa delle tecnologie ML. Questi attacchi comportano perturbazioni attentamente studiate per immettere dati che possono ingannare o fuorviare i modelli ML, portando a previsioni errate o classificazioni errate, come mostrato in Figura¬†17.20. L‚Äôimpatto degli attacchi avversari sui sistemi ML √® di vasta portata e pu√≤ avere gravi conseguenze in vari domini.\n\n\n\n\n\n\nFigura¬†17.20: Generazione di esempi avversari applicata a GoogLeNet (Szegedy et al., 2014a) su ImageNet. Fonte: Goodfellow\n\n\n\nUn esempio lampante dell‚Äôimpatto degli attacchi avversari √® stato dimostrato dai ricercatori nel 2017. Hanno sperimentato piccoli adesivi in bianco e nero sui segnali di stop (Eykholt et al. 2017). All‚Äôocchio umano, questi adesivi non oscuravano il segnale n√© ne impedivano l‚Äôinterpretazione. Tuttavia, quando le immagini dei segnali di stop modificati dagli adesivi sono state inserite nei modelli ML standard di classificazione dei segnali stradali, √® emerso un risultato scioccante. I modelli hanno classificato erroneamente i segnali di stop come segnali di limite di velocit√† nell‚Äô85% dei casi.\n\nEykholt, Kevin, Ivan Evtimov, Earlence Fernandes, Bo Li, Amir Rahmati, Chaowei Xiao, Atul Prakash, Tadayoshi Kohno, e Dawn Song. 2017. ¬´Robust Physical-World Attacks on Deep Learning Models¬ª. ArXiv preprint abs/1707.08945. https://arxiv.org/abs/1707.08945.\nQuesta dimostrazione ha fatto luce sul potenziale allarmante di semplici adesivi avversari per ingannare i sistemi ML e fargli interpretare male i segnali stradali critici. Le implicazioni di tali attacchi nel mondo reale sono significative, in particolare nel contesto dei veicoli autonomi. Se utilizzati su strade reali, questi adesivi avversari potrebbero far s√¨ che le auto a guida autonoma interpretino erroneamente i segnali di stop come limiti di velocit√†, portando a situazioni pericolose, come mostrato in Figura¬†17.21. I ricercatori hanno avvertito che ci√≤ potrebbe causare arresti a rotazione o accelerazioni involontarie negli incroci, mettendo a repentaglio la sicurezza pubblica.\n\n\n\n\n\n\nFigura¬†17.21: I graffiti su un segnale di stop hanno ingannato un‚Äôauto a guida autonoma facendole credere che si trattasse di un segnale di limite di velocit√† di 45 mph. Fonte: Eykholt\n\n\n\nIl caso di studio degli adesivi avversari sui segnali di stop fornisce un‚Äôillustrazione concreta di come gli esempi avversari sfruttino il modo in cui i modelli ML riconoscono i pattern. Manipolando in modo sottile i dati di input in modi invisibili agli esseri umani, gli aggressori possono indurre previsioni errate e creare gravi rischi, specialmente in applicazioni critiche per la sicurezza come i veicoli autonomi. La semplicit√† dell‚Äôattacco evidenzia la vulnerabilit√† dei modelli ML anche a piccole modifiche nell‚Äôinput, sottolineando la necessit√† di difese robuste contro tali minacce.\nL‚Äôimpatto degli attacchi avversari si estende oltre il degrado delle prestazioni del modello. Questi attacchi sollevano notevoli preoccupazioni in termini di sicurezza e protezione, in particolare nei domini in cui i modelli ML sono utilizzati per prendere decisioni critiche. Nelle applicazioni sanitarie, gli attacchi avversari sui modelli di imaging medico potrebbero portare a diagnosi errate o raccomandazioni di trattamento errate, mettendo a repentaglio il benessere del paziente (M.-J. Tsai, Lin, e Lee 2023). Nei sistemi finanziari, gli attacchi avversari potrebbero consentire frodi o manipolazioni di algoritmi di trading, con conseguenti perdite economiche sostanziali.\n\nTsai, Min-Jen, Ping-Yi Lin, e Ming-En Lee. 2023. ¬´Adversarial Attacks on Medical Image Classification¬ª. Cancers 15 (17): 4228. https://doi.org/10.3390/cancers15174228.\n\nFursov, Ivan, Matvey Morozov, Nina Kaploukhaya, Elizaveta Kovtun, Rodrigo Rivera-Castro, Gleb Gusev, Dmitry Babaev, Ivan Kireev, Alexey Zaytsev, e Evgeny Burnaev. 2021. ¬´Adversarial Attacks on Deep Models for Financial Transaction Records¬ª. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining, 2868‚Äì78. ACM. https://doi.org/10.1145/3447548.3467145.\nInoltre, le vulnerabilit√† avversarie compromettono l‚Äôaffidabilit√† e l‚Äôinterpretabilit√† dei modelli ML. Se perturbazioni attentamente realizzate possono facilmente ingannare i modelli, la fiducia nelle loro previsioni e decisioni si erode. Gli esempi avversari espongono la dipendenza dei modelli da pattern superficiali e l‚Äôincapacit√† di catturare i veri concetti sottostanti, mettendo in discussione l‚Äôaffidabilit√† dei sistemi ML (Fursov et al. 2021).\nLa difesa dagli attacchi avversari richiede spesso risorse computazionali aggiuntive e pu√≤ influire sulle prestazioni complessive del sistema. Tecniche come l‚Äôaddestramento avversariale, in cui i modelli vengono addestrati su esempi avversari per migliorare la robustezza, possono aumentare significativamente i tempi di addestramento e i requisiti computazionali (Bai et al. 2021). I meccanismi di rilevamento e mitigazione del runtime, come la preelaborazione dell‚Äôinput (Addepalli et al. 2020) o i controlli di coerenza delle previsioni, introducono latenza e influenzano le prestazioni in tempo reale dei sistemi ML.\n\nBai, Tao, Jinqi Luo, Jun Zhao, Bihan Wen, e Qian Wang. 2021. ¬´Recent advances in adversarial training for adversarial robustness¬ª. arXiv preprint arXiv:2102.01356.\n\nAddepalli, Sravanti, B. S. Vivek, Arya Baburaj, Gaurang Sriramanan, e R. Venkatesh Babu. 2020. ¬´Towards Achieving Adversarial Robustness by Enforcing Feature Consistency Across Bit Planes¬ª. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 1020‚Äì29. IEEE. https://doi.org/10.1109/cvpr42600.2020.00110.\nLa presenza di vulnerabilit√† avversarie complica anche l‚Äôimplementazione e la manutenzione dei sistemi ML. I progettisti e gli operatori di sistema devono considerare il potenziale di attacchi avversari e incorporare difese e meccanismi di monitoraggio appropriati. Aggiornamenti regolari e riqualificazione dei modelli diventano necessari per adattarsi alle nuove tecniche avversarie e mantenere la sicurezza e le prestazioni del sistema nel tempo.\nL‚Äôimpatto degli attacchi avversari sui sistemi ML √® significativo e multiforme. Questi attacchi espongono le vulnerabilit√† dei modelli ML, dal degrado delle prestazioni del modello e dall‚Äôaumento di preoccupazioni sulla sicurezza e la protezione alla sfida dell‚Äôaffidabilit√† e dell‚Äôinterpretabilit√† del modello. Sviluppatori e ricercatori devono dare priorit√† allo sviluppo di difese e contromisure robuste per mitigare i rischi posti dagli attacchi avversari. Affrontando queste sfide, possiamo creare sistemi ML pi√π sicuri, affidabili e degni di fiducia in grado di resistere al panorama in continua evoluzione delle minacce avversarie.\n\n\n\n\n\n\nEsercizio¬†17.2: Attacchi Avversari\n\n\n\n\n\nPreparatevi a diventare un avversario dell‚ÄôIA! In questo Colab, si diventer√† un hacker white-box, imparando a creare attacchi che ingannano i modelli di classificazione delle immagini. Ci concentreremo sul Fast Gradient Sign Method (FGSM), sfruttando i gradienti di un modello contro di esso! Si distorceranno deliberatamente le immagini con piccole perturbazioni, osservando come inganneranno sempre pi√π intensamente l‚ÄôIA. Questo esercizio pratico evidenzia l‚Äôimportanza di creare un‚ÄôIA sicura, un‚Äôabilit√† critica man mano che l‚ÄôIA si integra nelle auto e nell‚Äôassistenza sanitaria. Il Colab si collega direttamente al capitolo Robust AI del libro, spostando gli attacchi avversari dalla teoria alla esperienza pratica.\n\nPensate di poter superare in astuzia un‚ÄôIA? In questo Colab, scopriremo come ingannare i modelli di classificazione delle immagini con attacchi avversari. Utilizzeremo metodi come FGSM per modificare le immagini e ingannare sottilmente l‚ÄôIA. Scopriremo come progettare patch di immagini ingannevoli e osserveremo la sorprendente vulnerabilit√† di questi potenti modelli. Questa √® una conoscenza fondamentale per costruire sistemi di IA veramente robusti!\n\n\n\n\n\n\n\n17.4.2 Avvelenamento dei Dati\n\nDefinizione e Caratteristiche\nL‚Äôavvelenamento dei dati √® un attacco in cui i dati di addestramento vengono manomessi, portando alla compromissione del modello (Biggio, Nelson, e Laskov 2012), come mostrato in Figura¬†17.22. Gli aggressori possono modificare gli esempi di training esistenti, inserire nuovi dati dannosi o influenzare il processo di raccolta dati. I dati avvelenati vengono etichettati in modo tale da alterare il comportamento appreso del modello. Ci√≤ pu√≤ essere particolarmente dannoso nelle applicazioni in cui i modelli ML prendono decisioni automatizzate in base a pattern appresi. Oltre ai set di training, i test di avvelenamento e i dati di convalida possono consentire agli avversari di aumentare artificialmente le prestazioni del modello segnalate.\n\nBiggio, Battista, Blaine Nelson, e Pavel Laskov. 2012. ¬´Poisoning Attacks against Support Vector Machines¬ª. In Proceedings of the 29th International Conference on Machine Learning, ICML 2012, Edinburgh, Scotland, UK, June 26 - July 1, 2012. icml.cc / Omnipress. http://icml.cc/2012/papers/880.pdf.\n\n\n\n\n\n\nFigura¬†17.22: Effetti dell‚ÄôAvvelenamento di NightShade sulla Diffusione Stabile. Fonte: TOM√â\n\n\n\nIl processo di solito prevede i seguenti passaggi:\n\nInjection: L‚Äôaggressore aggiunge esempi errati o fuorvianti al set di training. Questi esempi sono spesso progettati per sembrare normali a un‚Äôispezione superficiale, ma sono stati attentamente elaborati per interrompere il processo di apprendimento.\nTraining: Il modello ML si allena su questo set di dati manipolato e sviluppa comprensioni distorte dei pattern di dati.\nDeployment: Una volta distribuito il modello, l‚Äôaddestramento corrotto porta a un processo decisionale imperfetto o a vulnerabilit√† prevedibili che l‚Äôaggressore pu√≤ sfruttare.\n\nL‚Äôimpatto dell‚Äôavvelenamento dei dati si estende oltre gli errori di classificazione o i cali di accuratezza. In applicazioni critiche come l‚Äôassistenza sanitaria, tali alterazioni possono portare a significativi problemi di fiducia e sicurezza (Marulli, Marrone, e Verde 2022). Pi√π avanti, discuteremo alcuni casi di studio di questi problemi.\n\nMarulli, Fiammetta, Stefano Marrone, e Laura Verde. 2022. ¬´Sensitivity of Machine Learning Approaches to Fake and Untrusted Data in Healthcare Domain¬ª. Journal of Sensor and Actuator Networks 11 (2): 21. https://doi.org/10.3390/jsan11020021.\n\nOprea, Alina, Anoop Singhal, e Apostol Vassilev. 2022. ¬´Poisoning Attacks Against Machine Learning: Can Machine Learning Be Trustworthy?¬ª Computer 55 (11): 94‚Äì99. https://doi.org/10.1109/mc.2022.3190787.\nEsistono sei categorie principali di avvelenamento dei dati (Oprea, Singhal, e Vassilev 2022):\n\nAttacchi alla Disponibilit√†: Questi attacchi mirano a compromettere la funzionalit√† complessiva di un modello. Fanno s√¨ che classifichi erroneamente la maggior parte dei campioni di test, rendendo il modello inutilizzabile per applicazioni pratiche. Un esempio √® il ‚Äúlabel flipping‚Äù, in cui le etichette di una classe specifica e mirata vengono sostituite con etichette di una classe diversa.\nAttacchi Mirati: A differenza degli attacchi alla disponibilit√†, gli attacchi mirati mirano a compromettere un piccolo numero di campioni di test. Quindi, l‚Äôeffetto √® localizzato a un numero limitato di classi, mentre il modello mantiene lo stesso livello originale di accuratezza per la maggior parte delle classi. La natura mirata dell‚Äôattacco richiede che l‚Äôaggressore conosca le classi del modello, rendendo pi√π difficile il rilevamento di questi attacchi.\nAttacchi Backdoor: In questi attacchi, un avversario prende di mira pattern specifici nei dati. L‚Äôaggressore introduce una backdoor (un trigger o un pattern nascosto e dannoso) nei dati di training, ad esempio manipolando determinate feature nei dati strutturati o manipolando un pattern di pixel in una posizione fissa. Ci√≤ fa s√¨ che il modello associ il pattern dannoso a etichette specifiche. Di conseguenza, quando il modello incontra campioni di prova che contengono un pattern dannoso, effettua previsioni false.\nAttacchi di Sotto-popolazione: Gli aggressori scelgono selettivamente di compromettere un sottoinsieme dei campioni di test mantenendo l‚Äôaccuratezza sul resto dei campioni. Questi attacchi si possono pensare come una combinazione di attacchi di disponibilit√† e mirati: eseguire attacchi di disponibilit√† (degrado delle prestazioni) nell‚Äôambito di un sottoinsieme mirato. Sebbene gli attacchi di sottopopolazione possano sembrare molto simili agli attacchi mirati, i due presentano chiare differenze:\nScope: Mentre gli attacchi mirati prendono di mira un set selezionato di campioni, gli attacchi di sotto-popolazione prendono di mira una sotto-popolazione generale con rappresentazioni di caratteristiche simili. Ad esempio, in un attacco mirato, un aggressore inserisce immagini manipolate di un cartello di avvertimento di ‚Äúdosso‚Äù (con perturbazioni o pattern accuratamente studiati), che fanno s√¨ che un‚Äôauto autonoma non riesca a riconoscere tale cartello e rallenti. D‚Äôaltro canto, manipolare tutti i campioni di persone con accento britannico in modo che un modello di riconoscimento vocale classifichi erroneamente il discorso di una persona britannica √® un esempio di attacco di sotto-popolazione.\nConoscenza: Mentre gli attacchi mirati richiedono un alto grado di familiarit√† con i dati, gli attacchi alla sotto-popolazione richiedono una conoscenza meno approfondita per essere efficaci.\n\nLe caratteristiche del data poisoning includono:\nManipolazioni sottili e difficili da rilevare dei dati di training: Il data poisoning spesso comporta manipolazioni sottili dei dati di training che sono attentamente studiate per essere difficili da rilevare tramite un‚Äôispezione casuale. Gli aggressori impiegano tecniche sofisticate per garantire che i campioni avvelenati si fondano perfettamente con i dati legittimi, rendendoli pi√π facili da identificare con un‚Äôanalisi approfondita. Queste manipolazioni possono mirare a caratteristiche o attributi specifici dei dati, come l‚Äôalterazione di valori numerici, la modifica di etichette categoriali o l‚Äôintroduzione di pattern attentamente progettati. L‚Äôobiettivo √® influenzare il processo di apprendimento del modello eludendo il rilevamento, consentendo ai dati avvelenati di corrompere sottilmente il comportamento del modello.\nPu√≤ essere eseguito da insider o aggressori esterni: Gli attacchi di data poisoning possono essere eseguiti da vari attori, tra cui insider malintenzionati con accesso ai dati di training e aggressori esterni che trovano modi per influenzare la raccolta dati o la pipeline di pre-elaborazione. Gli insider rappresentano una minaccia significativa perch√© spesso hanno accesso privilegiato e conoscenza del sistema, il che consente loro di introdurre dati avvelenati senza destare sospetti. D‚Äôaltro canto, gli aggressori esterni possono sfruttare le vulnerabilit√† nell‚Äôapprovvigionamento dei dati, nelle piattaforme di crowdsourcing o nei processi di aggregazione dei dati per iniettare campioni avvelenati nel set di dati di addestramento. Ci√≤ evidenzia l‚Äôimportanza di implementare controlli di accesso rigorosi, policy di governance dei dati e meccanismi di monitoraggio per mitigare il rischio di minacce interne e attacchi esterni.\nSfrutta le vulnerabilit√† nella raccolta e pre-elaborazione dei dati: Gli attacchi di avvelenamento dei dati spesso sfruttano le vulnerabilit√† nelle fasi di raccolta e pre-elaborazione dei dati della pipeline di apprendimento automatico. Gli aggressori progettano attentamente campioni avvelenati per eludere le comuni tecniche di convalida dei dati, assicurandosi che i dati manipolati rientrino comunque in intervalli accettabili, seguano le distribuzioni previste o mantengano la coerenza con altre funzionalit√†. Ci√≤ consente ai dati avvelenati di passare attraverso le fasi di pre-elaborazione dei dati senza essere rilevati. Inoltre, gli attacchi di avvelenamento possono sfruttare le debolezze nella preelaborazione dei dati, come una pulizia dei dati inadeguata, un rilevamento insufficiente di valori anomali o la mancanza di controlli di integrit√†. Gli aggressori possono anche sfruttare la mancanza di solidi meccanismi di tracciamento della provenienza e della discendenza dei dati per introdurre dati avvelenati senza lasciare una traccia. Per affrontare queste vulnerabilit√† sono necessarie rigorose tecniche di convalida dei dati, rilevamento delle anomalie e tracciamento della provenienza dei dati per garantire l‚Äôintegrit√† e l‚Äôaffidabilit√† dei dati di training.\nInterrompe il processo di apprendimento e distorce il comportamento del modello: Gli attacchi di avvelenamento dei dati sono progettati per interrompere il processo di apprendimento dei modelli di apprendimento automatico e distorcere il loro comportamento verso gli obiettivi dell‚Äôaggressore. I dati avvelenati vengono in genere manipolati con obiettivi specifici, come distorcere il comportamento del modello verso determinate classi, introdurre backdoor o degradare le prestazioni complessive. Queste manipolazioni non sono casuali, ma mirate a ottenere i risultati desiderati dall‚Äôaggressore. Introducendo incongruenze nelle etichette, in cui i campioni manipolati hanno etichette che non si allineano con la loro vera natura, gli attacchi di avvelenamento possono confondere il modello durante l‚Äôaddestramento e portare a previsioni distorte o errate. L‚Äôinterruzione causata dai dati avvelenati pu√≤ avere conseguenze di vasta portata, poich√© il modello compromesso pu√≤ prendere decisioni imperfette o mostrare un comportamento indesiderato quando viene distribuito in applicazioni del mondo reale.\nInfluisce sulle prestazioni, l‚Äôequit√† e l‚Äôaffidabilit√† del modello: I dati avvelenati nel dataset di addestramento possono avere gravi implicazioni sulle prestazioni, l‚Äôequit√† e l‚Äôaffidabilit√† dei modelli di apprendimento automatico. I dati avvelenati possono degradare l‚Äôaccuratezza e le prestazioni del modello addestrato, portando a un aumento delle classificazioni errate o degli errori nelle previsioni. Ci√≤ pu√≤ avere conseguenze significative, soprattutto nelle applicazioni critiche in cui gli output del modello influenzano decisioni importanti. Inoltre, gli attacchi di avvelenamento possono introdurre distorsioni e problemi di equit√†, facendo s√¨ che il modello prenda decisioni discriminatorie o ingiuste per determinati sottogruppi o classi. Ci√≤ mina le responsabilit√† etiche e sociali dei sistemi di apprendimento automatico e pu√≤ perpetuare o amplificare i pregiudizi esistenti. Inoltre, i dati avvelenati erodono l‚Äôaffidabilit√† e la credibilit√† dell‚Äôintero sistema di apprendimento automatico. Gli output del modello diventano discutibili e potenzialmente dannosi, portando a una perdita di fiducia nell‚Äôintegrit√† del sistema. L‚Äôimpatto dei dati avvelenati pu√≤ propagarsi nell‚Äôintera pipeline ML, influenzando i componenti downstream e le decisioni che si basano sul modello compromesso. Per affrontare queste preoccupazioni √® necessaria una solida governance dei dati, un auditing regolare del modello e un monitoraggio continuo per rilevare e mitigare gli effetti degli attacchi di avvelenamento dei dati.\n\n\nMeccanismi di Avvelenamento dei Dati\nGli attacchi di avvelenamento dei dati possono essere eseguiti tramite vari meccanismi, sfruttando diverse vulnerabilit√† della pipeline ML. Questi meccanismi consentono agli aggressori di manipolare i dati di training e introdurre campioni dannosi che possono compromettere le prestazioni, l‚Äôequit√† o l‚Äôintegrit√† del modello. Comprendere questi meccanismi √® fondamentale per sviluppare difese efficaci contro l‚Äôavvelenamento dei dati e garantire la robustezza dei sistemi ML. I meccanismi di avvelenamento dei dati possono essere ampiamente categorizzati in base all‚Äôapproccio dell‚Äôaggressore e alla fase della pipeline ML a cui mirano. Alcuni meccanismi comuni includono la modifica delle etichette dei dati di training, l‚Äôalterazione dei valori delle feature, l‚Äôiniezione di campioni dannosi accuratamente realizzati, lo sfruttamento delle vulnerabilit√† di raccolta e pre-elaborazione dei dati, la manipolazione dei dati alla fonte, l‚Äôavvelenamento dei dati in scenari di apprendimento online e la collaborazione con addetti ai lavori per manipolare i dati.\nOgnuno di questi meccanismi presenta sfide uniche e richiede diverse strategie di mitigazione. Ad esempio, rilevare la manipolazione delle etichette pu√≤ comportare l‚Äôanalisi della distribuzione delle etichette e l‚Äôidentificazione delle anomalie (Zhou et al. 2018), mentre prevenire la manipolazione delle feature pu√≤ richiedere tecniche di pre-elaborazione dei dati e rilevamento delle anomalie sicure (Carta et al. 2020). La difesa dalle minacce interne pu√≤ comportare rigide policy di controllo degli accessi e il monitoraggio dei pattern di accesso ai dati. Inoltre, l‚Äôefficacia degli attacchi di avvelenamento dei dati spesso dipende dalla conoscenza del sistema ML da parte dell‚Äôattaccante, tra cui l‚Äôarchitettura del modello, gli algoritmi di training e la distribuzione dei dati. Gli aggressori possono utilizzare tecniche di apprendimento automatico avversario o di sintesi dei dati per creare campioni che hanno maggiori probabilit√† di aggirare il rilevamento e raggiungere i loro obiettivi malevoli.\n\nZhou, Peng, Xintong Han, Vlad I. Morariu, e Larry S. Davis. 2018. ¬´Learning Rich Features for Image Manipulation Detection¬ª. In 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, 1053‚Äì61. IEEE. https://doi.org/10.1109/cvpr.2018.00116.\n\nCarta, Salvatore, Alessandro Sebastian Podda, Diego Reforgiato Recupero, e Roberto Saia. 2020. ¬´A Local Feature Engineering Strategy to Improve Network Anomaly Detection¬ª. Future Internet 12 (10): 177. https://doi.org/10.3390/fi12100177.\nModifica delle etichette dei dati di training: Uno dei meccanismi pi√π semplici di avvelenamento dei dati √® la modifica delle etichette dei dati di training. In questo approccio, l‚Äôaggressore modifica selettivamente le etichette di un sottoinsieme dei campioni di training per fuorviare il processo di apprendimento del modello, come mostrato in Figura¬†17.23. Ad esempio, in un‚Äôattivit√† di classificazione binaria, l‚Äôaggressore potrebbe capovolgere le etichette di alcuni campioni positivi in negativi o viceversa. Introducendo tale rumore di etichetta, l‚Äôaggressore degrada le prestazioni del modello o fa s√¨ che faccia previsioni errate per istanze target specifiche.\n\n\n\n\n\n\nFigura¬†17.23: Garbage In ‚Äì Garbage Out. Fonte: Information Matters\n\n\n\nAlterazione dei valori delle feature nei dati di training: Un altro meccanismo di avvelenamento dei dati consiste nell‚Äôalterare i valori delle caratteristiche dei campioni di training senza modificare le etichette. L‚Äôaggressore elabora attentamente i valori delle feature per introdurre specifici pregiudizi o vulnerabilit√† nel modello. Ad esempio, in un‚Äôattivit√† di classificazione delle immagini, l‚Äôaggressore potrebbe aggiungere perturbazioni impercettibili a un sottoinsieme di immagini, facendo s√¨ che il modello apprenda un particolare pattern o associazione. Questo tipo di avvelenamento pu√≤ creare backdoor o trojan nel modello addestrato, che possono essere attivati da specifici pattern di input.\nIniezione di campioni dannosi accuratamente realizzati: In questo meccanismo, l‚Äôaggressore crea campioni dannosi progettati per avvelenare il modello. Questi campioni sono realizzati per avere un impatto specifico sul comportamento del modello, mentre si fondono con i dati di addestramento legittimi. L‚Äôaggressore potrebbe utilizzare tecniche come perturbazioni avversarie o sintesi dei dati per generare campioni avvelenati difficili da rilevare. L‚Äôaggressore manipola i limiti decisionali del modello iniettando questi campioni dannosi nei dati di addestramento o introducendo classificazioni errate mirate.\nSfruttamento delle vulnerabilit√† di raccolta e preelaborazione dei dati: Gli attacchi di avvelenamento dei dati possono anche sfruttare le vulnerabilit√† della pipeline di raccolta e preelaborazione dei dati. Se il processo di raccolta dati non √® sicuro o ci sono debolezze nelle fasi di pre-elaborazione dei dati, un aggressore pu√≤ manipolare i dati prima che raggiungano la fase di addestramento. Ad esempio, se i dati vengono raccolti da fonti non attendibili o ci sono problemi nella pulizia o nell‚Äôaggregazione dei dati, un aggressore pu√≤ introdurre campioni avvelenati o manipolare i dati a proprio vantaggio.\nManipolazione dei dati alla fonte (ad esempio, dati dei sensori): In alcuni casi, gli aggressori possono manipolare i dati alla fonte, come dati dei sensori o dispositivi di input. Manomettendo i sensori o manipolando l‚Äôambiente in cui vengono raccolti i dati, gli aggressori possono introdurre campioni avvelenati o alterare la distribuzione dei dati. Ad esempio, in uno scenario di auto a guida autonoma, un aggressore potrebbe manipolare i sensori o l‚Äôambiente per immettere informazioni fuorvianti nei dati di addestramento, compromettendo la capacit√† del modello di prendere decisioni sicure e affidabili.\nAvvelenamento dei dati in scenari di apprendimento online: Gli attacchi di avvelenamento dei dati possono anche colpire sistemi ML che impiegano l‚Äôapprendimento online, in cui il modello viene costantemente aggiornato con nuovi dati in tempo reale. In tali scenari, un aggressore pu√≤ gradualmente iniettare campioni avvelenati nel tempo, manipolando lentamente il comportamento del modello. I sistemi di apprendimento online sono particolarmente vulnerabili all‚Äôavvelenamento dei dati perch√© si adattano ai nuovi dati senza una convalida estesa, rendendo pi√π facile per gli aggressori introdurre campioni dannosi, come mostrato in Figura¬†17.24.\n\n\n\n\n\n\nFigura¬†17.24: Attacco di Avvelenamento dei Dati. Fonte: Sikandar\n\n\n\nCollaborazione con addetti ai lavori per manipolare i dati: A volte, gli attacchi di avvelenamento dei dati possono comportare la collaborazione con addetti ai lavori con accesso ai dati di training. Gli addetti ai lavori malintenzionati, come dipendenti o provider di dati, possono manipolare i dati prima che vengano utilizzati per addestrare il modello. Le minacce interne sono particolarmente difficili da rilevare e prevenire, poich√© gli aggressori hanno un accesso legittimo ai dati e possono elaborare attentamente la strategia di avvelenamento per eludere il rilevamento.\nQuesti sono i meccanismi chiave dell‚Äôavvelenamento dei dati nei sistemi ML. Gli aggressori spesso impiegano questi meccanismi per rendere i loro attacchi pi√π efficaci e difficili da rilevare. Il rischio di attacchi di avvelenamento dei dati aumenta man mano che i sistemi ML diventano sempre pi√π complessi e si basano su set di dati pi√π grandi provenienti da fonti diverse. La difesa dall‚Äôavvelenamento dei dati richiede un approccio poliedrico. I professionisti ML e i progettisti di sistemi devono essere consapevoli dei vari meccanismi di avvelenamento dei dati e adottare un approccio completo alla sicurezza dei dati e alla resilienza del modello. Ci√≤ include la raccolta dati sicura, la convalida dati robusta e il monitoraggio continuo delle prestazioni del modello. L‚Äôimplementazione di pratiche di raccolta dati e pre-elaborazione sicure √® fondamentale per prevenire l‚Äôavvelenamento dei dati alla fonte. Le tecniche di convalida dati e rilevamento anomalie possono anche aiutare a identificare e mitigare potenziali tentativi di avvelenamento. Il monitoraggio delle prestazioni del modello per segnali di avvelenamento dei dati √® inoltre essenziale per rilevare e rispondere prontamente agli attacchi.\n\n\nImpatto sui Sistemi ML\nGli attacchi di avvelenamento dei dati possono avere gravi ripercussioni sui sistemi ML, compromettendone le prestazioni, l‚Äôaffidabilit√† e la credibilit√†. L‚Äôimpatto dell‚Äôavvelenamento dei dati pu√≤ manifestarsi in vari modi, a seconda degli obiettivi dell‚Äôaggressore e del meccanismo specifico utilizzato. Analizziamo in dettaglio ciascuno dei potenziali impatti.\nDegrado delle prestazioni del modello: Uno degli impatti principali dell‚Äôavvelenamento dei dati √® il degrado delle prestazioni complessive del modello. Manipolando i dati di training, gli aggressori possono introdurre rumore, distorsioni o incongruenze che ostacolano la capacit√† del modello di apprendere pattern accurati e fare previsioni affidabili. Ci√≤ pu√≤ ridurre accuratezza, precisione, richiamo o altre metriche delle prestazioni. Il degrado delle prestazioni del modello pu√≤ avere conseguenze significative, soprattutto in applicazioni critiche come sanit√†, finanza o sicurezza, dove l‚Äôaffidabilit√† delle previsioni √® fondamentale.\nErrore di classificazione di target specifici: Gli attacchi di avvelenamento dei dati possono anche essere progettati per far s√¨ che il modello classifichi in modo errato istanze target specifiche. Gli aggressori possono introdurre campioni avvelenati realizzati con cura simili alle istanze target, portando il modello ad apprendere associazioni errate. Ci√≤ pu√≤ comportare che il modello classifichi in modo errato le istanze target in modo coerente, anche se funziona bene su altri input. Tale errata classificazione mirata pu√≤ avere gravi conseguenze, come far s√¨ che un sistema di rilevamento malware trascuri file dannosi specifici o portare a una diagnosi errata in un‚Äôapplicazione di imaging medico.\nBackdoor e trojan nei modelli addestrati: L‚Äôavvelenamento dei dati pu√≤ introdurre backdoor o trojan nel modello addestrato. Le backdoor sono funzionalit√† nascoste che consentono agli aggressori di innescare comportamenti specifici o bypassare i normali meccanismi di autenticazione. D‚Äôaltro canto, i trojan sono componenti dannosi insinuati nel modello che possono attivare specifici pattern di input. Avvelenando i dati di training, gli aggressori possono creare modelli che sembrano funzionare normalmente ma contengono vulnerabilit√† nascoste che possono essere sfruttate in seguito. Backdoor e trojan possono compromettere l‚Äôintegrit√† e la sicurezza del sistema ML, consentendo agli aggressori di ottenere accesso non autorizzato, manipolare previsioni o esfiltrare informazioni sensibili.\nRisultati del modello distorti o ingiusti: Gli attacchi di avvelenamento dei dati possono introdurre distorsioni o ingiustizie nelle previsioni del modello. Manipolando la distribuzione dei dati di training o iniettando campioni con distorsioni specifiche, gli aggressori possono far s√¨ che il modello apprenda e perpetui pattern discriminatori. Ci√≤ pu√≤ portare a un trattamento ingiusto di determinati gruppi o individui in base ad attributi sensibili come razza, genere o et√†. I modelli distorti possono avere gravi implicazioni sociali, rafforzando le disuguaglianze e le pratiche discriminatorie esistenti. Garantire l‚Äôequit√† e mitigare i pregiudizi √® fondamentale per creare sistemi ML affidabili ed etici.\nAumento di falsi positivi o falsi negativi: L‚Äôavvelenamento dei dati pu√≤ anche influire sulla capacit√† del modello di identificare correttamente istanze positive o negative, portando a un aumento di falsi positivi o falsi negativi. I falsi positivi si verificano quando il modello identifica erroneamente un‚Äôistanza negativa come positiva, mentre i falsi negativi si verificano quando un‚Äôistanza positiva viene classificata erroneamente come negativa. Le conseguenze dell‚Äôaumento di falsi positivi o falsi negativi possono essere significative a seconda dell‚Äôapplicazione. Ad esempio, in un sistema di rilevamento delle frodi, un elevato numero di falsi positivi pu√≤ portare a indagini non necessarie e frustrazione dei clienti, mentre un elevato numero di falsi negativi pu√≤ consentire che le attivit√† fraudolente passino inosservate.\nAffidabilit√† e fiducia del sistema compromesse: Gli attacchi di avvelenamento dei dati possono minare l‚Äôaffidabilit√† e la fiducia complessiva dei sistemi ML. Quando i modelli vengono addestrati su dati contaminati, le loro previsioni diventano inaffidabili e inaffidabili. Ci√≤ pu√≤ erodere la fiducia dell‚Äôutente nel sistema e portare a una perdita di fiducia nelle decisioni prese dal modello. Nelle applicazioni critiche in cui si fa affidamento sui sistemi ML per il processo decisionale, come veicoli autonomi o diagnosi mediche, l‚Äôaffidabilit√† compromessa pu√≤ avere gravi conseguenze, mettendo a rischio vite e propriet√†.\nPer affrontare l‚Äôimpatto dell‚Äôavvelenamento dei dati √® necessario un approccio proattivo alla sicurezza dei dati, ai test dei modelli e al monitoraggio. Le organizzazioni devono implementare misure robuste per garantire l‚Äôintegrit√† e la qualit√† dei dati di training, impiegare tecniche per rilevare e mitigare i tentativi di avvelenamento e monitorare costantemente le prestazioni e il comportamento dei modelli distribuiti. La collaborazione tra professionisti ML, esperti di sicurezza e specialisti di dominio √® essenziale per sviluppare strategie complete per prevenire e rispondere agli attacchi di avvelenamento dei dati.\n\nCaso di Studio\n√à interessante notare che gli attacchi di ‚Äúdata poisoning‚Äù non sono sempre dannosi (Shan et al. 2023). Nightshade, uno strumento sviluppato da un team guidato dal professor Ben Zhao presso l‚ÄôUniversit√† di Chicago, utilizza l‚Äôavvelenamento dei dati per aiutare gli artisti a proteggere la propria arte da scraping e violazioni del copyright da parte di modelli di IA generativa. Gli artisti possono utilizzare lo strumento per apportare modifiche impercettibili alle proprie immagini prima di caricarle online, come mostrato in Figura¬†17.25.\n\n\n\n\n\n\nFigura¬†17.25: Campioni di dati avvelenati con etichette sbagliateriguardanti coppie testo/immagine non corrispondenti. Fonte: Shan\n\n\n\nSebbene queste modifiche siano impercettibili all‚Äôocchio umano, possono compromettere significativamente le prestazioni dei modelli di intelligenza artificiale generativa quando vengono incorporate nei dati di addestramento. I modelli generativi possono essere manipolati per generare allucinazioni e immagini strane. Ad esempio, con solo 300 immagini avvelenate, i ricercatori dell‚ÄôUniversit√† di Chicago potrebbero ingannare l‚Äôultimo modello Stable Diffusion per generare immagini di cani che sembrano gatti o immagini di mucche quando vengono richieste le auto.\nMan mano che aumenta il numero di immagini avvelenate su Internet, le prestazioni dei modelli che utilizzano dati acquisiti peggioreranno in modo esponenziale. In primo luogo, i dati avvelenati sono difficili da rilevare e richiedono l‚Äôeliminazione manuale. In secondo luogo, il ‚Äúveleno‚Äù si diffonde rapidamente ad altre etichette perch√© i modelli generativi si basano su connessioni tra parole e concetti mentre generano immagini. Quindi un‚Äôimmagine avvelenata di una ‚Äúmacchina‚Äù potrebbe diffondersi in immagini generate associate a parole come ‚Äúcamion‚Äù, ‚Äútreno‚Äù, ‚Äúautobus‚Äù, ecc.\nD‚Äôaltra parte, questo strumento pu√≤ essere utilizzato in modo dannoso e pu√≤ influenzare le applicazioni legittime dei modelli generativi. Ci√≤ dimostra la natura molto impegnativa e innovativa degli attacchi di apprendimento automatico.\nFigura¬†17.26 mostra gli effetti di diversi livelli di avvelenamento dei dati (50 campioni, 100 campioni e 300 campioni di immagini avvelenate) sulla generazione di immagini in diverse categorie. Notare come le immagini iniziano a deformarsi e deviare dalla categoria desiderata. Ad esempio, dopo 300 campioni di avvelenamento, una richiesta di un‚Äôauto genera una mucca.\n\n\n\n\n\n\nFigura¬†17.26: Avvelenamento dei Dati. Fonte: Shan et al. (2023))\n\n\nShan, Shawn, Wenxin Ding, Josephine Passananti, Haitao Zheng, e Ben Y Zhao. 2023. ¬´Prompt-Specific Poisoning Attacks on Text-to-Image Generative Models¬ª. ArXiv preprint abs/2310.13828. https://arxiv.org/abs/2310.13828.\n\n\n\n\n\n\n\n\nEsercizio¬†17.3: Attacchi Avvelenati\n\n\n\n\n\nPreparatevi a esplorare il lato oscuro della sicurezza dell‚ÄôIA! In questo Colab, impareremo cos‚Äô√® l‚Äôavvelenamento dei dati, ovvero come dati errati possono ingannare i modelli di IA e fargli prendere decisioni sbagliate. Ci concentreremo su un attacco reale contro una Support Vector Machine (SVM), osservando come cambia il comportamento dell‚ÄôIA sotto attacco. Questo esercizio pratico metter√† in evidenza perch√© proteggere i sistemi di IA √® fondamentale, soprattutto man mano che diventano pi√π integrati nelle nostre vite. Pensare come un hacker, comprendere la vulnerabilit√† e fare brainstorming su come difendere i sistemi di IA!\n\n\n\n\n\n\n\n\n17.4.3 Distribution Shift\n\nDefinizione e Caratteristiche\nLa ‚Äúdistribution shift‚Äù [slittamento della distribuzione] si riferisce al fenomeno in cui la distribuzione dei dati incontrata da un modello ML durante la distribuzione (inferenza) differisce dalla distribuzione su cui √® stato addestrato, come mostrato in Figura¬†17.27. Questo non √® tanto un attacco quanto il fatto che la robustezza del modello varier√† nel tempo. In altre parole, le propriet√† statistiche, i pattern o le ipotesi sottostanti dei dati possono cambiare tra le fasi di addestramento e di test.\n\n\n\n\n\n\nFigura¬†17.27: Le parentesi graffe racchiudono la ‚Äúdistribution shift‚Äù tra gli ambienti. Qui, z sta per la caratteristica spuria e y sta per la classe dell‚Äôetichetta. Fonte: Xin\n\n\n\nLe caratteristiche principali della ‚Äúdistribution shift‚Äù includono:\nDiscordanza di dominio: I dati di input durante l‚Äôinferenza provengono da un dominio o una distribuzione diversi rispetto ai dati di addestramento. Quando i dati di input durante l‚Äôinferenza provengono da un dominio o una distribuzione diversi dai dati di training, possono influenzare significativamente le prestazioni del modello. Questo perch√© il modello ha imparato pattern e relazioni specifici del dominio di training e, se applicati a un dominio diverso, tali pattern appresi potrebbero non essere validi. Ad esempio, si consideri un modello di analisi del sentiment addestrato sulle recensioni di film. Supponiamo che questo modello venga applicato per analizzare il sentiment nei tweet. In tal caso, potrebbe aver bisogno di aiuto per classificare accuratamente il sentiment perch√© la lingua, la grammatica e il contesto dei tweet possono differire dalle recensioni dei film. Questa discrepanza di dominio pu√≤ causare scarse prestazioni e previsioni inaffidabili, limitando l‚Äôutilit√† pratica del modello.\nDeriva temporale: La distribuzione dei dati si evolve, portando a uno spostamento graduale o improvviso nelle caratteristiche di input. La deriva temporale √® importante perch√© i modelli ML vengono spesso distribuiti in ambienti dinamici in cui la distribuzione dei dati pu√≤ cambiare nel tempo. Se il modello non viene aggiornato o adattato a questi cambiamenti, le sue prestazioni possono gradualmente peggiorare. Ad esempio, i pattern e i comportamenti associati alle attivit√† fraudolente possono evolversi in un sistema di rilevamento delle frodi man mano che i truffatori adattano le loro tecniche. Se il modello non viene riqualificato o aggiornato per catturare questi nuovi pattern, potrebbe non riuscire a rilevare efficacemente nuovi tipi di frode. La deriva temporale pu√≤ portare a un calo dell‚Äôaccuratezza e dell‚Äôaffidabilit√† del modello nel tempo, rendendo cruciale il monitoraggio e l‚Äôaffronto di questo tipo di spostamento della distribuzione.\nCambiamenti contestuali: Il contesto del modello ML pu√≤ variare, determinando diverse distribuzioni di dati in base a fattori quali posizione, comportamento dell‚Äôutente o condizioni ambientali. I cambiamenti contestuali sono importanti perch√© i modelli ML vengono spesso distribuiti in vari contesti o ambienti che possono avere diverse distribuzioni di dati. Se il modello non riesce a generalizzarsi bene a questi diversi contesti, le sue prestazioni potrebbero deteriorarsi. Ad esempio, si consideri un modello di visione artificiale addestrato per riconoscere oggetti in un ambiente di laboratorio controllato. Quando distribuito in un contesto reale, fattori quali condizioni di illuminazione, angoli della telecamera o confusione sullo sfondo possono variare in modo significativo, determinando una ‚Äúdistribution shift‚Äù. Se il modello √® robusto a questi cambiamenti contestuali, potrebbe essere in grado di riconoscere accuratamente gli oggetti nel nuovo ambiente, limitandone l‚Äôutilit√† pratica.\nDati di addestramento non rappresentativi: I dati di addestramento potrebbero catturare solo parzialmente la variabilit√† e la diversit√† dei dati del mondo reale riscontrati durante la distribuzione. I dati di training non rappresentativi possono portare a modelli parziali o distorti che funzionano male sui dati del mondo reale. Supponiamo che i dati di training debbano catturare adeguatamente la variabilit√† e la diversit√† dei dati del mondo reale. In tal caso, il modello potrebbe apprendere pattern specifici del set di training, ma deve essere meglio generalizzato a dati nuovi e mai visti. Ci√≤ pu√≤ comportare scarse prestazioni, previsioni parziali e limitata applicabilit√† del modello. Ad esempio, se un modello di riconoscimento facciale viene addestrato principalmente su immagini di individui di uno specifico gruppo demografico, potrebbe avere difficolt√† a riconoscere accuratamente i volti di altri gruppi demografici quando viene distribuito in un contesto reale. Garantire che i dati di training siano rappresentativi e diversificati √® fondamentale per creare modelli che possano essere generalizzati bene a scenari del mondo reale.\nLa ‚Äúdistribution shift‚Äù pu√≤ manifestarsi in varie forme, come:\nCovariate shift: La distribuzione delle feature di input (covariate) cambia mentre la distribuzione condizionale della variabile target dato l‚Äôinput rimane la stessa. La ‚Äúcovariate shift‚Äù √® importante perch√© pu√≤ influire sulla capacit√† del modello di fare previsioni accurate quando le feature di input (covariate) differiscono tra i dati di training e quelli di test. Anche se la relazione tra le feature di input e la variabile target rimane la stessa, un cambiamento nella distribuzione delle feature di input pu√≤ influire sulle prestazioni del modello. Ad esempio, si consideri un modello addestrato per prevedere i prezzi delle case in base a caratteristiche come la metratura, il numero di camere da letto e la posizione. Supponiamo che la distribuzione di queste caratteristiche nei dati di test differisca significativamente dai dati di training (ad esempio, i dati di test contengono case con una metratura molto pi√π ampia). In tal caso, le previsioni del modello potrebbero diventare meno accurate. √à importante tenere conto dei ‚Äúcovariate shift‚Äù per garantire la robustezza e l‚Äôaffidabilit√† del modello quando viene applicato a nuovi dati.\nConcept drift: La relazione tra le feature di input e la variabile target cambia nel tempo, alterando il concetto sottostante che il modello sta cercando di apprendere, come mostrato in Figura¬†17.28. Il ‚Äúconcept drift‚Äù √® importante perch√© indica cambiamenti nella relazione fondamentale tra le feature di input e la variabile target nel tempo. Quando il concetto sottostante che il modello sta cercando di apprendere cambia, le sue prestazioni possono deteriorarsi se non vengono adattate al nuovo concetto. Ad esempio, in un modello di previsione dell‚Äôabbandono dei clienti, i fattori che influenzano l‚Äôabbandono dei clienti possono evolversi a causa delle condizioni di mercato, delle offerte della concorrenza o delle preferenze dei clienti. Se il modello non viene aggiornato per catturare questi cambiamenti, le sue previsioni potrebbero diventare meno accurate e irrilevanti. Rilevare e adattarsi al ‚Äúconcept drift‚Äù √® fondamentale per mantenere l‚Äôefficacia del modello e l‚Äôallineamento con i concetti del mondo reale in evoluzione.\n\n\n\n\n\n\nFigura¬†17.28: La deriva concettuale si riferisce a un cambiamento nei pattern e nelle relazioni dei dati nel tempo. Fonte: Evidently AI\n\n\n\nGeneralizzazione del dominio: Il modello deve generalizzare a domini o distribuzioni invisibili non presenti durante l‚Äôaddestramento. La generalizzazione di dominio √® importante perch√© consente di applicare i modelli ML a nuovi domini mai visti senza richiedere un‚Äôampia riqualificazione o adattamento. Negli scenari del mondo reale, i dati di addestramento che coprono tutti i possibili domini o distribuzioni che il modello pu√≤ incontrare sono spesso irrealizzabili. Le tecniche di generalizzazione di dominio mirano ad apprendere caratteristiche o modelli invarianti al dominio che possono essere generalizzati bene a nuovi domini. Ad esempio, si consideri un modello addestrato per classificare immagini di animali. Se il modello pu√≤ apprendere caratteristiche invarianti a diversi sfondi, condizioni di illuminazione o pose, pu√≤ essere generalizzato bene per classificare animali in nuovi ambienti mai visti. La generalizzazione del dominio √® fondamentale per creare modelli che possono essere distribuiti in contesti reali diversi e in continua evoluzione.\nLa presenza di un ‚Äúdistribution shift‚Äù pu√≤ avere un impatto significativo sulle prestazioni e l‚Äôaffidabilit√† dei modelli ML, poich√© i modelli potrebbero aver bisogno di aiuto per generalizzare bene alla nuova distribuzione dei dati. Rilevare e adattarsi ai ‚Äúdistribution shift‚Äù √® fondamentale per garantire la robustezza e l‚Äôutilit√† pratica dei sistemi ML negli scenari del mondo reale.\n\n\nMeccanismi delle Distribution Shift\nI meccanismi della ‚Äúdistribution shift‚Äù, come cambiamenti nelle fonti dei dati, evoluzione temporale, variazioni specifiche del dominio, bias di selezione, cicli di feedback e manipolazioni avversarie, sono importanti da comprendere perch√© aiutano a identificarne le cause. Comprendendo questi meccanismi, i professionisti possono sviluppare strategie mirate per mitigarne l‚Äôimpatto e migliorare la robustezza del modello. Ecco alcuni meccanismi comuni:\n\n\n\n\n\n\nFigura¬†17.29: Evoluzione temporale. Fonte: Bia≈Çek\n\n\n\nCambiamenti nelle fonti di dati: Possono verificarsi cambiamenti di distribuzione quando le fonti di dati utilizzate per l‚Äôaddestramento e l‚Äôinferenza sono diverse. Ad esempio, se un modello viene addestrato sui dati di un sensore ma distribuito sui dati di un altro sensore con caratteristiche diverse, pu√≤ portare a un ‚Äúdistribution shift‚Äù.\nEvoluzione temporale: Nel tempo, la distribuzione dei dati sottostante pu√≤ evolversi a causa di cambiamenti nel comportamento dell‚Äôutente, dinamiche di mercato o altri fattori temporali. Ad esempio, in un sistema di raccomandazione, le preferenze dell‚Äôutente possono cambiare nel tempo, portando a un ‚Äúdistribution shift‚Äù nei dati di input, come mostrato in Figura¬†17.29.\nVariazioni specifiche del dominio: Domini o contesti diversi possono avere distribuzioni di dati distinte. Un modello addestrato sui dati di un dominio pu√≤ generalizzare bene con un altro dominio solo con tecniche di adattamento appropriate. Ad esempio, un modello di classificazione delle immagini addestrato su scene di interni potrebbe avere difficolt√† se applicato a scene in esterno.\nBias di selezione: Un ‚ÄúDistribution shift‚Äù pu√≤ derivare da un bias di selezione durante la raccolta o il campionamento dei dati. Se i dati di training non rappresentano la popolazione reale o determinati sottogruppi sono sovrarappresentati o sottorappresentati, si pu√≤ arrivare a una mancata corrispondenza tra le distribuzioni di training e di test.\nCicli di feedback: In alcuni casi, le previsioni o le azioni intraprese da un modello ML possono influenzare la futura distribuzione dei dati. Ad esempio, in un sistema di prezzi dinamici, i prezzi stabiliti dal modello possono influire sul comportamento dei clienti, determinando uno spostamento nella distribuzione dei dati nel tempo.\nManipolazioni avversarie: Gli avversari possono manipolare intenzionalmente i dati di input per creare uno spostamento della distribuzione e ingannare il modello ML. Introducendo perturbazioni attentamente studiate o generando campioni fuori distribuzione, gli aggressori possono sfruttare le vulnerabilit√† del modello e fargli fare previsioni errate.\nComprendere i meccanismi del ‚Äúdistribution shift‚Äù √® importante per sviluppare strategie efficaci per rilevare e mitigare il suo impatto sui sistemi ML. Identificando le fonti e le caratteristiche dello spostamento, i professionisti possono progettare tecniche appropriate, come l‚Äôadattamento del dominio, l‚Äôapprendimento tramite trasferimento o l‚Äôapprendimento continuo, per migliorare la robustezza e le prestazioni del modello in caso di cambiamenti distributivi.\n\n\nImpatto sui Sistemi ML\nI ‚Äúdistribution shift‚Äù possono avere un impatto negativo significativo sulle prestazioni e l‚Äôaffidabilit√† dei sistemi ML. Ecco alcuni modi chiave in cui il ‚Äúdistribution shift‚Äù pu√≤ influenzare i modelli ML:\nPrestazioni predittive degradate: Quando la distribuzione dei dati riscontrata durante l‚Äôinferenza differisce dalla distribuzione di training, l‚Äôaccuratezza predittiva del modello pu√≤ deteriorarsi. Il modello potrebbe aver bisogno di aiuto per generalizzare bene i nuovi dati, il che porta a un aumento degli errori e a prestazioni non ottimali.\nAffidabilit√† e attendibilit√† ridotte: Il ‚Äúdistribution shift‚Äù pu√≤ compromettere l‚Äôaffidabilit√† e l‚Äôattendibilit√† dei modelli ML. Se le previsioni del modello diventano inaffidabili o incoerenti a causa dello spostamento, gli utenti potrebbero perdere fiducia negli output del sistema, il che porta a un potenziale uso improprio o non uso del modello.\nPredizioni distorte: Lo spostamento di distribuzione pu√≤ introdurre ‚Äúbias‚Äù [distorsioni] nelle previsioni del modello. Se i dati di training non rappresentano la distribuzione nel mondo reale o alcuni sottogruppi sono sottorappresentati, il modello potrebbe fare previsioni distorte che discriminano determinati gruppi o perpetuano pregiudizi sociali.\nMaggiore incertezza e rischio: Lo spostamento della distribuzione introduce ulteriore incertezza e rischio nel sistema ML. Il comportamento e le prestazioni del modello potrebbero diventare meno prevedibili, rendendo difficile valutarne l‚Äôaffidabilit√† e l‚Äôidoneit√† per applicazioni critiche. Questa incertezza pu√≤ portare a maggiori rischi operativi e potenziali guasti.\nSfide di adattabilit√†: I modelli ML addestrati su una distribuzione dati specifica potrebbero aver bisogno di aiuto per adattarsi ad ambienti mutevoli o nuovi domini. La mancanza di adattabilit√† pu√≤ limitare l‚Äôutilit√† e l‚Äôapplicabilit√† del modello in scenari reali dinamici in cui la distribuzione dei dati si evolve.\nDifficolt√† di manutenzione e aggiornamento: Il ‚Äúdistribution shift‚Äù pu√≤ complicare la manutenzione e l‚Äôaggiornamento dei modelli ML. Man mano che la distribuzione dei dati cambia, il modello potrebbe richiedere frequenti riqualificazioni o ottimizzazioni per mantenere le sue prestazioni. Ci√≤ pu√≤ richiedere molto tempo e risorse, soprattutto se il cambiamento avviene rapidamente o continuamente.\nVulnerabilit√† agli attacchi avversari: Il ‚Äúdistribution shift‚Äù pu√≤ rendere i modelli ML pi√π vulnerabili agli attacchi avversari. Gli avversari possono sfruttare la sensibilit√† del modello ai cambiamenti distributivi creando esempi avversari al di fuori della distribuzione di addestramento, facendo s√¨ che il modello faccia previsioni errate o si comporti in modo inaspettato.\nPer mitigare l‚Äôimpatto dei ‚Äúdistribution shift‚Äù, √® fondamentale sviluppare sistemi ML robusti che rilevino e si adattino ai cambiamenti delle distribuzioni. Tecniche come l‚Äôadattamento del dominio, l‚Äôapprendimento tramite trasferimento e l‚Äôapprendimento continuo possono aiutare a migliorare la capacit√† di generalizzazione del modello su diverse distribuzioni. Il monitoraggio, il test e l‚Äôaggiornamento del modello ML sono inoltre necessari per garantirne le prestazioni e l‚Äôaffidabilit√† durante i ‚Äúdistribution shift‚Äù.\n\n\n\n17.4.4 Rilevamento e Mitigazione\n\nAttacchi Avversari\nCome si ricorder√† da quanto sopra, gli attacchi avversari rappresentano una minaccia significativa per la robustezza e l‚Äôaffidabilit√† dei sistemi ML. Questi attacchi comportano la creazione di input attentamente progettati, noti come ‚Äúesempi avversari‚Äù, per ingannare i modelli ML e fargli fare previsioni errate. Per proteggere i sistemi ML dagli attacchi avversari, √® fondamentale sviluppare tecniche efficaci per rilevare e mitigare queste minacce.\n\nTecniche di Rilevamento degli Esempi Avversari\nIl rilevamento degli esempi avversari √® la prima linea di difesa contro gli attacchi avversari. Sono state proposte diverse tecniche per identificare e segnalare input sospetti che potrebbero essere avversari.\nI metodi statistici mirano a rilevare gli esempi avversari analizzando le propriet√† statistiche dei dati di input. Questi metodi spesso confrontano la distribuzione dei dati di input con una di riferimento, come quella dei dati di training o una nota distribuzione benigna. Tecniche come il test Kolmogorov-Smirnov (Berger e Zhou 2014) o il test Anderson-Darling possono essere utilizzate per misurare la discrepanza tra le distribuzioni e segnalare gli input che si discostano in modo significativo dalla distribuzione prevista.\n\nBerger, Vance W, e YanYan Zhou. 2014. ¬´Kolmogorovsmirnov test: Overview¬ª. Wiley statsref: Statistics reference online.\nKernel density estimation (KDE) √® una tecnica non parametrica utilizzata per stimare la funzione di densit√† di probabilit√† di un set di dati. Nel contesto del rilevamento di esempi avversari, KDE pu√≤ essere utilizzato per stimare la densit√† di esempi benigni nello spazio di input. Gli esempi avversari spesso si trovano in regioni a bassa densit√† e possono essere rilevati confrontando la loro densit√† stimata con una soglia. Gli input con una densit√† stimata al di sotto della soglia vengono segnalati come potenziali esempi avversari.\nUn‚Äôaltra tecnica √® la compressione delle feature (Panda, Chakraborty, e Roy 2019), che riduce la complessit√† dello spazio di input applicando la riduzione della dimensionalit√† o la discretizzazione. L‚Äôidea alla base della compressione delle feature √® che gli esempi avversari spesso si basano su piccole perturbazioni impercettibili che possono essere eliminate o ridotte tramite queste trasformazioni. Le incongruenze possono essere rilevate confrontando le previsioni del modello sull‚Äôinput originale e sull‚Äôinput compresso, indicando la presenza di esempi avversari.\n\nPanda, Priyadarshini, Indranil Chakraborty, e Kaushik Roy. 2019. ¬´Discretization Based Solutions for Secure Machine Learning Against Adversarial Attacks¬ª. #IEEE_O_ACC# 7: 70157‚Äì68. https://doi.org/10.1109/access.2019.2919463.\nLe tecniche di stima dell‚Äôincertezza del modello mirano a quantificare la fiducia o l‚Äôincertezza associata alle previsioni di un modello. Gli esempi avversari spesso sfruttano regioni di elevata incertezza nel confine di decisione del modello. Stimando l‚Äôincertezza utilizzando tecniche come reti neurali bayesiane, stima dell‚Äôincertezza basata su dropout o metodi di ensemble, gli input con elevata incertezza possono essere contrassegnati come potenziali esempi avversari.\n\n\nStrategie di Difesa Avversarie\nUna volta rilevati gli esempi avversari, possono essere impiegate varie strategie di difesa per mitigarne l‚Äôimpatto e migliorare la robustezza dei modelli ML.\nL‚Äôaddestramento avversario √® una tecnica che prevede l‚Äôaumento dei dati di addestramento con esempi avversari e il riaddestramento del modello su questo set di dati aumentato. Esporre il modello a esempi avversari durante l‚Äôaddestramento gli insegna a classificarli correttamente e diventa pi√π robusto agli attacchi avversari. L‚Äôaddestramento avversario pu√≤ essere eseguito utilizzando vari metodi di attacco, come il Fast Gradient Sign Method (FGSM) o il Projected Gradient Descent (PGD) (Madry et al. 2017).\n\nMadry, Aleksander, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, e Adrian Vladu. 2017. ¬´Towards deep learning models resistant to adversarial attacks¬ª. arXiv preprint arXiv:1706.06083.\n\nPapernot, Nicolas, Patrick McDaniel, Xi Wu, Somesh Jha, e Ananthram Swami. 2016. ¬´Distillation as a Defense to Adversarial Perturbations Against Deep Neural Networks¬ª. In 2016 IEEE Symposium on Security and Privacy (SP), 582‚Äì97. IEEE; IEEE. https://doi.org/10.1109/sp.2016.41.\nLa distillazione difensiva (Papernot et al. 2016) √® una tecnica che addestra un secondo modello (il modello studente) per imitare il comportamento di quello originale (il modello insegnante). Il modello studente viene addestrato sulle etichette soft prodotte dal modello insegnante, che sono meno sensibili alle piccole perturbazioni. L‚Äôutilizzo del modello studente per l‚Äôinferenza pu√≤ ridurre l‚Äôimpatto delle perturbazioni avversarie, poich√© il modello studente impara a generalizzare meglio ed √® meno sensibile al rumore avversario.\nLe tecniche di pre-elaborazione e trasformazione dell‚Äôinput mirano a rimuovere o mitigare l‚Äôeffetto delle perturbazioni avversarie prima di alimentare l‚Äôinput nel modello ML. Queste tecniche includono la rimozione del rumore dalle immagini, la compressione JPEG, il ridimensionamento casuale, il padding o l‚Äôapplicazione di trasformazioni casuali ai dati di input. Riducendo l‚Äôimpatto delle perturbazioni avversarie, questi passaggi di pre-elaborazione possono aiutare a migliorare la robustezza del modello agli attacchi avversari.\nI metodi ensemble combinano pi√π modelli per fare previsioni pi√π robuste. L‚Äôensemble pu√≤ ridurre l‚Äôimpatto degli attacchi avversari utilizzando un set diversificato di modelli con diverse architetture, dati di training o iperparametri. Esempi avversari che ingannano un modello potrebbero non ingannare gli altri nell‚Äôinsieme, portando a previsioni pi√π affidabili e robuste. Le tecniche di diversificazione del modello, come l‚Äôutilizzo di diverse tecniche di pre-elaborazione o rappresentazioni delle caratteristiche per ogni modello nell‚Äôinsieme, possono migliorare ulteriormente la robustezza.\n\n\nValutazione e Test della Robustezza\nCondurre valutazioni e test approfonditi per valutare l‚Äôefficacia delle tecniche di difesa avversarie e misurare la robustezza dei modelli ML.\nLe metriche di robustezza avversaria quantificano la resilienza del modello agli attacchi avversari. Queste metriche possono includere l‚Äôaccuratezza del modello sugli esempi avversari, la distorsione media richiesta per ingannare il modello o le prestazioni del modello in base a diversi livelli di attacco. Confrontando queste metriche tra diversi modelli o tecniche di difesa, i professionisti possono valutare e confrontare i loro livelli di robustezza.\nI benchmark e i set di dati standardizzati per gli attacchi avversari forniscono una base comune per valutare e confrontare la robustezza dei modelli ML. Questi benchmark includono set di dati con esempi avversari pre-generati e strumenti e framework per generare attacchi avversari. Esempi di benchmark di attacchi avversari popolari includono i set di dati MNIST-C, CIFAR-10-C e ImageNet-C (Hendrycks e Dietterich 2019), che contengono versioni corrotte o perturbate dei set di dati originali.\n\nHendrycks, Dan, e Thomas Dietterich. 2019. ¬´Benchmarking neural network robustness to common corruptions and perturbations¬ª. arXiv preprint arXiv:1903.12261.\nI professionisti possono sviluppare sistemi ML pi√π robusti e resilienti sfruttando queste tecniche di rilevamento di esempi avversari, strategie di difesa e metodi di valutazione della robustezza. Tuttavia, √® importante notare che la robustezza avversaria √® un‚Äôarea di ricerca in corso e nessuna tecnica singola fornisce una protezione completa contro tutti i tipi di attacchi avversari. Un approccio completo che combina pi√π meccanismi di difesa e test regolari √® essenziale per mantenere la sicurezza e l‚Äôaffidabilit√† dei sistemi ML di fronte alle minacce avversarie in evoluzione.\n\n\n\nAvvelenamento dei Dati\nSi ricorda che il data poisoning √® un attacco che prende di mira l‚Äôintegrit√† dei dati di training utilizzati per creare modelli ML. Manipolando o corrompendo i dati di training, gli aggressori possono influenzare il comportamento del modello e fargli fare previsioni errate o eseguire azioni indesiderate. Rilevare e mitigare gli attacchi di data poisoning √® fondamentale per garantire l‚Äôaffidabilit√† e la sicurezza dei sistemi ML, come mostrato in Figura¬†17.30.\n\n\n\n\n\n\nFigura¬†17.30: Iniezione di dati dannosi. Fonte: Li\n\n\n\n\nTecniche di rilevamento delle anomalie per identificare i Dati Avvelenati\nI metodi di rilevamento statistico degli outlier identificano i dati che si discostano in modo significativo dalla maggior parte. Questi metodi presuppongono che le istanze di dati avvelenati siano probabilmente ‚Äúoutlier‚Äù statistici‚Äù. Tecniche come il Metodo Z-score, il Metodo di Tukey o la Distanza di Mahalanobis possono essere utilizzate per misurare la deviazione di ciascun punto dati dalla tendenza centrale del set di dati. I dati che superano una soglia predefinita vengono contrassegnati come potenziali valori anomali e considerati sospetti di avvelenamento dei dati.\nI metodi basati sul clustering raggruppano dati simili in base alle loro caratteristiche o attributi. Il presupposto √® che le istanze di dati avvelenate possano formare cluster distinti o trovarsi lontano dai normali cluster di dati. Applicando algoritmi di clustering come K-means, DBSCAN o clustering gerarchico, √® possibile identificare cluster anomali o dati che non appartengono a nessun cluster. Queste istanze anomale vengono poi trattate come dati potenzialmente avvelenati.\nGli autoencoder sono reti neurali addestrate per ricostruire i dati di input da una rappresentazione compressa, come mostrato in Figura¬†17.31. Possono essere utilizzati per il rilevamento di anomalie apprendendo i pattern normali nei dati e identificando le istanze che si discostano da essi. Durante l‚Äôaddestramento, l‚Äôautoencoder viene addestrato su dati puliti e non avvelenati. Al momento dell‚Äôinferenza, viene calcolato l‚Äôerrore di ricostruzione per ogni dato. I dati con errori di ricostruzione elevati sono considerati anomali e potenzialmente avvelenati, poich√© non sono conformi ai pattern normali appresi.\n\n\n\n\n\n\nFigura¬†17.31: Autoencoder. Fonte: Dertat\n\n\n\n\n\nTecniche di Sanificazione e Preelaborazione dei Dati\nL‚Äôavvelenamento dei dati pu√≤ essere evitato pulendo i dati, il che implica l‚Äôidentificazione e la rimozione o la correzione di dati rumorosi, incompleti o incoerenti. Tecniche come la deduplicazione dei dati, l‚Äôimputazione dei valori mancanti e la rimozione dei valori anomali possono essere applicate per migliorare la qualit√† dei dati di addestramento. Eliminando o filtrando i dati sospetti o anomali, √® possibile ridurre l‚Äôimpatto delle istanze avvelenate.\nLa validazione dei dati implica la verifica dell‚Äôintegrit√† e della coerenza dei dati di training. Ci√≤ pu√≤ includere il controllo della coerenza del tipo di dati, la convalida dell‚Äôintervallo e le dipendenze tra campi. Definendo e applicando le regole di validazione dei dati, i dati anomali o incoerenti indicativi di avvelenamento possono essere identificati e segnalati per ulteriori indagini.\nLa provenienza dei dati e il tracciamento della discendenza implicano il mantenimento di un registro dell‚Äôorigine, delle trasformazioni e dei movimenti dei dati in tutta la pipeline ML. Documentando le fonti dei dati, i passaggi di pre-elaborazione e qualsiasi modifica apportata, i professionisti possono risalire alle anomalie o ai pattern sospetti fino alla loro origine. Ci√≤ aiuta a identificare potenziali punti di avvelenamento dei dati e facilita il processo di indagine e mitigazione.\n\n\nTecniche di Training Robusti\n√à possibile utilizzare tecniche di ottimizzazione robuste per modificare l‚Äôobiettivo del training per ridurre al minimo l‚Äôimpatto di valori anomali o istanze avvelenate. Ci√≤ pu√≤ essere ottenuto utilizzando funzioni di perdita robuste meno sensibili ai valori estremi, come la ‚ÄúHuber loss‚Äù o la ‚Äúmodified Huber loss‚Äù. Le tecniche di regolarizzazione, come la regolarizzazione L1 o L2, possono anche aiutare a ridurre la sensibilit√† del modello ai dati avvelenati, limitando la complessit√† del modello e prevenendo l‚Äôoverfitting.\nLe funzioni di ‚Äúloss‚Äù [perdita] robuste sono progettate per essere meno sensibili ai valori anomali o ai dati rumorosi. Esempi includono la Huber loss modificata, la perdita di Tukey (Beaton e Tukey 1974) e la ‚Äútrimmed mean loss‚Äù. Queste funzioni di perdita riducono o ignorano il contributo delle istanze anomale durante il training, riducendo il loro impatto sul processo di apprendimento del modello. Le funzioni ‚Äúobiettivo‚Äù robuste, come l‚Äôobiettivo minimax o la ‚Äúdistributivamente robusto‚Äù, mirano a ottimizzare le prestazioni del modello negli scenari peggiori o in presenza di perturbazioni avversarie.\n\nBeaton, Albert E., e John W. Tukey. 1974. ¬´The Fitting of Power Series, Meaning Polynomials, Illustrated on Band-Spectroscopic Data¬ª. Technometrics 16 (2): 147. https://doi.org/10.2307/1267936.\nLe tecniche di ‚Äúdata augmentation‚Äù comportano la generazione di esempi di addestramento aggiuntivi applicando trasformazioni o perturbazioni casuali ai dati esistenti Figura¬†17.32. Ci√≤ aiuta ad aumentare la diversit√† e la robustezza del set di dati di addestramento. Introducendo variazioni controllate nei dati, il modello diventa meno sensibile a pattern o artefatti specifici che possono essere presenti in istanze avvelenate. Le tecniche di randomizzazione, come il sottocampionamento casuale o l‚Äôaggregazione bootstrap, possono anche aiutare a ridurre l‚Äôimpatto dei dati avvelenati addestrando pi√π modelli su diversi sottoinsiemi di dati e combinando le loro previsioni.\n\n\n\n\n\n\nFigura¬†17.32: Un‚Äôimmagine del numero ‚Äú3‚Äù nella forma originale e con applicati degli aumenti di base.\n\n\n\n\n\nApprovvigionamento di Dati Sicuro e Affidabile\nL‚Äôimplementazione delle migliori pratiche di raccolta e cura dei dati pu√≤ aiutare a mitigare il rischio di avvelenamento dei dati. Ci√≤ include l‚Äôistituzione di protocolli di raccolta dati chiari, la verifica dell‚Äôautenticit√† e dell‚Äôaffidabilit√† delle fonti dati e la conduzione di valutazioni regolari della qualit√† dei dati. L‚Äôapprovvigionamento di dati da provider affidabili e rispettabili e il rispetto di pratiche di gestione dei dati sicure possono ridurre la probabilit√† di introdurre dati avvelenati nella pipeline di training.\nSolidi meccanismi di governance dei dati e controllo degli accessi sono essenziali per prevenire modifiche non autorizzate o manomissioni dei dati di training. Ci√≤ implica la definizione di ruoli e responsabilit√† chiari per l‚Äôaccesso ai dati, l‚Äôimplementazione di policy di controllo degli accessi basate sul principio del privilegio minimo e il monitoraggio e il logging delle attivit√† di accesso ai dati. Limitando l‚Äôaccesso ai dati di training e mantenendo un audit trail, √® possibile rilevare e investigare potenziali tentativi di avvelenamento dei dati.\nRilevare e mitigare gli attacchi di avvelenamento dei dati richiede un approccio poliedrico che combini rilevamento delle anomalie, sanificazione dei dati, tecniche di training affidabili e pratiche di approvvigionamento dei dati sicure. Implementando queste misure, i professionisti del ML possono migliorare la resilienza dei loro modelli contro l‚Äôavvelenamento dei dati e garantire l‚Äôintegrit√† e l‚Äôaffidabilit√† dei dati di training. Tuttavia, √® importante notare che l‚Äôavvelenamento dei dati √® un‚Äôarea di ricerca attiva e continuano a emergere nuovi vettori di attacco e meccanismi di difesa. Rimanere informati sugli ultimi sviluppi e adottare un approccio proattivo e adattivo alla sicurezza dei dati √® fondamentale per mantenere la robustezza dei sistemi ML.\n\n\n\nDistribution Shift\n\nRilevamento e Mitigazione dei ‚ÄúDistribution Shift‚Äù\nRicordiamo che i ‚Äúdistribution shift‚Äù [spostamenti di distribuzione] si verificano quando la distribuzione dei dati incontrata da un modello di machine learning (ML) durante l‚Äôimplementazione differisce dalla distribuzione su cui √® stato addestrato. Questi spostamenti possono avere un impatto significativo sulle prestazioni e sulla capacit√† di generalizzazione del modello, portando a previsioni non ottimali o errate. Rilevare e mitigare i ‚Äúdistribution shift‚Äù √® fondamentale per garantire la robustezza e l‚Äôaffidabilit√† dei sistemi ML in scenari reali.\n\n\nTecniche di Rilevamento per i ‚ÄúDistribution Shift‚Äù\nI test statistici possono essere utilizzati per confrontare le distribuzioni dei dati di training e di test per identificare differenze significative. Tecniche come il test di Kolmogorov-Smirnov o il test di Anderson-Darling misurano la discrepanza tra due distribuzioni e forniscono una valutazione quantitativa della presenza di un ‚Äúdistribution shift‚Äù. Applicando questi test alle funzionalit√† di input o alle previsioni del modello, i professionisti possono rilevare se esiste una differenza statisticamente significativa tra le distribuzioni di training e di test.\nLe metriche di divergenza quantificano la dissimilarit√† tra due distribuzioni di probabilit√†. Le metriche di divergenza comunemente utilizzate includono la Divergenza Kullback-Leibler (KL) e la Divergenza Jensen-Shannon (JS). Calcolando la divergenza tra le distribuzioni dei dati di training e di test, i professionisti possono valutare l‚Äôentit√† dello ‚Äúspostamento della distribuzione‚Äù. Valori di divergenza elevati indicano una differenza significativa tra le distribuzioni, suggerendo la presenza di uno spostamento della distribuzione.\nLe tecniche di quantificazione dell‚Äôincertezza, come le reti neurali bayesiane o i metodi di ensemble, possono stimare l‚Äôincertezza associata alle previsioni del modello. Quando un modello viene applicato a dati da una distribuzione diversa, le sue previsioni potrebbero avere un‚Äôincertezza maggiore. Monitorando i livelli di incertezza, i professionisti possono rilevare gli spostamenti della distribuzione. Se l‚Äôincertezza supera costantemente una soglia predeterminata per i campioni di test, ci√≤ suggerisce che il modello sta operando al di fuori della sua distribuzione addestrata.\nInoltre, i classificatori di dominio sono addestrati a distinguere tra diversi domini o distribuzioni. I professionisti possono rilevare gli spostamenti di distribuzione addestrando un classificatore a distinguere tra i domini di addestramento e di test. Se il classificatore di dominio raggiunge un‚Äôelevata accuratezza nel distinguere tra i due domini, indica una differenza significativa nelle distribuzioni sottostanti. Le prestazioni del classificatore di dominio servono come misura dello spostamento di distribuzione.\n\n\nTecniche di Mitigazione per i ‚ÄúDistribution Shift‚Äù\nIl ‚Äútransfer learning‚Äù [trasferimento dell‚Äôapprendimento.] sfrutta le conoscenze acquisite da un dominio per migliorare le prestazioni in un altro, come mostrato in Figura¬†17.33. Utilizzando modelli pre-addestrati o trasferendo le feature apprese da un dominio di origine a un dominio di destinazione, il transfer learning pu√≤ aiutare a mitigare l‚Äôimpatto dei ‚Äúdistribution shift‚Äù. Il modello pre-addestrato pu√≤ essere messo a punto su una piccola quantit√† di dati etichettati dal dominio target, consentendogli di adattarsi alla nuova distribuzione. Il transfer learning √® particolarmente efficace quando i domini di origine e di destinazione condividono caratteristiche simili o quando i dati etichettati nel dominio di destinazione sono scarsi.\n\n\n\n\n\n\nFigura¬†17.33: Trasferimento dell‚Äôapprendimento. Fonte: Bhavsar\n\n\n\nL‚Äôapprendimento continuo, noto anche come apprendimento permanente, consente ai modelli ML di apprendere continuamente da nuove distribuzioni di dati, mantenendo al contempo le conoscenze delle distribuzioni precedenti. Tecniche come la ‚Äúelastic weight consolidation (EWC)‚Äù (Kirkpatrick et al. 2017) o la ‚Äúgradient episodic memory (GEM)‚Äù (Lopez-Paz e Ranzato 2017) consentono ai modelli di adattarsi alle distribuzioni di dati in evoluzione nel tempo. Queste tecniche mirano a bilanciare la plasticit√† del modello (capacit√† di apprendere da nuovi dati) con la stabilit√† del modello (mantenendo le conoscenze apprese in precedenza). Aggiornando gradualmente il modello con nuovi dati e mitigando l‚Äôoblio catastrofico, l‚Äôapprendimento continuo aiuta i modelli a rimanere robusti ai ‚Äúdistribution shift‚Äù.\n\nKirkpatrick, James, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A. Rusu, Kieran Milan, et al. 2017. ¬´Overcoming catastrophic forgetting in neural networks¬ª. Proc. Natl. Acad. Sci. 114 (13): 3521‚Äì26. https://doi.org/10.1073/pnas.1611835114.\n\nLopez-Paz, David, e Marc‚ÄôAurelio Ranzato. 2017. ¬´Gradient episodic memory for continual learning¬ª. Adv Neural Inf Process Syst 30.\nLe tecniche di aumento dei dati, come quelle viste in precedenza, comportano l‚Äôapplicazione di trasformazioni o perturbazioni ai dati di training esistenti per aumentarne la diversit√† e migliorare la robustezza del modello ai ‚Äúdistribution shift‚Äù. Introducendo variazioni nei dati, come rotazioni, traslazioni, ridimensionamenti o aggiunta di rumore, l‚Äôaumento dei dati aiuta il modello ad apprendere caratteristiche invarianti e a generalizzare meglio a distribuzioni mai viste. Il ‚Äúdata augmentation‚Äù pu√≤ essere eseguito durante l‚Äôaddestramento e l‚Äôinferenza per migliorare la capacit√† del modello di gestire i ‚Äúdistribution shift‚Äù.\nI metodi ensemble combinano pi√π modelli per rendere le previsioni pi√π robuste ai ‚Äúdistribution shift‚Äù. Addestrando i modelli su diversi sottoinsiemi di dati, utilizzando algoritmi diversi o con diversi iperparametri, i metodi ensemble possono catturare diversi aspetti della distribuzione dei dati. Quando viene presentata una distribuzione ‚Äúspostata‚Äù, l‚Äôensemble pu√≤ sfruttare i punti di forza dei singoli modelli per fare previsioni pi√π accurate e stabili. Tecniche come il bagging, il boosting o lo stacking possono creare ensemble efficaci.\nAggiornare regolarmente i modelli con nuovi dati dalla distribuzione target √® fondamentale per mitigare l‚Äôimpatto dei ‚Äúdistribution shift‚Äù. Man mano che la distribuzione dei dati si evolve, i modelli dovrebbero essere riaddestrati o perfezionati sui dati disponibili pi√π recenti per adattarsi ai pattern mutevoli. Il monitoraggio delle prestazioni del modello e delle caratteristiche dei dati pu√≤ aiutare a rilevare quando √® necessario un aggiornamento. Mantenendo aggiornati i modelli, i professionisti possono garantire che rimangano pertinenti e accurati di fronte ai distribution shift‚Äù.\nLa valutazione dei modelli utilizzando metriche robuste meno sensibili ai ‚Äúdistribution shift‚Äù pu√≤ fornire una valutazione pi√π affidabile delle prestazioni del modello. Metriche come l‚Äô‚Äúarea under the precision-recall curve (AUPRC)‚Äù [area sotto la curva di precisione-richiamo] o il punteggio F1 sono pi√π robuste allo squilibrio di classe e possono catturare meglio le prestazioni del modello su diverse distribuzioni. Inoltre, l‚Äôutilizzo di metriche di valutazione specifiche del dominio che si allineano con i risultati desiderati nel dominio target pu√≤ fornire una misura pi√π significativa dell‚Äôefficacia del modello.\nRilevare e mitigare i ‚Äúdistribution shift‚Äù √® un processo continuo che richiede monitoraggio, adattamento e miglioramento continui. Utilizzando una combinazione di tecniche di rilevamento e strategie di mitigazione, i professionisti del ML possono identificare e affrontare in modo proattivo i ‚Äúdistribution shift‚Äù, garantendo la robustezza e l‚Äôaffidabilit√† dei loro modelli nelle distribuzioni del mondo reale. √à importante notare che i ‚Äúdistribution shift‚Äù possono assumere varie forme e potrebbero richiedere approcci specifici del dominio a seconda della natura dei dati e dell‚Äôapplicazione. Rimanere informati sulle ultime ricerche e sulle best practice nella gestione dei ‚Äúdistribution shift‚Äù √® essenziale per creare sistemi ML resilienti.",
    "crumbs": [
      "<span class='chapter-number'>17</span>¬† <span class='chapter-title'>IA Robusta</span>"
    ]
  },
  {
    "objectID": "contents/core/robust_ai/robust_ai.it.html#errori-software",
    "href": "contents/core/robust_ai/robust_ai.it.html#errori-software",
    "title": "17¬† IA Robusta",
    "section": "17.5 Errori Software",
    "text": "17.5 Errori Software\n\n17.5.1 Definizione e Caratteristiche\nGli errori software si riferiscono a difetti, errori o bug nei framework software runtime e nei componenti che supportano l‚Äôesecuzione e la distribuzione di modelli ML (Myllyaho et al. 2022). Questi guasti possono derivare da varie fonti, come errori di programmazione, difetti di progettazione o problemi di compatibilit√† (H. Zhang 2008), e possono avere implicazioni significative per le prestazioni, l‚Äôaffidabilit√† e la sicurezza dei sistemi ML. Gli errori software nei framework ML presentano diverse caratteristiche chiave:\n\nMyllyaho, Lalli, Mikko Raatikainen, Tomi M√§nnist√∂, Jukka K. Nurminen, e Tommi Mikkonen. 2022. ¬´On misbehaviour and fault tolerance in machine learning systems¬ª. J. Syst. Software 183 (gennaio): 111096. https://doi.org/10.1016/j.jss.2021.111096.\n\nZhang, Hongyu. 2008. ¬´On the Distribution of Software Faults¬ª. IEEE Trans. Software Eng. 34 (2): 301‚Äì2. https://doi.org/10.1109/tse.2007.70771.\n\nDiversit√†: Gli errori software possono manifestarsi in forme diverse, che vanno da semplici errori di logica e sintassi a problemi pi√π complessi come perdite di memoria, condizioni di ‚Äúrace‚Äù e problemi di integrazione. La variet√† di tipi di errori aumenta la sfida di rilevarli e mitigarli in modo efficace.\nPropagazione: Nei sistemi ML, gli errori software possono propagarsi attraverso i vari layer e componenti del framework. Un errore in un modulo pu√≤ innescare una cascata di errori o comportamenti imprevisti in altre parti del sistema, rendendo difficile individuare la causa principale e valutare l‚Äôimpatto completo dell‚Äôerrore.\nIntermittenza: Alcuni errori software possono presentare un comportamento intermittente, che si verifica sporadicamente o in condizioni specifiche. Questi errori possono essere particolarmente difficili da riprodurre e correggere, poich√© possono manifestarsi in modo incoerente durante i test o il normale funzionamento.\nInterazione con i modelli ML: Gli errori software nei framework ML possono interagire con i modelli addestrati in modi sottili. Ad esempio, un errore nella pipeline di preelaborazione dei dati pu√≤ introdurre rumore o distorsione negli input del modello, causando prestazioni degradate o previsioni errate. Analogamente, gli errori nel componente di servizio del modello possono causare incongruenze tra gli ambienti di training e inferenza.\nImpatto sulle propriet√† del sistema: Gli errori software possono compromettere varie propriet√† desiderabili dei sistemi ML, come prestazioni, scalabilit√†, affidabilit√† e sicurezza. Gli errori possono causare rallentamenti, crash, output errati o vulnerabilit√† che gli aggressori possono sfruttare.\nDipendenza da fattori esterni: Il verificarsi e l‚Äôimpatto degli errori software nei framework ML dipendono spesso da fattori esterni, come la scelta di hardware, sistema operativo, librerie e configurazioni. Problemi di compatibilit√† e mancate corrispondenze di versione possono introdurre errori difficili da anticipare e mitigare.\n\nComprendere le caratteristiche degli errori software nei framework ML √® fondamentale per sviluppare strategie efficaci di prevenzione, rilevamento e mitigazione degli errori. Riconoscendo la diversit√†, la propagazione, l‚Äôintermittenza e l‚Äôimpatto dei guasti software, i professionisti del ML possono progettare sistemi pi√π robusti e affidabili, resilienti a questi problemi.\n\n\n17.5.2 Meccanismi degli Errori Software nei Framework ML\nI framework di apprendimento automatico, come TensorFlow, PyTorch e sci-kit-learn, forniscono potenti strumenti e astrazioni per la creazione e l‚Äôimplementazione di modelli ML. Tuttavia, questi framework non sono immuni da errori software che possono influire sulle prestazioni, l‚Äôaffidabilit√† e la correttezza dei sistemi ML. Esploriamo alcuni degli errori software comuni che possono verificarsi nei framework ML:\nMemory Leak e Problemi di Gestione delle Risorse: Una gestione della memoria non corretta, come il mancato rilascio di memoria o la chiusura di handle di file, pu√≤ portare a perdite di memoria e all‚Äôesaurimento delle risorse nel tempo. Questo problema √® aggravato dall‚Äôutilizzo inefficiente della memoria, in cui la creazione di copie non necessarie di grandi tensori o il mancato sfruttamento di strutture dati efficienti in termini di memoria pu√≤ causare un consumo eccessivo di memoria e degradare le prestazioni del sistema. Inoltre, la mancata gestione corretta della memoria GPU pu√≤ causare errori di ‚Äúout-of-memory‚Äù o un utilizzo non ottimale delle risorse GPU, aggravando ulteriormente il problema come mostrato in Figura¬†17.34.\n\n\n\n\n\n\nFigura¬†17.34: Esempio di problemi di memoria e utilizzo non ottimale della GPU\n\n\n\nProblemi di Sincronizzazione e Concorrenza: Una sincronizzazione non corretta tra thread o processi pu√≤ causare condizioni di ‚Äúrace‚Äù, deadlock o comportamento incoerente nei sistemi ML multi-thread o distribuiti. Questo problema √® spesso legato alla gestione impropria delle operazioni asincrone, come I/O non bloccante o caricamento dati parallelo, che pu√≤ causare problemi di sincronizzazione e influire sulla correttezza della pipeline ML. Inoltre, un coordinamento e una comunicazione adeguati tra nodi distribuiti in un cluster possono causare coerenza o dati obsoleti durante l‚Äôaddestramento o l‚Äôinferenza, compromettendo l‚Äôaffidabilit√† del sistema ML.\nProblemi di Compatibilit√†: Le discrepanze tra le versioni di framework, librerie o dipendenze ML possono introdurre problemi di compatibilit√† ed errori di runtime. L‚Äôaggiornamento o la modifica delle versioni delle librerie sottostanti senza testare a fondo l‚Äôimpatto sul sistema ML pu√≤ portare a comportamenti imprevisti o malfunzionamenti. Inoltre, le incongruenze tra gli ambienti di training e distribuzione, come differenze nell‚Äôhardware, nei sistemi operativi o nelle versioni dei pacchetti, possono causare problemi di compatibilit√† e influire sulla riproducibilit√† dei modelli ML, rendendo difficile garantire prestazioni coerenti sulle diverse piattaforme.\nInstabilit√† Numerica ed Errori di Precisione: Una gestione inadeguata delle instabilit√† numeriche, come la divisione per zero, l‚Äôunderflow o l‚Äôoverflow, pu√≤ portare a calcoli errati o problemi di convergenza durante l‚Äôaddestramento. Questo problema √® aggravato da errori di precisione o arrotondamento insufficienti, che possono accumularsi nel tempo e influire sull‚Äôaccuratezza dei modelli ML, specialmente nelle architetture di deep learning con molti livelli. Inoltre, un ridimensionamento o una normalizzazione impropri dei dati di input possono causare instabilit√† numeriche e influire sulla convergenza e sulle prestazioni degli algoritmi di ottimizzazione, con conseguenti prestazioni del modello non ottimali o inaffidabili.\nGestione degli Errori e delle Eccezioni Inadeguata: Una corretta gestione degli errori e delle eccezioni pu√≤ impedire ai sistemi ML di bloccarsi o comportarsi in modo imprevisto quando si verificano condizioni eccezionali o input non validi. Non riuscire a catturare e gestire eccezioni specifiche o affidarsi alla gestione generica delle eccezioni pu√≤ rendere difficile diagnosticare e recuperare gli errori in modo corretto, portando a instabilit√† del sistema e affidabilit√† ridotta. Inoltre, messaggi di errore incompleti o fuorvianti possono ostacolare la capacit√† di eseguire il debug e risolvere efficacemente gli errori software nei framework ML, prolungando il tempo necessario per identificare e risolvere i problemi.\n\n\n17.5.3 Impatto sui Sistemi ML\nGli errori software nei framework di apprendimento automatico possono avere impatti significativi e di vasta portata sulle prestazioni, l‚Äôaffidabilit√† e la sicurezza dei sistemi ML. Esploriamo i vari modi in cui gli errori software possono influenzare i sistemi ML:\nDegrado delle Prestazioni e Rallentamenti del Sistema: Memory leak e gestione inefficiente delle risorse possono portare a un graduale degrado delle prestazioni nel tempo, poich√© il sistema diventa sempre pi√π vincolato dalla memoria e impiega pi√π tempo nella garbage collection o nello swapping della memoria (Maas et al. 2024). Questo problema √® aggravato da problemi di sincronizzazione e bug di concorrenza, che possono causare ritardi, riduzione della produttivit√† e utilizzo non ottimale delle risorse di elaborazione, in particolare nei sistemi ML multi-thread o distribuiti. Inoltre, problemi di compatibilit√† o percorsi di codice inefficienti possono introdurre ulteriori overhead e rallentamenti, influenzando le prestazioni complessive del sistema ML.\n\nMaas, Martin, David G. Andersen, Michael Isard, Mohammad Mahdi Javanmard, Kathryn S. McKinley, e Colin Raffel. 2024. ¬´Combining Machine Learning and Lifetime-Based Resource Management for Memory Allocation and Beyond¬ª. Commun. ACM 67 (4): 87‚Äì96. https://doi.org/10.1145/3611018.\nPrevisioni o Output Errati: Gli errori software nella pre-elaborazione dei dati, nell‚Äôingegneria delle feature o nella valutazione del modello possono introdurre distorsioni, rumore o errori che si propagano attraverso la pipeline ML e che determinano previsioni o output errati. Nel tempo, instabilit√† numeriche, errori di precisione o problemi di arrotondamento possono accumularsi e portare a problemi di accuratezza o convergenza degradati nei modelli addestrati. Inoltre, gli errori nei componenti di servizio o inferenza del modello possono causare incongruenze tra gli output previsti e quelli effettivi, portando a previsioni errate o inaffidabili in produzione.\nProblemi di Affidabilit√† e Stabilit√†: Gli errori software possono causare eccezioni senza precedenti, crash o terminazioni improvvise che possono compromettere l‚Äôaffidabilit√† e la stabilit√† dei sistemi ML, specialmente negli ambienti di produzione. Gli errori intermittenti o sporadici possono essere difficili da riprodurre e diagnosticare, portando a un comportamento imprevedibile e a una ridotta fiducia negli output del sistema ML. Inoltre, errori nel checkpointing, nella serializzazione del modello o nella gestione dello stato possono causare perdite di dati o incongruenze, influenzando l‚Äôaffidabilit√† e la recuperabilit√† del sistema ML.\nVulnerabilit√† di Sicurezza: Errori software, come buffer overflow, vulnerabilit√† di ‚Äúinjection‚Äù o controllo di accesso improprio, possono introdurre rischi per la sicurezza ed esporre il sistema ML a potenziali attacchi o accessi non autorizzati. Gli avversari possono sfruttare errori nelle fasi di pre-elaborazione o estrazione delle funzionalit√† per manipolare i dati di input e ingannare i modelli ML, portando a comportamenti errati o dannosi. Inoltre, una protezione inadeguata dei dati sensibili, come le informazioni utente o i parametri riservati del modello, pu√≤ portare a violazioni dei dati o violazioni della privacy (Q. Li et al. 2023).\n\nLi, Qinbin, Zeyi Wen, Zhaomin Wu, Sixu Hu, Naibo Wang, Yuan Li, Xu Liu, e Bingsheng He. 2023. ¬´A Survey on Federated Learning Systems: Vision, Hype and Reality for Data Privacy and Protection¬ª. IEEE Trans. Knowl. Data Eng. 35 (4): 3347‚Äì66. https://doi.org/10.1109/tkde.2021.3124599.\nDifficolt√† nella Riproduzione e nel Debug: Gli errori software possono rendere difficile la riproduzione e il debug dei problemi nei sistemi ML, soprattutto quando gli errori sono intermittenti o dipendono da condizioni di runtime specifiche. Messaggi di errore incompleti o ambigui, uniti alla complessit√† dei framework e dei modelli ML, possono prolungare il processo di debug e ostacolare la capacit√† di identificare e correggere i guasti sottostanti. Inoltre, le incongruenze tra gli ambienti di sviluppo, test e produzione possono rendere difficile la riproduzione e la diagnosi dei guasti in contesti specifici.\nMaggiori Costi di Sviluppo e Manutenzione I guasti software possono comportare maggiori costi di sviluppo e manutenzione, poich√© i team dedicano pi√π tempo e risorse al debug, alla correzione e alla validazione del sistema ML. La necessit√† di test estesi, monitoraggio e meccanismi di tolleranza agli errori per mitigare l‚Äôimpatto degli errori software pu√≤ aggiungere complessit√† e sovraccarico al processo di sviluppo ML. Patch, aggiornamenti e correzioni di bug frequenti per risolvere gli errori software possono interrompere il flusso di lavoro di sviluppo e richiedere sforzi aggiuntivi per garantire la stabilit√† e la compatibilit√† del sistema ML.\nComprendere il potenziale impatto degli errori software sui sistemi ML √® fondamentale per dare priorit√† agli sforzi di test, implementare progetti di tolleranza agli errori e stabilire pratiche di monitoraggio e debug efficaci. Affrontando in modo proattivo gli errori software e le loro conseguenze, i professionisti ML possono creare sistemi ML pi√π solidi, affidabili e sicuri che forniscono risultati accurati e affidabili.\n\n\n17.5.4 Rilevamento e Mitigazione\nRilevare e mitigare i guasti software nei framework di apprendimento automatico √® essenziale per garantire l‚Äôaffidabilit√†, le prestazioni e la sicurezza dei sistemi ML. Esploriamo varie tecniche e approcci che possono essere impiegati per identificare e risolvere efficacemente i guasti software:\nTest e Validazione Approfonditi: ‚ÄúUnit test‚Äù completi di singoli componenti e moduli possono verificarne la correttezza e identificare potenziali guasti nelle prime fasi dello sviluppo. I test di integrazione convalidano l‚Äôinterazione e la compatibilit√† tra diversi componenti del framework ML, garantendo un‚Äôintegrazione senza soluzione di continuit√†. I test sistematici di casi limite, condizioni al contorno e scenari eccezionali aiutano a scoprire guasti e vulnerabilit√† nascosti. Il ‚Äúcontinuous testing‚Äù e i test di regressione come mostrato in Figura¬†17.35 rilevano i guasti introdotti da modifiche al codice o aggiornamenti al framework ML.\n\n\n\n\n\n\nFigura¬†17.35: Test di regressione automatizzati. Fonte: UTOR\n\n\n\nAnalisi Statica del Codice e Linting: L‚Äôutilizzo di strumenti di analisi statica del codice identifica automaticamente potenziali problemi di codifica, come errori di sintassi, variabili non definite o vulnerabilit√† di sicurezza. L‚Äôapplicazione di standard di codifica e best practice tramite strumenti di ‚Äúlinting‚Äù mantiene la qualit√† del codice e riduce la probabilit√† di comuni errori di programmazione. L‚Äôesecuzione di revisioni regolari del codice consente l‚Äôispezione manuale della base di codice, l‚Äôidentificazione di potenziali errori e garantisce l‚Äôaderenza alle linee guida di codifica e ai principi di progettazione.\nMonitoraggio e Logging in Fase di Esecuzione: L‚Äôimplementazione di meccanismi di logging completi cattura informazioni rilevanti durante l‚Äôesecuzione, come dati di input, parametri del modello ed eventi di sistema. Il monitoraggio delle metriche delle prestazioni chiave, dell‚Äôutilizzo delle risorse e dei tassi di errore aiuta a rilevare anomalie, colli di bottiglia delle prestazioni o comportamenti imprevisti. L‚Äôimpiego di controlli di asserzione in fase di esecuzione e invarianti, convalida le ipotesi e rileva violazioni delle condizioni previste durante l‚Äôesecuzione del programma. L‚Äôutilizzo di strumenti di profilazione consente di identificare colli di bottiglia nelle prestazioni, memory leak o percorsi di codice inefficienti che potrebbero indicare la presenza di errori software.\nDesign Pattern a Tolleranza di Errore: L‚Äôimplementazione di meccanismi di gestione degli errori e delle eccezioni consente una gestione e un ripristino controllato da condizioni eccezionali o errori di runtime. L‚Äôimpiego di meccanismi di ridondanza e failover, come sistemi di backup o calcoli ridondanti, garantisce la disponibilit√† e l‚Äôaffidabilit√† del sistema ML in presenza di errori. La progettazione di architetture modulari e debolmente accoppiate riduce al minimo la propagazione e l‚Äôimpatto dei guasti su diversi componenti del sistema ML. L‚Äôutilizzo di meccanismi di checkpointing e ripristino (Eisenman et al. 2022) consente al sistema di riprendere da uno stato stabile noto in caso di guasti o interruzioni.\n\nEisenman, Assaf, Kiran Kumar Matam, Steven Ingram, Dheevatsa Mudigere, Raghuraman Krishnamoorthi, Krishnakumar Nair, Misha Smelyanskiy, e Murali Annavaram. 2022. ¬´Check-N-Run: A checkpointing system for training deep learning recommendation models¬ª. In 19th USENIX Symposium on Networked Systems Design and Implementation (NSDI 22), 929‚Äì43.\nAggiornamenti e Patch Regolari: Rimanere aggiornati con le ultime versioni e patch dei framework, delle librerie e delle dipendenze ML offre vantaggi in termini di correzioni di bug, aggiornamenti di sicurezza e miglioramenti delle prestazioni. Il monitoraggio delle note di rilascio, degli avvisi di sicurezza e dei forum della community informa i professionisti su problemi noti, vulnerabilit√† o problemi di compatibilit√† nel framework ML. L‚Äôistituzione di un processo sistematico per testare e convalidare aggiornamenti e patch prima di applicarli ai sistemi di produzione garantisce stabilit√† e compatibilit√†.\nContainerizzazione e Isolamento: Sfruttando le tecnologie di containerizzazione, come Docker o Kubernetes, si incapsulano i componenti ML e le relative dipendenze in ambienti isolati. L‚Äôutilizzo della containerizzazione garantisce ambienti di runtime coerenti e riproducibili nelle fasi di sviluppo, test e produzione, riducendo la probabilit√† di problemi di compatibilit√† o errori specifici dell‚Äôambiente. L‚Äôimpiego di tecniche di isolamento, come ambienti virtuali o sandbox, impedisce che errori o vulnerabilit√† in un componente influiscano su altre parti del sistema ML.\nTest Automatizzati e Continuous Integration/Continuous Deployment (CI/CD): Implementare framework e script di test automatizzati, eseguire suite di test complete e individuare gli errori nelle prime fasi dello sviluppo. L‚Äôintegrazione di test automatizzati nella pipeline CI/CD, come mostrato in Figura¬†17.36, garantisce che le modifiche al codice siano testate a fondo prima di essere unite o distribuite in produzione. L‚Äôutilizzo di sistemi di monitoraggio continuo e di allerta automatizzati rilevano e notificano a sviluppatori e operatori potenziali guasti o anomalie in tempo reale.\n\n\n\n\n\n\nFigura¬†17.36: Procedura di Continuous Integration/Continuous Deployment (CI/CD). Fonte: geeksforgeeks\n\n\n\nL‚Äôadozione di un approccio proattivo e sistematico al rilevamento e alla mitigazione degli errori pu√≤ migliorare significativamente la robustezza, l‚Äôaffidabilit√† e la manutenibilit√† dei sistemi ML. Investendo in pratiche complete di test, monitoraggio e progettazione tollerante agli errori, le organizzazioni possono ridurre al minimo l‚Äôimpatto degli errori software e garantire il regolare funzionamento dei loro sistemi ML negli ambienti di produzione.\n\n\n\n\n\n\nEsercizio¬†17.4: Tolleranza agli Errori\n\n\n\n\n\nPreparatevi a diventare supereroi che combattono gli errori dell‚ÄôIA! I problemi software possono far deragliare i sistemi di apprendimento automatico, ma in questo Colab impareremo come renderli resilienti. Simuleremo errori software per vedere come l‚ÄôIA pu√≤ ‚Äúrompersi‚Äù, poi esploreremo tecniche per salvare i progressi del modello ML, come i checkpoint in un gioco. Vedremo come addestrare l‚ÄôIA a riprendersi dopo un crash, assicurando che rimanga sulla buona strada. Questo √® fondamentale per creare un‚ÄôIA affidabile e degna di fiducia, soprattutto nelle applicazioni critiche. Quindi preparatevi perch√© questo Colab si collega direttamente al capitolo IA Robusta‚Äîpasseremo dalla teoria alla risoluzione pratica dei problemi e creeremo sistemi di intelligenza artificiale in grado di gestire l‚Äôimprevisto!",
    "crumbs": [
      "<span class='chapter-number'>17</span>¬† <span class='chapter-title'>IA Robusta</span>"
    ]
  },
  {
    "objectID": "contents/core/robust_ai/robust_ai.it.html#strumenti-e-framework",
    "href": "contents/core/robust_ai/robust_ai.it.html#strumenti-e-framework",
    "title": "17¬† IA Robusta",
    "section": "17.6 Strumenti e Framework",
    "text": "17.6 Strumenti e Framework\nData l‚Äôimportanza di sviluppare sistemi di IA robusti, negli ultimi anni ricercatori e professionisti hanno sviluppato un‚Äôampia gamma di strumenti e framework per comprendere come i guasti hardware si manifestano e si propagano per avere un impatto sui sistemi ML. Questi strumenti e framework svolgono un ruolo cruciale nella valutazione della resilienza dei sistemi ML ai guasti hardware simulando vari scenari di guasto e analizzandone l‚Äôimpatto sulle prestazioni del sistema. Ci√≤ consente ai progettisti di identificare potenziali vulnerabilit√† e sviluppare strategie di mitigazione efficaci, creando in definitiva sistemi ML pi√π robusti e affidabili in grado di funzionare in sicurezza nonostante i guasti hardware. Questa sezione fornisce una panoramica dei modelli di guasto ampiamente utilizzati nella letteratura e degli strumenti e framework sviluppati per valutare l‚Äôimpatto di tali guasti sui sistemi ML.\n\n17.6.1 Modelli di Guasto e Modelli di Errore\nCome discusso in precedenza, i guasti hardware possono manifestarsi in vari modi, tra cui guasti transitori, permanenti e intermittenti. Oltre al tipo di guasto in esame, √® importante anche come si manifesta il guasto. Ad esempio, l‚Äôerrore si verifica in una cella di memoria o durante il calcolo di un‚Äôunit√† funzionale? L‚Äôimpatto √® su un singolo bit o su pi√π bit? L‚Äôerrore si propaga per tutto il percorso e ha un impatto sull‚Äôapplicazione (causando un errore) o viene mascherato rapidamente ed √® considerato benigno? Tutti questi dettagli hanno un impatto su ci√≤ che √® noto come fault model [modello di errore], che svolge un ruolo importante nella simulazione e nella misurazione di ci√≤ che accade a un sistema quando si verifica un errore.\nPer studiare e comprendere efficacemente l‚Äôimpatto degli errori hardware sui sistemi ML, √® essenziale comprendere i concetti di ‚Äúfault model‚Äù e ‚Äúerror model‚Äù. Un ‚Äúfault model‚Äù [guasto] descrive come si manifesta un errore hardware nel sistema, mentre un ‚Äúerror model‚Äù [modello di errore] rappresenta come l‚Äôerrore si propaga e influisce sul comportamento del sistema.\nI ‚Äúfault model‚Äù possono essere categorizzati in base a varie caratteristiche:\n\nDurata: I guasti transitori si verificano brevemente e poi scompaiono, mentre quelli permanenti persistono indefinitamente. I guasti intermittenti si verificano sporadicamente e possono essere difficili da diagnosticare.\nPosizione: I guasti possono verificarsi in componenti hardware, come celle di memoria, unit√† funzionali o interconnessioni.\nGranularit√†: I guastipossono interessare un singolo bit (ad esempio, bitflip) o pi√π bit (ad esempio, errori burst) all‚Äôinterno di un componente hardware.\n\nD‚Äôaltro canto, gli ‚Äúerror model‚Äù descrivono come un guasto si propaga nel sistema e si manifesta come un errore. Un errore pu√≤ causare la deviazione del sistema dal comportamento previsto, portando a risultati errati o persino a guasti del sistema. I modelli di errore possono essere definiti a diversi livelli di astrazione, da quello hardware (ad esempio, bitflip a livello di registro) al livello software (ad esempio, pesi o attivazioni corrotti in un modello ML).\nIl ‚Äúfault model‚Äù (o il modello di errore, in genere la terminologia pi√π applicabile per comprendere la robustezza di un sistema ML) svolge un ruolo importante nella simulazione e nella misura di ci√≤ che accade a un sistema quando si verifica un guasto. Il modello scelto informa le ipotesi fatte sul sistema in fase di studio. Ad esempio, un sistema incentrato su errori transitori a bit singolo (Sangchoolie, Pattabiraman, e Karlsson 2017) non sarebbe adatto a comprendere l‚Äôimpatto di errori permanenti di flip multi-bit (Wilkening et al. 2014), poich√© √® progettato presupponendo un modello completamente diverso.\n\nWilkening, Mark, Vilas Sridharan, Si Li, Fritz Previlon, Sudhanva Gurumurthi, e David R. Kaeli. 2014. ¬´Calculating Architectural Vulnerability Factors for Spatial Multi-Bit Transient Faults¬ª. In 2014 47th Annual IEEE/ACM International Symposium on Microarchitecture, 293‚Äì305. IEEE; IEEE. https://doi.org/10.1109/micro.2014.15.\nInoltre, anche l‚Äôimplementazione di un modello di errore √® una considerazione importante, in particolare per quanto riguarda il punto in cui si dice che si verifichi un errore nello stack di elaborazione. Ad esempio, un modello di flip a bit singolo a livello di registro architetturale differisce da un modello di flip a bit singolo nel peso di un modello a livello di PyTorch. Sebbene entrambi mirino a un modello di errore simile, il primo verrebbe solitamente modellato in un simulatore architetturalmente accurato (come gem5 [binkert2011gem5]), che cattura la propagazione dell‚Äôerrore rispetto al secondo, concentrandosi sulla propagazione del valore attraverso un modello.\nRicerche recenti hanno dimostrato che alcune caratteristiche dei modelli di errore possono mostrare comportamenti simili a diversi livelli di astrazione (Sangchoolie, Pattabiraman, e Karlsson 2017) (Papadimitriou e Gizopoulos 2021). Ad esempio, gli errori a bit singolo sono generalmente pi√π problematici degli errori a bit multiplo, indipendentemente dal fatto che siano modellati a livello hardware o software. Tuttavia, altre caratteristiche, come il mascheramento degli errori (Mohanram e Touba 2003) come mostrato in Figura¬†17.37, potrebbero non essere sempre catturate accuratamente dai modelli a livello software, poich√© possono nascondere gli effetti di sistema sottostanti.\n\nSangchoolie, Behrooz, Karthik Pattabiraman, e Johan Karlsson. 2017. ¬´One Bit is (Not) Enough: An Empirical Study of the Impact of Single and Multiple Bit-Flip Errors¬ª. In 2017 47th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN), 97‚Äì108. IEEE; IEEE. https://doi.org/10.1109/dsn.2017.30.\n\nPapadimitriou, George, e Dimitris Gizopoulos. 2021. ¬´Demystifying the System Vulnerability Stack: Transient Fault Effects Across the Layers¬ª. In 2021 ACM/IEEE 48th Annual International Symposium on Computer Architecture (ISCA), 902‚Äì15. IEEE; IEEE. https://doi.org/10.1109/isca52012.2021.00075.\n\nMohanram, K., e N. A. Touba. 2003. ¬´Partial error masking to reduce soft error failure rate in logic circuits¬ª. In Proceedings. 16th IEEE Symposium on Computer Arithmetic, 433‚Äì40. IEEE; IEEE Comput. Soc. https://doi.org/10.1109/dftvs.2003.1250141.\n\n\n\n\n\n\nFigura¬†17.37: Esempio di mascheramento degli errori nei componenti microarchitettonici (Ko 2021)\n\n\nKo, Yohan. 2021. ¬´Characterizing System-Level Masking Effects against Soft Errors¬ª. Electronics 10 (18): 2286. https://doi.org/10.3390/electronics10182286.\n\n\nAlcuni strumenti, come Fidelity (He, Balaprakash, e Li 2020), mirano a colmare il divario tra modelli di errore a livello hardware e software mappando i pattern tra i due livelli di astrazione (Cheng et al. 2016). Ci√≤ consente una modellazione pi√π accurata dei guasti hardware negli strumenti basati su software, essenziale per lo sviluppo di sistemi ML robusti e affidabili. Gli strumenti a pi√π basso livello in genere rappresentano caratteristiche di propagazione degli errori pi√π accurate, ma devono essere pi√π rapidi nella simulazione di molti errori a causa della natura complessa delle progettazioni dei sistemi hardware. D‚Äôaltro canto, gli strumenti a pi√π alto livello, come quelli implementati in framework ML come PyTorch o TensorFlow, di cui parleremo presto nelle sezioni successive, sono spesso pi√π rapidi ed efficienti per valutare la robustezza dei sistemi ML.\n\nCheng, Eric, Shahrzad Mirkhani, Lukasz G. Szafaryn, Chen-Yong Cher, Hyungmin Cho, Kevin Skadron, Mircea R. Stan, et al. 2016. ¬´Clear: uC/u ross u-L/u ayer uE/u xploration for uA/u rchitecting uR/u esilience - Combining hardware and software techniques to tolerate soft errors in processor cores¬ª. In Proceedings of the 53rd Annual Design Automation Conference, 1‚Äì6. ACM. https://doi.org/10.1145/2897937.2897996.\nNelle sottosezioni seguenti, discuteremo vari metodi e strumenti di iniezione di guasti basati su hardware e software, evidenziandone le capacit√†, le limitazioni e i modelli di guasti ed errori che supportano.\n\n\n17.6.2 Injection Hardware-based di Guasti\nUno strumento di ‚Äúiniezione di errori‚Äù √® uno strumento che consente all‚Äôutente di implementare un particolare modello di errore, come un singolo bit flip transitorio durante l‚Äôinferenza Figura¬†17.38. La maggior parte degli strumenti di iniezione di errori sono basati su software, poich√© sono pi√π rapidi per gli studi di robustezza ML. Tuttavia, i metodi di iniezione di guasti basati su hardware sono ancora importanti per radicare i modelli di errore ad alto livello, poich√© sono considerati il modo pi√π accurato per studiare l‚Äôimpatto dei guasti sui sistemi ML manipolando direttamente l‚Äôhardware per introdurli. Questi metodi consentono ai ricercatori di osservare il comportamento del sistema in condizioni di guasti reali. In questa sezione vengono descritti in modo pi√π dettagliato sia gli strumenti di iniezione di errori basati su software che quelli basati su hardware.\n\n\n\n\n\n\nFigura¬†17.38: Gli errori hardware possono verificarsi per una serie di motivi e in momenti e/o posizioni diverse in un sistema, il che pu√≤ essere esplorato quando si studia l‚Äôimpatto degli errori basati sull‚Äôhardware sui sistemi (Ahmadilivani et al. 2024)\n\n\nAhmadilivani, Mohammad Hasan, Mahdi Taheri, Jaan Raik, Masoud Daneshtalab, e Maksim Jenihhin. 2024. ¬´A Systematic Literature Review on Hardware Reliability Assessment Methods for Deep Neural Networks¬ª. ACM Comput. Surv. 56 (6): 1‚Äì39. https://doi.org/10.1145/3638242.\n\n\n\nMetodi\nDue dei metodi di iniezione di guasti basati su hardware pi√π comuni sono quelli basati su FPGA e il test di radiazione o di fascio.\nIniezione di Guasti FPGA-based: I ‚ÄúField-Programmable Gate Array (FPGA)‚Äù sono circuiti integrati riconfigurabili che possono essere programmati per implementare vari progetti hardware. Nel contesto dell‚Äôiniezione di guasti, gli FPGA offrono elevata precisione e accuratezza, poich√© i ricercatori possono mirare a bit specifici o set di bit all‚Äôinterno dell‚Äôhardware. Modificando la configurazione dell‚ÄôFPGA, i guasti possono essere introdotti in posizioni e tempi specifici durante l‚Äôesecuzione di un modello ML. L‚Äôiniezione di guasti basata su FPGA consente un controllo dettagliato sul ‚Äúfault model‚Äù, consentendo ai ricercatori di studiare l‚Äôimpatto di diversi tipi di guasti, come i flip di bit singoli o gli errori multi-bit. Questo livello di controllo rende l‚Äôiniezione di guasti basata su FPGA uno strumento prezioso per comprendere la resilienza dei sistemi ML ai guasti hardware.\nTest di Radiazioni o Fasci: Il test di radiazioni o fasci (Velazco, Foucard, e Peronnard 2010) comporta l‚Äôesposizione dell‚Äôhardware che esegue un modello ML a particelle ad alta energia, come protoni o neutroni, come illustrato in Figura¬†17.39. Queste particelle possono causare bitflip o altri tipi di guasti nell‚Äôhardware, imitando gli effetti di quelli indotti dalle radiazioni nel mondo reale. Il test di fasci √® ampiamente considerato un metodo altamente accurato per misurare il tasso di errore indotto da impatti di particelle su un‚Äôapplicazione in esecuzione. Fornisce una rappresentazione realistica dei guasti in ambienti reali, in particolare in applicazioni esposte ad alti livelli di radiazioni, come sistemi spaziali o esperimenti di fisica delle particelle. Tuttavia, a differenza dell‚Äôiniezione di guasti basata su FPGA, il test di fasci potrebbe essere pi√π preciso nel puntare a bit o componenti specifici all‚Äôinterno dell‚Äôhardware, poich√© potrebbe essere difficile puntare il fascio di particelle a un bit particolare nell‚Äôhardware. Nonostante sia piuttosto costoso dal punto di vista della ricerca, il test del fascio √® una pratica industriale molto apprezzata per l‚Äôaffidabilit√†.\n\nVelazco, Raoul, Gilles Foucard, e Paul Peronnard. 2010. ¬´Combining Results of Accelerated Radiation Tests and Fault Injections to Predict the Error Rate of an Application Implemented in SRAM-Based FPGAs¬ª. IEEE Trans. Nucl. Sci. 57 (6): 3500‚Äì3505. https://doi.org/10.1109/tns.2010.2087355.\n\n\n\n\n\n\nFigura¬†17.39: Configurazione del test di radiazione per componenti semiconduttori (Lee et al. 2022) Fonte: JD Instrument\n\n\nLee, Minwoong, Namho Lee, Huijeong Gwon, Jongyeol Kim, Younggwan Hwang, e Seongik Cho. 2022. ¬´Design of Radiation-Tolerant High-Speed Signal Processing Circuit for Detecting Prompt Gamma Rays by Nuclear Explosion¬ª. Electronics 11 (18): 2970. https://doi.org/10.3390/electronics11182970.\n\n\n\n\nLimitazioni\nNonostante la loro elevata accuratezza, i metodi di iniezione di guasti basati su hardware presentano diverse limitazioni che possono ostacolarne l‚Äôadozione diffusa:\nCosto: L‚Äôiniezione di guasti e il test del fascio basati su FPGA richiedono hardware e strutture specializzate, la cui configurazione e manutenzione possono essere costose. Il costo di questi metodi pu√≤ rappresentare un ostacolo significativo per ricercatori e organizzazioni con risorse limitate.\nScalabilit√†: I metodi basati su hardware sono generalmente pi√π lenti e meno scalabili rispetto ai metodi basati su software. L‚Äôiniezione di guasti e la raccolta di dati sull‚Äôhardware possono richiedere tempo, limitando il numero di esperimenti eseguiti in un determinato lasso di tempo. Ci√≤ pu√≤ essere particolarmente impegnativo quando si studia la resilienza di sistemi ML su larga scala o si conducono analisi statistiche che richiedono molti esperimenti di iniezione di guasti.\nFlessibilit√†: I metodi basati su hardware potrebbero non essere flessibili quanto quelli basati su software in termini di gamma di modelli di guasto e modelli di errore che possono supportare. Modificare la configurazione hardware o l‚Äôimpostazione sperimentale per adattarsi a diversi modelli di errore pu√≤ essere pi√π impegnativo e richiedere pi√π tempo rispetto ai metodi basati su software.\nNonostante queste limitazioni, i metodi di iniezione di errori basati su hardware rimangono strumenti essenziali per convalidare l‚Äôaccuratezza dei metodi basati su software e per studiare l‚Äôimpatto degli errori sui sistemi ML in contesti realistici. Combinando metodi basati su hardware e basati su software, i ricercatori possono acquisire una comprensione pi√π completa della resilienza dei sistemi ML ai guasti hardware e sviluppare strategie di mitigazione efficaci.\n\n\n\n17.6.3 Strumenti di Injection di Guasti Software-based\nCon il rapido sviluppo di framework ML negli ultimi anni, gli strumenti di iniezione di guasti basati su software hanno guadagnato popolarit√† nello studio della resilienza dei sistemi ML ai guasti hardware. Questi strumenti simulano gli effetti dei guasti hardware modificando la rappresentazione software del modello ML o il grafo computazionale sottostante. L‚Äôascesa di framework ML come TensorFlow, PyTorch e Keras ha facilitato lo sviluppo di strumenti di iniezione di guasti che sono strettamente integrati con questi framework, rendendo pi√π facile per i ricercatori condurre esperimenti di iniezione di guasti e analizzare i risultati.\n\nVantaggi e Compromessi\nGli strumenti di iniezione di guasti basati su software offrono diversi vantaggi rispetto a quelli basati su hardware:\nVelocit√†: Gli strumenti basati su software sono generalmente pi√π rapidi dei metodi basati su hardware, poich√© non richiedono la modifica dell‚Äôhardware fisico o la configurazione di apparecchiature specializzate. Ci√≤ consente ai ricercatori di condurre pi√π esperimenti di iniezione di guasti in tempi pi√π brevi, consentendo analisi pi√π complete della resilienza dei sistemi ML.\nFlessibilit√†: Gli strumenti basati su software sono pi√π flessibili di quelli basati su hardware in termini di gamma di modelli di guasti ed errori che possono supportare. I ricercatori possono facilmente modificare l‚Äôimplementazione software dello strumento di iniezione di guasti per adattarsi a diversi modelli di guasti o per indirizzare componenti specifici del sistema ML.\nAccessibilit√†: Gli strumenti basati su software sono pi√π accessibili dei metodi basati su hardware, poich√© non richiedono hardware o strutture specializzate. Ci√≤ semplifica per ricercatori e professionisti condurre esperimenti di iniezione di guasti e studiare la resilienza dei sistemi ML, anche con risorse limitate.\n\n\nLimitazioni\nGli strumenti di iniezione di guasti basati su software presentano anche alcune limitazioni rispetto ai metodi basati su hardware:\nPrecisione: Gli strumenti basati su software potrebbero non sempre catturano l‚Äôintera gamma di effetti che i guasti hardware possono avere sul sistema. Poich√© questi strumenti operano a un livello di astrazione pi√π elevato, potrebbero dover recuperare alcune delle interazioni hardware di basso livello e dei meccanismi di propagazione degli errori che possono influire sul comportamento del sistema ML.\nFedelt√†: Gli strumenti basati su software potrebbero fornire un livello di fedelt√† diverso rispetto ai metodi basati su hardware in termini di rappresentazione delle condizioni di guasto del mondo reale. L‚Äôaccuratezza dei risultati ottenuti dagli esperimenti di iniezione di guasti basati su software potrebbe dipendere da quanto il modello software si avvicini al comportamento hardware effettivo.\n\n\nTipi di Strumenti di Iniezione di Guasti\nGli strumenti di iniezione di guasti basati su software possono essere categorizzati in base ai loro framework di destinazione o casi d‚Äôuso. Qui, discuteremo alcuni degli strumenti pi√π popolari in ciascuna categoria:\nAres (Reagen et al. 2018), uno strumento di iniezione di guasti inizialmente sviluppato per il framework Keras nel 2018, √® emerso come uno dei primi strumenti per studiare l‚Äôimpatto dei guasti hardware sulle reti deep neural network (DNN) nel contesto della crescente popolarit√† dei framework ML a met√†-fine anni 2010. Lo strumento √® stato convalidato rispetto a un acceleratore DNN implementato in silicio, dimostrando la sua efficacia nella modellazione dei guasti hardware. Ares fornisce uno studio completo sull‚Äôimpatto dei guasti hardware sia nei pesi che nei valori di attivazione, caratterizzando gli effetti dei flip di bit singoli e dei bit-error rate (BER) sulle strutture hardware. Successivamente, il framework Ares √® stato esteso per supportare l‚Äôecosistema PyTorch, consentendo ai ricercatori di investigare i guasti hardware in un contesto pi√π moderno e ampliando ulteriormente la sua utilit√† sul campo.\n\nReagen, Brandon, Udit Gupta, Lillian Pentecost, Paul Whatmough, Sae Kyu Lee, Niamh Mulholland, David Brooks, e Gu-Yeon Wei. 2018. ¬´Ares: A framework for quantifying the resilience of deep neural networks¬ª. In 2018 55th ACM/ESDA/IEEE Design Automation Conference (DAC), 1‚Äì6. IEEE. https://doi.org/10.1109/dac.2018.8465834.\n\nMahmoud, Abdulrahman, Neeraj Aggarwal, Alex Nobbe, Jose Rodrigo Sanchez Vicarte, Sarita V. Adve, Christopher W. Fletcher, Iuri Frosio, e Siva Kumar Sastry Hari. 2020. ¬´PyTorchFI: A Runtime Perturbation Tool for DNNs¬ª. In 2020 50th Annual IEEE/IFIP International Conference on Dependable Systems and Networks Workshops (DSN-W), 25‚Äì31. IEEE; IEEE. https://doi.org/10.1109/dsn-w50199.2020.00014.\nPyTorchFI (Mahmoud et al. 2020), uno strumento di iniezione di guasti progettato specificamente per il framework PyTorch, √® stato sviluppato nel 2020 in collaborazione con Nvidia Research. Consente l‚Äôiniezione di guasti nei pesi, nelle attivazioni e nei gradienti dei modelli PyTorch, supportando un‚Äôampia gamma di modelli di guasti. Sfruttando le capacit√† di accelerazione GPU di PyTorch, PyTorchFI fornisce un‚Äôimplementazione rapida ed efficiente per condurre esperimenti di iniezione di guasti su sistemi ML su larga scala, come mostrato in Figura¬†17.40.\n\n\n\n\n\n\nFigura¬†17.40: I bitflip hardware nei carichi di lavoro ML possono causare oggetti fantasma e classificazioni errate, che possono essere erroneamente utilizzati a valle da sistemi pi√π grandi, come nella guida autonoma. Quella mostrata sopra √® una versione corretta e difettosa della stessa immagine che utilizza il framework di iniezione PyTorchFI.\n\n\n\nLa velocit√† e la facilit√† d‚Äôuso dello strumento hanno portato a un‚Äôadozione diffusa nella comunit√†, con conseguenti molteplici progetti guidati dagli sviluppatori, come PyTorchALFI di Intel xColabs, che si concentra sulla sicurezza negli ambienti automobilistici. Gli strumenti successivi incentrati su PyTorch per l‚Äôiniezione di guasti includono Dr.¬†DNA di Meta (Ma et al. 2024) (che facilita ulteriormente il modello di programmazione ‚ÄúPythonico‚Äù per facilit√† d‚Äôuso) e il framework GoldenEye (Mahmoud et al. 2022), che incorpora nuovi tipi di dati numerici (come AdaptivFloat (Tambe et al. 2020) e BlockFloat nel contesto di bit flip hardware.\n\nMa, Dongning, Fred Lin, Alban Desmaison, Joel Coburn, Daniel Moore, Sriram Sankar, e Xun Jiao. 2024. ¬´Dr. DNA: Combating Silent Data Corruptions in Deep Learning using Distribution of Neuron Activations¬ª. In Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3, 239‚Äì52. ACM. https://doi.org/10.1145/3620666.3651349.\n\nMahmoud, Abdulrahman, Thierry Tambe, Tarek Aloui, David Brooks, e Gu-Yeon Wei. 2022. ¬´GoldenEye: A Platform for Evaluating Emerging Numerical Data Formats in DNN Accelerators¬ª. In 2022 52nd Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN), 206‚Äì14. IEEE. https://doi.org/10.1109/dsn53405.2022.00031.\n\nTambe, Thierry, En-Yu Yang, Zishen Wan, Yuntian Deng, Vijay Janapa Reddi, Alexander Rush, David Brooks, e Gu-Yeon Wei. 2020. ¬´Algorithm-Hardware Co-Design of Adaptive Floating-Point Encodings for Resilient Deep Learning Inference¬ª. In 2020 57th ACM/IEEE Design Automation Conference (DAC), 1‚Äì6. IEEE; IEEE. https://doi.org/10.1109/dac18072.2020.9218516.\n\nChen, Zitao, Niranjhana Narayanan, Bo Fang, Guanpeng Li, Karthik Pattabiraman, e Nathan DeBardeleben. 2020. ¬´TensorFI: A Flexible Fault Injection Framework for TensorFlow Applications¬ª. In 2020 IEEE 31st International Symposium on Software Reliability Engineering (ISSRE), 426‚Äì35. IEEE; IEEE. https://doi.org/10.1109/issre5003.2020.00047.\n\nChen, Zitao, Guanpeng Li, Karthik Pattabiraman, e Nathan DeBardeleben. 2019. ¬´iBinFI/i: an efficient fault injector for safety-critical machine learning systems¬ª. In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis. SC ‚Äô19. New York, NY, USA: ACM. https://doi.org/10.1145/3295500.3356177.\nTensorFI (Chen et al. 2020), o TensorFlow Fault Injector, √® uno strumento di iniezione di guasti sviluppato specificamente per il framework TensorFlow. Analogo ad Ares e PyTorchFI, TensorFI √® considerato lo strumento all‚Äôavanguardia per gli studi di robustezza ML nell‚Äôecosistema TensorFlow. Consente ai ricercatori di iniettare guasti nel grafo computazionale di Modelli TensorFlow e studia il loro impatto sulle prestazioni del modello, supportando un‚Äôampia gamma di modelli di errore. Uno dei principali vantaggi di TensorFI √® la sua capacit√† di valutare la resilienza di vari modelli ML, non solo DNN. Ulteriori progressi, come BinFi (Chen et al. 2019), forniscono un meccanismo per accelerare gli esperimenti di iniezione di errori concentrandosi sui bit ‚Äúimportanti‚Äù nel sistema, accelerando il processo di analisi della robustezza ML e dando priorit√† ai componenti critici di un modello.\nNVBitFI (T. Tsai et al. 2021), uno strumento di iniezione di errori generico sviluppato da Nvidia per le sue piattaforme GPU, opera a un pi√π basso livello rispetto a strumenti specifici del framework come Ares, PyTorchFI e TensorFlow. Mentre questi strumenti si concentrano su varie piattaforme di deep learning per implementare ed eseguire analisi di robustezza, NVBitFI mira al codice di assemblaggio hardware sottostante per l‚Äôiniezione di guasti. Ci√≤ consente ai ricercatori di iniettare guasti in qualsiasi applicazione in esecuzione su GPU Nvidia, rendendolo uno strumento versatile per studiare la resilienza dei sistemi ML e di altre applicazioni accelerate da GPU. Consentendo agli utenti di iniettare errori a livello di architettura, NVBitFI fornisce un modello di guasto pi√π generico che non √® limitato ai soli modelli ML. Poich√© i sistemi GPU di Nvidia sono comunemente utilizzati in molti sistemi basati su ML, NVBitFI √® uno strumento prezioso per un‚Äôanalisi completa dell‚Äôiniezione di guasti in varie applicazioni.\n\nTsai, Timothy, Siva Kumar Sastry Hari, Michael Sullivan, Oreste Villa, e Stephen W. Keckler. 2021. ¬´NVBitFI: Dynamic Fault Injection for GPUs¬ª. In 2021 51st Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN), 284‚Äì91. IEEE; IEEE. https://doi.org/10.1109/dsn48987.2021.00041.\n\nEsempi specifici di dominio\nSono stati sviluppati strumenti di iniezione di guasti specifici per dominio per affrontare le sfide e i requisiti unici di vari domini applicativi ML, come veicoli autonomi e robotica. Questa sezione evidenzia tre strumenti di iniezione di guasti specifici per dominio: DriveFI e PyTorchALFI per veicoli autonomi e MAVFI per ‚Äúuncrewed aerial vehicles (UAV)‚Äù [veicoli aerei senza equipaggio]. Questi strumenti consentono ai ricercatori di iniettare guasti hardware nei sottosistemi di percezione, controllo e altri sistemi complessi, consentendo loro di studiare l‚Äôimpatto dei guasti sulle prestazioni e sulla sicurezza del sistema. Lo sviluppo di questi strumenti di iniezione di guasti basati su software ha notevolmente ampliato le capacit√† della comunit√† ML di sviluppare sistemi pi√π robusti e affidabili in grado di funzionare in modo sicuro ed efficace in presenza di guasti hardware.\nDriveFI (Jha et al. 2019) √® uno strumento di iniezione di guasti progettato per veicoli autonomi. Consente l‚Äôiniezione di guasti hardware nelle pipeline di percezione e controllo dei sistemi di veicoli autonomi, consentendo ai ricercatori di studiare l‚Äôimpatto di questi guasti sulle prestazioni e sulla sicurezza del sistema. DriveFI √® stato integrato con piattaforme di guida autonoma standard del settore, come Nvidia DriveAV e Baidu Apollo, rendendolo uno strumento prezioso per valutare la resilienza dei sistemi di veicoli autonomi.\n\nJha, Saurabh, Subho Banerjee, Timothy Tsai, Siva K. S. Hari, Michael B. Sullivan, Zbigniew T. Kalbarczyk, Stephen W. Keckler, e Ravishankar K. Iyer. 2019. ¬´ML-Based Fault Injection for Autonomous Vehicles: A Case for Bayesian Fault Injection¬ª. In 2019 49th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN), 112‚Äì24. IEEE; IEEE. https://doi.org/10.1109/dsn.2019.00025.\n\nGr√§fe, Ralf, Qutub Syed Sha, Florian Geissler, e Michael Paulitsch. 2023. ¬´Large-Scale Application of Fault Injection into PyTorch Models -an Extension to PyTorchFI for Validation Efficiency¬ª. In 2023 53rd Annual IEEE/IFIP International Conference on Dependable Systems and Networks - Supplemental Volume (DSN-S), 56‚Äì62. IEEE; IEEE. https://doi.org/10.1109/dsn-s58398.2023.00025.\nPyTorchALFI (Gr√§fe et al. 2023) √® un‚Äôestensione di PyTorchFI sviluppata da Intel xColabs per il dominio dei veicoli autonomi. Si basa sulle capacit√† di inserimento di guasti di PyTorchFI. Aggiunge funzionalit√† specificamente studiate per valutare la resilienza dei sistemi di veicoli autonomi, come la capacit√† di inserire guasti nei dati della telecamera e del sensore LiDAR.\nMAVFI (Hsiao et al. 2023) √® uno strumento di inserimento di guasti progettato per il dominio della robotica, in particolare per i veicoli aerei senza equipaggio (UAV). MAVFI √® basato sul framework Robot Operating System (ROS) e consente ai ricercatori di inserire guasti nei vari componenti di un sistema UAV, come sensori, attuatori e algoritmi di controllo. Valutando l‚Äôimpatto di questi guasti sulle prestazioni e sulla stabilit√† del UAV, i ricercatori possono sviluppare sistemi UAV pi√π resilienti e tolleranti ai guasti.\n\nHsiao, Yu-Shun, Zishen Wan, Tianyu Jia, Radhika Ghosal, Abdulrahman Mahmoud, Arijit Raychowdhury, David Brooks, Gu-Yeon Wei, e Vijay Janapa Reddi. 2023. ¬´MAVFI: An End-to-End Fault Analysis Framework with Anomaly Detection and Recovery for Micro Aerial Vehicles¬ª. In 2023 Design, Automation &amp; Test in Europe Conference &amp; Exhibition (DATE), 1‚Äì6. IEEE; IEEE. https://doi.org/10.23919/date56975.2023.10137246.\nLo sviluppo di strumenti di iniezione di guasti basati su software ha notevolmente ampliato le capacit√† di ricercatori e professionisti di studiare la resilienza dei sistemi ML ai guasti hardware. Sfruttando la velocit√†, la flessibilit√† e l‚Äôaccessibilit√† di questi strumenti, la comunit√† ML pu√≤ sviluppare sistemi pi√π robusti e affidabili in grado di funzionare in modo sicuro ed efficace in presenza di guasti hardware.\n\n\n\n\n17.6.4 Colmare il Divario tra Modelli di Errore Hardware e Software\nSebbene gli strumenti di iniezione di guasti basati su software offrano molti vantaggi in termini di velocit√†, flessibilit√† e accessibilit√†, potrebbero non sempre catturare accuratamente l‚Äôintera gamma di effetti che i guasti hardware possono avere sul sistema. Questo perch√© gli strumenti basati su software operano a un livello di astrazione pi√π alto rispetto ai metodi basati su hardware e potrebbero non rilevare alcune delle interazioni hardware di basso livello e dei meccanismi di propagazione degli errori che possono influire sul comportamento del sistema ML.\nCome illustra Bolchini et al. (2023) nel suo lavoro, gli errori hardware possono manifestarsi in complessi pattern di distribuzione spaziale che sono difficili da replicare completamente con la sola iniezione di guasti basata su software. Identificano quattro pattern distinti: (a) singolo punto, in cui il guasto corrompe un singolo valore in una feature map; (b) stessa riga, in cui il guasto corrompe una riga parziale o intera in una singola feature map; (c) bullet wake, in cui il guasto corrompe la stessa posizione su pi√π feature map; e (d) shatter glass, che combina gli effetti dei pattern della stessa riga e bullet wake, come mostrato in Figura¬†17.41. Questi intricati meccanismi di propagazione degli errori evidenziano la necessit√† di tecniche di iniezione di guasti consapevoli dell‚Äôhardware per valutare accuratamente la resilienza dei sistemi ML.\n\n\n\n\n\n\nFigura¬†17.41: Gli errori hardware possono manifestarsi in modi diversi a livello software, come classificato da Bolchini et al. (Bolchini et al. 2023)\n\n\nBolchini, Cristiana, Luca Cassano, Antonio Miele, e Alessandro Toschi. 2023. ¬´Fast and Accurate Error Simulation for CNNs Against Soft Errors¬ª. IEEE Trans. Comput. 72 (4): 984‚Äì97. https://doi.org/10.1109/tc.2022.3184274.\n\n\nI ricercatori hanno sviluppato strumenti per affrontare questo problema colmando il divario tra modelli di errore hardware di basso livello e modelli di errore software di livello superiore. Uno di questi strumenti √® Fidelity, progettato per mappare i pattern tra guasti a livello hardware e le loro manifestazioni a livello software.\n\nFidelity: Colmare il Gap\nFidelity (He, Balaprakash, e Li 2020) √® uno strumento per modellare accuratamente i guasti hardware negli esperimenti di iniezione di guasti basati su software. Ci√≤ avviene studiando attentamente la relazione tra i guasti a livello hardware e il loro impatto sulla rappresentazione software del sistema ML.\n\nHe, Yi, Prasanna Balaprakash, e Yanjing Li. 2020. ¬´FIdelity: Efficient Resilience Analysis Framework for Deep Learning Accelerators¬ª. In 2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO), 270‚Äì81. IEEE; IEEE. https://doi.org/10.1109/micro50266.2020.00033.\nLe intuizioni chiave alla base di Fidelity sono:\n\nPropagazione dei Guasti: Fidelity modella il modo in cui gli errori si propagano attraverso l‚Äôhardware e si manifestano come errori nello stato del sistema visibili al software. Comprendendo questi pattern di propagazione, Fidelity pu√≤ simulare con maggiore accuratezza gli effetti dei guasti hardware negli esperimenti basati sul software.\nEquivalenza dei Guasti: Fidelity identifica classi equivalenti di guasti hardware che producono errori simili a livello software. Ci√≤ consente ai ricercatori di progettare modelli di guasti basati sul software che siano rappresentativi dei guasti hardware sottostanti senza la necessit√† di modellare ogni possibile guasto hardware singolarmente.\nApproccio a Strati: Fidelity impiega un approccio a strati alla modellazione dei guasti, in cui gli effetti dei guasti hardware vengono propagati attraverso pi√π livelli di astrazione, dall‚Äôhardware al livello software. Questo approccio garantisce che i modelli di guasti basati sul software siano basati sul comportamento effettivo dell‚Äôhardware.\n\nIncorporando queste informazioni, Fidelity consente agli strumenti di iniezione di guasti basati su software di catturare con precisione gli effetti dei guasti hardware sui sistemi ML. Ci√≤ √® particolarmente importante per le applicazioni critiche per la sicurezza, in cui la resilienza del sistema ai guasti hardware √® fondamentale.\n\n\nL‚ÄôImportanza di Catturare il Vero Comportamento Hardware\nCatturare il vero comportamento hardware negli strumenti di iniezione di guasti basati su software √® fondamentale per diversi motivi:\n\nPrecisione: Modellando con precisione gli effetti dei guasti hardware, gli strumenti basati su software possono fornire informazioni pi√π affidabili sulla resilienza dei sistemi ML. Ci√≤ √® essenziale per progettare e convalidare sistemi tolleranti ai guasti che possono funzionare in modo sicuro ed efficace in presenza di guasti hardware.\nRiproducibilit√†: Quando gli strumenti basati su software catturano con precisione il comportamento hardware, gli esperimenti di iniezione di guasti diventano pi√π riproducibili su diverse piattaforme e ambienti. Ci√≤ √® importante per lo studio scientifico della resilienza del sistema ML, poich√© consente ai ricercatori di confrontare e convalidare i risultati su diversi studi e implementazioni.\nEfficienza: Gli strumenti basati su software che catturano il vero comportamento dell‚Äôhardware possono essere pi√π efficienti nei loro esperimenti di iniezione di guasti concentrandosi sui modelli di guasti pi√π rappresentativi e impattanti. Ci√≤ consente ai ricercatori di coprire una gamma pi√π ampia di scenari di guasti e configurazioni di sistema con risorse computazionali limitate.\nStrategie di Mitigazione: Comprendere come i guasti hardware si manifestano a livello software √® fondamentale per sviluppare strategie di mitigazione efficaci. Catturando con precisione il comportamento dell‚Äôhardware, gli strumenti di iniezione di guasti basati su software possono aiutare i ricercatori a identificare i componenti pi√π vulnerabili del sistema ML e progettare tecniche di rafforzamento mirate per migliorare la resilienza.\n\nStrumenti come Fidelity sono essenziali per far progredire lo stato dell‚Äôarte nella ricerca sulla resilienza del sistema ML. Questi strumenti consentono ai ricercatori di condurre esperimenti di iniezione di guasti pi√π accurati, riproducibili ed efficienti colmando il divario tra modelli di errore hardware e software. Man mano che la complessit√† e la criticit√† dei sistemi ML continuano a crescere, l‚Äôimportanza di catturare il vero comportamento hardware negli strumenti di iniezione di guasti basati su software diventer√† sempre pi√π evidente.\nLa ricerca in corso in quest‚Äôarea cerca di perfezionare la mappatura tra modelli di errore hardware e software e di sviluppare nuove tecniche per simulare in modo efficiente i guasti hardware negli esperimenti basati su software. Man mano che questi strumenti maturano, forniranno alla comunit√† ML mezzi sempre pi√π potenti e accessibili per studiare e migliorare la resilienza dei sistemi ML ai guasti hardware.",
    "crumbs": [
      "<span class='chapter-number'>17</span>¬† <span class='chapter-title'>IA Robusta</span>"
    ]
  },
  {
    "objectID": "contents/core/robust_ai/robust_ai.it.html#conclusione",
    "href": "contents/core/robust_ai/robust_ai.it.html#conclusione",
    "title": "17¬† IA Robusta",
    "section": "17.7 Conclusione",
    "text": "17.7 Conclusione\nSviluppare un‚ÄôIA solida e resiliente √® fondamentale man mano che i sistemi di apprendimento automatico diventano sempre pi√π integrati in applicazioni critiche per la sicurezza e in ambienti reali. Questo capitolo ha esplorato le principali sfide alla robustezza dell‚ÄôIA derivanti da guasti hardware, attacchi dannosi, cambiamenti di distribuzione e bug software.\nAlcune delle conclusioni principali includono quanto segue:\n\nGuasti Hardware: Guasti transitori, permanenti e intermittenti nei componenti hardware possono corrompere i calcoli e degradare le prestazioni dei modelli di apprendimento automatico se non vengono rilevati e mitigati correttamente. Tecniche come ridondanza, correzione degli errori e progetti fault-tolerant svolgono un ruolo cruciale nella creazione di sistemi ML resilienti in grado di resistere ai guasti hardware.\nRobustezza del Modello: Gli attori malintenzionati possono sfruttare le vulnerabilit√† nei modelli ML tramite attacchi avversari e avvelenamento dei dati, mirando a indurre classificazioni errate mirate, distorcere il comportamento appreso del modello o compromettere l‚Äôintegrit√† e l‚Äôaffidabilit√† del sistema. Inoltre, possono verificarsi ‚Äúdistribution shift‚Äù quando la distribuzione dei dati riscontrata durante l‚Äôimplementazione differisce da quella osservata durante il training, con conseguente degrado delle prestazioni. L‚Äôimplementazione di misure difensive, tra cui training avversario, rilevamento delle anomalie, architetture di modelli robuste e tecniche come adattamento del dominio, apprendimento per trasferimento e apprendimento continuo, √® essenziale per proteggersi da queste sfide e garantire l‚Äôaffidabilit√† e la generalizzazione del modello in ambienti dinamici.\nErrori Software: Gli errori nei framework ML, nelle librerie e negli stack software possono propagarsi, degradare le prestazioni e introdurre vulnerabilit√† di sicurezza. Test rigorosi, monitoraggio del runtime e adozione di ‚Äúdesign pattern‚Äù tolleranti agli errori sono essenziali per la creazione di un‚Äôinfrastruttura software robusta che supporti sistemi ML affidabili.\n\nPoich√© i sistemi ML affrontano attivit√† sempre pi√π complesse con conseguenze nel mondo reale, dare priorit√† alla resilienza diventa fondamentale. Gli strumenti e i framework discussi in questo capitolo, tra cui tecniche di ‚Äúfault injection‚Äù [iniezione di guasti], metodi di analisi degli errori e framework di valutazione della robustezza, forniscono ai professionisti i mezzi per testare a fondo e rafforzare i propri sistemi ML contro varie modalit√† di errore e condizioni avverse.\nAndando avanti, la resilienza deve essere un obiettivo centrale durante l‚Äôintero ciclo di vita dello sviluppo dell‚ÄôIA, dalla raccolta dei dati e dall‚Äôaddestramento del modello all‚Äôimplementazione e al monitoraggio. Affrontando in modo proattivo le molteplici sfide alla robustezza, possiamo sviluppare sistemi di apprendimento automatico affidabili e sicuri, in grado di affrontare le complessit√† e le incertezze degli ambienti del mondo reale.\nLa ricerca futura sul ML robusto dovrebbe continuare a far progredire le tecniche per rilevare e mitigare guasti, attacchi e ‚Äúshift‚Äù delle distribuzioni. Inoltre, esplorare nuovi paradigmi per lo sviluppo di architetture IA intrinsecamente resilienti, come sistemi auto-riparanti o meccanismi a prova di errore, sar√† fondamentale per spingere i confini della robustezza dell‚ÄôIA. Dando priorit√† alla resilienza e investendo nello sviluppo di sistemi di IA robusti, possiamo liberare il pieno potenziale delle tecnologie di apprendimento automatico, garantendone al contempo un‚Äôimplementazione sicura, affidabile e responsabile in applicazioni del mondo reale. Mentre l‚ÄôIA continua a plasmare il nostro futuro, la creazione di sistemi resilienti in grado di resistere alle sfide del mondo reale sar√† un fattore determinante per il successo e l‚Äôimpatto sociale di questa tecnologia trasformativa.",
    "crumbs": [
      "<span class='chapter-number'>17</span>¬† <span class='chapter-title'>IA Robusta</span>"
    ]
  },
  {
    "objectID": "contents/core/robust_ai/robust_ai.it.html#sec-robust-ai-resource",
    "href": "contents/core/robust_ai/robust_ai.it.html#sec-robust-ai-resource",
    "title": "17¬† IA Robusta",
    "section": "17.8 Risorse",
    "text": "17.8 Risorse\nEcco un elenco curato di risorse per supportare studenti e insegnanti nei loro percorsi di apprendimento e insegnamento. Lavoriamo continuamente per espandere questa raccolta e presto aggiungeremo nuovi esercizi.\n\n\n\n\n\n\nSlide\n\n\n\n\n\nQueste slide sono uno strumento prezioso per gli insegnanti per tenere lezioni e per gli studenti per rivedere il materiale secondo il proprio ritmo. Incoraggiamo sia gli studenti che gli insegnanti a sfruttare queste slide per migliorare la loro comprensione e facilitare un trasferimento efficace delle conoscenze.\n\nProssimamente.\n\n\n\n\n\n\n\n\n\n\nVideo\n\n\n\n\n\n\nProssimamente.\n\n\n\n\n\n\n\n\n\n\nEsercizi\n\n\n\n\n\nPer rafforzare i concetti trattati in questo capitolo, abbiamo curato una serie di esercizi che sfidano gli studenti ad applicare le proprie conoscenze e ad approfondire la propria comprensione.\n\nEsercizio¬†17.1\nEsercizio¬†17.2\nEsercizio¬†17.3\nEsercizio¬†17.4",
    "crumbs": [
      "<span class='chapter-number'>17</span>¬† <span class='chapter-title'>IA Robusta</span>"
    ]
  },
  {
    "objectID": "contents/core/generative_ai/generative_ai.it.html",
    "href": "contents/core/generative_ai/generative_ai.it.html",
    "title": "18¬† IA Generativa",
    "section": "",
    "text": "Prossimamente!\nImmaginate un capitolo che si scrive da solo e si adatta alla propria curiosit√†, generando nuove intuizioni mentre si legge. Stiamo lavorando a qualcosa di straordinario!\nQuesto capitolo trasformer√† il modo di leggere e imparare, generando dinamicamente contenuti man mano che si procede. Mentre perfezioniamo questa nuova entusiasmante funzionalit√†, speriamo che gli utenti si preparino per un‚Äôesperienza educativa dinamica e unica come. Segnate sul calendario la grande rivelazione e aggiungete questa pagina a quelle preferite.\nIl futuro dell‚Äôapprendimento generativo √® qui! ‚Äî Vijay Janapa Reddi",
    "crumbs": [
      "<span class='chapter-number'>18</span>¬† <span class='chapter-title'>IA Generativa</span>"
    ]
  },
  {
    "objectID": "contents/core/ai_for_good/ai_for_good.it.html",
    "href": "contents/core/ai_for_good/ai_for_good.it.html",
    "title": "19¬† AI for Good",
    "section": "",
    "text": "19.1 Panoramica\nPer darci un quadro attorno al quale riflettere sull‚Äôintelligenza artificiale per il bene sociale, seguiremo gli ‚ÄúObiettivi di Sviluppo Sostenibile delle Nazioni Unite (SDG)‚Äù. Gli SDG delle Nazioni Unite sono una raccolta di 17 obiettivi globali, mostrati in Figura¬†19.1, adottati dalle Nazioni Unite nel 2015 come parte dell‚ÄôAgenda 2030 per lo Sviluppo Sostenibile. Gli SDG affrontano le sfide globali relative a povert√†, disuguaglianza, cambiamenti climatici, degrado ambientale, prosperit√†, pace e giustizia.\nCi√≤ che rende speciali gli SDG √® che sono una raccolta di obiettivi interconnessi progettati per fungere da ‚Äúmodello condiviso per la pace e la prosperit√† per le persone e il pianeta, ora e in futuro‚Äù. Gli SDG enfatizzano gli aspetti ambientali, sociali ed economici interconnessi dello sviluppo sostenibile, ponendo la sostenibilit√† al centro.\nUno studio recente (Vinuesa et al. 2020) evidenzia l‚Äôinfluenza dell‚ÄôIA su tutti gli aspetti dello sviluppo sostenibile, in particolare sui 17 ‚ÄúSustainable Development Goals (SDG)‚Äù e 169 target definiti a livello internazionale nell‚ÄôAgenda 2030 per lo Sviluppo Sostenibile. Lo studio mostra che l‚ÄôIA pu√≤ fungere da abilitatore per 134 target attraverso miglioramenti tecnologici, ma evidenzia anche le sfide dell‚ÄôIA su alcuni target. Lo studio mostra che l‚ÄôIA pu√≤ avvantaggiare 67 target quando si considerano l‚ÄôIA e i risultati sociali. Tuttavia, mette anche in guardia sui problemi relativi all‚Äôimplementazione dell‚ÄôIA in paesi con valori culturali e ricchezza diversi.\nSebbene tutte le forme di IA e apprendimento automatico abbiano il potenziale per contribuire ai ‚ÄúSustainable Development Goals (SDG)‚Äù [Obiettivi di Sviluppo Sostenibile], questo capitolo si concentra su TinyML per la sua capacit√† unica di affrontare le sfide che si presentano in contesti con risorse limitate. I sistemi ML, in particolare quelli che si basano su infrastrutture cloud, spesso richiedono una notevole potenza di calcolo, una connettivit√† Internet costante e un investimento finanziario sostanziale, che pu√≤ limitarne l‚Äôadozione nelle regioni in via di sviluppo o nelle aree remote. Al contrario, TinyML consente soluzioni localizzate, a basso costo e a basso consumo energetico eseguendo modelli di apprendimento automatico efficienti direttamente sui microcontrollori. Queste qualit√† lo rendono particolarmente efficace per affrontare problemi come il monitoraggio agricolo, la diagnosi sanitaria in aree svantaggiate e la conservazione ambientale in cui l‚Äôinfrastruttura potrebbe essere minima.\nConcentrandosi su TinyML, questo capitolo evidenzia un ramo dell‚ÄôIA che fornisce soluzioni pratiche e localizzate in grado di funzionare indipendentemente dalle richieste di energia e connettivit√† tipicamente associate a implementazioni ML su larga scala. TinyML si allinea bene con l‚Äôenfasi degli SDG sulla sostenibilit√† e l‚Äôaccessibilit√† offrendo innovazioni scalabili che affrontano le sfide globali in contesti con risorse limitate.\nNel contesto di questo libro, TinyML potrebbe contribuire a promuovere i seguenti obiettivi SDG:\nLa portabilit√†, i requisiti di potenza inferiori e l‚Äôanalisi in tempo reale abilitati da TinyML lo rendono adatto ad affrontare diverse sfide di sostenibilit√† che le regioni in via di sviluppo si trovano ad affrontare. L‚Äôampia distribuzione di soluzioni di alimentazione ha il potenziale per fornire un monitoraggio localizzato e conveniente per aiutare a raggiungere alcuni degli Obiettivi di Sviluppo Sostenibile delle Nazioni Unite. Nelle restanti sezioni, approfondiremo il modo in cui TinyML √® utile in molti settori che possono affrontare le SDG delle Nazioni Unite.",
    "crumbs": [
      "<span class='chapter-number'>19</span>¬† <span class='chapter-title'>AI for Good</span>"
    ]
  },
  {
    "objectID": "contents/core/ai_for_good/ai_for_good.it.html#panoramica",
    "href": "contents/core/ai_for_good/ai_for_good.it.html#panoramica",
    "title": "19¬† AI for Good",
    "section": "",
    "text": "Figura¬†19.1: United Nations Sustainable Development Goals (SDG). Fonte: United Nations.\n\n\n\n\n\n\nVinuesa, Ricardo, Hossein Azizpour, Iolanda Leite, Madeline Balaam, Virginia Dignum, Sami Domisch, Anna Fell√§nder, Simone Daniela Langhans, Max Tegmark, e Francesco Fuso Nerini. 2020. ¬´The role of artificial intelligence in achieving the Sustainable Development Goals¬ª. Nat. Commun. 11 (1): 1‚Äì10. https://doi.org/10.1038/s41467-019-14108-y.\n\n\n\n\nObiettivo 1 - Nessuna Povert√†: TinyML potrebbe aiutare a fornire soluzioni a basso costo per il monitoraggio delle colture per migliorare le rese agricole nei paesi in via di sviluppo.\nObiettivo 2 - Zero Fame: TinyML potrebbe consentire un monitoraggio localizzato e preciso della salute delle colture e il rilevamento delle malattie per ridurre le perdite di raccolto.\nObiettivo 3 - Buona Salute e Benessere: TinyML potrebbe aiutare a abilitare strumenti di diagnosi medica a basso costo per la diagnosi precoce e la prevenzione delle malattie nelle aree remote.\nObiettivo 6 - Acqua Pulita e Servizi igienici: TinyML potrebbe monitorare la qualit√† dell‚Äôacqua e rilevare i contaminanti per garantire l‚Äôaccesso all‚Äôacqua potabile.\nObiettivo 7 - Energia Pulita e Accessibile: TinyML potrebbe ottimizzare il consumo di energia e consentire la manutenzione predittiva per le infrastrutture di energia rinnovabile.\nObiettivo 11 - Citt√† e Comunit√† Sostenibili: TinyML potrebbe consentire una gestione intelligente del traffico, il monitoraggio della qualit√† dell‚Äôaria e una gestione ottimizzata delle risorse nelle citt√† intelligenti.\nObiettivo 13 - Azione per il Clima: TinyML potrebbe monitorare la deforestazione e tracciare gli sforzi di riforestazione. Potrebbe anche aiutare a prevedere eventi meteorologici estremi.",
    "crumbs": [
      "<span class='chapter-number'>19</span>¬† <span class='chapter-title'>AI for Good</span>"
    ]
  },
  {
    "objectID": "contents/core/ai_for_good/ai_for_good.it.html#agricoltura",
    "href": "contents/core/ai_for_good/ai_for_good.it.html#agricoltura",
    "title": "19¬† AI for Good",
    "section": "19.2 Agricoltura",
    "text": "19.2 Agricoltura\nL‚Äôagricoltura √® essenziale per raggiungere molti degli Obiettivi di Sviluppo Sostenibile delle Nazioni Unite, tra cui l‚Äôeradicazione della fame e della malnutrizione, la promozione della crescita economica e l‚Äôuso sostenibile delle risorse naturali. TinyML pu√≤ essere uno strumento prezioso per aiutare a promuovere un‚Äôagricoltura sostenibile, in particolare per i piccoli agricoltori nelle regioni in via di sviluppo.\nLe soluzioni TinyML possono fornire monitoraggio in tempo reale e analisi dei dati per la salute delle colture e le condizioni di crescita‚Äîil tutto senza dipendere dall‚Äôinfrastruttura di connettivit√†. Ad esempio, i moduli di telecamere a basso costo collegati ai microcontrollori possono monitorare malattie, parassiti e carenze nutrizionali. Gli algoritmi TinyML possono analizzare le immagini per rilevare i problemi in anticipo prima che si diffondano e danneggino i raccolti. Il monitoraggio di precisione pu√≤ ottimizzare input come acqua, fertilizzanti e pesticidi‚Äîmigliorando l‚Äôefficienza e la sostenibilit√†.\nAltri sensori, come unit√† GPS e accelerometri, possono tracciare le condizioni del microclima, l‚Äôumidit√† del suolo e il benessere del bestiame. I dati locali in tempo reale aiutano gli agricoltori a rispondere e ad adattarsi meglio ai cambiamenti sul campo. L‚Äôanalisi TinyML nell‚Äôedge evita ritardi, interruzioni di rete e gli elevati costi dei dati dei sistemi basati su cloud. I sistemi localizzati consentono la personalizzazione di colture, malattie e problemi regionali specifici.\nLe applicazioni TinyML diffuse possono aiutare a digitalizzare le piccole aziende agricole per aumentare produttivit√†, redditi e resilienza. Il basso costo dell‚Äôhardware e i requisiti minimi di connettivit√† rendono le soluzioni accessibili. I progetti nei paesi in via di sviluppo hanno mostrato i vantaggi:\n\nIl progetto FarmBeats di Microsoft √® un approccio end-to-end per abilitare l‚Äôagricoltura basata sui dati utilizzando sensori a basso costo, droni e algoritmi di visione e apprendimento automatico. Il progetto cerca di risolvere il problema dell‚Äôadozione limitata della tecnologia nell‚Äôagricoltura a causa della necessit√† di maggiore potenza e connettivit√† Internet nelle aziende agricole e della limitata competenza tecnologica degli agricoltori. Il progetto tenta di aumentare la produttivit√† agricola e ridurre i costi accoppiando i dati con la conoscenza e l‚Äôintuizione degli agricoltori sulle loro aziende agricole. Il progetto ha consentito con successo di ottenere informazioni fruibili dai dati tramite la creazione di modelli di intelligenza artificiale (IA) o apprendimento automatico (ML) basati su set di dati fusi. Figura¬†19.2 illustra il funzionamento interno di FarmBeats di Microsoft.\nNell‚ÄôAfrica subsahariana, telecamere standard e IA edge hanno ridotto le perdite per malattie della manioca dal 40% al 5%, proteggendo una coltura di base (Ramcharan et al. 2017).\nIn Indonesia, i sensori monitorano i microclimi nelle risaie, ottimizzando l‚Äôuso dell‚Äôacqua anche in caso di piogge irregolari (Tirtalistyani, Murtiningrum, e Kanwar 2022).\n\n\nRamcharan, Amanda, Kelsee Baranowski, Peter McCloskey, Babuali Ahmed, James Legg, e David P. Hughes. 2017. ¬´Deep Learning for Image-Based Cassava Disease Detection¬ª. Front. Plant Sci. 8 (ottobre): 1852. https://doi.org/10.3389/fpls.2017.01852.\n\nTirtalistyani, Rose, Murtiningrum Murtiningrum, e Rameshwar S. Kanwar. 2022. ¬´Indonesia Rice Irrigation System: Time for Innovation¬ª. Sustainability 14 (19): 12477. https://doi.org/10.3390/su141912477.\nCon maggiori investimenti e integrazione nei servizi di consulenza rurale, TinyML potrebbe trasformare l‚Äôagricoltura su piccola scala e migliorare i mezzi di sostentamento degli agricoltori in tutto il mondo. La tecnologia porta efficacemente i vantaggi dell‚Äôagricoltura di precisione alle regioni disconnesse pi√π bisognose.\n\n\n\n\n\n\nFigura¬†19.2: Microsoft Farmbeats consente di prendere decisioni basate sui dati per migliorare la resa agricola, ridurre i costi complessivi e ridurre l‚Äôimpatto ambientale della produzione agricola. Fonte: MOLD\n\n\n\n\n\n\n\n\n\nEsercizio¬†19.1: Modellazione della Resa delle Colture\n\n\n\n\n\nQuesto esercizio insegna come prevedere le rese delle colture in Nepal combinando dati satellitari (Sentinel-2), dati climatici (WorldClim) e misure sul campo. Si user√† un algoritmo di apprendimento automatico chiamato XGBoost Regressor per creare un modello, dividere i dati per l‚Äôaddestramento e il test e perfezionare i parametri del modello per ottenere le migliori prestazioni. Questo notebook getta le basi per l‚Äôimplementazione di TinyML nel settore agricolo. Considerare come si potrebbe adattare questo processo per set di dati pi√π piccoli, meno funzionalit√† e modelli semplificati per renderlo compatibile con i vincoli di potenza e memoria dei dispositivi TinyML.",
    "crumbs": [
      "<span class='chapter-number'>19</span>¬† <span class='chapter-title'>AI for Good</span>"
    ]
  },
  {
    "objectID": "contents/core/ai_for_good/ai_for_good.it.html#assistenza-sanitaria",
    "href": "contents/core/ai_for_good/ai_for_good.it.html#assistenza-sanitaria",
    "title": "19¬† AI for Good",
    "section": "19.3 Assistenza Sanitaria",
    "text": "19.3 Assistenza Sanitaria\n\n19.3.1 Espansione dell‚ÄôAccesso\nLa copertura sanitaria universale e l‚Äôassistenza di qualit√† restano fuori dalla portata di milioni di persone in tutto il mondo. In molte regioni √® necessario un numero maggiore di professionisti sanitari per accedere a diagnosi e trattamenti di base. Inoltre, √® necessario migliorare le infrastrutture sanitarie come cliniche, ospedali e servizi di pubblica utilit√† per alimentare apparecchiature complesse. Queste lacune hanno un impatto sproporzionato sulle comunit√† emarginate, esacerbando le disparit√† sanitarie.\nTinyML offre una promettente soluzione tecnologica per aiutare ad ampliare l‚Äôaccesso a un‚Äôassistenza sanitaria di qualit√† a livello globale. TinyML si riferisce alla capacit√† di implementare algoritmi di apprendimento automatico su microcontrollori, piccoli chip con potenza di elaborazione, memoria e connettivit√† limitate. TinyML consente l‚Äôanalisi dei dati in tempo reale e l‚Äôintelligenza in dispositivi compatti e a bassa potenza.\nCi√≤ crea opportunit√† per strumenti medici trasformativi che sono portatili, convenienti e accessibili. Il software e l‚Äôhardware TinyML possono essere ottimizzati per funzionare anche in ambienti con risorse limitate. Ad esempio, un sistema TinyML potrebbe analizzare i sintomi o fare previsioni diagnostiche utilizzando una potenza di calcolo minima, nessuna connettivit√† Internet continua e una batteria o una fonte di energia solare. Queste capacit√† possono portare screening e monitoraggio di livello medico direttamente ai pazienti meno assistiti.\n\n\n19.3.2 Diagnosi Precoce\nLa diagnosi precoce delle malattie √® una delle principali applicazioni. Piccoli sensori abbinati al software TinyML possono identificare i sintomi prima che le condizioni peggiorino o compaiano segni visibili. Ad esempio, i monitoratori della tosse con apprendimento automatico embedded possono rilevare pattern acustici indicativi di malattie respiratorie, malaria o tubercolosi. Rilevare le malattie all‚Äôesordio migliora i risultati e riduce i costi sanitari.\nUn esempio dettagliato potrebbe essere il monitoraggio della polmonite nei bambini da parte di TinyML. La polmonite √® una delle principali cause di morte nei bambini sotto i 5 anni e rilevarla precocemente √® fondamentale. Una startup chiamata Respira xColabs ha sviluppato un sensore audio indossabile a basso costo che utilizza algoritmi TinyML per analizzare la tosse e identificare i sintomi di malattie respiratorie come la polmonite. Il dispositivo contiene un microfono e un microcontrollore che esegue un modello di rete neurale addestrato per classificare i suoni respiratori. Pu√≤ identificare feature come respiro sibilante, crepitio e stridore che possono indicare la polmonite. Il dispositivo √® progettato per essere altamente accessibile, dotato di un semplice cinturino, non richiede batteria o ricarica e fornisce risultati tramite luci LED e segnali acustici.\nUn altro esempio riguarda i ricercatori dell‚ÄôUNIFEI in Brasile che hanno sviluppato un dispositivo a basso costo che sfrutta TinyML per monitorare i ritmi cardiaci. La loro soluzione risponde a un‚Äôesigenza critica affrontando il problema della fibrillazione atriale e di altre anomalie del ritmo cardiaco, che spesso non vengono diagnosticate a causa del costo proibitivo e della limitata disponibilit√† di strumenti di screening. Utilizza un microcontrollore standard che costa solo pochi dollari, insieme a un sensore di pulsazioni di base. Riducendo al minimo la complessit√†, il dispositivo diventa accessibile alle popolazioni con risorse insufficienti. L‚Äôalgoritmo TinyML in esecuzione localmente sul microcontrollore analizza i dati delle pulsazioni in tempo reale per rilevare ritmi cardiaci irregolari. Questo dispositivo salvavita per il monitoraggio cardiaco dimostra come TinyML consenta di implementare potenti capacit√† di intelligenza artificiale in progetti convenienti e intuitivi.\nLa versatilit√† di TinyML promette anche di affrontare le malattie infettive. I ricercatori hanno proposto di applicare TinyML per identificare le zanzare che diffondono la malaria tramite i suoni del battito delle ali. Se dotati di microfoni, i piccoli microcontrollori possono eseguire modelli avanzati di classificazione audio per determinare le specie di zanzare. Questa soluzione compatta e a basso consumo produce risultati in tempo reale, adatti per l‚Äôuso in campo remoto. Rendendo l‚Äôanalisi entomologica conveniente e accessibile, TinyML potrebbe rivoluzionare il monitoraggio degli insetti che mettono a rischio la salute umana. TinyML sta ampliando l‚Äôaccesso all‚Äôassistenza sanitaria per le comunit√† vulnerabili, dalle malattie cardiache alla malaria.\n\n\n19.3.3 Controllo delle Malattie Infettive\nLe zanzare rimangono il vettore di malattie pi√π mortale al mondo, trasmettendo malattie che infettano oltre un miliardo di persone ogni anno (¬´Vector-borne diseases¬ª, s.d.). Malattie come la malaria, la dengue e lo Zika sono particolarmente diffuse nelle regioni con risorse limitate e prive di infrastrutture solide per il controllo delle zanzare. Il monitoraggio delle popolazioni locali di zanzare √® essenziale per prevenire le epidemie e indirizzare correttamente gli interventi.\n\n¬´Vector-borne diseases¬ª. s.d. https://www.who.int/news-room/fact-sheets/detail/vector-borne-diseases.\nI metodi di monitoraggio tradizionali sono costosi, richiedono molta manodopera e sono difficili da implementare da remoto. La soluzione TinyML proposta supera queste barriere. Piccoli microfoni abbinati ad algoritmi di apprendimento automatico possono classificare le zanzare per specie in base a piccole differenze nelle oscillazioni delle ali. Il software TinyML funziona in modo efficiente su microcontrollori a basso costo, eliminando la necessit√† di connettivit√† continua.\nUn team di ricerca collaborativo dell‚ÄôUniversit√† di Khartoum e dell‚ÄôICTP sta esplorando una soluzione innovativa utilizzando TinyML. In un recente articolo, hanno presentato un dispositivo a basso costo in grado di identificare le specie di zanzare che diffondono malattie attraverso i suoni del battito delle ali (Altayeb, Zennaro, e Rovai 2022).\n\nAltayeb, Moez, Marco Zennaro, e Marcelo Rovai. 2022. ¬´Classifying mosquito wingbeat sound using TinyML¬ª. In Proceedings of the 2022 ACM Conference on Information Technology for Social Good, 132‚Äì37. ACM. https://doi.org/10.1145/3524458.3547258.\nQuesto sistema portatile e autonomo promette molto bene per l‚Äôentomologia. I ricercatori suggeriscono che potrebbe rivoluzionare le strategie di monitoraggio degli insetti e di controllo dei vettori nelle aree remote. TinyML potrebbe rafforzare significativamente gli sforzi di eradicazione della malaria fornendo analisi delle zanzare pi√π economiche e semplici. La sua versatilit√† e il fabbisogno energetico minimo lo rendono ideale per l‚Äôuso sul campo in regioni isolate e fuori dalla rete con risorse scarse ma un elevato carico di malattie.\n\n\n19.3.4 TinyML Design Contest in Healthcare\nIl primo concorso TinyML in ambito sanitario, TDC‚Äô22 (Jia et al. 2023), si √® tenuto nel 2022 per motivare i team partecipanti a progettare algoritmi IA/ML per rilevare aritmie ventricolari (VA) potenzialmente letali e distribuirli su ‚ÄúImplantable Cardioverter Defibrillators (ICDs)‚Äù. Le VA sono la causa principale di ‚Äúsudden cardiac death (SCD)‚Äù [morte cardiaca improvvisa]. Le persone ad alto rischio di SCD si affidano all‚ÄôICD per erogare un trattamento di defibrillazione adeguato e tempestivo (ad esempio, riportando il cuore al ritmo normale) quando soffrono di VA potenzialmente letali.\n\nJia, Zhenge, Dawei Li, Xiaowei Xu, Na Li, Feng Hong, Lichuan Ping, e Yiyu Shi. 2023. ¬´Life-threatening ventricular arrhythmia detection challenge in implantable cardioverterdefibrillators¬ª. Nature Machine Intelligence 5 (5): 554‚Äì55. https://doi.org/10.1038/s42256-023-00659-9.\nUn algoritmo sul dispositivo per il rilevamento precoce e tempestivo di VA potenzialmente letali aumenter√† le possibilit√† di sopravvivenza. L‚Äôalgoritmo AI/ML proposto doveva essere implementato ed eseguito su un microcontrollore a bassissimo consumo energetico e con risorse limitate (una scheda di sviluppo da 10 dollari con un core ARM Cortex-M4 a 80 MHz, 256 kB di memoria flash e 64 kB di SRAM). I progetti presentati sono stati valutati tramite metriche misurate sul microcontrollore per (1) prestazioni di rilevamento, (2) latenza di inferenza e (3) occupazione di memoria da parte del programma di algoritmi AI/ML.\nIl campione, GaTech EIC Lab, ha ottenuto 0,972 in \\(F_\\beta\\) (punteggio F1 con un peso maggiore da richiamare), 1,747 ms di latenza e 26,39 kB di footprint di memoria con una rete neurale profonda. Un ICD con un algoritmo di rilevamento VA sul dispositivo √® stato impiantato in uno studio clinico.\n\n\n\n\n\n\nEsercizio¬†19.2: Dati Clinici: Sbloccare Informazioni con il Riconoscimento di Entit√† Denominate\n\n\n\n\n\nIn questo esercizio, si imparer√® il‚ÄùNamed Entity Recognition (NER)‚Äù [riconoscimento di entit√† denominate ], un potente strumento per estrarre informazioni preziose dal testo clinico. Utilizzando Spark NLP, una libreria specializzata per NLP sanitaria, esploreremo come i modelli NER come BiLSTM-CNN-Char e BERT possono identificare automaticamente importanti entit√† mediche come diagnosi, farmaci, risultati di test e altro ancora. Si acquisir√† esperienza pratica applicando queste tecniche con un‚Äôattenzione particolare all‚Äôestrazione di dati correlati all‚Äôoncologia, aiutandoti a sbloccare informazioni sui tipi di cancro e sui dettagli del trattamento dalle cartelle cliniche dei pazienti.",
    "crumbs": [
      "<span class='chapter-number'>19</span>¬† <span class='chapter-title'>AI for Good</span>"
    ]
  },
  {
    "objectID": "contents/core/ai_for_good/ai_for_good.it.html#scienza",
    "href": "contents/core/ai_for_good/ai_for_good.it.html#scienza",
    "title": "19¬† AI for Good",
    "section": "19.4 Scienza",
    "text": "19.4 Scienza\nIn molti campi scientifici, i ricercatori sono limitati dalla qualit√† e dalla risoluzione dei dati che possono raccogliere. Spesso devono dedurre indirettamente i veri parametri di interesse utilizzando correlazioni approssimative e modelli basati su punti dati sparsi. Ci√≤ limita l‚Äôaccuratezza della comprensione scientifica e delle previsioni.\nL‚Äôemergere di TinyML apre nuove possibilit√† per la raccolta di misurazioni scientifiche ad alta fedelt√†. Con l‚Äôapprendimento automatico embedded, piccoli sensori a basso costo possono elaborare e analizzare automaticamente i dati localmente in tempo reale. Ci√≤ crea reti di sensori intelligenti che catturano dati sfumati su scale e frequenze molto pi√π grandi.\nAd esempio, il monitoraggio delle condizioni ambientali per modellare il cambiamento climatico rimane una sfida a causa della necessit√† di dati diffusi e continui. Il progetto Ribbit dell‚ÄôUC Berkeley √® pioniere di una soluzione TinyML crowdsourcing (Rao 2021). Hanno sviluppato un sensore CO2 open source che utilizza un microcontrollore integrato per elaborare le misurazioni del gas. Un set di dati esteso pu√≤ essere aggregato distribuendo centinaia di questi sensori a basso costo. I dispositivi TinyML compensano i fattori ambientali e forniscono letture granulari, accurate e in precedenza impossibili.\n\nRao, Ravi. 2021. ¬´TinyML unlocks new possibilities for sustainable development technologies¬ª. www.wevolver.com. https://www.wevolver.com/article/tinyml-unlocks-new-possibilities-for-sustainable-development-technologies.\nIl potenziale di scalare massicciamente il rilevamento intelligente tramite TinyML ha profonde implicazioni scientifiche. I dati ad alta risoluzione possono portare a scoperte e capacit√† predittive in campi che vanno dall‚Äôecologia alla cosmologia. Altre applicazioni potrebbero includere sensori sismici per sistemi di allerta precoce sui terremoti, monitor meteorologici distribuiti per tracciare i cambiamenti del microclima e sensori acustici per studiare le popolazioni animali.\nMan mano che i sensori e gli algoritmi continuano a migliorare, le reti TinyML potrebbero generare mappe pi√π dettagliate che mai dei sistemi naturali. Democratizzare la raccolta di dati scientifici pu√≤ accelerare la ricerca e la comprensione tra le discipline. Tuttavia, solleva nuove sfide in merito alla qualit√† dei dati, alla privacy e alla modellazione di incognite. TinyML indica una crescente convergenza tra intelligenza artificiale e scienze naturali per rispondere a domande fondamentali.",
    "crumbs": [
      "<span class='chapter-number'>19</span>¬† <span class='chapter-title'>AI for Good</span>"
    ]
  },
  {
    "objectID": "contents/core/ai_for_good/ai_for_good.it.html#conservazione-e-ambiente",
    "href": "contents/core/ai_for_good/ai_for_good.it.html#conservazione-e-ambiente",
    "title": "19¬† AI for Good",
    "section": "19.5 Conservazione e Ambiente",
    "text": "19.5 Conservazione e Ambiente\nTinyML sta emergendo come un potente strumento per la conservazione ambientale e gli sforzi di sostenibilit√†. Una ricerca recente ha evidenziato numerose applicazioni del tiny machine learning in ambiti quali il monitoraggio della fauna selvatica, la gestione delle risorse naturali e il monitoraggio dei cambiamenti climatici.\nUn esempio √® l‚Äôutilizzo di TinyML per il monitoraggio e la protezione della fauna selvatica in tempo reale. I ricercatori hanno sviluppato dispositivi Smart Wildlife Tracker che sfruttano gli algoritmi TinyML per rilevare le attivit√† di bracconaggio. I collari contengono sensori come telecamere, microfoni e GPS per monitorare costantemente l‚Äôambiente circostante. I modelli di machine learning embedded analizzano i dati audio e visivi per identificare minacce come esseri umani nelle vicinanze o spari. Il rilevamento precoce del bracconaggio fornisce alle guardie forestali informazioni fondamentali per intervenire e agire.\nAltri progetti applicano TinyML per studiare il comportamento degli animali tramite sensori. Il collare intelligente per la fauna selvatica utilizza accelerometri e monitoraggio acustico per tracciare i movimenti, la comunicazione e gli stati d‚Äôanimo degli elefanti (Verma 2022). I dispositivi collare TinyML a basso consumo energetico trasmettono dati approfonditi sulle attivit√† degli elefanti evitando gravosi cambi di batteria. Ci√≤ aiuta i ricercatori a osservare in modo discreto le popolazioni di elefanti per informare sulle strategie di conservazione.\n\nVerma, Team Dual_Boot: Swapnil. 2022. ¬´Elephant AI¬ª. Hackster.io. https://www.hackster.io/dual\\_boot/elephant-ai-ba71e9.\nSu scala pi√π ampia, i dispositivi TinyML distribuiti sono concepiti per creare reti di sensori dense per la modellazione ambientale. Centinaia di monitor della qualit√† dell‚Äôaria a basso costo potrebbero mappare l‚Äôinquinamento nelle citt√†. I sensori sottomarini potrebbero rilevare le tossine e dare un avviso precoce di fioriture algali. Tali applicazioni sottolineano la versatilit√† di TinyML in ecologia, climatologia e sostenibilit√†.\nI ricercatori della Moulay Ismail University di Meknes in Marocco (Bamoumen et al. 2022) hanno pubblicato un sondaggio su come TinyML pu√≤ essere utilizzato per risolvere i problemi ambientali. Tuttavia, valutare attentamente i benefici, i rischi e l‚Äôaccesso equo sar√† fondamentale man mano che TinyML espande la ricerca e la conservazione ambientale. Con una considerazione etica degli impatti, TinyML offre soluzioni basate sui dati per proteggere la biodiversit√†, le risorse naturali e il nostro pianeta.\n\nBamoumen, Hatim, Anas Temouden, Nabil Benamar, e Yousra Chtouki. 2022. ¬´How TinyML Can be Leveraged to Solve Environmental Problems: A Survey¬ª. In 2022 International Conference on Innovation and Intelligence for Informatics, Computing, and Technologies (3ICT), 338‚Äì43. IEEE; IEEE. https://doi.org/10.1109/3ict56508.2022.9990661.",
    "crumbs": [
      "<span class='chapter-number'>19</span>¬† <span class='chapter-title'>AI for Good</span>"
    ]
  },
  {
    "objectID": "contents/core/ai_for_good/ai_for_good.it.html#risposta-ai-disastri",
    "href": "contents/core/ai_for_good/ai_for_good.it.html#risposta-ai-disastri",
    "title": "19¬† AI for Good",
    "section": "19.6 Risposta ai Disastri",
    "text": "19.6 Risposta ai Disastri\nNella risposta ai disastri, rapidit√† e sicurezza sono fondamentali, ma le macerie e i rottami creano ambienti pericolosi e angusti che ostacolano le attivit√† di ricerca umana. TinyML consente ai droni agili di assistere le squadre di soccorso in questi scenari pericolosi. L‚Äôelaborazione locale dei dati tramite TinyML consente una rapida interpretazione per guidare i soccorsi.\nQuando gli edifici crollano dopo i terremoti, i piccoli droni possono rivelarsi preziosi. Dotati di algoritmi di navigazione TinyML, i droni di piccole dimensioni come il CrazyFlie con meno di 200 KB di RAM e una frequenza di clock della CPU di soli 168 MHz possono attraversare in sicurezza spazi angusti e mappare percorsi oltre la portata umana (Bardienus P. Duisterhof et al. 2019). L‚Äôelusione degli ostacoli consente a questi droni di muoversi tra detriti instabili. Questa mobilit√† autonoma consente loro di spazzare rapidamente aree in cui gli umani non possono accedere. Fondamentalmente, i sensori di bordo e i processori TinyML analizzano i dati in tempo reale per identificare i segni dei sopravvissuti. Le telecamere termiche possono rilevare il calore corporeo, i microfoni possono captare le richieste di aiuto e i sensori di gas possono avvisare di perdite (Bardienus P. Duisterhof et al. 2021).\n\nDuisterhof, Bardienus P, Srivatsan Krishnan, Jonathan J Cruz, Colby R Banbury, William Fu, Aleksandra Faust, Guido CHE de Croon, e Vijay Janapa Reddi. 2019. ¬´Learning to seek: Autonomous source seeking with deep reinforcement learning onboard a nano drone microcontroller¬ª. ArXiv preprint abs/1909.11236. https://arxiv.org/abs/1909.11236.\nVideo¬†19.1 mostra come l‚Äôapprendimento di rinforzo profondo pu√≤ essere utilizzato per consentire ai droni di cercare autonomamente fonti di luce.\n\n\n\n\n\n\nVideo¬†19.1: Imparare a Cercare\n\n\n\n\n\n\nVideo¬†19.2 √® una panoramica dei droni autonomi per il rilevamento delle perdite di gas.\n\n\n\n\n\n\nVideo¬†19.2\n\n\n\n\n\n\nInoltre, gli sciami coordinati di droni sbloccano nuove capacit√†. Collaborando e condividendo informazioni, i team di droni hanno una visione completa della situazione. La copertura dei siti del disastro consente agli algoritmi TinyML di fondere e analizzare i dati da pi√π punti di vista, amplificando la consapevolezza della situazione oltre i singoli droni (Bardienus P. Duisterhof et al. 2021).\n\nDuisterhof, Bardienus P., Shushuai Li, Javier Burgues, Vijay Janapa Reddi, e Guido C. H. E. de Croon. 2021. ¬´Sniffy Bug: A Fully Autonomous Swarm of Gas-Seeking Nano Quadcopters in Cluttered Environments¬ª. In 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 9099‚Äì9106. IEEE; IEEE. https://doi.org/10.1109/iros51168.2021.9636217.\nAncora pi√π importante, la ricognizione iniziale dei droni aumenta la sicurezza per i soccorritori umani. Mantenere le squadre di soccorso a una distanza di sicurezza finch√© i rilievi dei droni non valutano i pericoli salva vite umane. Una volta protetti, i droni possono guidare il posizionamento preciso del personale.\nCombinando mobilit√† agile, dati in tempo reale e coordinamento dello sciame, i droni abilitati TinyML promettono di trasformare la risposta ai disastri. La loro versatilit√†, velocit√† e sicurezza li rendono una risorsa vitale per gli sforzi di soccorso in ambienti pericolosi e inaccessibili. L‚Äôintegrazione di droni autonomi con metodi tradizionali pu√≤ accelerare le risposte quando √® pi√π importante.",
    "crumbs": [
      "<span class='chapter-number'>19</span>¬† <span class='chapter-title'>AI for Good</span>"
    ]
  },
  {
    "objectID": "contents/core/ai_for_good/ai_for_good.it.html#istruzione-e-sensibilizzazione",
    "href": "contents/core/ai_for_good/ai_for_good.it.html#istruzione-e-sensibilizzazione",
    "title": "19¬† AI for Good",
    "section": "19.7 Istruzione e Sensibilizzazione",
    "text": "19.7 Istruzione e Sensibilizzazione\nTinyML ha un immenso potenziale per aiutare ad affrontare le sfide nelle regioni in via di sviluppo, ma per realizzare i suoi benefici √® necessaria un‚Äôistruzione mirata e un rafforzamento delle capacit√†. Riconoscendo questa necessit√†, i ricercatori accademici hanno guidato iniziative di sensibilizzazione per diffondere l‚Äôistruzione TinyML a livello globale.\nNel 2020, l‚ÄôUniversit√† di Harvard, la Columbia University, l‚ÄôInternational Centre for Theoretical Physics (ICTP) e l‚ÄôUNIFEI hanno fondato congiuntamente la rete ‚ÄúTinyML for Developing Communities (TinyML4D)‚Äù (Zennaro, Plancher, e Reddi 2022). Questa rete consente alle universit√† e ai ricercatori nei paesi in via di sviluppo di sfruttare TinyML per un impatto locale.\n\nZennaro, Marco, Brian Plancher, e V Janapa Reddi. 2022. ¬´TinyML: Applied AI for development¬ª. In The UN 7th Multi-stakeholder Forum on Science, Technology and Innovation for the Sustainable Development Goals, 2022‚Äì05.\nUn obiettivo fondamentale √® l‚Äôespansione dell‚Äôaccesso all‚Äôistruzione applicata all‚Äôapprendimento automatico. La rete TinyML4D fornisce formazione, programmi di studio e risorse di laboratorio ai membri. Workshop pratici e progetti di raccolta dati offrono agli studenti esperienza pratica. I membri possono condividere le best practice e creare una comunit√† attraverso conferenze e collaborazioni accademiche.\nLa rete d√† priorit√† all‚Äôabilitazione di soluzioni TinyML localmente rilevanti. I progetti affrontano sfide come agricoltura, salute e monitoraggio ambientale in base alle esigenze della comunit√†. Ad esempio, un‚Äôuniversit√† membro in Ruanda ha sviluppato un sistema di monitoraggio delle inondazioni a basso costo utilizzando TinyML e sensori.\nTinyML4D include oltre 50 istituzioni membri in Africa, Asia e America Latina. Tuttavia, sono necessari maggiori investimenti e partnership industriali per raggiungere tutte le regioni sottoservite. La visione finale √® quella di formare le nuove generazioni ad applicare eticamente TinyML per uno sviluppo sostenibile. Gli sforzi di sensibilizzazione odierni gettano le basi per democratizzare la tecnologia trasformativa per il futuro.",
    "crumbs": [
      "<span class='chapter-number'>19</span>¬† <span class='chapter-title'>AI for Good</span>"
    ]
  },
  {
    "objectID": "contents/core/ai_for_good/ai_for_good.it.html#accessibilit√†",
    "href": "contents/core/ai_for_good/ai_for_good.it.html#accessibilit√†",
    "title": "19¬† AI for Good",
    "section": "19.8 Accessibilit√†",
    "text": "19.8 Accessibilit√†\nLa tecnologia ha un potenziale immenso per abbattere le barriere affrontate dalle persone con disabilit√† e colmare le lacune nell‚Äôaccessibilit√†. TinyML apre specificamente nuove possibilit√† per lo sviluppo di dispositivi di assistenza intelligenti e personalizzati.\nCon algoritmi di apprendimento automatico eseguiti localmente su microcontrollori, gli strumenti di accessibilit√† compatti possono funzionare in tempo reale senza dipendere dalla connettivit√†. Il National Institute on Deafness and Other Communication Disorders (NIDCD) afferma che il 20% della popolazione mondiale ha una qualche forma di perdita dell‚Äôudito. Gli apparecchi acustici che sfruttano TinyML potrebbero riconoscere pi√π parlanti e amplificare la voce di un target scelto in stanze affollate. Ci√≤ consente alle persone con problemi di udito di concentrarsi su conversazioni specifiche.\nAnalogamente, i dispositivi di mobilit√† potrebbero utilizzare l‚Äôelaborazione della vista sul dispositivo per identificare ostacoli e caratteristiche del terreno. Ci√≤ consente una navigazione e una sicurezza migliorate per gli ipovedenti. Aziende come Envision stanno sviluppando occhiali intelligenti, convertendo le informazioni visive in parlato, con TinyML embedded per guidare le persone non vedenti rilevando oggetti, testo e segnali stradali. Video¬†19.3 di seguito mostra i diversi casi di utilizzo nella vita reale degli occhiali per l‚Äôausilio visivo Envision.\n\n\n\n\n\n\nVideo¬†19.3\n\n\n\n\n\n\nTinyML potrebbe persino alimentare arti protesici reattivi. Analizzando i segnali nervosi e i dati sensoriali come la tensione muscolare, protesi ed esoscheletri con ML embedded possono muoversi e regolare la presa in modo dinamico, rendendo il controllo pi√π naturale e intuitivo. Le aziende stanno creando mani bioniche economiche e per uso quotidiano utilizzando TinyML. Per coloro che hanno difficolt√† di linguaggio, i dispositivi abilitati alla voce con TinyML possono generare output vocali personalizzati da input non verbali. Pairs di Anthropic traduce i gesti in un linguaggio naturale su misura per i singoli utenti.\nAbilitando una tecnologia assistiva pi√π personalizzabile, TinyML rende i servizi pi√π accessibili e su misura per le esigenze individuali. E attraverso applicazioni di traduzione e interpretazione, TinyML pu√≤ abbattere le barriere comunicative. App come Microsoft Translator offrono traduzioni in tempo reale basate sugli algoritmi TinyML.\nCon il suo design ponderato e inclusivo, TinyML promette pi√π autonomia e dignit√† per le persone con disabilit√†. Tuttavia, gli sviluppatori dovrebbero coinvolgere direttamente le comunit√†, evitare di compromettere la privacy e considerare l‚Äôaccessibilit√† economica per massimizzare i benefici. TinyML ha un enorme potenziale per contribuire a un mondo pi√π giusto ed equo.",
    "crumbs": [
      "<span class='chapter-number'>19</span>¬† <span class='chapter-title'>AI for Good</span>"
    ]
  },
  {
    "objectID": "contents/core/ai_for_good/ai_for_good.it.html#infrastruttura-e-pianificazione-urbana",
    "href": "contents/core/ai_for_good/ai_for_good.it.html#infrastruttura-e-pianificazione-urbana",
    "title": "19¬† AI for Good",
    "section": "19.9 Infrastruttura e Pianificazione Urbana",
    "text": "19.9 Infrastruttura e Pianificazione Urbana\nCon l‚Äôaumento della popolazione urbana, le citt√† affrontano sfide immense nella gestione efficiente di risorse e infrastrutture. TinyML presenta un potente strumento per lo sviluppo di sistemi intelligenti per ottimizzare le operazioni e la sostenibilit√† della citt√†. Potrebbe rivoluzionare l‚Äôefficienza energetica negli edifici intelligenti.\nI modelli di apprendimento automatico possono imparare a prevedere e regolare l‚Äôuso di energia in base ai pattern di occupazione. I sensori miniaturizzati posizionati negli edifici possono fornire dati granulari e in tempo reale sull‚Äôutilizzo dello spazio, sulla temperatura e altro ancora (Seyedzadeh et al. 2018). Questa visibilit√† consente ai sistemi TinyML di ridurre al minimo gli sprechi ottimizzando riscaldamento, raffreddamento, illuminazione, ecc.\n\nSeyedzadeh, Saleh, Farzad Pour Rahimian, Ivan Glesk, e Marc Roper. 2018. ¬´Machine learning for estimation of building energy consumption and performance: A review¬ª. Visualization in Engineering 6 (1): 1‚Äì20. https://doi.org/10.1186/s40327-018-0064-7.\nQuesti esempi dimostrano l‚Äôenorme potenziale di TinyML per infrastrutture cittadine efficienti e sostenibili. Tuttavia, gli urbanisti devono considerare privacy, sicurezza e accessibilit√† per garantire un‚Äôadozione responsabile. Con un‚Äôimplementazione attenta, TinyML potrebbe modernizzare profondamente la vita urbana.",
    "crumbs": [
      "<span class='chapter-number'>19</span>¬† <span class='chapter-title'>AI for Good</span>"
    ]
  },
  {
    "objectID": "contents/core/ai_for_good/ai_for_good.it.html#sfide-e-considerazioni",
    "href": "contents/core/ai_for_good/ai_for_good.it.html#sfide-e-considerazioni",
    "title": "19¬† AI for Good",
    "section": "19.10 Sfide e Considerazioni",
    "text": "19.10 Sfide e Considerazioni\nSebbene TinyML offra immense opportunit√†, sar√† fondamentale considerare attentamente le sfide e le implicazioni etiche man mano che l‚Äôadozione si diffonde a livello globale. I ricercatori hanno evidenziato i fattori chiave da affrontare, soprattutto quando si distribuisce TinyML nelle regioni in via di sviluppo.\nUna delle sfide pi√π importanti √® l‚Äôaccesso limitato alla formazione e all‚Äôhardware (Ooko et al. 2021). Esistono solo programmi educativi su misura per TinyML e le economie emergenti hanno spesso bisogno di una solida catena di fornitura di elettronica. Saranno necessarie una formazione approfondita e partnership per coltivare le competenze e rendere i dispositivi disponibili alle comunit√† svantaggiate. Iniziative come la rete TinyML4D aiutano a fornire percorsi di apprendimento strutturati.\n\nOoko, Samson Otieno, Marvin Muyonga Ogore, Jimmy Nsenga, e Marco Zennaro. 2021. ¬´TinyML in Africa: Opportunities and Challenges¬ª. In 2021 IEEE Globecom Workshops (GC Wkshps), 1‚Äì6. IEEE; IEEE. https://doi.org/10.1109/gcwkshps52748.2021.9682107.\nAnche le limitazioni dei dati pongono ostacoli. I modelli TinyML richiedono set di dati localizzati di qualit√†, che sono scarsi in ambienti con risorse insufficienti. La creazione di framework per il crowdsourcing dei dati in modo etico potrebbe risolvere questo problema. Tuttavia, la raccolta dei dati dovrebbe avvantaggiare direttamente le comunit√† locali, non solo estrarre valore.\nL‚Äôottimizzazione dell‚Äôuso dell‚Äôenergia e della connettivit√† sar√† fondamentale per la sostenibilit√†. Le basse esigenze di potenza di TinyML lo rendono ideale per casi d‚Äôuso fuori dalla rete. L‚Äôintegrazione di batterie o energia solare pu√≤ consentire un funzionamento continuo. Adattare i dispositivi per la trasmissione a bassa larghezza di banda in cui Internet √® limitato massimizza anche l‚Äôimpatto.\nLe barriere culturali e linguistiche complicano ulteriormente l‚Äôadozione. Le interfacce utente e i dispositivi dovrebbero tenere conto di tutti i livelli di alfabetizzazione ed evitare di escludere sottogruppi. Le soluzioni controllabili vocalmente nei dialetti locali possono migliorare l‚Äôaccessibilit√†.\nAffrontare queste sfide richiede partnership olistiche, finanziamenti e supporto politico. Tuttavia, l‚Äôinclusione e l‚Äôetica di TinyML hanno un potenziale monumentale per elevare le popolazioni svantaggiate in tutto il mondo. Con un‚Äôimplementazione ponderata, la tecnologia potrebbe democratizzare profondamente le opportunit√†.",
    "crumbs": [
      "<span class='chapter-number'>19</span>¬† <span class='chapter-title'>AI for Good</span>"
    ]
  },
  {
    "objectID": "contents/core/ai_for_good/ai_for_good.it.html#conclusione",
    "href": "contents/core/ai_for_good/ai_for_good.it.html#conclusione",
    "title": "19¬† AI for Good",
    "section": "19.11 Conclusione",
    "text": "19.11 Conclusione\nTinyML offre un‚Äôenorme opportunit√† di sfruttare la potenza dell‚Äôintelligenza artificiale per promuovere gli Obiettivi di sviluppo sostenibile delle Nazioni Unite e guidare l‚Äôimpatto sociale a livello globale, come evidenziato da esempi in settori quali sanit√†, agricoltura, conservazione e altro; l‚Äôapprendimento automatico embedded sblocca nuove capacit√† per soluzioni accessibili e a basso costo, su misura per i contesti locali. TinyML aggira barriere come infrastrutture scadenti, connettivit√† limitata e costi elevati che spesso escludono le comunit√† in via di sviluppo dalle tecnologie emergenti.\nTuttavia, realizzare il pieno potenziale di TinyML richiede una collaborazione olistica. Ricercatori, politici, aziende e stakeholder locali devono collaborare per fornire formazione, stabilire quadri etici, progettare soluzioni in comune e adattarle alle esigenze della comunit√†. Attraverso uno sviluppo e un‚Äôimplementazione inclusivi, TinyML pu√≤ mantenere la promessa di colmare le disuguaglianze e migliorare le popolazioni vulnerabili senza lasciare indietro nessuno.\nSe coltivato in modo responsabile, TinyML potrebbe democratizzare le opportunit√† e accelerare i progressi sulle priorit√† globali, dall‚Äôalleviamento della povert√† alla resilienza climatica. La tecnologia rappresenta una nuova ondata di IA applicata per potenziare le societ√†, promuovere la sostenibilit√† e spingere l‚Äôumanit√† verso una maggiore giustizia, prosperit√† e pace. TinyML offre uno sguardo a un futuro abilitato dall‚ÄôIA accessibile a tutti.",
    "crumbs": [
      "<span class='chapter-number'>19</span>¬† <span class='chapter-title'>AI for Good</span>"
    ]
  },
  {
    "objectID": "contents/core/ai_for_good/ai_for_good.it.html#sec-ai-for-good-resource",
    "href": "contents/core/ai_for_good/ai_for_good.it.html#sec-ai-for-good-resource",
    "title": "19¬† AI for Good",
    "section": "19.12 Risorse",
    "text": "19.12 Risorse\nEcco un elenco curato di risorse per supportare studenti e insegnanti nei loro percorsi di apprendimento e insegnamento. Stiamo lavorando costantemente per espandere questa raccolta e presto aggiungeremo nuovi esercizi.\n\n\n\n\n\n\nSlide\n\n\n\n\n\nQueste slide sono uno strumento prezioso per gli insegnanti per tenere lezioni e per gli studenti per rivedere il materiale secondo il proprio ritmo. Incoraggiamo studenti e docenti a sfruttare queste slide per migliorare la loro comprensione e facilitare un trasferimento efficace delle conoscenze.\n\nTinyML for Social Impact.\n\n\n\n\n\n\n\n\n\n\nVideo\n\n\n\n\n\n\nVideo¬†19.1\nVideo¬†19.2\nVideo¬†19.3\n\n\n\n\n\n\n\n\n\n\nEsercizi\n\n\n\n\n\n\nEsercizio¬†19.1\nEsercizio¬†19.2",
    "crumbs": [
      "<span class='chapter-number'>19</span>¬† <span class='chapter-title'>AI for Good</span>"
    ]
  },
  {
    "objectID": "contents/core/conclusion/conclusion.it.html",
    "href": "contents/core/conclusion/conclusion.it.html",
    "title": "20¬† Conclusione",
    "section": "",
    "text": "20.1 Panoramica\nQuesto libro esamina il campo in rapida evoluzione dei sistemi ML (Capitolo 2). Ci siamo concentrati sui sistemi perch√©, nonostante esistano numerose risorse sui modelli e sugli algoritmi di ML, c‚Äô√® ancora molto da capire su come costruire i sistemi che li eseguono.\nPer fare un‚Äôanalogia, consideriamo il processo di costruzione di un‚Äôauto. Sebbene siano disponibili molte risorse sui vari componenti di un‚Äôauto, come motore, trasmissione e sospensioni, spesso √® necessario comprendere meglio come assemblare questi componenti in un veicolo funzionale. Proprio come un‚Äôauto richiede un sistema ben progettato e correttamente integrato per funzionare in modo efficiente e affidabile, anche i modelli ML richiedono un sistema robusto e costruito con cura per offrire il loro pieno potenziale. Inoltre, vi sono molte sfumature nella costruzione di sistemi ML, dato il loro caso d‚Äôuso specifico. Ad esempio, un‚Äôauto da corsa di Formula 1 deve essere assemblata in modo diverso da una normale auto Prius di tutti i giorni.\nIl nostro viaggio √® iniziato tracciando la traiettoria storica del ML, dalle sue fondamenta teoriche al suo stato attuale come forza trasformativa in tutti i settori (Capitolo 3). Questo viaggio ha evidenziato i notevoli progressi nel campo, le sfide e le opportunit√†.\nIn questo libro, abbiamo esaminato le complessit√† dei sistemi ML, esaminando i componenti critici e le best practice necessarie per creare una pipeline fluida ed efficiente. Dalla preelaborazione dei dati e dalla formazione del modello alla distribuzione e al monitoraggio, abbiamo fornito approfondimenti e indicazioni per aiutare i lettori a orientarsi nel complesso panorama dello sviluppo del sistema ML.\nI sistemi ML coinvolgono flussi di lavoro complessi, che abbracciano vari argomenti, dall‚Äôingegneria dei dati alla distribuzione del modello su sistemi diversi (Capitolo 4). Fornendo una panoramica di questi componenti del sistema ML, abbiamo mirato a mostrare l‚Äôenorme profondit√† e ampiezza del campo e le competenze necessarie. Comprendere le complessit√† dei flussi di lavoro di apprendimento automatico √® fondamentale sia per i professionisti sia per i ricercatori, poich√© consente loro di orientarsi in modo efficace nel panorama e di sviluppare soluzioni di apprendimento automatico solide, efficienti e di impatto.\nConcentrandoci sull‚Äôaspetto sistemico del ML, puntiamo a colmare il divario tra conoscenza teorica e implementazione pratica. Proprio come un sistema corporeo umano sano consente agli organi di funzionare in modo ottimale, un sistema ML ben progettato consente ai modelli di fornire costantemente risultati accurati e affidabili. L‚Äôobiettivo di questo libro √® quello di fornire ai lettori le conoscenze e gli strumenti necessari per creare sistemi ML che mostrino la potenza dei modelli sottostanti e garantiscano un‚Äôintegrazione e un funzionamento fluidi, proprio come un corpo umano ben funzionante.",
    "crumbs": [
      "<span class='chapter-number'>20</span>¬† <span class='chapter-title'>Conclusione</span>"
    ]
  },
  {
    "objectID": "contents/core/conclusion/conclusion.it.html#conoscere-limportanza-dei-dataset-ml",
    "href": "contents/core/conclusion/conclusion.it.html#conoscere-limportanza-dei-dataset-ml",
    "title": "20¬† Conclusione",
    "section": "20.2 Conoscere l‚ÄôImportanza dei Dataset ML",
    "text": "20.2 Conoscere l‚ÄôImportanza dei Dataset ML\nUno degli aspetti chiave che abbiamo sottolineato √® che i dati sono la base su cui sono costruiti i sistemi ML (Capitolo 5). I dati sono il nuovo codice che programma reti neurali profonde, rendendo l‚Äôingegneria dei dati la prima e pi√π critica fase di qualsiasi pipeline ML. Ecco perch√© abbiamo iniziato la nostra esplorazione immergendoci nelle basi dell‚Äôingegneria dei dati, riconoscendo che qualit√†, diversit√† e approvvigionamento etico sono fondamentali per creare modelli di apprendimento automatico solidi e affidabili.\nL‚Äôimportanza di dati di alta qualit√† deve essere bilanciata. Le carenze nella qualit√† dei dati possono portare a conseguenze negative significative, come previsioni errate, cessazioni di progetti e persino potenziali danni alle comunit√†. Questi effetti a cascata, spesso chiamati ‚ÄúData Cascades‚Äù [cascate di dati], evidenziano la necessit√† di pratiche diligenti di gestione e governance dei dati. I professionisti del ML devono dare priorit√† alla qualit√† dei dati, garantire diversit√† e rappresentativit√† e aderire a standard etici di raccolta e utilizzo dei dati. In questo modo, possiamo mitigare i rischi associati alla scarsa qualit√† dei dati e creare sistemi ML affidabili, sicuri e vantaggiosi per la societ√†.",
    "crumbs": [
      "<span class='chapter-number'>20</span>¬† <span class='chapter-title'>Conclusione</span>"
    ]
  },
  {
    "objectID": "contents/core/conclusion/conclusion.it.html#esplorare-il-panorama-dei-framework-di-ia",
    "href": "contents/core/conclusion/conclusion.it.html#esplorare-il-panorama-dei-framework-di-ia",
    "title": "20¬† Conclusione",
    "section": "20.3 Esplorare il Panorama dei Framework di IA",
    "text": "20.3 Esplorare il Panorama dei Framework di IA\nEsistono molti framework ML diversi. Pertanto, ci siamo immersi nell‚Äôevoluzione di diversi framework ML, analizzando il funzionamento interno di quelli pi√π popolari come TensorFlow e PyTorch e fornendo approfondimenti sui componenti principali e sulle funzionalit√† avanzate che li definiscono (Capitolo 6). Abbiamo anche esaminato la specializzazione di framework su misura per esigenze specifiche, come quelli progettati per l‚ÄôIA embedded. Abbiamo discusso i criteri per la selezione del framework pi√π adatto per un determinato progetto.\nLa nostra esplorazione ha anche toccato le tendenze future che dovrebbero plasmare il panorama dei framework ML nei prossimi anni. Man mano che il campo continua a evolversi, possiamo prevedere l‚Äôemergere di framework pi√π specializzati e ottimizzati che soddisfano i requisiti unici di diversi domini e scenari di distribuzione, come abbiamo visto con TensorFlow Lite per microcontrollori. Restando al passo con questi sviluppi e comprendendo i compromessi coinvolti nella selezione del framework, possiamo prendere decisioni informate e sfruttare gli strumenti pi√π appropriati per creare sistemi ML efficienti.\nInoltre, ci aspettiamo di vedere una crescente enfasi sull‚Äôinteroperabilit√† dei framework e sugli sforzi di standardizzazione, come il formato ONNX (Open Neural Network Exchange). Questo formato consente di addestrare i modelli in un framework e distribuirli in un altro, facilitando una maggiore collaborazione e portabilit√† su diverse piattaforme e ambienti.",
    "crumbs": [
      "<span class='chapter-number'>20</span>¬† <span class='chapter-title'>Conclusione</span>"
    ]
  },
  {
    "objectID": "contents/core/conclusion/conclusion.it.html#comprendere-i-fondamenti-del-training-ml",
    "href": "contents/core/conclusion/conclusion.it.html#comprendere-i-fondamenti-del-training-ml",
    "title": "20¬† Conclusione",
    "section": "20.4 Comprendere i Fondamenti del Training ML",
    "text": "20.4 Comprendere i Fondamenti del Training ML\nCome professionisti ML che creano sistemi ML, √® fondamentale comprendere a fondo il processo di addestramento dell‚ÄôIA e le sfide del sistema nel ridimensionarlo e ottimizzarlo. Sfruttando le capacit√† dei moderni framework di IA e restando aggiornati con gli ultimi progressi nelle tecniche di training, possiamo creare sistemi ML robusti, efficienti e scalabili in grado di affrontare problemi del mondo reale e guidare l‚Äôinnovazione in vari domini.\nAbbiamo iniziato esaminando i fondamenti della formazione AI (Capitolo 7), che comporta l‚Äôinserimento di dati nei modelli ML e la regolazione dei loro parametri per ridurre al minimo la differenza tra output previsti ed effettivi. Questo processo √® computazionalmente intensivo e richiede un‚Äôattenta considerazione di vari fattori, come la scelta di algoritmi di ottimizzazione, velocit√† di apprendimento, dimensioni del batch e tecniche di regolarizzazione. Comprendere questi concetti √® fondamentale per sviluppare pipeline di training efficaci ed efficienti.\nTuttavia, il training di modelli ML su larga scala pone sfide di sistema significative. Man mano che le dimensioni dei set di dati e la complessit√† dei modelli aumentano, le risorse computazionali richieste per la formazione possono diventare proibitive. Ci√≤ ha portato allo sviluppo di tecniche di training distribuite, come il parallelismo di dati e di modelli, che consentono a pi√π dispositivi di collaborare nel processo di training. Framework come TensorFlow e PyTorch si sono evoluti per supportare questi paradigmi di training distribuiti, consentendo ai professionisti di scalare i carichi di lavoro di training su cluster di GPU o TPU.\nOltre al training distribuito, abbiamo discusso tecniche per ottimizzare il processo di training, come il training a precisione mista e la compressione del gradiente. √à importante notare che, sebbene queste tecniche possano sembrare algoritmiche, hanno un impatto significativo sulle prestazioni del sistema. La scelta degli algoritmi di training, della precisione e delle strategie di comunicazione influisce direttamente sull‚Äôutilizzo delle risorse, sulla scalabilit√† e sull‚Äôefficienza del sistema ML. Pertanto, √® fondamentale adottare un approccio di co-progettazione algoritmo-hardware o algoritmo-sistema, in cui le scelte algoritmiche vengono effettuate in tandem con le considerazioni del sistema. Comprendendo l‚Äôinterazione tra algoritmi e hardware, possiamo prendere decisioni informate che ottimizzano le prestazioni del modello e l‚Äôefficienza del sistema, portando infine a soluzioni ML pi√π efficaci e scalabili.",
    "crumbs": [
      "<span class='chapter-number'>20</span>¬† <span class='chapter-title'>Conclusione</span>"
    ]
  },
  {
    "objectID": "contents/core/conclusion/conclusion.it.html#perseguire-lefficienza-nei-sistemi-di-ia",
    "href": "contents/core/conclusion/conclusion.it.html#perseguire-lefficienza-nei-sistemi-di-ia",
    "title": "20¬† Conclusione",
    "section": "20.5 Perseguire l‚ÄôEfficienza nei Sistemi di IA",
    "text": "20.5 Perseguire l‚ÄôEfficienza nei Sistemi di IA\nL‚Äôimplementazione di modelli ML addestrati √® pi√π complessa della semplice esecuzione delle reti; l‚Äôefficienza √® fondamentale (Capitolo 8). In questo capitolo sull‚Äôefficienza dell‚ÄôIA, abbiamo sottolineato che l‚Äôefficienza non √® solo un lusso, ma una necessit√† nei sistemi di intelligenza artificiale. Abbiamo approfondito i concetti chiave alla base dell‚Äôefficienza dei sistemi di IA, riconoscendo che le richieste computazionali sulle reti neurali possono essere scoraggianti, anche per i sistemi minimi. Per integrare perfettamente l‚ÄôIA nei dispositivi quotidiani e nei sistemi essenziali, deve funzionare in modo ottimale entro i vincoli delle risorse limitate, mantenendo al contempo la sua efficacia.\nIn tutto il libro, abbiamo evidenziato l‚Äôimportanza di perseguire l‚Äôefficienza per garantire che i modelli di IA siano semplificati, rapidi e sostenibili. Ottimizzando i modelli per l‚Äôefficienza, possiamo ampliare la loro applicabilit√† su varie piattaforme e scenari, consentendo all‚ÄôIA di essere distribuita in ambienti con risorse limitate come sistemi embedded e dispositivi edge. Questa ricerca dell‚Äôefficienza √® fondamentale per l‚Äôadozione diffusa e l‚Äôimplementazione pratica delle tecnologie di IA nelle applicazioni del mondo reale.",
    "crumbs": [
      "<span class='chapter-number'>20</span>¬† <span class='chapter-title'>Conclusione</span>"
    ]
  },
  {
    "objectID": "contents/core/conclusion/conclusion.it.html#ottimizzazione-delle-architetture-dei-modelli-ml",
    "href": "contents/core/conclusion/conclusion.it.html#ottimizzazione-delle-architetture-dei-modelli-ml",
    "title": "20¬† Conclusione",
    "section": "20.6 Ottimizzazione delle Architetture dei Modelli ML",
    "text": "20.6 Ottimizzazione delle Architetture dei Modelli ML\nAbbiamo quindi esplorato varie architetture di modelli, dal perceptron fondamentale alle sofisticate reti di trasformatori, ciascuna adattata a specifiche attivit√† e tipi di dati. Questa esplorazione ha messo in luce la notevole diversit√† e adattabilit√† dei modelli di apprendimento automatico, consentendo loro di affrontare vari problemi in tutti i domini.\nTuttavia, quando si distribuiscono questi modelli su sistemi, in particolare sistemi embedded con risorse limitate, l‚Äôottimizzazione del modello diventa una necessit√†. L‚Äôevoluzione delle architetture di modelli, dai primi MobileNet progettati per dispositivi mobili ai pi√π recenti modelli TinyML ottimizzati per microcontrollori, √® una testimonianza della continua innovazione.\nNel capitolo sull‚Äôottimizzazione del modello (Capitolo 9), abbiamo esaminato l‚Äôarte e la scienza dell‚Äôottimizzazione dei modelli di apprendimento automatico per garantire che siano leggeri, efficienti ed efficaci quando distribuiti in scenari TinyML. Abbiamo esplorato tecniche come la compressione del modello, la quantizzazione e la ricerca dell‚Äôarchitettura, che ci consentono di ridurre l‚Äôimpronta computazionale dei modelli mantenendone le prestazioni. Applicando queste tecniche di ottimizzazione, possiamo creare modelli su misura per i vincoli specifici dei sistemi embedded, consentendo l‚Äôimplementazione di potenti capacit√† di intelligenza artificiale su dispositivi edge. Ci√≤ apre molte possibilit√† per l‚Äôelaborazione e il processo decisionale intelligenti e in tempo reale in applicazioni IoT, robotica e mobile computing. Mentre continuiamo a spingere i confini dell‚Äôefficienza dell‚Äôintelligenza artificiale, ci aspettiamo di vedere soluzioni ancora pi√π innovative per l‚Äôimplementazione di modelli di apprendimento automatico in ambienti con risorse limitate.",
    "crumbs": [
      "<span class='chapter-number'>20</span>¬† <span class='chapter-title'>Conclusione</span>"
    ]
  },
  {
    "objectID": "contents/core/conclusion/conclusion.it.html#avanzamento-dellhardware-di-elaborazione-dellia",
    "href": "contents/core/conclusion/conclusion.it.html#avanzamento-dellhardware-di-elaborazione-dellia",
    "title": "20¬† Conclusione",
    "section": "20.7 Avanzamento dell‚ÄôHardware di Elaborazione dell‚ÄôIA",
    "text": "20.7 Avanzamento dell‚ÄôHardware di Elaborazione dell‚ÄôIA\nNel corso degli anni, abbiamo assistito a notevoli progressi nell‚Äôhardware ML, spinti dall‚Äôinsaziabile domanda di potenza di calcolo e dalla necessit√† di affrontare le sfide dei vincoli di risorse nelle distribuzioni nel mondo reale (Capitolo 10). Questi progressi sono stati cruciali nel consentire l‚Äôimplementazione di potenti funzionalit√† di intelligenza artificiale su dispositivi con risorse limitate, aprendo nuove possibilit√† in vari settori.\nL‚Äôaccelerazione hardware specializzata √® essenziale per superare questi vincoli e abilitare l‚Äôapprendimento automatico ad alte prestazioni. Gli acceleratori hardware, come GPU, FPGA e ASIC, ottimizzano le operazioni ad alta intensit√† di calcolo, in particolare l‚Äôinferenza, sfruttando i chip personalizzati progettati per efficienti moltiplicazioni di matrici. Questi acceleratori forniscono sostanziali accelerazioni rispetto alle CPU per uso generico, consentendo l‚Äôesecuzione in tempo reale di modelli ML avanzati su dispositivi con rigide limitazioni di dimensioni, peso e potenza.\nAbbiamo anche esplorato le varie tecniche e approcci per l‚Äôaccelerazione hardware nei sistemi di apprendimento automatico embedded. Abbiamo discusso i compromessi nella selezione dell‚Äôhardware appropriato per casi d‚Äôuso specifici e l‚Äôimportanza delle ottimizzazioni software per sfruttare appieno le capacit√† di questi acceleratori. Comprendendo questi concetti, i professionisti del ML possono prendere decisioni informate quando progettano e distribuiscono sistemi ML.\nData la pletora di soluzioni hardware ML disponibili, il benchmarking √® diventato essenziale per lo sviluppo e la distribuzione di sistemi di apprendimento automatico (Capitolo 11). Il benchmarking consente agli sviluppatori di misurare e confrontare le prestazioni di diverse piattaforme hardware, architetture di modello, procedure di training e strategie di distribuzione. Utilizzando benchmark consolidati come MLPerf, i professionisti ottengono preziose informazioni sugli approcci pi√π efficaci per un dato problema, considerando i vincoli unici dell‚Äôambiente di distribuzione del target.\nI progressi nell‚Äôhardware ML, combinati con le informazioni ottenute dalle tecniche di benchmarking e ottimizzazione, hanno aperto la strada alla distribuzione con successo delle capacit√† di apprendimento automatico su vari dispositivi, dai potenti server edge ai microcontrollori con risorse limitate. Man mano che il campo continua a evolversi, ci aspettiamo di vedere soluzioni hardware e approcci di benchmarking ancora pi√π innovativi che amplieranno ulteriormente i confini di ci√≤ che √® possibile con i sistemi di apprendimento automatico embedded.",
    "crumbs": [
      "<span class='chapter-number'>20</span>¬† <span class='chapter-title'>Conclusione</span>"
    ]
  },
  {
    "objectID": "contents/core/conclusion/conclusion.it.html#abbracciare-lapprendimento-on-device",
    "href": "contents/core/conclusion/conclusion.it.html#abbracciare-lapprendimento-on-device",
    "title": "20¬† Conclusione",
    "section": "20.8 Abbracciare l‚ÄôApprendimento ‚ÄúOn-Device‚Äù",
    "text": "20.8 Abbracciare l‚ÄôApprendimento ‚ÄúOn-Device‚Äù\nOltre ai progressi nell‚Äôhardware ML, abbiamo anche esplorato l‚Äôapprendimento ‚Äúon-device, in cui i modelli possono adattarsi e apprendere direttamente sul dispositivo (Capitolo 12). Questo approccio ha implicazioni significative per la privacy e la sicurezza dei dati, poich√© le informazioni sensibili possono essere elaborate localmente senza la necessit√† di trasmissione a server esterni.\nL‚Äôapprendimento ‚Äúon-device‚Äù migliora la privacy mantenendo i dati entro i confini del dispositivo, riducendo il rischio di accessi non autorizzati o violazioni dei dati. Riduce inoltre la dipendenza dalla connettivit√† cloud, consentendo ai modelli ML di funzionare efficacemente anche in scenari con accesso a Internet limitato o intermittente. Abbiamo discusso tecniche come l‚Äôapprendimento tramite trasferimento e l‚Äôapprendimento federato, che hanno ampliato le capacit√† dell‚Äôapprendimento sul dispositivo. L‚Äôapprendimento tramite trasferimento consente ai modelli di sfruttare le conoscenze acquisite da un‚Äôattivit√† o dominio per migliorare le prestazioni su un altro, consentendo un apprendimento pi√π efficiente ed efficace su dispositivi con risorse limitate. D‚Äôaltra parte, l‚Äôapprendimento federato consente aggiornamenti collaborativi del modello su dispositivi distribuiti senza aggregazione centralizzata dei dati. Questo approccio consente a pi√π dispositivi di contribuire all‚Äôapprendimento mantenendo i propri dati localmente, migliorando la privacy e la sicurezza.\nQuesti progressi nell‚Äôapprendimento su dispositivo hanno aperto la strada ad applicazioni di apprendimento automatico pi√π sicure, rispettose della privacy e decentralizzate. Mentre diamo priorit√† alla privacy e alla sicurezza dei dati nello sviluppo di sistemi ML, ci aspettiamo di vedere soluzioni pi√π innovative che consentano potenti capacit√† di IA proteggendo al contempo le informazioni sensibili e garantendo la privacy degli utenti.",
    "crumbs": [
      "<span class='chapter-number'>20</span>¬† <span class='chapter-title'>Conclusione</span>"
    ]
  },
  {
    "objectID": "contents/core/conclusion/conclusion.it.html#semplificazione-delle-operazioni-ml",
    "href": "contents/core/conclusion/conclusion.it.html#semplificazione-delle-operazioni-ml",
    "title": "20¬† Conclusione",
    "section": "20.9 Semplificazione delle Operazioni ML",
    "text": "20.9 Semplificazione delle Operazioni ML\nAnche se abbiamo capito bene i pezzi di cui sopra, sfide e considerazioni devono essere affrontate per garantire un‚Äôintegrazione e un funzionamento di successo dei modelli ML negli ambienti di produzione. Nel capitolo MLOps (Capitolo 13) abbiamo studiato le pratiche e le architetture necessarie per sviluppare, distribuire e gestire i modelli ML durante il loro intero ciclo di vita. Abbiamo esaminato le fasi di ML, dalla raccolta dati e dal training del modello alla valutazione, distribuzione e monitoraggio continuo.\nAbbiamo appreso l‚Äôimportanza dell‚Äôautomazione, della collaborazione e del miglioramento continuo in MLOps. Automatizzando i processi chiave, i team possono semplificare i loro flussi di lavoro, ridurre gli errori manuali e accelerare la distribuzione dei modelli ML. La collaborazione tra team diversi, tra cui data scientist, ingegneri ed esperti di dominio, garantisce lo sviluppo e la distribuzione di successo dei sistemi ML.\nL‚Äôobiettivo finale di questo capitolo era quello di fornire ai lettori una comprensione completa della gestione dei modelli ML, dotandoli delle conoscenze e degli strumenti necessari per creare ed eseguire applicazioni ML che forniscano un valore sostenibile con successo. Adottando le ‚Äúbest practices‚Äù in ambito MLOps, le organizzazioni possono garantire il successo e l‚Äôimpatto a lungo termine delle proprie iniziative di ML, promuovendo l‚Äôinnovazione e producendo risultati significativi.",
    "crumbs": [
      "<span class='chapter-number'>20</span>¬† <span class='chapter-title'>Conclusione</span>"
    ]
  },
  {
    "objectID": "contents/core/conclusion/conclusion.it.html#garantire-sicurezza-e-privacy",
    "href": "contents/core/conclusion/conclusion.it.html#garantire-sicurezza-e-privacy",
    "title": "20¬† Conclusione",
    "section": "20.10 Garantire Sicurezza e Privacy",
    "text": "20.10 Garantire Sicurezza e Privacy\nNessun sistema ML √® mai completo senza pensare a sicurezza e privacy. Sono di fondamentale importanza quando si sviluppano sistemi ML nel mondo reale. Poich√© l‚Äôapprendimento automatico trova sempre pi√π applicazione in domini sensibili come sanit√†, finanza e dati personali, salvaguardare la riservatezza e prevenire l‚Äôuso improprio di dati e modelli diventa un imperativo critico, e questi erano i concetti che abbiamo discusso in precedenza (Capitolo 14).\nPer creare sistemi ML robusti e responsabili, i professionisti devono comprendere a fondo i potenziali rischi per la sicurezza e la privacy. Questi rischi includono perdite di dati, che possono esporre informazioni sensibili; furto di modelli, in cui attori malintenzionati rubano modelli addestrati; attacchi avversari in grado di manipolare il comportamento del modello; pregiudizi nei modelli che possono portare a risultati ingiusti o discriminatori; e accesso involontario a informazioni private.\nPer mitigare questi rischi √® necessaria una profonda comprensione delle best practice in materia di sicurezza e privacy. Pertanto, abbiamo sottolineato che la sicurezza e la privacy non possono essere un ripensamento: devono essere affrontate in modo proattivo in ogni fase del ciclo di vita dello sviluppo del sistema ML. Sin dalle fasi iniziali di raccolta ed etichettatura dei dati, √® fondamentale garantire che i dati siano gestiti in modo sicuro e che la privacy sia protetta. Durante il training e la valutazione del modello, √® possibile impiegare tecniche come la privacy differenziale e il calcolo multi-parte sicuro per salvaguardare le informazioni sensibili.\nQuando si distribuiscono modelli ML, √® necessario implementare controlli di accesso, crittografia e meccanismi di monitoraggio robusti per impedire l‚Äôaccesso non autorizzato e rilevare potenziali violazioni della sicurezza. Il monitoraggio e l‚Äôaudit continui dei sistemi ML come parte di MLOps sono inoltre essenziali per identificare e affrontare le vulnerabilit√† emergenti di sicurezza o privacy.\nIntegrando considerazioni sulla sicurezza e sulla privacy in ogni fase di creazione, distribuzione e gestione dei sistemi ML, possiamo sbloccare in modo sicuro i vantaggi dell‚ÄôIA proteggendo al contempo i diritti degli individui e garantendo l‚Äôuso responsabile di queste potenti tecnologie. Solo attraverso questo approccio proattivo e completo possiamo creare sistemi ML che non siano solo tecnologicamente avanzati, ma anche eticamente solidi e degni della fiducia del pubblico.",
    "crumbs": [
      "<span class='chapter-number'>20</span>¬† <span class='chapter-title'>Conclusione</span>"
    ]
  },
  {
    "objectID": "contents/core/conclusion/conclusion.it.html#sostenere-considerazioni-etiche",
    "href": "contents/core/conclusion/conclusion.it.html#sostenere-considerazioni-etiche",
    "title": "20¬† Conclusione",
    "section": "20.11 Sostenere Considerazioni Etiche",
    "text": "20.11 Sostenere Considerazioni Etiche\nMentre accogliamo i progressi dell‚Äôapprendimento automatico in tutti gli aspetti della nostra vita, √® fondamentale tenere a mente le considerazioni etiche che plasmeranno il futuro dell‚ÄôIA (Capitolo 15). Equit√†, trasparenza, responsabilit√† e privacy nei sistemi di IA saranno fondamentali man mano che diventeranno pi√π integrati nelle nostre vite e nei nostri processi decisionali.\nPoich√© i sistemi di IA stanno diventando sempre pi√π diffusi e influenti, √® importante garantire che siano progettati e implementati nel rispetto dei principi etici. Ci√≤ significa mitigare attivamente i pregiudizi, promuovere l‚Äôequit√† e prevenire risultati discriminatori. Inoltre, la progettazione etica dell‚ÄôIA garantisce la trasparenza nel modo in cui i sistemi di IA prendono decisioni, consentendo agli utenti di comprendere e fidarsi dei loro risultati.\nLa responsabilit√† √® un‚Äôaltra considerazione etica fondamentale. Man mano che i sistemi di IA assumono maggiori responsabilit√† e prendono decisioni che hanno un impatto sugli individui e sulla societ√†, devono esserci meccanismi chiari per ritenere responsabili questi sistemi e i loro creatori. Ci√≤ include la definizione di ‚Äúframework‚Äù per l‚Äôaudit e il monitoraggio dei sistemi di IA e la definizione di meccanismi di responsabilit√† e risarcimento in caso di danni o conseguenze indesiderate.\nQuadri etici, regolamenti e standard saranno essenziali per affrontare queste sfide etiche. Questi quadri dovrebbero guidare lo sviluppo e l‚Äôimplementazione responsabili delle tecnologie di IA, assicurando che siano in linea con i valori della societ√† e promuovano il benessere di individui e comunit√†.\nInoltre, discussioni e collaborazioni in corso tra ricercatori, professionisti, politici e societ√† saranno cruciali per orientarsi nel panorama etico dell‚ÄôIA. Queste conversazioni dovrebbero essere inclusive e diversificate, riunendo diverse prospettive e competenze per sviluppare soluzioni complete ed eque. Mentre andiamo avanti, √® responsabilit√† collettiva di tutte le parti interessate dare priorit√† alle considerazioni etiche nello sviluppo e nell‚Äôimplementazione dei sistemi di IA.",
    "crumbs": [
      "<span class='chapter-number'>20</span>¬† <span class='chapter-title'>Conclusione</span>"
    ]
  },
  {
    "objectID": "contents/core/conclusion/conclusion.it.html#promuovere-la-sostenibilit√†-e-lequit√†",
    "href": "contents/core/conclusion/conclusion.it.html#promuovere-la-sostenibilit√†-e-lequit√†",
    "title": "20¬† Conclusione",
    "section": "20.12 Promuovere la Sostenibilit√† e l‚ÄôEquit√†",
    "text": "20.12 Promuovere la Sostenibilit√† e l‚ÄôEquit√†\nLe crescenti richieste computazionali dell‚Äôapprendimento automatico, in particolare per l‚Äôaddestramento di modelli di grandi dimensioni, hanno sollevato preoccupazioni circa il loro impatto ambientale dovuto all‚Äôelevato consumo energetico e alle emissioni di carbonio (Capitolo 16). Man mano che la scala e la complessit√† dei modelli continuano a crescere, affrontare le sfide di sostenibilit√† associate allo sviluppo dell‚ÄôIA diventa imperativo. Per mitigare l‚Äôimpatto ambientale dell‚ÄôIA, lo sviluppo di algoritmi efficienti dal punto di vista energetico √® fondamentale. Ci√≤ comporta l‚Äôottimizzazione di modelli e procedure di addestramento per ridurre al minimo i requisiti computazionali mantenendo le prestazioni. Tecniche come la compressione del modello, la quantizzazione e la ricerca efficiente dell‚Äôarchitettura neurale possono aiutare a ridurre il consumo energetico dei sistemi di IA.\nL‚Äôutilizzo di fonti di energia rinnovabili per alimentare l‚Äôinfrastruttura di IA √® un altro passo importante verso la sostenibilit√†. Passando a fonti di energia pulita come quella solare, eolica e idroelettrica, le emissioni di carbonio associate allo sviluppo dell‚ÄôIA possono essere notevolmente ridotte. Ci√≤ richiede uno sforzo concertato da parte della comunit√† dell‚ÄôIA e il supporto di politici e leader del settore per investire e adottare soluzioni di energia rinnovabile. Inoltre, l‚Äôesplorazione di paradigmi di elaborazione alternativi, come l‚Äôelaborazione neuromorfica e fotonica, promette di sviluppare sistemi di intelligenza artificiale pi√π efficienti dal punto di vista energetico. Sviluppando hardware e algoritmi che emulano i meccanismi di elaborazione del cervello, possiamo potenzialmente creare sistemi di intelligenza artificiale che siano sia potenti che sostenibili.\nLa comunit√† dell‚Äôintelligenza artificiale deve dare priorit√† alla sostenibilit√† come considerazione chiave nella ricerca e nello sviluppo. Ci√≤ implica investire in iniziative di elaborazione ecologica, come lo sviluppo di hardware efficiente dal punto di vista energetico e l‚Äôottimizzazione dei data center per ridurre il consumo di energia. Richiede inoltre la collaborazione tra discipline, riunendo esperti di IA, energia e sostenibilit√† per sviluppare soluzioni olistiche.\nInoltre, √® importante riconoscere che l‚Äôaccesso alle risorse di elaborazione dell‚ÄôIA e dell‚Äôapprendimento automatico potrebbe non essere distribuito equamente tra organizzazioni e regioni. Questa disparit√† pu√≤ portare a un divario crescente tra coloro che hanno i mezzi per sfruttare le tecnologie di IA avanzate e coloro che non li hanno. Organizzazioni come l‚ÄôOrganizzazione per la cooperazione e lo sviluppo economico (OCSE) stanno esplorando attivamente modi per affrontare questo problema e promuovere una maggiore equit√† nell‚Äôaccesso e nell‚Äôadozione dell‚ÄôIA. Promuovendo la cooperazione internazionale, condividendo le best practice e supportando iniziative di capacity building, possiamo garantire che i benefici dell‚ÄôIA siano pi√π ampiamente accessibili e che nessuno venga lasciato indietro nella rivoluzione dell‚ÄôIA.",
    "crumbs": [
      "<span class='chapter-number'>20</span>¬† <span class='chapter-title'>Conclusione</span>"
    ]
  },
  {
    "objectID": "contents/core/conclusion/conclusion.it.html#migliorare-la-robustezza-e-la-resilienza",
    "href": "contents/core/conclusion/conclusion.it.html#migliorare-la-robustezza-e-la-resilienza",
    "title": "20¬† Conclusione",
    "section": "20.13 Migliorare la Robustezza e la Resilienza",
    "text": "20.13 Migliorare la Robustezza e la Resilienza\nIl capitolo su IA Robusta approfondisce i concetti fondamentali, le tecniche e gli strumenti per la creazione di sistemi ML fault-tolerant e error-resilient (Capitolo 17). In quel capitolo, abbiamo esplorato come le tecniche di IA robuste possano affrontare le sfide poste da vari tipi di guasti hardware, inclusi guasti transitori, permanenti e intermittenti, nonch√© problemi software come bug, difetti di progettazione ed errori di implementazione.\nUtilizzando tecniche di IA robuste, i sistemi ML possono mantenere la loro affidabilit√†, sicurezza e prestazioni anche in condizioni avverse. Queste tecniche consentono ai sistemi di rilevare e ripristinare i guasti, adattarsi ad ambienti mutevoli e prendere decisioni in condizioni di incertezza.\nIl capitolo consente a ricercatori e professionisti di sviluppare soluzioni di IA in grado di resistere alle complessit√† e alle incertezze degli ambienti del mondo reale. Fornisce approfondimenti sui principi di progettazione, sulle architetture e sugli algoritmi alla base di sistemi IA robusti e una guida pratica per l‚Äôimplementazione e la convalida di questi sistemi.",
    "crumbs": [
      "<span class='chapter-number'>20</span>¬† <span class='chapter-title'>Conclusione</span>"
    ]
  },
  {
    "objectID": "contents/core/conclusion/conclusion.it.html#plasmare-il-futuro-dei-sistemi-ml",
    "href": "contents/core/conclusion/conclusion.it.html#plasmare-il-futuro-dei-sistemi-ml",
    "title": "20¬† Conclusione",
    "section": "20.14 Plasmare il Futuro dei Sistemi ML",
    "text": "20.14 Plasmare il Futuro dei Sistemi ML\nGuardando al futuro, la traiettoria dei sistemi ML punta verso un cambiamento di paradigma da un approccio incentrato sul modello a uno pi√π incentrato sui dati. Questo cambiamento riconosce che la qualit√† e la diversit√† dei dati sono fondamentali per sviluppare modelli di IA solidi, affidabili ed equi.\nPrevediamo una crescente enfasi sulle tecniche di cura dei dati, etichettatura e aumento nei prossimi anni. Queste pratiche mirano a garantire che i modelli siano addestrati su dati rappresentativi di alta qualit√† che riflettano accuratamente le complessit√† e le sfumature degli scenari del mondo reale. Concentrandoci sulla qualit√† e sulla diversit√† dei dati, possiamo mitigare i rischi di modelli parziali o distorti che possono perpetuare risultati ingiusti o discriminatori.\nQuesto approccio incentrato sui dati sar√† fondamentale per affrontare le sfide di pregiudizio, equit√† e generalizzabilit√† nei sistemi ML. Cercando e incorporando attivamente set di dati diversi e inclusivi, possiamo sviluppare modelli pi√π solidi, equi e applicabili per vari contesti e popolazioni. Inoltre, l‚Äôenfasi sui dati guider√† i progressi in tecniche come il ‚Äúdata augmentation‚Äù, in cui i set di dati esistenti vengono ampliati e diversificati tramite sintesi, traduzione e generazione di dati. Queste tecniche possono aiutare a superare i limiti dei set di dati piccoli o sbilanciati, consentendo lo sviluppo di modelli pi√π accurati e generalizzabili.\nNegli ultimi anni, l‚ÄôIA generativa ha preso d‚Äôassalto il campo, dimostrando notevoli capacit√† nella creazione di immagini, video e testo realistici. Tuttavia, l‚Äôascesa dell‚ÄôIA generativa porta anche nuove sfide per i sistemi ML (Capitolo 18). A differenza dei tradizionali sistemi ML, i modelli generativi spesso richiedono pi√π risorse computazionali e pongono sfide in termini di scalabilit√† ed efficienza. Inoltre, la valutazione e il benchmarking dei modelli generativi presentano difficolt√†, poich√© le metriche tradizionali utilizzate per le attivit√† di classificazione potrebbero non essere direttamente applicabili. Lo sviluppo di solidi framework di valutazione per i modelli generativi √® un‚Äôarea di ricerca attiva.\nComprendere e affrontare queste sfide di sistema e considerazioni etiche sar√† fondamentale per dare forma al futuro dell‚ÄôIA generativa e al suo impatto sulla societ√†. In qualit√† di professionisti e ricercatori di ML, siamo responsabili dello sviluppo delle capacit√† tecniche dei modelli generativi e dello sviluppo di sistemi e framework solidi in grado di mitigare i potenziali rischi e garantire l‚Äôapplicazione vantaggiosa di questa potente tecnologia.",
    "crumbs": [
      "<span class='chapter-number'>20</span>¬† <span class='chapter-title'>Conclusione</span>"
    ]
  },
  {
    "objectID": "contents/core/conclusion/conclusion.it.html#applicazione-di-ai-for-good",
    "href": "contents/core/conclusion/conclusion.it.html#applicazione-di-ai-for-good",
    "title": "20¬† Conclusione",
    "section": "20.15 Applicazione di ‚ÄúAI for Good‚Äù",
    "text": "20.15 Applicazione di ‚ÄúAI for Good‚Äù\nIl potenziale dell‚ÄôIA per essere utilizzata per il bene sociale √® vasto, a condizione che vengano sviluppati e distribuiti sistemi ML responsabili su larga scala in vari casi d‚Äôuso (Capitolo 19). Per realizzare questo potenziale, √® essenziale che ricercatori e professionisti si impegnino attivamente nel processo di apprendimento, sperimentazione e superamento dei limiti di ci√≤ che √® possibile.\nDurante lo sviluppo dei sistemi ML, √® fondamentale ricordare i temi e le lezioni chiave esplorati in questo libro. Questi includono l‚Äôimportanza della qualit√† e della diversit√† dei dati, la ricerca di efficienza e robustezza, il potenziale di TinyML e del calcolo neuromorfico e l‚Äôimperativo della sicurezza e della privacy. Queste intuizioni informano il lavoro e guidano le decisioni di coloro che sono coinvolti nello sviluppo di sistemi di IA.\n√à importante riconoscere che lo sviluppo dell‚ÄôIA non √® solo un‚Äôimpresa tecnica, ma anche profondamente umana. Richiede collaborazione, empatia e un impegno per comprendere le implicazioni sociali dei sistemi creati. Interagire con esperti di diversi campi, come etica, scienze sociali e politica, √® essenziale per garantire che i sistemi di IA sviluppati siano tecnicamente validi, socialmente responsabili e utili. Cogliere l‚Äôopportunit√† di far parte di questo campo trasformativo e plasmarne il futuro √® un privilegio e una responsabilit√†. Lavorando insieme, possiamo creare un mondo in cui i sistemi di ML fungono da strumenti per un cambiamento positivo e per migliorare la condizione umana.",
    "crumbs": [
      "<span class='chapter-number'>20</span>¬† <span class='chapter-title'>Conclusione</span>"
    ]
  },
  {
    "objectID": "contents/core/conclusion/conclusion.it.html#congratulazioni",
    "href": "contents/core/conclusion/conclusion.it.html#congratulazioni",
    "title": "20¬† Conclusione",
    "section": "20.16 Congratulazioni",
    "text": "20.16 Congratulazioni\nCongratulazioni per essere arrivati fin qui e buona fortuna per gli sforzi futuri! Il futuro dell‚Äôintelligenza artificiale √® luminoso e pieno di infinite possibilit√†. Sar√† emozionante vedere gli incredibili contributi che darete in questo campo.\nSentitevi liberi di contattarmi in qualsiasi momento all‚Äôindirizzo vj at eecs dot harvard dot edu.\n‚Äì Prof.¬†Vijay Janapa Reddi, Harvard University",
    "crumbs": [
      "<span class='chapter-number'>20</span>¬† <span class='chapter-title'>Conclusione</span>"
    ]
  },
  {
    "objectID": "contents/labs/overview.it.html",
    "href": "contents/labs/overview.it.html",
    "title": "Panoramica",
    "section": "",
    "text": "Obiettivi dell‚ÄôApprendimento\nCompletando questi ‚Äúlab‚Äù, speriamo che gli studenti:",
    "crumbs": [
      "LABORATORI",
      "Panoramica"
    ]
  },
  {
    "objectID": "contents/labs/overview.it.html#obiettivi-dellapprendimento",
    "href": "contents/labs/overview.it.html#obiettivi-dellapprendimento",
    "title": "Panoramica",
    "section": "",
    "text": "Consiglio\n\n\n\n\nAcquisiscano competenza nell‚Äôimpostazione e distribuzione di modelli ML su dispositivi supportati, consentendo di affrontare scenari di distribuzione ML nel mondo reale con sicurezza.\nComprendano i passaggi coinvolti nell‚Äôadattamento e nella sperimentazione di modelli ML per diverse applicazioni, consentendo di ottimizzare prestazioni ed efficienza.\nApprendano tecniche di risoluzione dei problemi specifiche per le distribuzioni ML embedded, dotandoli delle competenze per superare insidie e sfide comuni.\nAcquisiscano esperienza pratica nella distribuzione di modelli TinyML su dispositivi embedded, colmando il divario tra teoria e pratica.\nEsplorino varie modalit√† di sensori e le loro applicazioni, ampliando la comprensione di come ML pu√≤ essere sfruttato in diversi domini.\nFavoriscano una comprensione delle implicazioni e delle sfide del mondo reale associate alle distribuzioni di sistemi ML, preparandoli per progetti futuri.",
    "crumbs": [
      "LABORATORI",
      "Panoramica"
    ]
  },
  {
    "objectID": "contents/labs/overview.it.html#pubblico-target",
    "href": "contents/labs/overview.it.html#pubblico-target",
    "title": "Panoramica",
    "section": "Pubblico Target",
    "text": "Pubblico Target\nQuesti lab sono progettati per:\n\nPrincipianti nel campo dell‚Äôapprendimento automatico che hanno un vivo interesse nell‚Äôesplorare l‚Äôintersezione tra ML e sistemi embedded.\nSviluppatori e ingegneri che desiderano applicare modelli ML ad applicazioni del mondo reale utilizzando dispositivi a basso consumo e risorse limitate.\nAppassionati e ricercatori che desiderano acquisire esperienza pratica nell‚Äôimplementazione dell‚ÄôIA su dispositivi edge e comprendere le sfide uniche coinvolte.",
    "crumbs": [
      "LABORATORI",
      "Panoramica"
    ]
  },
  {
    "objectID": "contents/labs/overview.it.html#dispositivi-supportati",
    "href": "contents/labs/overview.it.html#dispositivi-supportati",
    "title": "Panoramica",
    "section": "Dispositivi Supportati",
    "text": "Dispositivi Supportati\nAbbiamo incluso materiali di laboratorio per tre dispositivi chiave che rappresentano diversi profili hardware e capacit√†.\n\nNicla Vision: Ottimizzato per applicazioni basate sulla visione come la classificazione delle immagini e il rilevamento di oggetti, ideale per casi d‚Äôuso compatti e a basso consumo.\nXIAO ESP32S3: Una scheda versatile e compatta adatta per attivit√† di individuazione di parole chiave e rilevamento del movimento.\nRaspberry Pi: Una piattaforma flessibile per attivit√† pi√π intensive dal punto di vista computazionale, inclusi piccoli modelli linguistici e varie applicazioni di classificazione e rilevamento.\n\n\n\n\nEsercizio\nNicla Vision\nXIAO ESP32S3\nRaspberry Pi\n\n\n\n\nInstallation & Setup\n‚úì\n‚úì\n‚úì\n\n\nKeyword Spotting (KWS)\n‚úì\n‚úì\n\n\n\nImage Classification\n‚úì\n‚úì\n‚úì\n\n\nObject Detection\n‚úì\n‚úì\n‚úì\n\n\nMotion Detection\n‚úì\n‚úì\n\n\n\nSmall Language Models (SLM)\n\n\n‚úì",
    "crumbs": [
      "LABORATORI",
      "Panoramica"
    ]
  },
  {
    "objectID": "contents/labs/overview.it.html#struttura-del-lab",
    "href": "contents/labs/overview.it.html#struttura-del-lab",
    "title": "Panoramica",
    "section": "Struttura del Lab",
    "text": "Struttura del Lab\nOgni lab segue un approccio strutturato:\n\nIntroduzione: Esplora l‚Äôapplicazione e la sua importanza in scenari reali.\nConfigurazione: Istruzioni dettagliate per configurare l‚Äôambiente hardware e software.\nDistribuzione: Guida al training e alla distribuzione dei modelli ML pre-addestrati sui dispositivi supportati.\nEsercizi: Attivit√† pratiche per modificare e sperimentare con i parametri del modello.\nDiscussione: Analisi dei risultati, potenziali miglioramenti e approfondimenti pratici.",
    "crumbs": [
      "LABORATORI",
      "Panoramica"
    ]
  },
  {
    "objectID": "contents/labs/overview.it.html#sequenza-dei-laboratori-consigliata",
    "href": "contents/labs/overview.it.html#sequenza-dei-laboratori-consigliata",
    "title": "Panoramica",
    "section": "Sequenza dei Laboratori Consigliata",
    "text": "Sequenza dei Laboratori Consigliata\nSe si √® alle prime armi con l‚ÄôML embedded, consigliamo di iniziare con la configurazione e l‚Äôindividuazione delle parole chiave prima di passare alla classificazione delle immagini e al rilevamento degli oggetti. Gli utenti di Raspberry Pi possono esplorare attivit√† pi√π avanzate, come piccoli modelli linguistici, dopo aver familiarizzato con le basi.",
    "crumbs": [
      "LABORATORI",
      "Panoramica"
    ]
  },
  {
    "objectID": "contents/labs/overview.it.html#risoluzione-dei-problemi-e-supporto",
    "href": "contents/labs/overview.it.html#risoluzione-dei-problemi-e-supporto",
    "title": "Panoramica",
    "section": "Risoluzione dei Problemi e Supporto",
    "text": "Risoluzione dei Problemi e Supporto\nSe si riscontrano problemi durante i laboratori, consultare i commenti sui ‚Äútroubleshooting‚Äù [risoluzione dei problemi] o controllare le FAQ all‚Äôinterno di ogni laboratorio. Per ulteriore assistenza, non esitate a contattare il nostro team di supporto o a interagire con i forum della community.",
    "crumbs": [
      "LABORATORI",
      "Panoramica"
    ]
  },
  {
    "objectID": "contents/labs/overview.it.html#crediti",
    "href": "contents/labs/overview.it.html#crediti",
    "title": "Panoramica",
    "section": "Crediti",
    "text": "Crediti\nUn ringraziamento speciale e un ringraziamento al Prof.¬†Marcelo Rovai per il suo prezioso contributo allo sviluppo e al continuo perfezionamento di questi laboratori.",
    "crumbs": [
      "LABORATORI",
      "Panoramica"
    ]
  },
  {
    "objectID": "contents/labs/getting_started.it.html",
    "href": "contents/labs/getting_started.it.html",
    "title": "Guida Introduttiva",
    "section": "",
    "text": "Requisiti Hardware\nPer seguire i laboratori pratici, ci sar√† bisogno del seguente hardware:\nArduino Nicla Vision √® progettata su misura per applicazioni di livello professionale, offrendo funzionalit√† avanzate e prestazioni adatte a progetti industriali impegnativi. D‚Äôaltro canto, Seeed Studio XIAO ESP32S3 Sense √® rivolta a maker, hobbisti e studenti che desiderano esplorare applicazioni IA edge in un formato pi√π accessibile e adatto ai principianti. Entrambe le schede hanno i loro punti di forza e il loro pubblico di riferimento, consentendo agli utenti di scegliere la soluzione migliore per le loro esigenze e il loro livello di competenza. Raspberry Pi √® destinato a progetti di ingegneria e apprendimento automatico pi√π avanzati.",
    "crumbs": [
      "LABORATORI",
      "Guida Introduttiva"
    ]
  },
  {
    "objectID": "contents/labs/getting_started.it.html#requisiti-hardware",
    "href": "contents/labs/getting_started.it.html#requisiti-hardware",
    "title": "Guida Introduttiva",
    "section": "",
    "text": "Scheda Arduino Nicla Vision\n\nArduino Nicla Vision √® una scheda potente e compatta progettata per applicazioni audio e di visione artificiale di livello professionale. √à dotata di un modulo telecamera di alta qualit√†, un microfono digitale e un‚ÄôIMU, che la rendono adatta a progetti impegnativi in settori quali robotica, automazione e sorveglianza.\nSpecifiche di Arduino Nicla Vision\nSchema e pinout di Arduino Nicla Vision\n\nScheda XIAO ESP32S3 Sense\n\nLa scheda Seeed Studio XIAO ESP32S3 Sense √® una scheda minuscola e ricca di funzionalit√†, progettata per maker, hobbisti e studenti interessati a esplorare applicazioni IA edge. √à dotata di fotocamera, microfono e IMU, rendendo facile iniziare con progetti come classificazione delle immagini, individuazione di parole chiave e rilevamento del movimento.\nSpecifiche di XIAO ESP32S3 Sense\nSchema e pinout di XIAO ESP32S3 Sense\n\nRaspberry Pi Single Computer board\n\n\nIl Raspberry Pi √® un potente e versatile computer a scheda singola che √® diventato uno strumento essenziale per gli ingegneri di varie discipline. Sviluppati dalla Raspberry Pi Foundation, questi dispositivi compatti offrono una combinazione unica di convenienza, potenza di calcolo e ampie capacit√† GPIO (General Purpose Input/Output), rendendoli ideali per la prototipazione, lo sviluppo di sistemi embedded e progetti di ingegneria avanzata.\nDocumentazione Hardware di Raspberry Pi\nDocumentazione della Telecamera\n\n\nAccessori aggiuntivi\n\nCavo USB-C per la programmazione e l‚Äôalimentazione di XIAO\nCavo micro-USB per la programmazione e l‚Äôalimentazione di Nicla\nAlimentatore per Raspberry\nBreadboard e cavi jumper (opzionali, per collegare sensori aggiuntivi)",
    "crumbs": [
      "LABORATORI",
      "Guida Introduttiva"
    ]
  },
  {
    "objectID": "contents/labs/getting_started.it.html#requisiti-software",
    "href": "contents/labs/getting_started.it.html#requisiti-software",
    "title": "Guida Introduttiva",
    "section": "Requisiti Software",
    "text": "Requisiti Software\nPer programmare le schede e sviluppare progetti di apprendimento automatico embedded, ci sar√† bisogno del seguente software:\n\nArduino IDE\n\nDownload e installazione\n\nInstall di Arduino IDE\nSeguire la guida all‚Äôinstallazione per il sistema operativo in uso.\nArduino CLI\nConfigurare l‚ÄôIDE Arduino per le schede Arduino Nicla Vision e XIAO ESP32S3 Sense.\n\n\nOpenMV IDE (opzionale)\n\nScaricare e installare OpenMV IDE per il sistema operativo in uso.\nConfigurare l‚ÄôIDE OpenMV per Arduino Nicla Vision.\n\nEdge Impulse Studio\n\nRegistrarsi per un account gratuito su Edge Impulse Studio.\nInstallare Edge Impulse CLI\nSeguire le guide per connettere le schede Arduino Nicla Vision e XIAO ESP32S3 Sense a Edge Impulse Studio.\n\nRaspberry Pi OS\n\n\nScaricare e installare Raspberry Pi Imager",
    "crumbs": [
      "LABORATORI",
      "Guida Introduttiva"
    ]
  },
  {
    "objectID": "contents/labs/getting_started.it.html#connettivit√†-di-rete",
    "href": "contents/labs/getting_started.it.html#connettivit√†-di-rete",
    "title": "Guida Introduttiva",
    "section": "Connettivit√† di Rete",
    "text": "Connettivit√† di Rete\nAlcuni progetti potrebbero richiedere la connettivit√† Internet per la raccolta dati o la distribuzione del modello. Assicurarsi che la connessione dell‚Äôambiente di sviluppo sia stabile tramite Wi-Fi o Ethernet. Per Raspberry Pi, √® necessaria una connessione Wi-Fi o Ethernet per il funzionamento remoto senza la necessit√† di collegare un monitor, una tastiera e un mouse.\n\nPer Arduino Nicla Vision, si pu√≤ utilizzare il modulo Wi-Fi integrato per connetterti a una rete wireless.\nPer XIAO ESP32S3 Sense, si pu√≤ utilizzare il modulo Wi-Fi integrato o collegare un modulo Wi-Fi o Ethernet esterno utilizzando i pin disponibili.\nPer Raspberry Pi, si pu√≤ utilizzare il modulo Wi-Fi integrato per collegare un modulo Wi-Fi o Ethernet esterno utilizzando il connettore disponibile.",
    "crumbs": [
      "LABORATORI",
      "Guida Introduttiva"
    ]
  },
  {
    "objectID": "contents/labs/getting_started.it.html#conclusione",
    "href": "contents/labs/getting_started.it.html#conclusione",
    "title": "Guida Introduttiva",
    "section": "Conclusione",
    "text": "Conclusione\nCon l‚Äôhardware e il software impostati, siamo pronti per intraprendere il viaggio di apprendimento automatico embedded. I laboratori pratici guideranno attraverso vari progetti, coprendo argomenti come classificazione delle immagini, rilevamento di oggetti, individuazione di parole chiave e classificazione del movimento.\nSe si riscontrano problemi o ci sono domande, non esitate a consultare le guide alla risoluzione dei problemi o i forum o a cercare supporto dalla community.\nTuffiamoci e sblocchiamo il potenziale dell‚Äôapprendimento automatico su (tiny) sistemi reali!",
    "crumbs": [
      "LABORATORI",
      "Guida Introduttiva"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/setup/setup.it.html",
    "href": "contents/labs/arduino/nicla_vision/setup/setup.it.html",
    "title": "Setup",
    "section": "",
    "text": "Panoramica\nLa Arduino Nicla Vision (a volte chiamata NiclaV) √® una scheda di sviluppo che include due processori in grado di eseguire attivit√† in parallelo. Fa parte di una famiglia di schede di sviluppo con lo stesso fattore di forma ma progettate per attivit√† specifiche, come Nicla Sense ME e la Nicla Voice. Le Nicla possono eseguire in modo efficiente processi creati con TensorFlow Lite. Ad esempio, uno dei core di NiclaV esegue un algoritmo di visione artificiale al volo (inferenza), mentre l‚Äôaltro esegue operazioni di basso livello come il controllo di un motore e la comunicazione o l‚Äôazione come interfaccia utente. Il modulo wireless integrato consente la gestione simultanea della connettivit√† WiFi e Bluetooth Low Energy (BLE).",
    "crumbs": [
      "Nicla Vision",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/setup/setup.it.html#hardware",
    "href": "contents/labs/arduino/nicla_vision/setup/setup.it.html#hardware",
    "title": "Setup",
    "section": "Hardware",
    "text": "Hardware\n\nDue Core Paralleli\nIl processore centrale √® il dual-core STM32H747, che include un Cortex M7 at 480 MHz e un Cortex M4 at 240 MHz. I due core comunicano tramite un meccanismo di Remote Procedure Call che consente di richiamare senza problemi le funzioni sull‚Äôaltro processore. Entrambi i processori condividono tutte le periferiche on-chip e possono eseguire:\n\nSketch Arduino su Arm Mbed OS\nApplicazioni Native Mbed\nMicroPython / JavaScript tramite un interprete\nTensorFlow Lite\n\n\n\n\nMemoria\nLa memoria √® fondamentale per i progetti di machine learning [apprendimento automatico] embedded. La scheda NiclaV pu√≤ ospitare fino a 16 MB di QSPI Flash per l‚Äôarchiviazione. Tuttavia, √® essenziale considerare che la SRAM MCU √® quella da utilizzare con le inferenze di machine learning; l‚ÄôSTM32H747 √® di soli 1 MB, condiviso da entrambi i processori. Questa MCU ha anche incluso 2 MB di FLASH, principalmente per l‚Äôarchiviazione del codice.\n\n\nSensori\n\nFotocamera: Una fotocamera CMOS a colori GC2145 da 2 MP.\nMicrofono: I‚ÄôMP34DT05 √® un microfono digitale MEMS omnidirezionale, ultracompatto, a basso consumo, costruito con un elemento di rilevamento capacitivo e l‚Äôinterfaccia IC.\nIMU a 6 Assi: Dati del giroscopio 3D e dell‚Äôaccelerometro 3D dall‚ÄôIMU a 6 assi LSM6DSOX.\nSensore del Time of Flight: Il sensore del tempo di volo VL53L1CBV0FY aggiunge capacit√† di misurazione precise e a bassa potenza alla Nicla Vision. Il laser invisibile VCSEL vicino all‚Äôinfrarosso (incluso il driver analogico) √® incapsulato con ottica ricevente in un piccolo modulo, tutto in uno, sotto la telecamera.",
    "crumbs": [
      "Nicla Vision",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/setup/setup.it.html#installazione-di-arduino-ide",
    "href": "contents/labs/arduino/nicla_vision/setup/setup.it.html#installazione-di-arduino-ide",
    "title": "Setup",
    "section": "Installazione di Arduino IDE",
    "text": "Installazione di Arduino IDE\nSi inizia collegando la scheda (microUSB) al computer:\n\nSi installa il core Mbed OS per le schede Nicla nell‚ÄôIDE Arduino. Con l‚ÄôIDE aperto, si va su Tools &gt; Board &gt; Board Manager, si cerca Arduino Nicla Vision nella finestra di ricerca e si installa la scheda.\n\nPoi, si va su Tools &gt; Board &gt; Arduino Mbed OS Nicla Boards e si seleziona Arduino Nicla Vision. Con la scheda collegata alla porta USB, si dovrebbe vedere ‚ÄúNicla on Port‚Äù e selezionarla.\n\nSi apre lo sketch Blink su Examples/Basic ed lo si esegue usando il pulsante ‚ÄúIDE Upload‚Äù. Si dovrebbe vedere il LED integrato (RGB verde) lampeggiare, il che significa che la scheda Nicla √® installata correttamente e funzionante!\n\n\nTest del Microfono\nSu Arduino IDE, si va su Examples &gt; PDM &gt; PDMSerialPlotter, si apre e si esegue lo sketch. Si apre il Plotter e si guarda la rappresentazione audio dal microfono:\n\n\nVariare la frequenza del suono generato e verificare che il microfono funzioni correttamente.\n\n\n\nTest dell‚ÄôIMU\nPrima di testare l‚ÄôIMU, sar√† necessario installare la libreria LSM6DSOX. Per farlo, si vai su Library Manager e si cerca LSM6DSOX. Si installa la libreria fornita da Arduino:\n\nPoi, si va su Examples &gt; Arduino_LSM6DSOX &gt; SimpleAccelerometer e si esegue il test dell‚Äôaccelerometro (si pu√≤ anche eseguire Gyro e temperatura della scheda):\n\n\n\nTest del sensore ToF (Time of Flight) [tempo di volo]\nCome fatto con l‚ÄôIMU, √® necessario installare la libreria ToF VL53L1X. Per farlo, si va su Library Manager e si cerca VL53L1X. Si installa la libreria fornita da Pololu:\n\nPoi, si esegue lo sketch proximity_detection.ino:\n\nSul monitor seriale, si vedr√† la distanza dalla telecamera di un oggetto di fronte ad essa (max 4 m).\n\n\n\nTest della Fotocamera\nPossiamo anche testare la fotocamera utilizzando, ad esempio, il codice fornito in Examples &gt; Camera &gt; CameraCaptureRawBytes. Non possiamo vedere l‚Äôimmagine direttamente, ma √® possibile ottenere i dati ‚Äúcrudi‚Äù dell‚Äôimmagine generati dalla telecamera.\nIn ogni caso, il test migliore con la telecamera √® vedere un‚Äôimmagine dal vivo. Per questo, useremo un altro IDE, OpenMV.",
    "crumbs": [
      "Nicla Vision",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/setup/setup.it.html#installazione-di-openmv-ide",
    "href": "contents/labs/arduino/nicla_vision/setup/setup.it.html#installazione-di-openmv-ide",
    "title": "Setup",
    "section": "Installazione di OpenMV IDE",
    "text": "Installazione di OpenMV IDE\nOpenMV IDE √® il principale ambiente di sviluppo integrato con le telecamere OpenMV come quella su Nicla Vision. √à dotato di un potente editor di testo, terminale di debug e visualizzatore di frame buffer con visualizzazione di istogrammi. Utilizzeremo MicroPython per programmare la telecamera.\nSi va alla pagina di OpenMV IDE, si scarica la versione corretta per il proprio sistema operativo e si seguono le istruzioni per l‚Äôinstallazione sul computer.\n\nL‚ÄôIDE dovrebbe aprirsi, con il codice helloworld_1.py predefinito nella sua ‚ÄúCode Area‚Äù. In caso contrario, lo si pu√≤ aprire da Files &gt; Examples &gt; HelloWord &gt; helloword.py\n\nTutti i messaggi inviati tramite una connessione seriale (utilizzando print() o i messaggi di errore) verranno visualizzati sul Serial Terminal durante l‚Äôesecuzione. L‚Äôimmagine catturata da una telecamera verr√† visualizzata nell‚Äôarea Camera Viewer (o Frame Buffer) e nell‚Äôarea Histogram immediatamente sotto Camera Viewer.\n\nPrima di collegare la Nicla all‚ÄôIDE OpenMV, si deve avere la versione pi√π recente del bootloader. Si va all‚ÄôIDE Arduino, si seleziona la scheda Nicla e si apre lo sketch su Examples &gt; STM_32H747_System STM32H747_manageBootloader. Upload-are il codice sulla board. Il Serial Monitor vi guider√†.\n\nDopo aver aggiornato il bootloader, si mette la Nicla Vision in modalit√† bootloader premendo due volte il pulsante di reset sulla scheda. Il LED verde integrato inizier√† a spegnersi e accendersi in dissolvenza [fading]. Ora si torna all‚ÄôIDE OpenMV e si clicca sull‚Äôicona di connessione (Left ToolBar):\n\nUn pop-up informer√† che √® stata rilevata una scheda in modalit√† DFU e chieder√† come si desidera procedere. Per prima cosa, si seleziona Install the latest release firmware (vX.Y.Z). Questa azione installer√† l‚Äôultimo firmware OpenMV su Nicla Vision.\n\nSi pu√≤ lasciare l‚Äôopzione Erase internal file system deselezionata e cliccare su [OK].\nIl LED verde di Nicla inizier√† a lampeggiare mentre il firmware OpenMV viene caricato sulla scheda e si aprir√† una finestra del terminale che mostrer√† l‚Äôavanzamento del flashing.\n\nAttendere che il LED verde smetta di lampeggiare in dissolvenza. Quando il processo termina, si vedr√† un messaggio che dice ‚ÄúDFU firmware update complete!‚Äù. Premere [OK].\n\nQuando Nicla Vison si collega alla Barra degli Strumenti, compare un pulsante verde per la riproduzione.\n\nNotare che sul computer apparir√† un‚Äôunit√† denominata ‚ÄúNO NAME‚Äù:\n\nOgni volta che si preme il pulsante [RESET] sulla board, viene automaticamente eseguito lo script main.py che vi √® memorizzato. Si pu√≤ caricare il codice di main.py sull‚ÄôIDE (File &gt; Open File...).\n\n\nQuesto √® il codice di ‚ÄúBlink‚Äù, che conferma che l‚ÄôHW √® OK.\n\nPer testare la telecamera, eseguiamo helloword_1.py. Per farlo, si seleziona lo script su File &gt; Examples &gt; HelloWorld &gt; helloword.py,\nQuando si clicca sul pulsante di riproduzione verde, lo script MicroPython (hellowolrd.py) nella ‚ÄúCode Area‚Äù verr√† caricato ed eseguito su Nicla Vision. Sul ‚ÄúCamera Viewer‚Äù, si vedr√† lo streaming video. Il ‚ÄúSerial Monitor‚Äù ci mostrer√† gli FPS (fotogrammi al secondo), che dovrebbero essere circa 14 fps.\n\nEcco lo script helloworld.py:\n# Hello World Example 2\n#\n# Welcome to the OpenMV IDE! Click on the green run arrow button below to run the script!\n\nimport sensor, image, time\n\nsensor.reset()                      # Reset and initialize the sensor.\nsensor.set_pixformat(sensor.RGB565) # Set pixel format to RGB565 (or GRAYSCALE)\nsensor.set_framesize(sensor.QVGA)   # Set frame size to QVGA (320x240)\nsensor.skip_frames(time = 2000)     # Wait for settings take effect.\nclock = time.clock()                # Create a clock object to track the FPS.\n\nwhile(True):\n    clock.tick()                    # Update the FPS clock.\n    img = sensor.snapshot()         # Take a picture and return the image.\n    print(clock.fps())\nIn GitHub, si trovano gli script Python utilizzati qui.\nIl codice pu√≤ essere suddiviso in due parti:\n\nSetup: Dove le librerie vengono importate, inizializzate e le variabili vengono definite e inizializzate.\nLoop: (ciclo while) parte del codice che viene eseguita continuamente. L‚Äôimmagine (la variabile img) viene catturata (un frame). Ognuno di questi frame pu√≤ essere utilizzato per l‚Äôinferenza nelle Applicazioni di Apprendimento Automatico.\n\nPer interrompere l‚Äôesecuzione del programma, premere il pulsante rosso [X].\n\nNota: OpenMV Cam funziona a circa la met√† della velocit√† quando √® connesso all‚ÄôIDE. Gli FPS dovrebbero aumentare una volta disconnessi.\n\nIn GitHub, si trovano altri script Python. Provare a testare i sensori di bordo.",
    "crumbs": [
      "Nicla Vision",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/setup/setup.it.html#collegamento-di-nicla-vision-a-edge-impulse-studio",
    "href": "contents/labs/arduino/nicla_vision/setup/setup.it.html#collegamento-di-nicla-vision-a-edge-impulse-studio",
    "title": "Setup",
    "section": "Collegamento di Nicla Vision a Edge Impulse Studio",
    "text": "Collegamento di Nicla Vision a Edge Impulse Studio\nAvremo bisogno di Edge Impulse Studio pi√π avanti in altri esercizi. Edge Impulse √® una piattaforma di sviluppo leader per il machine learning [apprendimento automatico] su dispositivi edge.\nEdge Impulse supporta ufficialmente Nicla Vision. Quindi, per iniziare, si crea un nuovo progetto su Studio e vi si collega Nicla. Per farlo, si seguono i passaggi:\n\nScaricare l‚ÄôEI Firmware pi√π aggiornato e decomprimerlo.\nAprire il file zip sul computer e selezionare l‚Äôuploader corrispondente al sistema operativo:\n\n\n\nMettere Nicla-Vision in ‚ÄúBoot Mode‚Äù, premendo due volte il pulsante di reset.\nEseguire il codice batch specifico per il proprio sistema operativo per caricare il binario arduino-nicla-vision.bin sulla board.\n\nAndare sul progetto sullo Studio e nella scheda Data Acquisition tab, selezionare WebUSB (1). Si aprir√† una finestra; scegliere l‚Äôopzione che mostra che Nicla is paired (2) e premere [Connect] (3).\n\nNella sezione Collect Data della scheda Data Acquisition, si possono scegliere quali dati del sensore raccogliere.\n\nPer esempio. IMU data:\n\nO Image (Camera):\n\nE cos√¨ via. Si pu√≤ anche testare un sensore esterno collegato all‚ÄôADC (Nicla pin 0) e agli altri sensori a bordo, come il microfono e il ToF.",
    "crumbs": [
      "Nicla Vision",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/setup/setup.it.html#espansione-della-nicla-vision-board-opzionale",
    "href": "contents/labs/arduino/nicla_vision/setup/setup.it.html#espansione-della-nicla-vision-board-opzionale",
    "title": "Setup",
    "section": "Espansione della Nicla Vision Board (opzionale)",
    "text": "Espansione della Nicla Vision Board (opzionale)\nUn ultimo elemento da esplorare √® che a volte, durante la prototipazione, √® essenziale sperimentare con sensori e dispositivi esterni, e un‚Äôeccellente espansione per Nicla √® l‚ÄôArduino MKR Connector Carrier (Grove compatible).\nLo shield ha 14 connettori Grove: cinque ingressi analogici singoli (A0-A5), un ingresso analogico doppio (A5/A6), cinque I/O digitali singoli (D0-D4), un I/O digitale doppio (D5/D6), una I2C (TWI) e una UART (Seriale). Tutti i connettori sono compatibili con 5 V.\n\nNotare che tutti i 17 pin Nicla Vision saranno collegati agli Shield Grove, ma alcune connessioni Grove rimangono scollegate.\n\n\nQuesto shield √® compatibile con MKR e pu√≤ essere utilizzato con Nicla Vision e Portenta.\n\nAd esempio, supponiamo che in un progetto TinyML si desideri inviare risultati di inferenza utilizzando un dispositivo LoRaWAN e aggiungere informazioni sulla luminosit√† locale. Spesso, con operazioni offline, si consiglia un display locale a basso consumo come un OLED. Questa configurazione pu√≤ essere visualizzata qui:\n\nIl Grove Light Sensor √® collegato a uno dei singoli pin analogici (A0/PC4), il LoRaWAN device alla UART e l‚ÄôOLED al connettore della I2C.\nI pin Nicla 3 (Tx) e 4 (Rx) sono collegati al connettore Serial Shield. La comunicazione UART √® utilizzata con il dispositivo LoRaWan. Ecco un semplice codice per utilizzare l‚ÄôUART:\n# UART Test - By: marcelo_rovai - Sat Sep 23 2023\n\nimport time\nfrom pyb import UART\nfrom pyb import LED\n\nredLED = LED(1) # built-in red LED\n\n# Init UART object.\n# Nicla Vision's UART (TX/RX pins) is on \"LP1\"\nuart = UART(\"LP1\", 9600)\n\nwhile(True):\n    uart.write(\"Hello World!\\r\\n\")\n    redLED.toggle()\n    time.sleep_ms(1000)\nPer verificare che l‚ÄôUART funzioni, si deve, ad esempio, collegare un altro dispositivo come Arduino UNO, visualizzando ‚ÄúHello Word‚Äù sul Serial Monitor. Ecco il codice.\n\nDi seguito √® riportato il codice Hello World da utilizzare con l‚ÄôOLED I2C. Il driver MicroPython SSD1306 OLED (ssd1306.py), creato da Adafruit, dovrebbe essere caricato anche su Nicla (lo script ssd1306.py si trova su GitHub).\n# Nicla_OLED_Hello_World - By: marcelo_rovai - Sat Sep 30 2023\n\n#Save on device: MicroPython SSD1306 OLED driver, I2C and SPI interfaces created by Adafruit\nimport ssd1306\n\nfrom machine import I2C\ni2c = I2C(1)\n\noled_width = 128\noled_height = 64\noled = ssd1306.SSD1306_I2C(oled_width, oled_height, i2c)\n\noled.text('Hello, World', 10, 10)\noled.show()\nInfine, ecco uno script semplice per leggere il valore ADC sul pin ‚ÄúPC4‚Äù (pin A0 di Nicla):\n# Light Sensor (A0) - By: marcelo_rovai - Wed Oct 4 2023\n\nimport pyb\nfrom time import sleep\n\nadc = pyb.ADC(pyb.Pin(\"PC4\"))     # create an analog object from a pin\nval = adc.read()                  # read an analog value\n\nwhile (True):\n\n    val = adc.read()  \n    print (\"Light={}\".format (val))\n    sleep (1)\nL‚ÄôADC pu√≤ essere utilizzato per altre variabili del sensore, come la Temperatura.\n\nNotare che gli script sopra (scaricati da Github) presentano solo come collegare dispositivi esterni alla scheda Nicla Vision utilizzando MicroPython.",
    "crumbs": [
      "Nicla Vision",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/setup/setup.it.html#conclusione",
    "href": "contents/labs/arduino/nicla_vision/setup/setup.it.html#conclusione",
    "title": "Setup",
    "section": "Conclusione",
    "text": "Conclusione\nArduino Nicla Vision √® un eccellente piccolo dispositivo per usi industriali e professionali! Tuttavia, √® potente, affidabile, a basso consumo e ha sensori adatti per le applicazioni di machine learning embedded pi√π comuni come visione, movimento, ‚Äúfusion‚Äù di sensori e suono.\n\nNel repository GitHub, si trova l‚Äôultima versione di tutti i codici utilizzati o commentati in questo esercizio pratico.",
    "crumbs": [
      "Nicla Vision",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/setup/setup.it.html#risorse",
    "href": "contents/labs/arduino/nicla_vision/setup/setup.it.html#risorse",
    "title": "Setup",
    "section": "Risorse",
    "text": "Risorse\n\nCodici Micropython\nCodici Arduino",
    "crumbs": [
      "Nicla Vision",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/image_classification/image_classification.it.html",
    "href": "contents/labs/arduino/nicla_vision/image_classification/image_classification.it.html",
    "title": "Classificazione delle Immagini",
    "section": "",
    "text": "Panoramica\nMentre iniziamo i nostri studi sul machine learning embedded o TinyML, √® impossibile ignorare l‚Äôimpatto trasformativo della Computer Vision (CV) e dell‚ÄôIntelligenza Artificiale (IA) nelle nostre vite. Queste due discipline interconnesse ridefiniscono ci√≤ che le macchine possono percepire e realizzare, dai veicoli autonomi e dalla robotica all‚Äôassistenza sanitaria e alla sorveglianza.\nSempre di pi√π, ci troviamo di fronte a una rivoluzione dell‚Äôintelligenza artificiale (IA) in cui, come affermato da Gartner, Edge AI ha un potenziale di impatto molto elevato, ed √® ora!\nNel ‚Äúcentro‚Äù del Radar c‚Äô√® la Edge Computer Vision, e quando parliamo di Machine Learning (ML) applicato alla visione, la prima cosa che viene in mente √® la Classificazione delle immagini, una specie di ‚ÄúHello World‚Äù di ML!\nQuesto esercizio esplorer√† un progetto di computer vision che utilizza Convolutional Neural Network (CNN) [Reti Neurali Convoluzionali] per la classificazione delle immagini in tempo reale. Sfruttando il robusto ecosistema di TensorFlow, implementeremo un modello MobileNet pre-addestrato e lo adatteremo per il ‚Äúdeployment‚Äù edge. L‚Äôattenzione sar√† rivolta all‚Äôottimizzazione del modello per un‚Äôesecuzione efficiente su hardware con risorse limitate senza sacrificare l‚Äôaccuratezza.\nUtilizzeremo tecniche come la quantizzazione e la potatura per ridurre il carico computazionale. Alla fine di questo tutorial, si avr√† un prototipo funzionante in grado di classificare le immagini in tempo reale, il tutto in esecuzione su un sistema embedded a basso consumo basato sulla scheda Arduino Nicla Vision.",
    "crumbs": [
      "Nicla Vision",
      "Classificazione delle Immagini"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/image_classification/image_classification.it.html#visione-artificiale",
    "href": "contents/labs/arduino/nicla_vision/image_classification/image_classification.it.html#visione-artificiale",
    "title": "Classificazione delle Immagini",
    "section": "Visione Artificiale",
    "text": "Visione Artificiale\nIn sostanza, la visione artificiale consente alle macchine di interpretare e prendere decisioni sulla base di dati visivi provenienti dal mondo esterno, imitando sostanzialmente la capacit√† del sistema ottico umano. Al contrario, l‚Äôintelligenza artificiale √® un campo pi√π ampio che comprende il machine learning [apprendimento automatico], elaborazione del linguaggio naturale e robotica, tra le altre tecnologie. Quando si introducono algoritmi di IA nei progetti di visione artificiale, si potenzia la capacit√† del sistema di comprendere, interpretare e reagire agli stimoli visivi.\nQuando si parla di progetti di visione artificiale applicati a dispositivi embedded, le applicazioni pi√π comuni che vengono in mente sono Classificazione delle Immagini e Rilevamento degli Oggetti.\n\nEntrambi i modelli possono essere implementati su dispositivi minuscoli come Arduino Nicla Vision e utilizzati in progetti reali. In questo capitolo, parleremo della classificazione delle immagini.",
    "crumbs": [
      "Nicla Vision",
      "Classificazione delle Immagini"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/image_classification/image_classification.it.html#obiettivo-del-progetto-di-classificazione-delle-immagini",
    "href": "contents/labs/arduino/nicla_vision/image_classification/image_classification.it.html#obiettivo-del-progetto-di-classificazione-delle-immagini",
    "title": "Classificazione delle Immagini",
    "section": "Obiettivo del Progetto di Classificazione delle Immagini",
    "text": "Obiettivo del Progetto di Classificazione delle Immagini\nIl primo passo in qualsiasi progetto ML √® definire l‚Äôobiettivo. In questo caso, √® rilevare e classificare due oggetti specifici presenti in un‚Äôimmagine. Per questo progetto, utilizzeremo due piccoli giocattoli: un robot e un piccolo pappagallo brasiliano (chiamato Periquito). Inoltre, raccoglieremo immagini di un background in cui quei due oggetti sono assenti.",
    "crumbs": [
      "Nicla Vision",
      "Classificazione delle Immagini"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/image_classification/image_classification.it.html#raccolta-dati",
    "href": "contents/labs/arduino/nicla_vision/image_classification/image_classification.it.html#raccolta-dati",
    "title": "Classificazione delle Immagini",
    "section": "Raccolta Dati",
    "text": "Raccolta Dati\nUna volta definito l‚Äôobiettivo del progetto di Machine Learning, il passaggio successivo e pi√π cruciale √® la raccolta del set di dati. Si pu√≤ utilizzare Edge Impulse Studio, l‚ÄôIDE OpenMV che abbiamo installato o persino il proprop telefono per l‚Äôacquisizione delle immagini. Qui, utilizzeremo l‚ÄôIDE OpenMV per questo.\n\nRaccolta del Dataset con OpenMV IDE\nPer prima cosa, si crea sul computer una cartella in cui verranno salvati i dati, ad esempio ‚Äúdata‚Äù. Quindi, su OpenMV IDE, si va in Tools &gt; Dataset Editor e si seleziona New Dataset per avviare la raccolta di dati:\n\nL‚ÄôIDE chieder√† di aprire il file in cui verranno salvati i dati e di scegliere la cartella ‚Äúdata‚Äù che √® stata creata. Notare che appariranno nuove icone sul pannello di sinistra.\n\nUtilizzando l‚Äôicona in alto (1), si inserisce il nome della prima classe, ad esempio ‚Äúperiquito‚Äù:\n\nEseguendo dataset_capture_script.py e cliccando sull‚Äôicona della fotocamera (2), inizier√† l‚Äôacquisizione delle immagini:\n\nRipetere la stessa procedura con le altre classi\n\n\nSuggeriamo circa 60 immagini da ogni categoria. Provare ad acquisire con diverse angolazioni, sfondi e condizioni di luce.\n\nLe immagini archiviate utilizzano una dimensione del fotogramma QVGA di 320x240 e RGB565 (formato pixel colore).\nDopo aver acquisito il dataset, si chiude il Tool Dataset Editor su Tools &gt; Dataset Editor.\nSul computer, si finir√† con un set di dati che contiene tre classi: periquito, robot e background.\n\nSi deve tornare a Edge Impulse Studio e caricare il set di dati nel progetto.",
    "crumbs": [
      "Nicla Vision",
      "Classificazione delle Immagini"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/image_classification/image_classification.it.html#addestramento-del-modello-con-edge-impulse-studio",
    "href": "contents/labs/arduino/nicla_vision/image_classification/image_classification.it.html#addestramento-del-modello-con-edge-impulse-studio",
    "title": "Classificazione delle Immagini",
    "section": "Addestramento del modello con Edge Impulse Studio",
    "text": "Addestramento del modello con Edge Impulse Studio\nUseremo Edge Impulse Studio per addestrare il nostro modello. Inserire le credenziali del proprio account e creare un nuovo progetto:\n\n\nQui si pu√≤ clonare un progetto simile: NICLA-Vision_Image_Classification.",
    "crumbs": [
      "Nicla Vision",
      "Classificazione delle Immagini"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/image_classification/image_classification.it.html#il-dataset",
    "href": "contents/labs/arduino/nicla_vision/image_classification/image_classification.it.html#il-dataset",
    "title": "Classificazione delle Immagini",
    "section": "Il Dataset",
    "text": "Il Dataset\nUtilizzando EI Studio (o Studio), esamineremo quattro passaggi principali per avere il nostro modello pronto per l‚Äôuso sulla scheda Nicla Vision: Dataset, Impulse, Tests e Deploy (su Edge Device, in questo caso, la NiclaV).\n\nPer quanto riguarda il Dataset, √® essenziale sottolineare che il nostro Dataset originale, acquisito con OpenMV IDE, sar√† suddiviso in Training, Validation e Test. Il Test Set sar√† suddiviso dall‚Äôinizio e una parte sar√† riservata per essere utilizzata solo nella fase di Test dopo l‚Äôaddestramento. Il Validation Set sar√† utilizzato durante l‚Äôaddestramento.\n\nSu Studio, nella scheda Data acquisition e nella sezione UPLOAD DATA, si caricano i file delle categorie scelte dal computer:\n\nLasciare in Studio la suddivisione del dataset originale in train and test e scegliere l‚Äôetichetta relativa a quei dati specifici:\n\nRipetere la procedura per tutte e tre le classi. Alla fine, si vedranno ‚Äúdati grezzi‚Äù in Studio:\n\nStudio consente di esplorare i dati, mostrando una vista completa di tutti i dati nel progetto. Si possono cancellare, ispezionare o modificare le etichette cliccando sui singoli elementi di dati. Nel nostro caso, un progetto molto semplice, i dati sembrano OK.",
    "crumbs": [
      "Nicla Vision",
      "Classificazione delle Immagini"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/image_classification/image_classification.it.html#impulse-design",
    "href": "contents/labs/arduino/nicla_vision/image_classification/image_classification.it.html#impulse-design",
    "title": "Classificazione delle Immagini",
    "section": "Impulse Design",
    "text": "Impulse Design\nIn questa fase, dovremmo definire come:\n\nPre-elaborare i nostri dati, il che consiste nel ridimensionare le singole immagini e determinare la profondit√† di colore da utilizzare (RGB o scala di grigi) e\nSpecificare un modello, in questo caso, sar√† il Transfer Learning (Images) per mettere a punto un modello di classificazione delle immagini MobileNet V2 pre-addestrato sui nostri dati. Questo metodo funziona bene anche con set di dati di immagini relativamente piccoli (circa 150 immagini nel nostro caso).\n\n\nTransfer Learning con MobileNet offre un approccio semplificato all‚Äôaddestramento del modello, che √® particolarmente utile per ambienti con risorse limitate e progetti con dati etichettati limitati. MobileNet, noto per la sua architettura leggera, √® un modello pre-addestrato che ha gi√† appreso funzionalit√† preziose da un ampio set di dati (ImageNet).\n\nSfruttando queste funzionalit√† apprese, si pu√≤ addestrare un nuovo modello per il compito specifico con meno dati e risorse computazionali e tuttavia ottenere una precisione competitiva.\n\nQuesto approccio riduce significativamente i tempi di addestramento e i costi computazionali, rendendolo ideale per la prototipazione rapida e l‚Äôimplementazione su dispositivi embedded in cui l‚Äôefficienza √® fondamentale.\nSi va alla scheda Impulse Design e si crea l‚Äôimpulse, definendo una dimensione dell‚Äôimmagine di 96x96 e schiacciandola (forma quadrata, senza ritaglio). Si seleziona Image e i blocchi Transfer Learning. Si salva l‚ÄôImpulse.\n\n\nPre-elaborazione delle Immagini\nTutte le immagini QVGA/RGB565 in ingresso verranno convertite in 27.640 feature (96x96x3).\n\nSi preme [Save parameters] e Generate all features:\n\n\n\nProgettazione del Modello\nNel 2007, Google ha introdotto MobileNetV1, una famiglia di reti neurali per la visione artificiale di uso generale progettate pensando ai dispositivi mobili per supportare la classificazione, il rilevamento e altro ancora. Le MobileNet sono modelli piccoli, a bassa latenza e a basso consumo, parametrizzati per soddisfare i vincoli di risorse di vari casi d‚Äôuso. Nel 2018, Google ha lanciato MobileNetV2: Inverted Residuals and Linear Bottlenecks.\nMobileNet V1 e MobileNet V2 mirano all‚Äôefficienza mobile e alle applicazioni di visione embedded, ma differiscono per complessit√† architettonica e prestazioni. Mentre entrambi utilizzano convoluzioni separabili in profondit√† per ridurre i costi computazionali, MobileNet V2 introduce Inverted Residual Blocks e Linear Bottlenecks per migliorare le prestazioni. Queste nuove funzionalit√† consentono a V2 di acquisire funzionalit√† pi√π complesse utilizzando meno parametri, rendendolo pi√π efficiente dal punto di vista computazionale e generalmente pi√π accurato rispetto al suo predecessore. Inoltre, V2 impiega un‚Äôattivazione non lineare nello layer di espansione intermedio. Utilizza ancora un‚Äôattivazione lineare per il layer del bottleneck [collo di bottiglia], una scelta di progettazione che si √® rivelata utile per preservare informazioni importanti attraverso la rete. MobileNet V2 offre un‚Äôarchitettura ottimizzata per una maggiore accuratezza ed efficienza e verr√† utilizzata in questo progetto.\nSebbene l‚Äôarchitettura di base di MobileNet sia gi√† minuscola e abbia una bassa latenza, molte volte, un caso d‚Äôuso o un‚Äôapplicazione specifica potrebbe richiedere che il modello sia ancora pi√π piccolo e veloce. MobileNets introduce un parametro semplice Œ± (alfa) chiamato moltiplicatore di larghezza per costruire questi modelli pi√π piccoli e meno costosi dal punto di vista computazionale. Il ruolo del moltiplicatore di larghezza Œ± √® quello di assottigliare una rete in modo uniforme a ogni layer.\nEdge Impulse Studio pu√≤ utilizzare sia MobileNetV1 (immagini 96x96) che V2 (immagini 96x96 o 160x160), con diversi valori di Œ± (da 0,05 a 1,0). Ad esempio, si otterr√† la massima accuratezza con V2, immagini 160x160 e Œ±=1,0. Naturalmente, c‚Äô√® un compromesso. Maggiore √® la precisione, pi√π memoria (circa 1,3 MB di RAM e 2,6 MB di ROM) sar√† necessaria per eseguire il modello, il che implica una maggiore latenza. L‚Äôingombro minore sar√† ottenuto all‚Äôaltro estremo con MobileNetV1 e Œ±=0,10 (circa 53,2 K di RAM e 101 K di ROM).\n\nPer questo progetto utilizzeremo MobileNetV2 96x96 0.1, con un costo di memoria stimato di 265,3 KB in RAM. Questo modello dovrebbe andare bene per Nicla Vision con 1 MB di SRAM. Nella scheda Transfer Learning, si seleziona questo modello:",
    "crumbs": [
      "Nicla Vision",
      "Classificazione delle Immagini"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/image_classification/image_classification.it.html#addestramento-del-modello",
    "href": "contents/labs/arduino/nicla_vision/image_classification/image_classification.it.html#addestramento-del-modello",
    "title": "Classificazione delle Immagini",
    "section": "Addestramento del Modello",
    "text": "Addestramento del Modello\nUn‚Äôaltra tecnica preziosa da utilizzare con il Deep Learning √® la Data Augmentation. Il ‚Äúdata augmentation‚Äù √® un metodo per migliorare l‚Äôaccuratezza dei modelli di apprendimento automatico mediante la creazione di dati artificiali aggiuntivi. Un sistema di Data Augmentation apporta piccole modifiche casuali ai dati di training (ad esempio capovolgendo, ritagliando o ruotando le immagini).\nGuardando ‚Äúsotto il cofano‚Äù, qui si pu√≤ vedere come Edge Impulse implementa una policy di Data Augmentation sui dati:\n# Implements the data augmentation policy\ndef augment_image(image, label):\n    # Flips the image randomly\n    image = tf.image.random_flip_left_right(image)\n\n    # Increase the image size, then randomly crop it down to\n    # the original dimensions\n    resize_factor = random.uniform(1, 1.2)\n    new_height = math.floor(resize_factor * INPUT_SHAPE[0])\n    new_width = math.floor(resize_factor * INPUT_SHAPE[1])\n    image = tf.image.resize_with_crop_or_pad(image, new_height, new_width)\n    image = tf.image.random_crop(image, size=INPUT_SHAPE)\n\n    # Vary the brightness of the image\n    image = tf.image.random_brightness(image, max_delta=0.2)\n\n    return image, label\nL‚Äôesposizione a queste variazioni durante l‚Äôaddestramento pu√≤ aiutare a impedire al modello di prendere scorciatoie ‚Äúmemorizzando‚Äù indizi superficiali nei dati di addestramento, il che significa che potrebbe riflettere meglio i pattern profondi in esame nel set di dati.\nL‚Äôultimo layer del nostro modello avr√† 12 neuroni con un dropout del 15% per prevenire l‚Äôoverfitting. Ecco il risultato del Training:\n\nIl risultato √® eccellente, con 77 ms di latenza, che dovrebbero tradursi in 13 fps (frame al secondo) durante l‚Äôinferenza.",
    "crumbs": [
      "Nicla Vision",
      "Classificazione delle Immagini"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/image_classification/image_classification.it.html#test-del-modello",
    "href": "contents/labs/arduino/nicla_vision/image_classification/image_classification.it.html#test-del-modello",
    "title": "Classificazione delle Immagini",
    "section": "Test del Modello",
    "text": "Test del Modello\n\nOra, si dovrebbe mettere da parte il set di dati all‚Äôinizio del progetto ed eseguire il modello addestrato usandolo come input:\n\nIl risultato √®, ancora una volta, eccellente.",
    "crumbs": [
      "Nicla Vision",
      "Classificazione delle Immagini"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/image_classification/image_classification.it.html#distribuzione-del-modello",
    "href": "contents/labs/arduino/nicla_vision/image_classification/image_classification.it.html#distribuzione-del-modello",
    "title": "Classificazione delle Immagini",
    "section": "Distribuzione del modello",
    "text": "Distribuzione del modello\nA questo punto, possiamo distribuire il modello addestrato come .tflite e usare l‚ÄôIDE OpenMV per eseguirlo usando MicroPython, oppure possiamo distribuirlo come C/C++ o libreria Arduino.\n\n\nLibreria Arduino\nPer prima cosa, distribuiamolo come Libreria Arduino:\n\nSi dovrebbe installare la libreria come .zip sull‚ÄôIDE Arduino ed eseguire lo sketch nicla_vision_camera.ino disponibile in Examples sotto il nome della libreria.\n\nNotare che Arduino Nicla Vision ha, per default, 512KB di RAM allocati per il core M7 e altri 244 KB sullo spazio di indirizzamento dell‚ÄôM4. Nel codice, questa allocazione √® stata modificata in 288 kB per garantire che il modello verr√† eseguito sul dispositivo (malloc_addblock((void*)0x30000000, 288 * 1024);).\n\nIl risultato √® buono, con 86 ms di latenza misurata.\n\nEcco un breve video che mostra i risultati dell‚Äôinferenza: \n\n\nOpenMV\n√à possibile distribuire il modello addestrato da utilizzare con OpenMV in due modi: come libreria e come firmware.\nCome libreria vengono generati tre file: il modello .tflite addestrato, un elenco con etichette e un semplice script MicroPython che pu√≤ effettuare inferenze utilizzando il modello.\n\nEseguire questo modello come .tflite direttamente in Nicla era impossibile. Quindi, possiamo sacrificare l‚Äôaccuratezza utilizzando un modello pi√π piccolo o distribuire il modello come Firmware OpenMV (FW). Scegliendo FW, Edge Impulse Studio genera modelli, librerie e framework ottimizzati necessari per effettuare l‚Äôinferenza. Esploriamo questa opzione.\nSelezionare OpenMV Firmware nella scheda Deploy e premere [Build].\n\nSul computer si trover√† un file ZIP. Lo si apre:\n\nSi usa il tool Bootloader sull‚ÄôIDE OpenMV per caricare il FW sulla board:\n\nSi seleziona il file appropriato (.bin per Nicla-Vision):\n\nDopo aver completato il download, si preme OK:\n\nSe un messaggio dice che il FW √® ‚Äúoutdated‚Äù [obsoleto], NON ESEGUIRE L‚ÄôAGGIORNAMENTO. Selezionare [NO].\n\nOra, si apre lo script ei_image_classification.py che √® stato scaricato da Studio e il file .bin per Nicla.\n\nEseguirlo. Puntando la telecamera sugli oggetti che vogliamo classificare, il risultato dell‚Äôinferenza verr√† visualizzato sul Serial Terminal.\n\n\nModifica del Codice per Aggiungere Etichette\nIl codice fornito da Edge Impulse pu√≤ essere modificato in modo da poter vedere, per motivi di test, il risultato dell‚Äôinferenza direttamente sull‚Äôimmagine visualizzata sull‚ÄôIDE OpenMV.\nCaricare il codice da GitHub o modificalo come di seguito:\n# Marcelo Rovai - NICLA Vision - Image Classification\n# Adapted from Edge Impulse - OpenMV Image Classification Example\n# @24Aug23\n\nimport sensor, image, time, os, tf, uos, gc\n\nsensor.reset()                         # Reset and initialize the sensor.\nsensor.set_pixformat(sensor.RGB565)    # Set pxl fmt to RGB565 (or GRAYSCALE)\nsensor.set_framesize(sensor.QVGA)      # Set frame size to QVGA (320x240)\nsensor.set_windowing((240, 240))       # Set 240x240 window.\nsensor.skip_frames(time=2000)          # Let the camera adjust.\n\nnet = None\nlabels = None\n\ntry:\n    # Load built in model\n    labels, net = tf.load_builtin_model('trained')\nexcept Exception as e:\n    raise Exception(e)\n\nclock = time.clock()\nwhile(True):\n    clock.tick()  # Starts tracking elapsed time.\n\n    img = sensor.snapshot()\n\n    # default settings just do one detection\n    for obj in net.classify(img, \n                            min_scale=1.0, \n                            scale_mul=0.8, \n                            x_overlap=0.5, \n                            y_overlap=0.5):\n        fps = clock.fps()\n        lat = clock.avg()\n\n        print(\"**********\\nPrediction:\")\n        img.draw_rectangle(obj.rect())\n        # This combines the labels and confidence values into a list of tuples\n        predictions_list = list(zip(labels, obj.output()))\n\n        max_val = predictions_list[0][1]\n        max_lbl = 'background'\n        for i in range(len(predictions_list)):\n            val = predictions_list[i][1]\n            lbl = predictions_list[i][0]\n\n            if val &gt; max_val:\n                max_val = val\n                max_lbl = lbl\n\n    # Print label with the highest probability\n    if max_val &lt; 0.5:\n        max_lbl = 'uncertain'\n    print(\"{} with a prob of {:.2f}\".format(max_lbl, max_val))\n    print(\"FPS: {:.2f} fps ==&gt; latency: {:.0f} ms\".format(fps, lat))\n\n    # Draw label with highest probability to image viewer\n    img.draw_string(\n        10, 10,\n        max_lbl + \"\\n{:.2f}\".format(max_val),\n        mono_space = False,\n        scale=2\n        )\nQui si pu√≤ vedere il risultato:\n\nNotare che la latenza (136 ms) √® quasi il doppio di quella che ottenuta direttamente con l‚ÄôIDE Arduino. Questo perch√© stiamo usando l‚ÄôIDE come interfaccia e anche il tempo di attesa per la fotocamera per essere pronta. Se avviamo il clock appena prima dell‚Äôinferenza:\n\nLa latenza scender√† a soli 71 ms.\n\n\nNiclaV funziona a circa la met√† della velocit√† quando √® connesso all‚ÄôIDE. Gli FPS dovrebbero aumentare una volta disconnessi.\n\n\n\nPost-elaborazione con i LED\nQuando lavoriamo con l‚Äôapprendimento automatico embedded, cerchiamo dispositivi che possano procedere continuamente con l‚Äôinferenza e il risultato, eseguendo un‚Äôazione direttamente sul mondo fisico e non visualizzando il risultato su un computer connesso. Per simulare ci√≤, accenderemo un LED diverso per ogni possibile risultato dell‚Äôinferenza.\n\nPer ottenere ci√≤, dovremmo caricare il codice da GitHub o modificare l‚Äôultimo codice per includere i LED:\n# Marcelo Rovai - NICLA Vision - Image Classification with LEDs\n# Adapted from Edge Impulse - OpenMV Image Classification Example\n# @24Aug23\n\nimport sensor, image, time, os, tf, uos, gc, pyb\n\nledRed = pyb.LED(1)\nledGre = pyb.LED(2)\nledBlu = pyb.LED(3)\n\nsensor.reset()                         # Reset and initialize the sensor.\nsensor.set_pixformat(sensor.RGB565)    # Set pixl fmt to RGB565 (or GRAYSCALE)\nsensor.set_framesize(sensor.QVGA)      # Set frame size to QVGA (320x240)\nsensor.set_windowing((240, 240))       # Set 240x240 window.\nsensor.skip_frames(time=2000)          # Let the camera adjust.\n\nnet = None\nlabels = None\n\nledRed.off()\nledGre.off()\nledBlu.off()\n\ntry:\n    # Load built in model\n    labels, net = tf.load_builtin_model('trained')\nexcept Exception as e:\n    raise Exception(e)\n\nclock = time.clock()\n\n\ndef setLEDs(max_lbl):\n\n    if max_lbl == 'uncertain':\n        ledRed.on()\n        ledGre.off()\n        ledBlu.off()\n\n    if max_lbl == 'periquito':\n        ledRed.off()\n        ledGre.on()\n        ledBlu.off()\n\n    if max_lbl == 'robot':\n        ledRed.off()\n        ledGre.off()\n        ledBlu.on()\n\n    if max_lbl == 'background':\n        ledRed.off()\n        ledGre.off()\n        ledBlu.off()\n\n\nwhile(True):\n    img = sensor.snapshot()\n    clock.tick()  # Starts tracking elapsed time.\n\n    # default settings just do one detection.\n    for obj in net.classify(img, \n                            min_scale=1.0, \n                            scale_mul=0.8, \n                            x_overlap=0.5, \n                            y_overlap=0.5):\n        fps = clock.fps()\n        lat = clock.avg()\n\n        print(\"**********\\nPrediction:\")\n        img.draw_rectangle(obj.rect())\n        # This combines the labels and confidence values into a list of tuples\n        predictions_list = list(zip(labels, obj.output()))\n\n        max_val = predictions_list[0][1]\n        max_lbl = 'background'\n        for i in range(len(predictions_list)):\n            val = predictions_list[i][1]\n            lbl = predictions_list[i][0]\n\n            if val &gt; max_val:\n                max_val = val\n                max_lbl = lbl\n\n    # Print label and turn on LED with the highest probability\n    if max_val &lt; 0.8:\n        max_lbl = 'uncertain'\n\n    setLEDs(max_lbl)\n\n    print(\"{} with a prob of {:.2f}\".format(max_lbl, max_val))\n    print(\"FPS: {:.2f} fps ==&gt; latency: {:.0f} ms\".format(fps, lat))\n\n    # Draw label with highest probability to image viewer\n    img.draw_string(\n        10, 10,\n        max_lbl + \"\\n{:.2f}\".format(max_val),\n        mono_space = False,\n        scale=2\n        )\nOra, ogni volta che una classe ottiene un risultato superiore a 0,8, il LED corrispondente si accender√†:\n\nLed Rosso 0n: incerto (nessuna classe supera 0,8)\nLed Green 0n: Periquito &gt; 0.8\nLed Blue 0n: Robot &gt; 0.8\nTutti i LED spenti: Sfondo &gt; 0.8\n\nEcco il risultato:\n\nPi√π in dettaglio",
    "crumbs": [
      "Nicla Vision",
      "Classificazione delle Immagini"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/image_classification/image_classification.it.html#classificazione-delle-immagini-benchmark-non-ufficiale",
    "href": "contents/labs/arduino/nicla_vision/image_classification/image_classification.it.html#classificazione-delle-immagini-benchmark-non-ufficiale",
    "title": "Classificazione delle Immagini",
    "section": "Classificazione delle immagini Benchmark (non ufficiale)",
    "text": "Classificazione delle immagini Benchmark (non ufficiale)\nDiverse schede di sviluppo possono essere utilizzate per l‚Äôapprendimento automatico embedded (TinyML) e le pi√π comuni per le applicazioni di Computer Vision (consumo energetico basso) sono ESP32 CAM, Seeed XIAO ESP32S3 Sense, Arduino Nicla Vison e Arduino Portenta.\n\nCogliendo l‚Äôoccasione, lo stesso modello addestrato √® stato distribuito su ESP-CAM, XIAO e Portenta (in questo caso, il modello √® stato addestrato di nuovo, utilizzando immagini in scala di grigi per essere compatibile con la sua fotocamera). Ecco il risultato, distribuendo i modelli come Libreria di Arduino:",
    "crumbs": [
      "Nicla Vision",
      "Classificazione delle Immagini"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/image_classification/image_classification.it.html#conclusione",
    "href": "contents/labs/arduino/nicla_vision/image_classification/image_classification.it.html#conclusione",
    "title": "Classificazione delle Immagini",
    "section": "Conclusione",
    "text": "Conclusione\nPrima di finire, si tenga presente che la Computer Vision √® pi√π di una semplice classificazione delle immagini. Ad esempio, si possono sviluppare progetti Edge Machine Learning sulla visione in diverse aree, come:\n\nVeicoli Autonomi: Usa un gruppo di sensori, i dati lidar e gli algoritmi di visione artificiale per navigare e prendere decisioni.\nSanit√†: Diagnosi automatizzata di malattie tramite analisi delle immagini di risonanza magnetica, raggi X e TAC\nVendita al Dettaglio: Sistemi di pagamento automatizzati che identificano i prodotti mentre passano attraverso uno scanner.\nSicurezza e Sorveglianza: Riconoscimento facciale, rilevamento di anomalie e tracciamento di oggetti in video in tempo reale.\nRealt√† Aumentata: Rilevamento e classificazione di oggetti per sovrapporre informazioni digitali al mondo reale.\nAutomazione Industriale: Ispezione visiva di prodotti, manutenzione predittiva e guida di robot e droni.\nAgricoltura; Monitoraggio delle colture basato su droni e raccolta automatizzata.\nElaborazione del Linguaggio Naturale: Didascalie delle immagini e risposte visive alle domande.\nRiconoscimento dei Gesti: Per giochi, traduzione del linguaggio dei segni e interazione uomo-macchina.\nRaccomandazione dei Contenuti: Sistemi di raccomandazione basati sulle immagini nell‚Äôe-commerce.",
    "crumbs": [
      "Nicla Vision",
      "Classificazione delle Immagini"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/image_classification/image_classification.it.html#risorse",
    "href": "contents/labs/arduino/nicla_vision/image_classification/image_classification.it.html#risorse",
    "title": "Classificazione delle Immagini",
    "section": "Risorse",
    "text": "Risorse\n\nCodici Micropython\nDataset\nProgetto Edge Impulse",
    "crumbs": [
      "Nicla Vision",
      "Classificazione delle Immagini"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/object_detection/object_detection.it.html",
    "href": "contents/labs/arduino/nicla_vision/object_detection/object_detection.it.html",
    "title": "Rilevamento degli Oggetti",
    "section": "",
    "text": "Panoramica\nQuesto √® il seguito di CV su Nicla Vision, che ora esplora l‚ÄôObject Detection sui microcontrollori.",
    "crumbs": [
      "Nicla Vision",
      "Rilevamento degli Oggetti"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/object_detection/object_detection.it.html#panoramica",
    "href": "contents/labs/arduino/nicla_vision/object_detection/object_detection.it.html#panoramica",
    "title": "Rilevamento degli Oggetti",
    "section": "",
    "text": "Object Detection e Image Classification\nIl compito principale dei modelli di Image Classification [classificazione delle immagini] √® quello di produrre un elenco delle categorie di oggetti pi√π probabili presenti in un‚Äôimmagine, ad esempio, per identificare un gatto soriano subito dopo cena:\n\nMa cosa succede quando il gatto salta vicino al bicchiere di vino? Il modello riconosce ancora solo la categoria predominante nell‚Äôimmagine, il gatto soriano:\n\nE cosa succede se non c‚Äô√® una categoria dominante nell‚Äôimmagine?\n\nIl modello identifica l‚Äôimmagine soprastante in modo completamente sbagliato come un ‚Äúashcan‚Äù [?pattumiera?], probabilmente a causa delle tonalit√† di colore.\n\nIl modello utilizzato in tutti gli esempi precedenti √® MobileNet, addestrato con un ampio set di dati, la ImageNet.\n\nPer risolvere questo problema, abbiamo bisogno di un altro tipo di modello, in cui non solo possono essere trovate pi√π categorie (o etichette), ma anche dove si trovano gli oggetti in una determinata immagine.\nCome possiamo immaginare, tali modelli sono molto pi√π complicati e pi√π grandi, ad esempio, MobileNetV2 SSD FPN-Lite 320x320, addestrato con il set di dati COCO. Questo modello di rilevamento degli oggetti pre-addestrato √® progettato per individuare fino a 10 oggetti all‚Äôinterno di un‚Äôimmagine, generando un riquadro di delimitazione per ogni oggetto rilevato. L‚Äôimmagine sottostante √® il risultato di un tale modello in esecuzione su un Raspberry Pi:\n\nQuei modelli utilizzati per il rilevamento degli oggetti (come MobileNet SSD o YOLO) hanno solitamente dimensioni di diversi MB, il che √® OK per l‚Äôuso con Raspberry Pi ma non adatto per l‚Äôuso con dispositivi embedded, dove la RAM solitamente √® inferiore a 1 M Byte.\n\n\nUna soluzione innovativa per il rilevamento degli oggetti: FOMO\nEdge Impulse ha lanciato nel 2022 FOMO (Faster Objects, More Objects), una nuova soluzione per eseguire il rilevamento di oggetti su dispositivi embedded, non solo su Nicla Vision (Cortex M7) ma anche su CPU Cortex M4F (serie Arduino Nano33 e OpenMV M4) e sui dispositivi Espressif ESP32 (ESP-CAM e XIAO ESP32S3 Sense).\nIn questo esercizio pratico, esploreremo l‚Äôuso di FOMO con Object Detection, senza entrare in molti dettagli sul modello stesso. Per saperne di pi√π su come funziona il modello, si pu√≤ esaminare l‚Äôannuncio ufficiale FOMO di Edge Impulse, dove Louis Moreau e Mat Kelcey spiegano in dettaglio come funziona.",
    "crumbs": [
      "Nicla Vision",
      "Rilevamento degli Oggetti"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/object_detection/object_detection.it.html#obiettivo-del-progetto-di-object-detection",
    "href": "contents/labs/arduino/nicla_vision/object_detection/object_detection.it.html#obiettivo-del-progetto-di-object-detection",
    "title": "Rilevamento degli Oggetti",
    "section": "Obiettivo del Progetto di Object Detection",
    "text": "Obiettivo del Progetto di Object Detection\nTutti i progetti di apprendimento automatico devono iniziare con un obiettivo dettagliato. Supponiamo di trovarci in una struttura industriale e di dover ordinare e contare ruote e scatole speciali.\n\nIn altre parole, dovremmo eseguire una classificazione multi-etichetta, in cui ogni immagine pu√≤ avere tre classi:\n\nBackground [Sfondo] (nessun oggetto)\nBox [Scatola]\nWheel [Ruota]\n\nEcco alcuni campioni di immagini non etichettate che dovremmo usare per rilevare gli oggetti (ruote e scatole):\n\nSiamo interessati a quale oggetto √® presente nell‚Äôimmagine, alla sua posizione (centroide) e a quanti ne possiamo trovare su di essa. La dimensione dell‚Äôoggetto non viene rilevata con FOMO, come con MobileNet SSD o YOLO, in cui il Bounding Box √® uno degli output del modello.\nSvilupperemo il progetto utilizzando Nicla Vision per l‚Äôacquisizione di immagini e l‚Äôinferenza del modello. Il progetto ML verr√† sviluppato utilizzando Edge Impulse Studio. Ma prima di iniziare il progetto di ‚Äúobject detection‚Äù in Studio, creiamo un dataset grezzo (non etichettato) con immagini che contengono gli oggetti da rilevare.",
    "crumbs": [
      "Nicla Vision",
      "Rilevamento degli Oggetti"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/object_detection/object_detection.it.html#raccolta-dati",
    "href": "contents/labs/arduino/nicla_vision/object_detection/object_detection.it.html#raccolta-dati",
    "title": "Rilevamento degli Oggetti",
    "section": "Raccolta Dati",
    "text": "Raccolta Dati\nPossiamo utilizzare Edge Impulse Studio, OpenMV IDE, il telefono o altri dispositivi per l‚Äôacquisizione delle immagini. Qui, utilizzeremo di nuovo OpenMV IDE per il nostro scopo.\n\nRaccolta del Dataset con OpenMV IDE\nPer prima cosa, si crea sul computer una cartella in cui verranno salvati i dati, ad esempio ‚Äúdata‚Äù. Quindi, su OpenMV IDE, si va in ‚ÄúTools &gt; Dataset Editor‚Äù e si seleziona ‚ÄúNew Dataset‚Äù per avviare la raccolta di dati:\n\nEdge impulse suggerisce che gli oggetti dovrebbero essere di dimensioni simili e non sovrapposti per prestazioni migliori. Questo va bene in una struttura industriale, dove la telecamera dovrebbe essere fissa, mantenendo la stessa distanza dagli oggetti da rilevare. Nonostante ci√≤, proveremo anche con dimensioni e posizioni miste per vedere il risultato.\n\nNon creeremo cartelle separate per le nostre immagini perch√© ciascuna contiene pi√π etichette.\n\nSi collega Nicla Vision a OpenMV IDE e si esegue dataset_capture_script.py. Cliccando sul pulsante ‚ÄúCapture Image‚Äù inizier√† l‚Äôacquisizione delle immagini:\n\nSuggeriamo circa 50 immagini che mescolano gli oggetti e variano il numero di ciascuno che appare sulla scena. Provare ad acquisire con diverse angolazioni, sfondi e condizioni di luce.\n\nLe immagini memorizzate utilizzano una dimensione del fotogramma QVGA 320x240 e RGB565 (formato pixel a colori).\n\nDopo aver acquisito il dataset, si chiude il Tool Dataset Editor su Tools &gt; Dataset Editor.",
    "crumbs": [
      "Nicla Vision",
      "Rilevamento degli Oggetti"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/object_detection/object_detection.it.html#edge-impulse-studio",
    "href": "contents/labs/arduino/nicla_vision/object_detection/object_detection.it.html#edge-impulse-studio",
    "title": "Rilevamento degli Oggetti",
    "section": "Edge Impulse Studio",
    "text": "Edge Impulse Studio\n\nSetup del progetto\nSi va su Edge Impulse Studio, si inseriscono le proprie credenziali in Login (o si crea un account) e si avvia un nuovo progetto.\n\n\nQui si pu√≤ clonare il progetto sviluppato per questa esercitazione pratica: NICLA_Vision_Object_Detection.\n\nNella ‚ÄúProject Dashboard‚Äù, si va in basso e su Project info e si seleziona Bounding boxes (object detection) e Nicla Vision come ‚ÄúTarget Device‚Äù:\n\n\n\nCaricamento dei dati non etichettati\nSu Studio, si va alla scheda Data acquisition e nella sezione UPLOAD DATA, si caricano dal computer i file acquisiti.\n\n\nSi pu√≤ lasciare che Studio divida automaticamente i dati tra ‚ÄúTrain‚Äù e ‚ÄúTest‚Äù o farlo manualmente.\n\n\nTutte le immagini non etichettate (51) sono state caricate, ma devono comunque essere etichettate in modo appropriato prima di utilizzarle come set di dati nel progetto. Lo Studio ha uno strumento per questo scopo, che si trova al link Labeling queue (51).\nCi sono due modi per eseguire l‚Äôetichettatura assistita dall‚ÄôIA su Edge Impulse Studio (versione gratuita):\n\nUtilizzando yolov5\nTracciando di oggetti tra i frame\n\n\nEdge Impulse ha lanciato una funzione di auto-labeling per i clienti Enterprise, semplificando le attivit√† di etichettatura nei progetti di rilevamento degli oggetti.\n\nGli oggetti ordinari possono essere rapidamente identificati ed etichettati utilizzando una libreria esistente di modelli di rilevamento degli oggetti pre-addestrati da YOLOv5 (addestrati con il set di dati COCO). Ma poich√©, nel nostro caso, gli oggetti non fanno parte dei set di dati COCO, dovremmo selezionare l‚Äôopzione di tracking objects. Con questa opzione, una volta disegnati i riquadri di delimitazione ed etichettate le immagini in un frame, gli oggetti verranno tracciati automaticamente da un frame all‚Äôaltro, etichettando parzialmente quelli nuovi (non tutti sono etichettati correttamente).\n\nSi pu√≤ usare EI uploader per importare i dati se si ha gi√† un dataset etichettato contenente dei ‚Äúbounding box‚Äù.\n\n\n\nEtichettatura del Dataset\nIniziando dalla prima immagine dei dati non etichettati, si usa il mouse per trascinare una casella attorno a un oggetto per aggiungere un‚Äôetichetta. Poi si clicca su Save labels per passare all‚Äôelemento successivo.\n\nSi continua con questo processo finch√© la coda non √® vuota. Alla fine, tutte le immagini dovrebbero avere gli oggetti etichettati come i campioni sottostanti:\n\nPoi, si esaminano i campioni etichettati nella scheda Data acquisition. Se un‚Äôetichetta √® sbagliata, la si pu√≤ modificare usando il men√π three dots dopo il nome del campione:\n\nSi verr√† guidati a sostituire l‚Äôetichetta sbagliata, correggendo il dataset.",
    "crumbs": [
      "Nicla Vision",
      "Rilevamento degli Oggetti"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/object_detection/object_detection.it.html#impulse-design",
    "href": "contents/labs/arduino/nicla_vision/object_detection/object_detection.it.html#impulse-design",
    "title": "Rilevamento degli Oggetti",
    "section": "Impulse Design",
    "text": "Impulse Design\nIn questa fase, si deve definire come:\n\nIl Pre-processing consiste nel ridimensionare le singole immagini da 320 x 240 a 96 x 96 e nel comprimerle (forma quadrata, senza ritaglio). Successivamente, le immagini vengono convertite da RGB a scala di grigi.\nDesign a Model, in questo caso, ‚ÄúObject Detection‚Äù.\n\n\n\nPre-elaborazione di tutti i dataset\nIn questa sezione, si seleziona Color depth come Grayscale, che √® adatta per l‚Äôuso con modelli FOMO e Save parameters.\n\nLo Studio passa automaticamente alla sezione successiva, Generate features, dove tutti i campioni saranno pre-elaborati, con conseguente set di dati con singole immagini 96x96x1 o 9.216 ‚Äúfeature‚Äù.\n\nL‚Äôesploratore di feature mostra che tutti i campioni evidenziano una buona separazione dopo la generazione delle feature.\n\nUno dei campioni (46) apparentemente si trova nello spazio sbagliato, ma cliccandoci sopra si pu√≤ confermare che l‚Äôetichettatura √® corretta.",
    "crumbs": [
      "Nicla Vision",
      "Rilevamento degli Oggetti"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/object_detection/object_detection.it.html#progettazione-addestramento-e-test-del-modello",
    "href": "contents/labs/arduino/nicla_vision/object_detection/object_detection.it.html#progettazione-addestramento-e-test-del-modello",
    "title": "Rilevamento degli Oggetti",
    "section": "Progettazione, Addestramento e Test del Modello",
    "text": "Progettazione, Addestramento e Test del Modello\nUseremo FOMO, un modello di rilevamento degli oggetti basato su MobileNetV2 (alpha 0.35) progettato per segmentare grossolanamente un‚Äôimmagine in una griglia di background rispetto a oggetti di interesse (in questo caso, scatole e ruote).\nFOMO √® un modello di apprendimento automatico innovativo per il rilevamento degli oggetti, che pu√≤ utilizzare fino a 30 volte meno energia e memoria rispetto ai modelli tradizionali come Mobilenet SSD e YOLOv5. FOMO pu√≤ funzionare su microcontrollori con meno di 200 KB di RAM. Il motivo principale per cui ci√≤ √® possibile √® che mentre altri modelli calcolano le dimensioni dell‚Äôoggetto disegnando un quadrato attorno ad esso (bounding box), FOMO ignora le dimensioni dell‚Äôimmagine, fornendo solo le informazioni su dove si trova l‚Äôoggetto nell‚Äôimmagine, tramite le coordinate del centroide.\nCome funziona FOMO?\nFOMO prende l‚Äôimmagine in scala di grigi e la divide in blocchi di pixel usando un fattore di 8. Per l‚Äôinput di 96x96, la griglia √® 12x12 (96/8=12). Successivamente, FOMO eseguir√† un classificatore attraverso ogni blocco di pixel per calcolare la probabilit√† che ci sia una scatola o una ruota in ognuno di essi e, successivamente, determiner√† le regioni che hanno la pi√π alta probabilit√† di contenere l‚Äôoggetto (se un blocco di pixel non ha oggetti, verr√† classificato come background). Dalla sovrapposizione della regione finale, FOMO fornisce le coordinate (relative alle dimensioni dell‚Äôimmagine) del centroide di questa regione.\n\nPer l‚Äôaddestramento, dovremmo selezionare un modello pre-addestrato. Usiamo FOMO (Faster Objects, More Objects) MobileNetV2 0.35. Questo modello utilizza circa 250 KB di RAM e 80 KB di ROM (Flash), che si adatta bene alla nostra scheda poich√© ha 1 MB di RAM e ROM.\n\nPer quanto riguarda gli iperparametri di training, il modello verr√† addestrato con:\n\nEpochs: 60,\nBatch size: 32\nLearning Rate: 0.001.\n\nPer la convalida durante l‚Äôaddestramento, il 20% del set di dati (validation_dataset) verr√† risparmiato. Per il restante 80% (train_dataset), applicheremo il ‚ÄúData Augmentation‚Äù, che capovolger√† casualmente, cambier√† le dimensioni e la luminosit√† dell‚Äôimmagine e le ritaglier√†, aumentando artificialmente il numero di campioni sul set di dati per l‚Äôaddestramento.\nDi conseguenza, il modello termina con praticamente 1,00 nel punteggio F1, con un risultato simile quando si utilizzano i dati di test.\n\nNotare che FOMO ha aggiunto automaticamente una terza etichetta di background [sfondo] ai due precedentemente definiti (box e wheel).\n\n\n\nNelle attivit√† di rilevamento degli oggetti, l‚Äôaccuratezza non √® generalmente la evaluation metric primaria. Il rilevamento degli oggetti comporta la classificazione degli oggetti e la fornitura di riquadri di delimitazione attorno a essi, il che lo rende un problema pi√π complesso della semplice classificazione. Il problema √® che non abbiamo il riquadro di delimitazione, solo i centroidi. In breve, usare l‚Äôaccuratezza come metrica potrebbe essere fuorviante e potrebbe non fornire una comprensione completa delle prestazioni del modello. Per questo motivo, useremo il punteggio F1.\n\n\nModello di test con ‚ÄúLive Classification‚Äù\nDato che Edge Impulse supporta ufficialmente Nicla Vision, colleghiamolo allo Studio. Per farlo, si seguono i passaggi:\n\nSi effettua il download dell‚Äôlast EI Firmware e lo si decomprime.\nSi apre il file zip sul computer e si seleziona l‚Äôuploader relativo al proprio sistema operativo:\n\n\n\nMettere Nicla-Vision in ‚ÄúBoot Mode‚Äù, premendo due volte il pulsante di reset.\nEseguire il codice batch specifico per il sistema operativo per caricare il binario (arduino-nicla-vision.bin) sulla board.\n\nSi va nella sezione Live classification su EI Studio e, tramite webUSB, si connette la Nicla Vision:\n\nUna volta connessa, si pu√≤ usare la Nicla per catturare immagini reali da testare col modello addestrato su Edge Impulse Studio.\n\nUna cosa da notare √® che il modello pu√≤ produrre falsi positivi e falsi negativi. Questo pu√≤ essere ridotto al minimo definendo una Confidence Threshold [Soglia di confidenza] (si usa il men√π Three dots [tre-punti] per la configurazione). Provare con 0,8 o pi√π.",
    "crumbs": [
      "Nicla Vision",
      "Rilevamento degli Oggetti"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/object_detection/object_detection.it.html#distribuzione-del-modello",
    "href": "contents/labs/arduino/nicla_vision/object_detection/object_detection.it.html#distribuzione-del-modello",
    "title": "Rilevamento degli Oggetti",
    "section": "Distribuzione del Modello",
    "text": "Distribuzione del Modello\nSelezionare OpenMV Firmware nella scheda Deploy e premere [Build].\n\nQuando si connette di nuovo Nicla con OpenMV IDE, prover√† ad aggiornarne il FW. Scegliere invece l‚Äôopzione Load a specific firmware.\n\nSi trover√† un file ZIP sul computer dallo Studio. Lo si apre:\n\nCaricare il file .bin sulla board:\n\nUna volta terminato il download, verr√† visualizzato un messaggio pop-up. Premere OK e aprire lo script ei_object_detection.py scaricato da Studio.\nPrima di eseguire lo script, modifichiamo alcune righe. Notare che si pu√≤ lasciare la definizione della finestra come 240 x 240 e la telecamera che cattura le immagini come QVGA/RGB. L‚Äôimmagine catturata verr√† pre-elaborata dal FW distribuito da Edge Impulse\n# Edge Impulse - OpenMV Object Detection Example\n\nimport sensor, image, time, os, tf, math, uos, gc\n\nsensor.reset()                         # Reset and initialize the sensor.\nsensor.set_pixformat(sensor.RGB565)    # Set pixel format to RGB565 (or GRAYSCALE)\nsensor.set_framesize(sensor.QVGA)      # Set frame size to QVGA (320x240)\nsensor.set_windowing((240, 240))       # Set 240x240 window.\nsensor.skip_frames(time=2000)          # Let the camera adjust.\n\nnet = None\nlabels = None\nRidefinire la confidenza minima, ad esempio, a 0,8 per ridurre al minimo i falsi positivi e negativi.\nmin_confidence = 0.8\nSe necessario, modificare il colore dei cerchi che saranno utilizzati per visualizzare il centroide dell‚Äôoggetto rilevato per un contrasto migliore.\ntry:\n    # Load built in model\n    labels, net = tf.load_builtin_model('trained')\nexcept Exception as e:\n    raise Exception(e)\n\ncolors = [ # Add more colors if you are detecting more than 7 types of classes at once.\n    (255, 255,   0), # background: yellow (not used)\n    (  0, 255,   0), # cube: green\n    (255,   0,   0), # wheel: red\n    (  0,   0, 255), # not used\n    (255,   0, 255), # not used\n    (  0, 255, 255), # not used\n    (255, 255, 255), # not used\n]\nMantenere il codice restante cos√¨ com‚Äô√® e premere il pulsante Play verde per eseguire il codice:\n\nNella vista della telecamera, possiamo vedere gli oggetti con i loro centroidi contrassegnati con 12 cerchi pixel-fixed (ogni cerchio ha un colore distinto, a seconda della sua classe). Sul terminale seriale, il modello mostra le etichette rilevate e la loro posizione sulla finestra dell‚Äôimmagine (240X240).\n\nAttenzione l‚Äôorigine delle coordinate √® nell‚Äôangolo in alto a sinistra.\n\n\nNotare che la frequenza dei fotogrammi al secondo √® di circa 8 fps (simile a quella ottenuta con il progetto Image Classification). Ci√≤ accade perch√© FOMO √® intelligentemente costruito su un modello CNN, non con un modello di rilevamento degli oggetti come SSD MobileNet. Ad esempio, quando si esegue un modello MobileNetV2 SSD FPN-Lite 320x320 su un Raspberry Pi 4, la latenza √® circa 5 volte superiore (circa 1,5 fps)\nEcco un breve video che mostra i risultati dell‚Äôinferenza:",
    "crumbs": [
      "Nicla Vision",
      "Rilevamento degli Oggetti"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/object_detection/object_detection.it.html#conclusione",
    "href": "contents/labs/arduino/nicla_vision/object_detection/object_detection.it.html#conclusione",
    "title": "Rilevamento degli Oggetti",
    "section": "Conclusione",
    "text": "Conclusione\nFOMO √® un salto significativo nello spazio di elaborazione delle immagini, come hanno affermato Louis Moreau e Mat Kelcey durante il suo lancio nel 2022:\n\nFOMO √® un algoritmo rivoluzionario che porta per la prima volta il rilevamento, il tracciamento e il conteggio degli oggetti in tempo reale sui microcontrollori.\n\nEsistono molteplici possibilit√† per esplorare il rilevamento degli oggetti (e, pi√π precisamente, il loro conteggio) su dispositivi embedded, ad esempio, per esplorare la il raggruppamento di sensori (telecamera + microfono) e il rilevamento degli oggetti di Nicla. Questo pu√≤ essere molto utile nei progetti che coinvolgono le api, ad esempio.",
    "crumbs": [
      "Nicla Vision",
      "Rilevamento degli Oggetti"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/object_detection/object_detection.it.html#risorse",
    "href": "contents/labs/arduino/nicla_vision/object_detection/object_detection.it.html#risorse",
    "title": "Rilevamento degli Oggetti",
    "section": "Risorse",
    "text": "Risorse\n\nProgetto Edge Impulse",
    "crumbs": [
      "Nicla Vision",
      "Rilevamento degli Oggetti"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/kws/kws.it.html",
    "href": "contents/labs/arduino/nicla_vision/kws/kws.it.html",
    "title": "Keyword Spotting (KWS)",
    "section": "",
    "text": "Panoramica\nDopo aver gi√† esplorato la scheda Nicla Vision nelle applicazioni di Image Classification e Object Detection, stiamo ora spostando la nostra attenzione sulle applicazioni attivate tramite comando vocale con un progetto su Keyword Spotting (KWS).\nCome introdotto nel tutorial Feature Engineering for Audio Classification, il Keyword Spotting (KWS) √® integrato in molti sistemi di riconoscimento vocale, consentendo ai dispositivi di rispondere a parole o frasi specifiche. Sebbene questa tecnologia sia alla base di dispositivi popolari come Google Assistant o Amazon Alexa, √® ugualmente applicabile e fattibile su dispositivi pi√π piccoli e a basso consumo. Questo tutorial guider√† nell‚Äôimplementazione di un sistema KWS utilizzando TinyML sulla scheda di sviluppo Nicla Vision dotata di un microfono digitale.\nIl nostro modello sar√† progettato per riconoscere parole chiave che possono riattivare un dispositivo o azioni specifiche, dando loro vita con comandi vocali.",
    "crumbs": [
      "Nicla Vision",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/kws/kws.it.html#come-funziona-un-assistente-vocale",
    "href": "contents/labs/arduino/nicla_vision/kws/kws.it.html#come-funziona-un-assistente-vocale",
    "title": "Keyword Spotting (KWS)",
    "section": "Come funziona un assistente vocale?",
    "text": "Come funziona un assistente vocale?\nCome detto, gli assistenti vocali sul mercato, come Google Home o Amazon Echo-Dot, reagiscono agli umani solo quando vengono ‚Äúsvegliati‚Äù da parole chiave particolari come ‚ÄúHey Google‚Äù sul primo e ‚ÄúAlexa‚Äù sul secondo.\n\nIn altre parole, il riconoscimento dei comandi vocali si basa su un modello multi-fase o Cascade Detection.\n\nFase 1: Un piccolo microprocessore all‚Äôinterno di Echo Dot o Google Home ascolta continuamente, in attesa che venga individuata la parola chiave, utilizzando un modello TinyML nel device (applicazione KWS).\nFase 2: Solo quando vengono attivati dall‚Äôapplicazione KWS nella Fase 1, i dati vengono inviati al cloud ed elaborati su un modello pi√π grande.\nIl video qui sotto mostra un esempio di un Google Assistant programmato su un Raspberry Pi (Fase 2), con un Arduino Nano 33 BLE come dispositivo TinyML (Fase 1).\n\n\nPer esplorare il progetto Google Assistant di cui sopra, consultare il tutorial: Building an Intelligent Voice Assistant From Scratch.\n\nIn questo progetto KWS, ci concentreremo sulla Fase 1 (KWS o Keyword Spotting), dove utilizzeremo Nicla Vision, che ha un microfono digitale che verr√† utilizzato per individuare la parola chiave.",
    "crumbs": [
      "Nicla Vision",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/kws/kws.it.html#il-progetto-pratico-kws",
    "href": "contents/labs/arduino/nicla_vision/kws/kws.it.html#il-progetto-pratico-kws",
    "title": "Keyword Spotting (KWS)",
    "section": "Il Progetto Pratico KWS",
    "text": "Il Progetto Pratico KWS\nIl diagramma seguente fornisce un‚Äôidea di come dovrebbe funzionare l‚Äôapplicazione KWS finale (durante l‚Äôinferenza):\n\nLa nostra applicazione KWS riconoscer√† quattro classi di suono:\n\nYES (Keyword 1)\nNO (Keyword 2)\nNOISE (nessuna parola pronunciata; √® presente solo rumore di fondo)\nUNKNOW (un mix di parole diverse da YES e NO)\n\n\nPer progetti reali, √® sempre consigliabile includere altri suoni oltre alle parole chiave, come ‚ÄúNoise‚Äù (o Background) e ‚ÄúUnknown‚Äù.\n\n\nIl Flusso di Lavoro del Machine Learning\nIl componente principale dell‚Äôapplicazione KWS √® il suo modello. Quindi, dobbiamo addestrare un modello del genere con le nostre parole chiave specifiche, rumore e altre parole (lo ‚Äúunknown‚Äù):",
    "crumbs": [
      "Nicla Vision",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/kws/kws.it.html#il-dataset",
    "href": "contents/labs/arduino/nicla_vision/kws/kws.it.html#il-dataset",
    "title": "Keyword Spotting (KWS)",
    "section": "Il Dataset",
    "text": "Il Dataset\nIl componente critico di qualsiasi flusso di lavoro di apprendimento automatico √® il dataset. Una volta decise le parole chiave specifiche, nel nostro caso (YES e NO), possiamo sfruttare il dataset sviluppato da Pete Warden, ‚ÄúSpeech Commands: A Dataset for Limited-Vocabulary Speech Recognition‚Äù. Questo dataset ha 35 parole chiave (con +1.000 campioni ciascuna), come yes, no, stop e go. In parole come yes e no, possiamo ottenere 1.500 campioni.\nSi pu√≤ scaricare una piccola parte del dataset da Edge Studio (Keyword spotting pre-built dataset), che include campioni dalle quattro classi che utilizzeremo in questo progetto: yes, no, noise e background. Per farlo, si seguono i passaggi seguenti:\n\nDownload del dataset delle parole chiave.\nUnzip del file in una posizione a scelta.\n\n\nCaricamento del set di dati su Edge Impulse Studio\nSi avvia un nuovo progetto su Edge Impulse Studio (EIS) e si seleziona il tool Upload Existing Data nella sezione Data Acquisition. Si scelgono i file da caricare:\n\nSi definisce l‚Äôetichetta, si seleziona Automatically split between train and test, e poi Upload data su EIS. Si ripete per tutte le classi.\n\nIl dataset apparir√† ora nella sezione Data acquisition. Notare che i circa 6.000 campioni (1.500 per ogni classe) sono suddivisi in set di Train (4.800) e di Test (1.200).\n\n\n\nAcquisizione di Dati Audio Aggiuntivi\nSebbene disponiamo di molti dati dal dataset di Pete, √® consigliabile raccogliere alcune parole pronunciate da noi. Lavorando con gli accelerometri, √® essenziale creare un set di dati con dati acquisiti dallo stesso tipo di sensore. Nel caso del suono, questo √® facoltativo perch√© ci√≤ che classificheremo sono, in realt√†, dati audio.\n\nLa differenza fondamentale tra suono e audio √® il tipo di energia. Il suono √® una perturbazione meccanica (onde sonore longitudinali) che si propagano attraverso un mezzo, causando variazioni di pressione in esso. L‚Äôaudio √® un segnale elettrico (analogico o digitale) che rappresenta il suono.\n\nQuando pronunciamo una parola chiave, le onde sonore devono essere convertite in dati audio. La conversione deve essere eseguita campionando il segnale generato dal microfono a una frequenza di 16 KHz con ampiezza di 16 bit per campione.\nQuindi, qualsiasi dispositivo in grado di generare dati audio con questa specifica di base (16 KHz/16 bit) funzioner√† correttamente. Come device, possiamo usare NiclaV, un computer o persino il cellulare.\n\n\nUtilizzo di NiclaV ed Edge Impulse Studio\nCome abbiamo appreso nel capitolo Configurazione di Nicla Vision, EIS supporta ufficialmente Nicla Vision, semplificando l‚Äôacquisizione dei dati dai suoi sensori, incluso il microfono. Quindi, si crea un nuovo progetto su EIS e vi si collega la Nicla, seguendo questi passaggi:\n\nDownload del Firmware EIS pi√π aggiornato e lo si decomprime.\nAprire il file zip sul computer e selezionare l‚Äôuploader corrispondente al sistema operativo:\n\n\n\nSi mette NiclaV in Boot Mode premendo due volte il pulsante di reset.\nSi carica il binario arduino-nicla-vision.bin sulla board eseguendo il codice batch corrispondente al sistema operativo.\n\nSi va sul proprio progetto EIS e nella scheda Data Acquisition tab, si seleziona WebUSB. Apparir√† una finestra; scegliere l‚Äôopzione che mostra che Nicla is paired e si preme [Connect].\nSi possono scegliere quali dati del sensore raccogliere nella sezione Collect Data nella scheda Data Acquisition. Si seleziona: Built-in microphone, si definisce la label (per esempio, yes), la Frequency[16000Hz] e la Sample length (in milliseconds), per esempio [10s]. Start sampling.\n\nI dati sul dataset di Pete hanno una lunghezza di 1s, ma i campioni registrati sono lunghi 10s e devono essere suddivisi in campioni da 1s. Cliccare sui tre puntini dopo il nome del campione e selezionare Split sample.\nSi aprir√† una finestra col tool Split.\n\nUna volta all‚Äôinterno dello strumento, si dividono i dati in record da 1 secondo (1000 ms). Se necessario, si aggiungono o rimuovono segmenti. Questa procedura deve essere ripetuta per tutti i nuovi campioni.\n\n\nUtilizzo di uno smartphone e di EI Studio\nSi pu√≤ anche utilizzare il PC o lo smartphone per acquisire dati audio, utilizzando una frequenza di campionamento di 16 KHz e una profondit√† di 16 bit.\nSi va su Devices, si scansiona il QR Code col telefono e si clicca sul link. Un‚Äôapp, Collection, per la raccolta dei dati apparir√† nel browser. Si seleziona Collecting Audio e si definisce la Label, la Length [lunghezza] dei dati catturati e la Category.\n\nSi ripete la stessa procedura usata con NiclaV.\n\nNotare che qualsiasi app, come Audacity, pu√≤ essere usata per la registrazione audio, a condizione che si usino campioni di 16KHz/16-bit.",
    "crumbs": [
      "Nicla Vision",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/kws/kws.it.html#creazione-di-impulse-pre-process-definizione-del-modello",
    "href": "contents/labs/arduino/nicla_vision/kws/kws.it.html#creazione-di-impulse-pre-process-definizione-del-modello",
    "title": "Keyword Spotting (KWS)",
    "section": "Creazione di Impulse (Pre-Process / Definizione del Modello)",
    "text": "Creazione di Impulse (Pre-Process / Definizione del Modello)\nUn impulse prende dati grezzi, usa l‚Äôelaborazione del segnale per estrarre le feature e poi usa un blocco di apprendimento per classificare nuovi dati.\n\nImpulse Design\n\nInnanzitutto, prenderemo i dati con una finestra di 1 secondo, aumentando i dati e facendo scorrere quella finestra in intervalli di 500 ms. Notare che √® impostata l‚Äôopzione Zero-pad data. √à essenziale riempire i campioni con degli ‚Äòzeri‚Äô inferiori a 1 secondo (in alcuni casi, alcuni campioni possono risultare inferiori alla finestra di 1000 ms sul tool di suddivisione per evitare rumore e picchi).\nOgni campione audio di 1 secondo dovrebbe essere pre-elaborato e convertito in un‚Äôimmagine (ad esempio, 13 x 49 x 1). Come discusso nel tutorial Feature Engineering for Audio Classification, utilizzeremo Audio (MFCC), che estrae le feature dai segnali audio utilizzando i Mel Frequency Cepstral Coefficients, che sono adatti alla voce umana, il nostro caso qui.\nSuccessivamente, selezioniamo il blocco Classification per costruire il nostro modello da zero utilizzando una Convolution Neural Network (CNN).\n\nIn alternativa, si pu√≤ utilizzare il blocco Transfer Learning (Keyword Spotting), che ottimizza un modello di keyword spotting pre-addestrato sui dati. Questo approccio ha buone prestazioni con set di dati di parole chiave relativamente piccoli.\n\n\n\nPre-elaborazione (MFCC)\nIl passaggio successivo consiste nel creare le feature da addestrare nella fase successiva:\nPotremmo mantenere i valori di default dei parametri, ma utilizzeremo l‚Äôopzione DSP Autotune parameters.\n\nPrenderemo le Raw features (i dati audio campionati a 16 KHz e 1 secondo) e utilizzeremo il blocco di elaborazione MFCC per calcolare le Processed features. Per ogni 16.000 feature grezze (16.000 x 1 secondo), otterremo 637 feature elaborate (13 x 49).\n\nIl risultato mostra che abbiamo utilizzato solo una piccola quantit√† di memoria per pre-elaborare i dati (16 KB) e una latenza di 34 ms, il che √® eccellente. Ad esempio, su un Arduino Nano (Cortex-M4f @ 64 MHz), lo stesso pre-processo richieder√† circa 480 ms. I parametri scelti, come la FFT length [512], avranno un impatto significativo sulla latenza.\nOra, Save parameters e passiamo alla scheda Generated features, dove verranno generate le feature effettive. Utilizzando UMAP, una tecnica di riduzione delle dimensioni, Feature explorer mostra come le feature sono distribuite su un grafico bidimensionale.\n\nIl risultato sembra OK, con una separazione visivamente netta tra feature yes (in rosso) e no (in blu). Le feature unknown [sconosciute] sembrano pi√π vicine allo spazio dei no che a quello degli yes. Questo suggerisce che la parola chiave no ha una maggiore propensione ai falsi positivi.\n\n\nAndiamo sotto il cofano\nPer comprendere meglio come viene pre-elaborato il suono grezzo, leggere il capitolo Feature Engineering for Audio Classification. Si pu√≤ sperimentare con la generazione di feature MFCC scaricando questo notebook da GitHub o [Opening it In Colab]",
    "crumbs": [
      "Nicla Vision",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/kws/kws.it.html#progettazione-e-addestramento-del-modello",
    "href": "contents/labs/arduino/nicla_vision/kws/kws.it.html#progettazione-e-addestramento-del-modello",
    "title": "Keyword Spotting (KWS)",
    "section": "Progettazione e Addestramento del Modello",
    "text": "Progettazione e Addestramento del Modello\nUseremo un semplice modello di rete neurale convoluzionale (CNN), testato con convoluzioni 1D e 2D. L‚Äôarchitettura di base ha due blocchi di Convolution + MaxPooling (filtri ([8] e [16], rispettivamente) e un Dropout di [0.25] per 1D e [0.5] per 2D. Per l‚Äôultimo layer, dopo Flattening, abbiamo [4] neuroni, uno per ogni classe:\n\nCome iperparametri, avremo un Learning Rate di [0.005] e un modello addestrato da [100] epoche. Includeremo anche un metodo di aumento dei dati basato su SpecAugment. Abbiamo addestrato i modelli 1D e 2D con gli stessi iperparametri. L‚Äôarchitettura 1D ha avuto un risultato complessivo migliore (90,5% di accuratezza rispetto all‚Äô88% del 2D, quindi useremo l‚Äô1D.\n\n\nL‚Äôutilizzo di convoluzioni 1D √® pi√π efficiente perch√© richiede meno parametri rispetto alle convoluzioni 2D, rendendole pi√π adatte ad ambienti con risorse limitate.\n\n√à anche interessante prestare attenzione alla Matrice di Confusione 1D. Lo Il punteggio F1 per yes √® 95% e per il no, 91%. Ci√≤ era previsto da ci√≤ che abbiamo visto con Feature Explorer (no e unknown a distanza ravvicinata). Nel tentativo di migliorare il risultato, si possono esaminare attentamente i risultati dei campioni con un errore.\n\nSi ascoltino i campioni che sono andati male. Ad esempio, per yes, la maggior parte degli errori erano correlati a un s√¨ pronunciato come ‚Äúyeh‚Äù. Si possono acquisire campioni aggiuntivi e quindi riaddestrare il modello.\n\nAndiamo sotto il cofano\nPer capire cosa sta succedendo ‚Äúsotto il cofano‚Äù, si pu√≤ scaricare il set di dati pre-elaborato ((MFCC training data) dalla scheda Dashboard ed eseguire questo Jupyter Notebook, giocando con il codice o [Aprirlo in Colab]. Ad esempio, si pu√≤ analizzare l‚Äôaccuratezza per ogni epoca:",
    "crumbs": [
      "Nicla Vision",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/kws/kws.it.html#test",
    "href": "contents/labs/arduino/nicla_vision/kws/kws.it.html#test",
    "title": "Keyword Spotting (KWS)",
    "section": "Test",
    "text": "Test\nTestando il modello con i dati riservati per il training (Test Data), abbiamo ottenuto un‚Äôaccuratezza di circa il 76%.\n\nIspezionando il punteggio F1, possiamo vedere che per YES abbiamo ottenuto 0.90, un risultato eccellente poich√© prevediamo di utilizzare questa parola chiave come ‚Äútrigger‚Äù primario per il nostro progetto KWS. Il risultato peggiore (0.70) √® per UNKNOWN, il che √® OK.\nPer NO, abbiamo ottenuto 0,72, come previsto, ma per migliorare questo risultato, possiamo spostare i campioni che non sono stati classificati correttamente nel set di dati di training e quindi ripetere il processo di training.\n\nClassificazione Live\nPossiamo procedere alla fase successiva del progetto, ma consideriamo anche che √® possibile eseguire la Classificazione live utilizzando NiclaV o uno smartphone per catturare campioni dal vivo, testando il modello addestrato prima della distribuzione sul nostro dispositivo.",
    "crumbs": [
      "Nicla Vision",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/kws/kws.it.html#distribuzione-e-inferenza",
    "href": "contents/labs/arduino/nicla_vision/kws/kws.it.html#distribuzione-e-inferenza",
    "title": "Keyword Spotting (KWS)",
    "section": "Distribuzione e Inferenza",
    "text": "Distribuzione e Inferenza\nL‚ÄôEIS impacchetter√† tutte le librerie necessarie, le funzioni di pre-elaborazione e i modelli addestrati, scaricandoli sul computer. Si va alla sezione Deployment, si seleziona Arduino Library, e, in basso, si sceglie Quantized (Int8) e si preme Build.\n\nQuando si seleziona il pulsante Build, verr√† creato un file zip che verr√† scaricato sul computer. Sull‚ÄôArduino IDE, si va alla scheda Sketch, si seleziona l‚Äôopzione Add .ZIP Library e si sceglie il file .zip scaricato da EIS:\n\nOra √® il momento di un vero test. Faremo delle inferenze completamente disconnesse da EIS. Usiamo l‚Äôesempio di codice NiclaV creato quando abbiamo distribuito la Libreria Arduino.\nNell‚ÄôIDE Arduino, si va alla scheda File/Examples, si cerca il progetto e si seleziona nicla-vision/nicla-vision_microphone (o nicla-vision_microphone_continuous)\n\nPremere due volte il pulsante di reset per mettere NiclaV in modalit√† di avvio, caricare lo sketch sulla board e provare alcune inferenze reali:",
    "crumbs": [
      "Nicla Vision",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/kws/kws.it.html#post-elaborazione",
    "href": "contents/labs/arduino/nicla_vision/kws/kws.it.html#post-elaborazione",
    "title": "Keyword Spotting (KWS)",
    "section": "Post-elaborazione",
    "text": "Post-elaborazione\nOra che sappiamo che il modello funziona perch√© rileva le nostre parole chiave, modifichiamo il codice per vedere il risultato con NiclaV completamente offline (scollegato dal PC e alimentato da una batteria, un power bank o un alimentatore indipendente da 5 V).\nL‚Äôidea √® che ogni volta che viene rilevata la parola chiave YES, si accende il LED verde; se si sente un NO, si accende il LED rosso, se √® un UNKNOW, si accende il LED blu; e in presenza di rumore (nessuna parola chiave), i LED saranno SPENTI.\nDovremmo modificare uno degli esempi di codice. Facciamolo ora con nicla-vision_microphone_continuous.\nPrima di tutto l‚Äôinizializzazione dei LED:\n...\nvoid setup()\n{\n        // Once you finish debugging your code, you can comment or delete the Serial part of the code\n    Serial.begin(115200);\n    while (!Serial);\n    Serial.println(\"Inferencing - Nicla Vision KWS with LEDs\");\n    \n    // Pins for the built-in RGB LEDs on the Arduino NiclaV\n    pinMode(LEDR, OUTPUT);\n    pinMode(LEDG, OUTPUT);\n    pinMode(LEDB, OUTPUT);\n\n    // Ensure the LEDs are OFF by default.\n    // Note: The RGB LEDs on the Arduino Nicla Vision\n    // are ON when the pin is LOW, OFF when HIGH.\n    digitalWrite(LEDR, HIGH);\n    digitalWrite(LEDG, HIGH);\n    digitalWrite(LEDB, HIGH);\n...\n}\nCreare due funzioni, la funzione turn_off_leds(), per spegnere tutti i LED RGB\n/*\n * @brief      turn_off_leds function - turn-off all RGB LEDs\n */\nvoid turn_off_leds(){\n    digitalWrite(LEDR, HIGH);\n    digitalWrite(LEDG, HIGH);\n    digitalWrite(LEDB, HIGH);\n}\nUn‚Äôaltra funzione turn_on_led() viene utilizzata per accendere i LED RGB in base al risultato pi√π probabile del classificatore.\n/*\n * @brief      turn_on_leds function used to turn on the RGB LEDs\n * @param[in]  pred_index     \n *             no:       [0] ==&gt; Red ON\n *             noise:    [1] ==&gt; ALL OFF \n *             unknown:  [2] ==&gt; Blue ON\n *             Yes:      [3] ==&gt; Green ON\n */\nvoid turn_on_leds(int pred_index) {\n  switch (pred_index)\n  {\n    case 0:\n      turn_off_leds();\n      digitalWrite(LEDR, LOW);\n      break;\n\n    case 1:\n      turn_off_leds();\n      break;\n    \n    case 2:\n      turn_off_leds();\n      digitalWrite(LEDB, LOW);\n      break;\n\n    case 3:\n      turn_off_leds();\n      digitalWrite(LEDG, LOW);\n      break;\n  }\n}\nE modificare la parte // print the predictions del codice su loop():\n...\n\n    if (++print_results &gt;= (EI_CLASSIFIER_SLICES_PER_MODEL_WINDOW)) {\n        // print the predictions\n        ei_printf(\"Predictions \");\n        ei_printf(\"(DSP: %d ms., Classification: %d ms., Anomaly: %d ms.)\",\n            result.timing.dsp, result.timing.classification, result.timing.anomaly);\n        ei_printf(\": \\n\");\n\n        int pred_index = 0;     // Initialize pred_index\n        float pred_value = 0;   // Initialize pred_value\n\n        for (size_t ix = 0; ix &lt; EI_CLASSIFIER_LABEL_COUNT; ix++) {\n            if (result.classification[ix].value &gt; pred_value){\n                pred_index = ix;\n                pred_value = result.classification[ix].value;\n            }\n\n            // ei_printf(\"    %s: \", result.classification[ix].label);\n            // ei_printf_float(result.classification[ix].value);\n            // ei_printf(\"\\n\");\n        }\n\n        ei_printf(\"  PREDICTION: ==&gt; %s with probability %.2f\\n\", \n                  result.classification[pred_index].label, pred_value);\n        turn_on_leds (pred_index);\n\n        \n#if EI_CLASSIFIER_HAS_ANOMALY == 1\n        ei_printf(\"    anomaly score: \");\n        ei_printf_float(result.anomaly);\n        ei_printf(\"\\n\");\n#endif\n\n        print_results = 0;\n    }\n}\n\n...\nIl codice completo si trova tra i progetti GitHub.\nCaricarere lo sketch sulla board e provare alcune inferenze reali. L‚Äôidea √® che il LED verde sar√† ACCESO ogni volta che viene rilevata la parola chiave YES, il rosso si accender√† per un NO e qualsiasi altra parola accender√† il LED blu. Tutti i LED dovrebbero essere spenti se √® presente silenzio o rumore di fondo. Ricordare che la stessa procedura pu√≤ ‚Äúattivare‚Äù un dispositivo esterno per eseguire un‚Äôazione desiderata invece di accendere un LED, come abbiamo visto nell‚Äôintroduzione.",
    "crumbs": [
      "Nicla Vision",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/kws/kws.it.html#conclusione",
    "href": "contents/labs/arduino/nicla_vision/kws/kws.it.html#conclusione",
    "title": "Keyword Spotting (KWS)",
    "section": "Conclusione",
    "text": "Conclusione\n\nSi troveranno i notebook e i codici utilizzati in questo tutorial nel repository GitHub.\n\nPrima di concludere, considerare che la classificazione dei suoni √® pi√π di una semplice voce. Ad esempio, si possono sviluppare progetti TinyML sul suono in diverse aree, come:\n\nSicurezza (Rilevamento di vetri rotti, spari)\nIndustria (Rilevamento di Anomalie)\nMedicina (Russamento, tosse, malattie polmonari)\nNatura (Controllo degli alveari, suono degli insetti, riduzione dei sacchetti di raccolta)",
    "crumbs": [
      "Nicla Vision",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/kws/kws.it.html#risorse",
    "href": "contents/labs/arduino/nicla_vision/kws/kws.it.html#risorse",
    "title": "Keyword Spotting (KWS)",
    "section": "Risorse",
    "text": "Risorse\n\nSottoinsieme del Dataset dei Comandi Vocali di Google\nKWS MFCC Analysis Colab Notebook\nKWS_CNN_training Colab Notebook\nCodice di Post-elaborazione Arduino\nProgetto Edge Impulse",
    "crumbs": [
      "Nicla Vision",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/motion_classification/motion_classification.it.html",
    "href": "contents/labs/arduino/nicla_vision/motion_classification/motion_classification.it.html",
    "title": "Classificazione del Movimento e Rilevamento delle Anomalie",
    "section": "",
    "text": "Panoramica\nI trasporti sono la spina dorsale del commercio globale. Milioni di container vengono trasportati ogni giorno tramite vari mezzi, come navi, camion e treni, verso destinazioni in tutto il mondo. Garantire il transito sicuro ed efficiente di questi container √® un compito monumentale che richiede di sfruttare la tecnologia moderna e TinyML √® senza dubbio una di queste.\nIn questo tutorial, lavoreremo per risolvere problemi reali relativi al trasporto. Svilupperemo un sistema di Motion Classification e Anomaly Detection utilizzando la scheda Arduino Nicla Vision, l‚ÄôIDE Arduino e Edge Impulse Studio. Questo progetto ci aiuter√† a comprendere come i container subiscono forze e movimenti diversi durante le varie fasi del trasporto, come il transito terrestre e marittimo, il movimento verticale tramite carrelli elevatori e i periodi di stazionamento nei magazzini.\nAlla fine di questo tutorial, si avr√† un prototipo funzionante in grado di classificare diversi tipi di movimento e rilevare anomalie durante il trasporto di container. Questa conoscenza pu√≤ essere un trampolino di lancio per progetti pi√π avanzati nel campo emergente di TinyML che coinvolge le vibrazioni.",
    "crumbs": [
      "Nicla Vision",
      "Classificazione del Movimento e Rilevamento delle Anomalie"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/motion_classification/motion_classification.it.html#panoramica",
    "href": "contents/labs/arduino/nicla_vision/motion_classification/motion_classification.it.html#panoramica",
    "title": "Classificazione del Movimento e Rilevamento delle Anomalie",
    "section": "",
    "text": "Obiettivi dell‚ÄôApprendimento\n\n\n\n\nImpostazione della scheda Arduino Nicla Vision\nRaccolta e pre-elaborazione dei dati\nCreazione del modello di classificazione del movimento\nImplementazione del rilevamento delle anomalie\nTest e Analisi nel Mondo Reale",
    "crumbs": [
      "Nicla Vision",
      "Classificazione del Movimento e Rilevamento delle Anomalie"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/motion_classification/motion_classification.it.html#installazione-e-test-dellimu",
    "href": "contents/labs/arduino/nicla_vision/motion_classification/motion_classification.it.html#installazione-e-test-dellimu",
    "title": "Classificazione del Movimento e Rilevamento delle Anomalie",
    "section": "Installazione e test dell‚ÄôIMU",
    "text": "Installazione e test dell‚ÄôIMU\nPer questo progetto, useremo un accelerometro. Come spiegato nel Tutorial, Configurazione di Nicla Vision, la Nicla Vision Board ha un IMU a 6 assi integrato: giroscopio 3D e accelerometro 3D, il LSM6DSOX. Verifichiamo se la libreria IMU LSM6DSOX √® installata. In caso contrario, la si installa.\n\nPoi si va su Examples &gt; Arduino_LSM6DSOX &gt; SimpleAccelerometer e si esegue il test dell‚Äôaccelerometro. Si pu√≤ verificare se funziona aprendo l‚ÄôIDE Serial Monitor o Plotter. I valori sono in g (gravit√† terrestre), con un range di default di +/- 4g:\n\n\nDefinizione della Frequenza di Campionamento:\nLa scelta di una frequenza di campionamento appropriata √® fondamentale per catturare le caratteristiche del movimento che interessa studiare. Il teorema di campionamento di Nyquist-Shannon afferma che la frequenza di campionamento dovrebbe essere almeno il doppio della componente di frequenza pi√π alta nel segnale per ricostruirlo correttamente. Nel contesto della classificazione del movimento e del rilevamento delle anomalie per il trasporto, la scelta della frequenza di campionamento dipenderebbe da diversi fattori:\n\nNatura del movimento: Diversi tipi di trasporto (terrestre, marittimo, ecc.) possono comportare diversi intervalli di frequenze di movimento. Movimenti pi√π rapidi potrebbero richiedere frequenze di campionamento pi√π elevate.\nLimitazioni hardware: La scheda Arduino Nicla Vision e tutti i sensori associati potrebbero avere limitazioni sulla velocit√† con cui possono campionare i dati.\nRisorse di Calcolo: Frequenze di campionamento pi√π elevate genereranno pi√π dati, il che potrebbe essere computazionalmente intensivo, particolarmente critico in un ambiente TinyML.\nDurata della Batteria: Una frequenza di campionamento pi√π elevata consumer√† pi√π energia. Se il sistema funziona a batteria, questa √® una considerazione importante.\nArchiviazione Dati: Un campionamento pi√π frequente richieder√† pi√π spazio di archiviazione, un‚Äôaltra considerazione cruciale per i sistemi embedded con memoria limitata.\n\nIn molte attivit√† di riconoscimento delle attivit√† umane, vengono comunemente utilizzate frequenze di campionamento da 50 Hz a 100 Hz circa. Dato che stiamo simulando scenari di trasporto, che in genere non sono eventi ad alta frequenza, una frequenza di campionamento in quell‚Äôintervallo (50-100 Hz) potrebbe essere un punto di partenza ragionevole.\nDefiniamo uno sketch che ci consentir√† di acquisire i nostri dati con una frequenza di campionamento definita (ad esempio, 50 Hz):\n/*\n * Based on Edge Impulse Data Forwarder Example (Arduino)\n  - https://docs.edgeimpulse.com/docs/cli-data-forwarder\n * Developed by M.Rovai @11May23\n */\n\n/* Include ----------------------------------------------------------------- */\n#include &lt;Arduino_LSM6DSOX.h&gt;\n\n/* Constant defines -------------------------------------------------------- */\n#define CONVERT_G_TO_MS2 9.80665f\n#define FREQUENCY_HZ        50\n#define INTERVAL_MS         (1000 / (FREQUENCY_HZ + 1))\n\nstatic unsigned long last_interval_ms = 0;\nfloat x, y, z;\n\nvoid setup() {\n  Serial.begin(9600);\n  while (!Serial);\n\n  if (!IMU.begin()) {\n    Serial.println(\"Failed to initialize IMU!\");\n    while (1);\n  }\n}\n\nvoid loop() {\n  if (millis() &gt; last_interval_ms + INTERVAL_MS) {\n    last_interval_ms = millis();\n    \n    if (IMU.accelerationAvailable()) {\n      // Read raw acceleration measurements from the device\n      IMU.readAcceleration(x, y, z);\n\n      // converting to m/s2\n      float ax_m_s2 = x * CONVERT_G_TO_MS2;\n      float ay_m_s2 = y * CONVERT_G_TO_MS2;\n      float az_m_s2 = z * CONVERT_G_TO_MS2;\n\n      Serial.print(ax_m_s2); \n      Serial.print(\"\\t\");\n      Serial.print(ay_m_s2); \n      Serial.print(\"\\t\");\n      Serial.println(az_m_s2); \n    }\n\n  }\n}\nCaricando lo sketch e ispezionando il Serial Monitor, possiamo vedere che stiamo catturando 50 campioni al secondo.\n\n\nSi noti che con la scheda Nicla appoggiata su un tavolo (con la telecamera rivolta verso il basso), l‚Äôasse z misura circa 9.8m/s\\(^2\\), l‚Äôaccelerazione terrestre prevista.",
    "crumbs": [
      "Nicla Vision",
      "Classificazione del Movimento e Rilevamento delle Anomalie"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/motion_classification/motion_classification.it.html#il-caso-di-studio-trasporto-simulato-di-container",
    "href": "contents/labs/arduino/nicla_vision/motion_classification/motion_classification.it.html#il-caso-di-studio-trasporto-simulato-di-container",
    "title": "Classificazione del Movimento e Rilevamento delle Anomalie",
    "section": "Il Caso di Studio: Trasporto Simulato di Container",
    "text": "Il Caso di Studio: Trasporto Simulato di Container\nSimuleremo il trasporto di container (o meglio di pacchi) attraverso diversi scenari per rendere questo tutorial pi√π comprensibile e pratico. Utilizzando l‚Äôaccelerometro integrato della scheda Arduino Nicla Vision, cattureremo i dati di movimento simulando manualmente le condizioni di:\n\nTrasporto Terrestre (su strada o treno)\nTrasporto Marittimo\nMovimento Verticale tramite Carrello Elevatore\nPeriodo di stazionamento (inattivo) in un Magazzino\n\n\nDalle immagini sopra, possiamo definire per la nostra simulazione che i movimenti principalmente orizzontali (asse x o y) dovrebbero essere associati alla ‚Äúclasse Terrestre‚Äù, i movimenti verticali (asse z) alla ‚Äúclasse di Sollevamento‚Äù, nessuna attivit√† alla ‚Äúclasse di Inattivit√†‚Äù e il movimento su tutti e tre gli assi alla classe Marittima.",
    "crumbs": [
      "Nicla Vision",
      "Classificazione del Movimento e Rilevamento delle Anomalie"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/motion_classification/motion_classification.it.html#raccolta-dati",
    "href": "contents/labs/arduino/nicla_vision/motion_classification/motion_classification.it.html#raccolta-dati",
    "title": "Classificazione del Movimento e Rilevamento delle Anomalie",
    "section": "Raccolta Dati",
    "text": "Raccolta Dati\nPer la raccolta dati, possiamo avere diverse opzioni. In un caso reale, possiamo avere il nostro dispositivo, ad esempio, collegato direttamente a un contenitore e i dati raccolti su un file (ad esempio .CSV) e archiviati su una scheda SD (tramite connessione SPI) o un repository offline nel computer. I dati possono anche essere inviati in remoto a un repository nelle vicinanze, come un telefono cellulare, tramite Bluetooth (come fatto in questo progetto: Sensor DataLogger). Una volta che il dataset √® stato raccolto e archiviato come file .CSV, pu√≤ essere caricato su Studio utilizzando lo strumento CSV Wizard.\n\nIn questo video, si possono imparare modi alternativi per inviare dati a Edge Impulse Studio.\n\n\nCollegamento del dispositivo a Edge Impulse\nCollegheremo Nicla direttamente a Edge Impulse Studio, che verr√† utilizzato anche per la pre-elaborazione dei dati, l‚Äôaddestramento del modello, i test e la distribuzione. Per questo, ci sono due possibilit√†:\n\nScaricare il firmware pi√π recente e collegarlo direttamente alla sezione Raccolta Dati.\nUtilizzare il tool CLI Data Forwarder per acquisire i dati dal sensore e inviarli a Studio.\n\nL‚Äôopzione 1 √® pi√π semplice, come abbiamo visto nell‚Äôesercitazione Configurazione di Nicla Vision, ma l‚Äôopzione 2 dar√† maggiore flessibilit√† per quanto riguarda l‚Äôacquisizione dei dati, come la definizione della frequenza di campionamento. Facciamolo con l‚Äôultima.\nCreare un nuovo progetto su Edge Impulse Studio (EIS) e collegarvi Nicla, seguendo questi passaggi:\n\nInstallare Edge Impulse CLI e Node.js sul computer.\nCaricare uno sketch per l‚Äôacquisizione dati (quello discusso in precedenza in questo tutorial).\nUtilizzare CLI Data Forwarder per acquisire dati dall‚Äôaccelerometro di Nicla e inviarli a Studio, come mostrato in questo diagramma:\n\n\nAvviare CLI Data Forwarder sul terminale, immettendo (se √® la prima volta) il seguente comando:\n$ edge-impulse-data-forwarder --clean\nQuindi, immettere le proprie credenziali EI e scegliere il progetto, le variabili (ad esempio, accX, accY e accZ) e il nome del dispositivo (ad esempio, NiclaV):\n\nAndare alla sezione Devices sul progetto EI e verificare se il dispositivo √® connesso (il punto dovrebbe essere verde):\n\n\nSi pu√≤ clonare il progetto sviluppato per questa esercitazione pratica: NICLA Vision Movement Classification.\n\n\n\nRaccolta Dati\nNella sezione Data Acquisition, si dovrebbe vedere che la scheda [NiclaV] √® connessa. Il sensore √® disponibile: [sensor with 3 axes (accX, accY, accZ)] con una frequenza di campionamento di [50Hz]. Studio suggerisce una lunghezza di campionamento di [10000] ms (10s). L‚Äôultima cosa rimasta √® definire l‚Äôetichetta del campione. Cominciamo con [terrestrial]:\n\nTerrestrial (pallet in un Camion o Treno), muovendosi orizzontalmente. Premere [Start Sample] e spostare il device orizzontalmente, mantenendo una direzione sopra il tavolo. Dopo 10s, i dati saranno caricati nello Studio. Ecco come √® stato raccolto il campione:\n\nCome previsto, il movimento √® stato catturato principalmente nell‚Äôasse Y (verde). Nel blu, vediamo l‚Äôasse Z, circa -10 m/s\\(^2\\) (Nicla ha la telecamera rivolta verso l‚Äôalto).\nCome discusso in precedenza, dovremmo catturare dati da tutte e quattro le classi Transportation‚Äù. Quindi, si immagini di avere un container con un accelerometro integrato nelle seguenti situazioni:\nMaritime (pallet in barche in un oceano in tempesta). Il movimento viene catturato su tutti e tre gli assi:\n\nLift (Pallet movimentati verticalmente da un Carrello elevatore). Movimento catturato solo sull‚Äôasse Z:\n\nIdle (Pallet in un magazzino). Nessun movimento rilevato dall‚Äôaccelerometro:\n\nAd esempio, si possono catturare 2 minuti (dodici campioni di 10 secondi) per ciascuna delle quattro classi (per un totale di 8 minuti di dati). Utilizzando il men√π tre puntini dopo ciascuno dei campioni, selezionarne 2, riservandoli per il set di Test. In alternativa, si pu√≤ utilizzare lo strumento automatico Train/Test Split tool nella scheda Danger Zone della scheda Dashboard. Di seguito il set di dati risultante:\n\nUna volta acquisito il dataset, lo si pu√≤ esplorare pi√π in dettaglio utilizzando Data Explorer, uno strumento visivo per trovare valori anomali o dati etichettati in modo errato (aiutando a correggerli). Data Explorer tenta prima di estrarre feature significative dai dati (applicando l‚Äôelaborazione del segnale e gli ‚Äúembedding‚Äù della rete neurale) e poi utilizza un algoritmo di riduzione della dimensionalit√† come PCA o t-SNE per mappare queste feature in uno spazio 2D. Questo fornisce una panoramica immediata del dataset completo.\n\nNel nostro caso, il set di dati sembra OK (buona separazione). Ma il PCA mostra che possiamo avere problemi tra marittimo (verde) e sollevamento (arancione). Ci√≤ √® prevedibile, una volta su una barca, a volte il movimento pu√≤ essere solo ‚Äúverticale‚Äù.",
    "crumbs": [
      "Nicla Vision",
      "Classificazione del Movimento e Rilevamento delle Anomalie"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/motion_classification/motion_classification.it.html#impulse-design",
    "href": "contents/labs/arduino/nicla_vision/motion_classification/motion_classification.it.html#impulse-design",
    "title": "Classificazione del Movimento e Rilevamento delle Anomalie",
    "section": "Impulse Design",
    "text": "Impulse Design\nIl passo successivo √® la definizione del nostro Impulse, che prende i dati grezzi e usa l‚Äôelaborazione del segnale per estrarre le feature, passandole come tensore di input di un learning block [blocco di apprendimento] per classificare nuovi dati. Si va in Impulse Design e Create Impulse. Studio suggerir√† la progettazione di base. Aggiungiamo anche un secondo Learning Block per Anomaly Detection.\n\nQuesto secondo modello usa un modello K-means. Se immaginiamo di poter avere le nostre classi note come cluster, qualsiasi campione che si adatterebbe potrebbe essere un ‚Äúoutlier‚Äù, un‚Äôanomalia come un container che rotola fuori da una nave in mare o cade da un carrello elevatore.\n\nLa frequenza di campionamento dovrebbe essere catturata automaticamente, in caso contrario, inserirla: [50]Hz. Studio suggerisce una Window Size di 2 secondi ([2000] ms) con una sliding window di [20]ms. Ci√≤ che stiamo definendo in questa fase √® che pre-elaboreremo i dati catturati (dati di serie temporali), creando un dataset tabellare (caratteristiche) che saranno l‚Äôinput per un classificatore di reti neurali (DNN) e un modello di rilevamento delle anomalie (K-Means), come mostrato di seguito:\n\nAnalizziamo attentamente quei passaggi e i parametri per capire meglio cosa stiamo facendo qui.\n\nPanoramica sulla Pre-Elaborazione dei Dati\nLa pre-elaborazione dei dati consiste nell‚Äôestrapolare le feature dal set di dati acquisito con l‚Äôaccelerometro, il che implica l‚Äôelaborazione e l‚Äôanalisi dei dati grezzi. Gli accelerometri misurano l‚Äôaccelerazione di un oggetto lungo uno o pi√π assi (in genere tre, indicati come X, Y e Z). Queste misure possono essere utilizzate per comprendere vari aspetti del movimento dell‚Äôoggetto, come pattern di movimento e vibrazioni.\nI dati grezzi dell‚Äôaccelerometro possono essere rumorosi e contenere errori o informazioni irrilevanti. Le fasi di pre-elaborazione, come il filtraggio e la normalizzazione, possono pulire e standardizzare i dati, rendendoli pi√π adatti all‚Äôestrazione di feature. Nel nostro caso, dovremmo dividere i dati in segmenti pi√π piccoli o windows [finestre]. Ci√≤ pu√≤ aiutare a concentrarsi su eventi o attivit√† specifici all‚Äôinterno del dataset, rendendo l‚Äôestrazione di feature pi√π gestibile e significativa. La scelta della window size e della sovrapposizione (window increase) dipende dall‚Äôapplicazione e dalla frequenza degli eventi di interesse. Come regola generale, dovremmo provare a catturare un paio di ‚Äúcicli di dati‚Äù.\n\nCon un ‚Äúsampling rate‚Äù (SR) [frequenza di campionamento] di 50 Hz e una dimensione della finestra di 2 secondi, otterremo 100 campioni per asse, o 300 in totale (3 assi x 2 secondi x 50 campioni). Faremo scorrere questa finestra ogni 200 ms, creando un set di dati pi√π grande in cui ogni istanza ha 300 feature grezze.\n\n\nUna volta che i dati sono stati pre-elaborati e segmentati, si possono estrarre feature che descrivono le caratteristiche del movimento. Alcune feature tipiche estratte dai dati dell‚Äôaccelerometro includono:\n\nLe feature del Time-domain descrivono le propriet√† statistiche dei dati all‚Äôinterno di ciascun segmento, come media, mediana, deviazione standard, asimmetria, curtosi e tasso di attraversamento dello zero.\nLe feature Frequency-domain si ottengono trasformando i dati nel dominio della frequenza utilizzando tecniche come la Fast Fourier Transform (FFT). Alcune feature tipiche del dominio della frequenza includono lo spettro di potenza, l‚Äôenergia spettrale, le frequenze dominanti (ampiezza e frequenza) e l‚Äôentropia spettrale.\nLe feature del dominio Time-frequency combinano le informazioni del dominio del tempo e della frequenza, come la Short-Time Fourier Transform (STFT) o la Discrete Wavelet Transform (DWT). Possono fornire una comprensione pi√π dettagliata di come il contenuto di frequenza del segnale cambia nel tempo.\n\nIn molti casi, il numero di feature estratte pu√≤ essere elevato, il che pu√≤ portare a un overfitting o a una maggiore complessit√† computazionale. Le tecniche di selezione delle feature, come le informazioni reciproche, i metodi basati sulla correlazione o l‚Äôanalisi delle componenti principali (PCA), possono aiutare a identificare le feature pi√π rilevanti per una determinata applicazione e ridurre la dimensionalit√† del dataset. Studio pu√≤ aiutare con tali calcoli di importanza delle feature.\n\n\nFeature Spettrali di EI Studio\nLa pre-elaborazione dei dati √® un‚Äôarea impegnativa per il machine learning embedded; tuttavia, Edge Impulse aiuta a superarla con la sua fase di pre-elaborazione del segnale digitale (DSP) e, pi√π specificamente, con lo Spectral Features Block.\nIn Studio, il dataset grezzo raccolto sar√† l‚Äôinput di un blocco di Spectral Analysis, che √® eccellente per analizzare il movimento ripetitivo, come i dati degli accelerometri. Questo blocco eseguir√† un DSP (Digital Signal Processing), estraendo feature come la FFT o le Wavelet.\nPer il nostro progetto, una volta che il segnale temporale √® continuo, dovremmo usare FFT con, ad esempio, una lunghezza di [32].\nLe feature Time Domain Statistical per asse/canale sono:\n\nRMS: 1 feature\nSkewness [Asimmetria]: 1 feature\nCurtosi: 1 feature\n\nLe feature Frequency Domain Spectral features per asse/canale sono:\n\nPotenza Spettrale: 16 feature (lunghezza FFT/2)\nAsimmetria: 1 feature\nCurtosi: 1 feature\n\nQuindi, per una lunghezza FFT di 32 punti, l‚Äôoutput risultante dello ‚ÄúSpectral Analysis Block‚Äù sar√† di 21 feature per asse (un totale di 63 feature).\n\nSi pu√≤ scoprire di pi√π su come viene calcolata ogni feature scaricando il notebook Edge Impulse - Spectral Features Block Analysis TinyML under the hood: Spectral Analysis o aprendolo direttamente su Google CoLab.\n\n\n\nGenerazione di feature\nUna volta capito cosa fa la pre-elaborazione, √® il momento di finire il lavoro. Quindi, prendiamo i dati grezzi (tipo serie temporale) e convertiamoli in dati tabellari. Per farlo, si va alla sezione Spectral Features nella scheda Parameters, si definiscono i parametri principali come discusso nella sezione precedente ([FFT] con [32] punti), e si seleziona [Save Parameters]:\n\nNel men√π in alto, si seleziona l‚Äôopzione Generate Features e il pulsante Generate Features. Ogni dato della finestra di 2 secondi verr√† convertito in un punto dati di 63 feature.\n\nFeature Explorer mostrer√† tali dati in 2D utilizzando UMAP. Uniform Manifold Approximation and Projection (UMAP) √® una tecnica di riduzione delle dimensioni che pu√≤ essere utilizzata per la visualizzazione in modo simile a t-SNE, ma √® applicabile anche per la riduzione generale delle dimensioni non lineari.\n\nLa visualizzazione consente di verificare che dopo la generazione delle feature, le classi presenti mantengano la loro eccellente separazione, il che indica che il classificatore dovrebbe funzionare bene. Facoltativamente, si pu√≤ analizzare quanto √® importante ciascuna delle feature per una classe rispetto alle altre.",
    "crumbs": [
      "Nicla Vision",
      "Classificazione del Movimento e Rilevamento delle Anomalie"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/motion_classification/motion_classification.it.html#addestramento-dei-modelli",
    "href": "contents/labs/arduino/nicla_vision/motion_classification/motion_classification.it.html#addestramento-dei-modelli",
    "title": "Classificazione del Movimento e Rilevamento delle Anomalie",
    "section": "Addestramento dei Modelli",
    "text": "Addestramento dei Modelli\nIl nostro classificatore sar√† una Dense Neural Network (DNN) che avr√† 63 neuroni sul suo layer di input, due layer nascosti con 20 e 10 neuroni e un layer di output con quattro neuroni (uno per ogni classe), come mostrato qui:\n\nCome iperparametri, useremo un Learning Rate di [0.005], una dimensione Batch di [32] e [20]% di dati per la convalida per [30] epoche. Dopo l‚Äôaddestramento, possiamo vedere che l‚Äôaccuratezza √® del 98.5%. Il costo della memoria e della latenza √® esiguo.\n\nPer la ‚ÄúAnomaly Detection‚Äù, sceglieremo le feature suggerite che sono esattamente le pi√π importanti nell‚Äôestrazione delle feature, pi√π l‚ÄôRMS accZ. Il numero di cluster sar√† [32], come suggerito da Studio:",
    "crumbs": [
      "Nicla Vision",
      "Classificazione del Movimento e Rilevamento delle Anomalie"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/motion_classification/motion_classification.it.html#test",
    "href": "contents/labs/arduino/nicla_vision/motion_classification/motion_classification.it.html#test",
    "title": "Classificazione del Movimento e Rilevamento delle Anomalie",
    "section": "Test",
    "text": "Test\nPossiamo verificare come si comporter√† il nostro modello con dati sconosciuti utilizzando il 20% dei dati lasciati da parte durante la fase di acquisizione dei dati. Il risultato √® stato quasi del 95%, il che √® positivo. Si pu√≤ sempre lavorare per migliorare i risultati, ad esempio, per capire cosa √® andato storto con uno dei risultati sbagliati. Se si tratta di una situazione unica, la si pu√≤ aggiungere al set di dati di training e quindi ripeterla.\nLa soglia minima di default per un risultato considerato incerto √® [0.6] per la classificazione e [0.3] per l‚Äôanomalia. Una volta che abbiamo quattro classi (la loro somma di output dovrebbe essere 1.0), si pu√≤ anche impostare una soglia inferiore per una classe da considerare valida (ad esempio, 0.4). Si possono Impostare le soglie di confidenza nel men√π tre puntini, oltre al pulsante Classify all.\n\nPuoi anche eseguire ‚ÄúLive Classification‚Äù col dispositivo (che dovrebbe essere ancora connesso a Studio).\n\nDa tenere presente che qui, si cattureranno dati reali col dispositivo e si caricheranno in Studio, dove verr√† presa un‚Äôinferenza utilizzando il modello addestrato (ma il modello NON √® nel device).",
    "crumbs": [
      "Nicla Vision",
      "Classificazione del Movimento e Rilevamento delle Anomalie"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/motion_classification/motion_classification.it.html#distribuzione",
    "href": "contents/labs/arduino/nicla_vision/motion_classification/motion_classification.it.html#distribuzione",
    "title": "Classificazione del Movimento e Rilevamento delle Anomalie",
    "section": "Distribuzione",
    "text": "Distribuzione\n√à il momento di distribuire il blocco di pre-elaborazione e il modello addestrato al Nicla. Studio impacchetter√† tutte le librerie necessarie, le funzioni di pre-elaborazione e i modelli addestrati, scaricandoli sul computer. Si deve selezionare l‚Äôopzione Arduino Library, e in basso si pu√≤ scegliere Quantized (Int8) o Unoptimized (float32) e [Build]. Verr√† creato un file Zip e scaricato sul computer.\n\nSu Arduino IDE, si va alla scheda Sketch, si seleziona Add.ZIP Library e si sceglie il file .zip scaricato da Studio. Un messaggio apparir√† nel terminale IDE: Library installed.\n\nInferenza\nOra √® il momento di un vero test. Faremo inferenze completamente scollegate da Studio. Modifichiamo uno degli esempi di codice creati quando si distribuisce la libreria Arduino.\nNell‚ÄôIDE Arduino, si va alla scheda File/Examples e si cerca il progetto, e negli esempi, si seleziona Nicla_vision_fusion:\n\nNotare che il codice creato da Edge Impulse considera un approccio sensor fusion in cui vengono utilizzati IMU (accelerometro e giroscopio) e ToF. All‚Äôinizio del codice, ci sono le librerie relative al nostro progetto, IMU e ToF:\n/* Includes ---------------------------------------------------------------- */\n#include &lt;NICLA_Vision_Movement_Classification_inferencing.h&gt; \n#include &lt;Arduino_LSM6DSOX.h&gt; //IMU\n#include \"VL53L1X.h\" // ToF\n\nSi pu√≤ mantenere il codice in questo modo per i test perch√© il modello addestrato utilizzer√† solo le funzionalit√† pre-elaborate dall‚Äôaccelerometro. Ma si consideri che si scriver√† il codice solo con le librerie necessarie per un progetto reale.\n\nE questo √® tutto!\nOra si pu√≤ caricare il codice sul dispositivo e procedere con le inferenze. Si preme due volte il pulsante [RESET] di Nicla per metterlo in modalit√† boot (disconnetterlo da Studio se √® ancora connesso) e caricare lo sketch sulla board.\nOra si devono provare diversi movimenti con la scheda (simili a quelli eseguiti durante l‚Äôacquisizione dei dati), osservando il risultato dell‚Äôinferenza di ciascuna classe sul Serial Monitor:\n\nClassi Idle e lift:\n\n\n\nMaritime e terrestrial:\n\n\nNotare che in tutte le situazioni sopra, il valore di anomaly score era inferiore a 0.0. Provare un nuovo movimento che non faceva parte del dataset originale, ad esempio, ‚Äúfacendo rotolare‚Äù la Nicla, rivolta verso la telecamera capovolta, come un contenitore che cade da una barca o persino un incidente in barca:\n\nRilevamento delle Anomalie\n\n\nIn questo caso, l‚Äôanomalia √® molto pi√π grande, oltre 1.00\n\n\nPost-elaborazione\nOra che sappiamo che il modello funziona poich√© rileva i movimenti, suggeriamo di modificare il codice per vedere il risultato con il NiclaV completamente offline (scollegato dal PC e alimentato da una batteria, un power bank o un alimentatore indipendente da 5V).\nL‚Äôidea √® di fare lo stesso del progetto KWS: se viene rilevato un movimento specifico, potrebbe accendersi un LED specifico. Ad esempio, se viene rilevato un movimento terrestre, si accender√† il LED verde; se √® un movimento marittimo, si accender√† il LED rosso, se √® un ascensore, si accender√† il LED blu; e se non viene rilevato alcun movimento (inattivo), i LED saranno SPENTI. Si pu√≤ anche aggiungere una condizione quando viene rilevata un‚Äôanomalia, in questo caso, ad esempio, pu√≤ essere utilizzato un colore bianco (tutti i LED e si accendono contemporaneamente).",
    "crumbs": [
      "Nicla Vision",
      "Classificazione del Movimento e Rilevamento delle Anomalie"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/motion_classification/motion_classification.it.html#conclusione",
    "href": "contents/labs/arduino/nicla_vision/motion_classification/motion_classification.it.html#conclusione",
    "title": "Classificazione del Movimento e Rilevamento delle Anomalie",
    "section": "Conclusione",
    "text": "Conclusione\n\nI notebook e i codici utilizzati in questo tutorial pratico si trovano nel repository GitHub.\n\nPrima di concludere, considerare che la classificazione del movimento e il rilevamento degli oggetti possono essere utilizzati in molte applicazioni in vari domini. Ecco alcune delle potenziali applicazioni:\n\nApplicazioni di Casi\n\nIndustriale e Manifatturiero\n\nManutenzione Predittiva: Rilevamento di anomalie nel movimento dei macchinari per prevedere guasti prima che si verifichino.\nControllo Qualit√†: Monitoraggio del movimento di linee di assemblaggio o bracci robotici per la valutazione della precisione e il rilevamento delle deviazioni dal pattern di movimenti standard.\nLogistica di Magazzino: Gestione e tracciamento del movimento delle merci con sistemi automatizzati che classificano diversi tipi di movimento e rilevano anomalie nella movimentazione.\n\n\n\nAssistenza Sanitaria\n\nMonitoraggio Pazienti: Rilevamento di cadute o movimenti anomali negli anziani o in coloro che hanno problemi di mobilit√†.\nRiabilitazione: Monitoraggio dei progressi dei pazienti in fase di recupero da infortuni tramite classificazione dei pattern di movimento durante le sedute di fisioterapia.\nRiconoscimento Attivit√†: Classificazione dei tipi di attivit√† fisica per applicazioni di fitness o monitoraggio dei pazienti.\n\n\n\nElettronica di Consumo\n\nGesture Control: Interpretazione di movimenti specifici per controllare i dispositivi, come accendere le luci con un gesto della mano.\nGaming: Miglioramento delle esperienze di gioco con input controllati dal movimento.\n\n\n\nTrasporti e Logistica\n\nTelematica dei Veicoli: Monitoraggio del movimento del veicolo per comportamenti insoliti come frenate brusche, curve strette o incidenti.\nMonitoraggio del Carico: Garantire l‚Äôintegrit√† delle merci durante il trasporto rilevando movimenti insoliti che potrebbero indicare manomissioni o cattiva gestione.\n\n\n\nCitt√† Intelligenti e Infrastrutture\n\nMonitoraggio Strutturale: Rilevare vibrazioni o movimenti all‚Äôinterno delle strutture che potrebbero indicare potenziali guasti o necessit√† di manutenzione.\nGestione del Traffico: Analizzare il flusso di pedoni o veicoli per migliorare la mobilit√† e la sicurezza urbana.\n\n\n\nSicurezza e Sorveglianza\n\nRilevamento Intrusi: Rilevare pattern di movimento tipici di accessi non autorizzati o altre violazioni della sicurezza.\nMonitoraggio della Fauna Selvatica: Rilevare bracconieri o movimenti anomali di animali in aree protette.\n\n\n\nAgricoltura\n\nMonitoraggio delle Attrezzature: Monitoraggio delle prestazioni e dell‚Äôutilizzo di macchinari agricoli.\nAnalisi del Comportamento Animale: Monitoraggio dei movimenti del bestiame per rilevare comportamenti che indicano problemi di salute o stress.\n\n\n\nMonitoraggio Ambientale\n\nAttivit√† Sismica: Rilevamento di pattern di movimento irregolari che precedono terremoti o altri eventi geologicamente rilevanti.\nOceanografia: Studio di pattern di onde o movimenti marini per scopi di ricerca e sicurezza.\n\n\n\n\nCustodia per Nicla 3D\nPer applicazioni reali, come alcuni hanno descritto in precedenza, possiamo aggiungere una custodia al nostro dispositivo ed Eoin Jordan, di Edge Impulse, ha sviluppato un‚Äôottima custodia indossabile e per la salute delle macchine per la gamma di schede Nicla. Funziona con un magnete da 10 mm, viti da 2M e una cinghia da 16mm per scenari di utilizzo per la salute umana e delle macchine. Ecco il link: Arduino Nicla Voice e Vision Wearable Case.\n\nLe applicazioni per la classificazione del movimento e il rilevamento delle anomalie sono estese e Arduino Nicla Vision √® adatto per scenari in cui il basso consumo energetico e l‚Äôedge processing sono vantaggiosi. Il suo piccolo fattore di forma e l‚Äôefficienza nell‚Äôelaborazione lo rendono una scelta ideale per l‚Äôimplementazione di applicazioni portatili e remote in cui l‚Äôelaborazione in tempo reale √® fondamentale e la connettivit√† potrebbe essere limitata.",
    "crumbs": [
      "Nicla Vision",
      "Classificazione del Movimento e Rilevamento delle Anomalie"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/motion_classification/motion_classification.it.html#risorse",
    "href": "contents/labs/arduino/nicla_vision/motion_classification/motion_classification.it.html#risorse",
    "title": "Classificazione del Movimento e Rilevamento delle Anomalie",
    "section": "Risorse",
    "text": "Risorse\n\nCodice Arduino\nEdge Impulse Spectral Features Block Colab Notebook\nProgetto Edge Impulse",
    "crumbs": [
      "Nicla Vision",
      "Classificazione del Movimento e Rilevamento delle Anomalie"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/setup/setup.it.html",
    "href": "contents/labs/seeed/xiao_esp32s3/setup/setup.it.html",
    "title": "Setup",
    "section": "",
    "text": "Panoramica\nXIAO ESP32S3 Sense √® la scheda di sviluppo conveniente di Seeed Studio, che integra un sensore della fotocamera, un microfono digitale e il supporto per schede SD. Combinando la potenza di elaborazione ML embedded e la capacit√† fotografica, questa scheda di sviluppo √® un ottimo strumento per iniziare con TinyML (IA vocale e visiva).\nCaratteristiche Principali di XIAO ESP32S3 Sense\nDi seguito √® riportato il pinout generale della scheda:",
    "crumbs": [
      "XIAO ESP32S3",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/setup/setup.it.html#panoramica",
    "href": "contents/labs/seeed/xiao_esp32s3/setup/setup.it.html#panoramica",
    "title": "Setup",
    "section": "",
    "text": "Potente Scheda MCU: Incorpora il chip del processore Xtensa dual-core ESP32S3 a 32 bit che funziona fino a 240 MHz, pi√π porte di sviluppo montate, supporto per Arduino/MicroPython\nFunzionalit√† Avanzate: Sensore della fotocamera OV2640 staccabile per una risoluzione di 1600 * 1200, compatibile con il sensore della fotocamera OV5640, integra un microfono digitale aggiuntivo\nProgetto di Alimentazione Elaborato: La capacit√† di gestione della carica della batteria al litio offre quattro modelli di consumo energetico, che consentono la modalit√† di ‚Äúdeep sleep‚Äù [sospensione profonda] con un consumo energetico basso fino a 14ŒºA\nGrande Memoria per pi√π Possibilit√†: Offre 8 MB di PSRAM e 8 MB di FLASH, supportando uno slot per schede SD di memoria FAT esterno da 32 GB\nPrestazioni RF Eccezionali: Supporta la comunicazione wireless duale Wi-Fi e BLE a 2,4 GHz, supporta la comunicazione remota a 100m+ se connesso all‚Äôantenna U.FL\nDesign Compatto delle Dimensioni di un Pollice: 21 x 17,5 mm, adotta il fattore di forma classico di XIAO, adatto per progetti con spazio limitato come i dispositivi indossabili\n\n\n\n\n\nPer maggiori dettagli, fare riferimento alla pagina Seeed Studio WiKi:  https://wiki.seeedstudio.com/xiao_esp32s3_getting_started/",
    "crumbs": [
      "XIAO ESP32S3",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/setup/setup.it.html#installazione-di-xiao-esp32s3-sense-su-arduino-ide",
    "href": "contents/labs/seeed/xiao_esp32s3/setup/setup.it.html#installazione-di-xiao-esp32s3-sense-su-arduino-ide",
    "title": "Setup",
    "section": "Installazione di XIAO ESP32S3 Sense su Arduino IDE",
    "text": "Installazione di XIAO ESP32S3 Sense su Arduino IDE\nSu Arduino IDE, si va su File &gt; Preferences e si inserisce l‚ÄôURL:\nhttps://raw.githubusercontent.com/espressif/arduino-esp32/gh-pages/package_esp32_dev_index.json\nnel campo ==&gt; Additional Boards Manager URLs\n\nPoi si apre il gestore delle schede. Si va su Tools &gt; Board &gt; Boards Manager‚Ä¶ e immettendo esp32. Selezionare e installare il pacchetto pi√π aggiornato e stabile (evitare le versioni alpha) :\n\n\n‚ö†Ô∏è Attenzione\nLe versioni Alpha (ad esempio, 3.x-alpha) non funzionano correttamente con XIAO ed Edge Impulse. Utilizzare invece l‚Äôultima versione stabile (ad esempio, 2.0.11).\n\nSu Tools, selezionare la Board (XIAO ESP32S3):\n\nUltimo ma non meno importante, scegliere la Porta a cui √® collegato l‚ÄôESP32S3.\nEcco fatto! Il dispositivo dovrebbe funzionare. Facciamo qualche test.",
    "crumbs": [
      "XIAO ESP32S3",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/setup/setup.it.html#test-della-scheda-con-blink",
    "href": "contents/labs/seeed/xiao_esp32s3/setup/setup.it.html#test-della-scheda-con-blink",
    "title": "Setup",
    "section": "Test della scheda con BLINK",
    "text": "Test della scheda con BLINK\nXIAO ESP32S3 Sense ha un LED integrato che √® collegato a GPIO21. Quindi, si pu√≤ eseguire lo sketch Blink cos√¨ com‚Äô√® (utilizzando la costante Arduino LED_BUILTIN) o modificando di conseguenza lo sketch Blink:\n#define LED_BUILT_IN 21 \n\nvoid setup() {\n  pinMode(LED_BUILT_IN, OUTPUT); // Set the pin as output\n}\n\n// Remember that the pin work with inverted logic\n// LOW to Turn on and HIGH to turn off\nvoid loop() {\n  digitalWrite(LED_BUILT_IN, LOW); //Turn on\n  delay (1000); //Wait 1 sec\n  digitalWrite(LED_BUILT_IN, HIGH); //Turn off\n  delay (1000); //Wait 1 sec\n}\n\nNotare che i pin funzionano con logica invertita: BASSO per accendere e ALTO per spegnere.",
    "crumbs": [
      "XIAO ESP32S3",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/setup/setup.it.html#collegamento-del-modulo-sense-scheda-di-espansione",
    "href": "contents/labs/seeed/xiao_esp32s3/setup/setup.it.html#collegamento-del-modulo-sense-scheda-di-espansione",
    "title": "Setup",
    "section": "Collegamento del Modulo Sense (Scheda di Espansione)",
    "text": "Collegamento del Modulo Sense (Scheda di Espansione)\nQuando viene acquistata, la scheda di espansione √® separata da quella principale, ma installarla √® molto semplice. Si deve allineare il connettore sulla scheda di espansione col connettore B2B sullo XIAO ESP32S3, premerlo con forza e quando si sente un ‚Äúclic‚Äù, l‚Äôinstallazione √® completa.\nCome commentato nell‚Äôintroduzione, la scheda di espansione, o la parte ‚Äúsense‚Äù del dispositivo, ha una fotocamera OV2640 da 1600x1200, uno slot per schede SD e un microfono digitale.",
    "crumbs": [
      "XIAO ESP32S3",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/setup/setup.it.html#test-del-microfono",
    "href": "contents/labs/seeed/xiao_esp32s3/setup/setup.it.html#test-del-microfono",
    "title": "Setup",
    "section": "Test del Microfono",
    "text": "Test del Microfono\nCominciamo con il rilevamento del suono. Si va nei progetti GitHub e si scarica lo sketch: XIAOEsp2s3_Mic_Test poi lo si esegue sull‚ÄôIDE Arduino:\n\nQuando si produce un suono, √® possibile verificarlo sul Serial Plotter.\nSalvare il suono registrato (file audio .wav) su una scheda microSD.\nOra, il lettore di schede SD integrato pu√≤ salvare file audio .wav. Per farlo, dobbiamo abilitare la PSRAM XIAO.\n\nESP32-S3 ha solo poche centinaia di kilobyte di RAM interna sul chip MCU. Questo pu√≤ essere insufficiente per alcuni scopi, quindi fino a 16 MB di PSRAM esterna (RAM pseudo-statica) possono essere collegati al chip flash SPI. La memoria esterna √® incorporata nella mappa di memoria e, con alcune restrizioni, √® utilizzabile allo stesso modo della RAM dati interna.\n\nPer iniziare, inserire la scheda SD su XIAO come mostrato nella foto qui sotto (la scheda SD deve essere formattata in FAT32).\n\n\nScaricare lo sketch Wav_Record, che si pu√≤ trovare su GitHub.\nPer eseguire il codice (Wav Record), √® necessario utilizzare la funzione PSRAM del chip ESP-32, quindi la si deve attivare prima di caricare: Tools&gt;PSRAM: ‚ÄúOPI PSRAM‚Äù&gt;OPI PSRAM\n\n\n\nEseguire il codice Wav_Record.ino\nQuesto programma viene eseguito solo una volta dopo che l‚Äôutente accende il monitor seriale. Registra per 20 secondi e salva il file su una scheda microSD come ‚Äúarduino_rec.wav.‚Äù\nQuando ‚Äú.‚Äù viene emesso ogni 1 secondo nel monitor seriale, l‚Äôesecuzione del programma √® terminata e si pu√≤ riprodurre il file audio registrato con l‚Äôaiuto di un lettore di schede.\n\n\nLa qualit√† del suono √® eccellente!\n\nLa spiegazione di come funziona il codice va oltre lo scopo di questo tutorial, ma c‚Äô√® un‚Äôeccellente descrizione sulla pagina wiki.",
    "crumbs": [
      "XIAO ESP32S3",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/setup/setup.it.html#test-della-fotocamera",
    "href": "contents/labs/seeed/xiao_esp32s3/setup/setup.it.html#test-della-fotocamera",
    "title": "Setup",
    "section": "Test della Fotocamera",
    "text": "Test della Fotocamera\nPer testare la fotocamera, si deve scaricare la cartella take_photos_command da GitHub. La cartella contiene lo sketch (.ino) e due file .h con i dettagli della fotocamera.\n\nEseguire il codice: take_photos_command.ino. Aprire il Serial Monitor e inviare il comando capture per catturare e salvare l‚Äôimmagine sulla scheda SD:\n\n\nVerificare che [Both NL & CR] sia selezionato sul Serial Monitor.\n\n\nEcco un esempio di una foto scattata:",
    "crumbs": [
      "XIAO ESP32S3",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/setup/setup.it.html#test-del-wifi",
    "href": "contents/labs/seeed/xiao_esp32s3/setup/setup.it.html#test-del-wifi",
    "title": "Setup",
    "section": "Test del WiFi",
    "text": "Test del WiFi\nUno dei punti di forza di XIAO ESP32S3 √® la sua capacit√† WiFi. Quindi, testiamo la sua radio eseguendo la scansione delle reti Wi-Fi attorno ad essa. Lo si pu√≤ fare eseguendo uno degli esempi di codice sulla scheda.\nSi va su Arduino IDE Examples e si cerca WiFI ==&gt; WiFIScan\nSi dovrebbero vedere le reti Wi-Fi (SSID e RSSI) nel raggio d‚Äôazione del dispositivo sul monitor seriale. Ecco cosa abbiamo ottenuto in laboratorio:\n\nUn Semplice Server WiFi (accensione/spegnimento del LED)\nTestiamo la capacit√† del dispositivo di comportarsi come un server WiFi. Ospiteremo una semplice pagina sul dispositivo che invia comandi per accendere e spegnere il LED integrato di XIAO.\nCome prima, si va su GitHub per scaricare la cartella usando lo sketch SimpleWiFiServer.\nPrima di eseguire lo sketch, si devono inserire le credenziali di rete:\nconst char* ssid     = \"Your credentials here\";\nconst char* password = \"Your credentials here\";\nSi pu√≤ monitorare il funzionamento del server con Serial Monitor.\n\nPrendere l‚Äôindirizzo IP e inserirlo nel browser:\n\nSi vedr√† una pagina con link che possono accendere e spegnere il LED integrato del XIAO.\nStreaming video sul Web\nOra che sappiamo che si possono inviare comandi dalla pagina Web al dispositivo, facciamo il contrario. Prendiamo l‚Äôimmagine catturata dalla telecamera e la trasmettiamo in streaming su una pagina Web:\nScaricare da GitHub la cartella che contiene il codice: XIAO-ESP32S3-Streeming_Video.ino.\n\nRicordarsi che la cartella contiene il file .ino e un paio di file .h necessari per gestire la telecamera.\n\nInserire le credenziali ed eseguire lo sketch. Sul monitor seriale, si pu√≤ trovare l‚Äôindirizzo della pagina da inserire nel browser:\n\nApri la pagina sul browser (attendere qualche secondo per avviare lo streaming). Tutto qui.\n\nMettere in streaming ci√≤ che la telecamera ‚Äúvede‚Äù pu√≤ essere importante quando la si posiziona per catturare un set di dati per un progetto ML (ad esempio, usando il codice ‚Äútake_phots_commands.ino‚Äù.\nNaturalmente, possiamo fare entrambe le cose contemporaneamente: mostrare ci√≤ che la telecamera vede sulla pagina e inviare un comando per catturare e salvare l‚Äôimmagine sulla scheda SD. Per questo, si pu√≤ usare il codice Camera_HTTP_Server_STA, scaricabile da GitHub.\n\nIl programma eseguir√† le seguenti attivit√†:\n\nImposta la fotocamera in modalit√† di output JPEG.\nCrea una pagina Web (ad esempio ==&gt; http://192.168.4.119//). L‚Äôindirizzo corretto verr√† visualizzato sul Serial Monitor.\nSe server.on (‚Äú/capture‚Äù, HTTP_GET, serverCapture), il programma scatta una foto e la invia al Web.\n√à possibile ruotare l‚Äôimmagine sulla pagina Web utilizzando il pulsante [ROTATE]\nIl comando [CAPTURE] visualizzer√† solo l‚Äôanteprima dell‚Äôimmagine sulla pagina Web, mostrandone le dimensioni sul Serial Monitor\nIl comando [SAVE] salver√† un‚Äôimmagine sulla scheda SD e la mostrer√† sul browser.\nLe immagini salvate seguiranno una denominazione sequenziale (image1.jpg, image2.jpg.\n\n\n\nQuesto programma pu√≤ catturare un set di dati di immagini con un progetto di classificazione delle immagini.\n\nEsaminate il codice; sar√† pi√π facile capire come funziona la fotocamera. Questo codice √® stato sviluppato sulla base del fantastico tutorial di Rui Santos ESP32-CAM Take Photo and Display in Web Server, che invito tutti a visitare.\nUso di CameraWebServer\nNell‚ÄôIDE Arduino, si va su File &gt; Examples &gt; ESP32 &gt; Camera e si seleziona CameraWebServer\nSi devono anche commentare tutti i modelli di fotocamere, eccetto i pin del modello XIAO:\n#define CAMERA_MODEL_XIAO_ESP32S3 // Has PSRAM\nNon dimenticare Tools per abilitare la PSRAM.\nInserire le credenziali wifi e caricare il codice sul dispositivo:\n\nSe il codice viene eseguito correttamente, si vedr√† l‚Äôindirizzo sul monitor seriale:\n\nCopiare l‚Äôindirizzo sul browser e attendere che la pagina venga caricata. Selezionare la risoluzione della telecamera (ad esempio, QVGA) e selezionare [START STREAM]. Attendi qualche secondo/minuto, a seconda della connessione. Utilizzando il pulsante [Save], si pu√≤ salvare un‚Äôimmagine nell‚Äôarea download del computer.\n\nEcco fatto! Si possono salvare le immagini direttamente sul computer per usarle nei progetti.",
    "crumbs": [
      "XIAO ESP32S3",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/setup/setup.it.html#conclusione",
    "href": "contents/labs/seeed/xiao_esp32s3/setup/setup.it.html#conclusione",
    "title": "Setup",
    "section": "Conclusione",
    "text": "Conclusione\nThe XIAO ESP32S3 Sense √® flessibile, economico e facile da programmare. Con 8 MB di RAM, la memoria non √® un problema e il dispositivo pu√≤ gestire molte attivit√† di post-elaborazione, inclusa la comunicazione.\nL‚Äôultima versione del codice si trova nel repository GitHub: XIAO-ESP32S3-Sense.",
    "crumbs": [
      "XIAO ESP32S3",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/setup/setup.it.html#risorse",
    "href": "contents/labs/seeed/xiao_esp32s3/setup/setup.it.html#risorse",
    "title": "Setup",
    "section": "Risorse",
    "text": "Risorse\n\nXIAO ESP32S3 Code",
    "crumbs": [
      "XIAO ESP32S3",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/image_classification/image_classification.it.html",
    "href": "contents/labs/seeed/xiao_esp32s3/image_classification/image_classification.it.html",
    "title": "Classificazione delle Immagini",
    "section": "",
    "text": "Panoramica\nSempre di pi√π, ci troviamo di fronte a una rivoluzione dell‚Äôintelligenza artificiale (IA) in cui, come affermato da Gartner, Edge AI ha un potenziale di impatto molto elevato, ed √® ora!\nIn prima linea nel Radar delle Tecnologie Emergenti c‚Äô√® il linguaggio universale di Edge Computer Vision. Quando esaminiamo il Machine Learning (ML) applicato alla visione, il primo concetto che ci accoglie √® la classificazione delle immagini, una specie ‚ÄúHello World‚Äù del ML che √® sia semplice che profondo!\nSeeed Studio XIAO ESP32S3 Sense √® un potente strumento che combina il supporto per fotocamera e scheda SD. Con la sua potenza di elaborazione ML embedded e la capacit√† di fotografia, √® un ottimo punto di partenza per esplorare l‚Äôintelligenza artificiale per la visione TinyML.",
    "crumbs": [
      "XIAO ESP32S3",
      "Classificazione delle Immagini"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/image_classification/image_classification.it.html#un-progetto-tinyml-di-motion-classification-frutta-contro-verdura",
    "href": "contents/labs/seeed/xiao_esp32s3/image_classification/image_classification.it.html#un-progetto-tinyml-di-motion-classification-frutta-contro-verdura",
    "title": "Classificazione delle Immagini",
    "section": "Un progetto TinyML di Motion Classification: Frutta contro Verdura",
    "text": "Un progetto TinyML di Motion Classification: Frutta contro Verdura\n\nL‚Äôidea di base del nostro progetto sar√† quella di addestrare un modello e procedere con l‚Äôinferenza su XIAO ESP32S3 Sense. Per l‚Äôaddestramento, dovremmo trovare alcuni dati (in effetti, tonnellate di dati!).\nMa prima di tutto, abbiamo bisogno di un obiettivo! Cosa vogliamo classificare?\nCon TinyML, un set di tecniche associate all‚Äôinferenza di apprendimento automatico su dispositivi embedded, dovremmo limitare la classificazione a tre o quattro categorie a causa di limitazioni (principalmente della memoria). Differenziamo mele da banane e patate (si possono provare altre categorie).\nQuindi, cerchiamo un set di dati specifico che includa immagini da quelle categorie. Kaggle √® un buon inizio:\nhttps://www.kaggle.com/kritikseth/fruit-and-vegetable-image-recognition\nQuesto set di dati contiene immagini dei seguenti alimenti:\n\nFrutta - banana, mela, pera, uva, arancia, kiwi, anguria, melograno, ananas, mango.\nVerdura - cetriolo, carota, peperone, cipolla, patata, limone, pomodoro, ravanello, barbabietola, cavolo, lattuga, spinaci, soia, cavolfiore, peperone, peperoncino, rapa, mais, mais dolce, patata dolce, paprika, jalepe√±o, zenzero, aglio, piselli, melanzane.\n\nOgni categoria √® suddivisa in train [addestramento] (100 immagini), test (10 immagini) e validation (10 immagini).\n\nScaricare il dataset dal sito Web di Kaggle e metterlo sul computer.\n\n\nFacoltativamente, si possono aggiungere alcune foto fresche di banane, mele e patate direttamente dalla cucina di casa, utilizzando, ad esempio, il codice discusso nel lab della configurazione.",
    "crumbs": [
      "XIAO ESP32S3",
      "Classificazione delle Immagini"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/image_classification/image_classification.it.html#addestramento-del-modello-con-edge-impulse-studio",
    "href": "contents/labs/seeed/xiao_esp32s3/image_classification/image_classification.it.html#addestramento-del-modello-con-edge-impulse-studio",
    "title": "Classificazione delle Immagini",
    "section": "Addestramento del modello con Edge Impulse Studio",
    "text": "Addestramento del modello con Edge Impulse Studio\nUtilizzeremo Edge Impulse Studio per addestrare il modello. Come si sapr√†, Edge Impulse √® una piattaforma di sviluppo leader per l‚Äôapprendimento automatico su dispositivi edge.\nInserire le credenziali del proprio account (o crearne uno gratuito) su Edge Impulse. Quindi, creare un nuovo progetto:\n\n\nRaccolta Dati\nSuccessivamente, nella sezione UPLOAD DATA, caricare dal computer i file delle categorie scelte:\n\nSarebbe meglio se ora si avesse il proprio set di dati di training suddiviso in tre classi di dati:\n\n\nSi possono caricare dati extra per ulteriori test del modello o dividere i dati di training. Lo lasceremo cos√¨ com‚Äô√® per utilizzare pi√π dati possibili.\n\n\n\nImpulse Design\n\nUn impulso prende dati grezzi (in questo caso, immagini), estrae feature (ridimensiona le immagini) e poi utilizza un blocco di apprendimento per classificare nuovi dati.\n\nLa classificazione delle immagini √® l‚Äôuso pi√π comune del deep learning, ma per portare a termine questo compito dovrebbero essere utilizzati molti dati. Abbiamo circa 90 immagini per ogni categoria. Questo numero √® sufficiente? No, per niente! Avremo bisogno di migliaia di immagini per ‚Äúinsegnare o modellare‚Äù per distinguere una mela da una banana. Ma possiamo risolvere questo problema riaddestrando un modello precedentemente addestrato con migliaia di immagini. Chiamiamo questa tecnica ‚ÄúTransfer Learning‚Äù (TL).\n\nCol TL, possiamo mettere a punto un modello di classificazione delle immagini pre-addestrato sui nostri dati, che funziona bene anche con set di dati di immagini relativamente piccoli (il nostro caso).\nQuindi, partendo dalle immagini grezze, le ridimensioneremo (96x96) pixel e le invieremo al nostro blocco Transfer Learning:\n\n\nPre-elaborazione (Generazione di Feature)\nOltre a ridimensionare le immagini, possiamo cambiarle in scala di grigi o mantenere la profondit√† di colore RGB effettiva. Iniziamo selezionando Grayscale. In questo modo, ognuno dei nostri campioni di dati avr√† dimensione 9.216 feature (96x96x1). Con RGB, questa dimensione sarebbe tre volte pi√π grande. Lavorare con la scala di grigi aiuta a ridurre la quantit√† di memoria finale necessaria per l‚Äôinferenza.\n\nRicordarsi di [Save parameters]. Questo generer√† le feature da utilizzare nel training.\n\n\nProgettazione del Modello\nTransfer Learning\nNel 2007, Google ha introdotto MobileNetV1, una famiglia di reti neurali di visione artificiale di uso generale progettate pensando ai dispositivi mobili per supportare classificazione, rilevamento e altro. Le MobileNet sono modelli piccoli, a bassa latenza e a basso consumo, parametrizzati per soddisfare i vincoli di risorse di vari casi d‚Äôuso.\nSebbene l‚Äôarchitettura di base di MobileNet sia gi√† minuscola e abbia una bassa latenza, spesso un caso d‚Äôuso o un‚Äôapplicazione specifica potrebbe richiedere che il modello sia pi√π piccolo e veloce. MobileNet introduce un parametro semplice Œ± (alfa) chiamato moltiplicatore di larghezza per costruire questi modelli pi√π piccoli e meno costosi dal punto di vista computazionale. Il ruolo del moltiplicatore di larghezza Œ± √® di assottigliare una rete in modo uniforme a ogni livello.\nEdge Impulse Studio ha MobileNet V1 (immagini 96x96) e V2 (immagini 96x96 e 160x160) disponibili, con diversi valori Œ± (da 0,05 a 1,0). Ad esempio, si otterr√† la massima accuratezza con V2, immagini 160x160 e Œ±=1,0. Naturalmente, c‚Äô√® un compromesso. Maggiore √® la precisione, maggiore sar√† la memoria (circa 1,3 M di RAM e 2,6 M di ROM) necessaria per eseguire il modello, il che implica una maggiore latenza.\nL‚Äôingombro pi√π piccolo sar√† ottenuto all‚Äôaltro estremo con MobileNet V1 e Œ±=0,10 (circa 53,2 K di RAM e 101 K di ROM).\nPer questo primo passaggio, utilizzeremo MobileNet V1 e Œ±=0,10.\n\n\n\nTraining\nData Augmentation\nUn‚Äôaltra tecnica necessaria da usare con il deep learning √® il data augmentation. Il data augmentation √® un metodo che pu√≤ aiutare a migliorare l‚Äôaccuratezza dei modelli di machine learning, creando dati artificiali aggiuntivi. Un sistema di Data Augmentation apporta piccole modifiche casuali ai dati di training (ad esempio capovolgendo, ritagliando o ruotando le immagini).\nSotto, qui si pu√≤ vedere come Edge Impulse implementa una policy di data Augmentation sui dati:\n# Implements the data augmentation policy\ndef augment_image(image, label):\n    # Flips the image randomly\n    image = tf.image.random_flip_left_right(image)\n\n    # Increase the image size, then randomly crop it down to\n    # the original dimensions\n    resize_factor = random.uniform(1, 1.2)\n    new_height = math.floor(resize_factor * INPUT_SHAPE[0])\n    new_width = math.floor(resize_factor * INPUT_SHAPE[1])\n    image = tf.image.resize_with_crop_or_pad(image, new_height, new_width)\n    image = tf.image.random_crop(image, size=INPUT_SHAPE)\n\n    # Vary the brightness of the image\n    image = tf.image.random_brightness(image, max_delta=0.2)\n\n    return image, label\nL‚Äôesposizione a queste variazioni durante l‚Äôaddestramento pu√≤ aiutare a impedire al modello di prendere scorciatoie ‚Äúmemorizzando‚Äù indizi superficiali nei dati di addestramento, il che significa che potrebbe riflettere meglio i pattern profondi in esame nel set di dati.\nL‚Äôultimo layer del nostro modello avr√† 16 neuroni con un dropout del 10% per prevenire l‚Äôoverfitting. Ecco l‚Äôoutput del Training:\n\nIl risultato potrebbe essere migliore. Il modello ha raggiunto circa il 77% di accuratezza, ma la quantit√† di RAM prevista per essere utilizzata durante l‚Äôinferenza √® relativamente piccola (circa 60 KByte), il che √® molto buono.\n\n\nDeployment\nIl modello addestrato verr√† distribuito come libreria Arduino .zip:\n\nApri l‚ÄôIDE Arduino e in Sketch, si va su Include Library e add.ZIP Library. Selezionare il file che scaricato da Edge Impulse Studio e il gioco √® fatto!\n\nNella scheda Examples su Arduino IDE, si trova un codice sketch sotto il nome del proprio progetto.\n\nAprire l‚Äôesempio Static Buffer:\n\nLa prima riga di codice √® esattamente la chiamata di una libreria con tutto il necessario per eseguire l‚Äôinferenza sul dispositivo.\n#include &lt;XIAO-ESP32S3-CAM-Fruits-vs-Veggies_inferencing.h&gt;\nOvviamente, questo √® un codice generico (un ‚Äútemplate‚Äù) che ottiene solo un campione di dati grezzi (memorizzati nella variabile: features = {} ed esegue il classificatore, eseguendo l‚Äôinferenza. Il risultato viene mostrato sul monitor seriale.\nDovremmo ottenere il campione (immagine) dalla fotocamera e pre-elaborarlo (ridimensionandolo a 96x96, convertendolo in scala di grigi e appiattendolo). Questo sar√† il tensore di input del nostro modello. Il tensore di output sar√† un vettore con tre valori (etichette), che mostrano le probabilit√† di ciascuna delle classi.\n\nTornando al progetto (Tab Image), copiare uno dei Raw Data Sample:\n\n9.216 feature verranno copiate negli appunti. Questo √® il tensore di input (un‚Äôimmagine appiattita di 96x96x1), in questo caso, banane. ‚ÄúIncollare‚Äù questo tensore di input su features[] = {0xb2d77b, 0xb5d687, 0xd8e8c0, 0xeaecba, 0xc2cf67, ...}\n\nEdge Impulse ha incluso la libreria ESP NN nel suo SDK, che contiene funzioni NN (Neural Network) ottimizzate per vari chip Espressif, tra cui ESP32S3 (in esecuzione su Arduino IDE).\nQuando si esegue l‚Äôinferenza, si deve ottenere il punteggio pi√π alto per ‚Äúbanana‚Äù.\n\nOttime notizie! Il nostro dispositivo gestisce un‚Äôinferenza, scoprendo che l‚Äôimmagine in ingresso √® una banana. Notare inoltre che il tempo di inferenza √® stato di circa 317 ms, con un massimo di 3 fps se si √® provato a classificare le immagini da un video.\nOra, dovremmo incorporare la telecamera e classificare le immagini in tempo reale.\nSi va su Arduino IDE Examples e si scarica dal progetto lo sketch esp32_camera:\n\nSi devono cambiare le righe dalla 32 alla 75, che definiscono il modello e i pin della telecamera, utilizzando i dati relativi al nostro modello. Copiare e incollare le righe seguenti, sostituendo le righe 32-75:\n#define PWDN_GPIO_NUM     -1 \n#define RESET_GPIO_NUM    -1 \n#define XCLK_GPIO_NUM     10 \n#define SIOD_GPIO_NUM     40 \n#define SIOC_GPIO_NUM     39\n#define Y9_GPIO_NUM       48 \n#define Y8_GPIO_NUM       11 \n#define Y7_GPIO_NUM       12 \n#define Y6_GPIO_NUM       14 \n#define Y5_GPIO_NUM       16 \n#define Y4_GPIO_NUM       18 \n#define Y3_GPIO_NUM       17 \n#define Y2_GPIO_NUM       15 \n#define VSYNC_GPIO_NUM    38 \n#define HREF_GPIO_NUM     47 \n#define PCLK_GPIO_NUM     13\nEcco il codice risultante:\n\nLo sketch modificato √® scaricabile da GitHub: xiao_esp32s3_camera.\n\nNotare che si possono facoltativamente mantenere i pin come file .h come abbiamo fatto nel Setup Lab.\n\nCaricare il codice sullo XIAO ESP32S3 Sense e si sar√† pronti per iniziare a classificare la frutta e la verdura! Si pu√≤ controllare il risultato su Serial Monitor.",
    "crumbs": [
      "XIAO ESP32S3",
      "Classificazione delle Immagini"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/image_classification/image_classification.it.html#test-del-modello-inferenza",
    "href": "contents/labs/seeed/xiao_esp32s3/image_classification/image_classification.it.html#test-del-modello-inferenza",
    "title": "Classificazione delle Immagini",
    "section": "Test del modello (inferenza)",
    "text": "Test del modello (inferenza)\n\nScattando una foto con la fotocamera, il risultato della classificazione apparir√† su Serial Monitor:\n\nAltri test:",
    "crumbs": [
      "XIAO ESP32S3",
      "Classificazione delle Immagini"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/image_classification/image_classification.it.html#test-con-un-modello-pi√π-grande",
    "href": "contents/labs/seeed/xiao_esp32s3/image_classification/image_classification.it.html#test-con-un-modello-pi√π-grande",
    "title": "Classificazione delle Immagini",
    "section": "Test con un Modello Pi√π Grande",
    "text": "Test con un Modello Pi√π Grande\nOra, passiamo all‚Äôaltro lato delle dimensioni del modello. Selezioniamo un MobilinetV2 96x96 0.35, con immagini RGB in input.\n\nAnche con un modello pi√π grande, la precisione potrebbe essere migliore e la quantit√† di memoria necessaria per eseguire il modello aumenta di cinque volte, con una latenza che aumenta di sette volte.\n\nNotare che le prestazioni qui sono stimate con un dispositivo pi√π piccolo, l‚ÄôESP-EYE. L‚Äôinferenza effettiva con l‚ÄôESP32S3 dovrebbe essere migliore.\n\nPer migliorare il nostro modello, dovremo addestrare pi√π immagini.\nAnche se il nostro modello non ha migliorato la precisione, testiamo se l‚ÄôXIAO pu√≤ gestire un modello cos√¨ grande. Faremo un semplice test di inferenza con lo sketch Static Buffer.\nRidistribuiamo il modello. Se il compilatore EON √® abilitato quando si genera la libreria, la memoria totale necessaria per l‚Äôinferenza dovrebbe essere ridotta, ma ci√≤ non influisce sulla precisione.\n\n‚ö†Ô∏è Attenzione - Xiao ESP32S3 con PSRAM abilitata ha memoria sufficiente per eseguire l‚Äôinferenza, anche in un modello cos√¨ grande. Mantenere il Compilatore EON NOT ENABLED.\n\n\nFacendo un‚Äôinferenza con MobilinetV2 96x96 0.35, con immagini RGB in input, la latenza √® stata di 219 ms, il che √® ottimo per un modello cos√¨ grande.\n\nPer il test, possiamo addestrare di nuovo il modello, usando la versione pi√π piccola di MobileNet V2, con un alpha di 0,05. √à interessante che il risultato in accuratezza sia stato pi√π alto.\n\n\nNotare che la latenza stimata per un Arduino Portenta (o Nicla), in esecuzione con un clock di 480 MHz √® di 45 ms.\n\nDistribuendo il modello, abbiamo ottenuto un‚Äôinferenza di soli 135 ms, ricordando che XIAO funziona con met√† del clock utilizzato da Portenta/Nicla (240 MHz):",
    "crumbs": [
      "XIAO ESP32S3",
      "Classificazione delle Immagini"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/image_classification/image_classification.it.html#esecuzione-dellinferenza-su-sensecraft-web-toolkit",
    "href": "contents/labs/seeed/xiao_esp32s3/image_classification/image_classification.it.html#esecuzione-dellinferenza-su-sensecraft-web-toolkit",
    "title": "Classificazione delle Immagini",
    "section": "Esecuzione dell‚Äôinferenza su SenseCraft-Web-Toolkit",
    "text": "Esecuzione dell‚Äôinferenza su SenseCraft-Web-Toolkit\nUna limitazione significativa della visualizzazione dell‚Äôinferenza su Arduino IDE √® che non possiamo vedere su cosa punta la telecamera. Una buona alternativa √® SenseCraft-Web-Toolkit, uno strumento di distribuzione del modello visivo fornito da SSCMA(Seeed SenseCraft Model Assistant). Questo strumento consente di distribuire facilmente modelli su varie piattaforme tramite semplici operazioni. Lo strumento offre un‚Äôinterfaccia intuitiva e non richiede alcuna codifica.\nSeguire i seguenti passaggi per avviare SenseCraft-Web-Toolkit:\n\nAprire il sito web di SenseCraft-Web-Toolkit.\nCollega XIAO al computer:\n\n\nDopo aver collegato XIAO, selezionarlo come di seguito:\n\n\n\nSelezionare il dispositivo/Porta e premere [Connect]:\n\n\n\nSi possono provare diversi modelli di Computer Vision caricati in precedenza da Seeed Studio. Da provare e verificarli!\n\nNel nostro caso, useremo il pulsante blu in fondo alla pagina: [Upload Custom AI Model].\nMa prima, dobbiamo scaricare da Edge Impulse Studio il modello quantized.tflite.\n\nSi va sul proprio progetto su Edge Impulse Studio, oppure si clona questo:\n\n\nXIAO-ESP32S3-CAM-Fruits-vs-Veggies-v1-ESP-NN\n\n\nNella Dashboard, scaricare il modello (‚Äúblock output‚Äù): Transfer learning model - TensorFlow Lite (int8 quantized).\n\n\n\nSu SenseCraft-Web-Toolkit, usare il pulsante blu in fondo alla pagina: [Upload Custom AI Model]. Si aprir√† una finestra. Inserire il file del Modello scaricato sul computer da Edge Impulse Studio, scegliere un nome del modello e inserirlo con le etichette (ID: Object):\n\n\n\nNotare che si devono usare le etichette addestrate su EI Studio, inserendole in ordine alfabetico (nel nostro caso: apple, banana, potato).\n\nDopo alcuni secondi (o minuti), il modello verr√† caricato sul dispositivo e l‚Äôimmagine della telecamera apparir√† in tempo reale nel Preview Sector:\n\nIl risultato della classificazione sar√† in cima all‚Äôimmagine. Si pu√≤ anche selezionare la ‚ÄúConfidence‚Äù del cursore di inferenza Confidence.\nCliccando sul pulsante in alto (Device Log), si pu√≤ aprire un Serial Monitor per seguire l‚Äôinferenza, come fatto con l‚ÄôIDE Arduino:\n\nSu Device Log, si otterranno informazioni come:\n\n\nTempo di pre-elaborazione (cattura immagine e Crop): 4ms,\nTempo di inferenza (latenza modello): 106ms,\nTempo di Post-elaborazione (visualizzazione dell‚Äôimmagine e inclusione dei dati): 0ms,\nTensore di output (classi), ad esempio: [[89,0]]; dove 0 √® Apple (1 √® banana e 2 √® patata).\n\nEcco altri screenshot:",
    "crumbs": [
      "XIAO ESP32S3",
      "Classificazione delle Immagini"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/image_classification/image_classification.it.html#conclusione",
    "href": "contents/labs/seeed/xiao_esp32s3/image_classification/image_classification.it.html#conclusione",
    "title": "Classificazione delle Immagini",
    "section": "Conclusione",
    "text": "Conclusione\nXIAO ESP32S3 Sense √® molto flessibile, poco costoso e facile da programmare. Il progetto dimostra il potenziale di TinyML. La memoria non √® un problema; il dispositivo pu√≤ gestire molte attivit√† di post-elaborazione, tra cui la comunicazione.\nL‚Äôultima versione del codice si trova nel repository GitHub: XIAO-ESP32S3-Sense.",
    "crumbs": [
      "XIAO ESP32S3",
      "Classificazione delle Immagini"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/image_classification/image_classification.it.html#risorse",
    "href": "contents/labs/seeed/xiao_esp32s3/image_classification/image_classification.it.html#risorse",
    "title": "Classificazione delle Immagini",
    "section": "Risorse",
    "text": "Risorse\n\nCodici XIAO ESP32S3\nDataset\nProgetto Edge Impulse",
    "crumbs": [
      "XIAO ESP32S3",
      "Classificazione delle Immagini"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.it.html",
    "href": "contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.it.html",
    "title": "Rilevamento degli Oggetti",
    "section": "",
    "text": "Panoramica\nNell‚Äôultima sezione riguardante Computer Vision (CV) e XIAO ESP32S3, Classificazione delle immagini, abbiamo imparato come impostare e classificare le immagini con questa straordinaria scheda di sviluppo. Continuando il nostro viaggio con CV, esploreremo il Rilevamento degli oggetti sui microcontrollori.",
    "crumbs": [
      "XIAO ESP32S3",
      "Rilevamento degli Oggetti"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.it.html#panoramica",
    "href": "contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.it.html#panoramica",
    "title": "Rilevamento degli Oggetti",
    "section": "",
    "text": "Object Detection e Image Classification\nIl compito principale con i modelli di Classificazione delle immagini √® identificare la categoria di oggetti pi√π probabile presente su un‚Äôimmagine, ad esempio, per classificare tra un gatto o un cane, gli ‚Äúoggetti‚Äù dominanti in un‚Äôimmagine:\n\nMa cosa succede se non c‚Äô√® una categoria dominante nell‚Äôimmagine?\n\nUn modello di classificazione delle immagini identifica l‚Äôimmagine soprastante in modo completamente sbagliato come un ‚Äúashcan‚Äù, probabilmente a causa delle tonalit√† di colore.\n\nIl modello utilizzato nelle immagini precedenti √® MobileNet, che √® addestrato con un ampio set di dati, ImageNet, in esecuzione su un Raspberry Pi.\n\nPer risolvere questo problema, abbiamo bisogno di un altro tipo di modello, in cui non solo possono essere trovate pi√π categorie (o etichette), ma anche dove si trovano gli oggetti in una determinata immagine.\nCome possiamo immaginare, tali modelli sono molto pi√π complicati e pi√π grandi, ad esempio, MobileNetV2 SSD FPN-Lite 320x320, addestrato con il set di dati COCO. Questo modello di rilevamento degli oggetti pre-addestrato √® progettato per individuare fino a 10 oggetti all‚Äôinterno di un‚Äôimmagine, generando un riquadro di delimitazione per ogni oggetto rilevato. L‚Äôimmagine sottostante √® il risultato di un tale modello in esecuzione su un Raspberry Pi:\n\nI modelli utilizzati per il rilevamento di oggetti (come MobileNet SSD o YOLO) hanno solitamente dimensioni di diversi MB, il che √® OK per l‚Äôuso con Raspberry Pi ma non √® adatto per l‚Äôuso con dispositivi embedded, dove la RAM di solito ha, al massimo, pochi MB come nel caso di XIAO ESP32S3.\n\n\nUna soluzione innovativa per il Rilevamento di Oggetti: FOMO\nEdge Impulse ha lanciato nel 2022, FOMO (Faster Objects, More Objects), una nuova soluzione per eseguire il rilevamento di oggetti su dispositivi embedded, come Nicla Vision e Portenta (Cortex M7), su CPU Cortex M4F (serie Arduino Nano33 e OpenMV M4) e sui dispositivi Espressif ESP32 (ESP-CAM, ESP-EYE e XIAO ESP32S3 Sense).\nIn questo progetto pratico, esploreremo l‚ÄôObject Detection utilizzando FOMO.\n\nPer saperne di pi√π sulla FOMO, si pu√≤ leggere l‚Äôannuncio ufficiale su FOMO di Edge Impulse, dove Louis Moreau e Mat Kelcey spiegano in dettaglio come funziona.",
    "crumbs": [
      "XIAO ESP32S3",
      "Rilevamento degli Oggetti"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.it.html#obiettivo-del-progetto-di-object-detection",
    "href": "contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.it.html#obiettivo-del-progetto-di-object-detection",
    "title": "Rilevamento degli Oggetti",
    "section": "Obiettivo del Progetto di Object Detection",
    "text": "Obiettivo del Progetto di Object Detection\nTutti i progetti di apprendimento automatico devono iniziare con un obiettivo dettagliato. Supponiamo di trovarci in una struttura industriale o rurale e di dover smistare e contare arance (frutti) e in particolare rane (insetti).\n\nIn altre parole, dovremmo eseguire una classificazione multi-etichetta, in cui ogni immagine pu√≤ avere tre classi:\n\nBackground [Sfondo] (nessun oggetto)\nFruit\nBug\n\nEcco alcuni esempi di immagini non etichettate che dovremmo utilizzare per rilevare gli oggetti (frutti e insetti):\n\nSiamo interessati a quale oggetto √® presente nell‚Äôimmagine, alla sua posizione (centroide) e a quanti ne possiamo trovare su di essa. La dimensione dell‚Äôoggetto non viene rilevata con FOMO, come con MobileNet SSD o YOLO, in cui il Bounding Box √® uno degli output del modello.\nSvilupperemo il progetto utilizzando XIAO ESP32S3 per l‚Äôacquisizione di immagini e l‚Äôinferenza del modello. Il progetto ML verr√† sviluppato utilizzando Edge Impulse Studio. Ma prima di iniziare il progetto di ‚Äúobject detection‚Äù in Studio, creiamo un dataset grezzo (non etichettato) con immagini che contengono gli oggetti da rilevare.",
    "crumbs": [
      "XIAO ESP32S3",
      "Rilevamento degli Oggetti"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.it.html#raccolta-dati",
    "href": "contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.it.html#raccolta-dati",
    "title": "Rilevamento degli Oggetti",
    "section": "Raccolta Dati",
    "text": "Raccolta Dati\nSi possono catturare immagini usando XIAO, il telefono o altri dispositivi. Qui, useremo XIAO con codice dalla libreria Arduino IDE ESP32.\n\nRaccolta di Dataset con XIAO ESP32S3\nAprire Arduino IDE e selezionare la scheda XIAO_ESP32S3 (e la porta a cui √® collegata). Su File &gt; Examples &gt; ESP32 &gt; Camera, selezionare CameraWebServer.\nNel pannello BOARDS MANAGER, confermare di aver installato l‚Äôultimo pacchetto ‚Äústable‚Äù.\n\n‚ö†Ô∏è Attenzione\nLe versioni Alpha (ad esempio, 3.x-alpha) non funzionano correttamente con XIAO ed Edge Impulse. Utilizzare invece l‚Äôultima versione stabile (ad esempio, 2.0.11).\n\nSi devono anche commentare tutti i modelli di fotocamere, eccetto i pin del modello XIAO:\n#define CAMERA_MODEL_XIAO_ESP32S3 // Has PSRAM\nE su Tools, abilitare la PSRAM. Inserisci le credenziali wifi e caricare il codice sul dispositivo:\n\nSe il codice viene eseguito correttamente, si vedr√† l‚Äôindirizzo sul monitor seriale:\n\nCopiare l‚Äôindirizzo sul browser e attendere che la pagina venga caricata. Selezionare la risoluzione della telecamera (ad esempio, QVGA) e selezionare [START STREAM]. Attendi qualche secondo/minuto, a seconda della connessione. Si pu√≤ salvare un‚Äôimmagine nell‚Äôarea download del computer usando il pulsante [Save].\n\nEdge impulse suggerisce che gli oggetti dovrebbero essere simili per dimensione e non sovrapposti per prestazioni migliori. Questo va bene in una struttura industriale, dove la telecamera dovrebbe essere fissa, mantenendo la stessa distanza dagli oggetti da rilevare. Nonostante ci√≤, proveremo anche a usare dimensioni e posizioni miste per vedere il risultato.\n\nNon abbiamo bisogno di creare cartelle separate per le nostre immagini perch√© ognuna contiene pi√π etichette.\n\nSuggeriamo di usare circa 50 immagini per mescolare gli oggetti e variare il numero di ciascuno che appare sulla scena. Provare ad acquisire con diverse angolazioni, sfondi e condizioni di luce.\n\nLe immagini archiviate usano una dimensione del frame QVGA di 320x240 e RGB565 (formato pixel colore).\n\nDopo aver acquisito il dataset, [Stop Stream] e spostare le immagini in una cartella.",
    "crumbs": [
      "XIAO ESP32S3",
      "Rilevamento degli Oggetti"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.it.html#edge-impulse-studio",
    "href": "contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.it.html#edge-impulse-studio",
    "title": "Rilevamento degli Oggetti",
    "section": "Edge Impulse Studio",
    "text": "Edge Impulse Studio\n\nSetup del progetto\nSi va su Edge Impulse Studio, si inseriscono le proprie credenziali in Login (o si crea un account) e si avvia un nuovo progetto.\n\n\nQui, √® possibile clonare il progetto sviluppato per questa esercitazione pratica: XIAO-ESP32S3-Sense-Object_Detection\n\nNella dashboard del progetto, andare in basso e su Project info e selezionare Bounding boxes (object detection) e Espressif ESP-EYE (il pi√π simile alla nostra scheda) come Target Device:\n\n\n\nCaricamento dei dati non etichettati\nIn Studio, si va alla scheda Data acquisition e nella sezione UPLOAD DATA caricare i file acquisiti come cartella dal computer.\n\n\nSi pu√≤ lasciare che Studio divida automaticamente i dati tra ‚ÄúTrain‚Äù e ‚ÄúTest‚Äù o farlo manualmente. Caricheremo tutti come training.\n\n\nTutte le immagini non etichettate (47) sono state caricate, ma devono essere etichettate in modo appropriato prima di essere utilizzate come dataset del progetto. Studio ha uno strumento per questo scopo, che che si trova nel link Labeling queue (47).\nCi sono due modi per eseguire l‚Äôetichettatura assistita dall‚ÄôIA su Edge Impulse Studio (versione gratuita):\n\nUtilizzando yolov5\nTracciando di oggetti tra i frame\n\n\nEdge Impulse ha lanciato una funzione di auto-labeling per i clienti Enterprise, semplificando le attivit√† di etichettatura nei progetti di rilevamento degli oggetti.\n\nGli oggetti ordinari possono essere rapidamente identificati ed etichettati utilizzando una libreria esistente di modelli di rilevamento degli oggetti pre-addestrati da YOLOv5 (addestrati con il set di dati COCO). Ma poich√©, nel nostro caso, gli oggetti non fanno parte dei dataset COCO, dovremmo selezionare l‚Äôopzione di ‚Äútracking‚Äù [tracciamento] degli oggetti. Con questa opzione, una volta disegnati i bounding box ed etichettate le immagini in un frame, gli oggetti verranno tracciati automaticamente da un frame all‚Äôaltro, etichettando partially quelli nuovi (non tutti sono etichettati correttamente).\n\nSi pu√≤ usare EI uploader per importare i dati se si ha gi√† un dataset etichettato contenente dei ‚Äúbounding box‚Äù.\n\n\n\nEtichettatura del Dataset\nIniziando dalla prima immagine dei dati non etichettati, si usa il mouse per trascinare una casella attorno a un oggetto per aggiungere un‚Äôetichetta. Poi si clicca su Save labels per passare all‚Äôelemento successivo.\n\nSi continua con questo processo finch√© la coda non √® vuota. Alla fine, tutte le immagini dovrebbero avere gli oggetti etichettati come i campioni sottostanti:\n\nPoi, si esaminano i campioni etichettati nella scheda Data acquisition. Se una delle etichette √® errata, la si pu√≤ modificarla utilizzando il men√π tre puntini dopo il nome del campione:\n\nSi verr√† guidati a sostituire l‚Äôetichetta errata e a correggere il dataset.\n\n\n\nBilanciamento del dataset e suddivisione Train/Test\nDopo aver etichettato tutti i dati, ci siamo resi conto che la classe fruit aveva molti pi√π campioni di bug. Quindi, sono state raccolte 11 immagini di bug nuove e aggiuntive (per un totale di 58 immagini). Dopo averle etichettate, √® il momento di selezionare alcune immagini e spostarle nel dataset di test. Per farlo si usa il men√π a tre punti dopo il nome dell‚Äôimmagine. Sono state selezionate sei immagini, che rappresentano il 13% del set di dati totale.",
    "crumbs": [
      "XIAO ESP32S3",
      "Rilevamento degli Oggetti"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.it.html#impulse-design",
    "href": "contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.it.html#impulse-design",
    "title": "Rilevamento degli Oggetti",
    "section": "Impulse Design",
    "text": "Impulse Design\nIn questa fase, si deve definire come:\n\nIl Pre-processing consiste nel ridimensionare le singole immagini da 320 x 240 a 96 x 96 e nel ridurle (forma quadrata, senza ritaglio). In seguito, le immagini vengono convertite da RGB a scala di grigi.\nDesign a Model, in questo caso, ‚ÄúObject Detection‚Äù.\n\n\n\nPre-elaborazione di tutti i dataset\nIn questa sezione, selezionare Color depth come Grayscale, adatta per l‚Äôuso con modelli FOMO ed eseguire il ‚ÄúSave‚Äù dei parametri.\n\nStudio passa automaticamente alla sezione successiva, ‚ÄúGenerate features‚Äù, in cui tutti i campioni verranno pre-elaborati, generando un set di dati con singole immagini 96x96x1 o 9.216 feature.\n\nL‚Äôesploratore di feature mostra che tutti i campioni evidenziano una buona separazione dopo la generazione delle feature.\n\nAlcuni campioni sembrano stare nello spazio sbagliato, ma cliccandoci sopra si conferma la corretta etichettatura.",
    "crumbs": [
      "XIAO ESP32S3",
      "Rilevamento degli Oggetti"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.it.html#progettazione-addestramento-e-test-del-modello",
    "href": "contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.it.html#progettazione-addestramento-e-test-del-modello",
    "title": "Rilevamento degli Oggetti",
    "section": "Progettazione, Addestramento e Test del Modello",
    "text": "Progettazione, Addestramento e Test del Modello\nUseremo FOMO, un modello di rilevamento degli oggetti basato su MobileNetV2 (alpha 0.35) progettato per segmentare grossolanamente un‚Äôimmagine in una griglia di background rispetto a oggetti di interesse (in questo caso, scatole e ruote).\nFOMO √® un modello di apprendimento automatico innovativo per il rilevamento degli oggetti, che pu√≤ utilizzare fino a 30 volte meno energia e memoria rispetto ai modelli tradizionali come Mobilenet SSD e YOLOv5. FOMO pu√≤ funzionare su microcontrollori con meno di 200 KB di RAM. Il motivo principale per cui ci√≤ √® possibile √® che mentre altri modelli calcolano le dimensioni dell‚Äôoggetto disegnando un quadrato attorno ad esso (bounding box), FOMO ignora le dimensioni dell‚Äôimmagine, fornendo solo le informazioni su dove si trova l‚Äôoggetto nell‚Äôimmagine tramite le sue coordinate del centroide.\nCome funziona FOMO?\nFOMO prende l‚Äôimmagine in scala di grigi e la divide in blocchi di pixel usando un fattore di 8. Per l‚Äôinput di 96x96, la griglia √® 12x12 (96/8=12). Successivamente, FOMO eseguir√† un classificatore attraverso ogni blocco di pixel per calcolare la probabilit√† che ci sia un box o una ruota in ognuno di essi e, successivamente, determiner√† le regioni che hanno la pi√π alta probabilit√† di contenere l‚Äôoggetto (se un blocco di pixel non ha oggetti, verr√† classificato come background). Dalla sovrapposizione della regione finale, FOMO fornisce le coordinate (relative alle dimensioni dell‚Äôimmagine) del centroide di questa regione.\n\nPer l‚Äôaddestramento, dovremmo selezionare un modello pre-addestrato. Usiamo FOMO (Faster Objects, More Objects) MobileNetV2 0.35. Questo modello utilizza circa 250 KB di RAM e 80 KB di ROM (Flash), che si adatta bene alla nostra scheda.\n\nPer quanto riguarda gli iperparametri di training, il modello verr√† addestrato con:\n\nEpochs: 60\nBatch size: 32\nLearning Rate: 0.001.\n\nPer la convalida durante l‚Äôaddestramento, il 20% del set di dati (validation_dataset) verr√† risparmiato. Per il restante 80% (train_dataset), applicheremo il ‚ÄúData Augmentation‚Äù, che capovolger√† casualmente, cambier√† le dimensioni e la luminosit√† dell‚Äôimmagine e le ritaglier√†, aumentando artificialmente il numero di campioni sul set di dati per l‚Äôaddestramento.\nDi conseguenza, il modello termina con un punteggio F1 complessivo dell‚Äô85%, simile al risultato ottenuto utilizzando i dati di prova (83%).\n\nNotare che FOMO ha aggiunto automaticamente una terza etichetta di background [sfondo] ai due precedentemente definiti (box e wheel).\n\n\n\nNelle attivit√† di rilevamento di oggetti, l‚Äôaccuratezza non √® in genere la metrica di valutazione primaria. Il rilevamento di oggetti comporta la classificazione degli oggetti e la definizione di riquadri di delimitazione attorno a essi, il che lo rende un problema pi√π complesso della semplice classificazione. Il problema √® che non abbiamo il riquadro di delimitazione, solo i centroidi. In breve, usare l‚Äôaccuratezza come metrica potrebbe essere fuorviante e potrebbe non fornire una comprensione completa delle prestazioni del modello. Per questo motivo, useremo il punteggio F1.\n\n\nModello di test con ‚ÄúLive Classification‚Äù\nUna volta addestrato il nostro modello, possiamo testarlo utilizzando lo strumento Live Classification. Nella sezione corrispondente, cliccare sull‚Äôicona ‚ÄúConnect a development board‚Äù (una piccola MCU) e scansionare il codice QR col telefono.\n\nUna volta connesso, si pu√≤ usare lo smartphone per catturare immagini reali da testare col modello addestrato su Edge Impulse Studio.\n\nUna cosa da notare √® che il modello pu√≤ produrre falsi positivi e falsi negativi. Questo pu√≤ essere ridotto al minimo definendo una ‚ÄúConfidence Threshold‚Äù appropriata (usare il men√π ‚ÄúTre puntini‚Äù per la configurazione). Provare con 0,8 o pi√π.",
    "crumbs": [
      "XIAO ESP32S3",
      "Rilevamento degli Oggetti"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.it.html#deploying-del-modello-arduino-ide",
    "href": "contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.it.html#deploying-del-modello-arduino-ide",
    "title": "Rilevamento degli Oggetti",
    "section": "Deploying del Modello (Arduino IDE)",
    "text": "Deploying del Modello (Arduino IDE)\nSelezionare la Libreria Arduino e il modello Quantized (int8), abilitare il compilatore EON nella scheda Deploy e premere [Build].\n\nApri l‚ÄôArduino IDE e, in Sketch, andare su Include Library e aggiungere .ZIP Library. Selezionare il file che scaricato da Edge Impulse Studio e il gioco √® fatto!\n\nNella scheda Examples su Arduino IDE, si trova il codice di uno sketch (esp32 &gt; esp32_camera) sotto il nome del progetto.\n\nSi devono cambiare le righe dalla 32 alla 75, che definiscono il modello e i pin della telecamera, utilizzando i dati relativi al nostro modello. Copiare e incollare le righe seguenti, sostituendo le righe 32-75:\n#define PWDN_GPIO_NUM     -1 \n#define RESET_GPIO_NUM    -1 \n#define XCLK_GPIO_NUM     10 \n#define SIOD_GPIO_NUM     40 \n#define SIOC_GPIO_NUM     39\n#define Y9_GPIO_NUM       48 \n#define Y8_GPIO_NUM       11 \n#define Y7_GPIO_NUM       12 \n#define Y6_GPIO_NUM       14 \n#define Y5_GPIO_NUM       16 \n#define Y4_GPIO_NUM       18 \n#define Y3_GPIO_NUM       17 \n#define Y2_GPIO_NUM       15 \n#define VSYNC_GPIO_NUM    38 \n#define HREF_GPIO_NUM     47 \n#define PCLK_GPIO_NUM     13\nEcco il codice risultante:\n\nCaricare il codice sul XIAO ESP32S3 Sense e si √® pronti a iniziare a rilevare frutta e insetti. Si pu√≤ controllare il risultato su Serial Monitor.\nBackground\n\nFruits\n\nBugs\n\nSi noti che la latenza del modello √® di 143ms e il frame rate al secondo √® di circa 7 fps (simile a quanto ottenuto con il progetto Image Classification). Ci√≤ accade perch√© FOMO √® intelligentemente costruito su un modello CNN, non con un modello di rilevamento degli oggetti come SSD MobileNet. Ad esempio, quando si esegue un modello MobileNetV2 SSD FPN-Lite 320x320 su un Raspberry Pi 4, la latenza √® circa cinque volte superiore (circa 1,5 fps).",
    "crumbs": [
      "XIAO ESP32S3",
      "Rilevamento degli Oggetti"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.it.html#distribuzione-del-modello-sensecraft-web-toolkit",
    "href": "contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.it.html#distribuzione-del-modello-sensecraft-web-toolkit",
    "title": "Rilevamento degli Oggetti",
    "section": "Distribuzione del Modello (SenseCraft-Web-Toolkit)",
    "text": "Distribuzione del Modello (SenseCraft-Web-Toolkit)\nCome discusso nel capitolo Image Classification, verificare l‚Äôinferenza con i modelli di immagine su Arduino IDE √® molto impegnativo perch√© non possiamo vedere su cosa punta la telecamera. Di nuovo, utilizziamo SenseCraft-Web Toolkit.\nSeguire i seguenti passaggi per avviare SenseCraft-Web-Toolkit:\n\nAprire il sito web di SenseCraft-Web-Toolkit.\nCollega XIAO al computer:\n\n\nDopo aver collegato XIAO, selezionarlo come di seguito:\n\n\n\nSelezionare il dispositivo/Porta e premere [Connect]:\n\n\n\nSi possono provare diversi modelli di Computer Vision caricati in precedenza da Seeed Studio. Da provare e verificarli!\n\nNel nostro caso, useremo il pulsante blu in fondo alla pagina: [Upload Custom AI Model].\nMa prima, dobbiamo scaricare da Edge Impulse Studio il modello quantized .tflite.\n\nSi va sul proprio progetto su Edge Impulse Studio, oppure si clona questo:\n\n\nXIAO-ESP32S3-CAM-Fruits-vs-Veggies-v1-ESP-NN\n\n\nSu Dashboard, scaricare il modello (‚Äúblock output‚Äù): Object Detection model - TensorFlow Lite (int8 quantized)\n\n\n\nSu SenseCraft-Web-Toolkit, usare il pulsante blu in fondo alla pagina: [Upload Custom AI Model]. Si aprir√† una finestra. Inserire il file del Modello scaricato sul computer da Edge Impulse Studio, scegliere un nome del modello e inserirlo con le etichette (ID: Object):\n\n\n\nNotare che si devono utilizzare le etichette apprese su EI Studio e inserirle in ordine alfabetico (nel nostro caso, background, bug, fruit).\n\nDopo alcuni secondi (o minuti), il modello verr√† caricato sul dispositivo e l‚Äôimmagine della telecamera apparir√† in tempo reale nel Preview Sector:\n\nGli oggetti rilevati saranno contrassegnati (il centroide). √à possibile selezionare l‚Äôaffidabilit√† del cursore di inferenza Confidence e IoU, che viene utilizzata per valutare l‚Äôaccuratezza delle ‚Äúbounding box‚Äù previste rispetto a quelle vere.\nCliccando sul pulsante in alto (Device Log), si pu√≤ aprire un Serial Monitor per seguire l‚Äôinferenza, come abbiamo fatto con l‚ÄôIDE Arduino.\n\nSu Device Log, si otterranno informazioni come:\n\nTempo di pre-elaborazione (acquisizione dell‚Äôimmagine e Crop): 3 ms,\nTempo di inferenza (latenza del modello): 115 ms,\nTempo di post-elaborazione (visualizzazione dell‚Äôimmagine e marcatura degli oggetti): 1 ms.\nTensore di output (box), ad esempio, uno dei box: [[30,150, 20, 20, 97, 2]]; dove 30,150, 20, 20 sono le coordinate della casella (intorno al centroide); 97 √® il risultato dell‚Äôinferenza e 2 √® la classe (in questo caso 2: frutto).\n\n\nNotare che nell‚Äôesempio precedente, abbiamo ottenuto 5 caselle perch√© nessuno dei frutti ha ottenuto 3 centroidi. Una soluzione sar√† la post-elaborazione, dove possiamo aggregare centroidi vicini in uno.\n\nEcco altri screenshot:",
    "crumbs": [
      "XIAO ESP32S3",
      "Rilevamento degli Oggetti"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.it.html#conclusione",
    "href": "contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.it.html#conclusione",
    "title": "Rilevamento degli Oggetti",
    "section": "Conclusione",
    "text": "Conclusione\nFOMO √® un salto significativo nello spazio di elaborazione delle immagini, come hanno affermato Louis Moreau e Mat Kelcey durante il suo lancio nel 2022:\n\nFOMO √® un algoritmo rivoluzionario che porta per la prima volta il rilevamento, il tracciamento e il conteggio degli oggetti in tempo reale sui microcontrollori.\n\nEsistono molteplici possibilit√† per esplorare il rilevamento di oggetti (e, pi√π precisamente, il loro conteggio) su dispositivi embedded.",
    "crumbs": [
      "XIAO ESP32S3",
      "Rilevamento degli Oggetti"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.it.html#risorse",
    "href": "contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.it.html#risorse",
    "title": "Rilevamento degli Oggetti",
    "section": "Risorse",
    "text": "Risorse\n\nProgetto Edge Impulse",
    "crumbs": [
      "XIAO ESP32S3",
      "Rilevamento degli Oggetti"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/kws/kws.it.html",
    "href": "contents/labs/seeed/xiao_esp32s3/kws/kws.it.html",
    "title": "Keyword Spotting (KWS)",
    "section": "",
    "text": "Panoramica\nKeyword Spotting (KWS) √® parte integrante di molti sistemi di riconoscimento vocale, consentendo ai dispositivi di rispondere a parole o frasi specifiche. Sebbene questa tecnologia sia alla base di dispositivi popolari come Google Assistant o Amazon Alexa, √® ugualmente applicabile e realizzabile su dispositivi pi√π piccoli e a basso consumo. Questo lab guider√† nell‚Äôimplementazione di un sistema KWS utilizzando TinyML sulla scheda microcontrollore XIAO ESP32S3.\nThe XIAO ESP32S3, dotato del chip ESP32-S3 di Espressif, √® un microcontrollore compatto e potente che offre un processore Xtensa LX7 dual-core, Wi-Fi integrato e Bluetooth. Il suo equilibrio di potenza di calcolo, efficienza energetica e connettivit√† versatile lo rendono una piattaforma fantastica per le applicazioni TinyML. Inoltre, con la sua scheda di espansione, avremo accesso alla parte ‚Äúsense‚Äù del dispositivo, che ha una fotocamera OV2640 da 1600x1200, uno slot per schede SD e un microfono digitale. Il microfono integrato e la scheda SD saranno essenziali in questo progetto.\nUtilizzeremo Edge Impulse Studio, una piattaforma potente e intuitiva che semplifica la creazione e l‚Äôimplementazione di modelli di apprendimento automatico su dispositivi edge. Addestreremo un modello KWS passo dopo passo, ottimizzandolo e distribuendolo su XIAO ESP32S3 Sense.\nIl nostro modello sar√† progettato per riconoscere parole chiave che possono attivare il ‚Äúwake-up‚Äù [risveglio] del dispositivo o azioni specifiche (nel caso di ‚ÄúYES‚Äù), dando vita a progetti con comandi vocali.\nSfruttando la nostra esperienza con TensorFlow Lite per microcontrollori (il motore ‚Äúsotto il cofano‚Äù di EI Studio), creeremo un sistema KWS in grado di apprendere in tempo reale sul dispositivo.\nProcedendo nel lab, analizzeremo ogni fase del processo, dalla raccolta e preparazione dei dati all‚Äôaddestramento e distribuzione del modello, per fornire una comprensione completa dell‚Äôimplementazione di un sistema KWS su un microcontrollore.",
    "crumbs": [
      "XIAO ESP32S3",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/kws/kws.it.html#panoramica",
    "href": "contents/labs/seeed/xiao_esp32s3/kws/kws.it.html#panoramica",
    "title": "Keyword Spotting (KWS)",
    "section": "",
    "text": "Come funziona un assistente vocale?\nKeyword Spotting (KWS) √® fondamentale per molti assistenti vocali, consentendo ai dispositivi di rispondere a parole o frasi specifiche. Per iniziare, √® essenziale rendersi conto che gli assistenti vocali sul mercato, come Google Home o Amazon Echo-Dot, reagiscono agli umani solo quando vengono ‚Äúsvegliati‚Äù da parole chiave specifiche come ‚ÄúHey Google‚Äù sul primo e ‚ÄúAlexa‚Äù sul secondo.\n\nIn altre parole, il riconoscimento dei comandi vocali si basa su un modello multi-fase o Cascade Detection.\n\nFase 1: Un microprocessore pi√π piccolo all‚Äôinterno dell‚ÄôEcho Dot o Google Home ascolta continuamente il suono, in attesa che venga individuata la parola chiave. Per tale rilevamento, viene utilizzato un modello TinyML all‚Äôedge (applicazione KWS).\nFase 2: Solo quando vengono attivati dall‚Äôapplicazione KWS nella Fase 1, i dati vengono inviati al cloud ed elaborati su un modello pi√π grande.\nIl video qui sotto mostra un esempio in cui si emula un Google Assistant su un Raspberry Pi (Fase 2), con un Arduino Nano 33 BLE come dispositivo tinyML (Fase 1).\n\n\n\nPer approfondire il progetto completo, guardare il tutorial: Building an Intelligent Voice Assistant From Scratch.\n\nIn questo lab, ci concentreremo sulla Fase 1 (KWS o Keyword Spotting), dove utilizzeremo XIAO ESP2S3 Sense, che ha un microfono digitale per individuare la parola chiave.\n\n\nIl progetto KWS\nIl diagramma seguente dar√† un‚Äôidea di come dovrebbe funzionare l‚Äôapplicazione KWS finale (durante l‚Äôinferenza):\n\nLa nostra applicazione KWS riconoscer√† quattro classi di suono:\n\nYES (Keyword 1)\nNO (Keyword 2)\nNOISE [rumore] (nessuna parola chiave pronunciata, √® presente solo rumore di fondo)\nUNKNOW (un mix di parole diverse da YES e NO)\n\n\nFacoltativamente, per progetti reali, si consiglia sempre di includere parole diverse dalle parole chiave, come ‚ÄúRumore‚Äù (o Sfondo) e ‚ÄúSconosciuto‚Äù.\n\n\n\nIl Flusso di Lavoro del Machine Learning\nIl componente principale dell‚Äôapplicazione KWS √® il suo modello. Quindi, dobbiamo addestrare un modello del genere con le nostre parole chiave specifiche, rumore e altre parole (lo ‚Äúunknown‚Äù):",
    "crumbs": [
      "XIAO ESP32S3",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/kws/kws.it.html#il-dataset",
    "href": "contents/labs/seeed/xiao_esp32s3/kws/kws.it.html#il-dataset",
    "title": "Keyword Spotting (KWS)",
    "section": "Il Dataset",
    "text": "Il Dataset\nIl componente critico del flusso di lavoro di apprendimento automatico √® il dataset. Una volta decise le parole chiave specifiche (YES e NO), possiamo sfruttare il dataset sviluppato da Pete Warden, ‚ÄúSpeech Commands: A Dataset for Limited-Vocabulary Speech Recognition‚Äú. Questo set di dati ha 35 parole chiave (con +1.000 campioni ciascuna), come yes, no, stop e go. In altre parole, possiamo ottenere 1.500 campioni di yes e no.\nSi pu√≤ scaricare una piccola parte del dataset da Edge Studio (Keyword spotting pre-built dataset), che include campioni dalle quattro classi che utilizzeremo in questo progetto: yes, no, noise e background. Per farlo, si seguono i passaggi seguenti:\n\nDownload del dataset delle parole chiave.\nDecomprimere il file in una posizione a scelta.\n\nSebbene disponiamo di molti dati dal dataset di Pete, √® consigliabile raccogliere alcune parole pronunciate da noi. Lavorando con gli accelerometri, creare un dataset con dati acquisiti dallo stesso tipo di sensore era essenziale. Nel caso del suono, √® diverso perch√© ci√≤ che classificheremo sono, in realt√†, dati audio.\n\nLa differenza fondamentale tra suono e audio √® la loro forma di energia. Il suono √® energia delle onde meccaniche (onde sonore longitudinali) che si propagano attraverso un mezzo causando variazioni di pressione all‚Äôinterno del mezzo. L‚Äôaudio √® costituito da energia elettrica (segnali analogici o digitali) che rappresentano il suono elettricamente.\n\nLe onde sonore dovrebbero essere convertite in dati audio quando pronunciamo una parola chiave. La conversione dovrebbe essere eseguita campionando il segnale generato dal microfono a 16 KHz con una profondit√† di 16 bit.\nQuindi, qualsiasi dispositivo in grado di generare dati audio con questa specifica di base (16Khz/16bit) funzioner√† bene. Come dispositivo, possiamo usare il XIAO ESP32S3 Sense appropriato, un computer o persino il telefono cellulare.\n\nAcquisizione di Dati Audio online con Edge Impulse e uno smartphone\nNel lab ‚ÄúMotion Classification‚Äù e ‚ÄúAnomaly Detection‚Äù, colleghiamo il nostro dispositivo direttamente a Edge Impulse Studio per l‚Äôacquisizione dei dati (con una frequenza di campionamento da 50 Hz a 100 Hz). Per una frequenza cos√¨ bassa, potremmo usare la funzione EI CLI Data Forwarder, ma secondo Jan Jongboom, CTO di Edge Impulse, l‚Äôaudio (16 KHz)* √® troppo veloce perch√© il data forwarder possa essere acquisito. Quindi, una volta che i dati digitali sono stati acquisiti dal microfono, possiamo trasformarli in un file WAV* da inviare a Studio tramite Data Uploader (lo stesso che faremo con il set di dati di Pete).\n\nSe vogliamo raccogliere dati audio direttamente sullo Studio, possiamo usare qualsiasi smartphone connesso online. Non esploreremo questa opzione qui, ma si pu√≤ facilmente seguire la documentazione EI.\n\n\nAcquisizione (offline) di dati audio con XIAO ESP32S3 Sense\nIl microfono integrato √® il MSM261D3526H1CPM, un microfono MEMS con uscita digitale PDM con Multi-mode. Internamente, √® collegato all‚ÄôESP32S3 tramite un bus I2S utilizzando i pin IO41 (Clock) e IO41 (Data).\n\nCos‚Äô√® I2S?\nI2S, o Inter-IC Sound, √® un protocollo standard per la trasmissione di audio digitale da un dispositivo a un altro. Inizialmente √® stato sviluppato da Philips Semiconductor (ora NXP Semiconductors). √à comunemente utilizzato in dispositivi audio come processori di segnale digitale, processori audio digitali e, pi√π di recente, microcontrollori con capacit√† audio digitali (il nostro caso qui).\nIl protocollo I2S √® composto da almeno tre linee:\n\n1. Linea di clock di bit (o seriale) (BCLK o CLK): Questa linea si attiva/disattiva per indicare l‚Äôinizio di un nuovo bit di dati (pin IO42).\n2. Linea di ‚ÄúWord select (WS)‚Äù: Questa linea si attiva/disattiva per indicare l‚Äôinizio di una nuova parola (canale sinistro o canale destro). La frequenza del clock di Word select (WS) definisce la frequenza di campionamento. Nel nostro caso, L/R sul microfono √® impostato su massa, il che significa che utilizzeremo solo il canale sinistro (mono).\n3. Data line (SD): Questa linea trasporta i dati audio (pin IO41)\nIn un flusso di dati I2S, i dati vengono inviati come una sequenza di frame, ciascuno contenente una parola del canale sinistro e una parola del canale destro. Ci√≤ rende I2S particolarmente adatto per la trasmissione di dati audio stereo. Tuttavia, pu√≤ anche essere utilizzato per audio mono o multicanale con linee dati aggiuntive.\nCominciamo a capire come catturare dati grezzi usando il microfono. Su va sul progetto GitHube si scarica lo sketch: XIAOEsp2s3_Mic_Test:\n/*\n  XIAO ESP32S3 Simple Mic Test\n*/\n\n#include &lt;I2S.h&gt;\n\nvoid setup() {\n  Serial.begin(115200);\n  while (!Serial) {\n  }\n\n\n  // start I2S at 16 kHz with 16-bits per sample\n  I2S.setAllPins(-1, 42, 41, -1, -1);\n  if (!I2S.begin(PDM_MONO_MODE, 16000, 16)) {\n    Serial.println(\"Failed to initialize I2S!\");\n    while (1); // do nothing\n  }\n}\n\nvoid loop() {\n  // read a sample\n  int sample = I2S.read();\n\n  if (sample && sample != -1 && sample != 1) {\n    Serial.println(sample);\n  }\n}\nQuesto codice √® un semplice test del microfono per XIAO ESP32S3 che utilizza l‚Äôinterfaccia I2S (Inter-IC Sound). Imposta l‚Äôinterfaccia I2S per catturare dati audio a una frequenza di campionamento di 16 kHz con 16 bit per campione e quindi legge continuamente i campioni dal microfono e li stampa sul monitor seriale.\nAnalizziamo le parti principali del codice:\n\nInclude la libreria I2S: Questa libreria fornisce funzioni per configurare e utilizzare l‚Äôinterfaccia I2S, che √® uno standard per la connessione di dispositivi audio digitali.\nI2S.setAllPins(-1, 42, 41, -1, -1): Imposta i pin I2S. I parametri sono (-1, 42, 41, -1, -1), dove il secondo parametro (42) √® il PIN per il clock I2S (CLK) e il terzo parametro (41) √® il PIN per la linea dati I2S (DATA). Gli altri parametri sono impostati su -1, il che significa che quei pin non vengono utilizzati.\nI2S.begin(PDM_MONO_MODE, 16000, 16): Inizializza l‚Äôinterfaccia I2S in modalit√† mono Pulse Density Modulation (PDM), con una frequenza di campionamento di 16 kHz e 16 bit per campione. Se l‚Äôinizializzazione fallisce, viene stampato un messaggio di errore e il programma si arresta.\nint sample = I2S.read(): Legge un campione audio dall‚Äôinterfaccia I2S.\n\nSe il campione √® valido, viene stampato sul monitor seriale e sul plotter.\nDi seguito √® riportato un test ‚Äúsussurrato‚Äù in due toni diversi.\n\n\n\nSalvare campioni audio registrati (dataset) come file audio .wav su una scheda microSD\nUtilizziamo il lettore di schede SD integrato per salvare i file audio .wav; dobbiamo prima abilitare la PSRAM XIAO.\n\nESP32-S3 ha solo poche centinaia di kilobyte di RAM interna sul chip MCU. Potrebbe essere insufficiente per alcuni scopi, quindi ESP32-S3 pu√≤ utilizzare fino a 16 MB di PSRAM esterna (Psuedostatic RAM) collegata in parallelo con il chip flash SPI. La memoria esterna √® incorporata nella mappa di memoria e, con alcune restrizioni, √® utilizzabile allo stesso modo della RAM dati interna.\n\nPer iniziare, si inserisce la scheda SD sullo XIAO come mostrato nella foto qui sotto (la scheda SD deve essere formattata in FAT32).\n\nAttivare la funzione PSRAM del chip ESP-32 (Arduino IDE): Tools&gt;PSRAM: ‚ÄúOPI PSRAM‚Äù&gt;OPI PSRAM\n\n\nScaricare lo sketch Wav_Record_dataset, che si trova sul GitHub del progetto.\n\nQuesto codice registra l‚Äôaudio usando l‚Äôinterfaccia I2S della scheda Seeed XIAO ESP32S3 Sense, salva la registrazione come file .wav su una scheda SD e consente il controllo del processo di registrazione tramite comandi inviati dal monitor seriale. Il nome del file audio √® personalizzabile (dovrebbe essere le etichette della classe da usare con la formazione) e possono essere effettuate pi√π registrazioni, ciascuna salvata in un nuovo file. Il codice include anche funzionalit√† per aumentare il volume delle registrazioni.\nAnalizziamo le parti pi√π essenziali:\n#include &lt;I2S.h&gt;\n#include \"FS.h\"\n#include \"SD.h\"\n#include \"SPI.h\"\nQueste sono le librerie necessarie per il programma. I2S.h consente l‚Äôinput audio, FS.h fornisce capacit√† di gestione del file system, SD.h consente al programma di interagire con una scheda SD e SPI.h gestisce la comunicazione SPI con la scheda SD.\n#define RECORD_TIME   10  \n#define SAMPLE_RATE 16000U\n#define SAMPLE_BITS 16\n#define WAV_HEADER_SIZE 44\n#define VOLUME_GAIN 2\nQui vengono definite varie costanti per il programma.\n\nRECORD_TIME specifica la lunghezza della registrazione audio in secondi.\nSAMPLE_RATE e SAMPLE_BITS definiscono la qualit√† audio della registrazione.\nWAV_HEADER_SIZE specifica la dimensione dell‚Äôintestazione del file .wav.\nVOLUME_GAIN viene utilizzato per aumentare il volume della registrazione.\n\nint fileNumber = 1;\nString baseFileName;\nbool isRecording = false;\nQueste variabili tengono traccia del numero di file corrente (per creare nomi di file univoci), del nome del file di base e se il sistema sta attualmente registrando.\nvoid setup() {\n  Serial.begin(115200);\n  while (!Serial);\n  \n  I2S.setAllPins(-1, 42, 41, -1, -1);\n  if (!I2S.begin(PDM_MONO_MODE, SAMPLE_RATE, SAMPLE_BITS)) {\n    Serial.println(\"Failed to initialize I2S!\");\n    while (1);\n  }\n\n  \n  if(!SD.begin(21)){\n    Serial.println(\"Failed to mount SD Card!\");\n    while (1);\n  }\n\n  Serial.printf(\"Enter with the label name\\n\");\n}\nLa funzione di configurazione inizializza la comunicazione seriale, l‚Äôinterfaccia I2S per l‚Äôingresso audio e l‚Äôinterfaccia della scheda SD. Se l‚ÄôI2S non si inizializza o la scheda SD non riesce a essere montata, verr√† visualizzato un messaggio di errore e l‚Äôesecuzione verr√† interrotta.\nvoid loop() {\n  if (Serial.available() &gt; 0) {\n    String command = Serial.readStringUntil('\\n');\n    command.trim();\n    if (command == \"rec\") {\n      isRecording = true;\n    } else {\n      baseFileName = command;\n      fileNumber = 1; //reset file number each time a new basefile name is set\n      Serial.printf(\"Send rec for starting recording label \\n\");\n    }\n\n  }\n\n  if (isRecording && baseFileName != \"\") {\n    String fileName = \"/\" + baseFileName + \".\" + String(fileNumber) + \".wav\";\n    fileNumber++;\n    record_wav(fileName);\n    delay(1000); // delay to avoid recording multiple files at once\n    isRecording = false;\n  }\n}\nNel ciclo principale, il programma attende un comando dal monitor seriale. Se il comando √® rec, il programma inizia a registrare. Altrimenti, si presume che il comando sia il nome base per i file .wav. Se sta attualmente registrando e un nome file base √® impostato, registra l‚Äôaudio e lo salva come file .wav. I nomi dei file vengono generati aggiungendo il numero al nome file base.\nvoid record_wav(String fileName)\n{\n  ...\n  \n  File file = SD.open(fileName.c_str(), FILE_WRITE);\n  ...\n  rec_buffer = (uint8_t *)ps_malloc(record_size);\n  ...\n\n  esp_i2s::i2s_read(esp_i2s::I2S_NUM_0, \n                    rec_buffer, \n                    record_size, \n                    &sample_size, \n                    portMAX_DELAY);\n  ...\n}\nQuesta funzione registra l‚Äôaudio e lo salva come file .wav con il nome specificato. Inizia inizializzando le variabili sample_size e record_size. record_size viene calcolato in base alla frequenza di campionamento, alla dimensione e al tempo di registrazione desiderato. Analizziamo le sezioni essenziali;\nFile file = SD.open(fileName.c_str(), FILE_WRITE);\n// Write the header to the WAV file\nuint8_t wav_header[WAV_HEADER_SIZE];\ngenerate_wav_header(wav_header, record_size, SAMPLE_RATE);\nfile.write(wav_header, WAV_HEADER_SIZE);\nQuesta sezione del codice apre il file sulla scheda SD per la scrittura e poi genera l‚Äôintestazione del file .wav utilizzando la funzione generate_wav_header. Quindi scrive l‚Äôintestazione nel file.\n// PSRAM malloc for recording\nrec_buffer = (uint8_t *)ps_malloc(record_size);\nif (rec_buffer == NULL) {\n  Serial.printf(\"malloc failed!\\n\");\n  while(1) ;\n}\nSerial.printf(\"Buffer: %d bytes\\n\", ESP.getPsramSize() - ESP.getFreePsram());\nLa funzione ps_malloc alloca memoria nella PSRAM per la registrazione. Se l‚Äôallocazione fallisce (ad esempio, rec_buffer √® NULL), stampa un messaggio di errore e interrompe l‚Äôesecuzione.\n// Start recording\nesp_i2s::i2s_read(esp_i2s::I2S_NUM_0, \n         rec_buffer, \n         record_size, \n         &sample_size, \n         portMAX_DELAY);\nif (sample_size == 0) {\n  Serial.printf(\"Record Failed!\\n\");\n} else {\n    Serial.printf(\"Record %d bytes\\n\", sample_size);\n  }\nLa funzione i2s_read legge i dati audio dal microfono in rec_buffer. Stampa un messaggio di errore se non vengono letti dati (sample_size √® 0).\n// Increase volume\nfor (uint32_t i = 0; i &lt; sample_size; i += SAMPLE_BITS/8) {\n  (*(uint16_t *)(rec_buffer+i)) &lt;&lt;= VOLUME_GAIN;\n}\nQuesta sezione del codice aumenta il volume di registrazione spostando i valori del campione di VOLUME_GAIN.\n// Write data to the WAV file\nSerial.printf(\"Writing to the file ...\\n\");\nif (file.write(rec_buffer, record_size) != record_size)\n  Serial.printf(\"Write file Failed!\\n\");\n\nfree(rec_buffer);\nfile.close();\nSerial.printf(\"Recording complete: \\n\");\nSerial.printf(\"Send rec for a new sample or enter a new label\\n\\n\");\nInfine, i dati audio vengono scritti nel file .wav. Se l‚Äôoperazione di scrittura fallisce, viene stampato un messaggio di errore. Dopo la scrittura, la memoria allocata per rec_buffer viene liberata e il file viene chiuso. La funzione termina stampando un messaggio di completamento e chiedendo all‚Äôutente di inviare un nuovo comando.\nvoid generate_wav_header(uint8_t *wav_header,  \n             uint32_t wav_size, \n             uint32_t sample_rate)\n{\n  ...\n  memcpy(wav_header, set_wav_header, sizeof(set_wav_header));\n}\nLa funzione generate_wav_header crea un‚Äôintestazione di file .wav in base ai parametri (wav_size e sample_rate). Genera un array di byte in base al formato di file .wav, che include campi per la dimensione del file, il formato audio, il numero di canali, la frequenza di campionamento, la frequenza di byte, l‚Äôallineamento dei blocchi, i bit per campione e la dimensione dei dati. L‚Äôintestazione generata viene poi copiata nell‚Äôarray wav_header passato alla funzione.\nOra, caricare il codice su XIAO e ottenere campioni dalle parole chiave (yes e no). Si possono anche catturare rumore e altre parole.\nIl monitor seriale chieder√† di ricevere l‚Äôetichetta da registrare.\n\nInvia l‚Äôetichetta (ad esempio, yes). Il programma attender√† un altro comando: rec\n\nE il programma inizier√† a registrare nuovi campioni ogni volta che viene inviato un comando rec. I file verranno salvati come yes.1.wav, yes.2.wav, yes.3.wav, ecc., finch√© non verr√† inviata una nuova etichetta (ad esempio, no). In questo caso, si deve inviare il comando rec per ogni nuovo campione, che verr√† salvato come no.1.wav, no.2.wav, no.3.wav, ecc.\n\nAlla fine, otterremo i file salvati sulla scheda SD.\n\nI file sono pronti per essere caricati su Edge Impulse Studio\n\n\nApp di Acquisizione dei Dati Audio (offline)\nIn alternativa, si pu√≤ anche usare il PC o lo smartphone per acquisire dati audio con una frequenza di campionamento di 16 KHz e una profondit√† di bit di 16 bit. Una buona app per questo √® Voice Recorder Pro (IOS). Le registrazioni si devono salvare come file .wav e inviarle al computer.\n\n\nNotare che qualsiasi app, come Audacity, pu√≤ essere usata per la registrazione audio o anche il computer.",
    "crumbs": [
      "XIAO ESP32S3",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/kws/kws.it.html#modello-di-training-con-edge-impulse-studio",
    "href": "contents/labs/seeed/xiao_esp32s3/kws/kws.it.html#modello-di-training-con-edge-impulse-studio",
    "title": "Keyword Spotting (KWS)",
    "section": "Modello di training con Edge Impulse Studio",
    "text": "Modello di training con Edge Impulse Studio\n\nCaricamento dei Dati\nQuando il dataset grezzo √® definito e raccolto (dataset di Pete + parole chiave registrate), dovremmo avviare un nuovo progetto in Edge Impulse Studio:\n\nUna volta creato il progetto, selezionare lo strumento ‚ÄúUpload Existing Data‚Äù nella sezione ‚ÄúAcquisition section‚Äù. Si scelgono i file da caricare:\n\nE si caricano nello Studio (si possono dividere automaticamente i dati in train/test). Ripetere per tutte le classi e tutti i dati grezzi.\n\nI campioni appariranno ora nella sezione ‚ÄúData acquisition‚Äù.\n\nTutti i dati sul dataset di Pete hanno una lunghezza di 1s, ma i campioni registrati nella sezione precedente hanno 10s e devono essere divisi in campioni da 1s per essere compatibili.\nCliccare sui tre punti dopo il nome del campione e selezionare ‚ÄúSplit sample‚Äù.\n\nUna volta all‚Äôinterno dello strumento, dividere i dati in record da 1 secondo. Se necessario, aggiungere o rimuovere segmenti:\n\nQuesta procedura deve essere ripetuta per tutti i campioni.\n\nNota: Per file audio pi√π lunghi (minuti), prima si dividono in segmenti da 10 secondi e poi usa di nuovo lo strumento per ottenere le divisioni finali da 1 secondo.\n\nSupponiamo di non dividere automaticamente i dati in train/test durante il caricamento. In tal caso, possiamo farlo manualmente (utilizzando il men√π a tre punti, spostando i campioni singolarmente) o utilizzando Perform Train / Test Split su Dashboard - Danger Zone.\n\nPossiamo facoltativamente controllare tutti i dataset utilizzando la scheda Data Explorer.\n\n\n\nCreazione di Impulse (Pre-Process / Definizione del Modello)\nUn impulse prende dati grezzi, usa l‚Äôelaborazione del segnale per estrarre le feature e poi usa un blocco di apprendimento per classificare nuovi dati.\n\nPer prima cosa, prenderemo i dati con una finestra di 1 secondo, aumentando i dati, facendo scorrere quella finestra ogni 500 ms. Notare che √® impostata l‚Äôopzione Zero-pad data. √à essenziale riempire con zeri i campioni inferiori a 1 secondo (in alcuni casi, si √® ridotta la finestra di 1000 ms sullo strumento di divisione per evitare rumori e picchi).\nOgni campione audio di 1 secondo dovrebbe essere pre-elaborato e convertito in un‚Äôimmagine (ad esempio, 13 x 49 x 1). Useremo MFCC, che estrae le caratteristiche dai segnali audio usando Mel Frequency Cepstral Coefficients, che sono ottimi per la voce umana.\n\nSuccessivamente, selezioniamo KERAS per la classificazione e costruiamo il nostro modello da zero eseguendo la classificazione delle immagini tramite la rete neurale convoluzionale).\n\n\nPre-elaborazione (MFCC)\nIl passo successivo √® creare le immagini da addestrare nella fase successiva:\nPossiamo mantenere i valori dei parametri di default o sfruttare l‚Äôopzione DSP Autotuneparameters, cosa che faremo.\n\nIl risultato non impiegher√† molta memoria per pre-elaborare i dati (solo 16 KB). Tuttavia, il tempo di elaborazione stimato √® elevato, 675 ms per un Espressif ESP-EYE (il riferimento pi√π vicino disponibile), con un clock di 240 KHz (lo stesso del nostro dispositivo), ma con una CPU pi√π piccola (XTensa LX6, rispetto alla LX7 sull‚ÄôESP32S). Il tempo di inferenza reale dovrebbe essere inferiore.\nSupponiamo di dover ridurre il tempo di inferenza in seguito. In tal caso, dovremmo tornare alla fase di pre-elaborazione e, ad esempio, ridurre la lunghezza FFT a 256, modificare il numero di coefficienti o un altro parametro.\nPer ora, manteniamo i parametri definiti dallo strumento Autotuning. Salviamo i parametri e generiamo le funzionalit√†.\n\n\nPer andare oltre con la conversione di dati seriali temporali in immagini usando FFT, spettrogramma, ecc., si pu√≤ giocare con questo CoLab: Audio Raw Data Analysis.\n\n\n\nProgettazione e Addestramento del Modello\nUseremo un modello di Rete Neurale Convoluzionale (CNN). L‚Äôarchitettura di base √® definita con due blocchi di Conv1D + MaxPooling (rispettivamente con 8 e 16 neuroni) e un Dropout di 0,25. E sull‚Äôultimo layer, dopo aver appiattito quattro neuroni, uno per ogni classe:\n\nCome iperparametri, avremo un Learning Rate di 0,005 e un modello che verr√† addestrato per 100 epoche. Includeremo anche l‚Äôaumento dei dati, come un po‚Äô di rumore. Il risultato sembra OK:\n\nPer capire cosa sta succedendo ‚Äúsotto il cofano‚Äù, si pu√≤ scaricare il dataset ed eseguire un Jupyter Notebook giocando con il codice. Ad esempio, si pu√≤ analizzare l‚Äôaccuratezza per ogni epoca:\n\nQuesto CoLab Notebook pu√≤ spiegare come si pu√≤ andare oltre: KWS Classifier Project - Looking ‚ÄúUnder the hood Training/xiao_esp32s3_keyword_spotting_project_nn_classifier.ipynb)‚Äù.",
    "crumbs": [
      "XIAO ESP32S3",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/kws/kws.it.html#test",
    "href": "contents/labs/seeed/xiao_esp32s3/kws/kws.it.html#test",
    "title": "Keyword Spotting (KWS)",
    "section": "Test",
    "text": "Test\nTestando il modello con i dati messi da parte prima dell‚Äôaddestramento (Test Data), abbiamo ottenuto un‚Äôaccuratezza di circa l‚Äô87%.\n\nEsaminando il punteggio F1, possiamo vedere che per YES abbiamo ottenuto 0.95, un risultato eccellente una volta utilizzata questa parola chiave per ‚Äúattivare‚Äù la nostra fase di post-elaborazione (accendere il LED integrato). Anche per NO, abbiamo ottenuto 0,90. Il risultato peggiore √® per unknown, che √® OK.\nPossiamo procedere con il progetto, ma √® possibile eseguire Live Classification utilizzando uno smartphone prima della distribuzione sul nostro dispositivo. Si va alla sezione Live Classification e si clicca su Connect a Development board:\n\nPuntare il telefono sul codice a barre e selezionare il link.\n\nIl telefono sar√† connesso allo Studio. Selezionare l‚Äôopzione Classification sull‚Äôapp e, quando √® in esecuzione, iniziare a testare le parole chiave, confermando che il modello funziona con dati live e reali:",
    "crumbs": [
      "XIAO ESP32S3",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/kws/kws.it.html#distribuzione-e-inferenza",
    "href": "contents/labs/seeed/xiao_esp32s3/kws/kws.it.html#distribuzione-e-inferenza",
    "title": "Keyword Spotting (KWS)",
    "section": "Distribuzione e Inferenza",
    "text": "Distribuzione e Inferenza\nStudio impacchetter√† tutte le librerie necessarie, le funzioni di pre-elaborazione e i modelli addestrati, scaricandoli sul computer. Si deve selezionare l‚Äôopzione Arduino Library e, in basso, scegliere Quantized (Int8) e premere il pulsante Build.\n\nOra √® il momento di un vero test. Faremo inferenze completamente scollegate da Studio. Modifichiamo uno degli esempi di codice ESP32 creati quando si distribuisce la libreria Arduino.\nNell‚ÄôIDE Arduino, si va alla scheda File/Examples, si cerca il progetto e si seleziona esp32/esp32_microphone:\n\nQuesto codice √® stato creato per il microfono integrato ESP-EYE, che dovrebbe essere adattato al nostro dispositivo.\nIniziare a modificare le librerie per gestire il bus I2S:\n\nDa:\n#include &lt;I2S.h&gt;\n#define SAMPLE_RATE 16000U\n#define SAMPLE_BITS 16\nInizializzare il microfono IS2 in setup(), includendo le righe:\nvoid setup()\n{\n...\n    I2S.setAllPins(-1, 42, 41, -1, -1);\n    if (!I2S.begin(PDM_MONO_MODE, SAMPLE_RATE, SAMPLE_BITS)) {\n      Serial.println(\"Failed to initialize I2S!\");\n    while (1) ;\n...\n}\nNella funzione static void capture_samples(void* arg), sostituire la riga 153 che legge i dati dal microfono I2S:\n\nDa:\n/* read data at once from i2s */\nesp_i2s::i2s_read(esp_i2s::I2S_NUM_0, \n                 (void*)sampleBuffer, \n                 i2s_bytes_to_read, \n                 &bytes_read, 100);\nNella funzione static bool microphone_inference_start(uint32_t n_samples), dovremmo commentare o eliminare le righe da 198 a 200, dove viene chiamata la funzione di inizializzazione del microfono. Ci√≤ non √® necessario perch√© il microfono I2S √® gi√† stato inizializzato durante setup().\n\nInfine, nella funzione static void microphone_inference_end(void), sostituire la riga 243:\n\nDa:\nstatic void microphone_inference_end(void)\n{\n    free(sampleBuffer);\n    ei_free(inference.buffer);\n}\nIl codice completo si trova tra i progetti GitHub. Caricare lo sketch sulla bacheca e provare alcune inferenze reali:",
    "crumbs": [
      "XIAO ESP32S3",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/kws/kws.it.html#post-elaborazione",
    "href": "contents/labs/seeed/xiao_esp32s3/kws/kws.it.html#post-elaborazione",
    "title": "Keyword Spotting (KWS)",
    "section": "Post-elaborazione",
    "text": "Post-elaborazione\nOra che sappiamo che il modello funziona rilevando le nostre parole chiave, modifichiamo il codice per vedere il LED interno accendersi ogni volta che viene rilevato uno YES.\nSi deve inizializzare il LED:\n#define LED_BUILT_IN 21\n...\nvoid setup()\n{\n...\n  pinMode(LED_BUILT_IN, OUTPUT); // Set the pin as output\n  digitalWrite(LED_BUILT_IN, HIGH); //Turn off\n...\n}\nE modifica la parte ‚Äú// print the predictions‚Äù del codice precedente (su loop():\nint pred_index = 0;     // Initialize pred_index\nfloat pred_value = 0;   // Initialize pred_value\n\n// print the predictions\nei_printf(\"Predictions \");\nei_printf(\"(DSP: %d ms., Classification: %d ms., Anomaly: %d ms.)\",\n     result.timing.dsp, result.timing.classification, result.timing.anomaly);\nei_printf(\": \\n\");\nfor (size_t ix = 0; ix &lt; EI_CLASSIFIER_LABEL_COUNT; ix++) {\n      ei_printf(\"    %s: \", result.classification[ix].label);\n      ei_printf_float(result.classification[ix].value);\n      ei_printf(\"\\n\");\n\n      if (result.classification[ix].value &gt; pred_value){\n         pred_index = ix;\n         pred_value = result.classification[ix].value;\n      }\n}\n\n// show the inference result on LED\nif (pred_index == 3){\n    digitalWrite(LED_BUILT_IN, LOW); //Turn on\n}\nelse{\n   digitalWrite(LED_BUILT_IN, HIGH); //Turn off\n}\nIl codice completo si trova sul GitHub del progetto. Caricare lo sketch sulla scheda e provare alcune inferenze reali:\n\nL‚Äôidea √® che il LED sar√† ACCESO ogni volta che viene rilevata la parola chiave YES. Allo stesso modo, invece di accendere un LED, questo potrebbe essere un ‚Äútrigger‚Äù per un dispositivo esterno, come abbiamo visto nell‚Äôintroduzione.",
    "crumbs": [
      "XIAO ESP32S3",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/kws/kws.it.html#conclusione",
    "href": "contents/labs/seeed/xiao_esp32s3/kws/kws.it.html#conclusione",
    "title": "Keyword Spotting (KWS)",
    "section": "Conclusione",
    "text": "Conclusione\nThe Seeed XIAO ESP32S3 Sense √® un tiny device gigante! Tuttavia, √® potente, affidabile, non costoso, a basso consumo e ha sensori adatti per essere utilizzato nelle applicazioni di apprendimento automatico embedded pi√π comuni come visione e suono. Anche se Edge Impulse non supporta ufficialmente XIAO ESP32S3 Sense (ancora!), ci siamo resi conto che utilizzare Studio per la formazione e l‚Äôimplementazione √® semplice.\n\nNel repository GitHub, si trova l‚Äôultima versione di tutto il codice utilizzato in questo progetto e nei precedenti della serie XIAO ESP32S3.\n\nPrima di concludere, considerare che la classificazione dei suoni √® pi√π di una semplice voce. Ad esempio, si possono sviluppare progetti TinyML sul suono in diverse aree, come:\n\nSicurezza (Rilevamento vetri rotti)\nIndustria (Rilevamento di Anomalie)\nMedicina (Russare, Tosse, Malattie polmonari)\nNatura (Controllo alveari, suono degli insetti)",
    "crumbs": [
      "XIAO ESP32S3",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/kws/kws.it.html#risorse",
    "href": "contents/labs/seeed/xiao_esp32s3/kws/kws.it.html#risorse",
    "title": "Keyword Spotting (KWS)",
    "section": "Risorse",
    "text": "Risorse\n\nCodici XIAO ESP32S3\nSottoinsieme del Dataset dei Comandi Vocali di Google\nKWS MFCC Analysis Colab Notebook\nKWS CNN training Colab Notebook\nXIAO ESP32S3 Post-processing Code\nProgetto Edge Impulse",
    "crumbs": [
      "XIAO ESP32S3",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.it.html",
    "href": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.it.html",
    "title": "Classificazione del Movimento e Rilevamento delle Anomalie",
    "section": "",
    "text": "Panoramica\nThe XIAO ESP32S3 Sense, con la sua fotocamera e microfono integrati, √® un dispositivo versatile. Ma cosa succede se c‚Äô√® bisogno di aggiungere un altro tipo di sensore, come un IMU? Nessun problema! Una delle caratteristiche distintive di XIAO ESP32S3 sono i suoi molteplici pin che possono essere utilizzati come bus I2C (pin SDA/SCL), rendendolo una piattaforma adatta per l‚Äôintegrazione di sensori.",
    "crumbs": [
      "XIAO ESP32S3",
      "Classificazione del Movimento e Rilevamento delle Anomalie"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.it.html#installazione-dellimu",
    "href": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.it.html#installazione-dellimu",
    "title": "Classificazione del Movimento e Rilevamento delle Anomalie",
    "section": "Installazione dell‚ÄôIMU",
    "text": "Installazione dell‚ÄôIMU\nQuando si seleziona l‚ÄôIMU, il mercato offre un‚Äôampia gamma di dispositivi, ognuno con caratteristiche e capacit√† uniche. Si potrebbe scegliere, ad esempio, ADXL362 (3 assi), MAX21100 (6 assi), MPU6050 (6 assi), LIS3DHTR (3 assi) o LCM20600Seeed Grove‚Äî (6 assi), che fa parte dell‚ÄôIMU 9DOF (lcm20600+AK09918). Questa variet√† consente di adattare la scelta alle esigenze specifiche del progetto.\nPer questo progetto, utilizzeremo un‚ÄôIMU, la MPU6050 (o 6500), un‚Äôunit√† accelerometro/giroscopio a 6 assi a basso costo (meno di 2,00 USD).\n\nAlla fine del lab, commenteremo anche l‚Äôutilizzo dell‚ÄôLCM20600.\n\nMPU-6500 √® un dispositivo di Motion Tracking a 6 assi che combina un giroscopio a 3 assi, un accelerometro a 3 assi e un Digital Motion ProcessorTM (DMP) in un piccolo package da 3x3x0,9 mm. √à inoltre dotato di un FIFO da 4096 byte che pu√≤ ridurre il traffico sull‚Äôinterfaccia del bus seriale e ridurre il consumo energetico consentendo al processore di sistema di leggere i dati del sensore in modalit√† burst e quindi passare a una modalit√† a basso consumo.\nCon il suo bus I2C dedicato al sensore, MPU-6500 accetta direttamente input da dispositivi I2C esterni. MPU-6500, con la sua integrazione a 6 assi, DMP on-chip e firmware di calibrazione runtime, consente ai produttori di eliminare la selezione, la qualificazione e l‚Äôintegrazione a livello di sistema costose e complesse di dispositivi discreti, garantendo prestazioni di movimento ottimali per i consumatori. MPU-6500 √® progettato anche per interfacciarsi con pi√π sensori digitali non inerziali, come i sensori di pressione, sulla sua porta I2C ausiliaria.\n\n\nDi solito, le librerie disponibili sono per MPU6050, ma funzionano per entrambi i dispositivi.\n\nCollegamento dell‚ÄôHW\nCollegare l‚ÄôIMU allo XIAO secondo lo schema seguente:\n\nMPU6050 SCL ‚Äì&gt; XIAO D5\nMPU6050 SDA ‚Äì&gt; XIAO D4\nMPU6050 VCC ‚Äì&gt; XIAO 3.3V\nMPU6050 GND ‚Äì&gt; XIAO GND\n\n\nInstallare la libreria\nSi va su Arduino Library Manager e si digita MPU6050. Installare la versione pi√π recente.\n\nScaricare lo sketch MPU6050_Acc_Data_Acquisition.in:\n/*\n * Based on I2C device class (I2Cdev) Arduino sketch for MPU6050 class \n   by Jeff Rowberg &lt;jeff@rowberg.net&gt;\n * and Edge Impulse Data Forwarder Exampe (Arduino) \n   - https://docs.edgeimpulse.com/docs/cli-data-forwarder\n * \n * Developed by M.Rovai @11May23\n */\n\n#include \"I2Cdev.h\"\n#include \"MPU6050.h\"\n#include \"Wire.h\"\n\n#define FREQUENCY_HZ        50\n#define INTERVAL_MS         (1000 / (FREQUENCY_HZ + 1))\n#define ACC_RANGE           1 // 0: -/+2G; 1: +/-4G\n\n// convert factor g to m/s^2^ ==&gt; [-32768, +32767] ==&gt; [-2g, +2g]\n#define CONVERT_G_TO_MS2    (9.81/(16384.0/(1.+ACC_RANGE))) \n\nstatic unsigned long last_interval_ms = 0;\n\nMPU6050 imu;\nint16_t ax, ay, az;\n\nvoid setup() {\n  \n    Serial.begin(115200);\n\n    \n    // initialize device\n    Serial.println(\"Initializing I2C devices...\");\n    Wire.begin();\n    imu.initialize();\n    delay(10);\n    \n//    // verify connection\n//    if (imu.testConnection()) {\n//      Serial.println(\"IMU connected\");\n//    }\n//    else {\n//      Serial.println(\"IMU Error\");\n//    }\n    delay(300);\n    \n    //Set MCU 6050 OffSet Calibration \n    imu.setXAccelOffset(-4732);\n    imu.setYAccelOffset(4703);\n    imu.setZAccelOffset(8867);\n    imu.setXGyroOffset(61);\n    imu.setYGyroOffset(-73);\n    imu.setZGyroOffset(35);\n    \n    /* Set full-scale accelerometer range.\n     * 0 = +/- 2g\n     * 1 = +/- 4g\n     * 2 = +/- 8g\n     * 3 = +/- 16g\n     */\n    imu.setFullScaleAccelRange(ACC_RANGE);\n}\n\nvoid loop() {\n\n      if (millis() &gt; last_interval_ms + INTERVAL_MS) {\n        last_interval_ms = millis();\n        \n        // read raw accel/gyro measurements from device\n        imu.getAcceleration(&ax, &ay, &az);\n\n        // converting to m/s^2^\n        float ax_m_s^2^ = ax * CONVERT_G_TO_MS2;\n        float ay_m_s^2^ = ay * CONVERT_G_TO_MS2;\n        float az_m_s^2^ = az * CONVERT_G_TO_MS2;\n\n        Serial.print(ax_m_s^2^); \n        Serial.print(\"\\t\");\n        Serial.print(ay_m_s^2^); \n        Serial.print(\"\\t\");\n        Serial.println(az_m_s^2^); \n      }\n}\nAlcuni commenti sul codice:\nNotare che i valori generati dall‚Äôaccelerometro e dal giroscopio hanno un intervallo: [-32768, +32767], quindi, ad esempio, se viene utilizzato l‚Äôintervallo di default dell‚Äôaccelerometro, l‚Äôintervallo in G dovrebbe essere: [-2g, +2g]. Quindi, ‚Äú1G‚Äù significa 16384.\nPer la conversione in m/s2, ad esempio, si pu√≤ definire quanto segue:\n#define CONVERT_G_TO_MS2 (9.81/16384.0)\nNel codice, √® stata lasciata un‚Äôopzione (ACC_RANGE) da impostare su 0 (+/-2G) o 1 (+/- 4G). Useremo +/-4G; dovrebbe bastare per noi. In questo caso.\nAcquisiremo i dati dell‚Äôaccelerometro a una frequenza di 50 Hz e i dati di accelerazione saranno inviati alla porta seriale come metri al secondo quadrato (m/s2).\nQuando si esegue il codice con l‚ÄôIMU appoggiata sul tavolo, i dati dell‚Äôaccelerometro mostrati sul monitor seriale dovrebbero essere circa 0.00, 0.00 e 9.81. Se i valori sono molto diversi, si deve calibrare l‚ÄôIMU.\nL‚ÄôMCU6050 pu√≤ essere calibrato usando lo sketch: mcu6050-calibration.ino.\nEseguire il codice. Sul Serial Monitor verr√† visualizzato quanto segue:\n\nInviare un carattere qualsiasi (nell‚Äôesempio sopra, ‚Äúx‚Äù) e la calibrazione dovrebbe iniziare.\n\nNotare che c‚Äô√® un messaggio MPU6050 di connessione fallita. Ignorare questo messaggio. Per qualche motivo, imu.testConnection() non restituisce un risultato corretto.\n\nAlla fine, si riceveranno i valori di offset da utilizzare su tutti gli sketch:\n\nPrendere i valori e usarli nella configurazione:\n//Set MCU 6050 OffSet Calibration \nimu.setXAccelOffset(-4732);\nimu.setYAccelOffset(4703);\nimu.setZAccelOffset(8867);\nimu.setXGyroOffset(61);\nimu.setYGyroOffset(-73);\nimu.setZGyroOffset(35);\nOra, eseguire lo sketch MPU6050_Acc_Data_Acquisition.in:\nUna volta eseguito lo sketch precedente, aprire il Serial Monitor:\n\nOppure controllare il Plotter:\n\nSpostare il dispositivo sui tre assi. Si dovrebbe vedere la variazione sul Plotter:",
    "crumbs": [
      "XIAO ESP32S3",
      "Classificazione del Movimento e Rilevamento delle Anomalie"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.it.html#il-progetto-tinyml-motion-classification",
    "href": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.it.html#il-progetto-tinyml-motion-classification",
    "title": "Classificazione del Movimento e Rilevamento delle Anomalie",
    "section": "Il Progetto TinyML Motion Classification",
    "text": "Il Progetto TinyML Motion Classification\nPer il nostro lab, simuleremo sollecitazioni meccaniche nel trasporto. Il nostro problema sar√† classificare quattro classi di movimento:\n\nMaritime (Pallet in navi)\nTerrestrial (Pallet in un camion o treno)\nLift (Pallet movimentati da carrello elevatore)\nIdle (Pallet in magazzini)\n\nQuindi, per iniziare, dovremmo raccogliere dati. Quindi, gli accelerometri forniranno i dati sul pallet (o sul contenitore).\n\nDalle immagini sopra, possiamo vedere che i movimenti principalmente orizzontali dovrebbero essere associati alla ‚Äúclasse Terrestrial‚Äù, i movimenti verticali alla ‚Äúclasse Lift‚Äù, nessuna attivit√† alla ‚Äúclasse Idle‚Äù e il movimento su tutti e tre gli assi alla classe Maritime.",
    "crumbs": [
      "XIAO ESP32S3",
      "Classificazione del Movimento e Rilevamento delle Anomalie"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.it.html#collegamento-del-dispositivo-a-edge-impulse",
    "href": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.it.html#collegamento-del-dispositivo-a-edge-impulse",
    "title": "Classificazione del Movimento e Rilevamento delle Anomalie",
    "section": "Collegamento del dispositivo a Edge Impulse",
    "text": "Collegamento del dispositivo a Edge Impulse\nPer la raccolta dei dati, dovremmo prima collegare il dispositivo a Edge Impulse Studio, che verr√† utilizzato anche per la pre-elaborazione dei dati, l‚Äôaddestramento del modello, i test e la distribuzione.\n\nSeguire le istruzioni qui per installare Node.js e Edge Impulse CLI sul computer.\n\nDal momento che XIAO ESP32S3 non √® pi√π una scheda di sviluppo completamente supportata da Edge Impulse, dovremmo, ad esempio, usare CLI Data Forwarder per acquisire dati dal nostro sensore e inviarli allo Studio, come mostrato in questo diagramma:\n\n\nIn alternativa, si possono acquisire i dati ‚Äúoffline‚Äù, memorizzarli su una scheda SD o inviarli al computer tramite Bluetooth o Wi-Fi. In questo video, si possono imparare modi alternativi per inviare dati a Edge Impulse Studio.\n\nCollegare il dispositivo alla porta seriale ed eseguire il codice precedente per catturare i dati IMU (Accelerometro), ‚Äústampandoli‚Äù sulla seriale. Questo consentir√† a Edge Impulse Studio di ‚Äúcatturarli‚Äù.\nSi va alla pagina Edge Impulse e si crea un progetto.\n\n\nLa lunghezza massima per un nome di libreria Arduino √® di 63 caratteri. Notare che Studio nominer√† la libreria finale usando il nome del progetto e vi includer√† ‚Äú_inference‚Äù. Il nome scelto inizialmente non ha funzionato quando √® stato provato a distribuire la libreria Arduino perch√© risultava in 64 caratteri. Quindi, lo si deve cambiare eliminando la parte ‚Äúanomaly detection‚Äù.\n\nAvviare CLI Data Forwarder sul terminale, immettendo (se √® la prima volta) il seguente comando:\nedge-impulse-data-forwarder --clean\nQuindi, immettere le credenziali EI e scegli il progetto, le variabili e i nomi dei dispositivi:\n\nSi va al progetto EI e si verifica se il dispositivo √® connesso (il punto dovrebbe essere verde):",
    "crumbs": [
      "XIAO ESP32S3",
      "Classificazione del Movimento e Rilevamento delle Anomalie"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.it.html#raccolta-dati",
    "href": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.it.html#raccolta-dati",
    "title": "Classificazione del Movimento e Rilevamento delle Anomalie",
    "section": "Raccolta Dati",
    "text": "Raccolta Dati\nCome discusso in precedenza, dovremmo catturare dati da tutte e quattro le classi Transportation‚Äù. Immaginiamo di avere un contenitore con un accelerometro incorporato:\n\nOra immaginiamo che il contenitore sia su una barca, di fronte a un oceano in tempesta, su un camion, ecc.:\n\nMaritime (Pallet in navi)\n\nSpostare lo XIAO in tutte le direzioni, simulando un movimento ondulatorio della barca.\n\nTerrestrial (Pallet in un camion o treno)\n\nSpostare lo XIAO su una linea orizzontale.\n\nLift (Pallet movimentati da carrello elevatore)\n\nSpostare lo XIAO su una linea verticale.\n\nIdle (Pallet in magazzini)\n\nLasciare lo XIAO sul tavolo.\n\n\n\nDi seguito √® riportato un campione (dati grezzi) di 10 secondi:\n\nAd esempio, si possono catturare 2 minuti (dodici campioni da 10 secondi ciascuno) per le quattro classi. Utilizzando i ‚Äú3 punti‚Äù dopo ciascuno dei campioni, selezionarne 2, spostandoli per il set di test (o utilizzare lo strumento automatico Train/Test Split nella scheda Danger Zone della Dashboard). Di seguito, si possono vedere i dataset dei risultati:",
    "crumbs": [
      "XIAO ESP32S3",
      "Classificazione del Movimento e Rilevamento delle Anomalie"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.it.html#pre-elaborazione-dei-dati",
    "href": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.it.html#pre-elaborazione-dei-dati",
    "title": "Classificazione del Movimento e Rilevamento delle Anomalie",
    "section": "Pre-elaborazione dei Dati",
    "text": "Pre-elaborazione dei Dati\nIl tipo di dati grezzi catturato dall‚Äôaccelerometro √® una ‚Äúserie temporale‚Äù e dovrebbe essere convertito in ‚Äúdati tabulari‚Äù. Possiamo effettuare questa conversione utilizzando una finestra scorrevole sui dati campione. Ad esempio, nella figura sottostante,\n\nPossiamo vedere 10 secondi di dati dell‚Äôaccelerometro catturati con un ‚Äúsample rate (SR)‚Äù [frequenza di campionamento] di 50 Hz. Una finestra di 2 secondi catturer√† 300 dati (3 assi x 2 secondi x 50 campioni). Faremo scorrere questa finestra ogni 200 ms, creando un set di dati pi√π grande in cui ogni istanza ha 300 feature grezze.\n\nSi deve usare il miglior SR per il proprio caso, considerando il teorema di Nyquist, che afferma che un segnale periodico deve essere campionato a pi√π del doppio della componente di frequenza pi√π alta del segnale.\n\nLa pre-elaborazione dei dati √® un‚Äôarea impegnativa per l‚Äôapprendimento automatico embedded. Tuttavia, Edge Impulse aiuta a superare questo problema con la sua fase di pre-elaborazione dell‚Äôelaborazione del segnale digitale (DSP) e, pi√π specificamente, le Spectral Feature.\nSu Studio, questo dataset sar√† l‚Äôinput di un blocco Spectral Analysis, che √® eccellente per analizzare il movimento ripetitivo, come i dati degli accelerometri. Questo blocco eseguir√† un DSP (Digital Signal Processing), estraendo caratteristiche come ‚ÄúFFT‚Äù o ‚ÄúWavelets‚Äù. Nel caso pi√π comune, FFT, le le feature Time Domain Statistical  per asse/canale sono:\n\nRMS\nSkewness\nKurtosis\n\nE la Frequency Domain Spectral features per asse/canale sono:\n\nSpectral Power [Potenza spettrale]\nSkewness\nKurtosis\n\nAd esempio, per una lunghezza FFT di 32 punti, l‚Äôoutput risultante del blocco di analisi spettrale sar√† di 21 feature per asse (un totale di 63 feature).\nQuelle 63 feature saranno il tensore di input di un classificatore di reti neurali e il modello di rilevamento delle anomalie (K-Means).\n\nSe ne pu√≤ sapere di pi√π esplorando il laboratorio DSP Spectral Features",
    "crumbs": [
      "XIAO ESP32S3",
      "Classificazione del Movimento e Rilevamento delle Anomalie"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.it.html#progettazione-del-modello",
    "href": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.it.html#progettazione-del-modello",
    "title": "Classificazione del Movimento e Rilevamento delle Anomalie",
    "section": "Progettazione del Modello",
    "text": "Progettazione del Modello\nIl nostro classificatore sar√† una Dense Neural Network (DNN) che avr√† 63 neuroni sul suo layer di input, due layer nascosti con 20 e 10 neuroni e un layer di output con quattro neuroni (uno per ogni classe), come mostrato qui:",
    "crumbs": [
      "XIAO ESP32S3",
      "Classificazione del Movimento e Rilevamento delle Anomalie"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.it.html#impulse-design",
    "href": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.it.html#impulse-design",
    "title": "Classificazione del Movimento e Rilevamento delle Anomalie",
    "section": "Impulse Design",
    "text": "Impulse Design\nUn impulso prende dati grezzi, usa l‚Äôelaborazione del segnale per estrarre le caratteristiche e poi usa un blocco di apprendimento per classificare nuovi dati.\nSfruttiamo anche un secondo modello, il K-means, che pu√≤ essere usato per le ‚ÄúAnomaly Detection‚Äù [rilevamento delle anomalie]. Se immaginiamo di poter avere le nostre classi note come cluster, qualsiasi campione che non potrebbe adattarsi potrebbe essere un valore anomalo, un‚Äôanomalia (ad esempio, un container che rotola fuori da una nave in mare).\n\n\nImmaginiamo il nostro XIAO che rotola o si muove capovolto, su un complemento di movimento diverso da quello addestrato\n\n\nDi seguito √® riportato il nostro progetto Impulse finale:",
    "crumbs": [
      "XIAO ESP32S3",
      "Classificazione del Movimento e Rilevamento delle Anomalie"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.it.html#generazione-di-feature",
    "href": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.it.html#generazione-di-feature",
    "title": "Classificazione del Movimento e Rilevamento delle Anomalie",
    "section": "Generazione di feature",
    "text": "Generazione di feature\nA questo punto del nostro progetto, abbiamo definito il metodo di pre-elaborazione e il modello progettato. Ora √® il momento di terminare il lavoro. Per prima cosa, prendiamo i dati grezzi (tipo serie temporale) e convertiamoli in dati tabulari. Andiamo alla scheda Spectral Features e selezioniamo Save Parameters:\n\nNel men√π in alto, selezionare l‚Äôopzione Generate Features e il pulsante Generate Features. Ogni dato della finestra di 2 secondi verr√† convertito in un punto dati di 63 feature.\n\nFeature Explorer mostrer√† quei dati in 2D usando UMAP. Uniform Manifold Approximation and Projection (UMAP) √® una tecnica di riduzione delle dimensioni che pu√≤ essere usata per la visualizzazione in modo simile a t-SNE ma anche per la riduzione generale delle dimensioni non lineari.\n\nLa visualizzazione consente di verificare che le classi presentino un‚Äôeccellente separazione, il che indica che il classificatore dovrebbe funzionare bene.\n\nFacoltativamente, si pu√≤ analizzare l‚Äôimportanza relativa di ogni feature per una classe rispetto ad altre classi.",
    "crumbs": [
      "XIAO ESP32S3",
      "Classificazione del Movimento e Rilevamento delle Anomalie"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.it.html#training",
    "href": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.it.html#training",
    "title": "Classificazione del Movimento e Rilevamento delle Anomalie",
    "section": "Training",
    "text": "Training\nIl nostro modello ha quattro layer, come mostrato di seguito:\n\nCome iperparametri, useremo un Learning Rate di 0,005 e il 20% di dati per la convalida per 30 epoche. Dopo l‚Äôaddestramento, possiamo vedere che l‚Äôaccuratezza √® del 97%.\n\nPer il rilevamento delle anomalie, dovremmo scegliere le feature suggerite che sono esattamente le pi√π importanti nell‚Äôestrazione delle feature. Il numero di cluster sar√† 32, come suggerito dallo Studio:",
    "crumbs": [
      "XIAO ESP32S3",
      "Classificazione del Movimento e Rilevamento delle Anomalie"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.it.html#test",
    "href": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.it.html#test",
    "title": "Classificazione del Movimento e Rilevamento delle Anomalie",
    "section": "Test",
    "text": "Test\nUtilizzando il 20% dei dati lasciati indietro durante la fase di acquisizione dati, possiamo verificare come si comporter√† il nostro modello con dati sconosciuti; se non al 100% (come previsto), il risultato non √® stato cos√¨ buono (8%), principalmente a causa della classe ‚Äúterrestrial‚Äù. Una volta che abbiamo quattro classi (il cui output dovrebbe aggiungere 1,0), possiamo impostare una soglia inferiore affinch√© una classe sia considerata valida (ad esempio, 0.4):\n\nOra, l‚Äôaccuratezza del test salir√† al 97%.\n\nSi deve anche usare il dispositivo (che √® ancora connesso allo Studio) ed eseguire una ‚ÄúLive Classification‚Äù.\n\nNotare che qui si cattureranno dati reali con il dispositivo e si caricheranno sullo Studio, dove verr√† presa un‚Äôinferenza usando il modello addestrato (ma il modello NON √® nel dispositivo).",
    "crumbs": [
      "XIAO ESP32S3",
      "Classificazione del Movimento e Rilevamento delle Anomalie"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.it.html#distribuzione",
    "href": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.it.html#distribuzione",
    "title": "Classificazione del Movimento e Rilevamento delle Anomalie",
    "section": "Distribuzione",
    "text": "Distribuzione\nAdesso √® il momento della magia! Studio impacchetter√† tutte le librerie necessarie, le funzioni di pre-elaborazione e i modelli addestrati, scaricandoli sul computer. Si deve selezionare l‚Äôopzione ‚ÄúArduino Library‚Äù e, in basso, scegliere Quantized (Int8) e Build. Verr√† creato un file Zip e scaricato sul computer.\n\nSull‚ÄôIDE Arduino, si va alla scheda Sketch, si seleziona l‚Äôopzione Add.ZIP Library e si sceglie il file.zip scaricato da Studio:",
    "crumbs": [
      "XIAO ESP32S3",
      "Classificazione del Movimento e Rilevamento delle Anomalie"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.it.html#inferenza",
    "href": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.it.html#inferenza",
    "title": "Classificazione del Movimento e Rilevamento delle Anomalie",
    "section": "Inferenza",
    "text": "Inferenza\nOra √® il momento di un vero test. Faremo inferenze completamente scollegate dallo Studio. Modifichiamo uno degli esempi di codice creati quando si distribuisce la libreria Arduino.\nNell‚ÄôIDE Arduino, si va alla scheda File/Examples e si cerca il progetto, e negli esempi, si seleziona nano_ble_sense_accelerometer:\n\nOvviamente, questa non √® la propria board, ma possiamo far funzionare il codice con solo poche modifiche.\nAd esempio, all‚Äôinizio del codice, c‚Äô√® la libreria relativa ad Arduino Sense IMU:\n/* Includes --------------------------------------------------------------- */\n#include &lt;XIAO-ESP32S3-Motion-Classification_inferencing.h&gt;\n#include &lt;Arduino_LSM9DS1.h&gt;\nSostituire la parte ‚Äúinclude‚Äù con il codice relativo all‚ÄôIMU:\n#include &lt;XIAO-ESP32S3-Motion-Classification_inferencing.h&gt;\n#include \"I2Cdev.h\"\n#include \"MPU6050.h\"\n#include \"Wire.h\"\nCambiare le Constant Defines\n/* Constant defines ------------------------------------------------------- */\nMPU6050 imu;\nint16_t ax, ay, az;\n\n#define ACC_RANGE           1 // 0: -/+2G; 1: +/-4G\n#define CONVERT_G_TO_MS2    (9.81/(16384/(1.+ACC_RANGE)))\n#define MAX_ACCEPTED_RANGE  (2*9.81)+(2*9.81)*ACC_RANGE\nNella funzione di configurazione, avviare l‚ÄôIMU per impostare i valori di offset e il range:\n// initialize device\nSerial.println(\"Initializing I2C devices...\");\nWire.begin();\nimu.initialize();\ndelay(10);\n\n//Set MCU 6050 OffSet Calibration \nimu.setXAccelOffset(-4732);\nimu.setYAccelOffset(4703);\nimu.setZAccelOffset(8867);\nimu.setXGyroOffset(61);\nimu.setYGyroOffset(-73);\nimu.setZGyroOffset(35);\n\nimu.setFullScaleAccelRange(ACC_RANGE);\nNella funzione loop, i buffer buffer[ix], buffer[ix + 1] e buffer[ix + 2] riceveranno i dati a 3 assi catturati dall‚Äôaccelerometro. Nel codice originale, si ha la riga:\nIMU.readAcceleration(buffer[ix], buffer[ix + 1], buffer[ix + 2]);\nModificarlo con questo blocco di codice:\nimu.getAcceleration(&ax, &ay, &az);       \nbuffer[ix + 0] = ax;\nbuffer[ix + 1] = ay;\nbuffer[ix + 2] = az;\nSi deve cambiare l‚Äôordine dei due blocchi di codice seguenti. Per prima cosa, si esegue la conversione in dati grezzi in ‚ÄúMeters per squared second (ms2)‚Äù, seguita dal test riguardante il range di accettazione massimo (che qui √® in ms2, ma su Arduino era in Gs):\nbuffer[ix + 0] *= CONVERT_G_TO_MS2;\nbuffer[ix + 1] *= CONVERT_G_TO_MS2;\nbuffer[ix + 2] *= CONVERT_G_TO_MS2;\n\nfor (int i = 0; i &lt; 3; i++) {\n     if (fabs(buffer[ix + i]) &gt; MAX_ACCEPTED_RANGE) {\n        buffer[ix + i] = ei_get_sign(buffer[ix + i]) * MAX_ACCEPTED_RANGE;\n     }\n}\nE questo √® tutto! Ora si pu√≤ caricare il codice sul dispositivo e procedere con le inferenze. Il codice completo √® disponibile sul GitHub del progetto.\nOra si devono provare i movimenti, osservando il risultato dell‚Äôinferenza di ogni classe sulle immagini:\n\n\n\n\nE, naturalmente, qualche ‚Äúanomalia‚Äù, ad esempio, capovolgere lo XIAO. Il punteggio anomalo sar√† superiore a 1:",
    "crumbs": [
      "XIAO ESP32S3",
      "Classificazione del Movimento e Rilevamento delle Anomalie"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.it.html#conclusione",
    "href": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.it.html#conclusione",
    "title": "Classificazione del Movimento e Rilevamento delle Anomalie",
    "section": "Conclusione",
    "text": "Conclusione\nPer quanto riguarda l‚ÄôIMU, questo progetto ha utilizzato l‚ÄôMPU6050 a basso costo, ma potrebbe utilizzare anche altre IMU, ad esempio l‚ÄôLCM20600 (6 assi), che fa parte del Seeed Grove - IMU 9DOF (lcm20600+AK09918). Si pu√≤ sfruttare questo sensore, che ha integrato un connettore Grove, che pu√≤ essere utile nel caso in cui si utilizza lo XIAO con una scheda di estensione, come mostrato di seguito:\n\nSi possono seguire le istruzioni qui per collegare l‚ÄôIMU alla MCU. Notare solo che per usare l‚Äôaccelerometro Grove ICM20600, √® essenziale aggiornare i file I2Cdev.cpp e I2Cdev.h scaricabili dalla libreria fornita da Seeed Studio. Per farlo, si sostituiscono entrambi i file da questo link. Si trova uno sketch per testare l‚ÄôIMU sul progetto GitHub: accelerometer_test.ino.\n\nNel repository GitHub del progetto, si trova l‚Äôultima versione di tutto il codice e altri documenti: XIAO-ESP32S3 - IMU.",
    "crumbs": [
      "XIAO ESP32S3",
      "Classificazione del Movimento e Rilevamento delle Anomalie"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.it.html#risorse",
    "href": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.it.html#risorse",
    "title": "Classificazione del Movimento e Rilevamento delle Anomalie",
    "section": "Risorse",
    "text": "Risorse\n\nCodici XIAO ESP32S3\nEdge Impulse Spectral Features Block Colab Notebook\nProgetto Edge Impulse",
    "crumbs": [
      "XIAO ESP32S3",
      "Classificazione del Movimento e Rilevamento delle Anomalie"
    ]
  },
  {
    "objectID": "contents/labs/raspi/setup/setup.it.html",
    "href": "contents/labs/raspi/setup/setup.it.html",
    "title": "Setup",
    "section": "",
    "text": "Panoramica\nIl Raspberry Pi √® un potente e versatile computer a scheda singola che √® diventato uno strumento essenziale per gli ingegneri di varie discipline. Sviluppati dalla Raspberry Pi Foundation, questi dispositivi compatti offrono una combinazione unica di convenienza, potenza di calcolo e ampie capacit√† GPIO (General Purpose Input/Output), rendendoli ideali per la prototipazione, lo sviluppo di sistemi embedded e progetti di ingegneria avanzata.",
    "crumbs": [
      "Raspberry Pi",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/raspi/setup/setup.it.html#panoramica",
    "href": "contents/labs/raspi/setup/setup.it.html#panoramica",
    "title": "Setup",
    "section": "",
    "text": "Caratteristiche Principali\n\nPotenza di Calcolo: Nonostante le dimensioni ridotte, Raspberry Pi offre notevoli capacit√† di elaborazione, con gli ultimi modelli dotati di processori ARM multi-core e fino a 8 GB di RAM.\nInterfaccia GPIO: L‚Äôheader GPIO a 40 pin consente l‚Äôinterazione diretta con sensori, attuatori e altri componenti elettronici, facilitando i progetti di integrazione hardware-software.\nAmpia Connettivit√†: Wi-Fi, Bluetooth, Ethernet e pi√π porte USB integrate consentono diversi progetti di comunicazione e networking.\nAccesso Hardware di Basso Livello: Raspberry Pi fornisce accesso a interfacce come I2C, SPI e UART, consentendo un controllo dettagliato e la comunicazione con dispositivi esterni.\nCapacit√† in Tempo Reale: Con una configurazione appropriata, Raspberry Pi pu√≤ essere utilizzato per applicazioni soft in tempo reale, rendendolo adatto per sistemi di controllo e attivit√† di elaborazione del segnale.\nEfficienza Energetica: Il basso consumo energetico consente progetti alimentati a batteria e a basso consumo energetico, in particolare in modelli come Pi Zero.\n\n\n\nModelli Raspberry Pi (trattati in questo libro)\n\nRaspberry Pi Zero 2 W (Raspi-Zero):\n\nIdeale per: sistemi embedded compatti\nSpecifiche principali: CPU single-core da 1 GHz (ARM Cortex-A53), 512 MB di RAM, consumo energetico minimo\n\nRaspberry Pi 5 (Raspi-5):\n\nIdeale per: applicazioni pi√π esigenti come edge computing, computer vision e applicazioni edgeAI, inclusi LLM.\nSpecifiche principali: CPU quad-core da 2,4 GHz (ARM Cortex A-76), fino a 8 GB di RAM, interfaccia PCIe per espansioni\n\n\n\n\nApplicazioni di Ingegneria\n\nProgettazione di Sistemi Embedded: Sviluppo e prototipi di sistemi embedded per applicazioni reali.\nIoT e Dispositivi in Rete: Creazione di dispositivi interconnessi ed esplorazione di protocolli come MQTT, CoAP e HTTP/HTTPS.\nSistemi di Controllo: Implementazione di loop di controllo feedback, controller PID e interfaccia per attuatori.\nVisione Artificiale e IA: Utilizzo di librerie come OpenCV e TensorFlow Lite per l‚Äôelaborazione delle immagini e l‚Äôapprendimento automatico in edge.\nAcquisizione e Analisi dei Dati: Raccolta dati dei sensori, analisi in tempo reale e creazione di sistemi di registrazione dei dati.\nRobotica: Creazione di controller per robot, algoritmi di pianificazione del movimento e interfaccie con driver di motori.\nElaborazione del Segnale: Analisi del segnale in tempo reale, filtraggio e applicazioni DSP.\nSicurezza di Rete: Imposta VPN, firewall ed esplora i test di penetrazione della rete.\n\nQuesto tutorial guider√† nell‚Äôimpostazione dei modelli Raspberry Pi pi√π comuni, consentendo di iniziare rapidamente il progetto di apprendimento automatico. Tratteremo la configurazione hardware, l‚Äôinstallazione del sistema operativo e la configurazione iniziale, concentrandoci sulla preparazione del Pi per le applicazioni di apprendimento automatico.",
    "crumbs": [
      "Raspberry Pi",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/raspi/setup/setup.it.html#panoramica-hardware",
    "href": "contents/labs/raspi/setup/setup.it.html#panoramica-hardware",
    "title": "Setup",
    "section": "Panoramica Hardware",
    "text": "Panoramica Hardware\n\nRaspberry Pi Zero 2W\n\n\nProcessore: CPU Arm Cortex-A53 quad-core a 64 bit da 1 GHz\nRAM: SDRAM da 512 MB\nWireless: LAN wireless 802.11 b/g/n da 2,4 GHz, Bluetooth 4.2, BLE\nPorte: Mini HDMI, micro USB OTG, connettore per fotocamera CSI-2\nAlimentazione: 5 V tramite porta micro USB\n\n\n\nRaspberry Pi 5\n\n\nProcessore:\n\nPi 5: CPU Arm Cortex-A76 quad-core a 64 bit @ 2,4 GHz\nPi 4: SoC Cortex-A72 quad-core (ARM v8) a 64 bit @ 1,5 GHz\n\nRAM: opzioni da 2 GB, 4 GB o 8 GB (8 GB consigliati per attivit√† di IA)\nWireless: Wireless 802.11ac dual-band, Bluetooth 5.0\nPorte: 2 porte micro HDMI, 2 porte USB 3.0, 2 porte USB 2.0, porta fotocamera CSI, porta display DSI\nAlimentazione: 5 V CC tramite connettore USB-C (3 A)\n\n\nNei laboratori, useremo nomi diversi per riferirci al Raspberry: Raspi, Raspi-5, Raspi-Zero, ecc. Di solito, si usa Raspi quando le istruzioni o i commenti si applicano a tutti i modelli.",
    "crumbs": [
      "Raspberry Pi",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/raspi/setup/setup.it.html#installazione-del-sistema-operativo",
    "href": "contents/labs/raspi/setup/setup.it.html#installazione-del-sistema-operativo",
    "title": "Setup",
    "section": "Installazione del Sistema Operativo",
    "text": "Installazione del Sistema Operativo\n\nIl sistema operativo (SO)\nUn sistema operativo (SO) √® un software fondamentale che gestisce le risorse hardware e software del computer, fornendo servizi standard per i programmi per computer. √à il software principale che gira su un computer, fungendo da intermediario tra hardware e software applicativo. Il SO gestisce la memoria, i processi, i driver dei dispositivi, i file e i protocolli di sicurezza del computer.\n\nFunzioni chiave:\n\nGestione dei processi: Assegnazione del tempo della CPU a diversi programmi\nGestione della memoria: Assegnazione e liberazione della memoria in base alle necessit√†\nGestione del file system: Organizzazione e monitoraggio di file e directory\nGestione dei dispositivi: Comunicazione con dispositivi hardware collegati\nInterfaccia utente: Fornitura di un modo per gli utenti di interagire con il computer\n\nComponenti:\n\nKernel: Il nucleo del sistema operativo che gestisce le risorse hardware\nShell: L‚Äôinterfaccia utente per interagire con il sistema operativo\nFile system: Organizzazione e gestione dell‚Äôarchiviazione dei dati\nDevice driver: Software che consente al sistema operativo di comunicare con l‚Äôhardware\n\n\nIl Raspberry Pi esegue una versione specializzata di Linux progettata per sistemi embedded. Questo sistema operativo, in genere una variante di Debian chiamata Raspberry Pi OS (in precedenza Raspbian), √® ottimizzato per l‚Äôarchitettura basata su ARM del Pi e per le risorse limitate.\n\nL‚Äôultima versione di Raspberry Pi OS √® basata su Debian Bookworm.\n\nCaratteristiche principali:\n\nLeggero: Progettato per funzionare in modo efficiente sull‚Äôhardware del Pi.\nVersatile: Supporta un‚Äôampia gamma di applicazioni e linguaggi di programmazione.\nOpen source: Consente la personalizzazione e i miglioramenti guidati dalla comunit√†.\nSupporto GPIO: Consente l‚Äôinterazione con sensori e altri hardware tramite i pin del Pi.\nAggiornamenti regolari: Costantemente migliorato per prestazioni e sicurezza.\n\nEmbedded Linux sul Raspberry Pi fornisce un sistema operativo completo in un pacchetto compatto, rendendolo ideale per progetti che vanno da semplici dispositivi IoT ad applicazioni di apprendimento automatico edge pi√π complesse. La sua compatibilit√† con gli strumenti e le librerie Linux standard lo rende una potente piattaforma per lo sviluppo e la sperimentazione.\n\n\nInstallazione\nPer usare Raspberry Pi, avremo bisogno di un sistema operativo. Di default, Raspberry Pi verifica la presenza di un sistema operativo su qualsiasi scheda SD inserita nello slot, quindi dovremmo installare un sistema operativo usando Raspberry Pi Imager.\nRaspberry Pi Imager √® uno strumento per scaricare e scrivere immagini su macOS, Windows e Linux. Include molte immagini di sistemi operativi popolari per Raspberry Pi. Useremo Imager anche per preconfigurare credenziali e impostazioni di accesso remoto.\nSeguire i passaggi per installare il sistema operativo nel Raspi.\n\nScaricare e installare Raspberry Pi Imager sul computer.\nInserire una scheda microSD nel computer (si consiglia una scheda SD da 32 GB).\nAprire Raspberry Pi Imager e selezionare il modello di Raspberry Pi.\nScegliere il sistema operativo appropriato:\n\nFor Raspi-Zero: Ad esempio, si pu√≤ selezionare: Raspberry Pi OS Lite (64-bit).\n\n\n\n\nimg\n\n\n\nGrazie alla ridotta SDRAM (512 MB), il sistema operativo consigliato per Raspi-Zero √® la versione a 32 bit. Tuttavia, per eseguire alcuni modelli di apprendimento automatico, come YOLOv8 di Ultralitics, dovremmo usare la versione a 64 bit. Sebbene Raspi-Zero possa eseguire un desktop, sceglieremo la versione LITE (senza Desktop) per ridurre la RAM necessaria per il normale funzionamento.\n\n\nPer Raspi-5: possiamo selezionare la versione completa a 64 bit, che include un desktop: Raspberry Pi OS (64-bit)\n\n\nSelezionare la scheda microSD come dispositivo di archiviazione.\nCliccare su Next e poi sull‚Äôicona dell‚Äôingranaggio per accedere alle opzioni avanzate.\nImpostare hostname, nome utente e password di Raspi, configurare il WiFi e abilitare SSH (molto importante!)\n\n\n\nScrivere l‚Äôimmagine sulla scheda microSD.\n\n\nNegli esempi qui, useremo nomi host diversi a seconda del dispositivo utilizzato: raspi, raspi-5, raspi-Zero, ecc. Sarebbe utile se lo si sostituisse con quello in uso.\n\n\n\nConfigurazione Iniziale\n\nInserire la scheda microSD nel Raspberry Pi.\nCollegare l‚Äôalimentazione per avviare il Raspberry Pi.\nAttendere il completamento del processo di boot iniziale (potrebbero volerci alcuni minuti).\n\n\nI comandi Linux pi√π comuni da usare con Raspi si trovano qui o qui.",
    "crumbs": [
      "Raspberry Pi",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/raspi/setup/setup.it.html#accesso-remoto",
    "href": "contents/labs/raspi/setup/setup.it.html#accesso-remoto",
    "title": "Setup",
    "section": "Accesso Remoto",
    "text": "Accesso Remoto\n\nAccesso SSH\nIl modo pi√π semplice per interagire con Raspi-Zero √® tramite SSH (‚ÄúHeadless‚Äù). Si pu√≤ usare un terminale (MAC/Linux), PuTTy (Windows) o qualsiasi altro.\n\nTrovare l‚Äôindirizzo IP del Raspberry Pi (ad esempio, controllando il router).\nSul computer, aprire un terminale e connettersi tramite SSH:\nssh username@[raspberry_pi_ip_address]\nIn alternativa, se non si ha l‚Äôindirizzo IP, si pu√≤ provare quanto segue:\nssh username@hostname.local\nper esempio, ssh mjrovai@rpi-5.local , ssh mjrovai@raspi.local , ecc.\n\n\n\nimg\n\n\nQuando si vede il prompt:\nmjrovai@rpi-5:~ $\nSignifica che si sta interagendo da remoto con il Raspi. √à una buona norma aggiornare/migliorare il sistema regolarmente. Per questo, si deve eseguire:\nsudo apt-get update\nsudo apt upgrade\nSi deve confermare l‚Äôindirizzo IP Raspi. Sul terminale, si pu√≤ usare:\nhostname -I\n\n\n\n\nPer spegnere il Raspi tramite terminale:\nPer spegnere il Raspberry Pi, ci sono idee migliori che staccare semplicemente il cavo di alimentazione. Questo perch√© il Raspi potrebbe ancora scrivere dati sulla scheda SD, nel qual caso il semplice spegnimento potrebbe causare la perdita di dati o, peggio ancora, una scheda SD danneggiata.\nPer uno spegnimento di sicurezza, usare la riga di comando:\nsudo shutdown -h now\n\nPer evitare possibili perdite di dati e danneggiamenti della scheda SD, prima di rimuovere l‚Äôalimentazione, si deve attendere qualche secondo dopo lo spegnimento affinch√© il LED del Raspberry Pi smetta di lampeggiare e si spenga. Una volta che il LED si spegne, √® sicuro spegnere.\n\n\n\nTrasferire file tra il Raspi e un computer\nIl trasferimento di file tra il Raspi e il nostro computer principale pu√≤ essere effettuato tramite una chiavetta USB, direttamente sul terminale (con scp) o un programma FTP sulla rete.\n\nUtilizzo del Protocollo Secure Copy Protocol (scp):\n\nCopiare i file sul Raspberry Pi\nCreiamo un file di testo sul nostro computer, ad esempio, test.txt.\n\n\nSi pu√≤ usare qualsiasi editor di testo. Nello stesso terminale, un‚Äôopzione √® nano.\n\nPer copiare il file test.txt dal personal computer alla cartella home di un utente sul Raspberry Pi, si esegue il seguente comando dalla directory contenente test.txt, sostituendo il segnaposto &lt;username&gt; con il nome utente che si usa per accedere al Raspberry Pi e il segnaposto &lt;pi_ip_address&gt; con l‚Äôindirizzo IP del Raspberry Pi:\n$ scp test.txt &lt;username&gt;@&lt;pi_ip_address&gt;:~/\n\nNotare che ~/ significa che sposteremo il file nella ROOT del Raspi. Si pu√≤ scegliere qualsiasi cartella nel Raspi. Ma si deve creare la cartella prima di eseguire scp, poich√© scp non lo fa automaticamente.\n\nAd esempio, trasferiamo il file test.txt nella ROOT dell‚ÄôRaspi-zero, che ha un IP di 192.168.4.210:\nscp test.txt mjrovai@192.168.4.210:~/\n\nAbbiamo usato un profilo diverso per differenziare i terminali. L‚Äôazione di cui sopra avviene sul computer. Ora, andiamo sul Raspi (utilizzando SSH) e controlliamo se il file √® l√¨:\n\n\n\nCopiare i file dal Raspberry Pi\nPer copiare un file chiamato test.txt dalla directory home di un utente su un Raspberry Pi alla directory corrente su un altro computer, si esegue il seguente comando sul computer host:\n$ scp &lt;username&gt;@&lt;pi_ip_address&gt;:myfile.txt .\nPer esempio:\nSul Raspi, creiamo una copia del file con un altro nome:\ncp test.txt test_2.txt\nE sul computer host (in questo caso, un Mac)\nscp mjrovai@192.168.4.210:test_2.txt .\n\n\n\n\nTrasferimento di file tramite FTP\n√à anche possibile trasferire file tramite FTP, come FileZilla FTP Client. Seguire le istruzioni, installare il programma per il proprio sistema operativo desktop e usare l‚Äôindirizzo IP Raspi come Host. Per esempio:\nsftp://192.168.4.210\ne inserire nome utente e password di Raspi . Premendo Quickconnect si apriranno due finestre, una per il desktop del computer host (destra) e un‚Äôaltra per il Raspi (sinistra).",
    "crumbs": [
      "Raspberry Pi",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/raspi/setup/setup.it.html#aumentare-la-memoria-swap",
    "href": "contents/labs/raspi/setup/setup.it.html#aumentare-la-memoria-swap",
    "title": "Setup",
    "section": "Aumentare la memoria SWAP",
    "text": "Aumentare la memoria SWAP\nUtilizzando htop, un visualizzatore di processi interattivo multipiattaforma, si possono facilmente monitorare, in tempo reale, le risorse in esecuzione sul Raspi, come l‚Äôelenco dei processi, le CPU in esecuzione e la memoria utilizzata. Per lanciare hop, si immette il vomando sul terminale:\nhtop\n\nPer quanto riguarda la memoria, tra i dispositivi della famiglia Raspberry Pi, il Raspi-Zero ha la quantit√† pi√π piccola di SRAM (500 MB), rispetto a una selezione da 2 GB a 8 GB sui Raspi 4 o 5. Per qualsiasi Raspi, √® possibile aumentare la memoria disponibile per il sistema con ‚ÄúSwap‚Äù. La memoria di swap, nota anche come spazio di swap, √® una tecnica utilizzata nei sistemi operativi dei computer per archiviare temporaneamente i dati dalla RAM (Random Access Memory) sulla scheda SD quando la RAM fisica √® completamente utilizzata. Ci√≤ consente al sistema operativo (SO) di continuare a funzionare anche quando la RAM √® piena, il che pu√≤ prevenire crash o rallentamenti del sistema.\nLa memoria di swap avvantaggia i dispositivi con RAM limitata, come il Raspi-Zero. Aumentare lo swap pu√≤ aiutare a eseguire applicazioni o processi pi√π impegnativi, ma √® essenziale bilanciare questo con il potenziale impatto sulle prestazioni dell‚Äôaccesso frequente al disco.\nPer default, la memoria SWAP (Swp) di Rapi-Zero √® di soli 100 MB, il che √® molto poco per l‚Äôesecuzione di alcune applicazioni di apprendimento automatico pi√π complesse ed esigenti (ad esempio, YOLO). Aumentiamola a 2 MB:\nPer prima cosa, si disattiva lo swap-file:\nsudo dphys-swapfile swapoff\nPoi, si deve aprire e modificare il file /etc/dphys-swapfile. Per farlo, useremo nano:\nsudo nano /etc/dphys-swapfile\nCercare la variabile CONF_SWAPSIZE (il valore predefinito √® 200) e aggiornarla a 2000:\nCONF_SWAPSIZE=2000\nE salvare il file.\nQuindi, riattivare lo swapfile e riavviare Raspi-zero:\nsudo dphys-swapfile setup\nsudo dphys-swapfile swapon\nsudo reboot\nQuando il dispositivo viene riavviato (so deve entrare di nuovo con SSH), ci si accorger√† che il valore massimo della memoria di swap mostrato in alto √® ora qualcosa come 2 GB (nel nostro caso, 1.95 GB).\n\nPer mantenere in esecuzione htop, si deve aprire un‚Äôaltra finestra del terminale per interagire continuamente con il Raspi.",
    "crumbs": [
      "Raspberry Pi",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/raspi/setup/setup.it.html#installazione-di-una-fotocamera",
    "href": "contents/labs/raspi/setup/setup.it.html#installazione-di-una-fotocamera",
    "title": "Setup",
    "section": "Installazione di una Fotocamera",
    "text": "Installazione di una Fotocamera\nIl Raspi √® un dispositivo eccellente per applicazioni di visione artificiale; √® necessaria una fotocamera. Possiamo installare una webcam USB standard sulla porta micro-USB utilizzando un adattatore USB OTG (Raspi-Zero e Raspi-5) oppure un modulo telecamera collegato alla porta Raspi CSI (Camera Serial Interface).\n\nLe webcam USB hanno generalmente una qualit√† inferiore rispetto ai moduli fotocamera che si collegano alla porta CSI. Inoltre, non possono essere controllate utilizzando i comandi raspistill e raspivid nel terminale o il pacchetto di registrazione picamera in Python. Tuttavia, potrebbero esserci dei motivi per collegare una telecamera USB al Raspberry Pi, ad esempio perch√© √® molto pi√π facile configurare pi√π telecamere con un singolo Raspberry Pi, cavi lunghi o semplicemente perch√© si ha una telecamera del genere a portata di mano.\n\n\nInstallazione di una Webcam USB\n\nSpegnere il Raspi:\n\nsudo shutdown -h no\n\nCollegare la webcam USB (modulo telecamera USB 30fps,1280x720) al Raspi (in questo esempio, si usa il Raspi-Zero, ma le istruzioni funzionano per tutti i Raspi).\n\n\n\nRiaccendere ed eseguire SSH\nPer verificare che la fotocamera USB √® riconosciuta, eseguire:\n\nlsusb\nSi dovrebbe vedere la fotocamera elencata nell‚Äôoutput.\n\n\nPer scattare una foto di prova con la fotocamera USB, si usa:\n\nfswebcam test_image.jpg\nQuesto salver√† un‚Äôimmagine denominata ‚Äútest_image.jpg‚Äù nella directory corrente.\n\n\nDato che stiamo usando SSH per connetterci al Rapsi, dobbiamo trasferire l‚Äôimmagine al computer principale in modo da poterla visualizzare. Possiamo usare FileZilla o SCP per questo:\n\nSi apre un terminale sul computer host e si esegue:\nscp mjrovai@raspi-zero.local:~/test_image.jpg .\n\nSostituire ‚Äúmjrovai‚Äù col nome utente e ‚Äúraspi-zero‚Äù con il nome host di Pi.\n\n\n\nSe la qualit√† dell‚Äôimmagine non √® soddisfacente, si possono regolare varie impostazioni; ad esempio, definire una risoluzione adatta a YOLO (640x640):\n\nfswebcam -r 640x640 --no-banner test_image_yolo.jpg\nQuesto cattura un‚Äôimmagine ad alta risoluzione senza il banner di default.\n\nSi pu√≤ anche usare una normale webcam USB:\n\nVerificandola con lsusb\n\n\nStreaming Video\nPer lo streaming video (che richiede pi√π risorse), possiamo installare e usare mjpg-streamer:\nPer prima cosa, si installa Git:\nsudo apt install git\nOra, dovremmo installare le dipendenze necessarie per mjpg-streamer, clonare il repository e procedere con l‚Äôinstallazione:\nsudo apt install cmake libjpeg62-turbo-dev\ngit clone https://github.com/jacksonliam/mjpg-streamer.git\ncd mjpg-streamer/mjpg-streamer-experimental\nmake\nsudo make install\nQuindi avviare lo streaming con:\nmjpg_streamer -i \"input_uvc.so\" -o \"output_http.so -w ./www\"\nPossiamo poi accedere allo streaming aprendo un browser Web e andando su:\nhttp://&lt;your_pi_ip_address&gt;:8080. In questo caso: http://192.168.4.210:8080\nDovremmo vedere una pagina web con opzioni per visualizzare lo stream. Cliccare suk che dice ‚ÄúStream‚Äù o provare ad accedere a:\nhttp://&lt;raspberry_pi_ip_address&gt;:8080/?action=stream\n\n\n\n\nInstallazione di un Modulo Fotocamera sulla porta CSI\nOra ci sono diversi moduli fotocamera Raspberry Pi. Il modello originale da 5 megapixel √® stato rilasciato nel 2013, seguito da un 8-megapixel Camera Module 2 che √® stato rilasciato nel 2016. L‚Äôultimo modello di fotocamera √® il 12-megapixel Camera Module 3, rilasciato nel 2023.\nLa fotocamera originale da 5 MP (Arducam OV5647) non √® pi√π disponibile da Raspberry Pi, ma pu√≤ essere trovata da diversi fornitori alternativi. Di seguito √® riportato un esempio di tale fotocamera su un Raspi-Zero.\n\nEcco un altro esempio di un modulo fotocamera v2, dotato di un sensore Sony IMX219 da 8 megapixel:\n\nQualsiasi modulo fotocamera funzioner√† sul Raspberry Pi, ma per questo √® necessario aggiornare il file configuration.txt:\nsudo nano /boot/firmware/config.txt\nIn fondo al file, ad esempio, per usare la fotocamera Arducam OV5647 da 5 MP, si deve aggiungere la riga:\ndtoverlay=ov5647,cam0\nOppure per il modulo v2, che ha la fotocamera Sony IMX219 da 8 MP:\ndtoverlay=imx219,cam0\nSalvare il file (CTRL+O [INVIO] CRTL+X) e riavviare il Raspi:\nSudo reboot\nDopo l‚Äôavvio, si pu√≤ vedere se la fotocamera √® elencata:\nlibcamera-hello --list-cameras\n\n\n\nlibcamera √® una libreria software open source che supporta i sistemi di telecamere direttamente dal sistema operativo Linux sui processori Arm. Riduce al minimo il codice proprietario in esecuzione sulla GPU Broadcom.\n\nCatturiamo un‚Äôimmagine jpeg con una risoluzione di 640 x 480 per il test e salviamola in un file denominato test_cli_camera.jpg\nrpicam-jpeg --output test_cli_camera.jpg --width 640 --height 480\nse vogliamo vedere il file salvato, dovremmo usare ls -f, che elenca tutto il contenuto della directory corrente in formato lungo. Come prima, possiamo usare scp per visualizzare l‚Äôimmagine:",
    "crumbs": [
      "Raspberry Pi",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/raspi/setup/setup.it.html#esecuzione-del-desktop-raspi-da-remoto",
    "href": "contents/labs/raspi/setup/setup.it.html#esecuzione-del-desktop-raspi-da-remoto",
    "title": "Setup",
    "section": "Esecuzione del desktop Raspi da remoto",
    "text": "Esecuzione del desktop Raspi da remoto\nSebbene abbiamo interagito principalmente con il Raspberry Pi utilizzando comandi del terminale tramite SSH, possiamo accedere all‚Äôintero ambiente desktop grafico da remoto se abbiamo installato il sistema operativo Raspberry Pi completo (ad esempio, Raspberry Pi OS (64-bit). Questo pu√≤ essere particolarmente utile per le attivit√† che traggono vantaggio da un‚Äôinterfaccia visuale. Per abilitare questa funzionalit√†, dobbiamo configurare un server VNC (Virtual Network Computing) sul Raspberry Pi. Ecco come fare:\n\nAbilitare il server VNC:\n\nConnettersi al Raspberry Pi tramite SSH.\nEsegui lo strumento di configurazione Raspberry Pi immettendo: bash  sudo raspi-config\nAndare su Interface Options utilizzando i tasti freccia.\n\n\n\nSelezionare VNC e Yes per abilitare server VNC.\n\n\n\nUscire dallo strumento di configurazione, salvando le modifiche quando richiesto.\n\n\nInstallare un VNC Viewer sul computer:\n\nScaricare e installare un‚Äôapplicazione di visualizzazione VNC sul computer principale. Le opzioni pi√π diffuse includono RealVNC Viewer, TightVNC o VNC Viewer di RealVNC. Installeremo VNC Viewer di RealVNC.\n\nUna volta installato, si deve confermare l‚Äôindirizzo IP Raspi. Ad esempio, sul terminale, si pu√≤ usare:\nhostname -I\n\nConnettersi al Raspberry Pi:\n\nAprire l‚Äôapplicazione di visualizzazione VNC.\n\n\n\nInserire l‚Äôindirizzo IP e il nome host del Raspberry Pi.\nQuando richiesto, inserire il nome utente e la password del Raspberry Pi.\n\n\nIl Raspberry Pi 5 Desktop dovrebbe apparire sul monitor del computer.\n\nRegolare le Impostazioni dello Schermo (se necessario):\n\nUna volta connessi, regolare la risoluzione dello schermo per una visualizzazione ottimale. Questo pu√≤ essere fatto tramite le impostazioni desktop del Raspberry Pi o modificando il file config.txt.\nFacciamolo tramite le impostazioni desktop. Raggiungere il men√π (l‚Äôicona Raspberry nell‚Äôangolo in alto a sinistra) e selezionare la migliore definizione dello schermo per il proprio monitor:",
    "crumbs": [
      "Raspberry Pi",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/raspi/setup/setup.it.html#aggiornamento-e-installazione-del-software",
    "href": "contents/labs/raspi/setup/setup.it.html#aggiornamento-e-installazione-del-software",
    "title": "Setup",
    "section": "Aggiornamento e Installazione del Software",
    "text": "Aggiornamento e Installazione del Software\n\nAggiorna il sistema:\nsudo apt update && sudo apt upgrade -y\nInstallare il software essenziale:\nsudo apt install python3-pip -y\nAbilita pip per i progetti Python:\nsudo rm /usr/lib/python3.11/EXTERNALLY-MANAGED",
    "crumbs": [
      "Raspberry Pi",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/raspi/setup/setup.it.html#considerazioni-specifiche-del-modello",
    "href": "contents/labs/raspi/setup/setup.it.html#considerazioni-specifiche-del-modello",
    "title": "Setup",
    "section": "Considerazioni Specifiche del Modello",
    "text": "Considerazioni Specifiche del Modello\n\nRaspberry Pi Zero (Raspi-Zero)\n\nPotenza di elaborazione limitata, ideale per progetti leggeri\nPer risparmiare risorse √® meglio utilizzare una configurazione headless (SSH).\nValutare l‚Äôaumento dello spazio di swap per attivit√† che richiedono molta memoria.\nPu√≤ essere utilizzato per Image Classification e Object Detection Labs ma non per LLM (SLM).\n\n\n\nRaspberry Pi 4 o 5 (Raspi-4 o Raspi-5)\n\nAdatto per progetti pi√π impegnativi, tra cui IA e apprendimento automatico.\nPu√≤ eseguire l‚Äôintero ambiente desktop senza problemi.\nRaspi-4 pu√≤ essere utilizzato per Image Classification e Object Detection Labs ma non funzioner√† bene con LLM (SLM).\nPer Raspi-5, prendere in considerazione l‚Äôutilizzo di un dissipatore attivo per la gestione della temperatura durante attivit√† intensive, come nel laboratorio LLM (SLM).\n\nRicordarsi di adattare i requisiti del progetto in base allo specifico modello di Raspberry Pi in uso. Raspi-Zero √® ottimo per progetti a basso consumo e con vincoli di spazio, mentre i modelli Raspi-4 o 5 sono pi√π adatti per attivit√† pi√π intensive dal punto di vista computazionale.",
    "crumbs": [
      "Raspberry Pi",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/raspi/image_classification/image_classification.it.html",
    "href": "contents/labs/raspi/image_classification/image_classification.it.html",
    "title": "Classificazione delle Immagini",
    "section": "",
    "text": "Panoramica\nLa classificazione delle immagini √® un‚Äôattivit√† fondamentale nella visione artificiale che comporta la categorizzazione di un‚Äôimmagine in una delle diverse classi predefinite. √à una pietra angolare dell‚Äôintelligenza artificiale, che consente alle macchine di interpretare e comprendere le informazioni visive in un modo che imita la percezione umana.\nLa classificazione delle immagini si riferisce all‚Äôassegnazione di un‚Äôetichetta o di una categoria a un‚Äôintera immagine in base al suo contenuto visivo. Questa attivit√† √® fondamentale nella visione artificiale e ha numerose applicazioni in vari settori. L‚Äôimportanza della classificazione delle immagini risiede nella sua capacit√† di automatizzare le attivit√† di comprensione visiva che altrimenti richiederebbero l‚Äôintervento umano.",
    "crumbs": [
      "Raspberry Pi",
      "Classificazione delle Immagini"
    ]
  },
  {
    "objectID": "contents/labs/raspi/image_classification/image_classification.it.html#panoramica",
    "href": "contents/labs/raspi/image_classification/image_classification.it.html#panoramica",
    "title": "Classificazione delle Immagini",
    "section": "",
    "text": "Applicazioni in Scenari del Mondo Reale\nLa classificazione delle immagini ha trovato la sua strada in numerose applicazioni del mondo reale, rivoluzionando vari settori:\n\nSanit√†: Assistenza nell‚Äôanalisi delle immagini mediche, come l‚Äôidentificazione di anomalie nelle radiografie o nelle risonanze magnetiche.\nAgricoltura: Monitoraggio della salute delle colture e rilevamento delle malattie delle piante tramite immagini aeree.\nAutomotive: Abilitazione di sistemi avanzati di assistenza alla guida e veicoli autonomi per riconoscere segnali stradali, pedoni e altri veicoli.\nVendita al Dettaglio: Potenziamento delle capacit√† di ricerca visiva e sistemi di gestione automatizzata dell‚Äôinventario.\nSicurezza e Sorveglianza: Potenziamento dei sistemi di rilevamento delle minacce e riconoscimento facciale.\nMonitoraggio Ambientale: Analisi delle immagini satellitari per studi su deforestazione, pianificazione urbana e cambiamenti climatici.\n\n\n\nVantaggi dell‚ÄôEsecuzione della Classificazione su Dispositivi Edge come Raspberry Pi\nL‚Äôimplementazione della classificazione delle immagini su dispositivi edge come Raspberry Pi offre diversi vantaggi interessanti:\n\nBassa latenza: L‚Äôelaborazione delle immagini in locale elimina la necessit√† di inviare dati ai server cloud, riducendo significativamente i tempi di risposta.\nFunzionalit√† Offline: La classificazione pu√≤ essere eseguita senza una connessione Internet, rendendola adatta ad ambienti remoti o con problemi di connettivit√†.\nPrivacy e Sicurezza: I dati sensibili delle immagini rimangono sul dispositivo locale, affrontando i problemi di privacy dei dati e i requisiti di conformit√†.\nEfficacia in Termini di Costi: Elimina la necessit√† di costose risorse di cloud computing, in particolare per attivit√† di classificazione continue o ad alto volume.\nScalabilit√†: Consente architetture di elaborazione distribuite in cui pi√π dispositivi possono funzionare in modo indipendente o in una rete.\nEfficienza Energetica: I modelli ottimizzati su hardware dedicato possono essere pi√π efficienti dal punto di vista energetico rispetto alle soluzioni basate su cloud, il che √® fondamentale per applicazioni alimentate a batteria o remote.\nPersonalizzazione: L‚Äôimplementazione di modelli specializzati o aggiornati di frequente, su misura per casi d‚Äôuso specifici, √® pi√π gestibile.\n\nPossiamo creare soluzioni di visione artificiale pi√π reattive, sicure ed efficienti sfruttando la potenza di dispositivi edge come Raspberry Pi per la classificazione delle immagini. Questo approccio apre nuove possibilit√† per integrare l‚Äôelaborazione visiva intelligente in varie applicazioni e ambienti.\nNelle sezioni seguenti, esploreremo come implementare e ottimizzare la classificazione delle immagini su Raspberry Pi, sfruttando questi vantaggi per creare sistemi di visione artificiale potenti ed efficienti.",
    "crumbs": [
      "Raspberry Pi",
      "Classificazione delle Immagini"
    ]
  },
  {
    "objectID": "contents/labs/raspi/image_classification/image_classification.it.html#impostazione-dellambiente",
    "href": "contents/labs/raspi/image_classification/image_classification.it.html#impostazione-dellambiente",
    "title": "Classificazione delle Immagini",
    "section": "Impostazione dell‚ÄôAmbiente",
    "text": "Impostazione dell‚ÄôAmbiente\n\nAggiornamento di Raspberry Pi\nInnanzitutto, assicurarsi che il Raspberry Pi sia aggiornato:\nsudo apt update\nsudo apt upgrade -y\n\n\nInstallazione delle Librerie Richieste\nInstallare le librerie necessarie per l‚Äôelaborazione delle immagini e l‚Äôapprendimento automatico:\nsudo apt install python3-pip\nsudo rm /usr/lib/python3.11/EXTERNALLY-MANAGED\npip3 install --upgrade pip\n\n\nImpostazione di un Ambiente Virtuale (Facoltativo ma Consigliato)\nCreare un ambiente virtuale per gestire le dipendenze:\npython3 -m venv ~/tflite\nsource ~/tflite/bin/activate\n\n\nInstallazione di TensorFlow Lite\nSiamo interessati a eseguire inferenza, ovvero l‚Äôesecuzione di un modello TensorFlow Lite su un dispositivo per effettuare previsioni basate sui dati di input. Per eseguire un‚Äôinferenza con un modello TensorFlow Lite, dobbiamo eseguirla tramite un interprete. L‚Äôinterprete TensorFlow Lite √® progettato per essere snello e veloce. L‚Äôinterprete utilizza un ordinamento grafico statico e un allocatore di memoria personalizzato (meno dinamico) per garantire un carico, inizializzazione ed esecuzione latenza minimi.\nUtilizzeremo il runtime TensorFlow Lite per Raspberry Pi, una libreria semplificata per l‚Äôesecuzione di modelli di apprendimento automatico su dispositivi mobili e embedded, senza includere tutti i pacchetti TensorFlow.\npip install tflite_runtime --no-deps\n\nLa wheel installata: tflite_runtime-2.14.0-cp311-cp311-manylinux_2_34_aarch64.whl\n\n\n\nInstallazione di Librerie Python Aggiuntive\nInstallare le librerie Python richieste per l‚Äôuso con Image Classification:\nSe √® installata un‚Äôaltra versione di Numpy, disinstallarla prima.\npip3 uninstall numpy\nInstallare la versione 1.23.2, che √® compatibile con tflite_runtime.\npip3 install numpy==1.23.2\npip3 install Pillow matplotlib\n\n\nCreazione di una directory di lavoro:\nSe si lavora su Raspi-Zero con il sistema operativo minimo (No Desktop), non si ha un albero di directory user-pre-defined (lo si pu√≤ verificare con ls. Quindi, creiamone uno:\nmkdir Documents\ncd Documents/\nmkdir TFLITE\ncd TFLITE/\nmkdir IMG_CLASS\ncd IMG_CLASS\nmkdir models\ncd models\n\nSu Raspi-5, /Documents dovrebbe esserci.\n\nOttenere un Modello di Classificazione delle Immagini Pre-addestrato:\nUn modello pre-addestrato appropriato √® fondamentale per una classificazione delle immagini di successo su dispositivi con risorse limitate come Raspberry Pi. MobileNet √® progettato per applicazioni di visione mobile e embedded con un buon equilibrio tra accuratezza e velocit√†. Versioni: MobileNetV1, MobileNetV2, MobileNetV3. Scarichiamo la V2:\nwget https://storage.googleapis.com/download.tensorflow.org/models/\ntflite_11_05_08/mobilenet_v2_1.0_224_quant.tgz\n\ntar xzf mobilenet_v2_1.0_224_quant.tgz\nPrelevarne le etichette:\nwget https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/IMG_CLASS/models/labels.txt\nAlla fine, si dovrebbero avere i modelli nella sua directory:\n\n\nCi serviranno solo il modello mobilenet_v2_1.0_224_quant.tflite e labels.txt. Si possono eliminare gli altri file.\n\n\n\nImpostazione di Jupyter Notebook (Facoltativo)\nSe si preferisce usare Jupyter Notebook per lo sviluppo:\npip3 install jupyter\njupyter notebook --generate-config\nPer eseguire Jupyter Notebook, si lancia il comando (cambiare l‚Äôindirizzo IP per il proprio):\njupyter notebook --ip=192.168.4.210 --no-browser\nSul terminale, si pu√≤ vedere l‚Äôindirizzo URL locale per aprire il notebook:\n\nVi si pu√≤ accedere da un altro dispositivo inserendo l‚Äôindirizzo IP del Raspberry Pi e il token fornito in un browser Web (il token lo si pu√≤ copiare dal terminale).\n\nDefinire la directory di lavoro nel Raspi e creare un nuovo notebook Python 3.\n\n\nVerifica della Configurazione\nTestare la configurazione eseguendo un semplice script Python:\nimport tflite_runtime.interpreter as tflite\nimport numpy as np\nfrom PIL import Image\n\nprint(\"NumPy:\", np.__version__)\nprint(\"Pillow:\", Image.__version__)\n\n# Try to create a TFLite Interpreter\nmodel_path = \"./models/mobilenet_v2_1.0_224_quant.tflite\"\ninterpreter = tflite.Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\nprint(\"TFLite Interpreter created successfully!\")\nSi pu√≤ creare lo script Python usando nano sul terminale, salvandolo con CTRL+0 + ENTER + CTRL+X\n\nEd eseguirlo col comando:\n\nOppure lo si pu√≤ lanciare direttamente sul Notebook:",
    "crumbs": [
      "Raspberry Pi",
      "Classificazione delle Immagini"
    ]
  },
  {
    "objectID": "contents/labs/raspi/image_classification/image_classification.it.html#fare-inferenze-con-mobilenet-v2",
    "href": "contents/labs/raspi/image_classification/image_classification.it.html#fare-inferenze-con-mobilenet-v2",
    "title": "Classificazione delle Immagini",
    "section": "Fare inferenze con Mobilenet V2",
    "text": "Fare inferenze con Mobilenet V2\nNell‚Äôultima sezione, abbiamo impostato l‚Äôambiente, incluso il download di un modello pre-addestrato popolare, Mobilenet V2, addestrato sulle immagini 224x224 di ImageNet (1,2 milioni) per 1.001 classi (1.000 categorie di oggetti pi√π 1 sfondo). Il modello √® stato convertito in un formato TensorFlow Lite compatto da 3,5 MB, rendendolo adatto allo spazio di archiviazione e alla memoria limitati di un Raspberry Pi.\n\nApriamo un nuovo notebook per seguire tutti i passaggi per classificare un‚Äôimmagine:\nImportare le librerie necessarie:\nimport time\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport tflite_runtime.interpreter as tflite\nCaricare il modello TFLite e allocare i tensori:\nmodel_path = \"./models/mobilenet_v2_1.0_224_quant.tflite\"\ninterpreter = tflite.Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\nOttenere i tensori di input e output.\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\nInput details ci daranno informazioni su come il modello dovrebbe essere alimentato con un‚Äôimmagine. Il profilo di (1, 224, 224, 3) ci informa che un‚Äôimmagine con dimensioni (224x224x3) dovrebbe essere inserita una alla volta (Batch Dimension: 1).\n\nGli output details mostrano che l‚Äôinferenza risulter√† in un array di 1.001 valori interi. Tali valori derivano dalla classificazione dell‚Äôimmagine, dove ogni valore √® la probabilit√† che quella specifica etichetta sia correlata all‚Äôimmagine.\n\nEsaminiamo anche il dtype dei dettagli di input del modello\ninput_dtype = input_details[0]['dtype']\ninput_dtype\ndtype('uint8')\nQuesto mostra che l‚Äôimmagine di input dovrebbe essere composta da pixel grezzi (0 - 255).\nPrendiamo un‚Äôimmagine di prova. La si pu√≤ trasferire dal computer o scaricarne una per testarla. Per prima cosa creiamo una cartella nella nostra directory di lavoro:\nmkdir images\ncd images\nwget https://upload.wikimedia.org/wikipedia/commons/3/3a/Cat03.jpg\nCarichiamo e visualizziamo l‚Äôimmagine:\n# Load he image\nimg_path = \"./images/Cat03.jpg\"\nimg = Image.open(img_path)\n\n# Display the image\nplt.figure(figsize=(8, 8))\nplt.imshow(img)\nplt.title(\"Original Image\")\nplt.show()\n\nPossiamo vedere la dimensione dell‚Äôimmagine eseguendo il comando:\nwidth, height = img.size\nQuesto ci mostra che l‚Äôimmagine √® RGB con una larghezza di 1600 e un‚Äôaltezza di 1600 pixel. Quindi, per usare il nostro modello, dovremmo rimodellarlo in (224, 224, 3) e aggiungere una dimensione batch di 1, come definito nei dettagli di input: (1, 224, 224, 3). Il risultato dell‚Äôinferenza, come mostrato nei dettagli di output, sar√† un array con una dimensione di 1001, come mostrato di seguito:\n\nQuindi, rimodelliamo l‚Äôimmagine, aggiungiamo la dimensione batch e vediamo il risultato:\nimg = img.resize((input_details[0]['shape'][1], input_details[0]['shape'][2]))\ninput_data = np.expand_dims(img, axis=0)\ninput_data.shape\nLa forma di input_data √® come previsto: (1, 224, 224, 3)\nConfermiamo il dtype dei dati di input:\ninput_data.dtype\ndtype('uint8')\nIl dtype dei dati di input √® ‚Äòuint8‚Äô, che √® compatibile con il dtype previsto per il modello.\nUtilizzando input_data, eseguiamo l‚Äôinterprete e otteniamo le previsioni (output):\ninterpreter.set_tensor(input_details[0]['index'], input_data)\ninterpreter.invoke()\npredictions = interpreter.get_tensor(output_details[0]['index'])[0]\nLa previsione √® un array con 1001 elementi. Otteniamo i primi 5 indici in cui i loro elementi hanno valori elevati:\ntop_k_results = 5\ntop_k_indices = np.argsort(predictions)[::-1][:top_k_results]\ntop_k_indices\ntop_k_indices √® un array con 5 elementi: array([283, 286, 282])\nQuindi, 283, 286, 282, 288 e 479 sono le classi pi√π probabili dell‚Äôimmagine. Avendo l‚Äôindice, dobbiamo trovare a quale classe √® assegnato (ad esempio auto, gatto o cane). Il file di testo scaricato con il modello ha un‚Äôetichetta associata a ciascun indice da 0 a 1.000. Usiamo una funzione per caricare il file .txt come un elenco:\ndef load_labels(filename):\n    with open(filename, 'r') as f:\n        return [line.strip() for line in f.readlines()]\nE otteniamo l‚Äôelenco, stampando le etichette associate agli indici:\nlabels_path = \"./models/labels.txt\"\nlabels = load_labels(labels_path)\n\nprint(labels[286])\nprint(labels[283])\nprint(labels[282])\nprint(labels[288])\nprint(labels[479])\nDi conseguenza abbiamo:\nEgyptian cat\ntiger cat\ntabby\nlynx\ncarton\nAlmeno i quattro indici principali sono correlati ai felini. Il contenuto di prediction √® la probabilit√† associata a ciascuna delle etichette. Come abbiamo visto nei dettagli dell‚Äôoutput, quei valori sono quantizzati e dovrebbero essere dequantizzati e applicare la softmax.\nscale, zero_point = output_details[0]['quantization']\ndequantized_output = (predictions.astype(np.float32) - zero_point) * scale\nexp_output = np.exp(dequantized_output - np.max(dequantized_output))\nprobabilities = exp_output / np.sum(exp_output)\nStampiamo le prime 5 probabilit√†:\nprint (probabilities[286])\nprint (probabilities[283])\nprint (probabilities[282])\nprint (probabilities[288])\nprint (probabilities[479])\n0.27741462\n0.3732285\n0.16919471\n0.10319158\n0.023410844\nPer chiarezza, creiamo una funzione per mettere in relazione le etichette con le probabilit√†:\nfor i in range(top_k_results):\n    print(\"\\t{:20}: {}%\".format(\n        labels[top_k_indices[i]],\n        (int(probabilities[top_k_indices[i]]*100))))\ntiger cat           : 37%\nEgyptian cat        : 27%\ntabby               : 16%\nlynx                : 10%\ncarton              : 2%\n\nDefinire una funzione generale di Classificazione delle Immagini\nCreiamo una funzione generale per dare un‚Äôimmagine come input e otteniamo le prime 5 classi possibili:\n\ndef image_classification(img_path, model_path, labels, top_k_results=5):\n    # load the image\n    img = Image.open(img_path)\n    plt.figure(figsize=(4, 4))\n    plt.imshow(img)\n    plt.axis('off')\n\n    # Load the TFLite model\n    interpreter = tflite.Interpreter(model_path=model_path)\n    interpreter.allocate_tensors()\n    \n    # Get input and output tensors\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n    \n    # Preprocess\n    img = img.resize((input_details[0]['shape'][1], \n                      input_details[0]['shape'][2]))\n    input_data = np.expand_dims(img, axis=0)\n    \n    # Inference on Raspi-Zero\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n    \n    # Obtain results and map them to the classes\n    predictions = interpreter.get_tensor(output_details[0]['index'])[0]\n    \n    # Get indices of the top k results\n    top_k_indices = np.argsort(predictions)[::-1][:top_k_results]\n    \n    # Get quantization parameters\n    scale, zero_point = output_details[0]['quantization']\n    \n    # Dequantize the output and apply softmax\n    dequantized_output = (predictions.astype(np.float32) - zero_point) * scale\n    exp_output = np.exp(dequantized_output - np.max(dequantized_output))\n    probabilities = exp_output / np.sum(exp_output)\n    \n    print(\"\\n\\t[PREDICTION]        [Prob]\\n\")\n    for i in range(top_k_results):\n        print(\"\\t{:20}: {}%\".format(\n            labels[top_k_indices[i]],\n            (int(probabilities[top_k_indices[i]]*100))))\n\nE caricando alcune immagini per i test, abbiamo:\n\n\n\nTest con un modello addestrato da zero\nAddestriamo un modello TFLite da zero. Per questo, si pu√≤ seguire il Notebook:\nCNN to classify Cifar-10 dataset\nNel notebook, abbiamo addestrato un modello utilizzando il dataset CIFAR10, che contiene 60.000 immagini da 10 classi di CIFAR (airplane, automobile, bird, cat, deer, dog, frog, horse, ship, and truck). CIFAR ha immagini a colori 32x32 (3 canali colore) in cui gli oggetti non sono centrati e possono avere l‚Äôoggetto con uno sfondo, come gli aerei che potrebbero avere un cielo nuvoloso dietro di loro! In breve, immagini piccole ma reali.\nIl modello addestrato dalla CNN (cifar10_model.keras) aveva una dimensione di 2,0 MB. Utilizzando TFLite Converter, il modello cifar10.tflite √® diventato di 674 MB (circa 1/3 della dimensione originale).\n\nSul notebook Cifar 10 - Image Classification on a Raspi with TFLite (che pu√≤ essere eseguito sul Raspi), possiamo seguire gli stessi passaggi che abbiamo fatto con mobilenet_v2_1.0_224_quant.tflite. Di seguito sono riportati esempi di immagini che utilizzano la General Function for Image Classification su un Raspi-Zero, come mostrato nell‚Äôultima sezione.\n\n\n\nInstalling Picamera2\nPicamera2, una libreria Python per interagire con la fotocamera del Raspberry Pi, √® basata sullo stack della fotocamera libcamera e la Raspberry Pi Foundation la mantiene. La libreria Picamera2 √® supportata su tutti i modelli Raspberry Pi, dal Pi Zero al RPi 5. √à gi√† installata in tutto il sistema Raspi, ma dovremmo renderla accessibile all‚Äôinterno dell‚Äôambiente virtuale.\n\nPer prima cosa, attivare l‚Äôambiente virtuale se non √® gi√† attivato:\nsource ~/tflite/bin/activate\nOra, creiamo un file .pth nelll‚Äôambiente virtuale per aggiungere il percorso del sistema del pacchetto sul sito:\necho \"/usr/lib/python3/dist-packages\" &gt; $VIRTUAL_ENV/lib/python3.11/\nsite-packages/system_site_packages.pth\n\nNota: Se la versione di Python √® diversa, sostituire python3.11 con la versione appropriata.\n\nDopo aver creato questo file, provare a importare picamera2 in Python:\npython3\n&gt;&gt;&gt; import picamera2\n&gt;&gt;&gt; print(picamera2.__file__)\n\nIl codice sopra mostrer√† la posizione del file del modulo picamera2 stesso, dimostrando che la libreria √® accessibile dall‚Äôambiente.\n/home/mjrovai/tflite/lib/python3.11/site-packages/picamera2/__init__.py\n√à anche possibile elencare le telecamere disponibili nel sistema:\n&gt;&gt;&gt; print(Picamera2.global_camera_info())\nNel nostro caso, con una USB installata, si √® ottenuto:\n\nOra che abbiamo confermato che picamera2 funziona nell‚Äôambiente con un indice 0, proviamo un semplice script Python per catturare un‚Äôimmagine dalla fotocamera USB:\nfrom picamera2 import Picamera2\nimport time\n\n# Initialize the camera\npicam2 = Picamera2() # default is index 0\n\n# Configure the camera\nconfig = picam2.create_still_configuration(main={\"size\": (640, 480)})\npicam2.configure(config)\n\n# Start the camera\npicam2.start()\n\n# Wait for the camera to warm up\ntime.sleep(2)\n\n# Capture an image\npicam2.capture_file(\"usb_camera_image.jpg\")\nprint(\"Image captured and saved as 'usb_camera_image.jpg'\")\n\n# Stop the camera\npicam2.stop()\nUtilizzare l‚Äôeditor di testo Nano, Jupyter Notebook o qualsiasi altro editor. Salvarlo come script Python (ad esempio, capture_image.py) ed eseguilo. Questo dovrebbe catturare un‚Äôimmagine dalla fotocamera e salvarla come ‚Äúusb_camera_image.jpg‚Äù nella stessa directory dello script.\n\nSe Jupyter √® aperto, si pu√≤ vedere l‚Äôimmagine catturata sul computer. Altrimenti, si trasferisce il file dal Raspi al computer.\n\n\nSe si sta lavorando con un Raspi-5 con un intero desktop, si pu√≤ aprire il file direttamente sul dispositivo.",
    "crumbs": [
      "Raspberry Pi",
      "Classificazione delle Immagini"
    ]
  },
  {
    "objectID": "contents/labs/raspi/image_classification/image_classification.it.html#progetto-di-classificazione-delle-immagini",
    "href": "contents/labs/raspi/image_classification/image_classification.it.html#progetto-di-classificazione-delle-immagini",
    "title": "Classificazione delle Immagini",
    "section": "Progetto di Classificazione delle Immagini",
    "text": "Progetto di Classificazione delle Immagini\nOra, svilupperemo un progetto completo di Classificazione delle Immagini utilizzando Edge Impulse Studio. Come abbiamo fatto con Movilinet V2, il modello TFLite addestrato e convertito verr√† utilizzato per l‚Äôinferenza.\n\nL‚ÄôObiettivo\nIl primo passo in qualsiasi progetto ML √® definire il suo obiettivo. In questo caso, √® rilevare e classificare due oggetti specifici presenti in un‚Äôimmagine. Per questo progetto, utilizzeremo due piccoli giocattoli: un robot e un piccolo pappagallo brasiliano (chiamato Periquito). Raccoglieremo anche immagini di un background in cui questi due oggetti sono assenti.\n\n\n\nRaccolta Dati\nUna volta definito l‚Äôobiettivo del nostro progetto di apprendimento automatico, il passaggio successivo, e pi√π cruciale, √® la raccolta del dataset. Possiamo utilizzare un telefono per l‚Äôacquisizione delle immagini, ma qui utilizzeremo il Raspi. Impostiamo un semplice server Web sul nostro Raspberry Pi per visualizzare le immagini QVGA (320 x 240) acquisite in un browser.\n\nPer prima cosa, installiamo Flask, un framework Web leggero per Python:\npip3 install flask\nCreiamo un nuovo script Python che combina l‚Äôacquisizione delle immagini con un server Web. Lo chiameremo get_img_data.py:\n\n\nfrom flask import Flask, Response, render_template_string, request, redirect, url_for\nfrom picamera2 import Picamera2\nimport io\nimport threading\nimport time\nimport os\nimport signal\n\napp = Flask(__name__)\n\n# Global variables\nbase_dir = \"dataset\"\npicam2 = None\nframe = None\nframe_lock = threading.Lock()\ncapture_counts = {}\ncurrent_label = None\nshutdown_event = threading.Event()\n\ndef initialize_camera():\n    global picam2\n    picam2 = Picamera2()\n    config = picam2.create_preview_configuration(main={\"size\": (320, 240)})\n    picam2.configure(config)\n    picam2.start()\n    time.sleep(2)  # Wait for camera to warm up\n\ndef get_frame():\n    global frame\n    while not shutdown_event.is_set():\n        stream = io.BytesIO()\n        picam2.capture_file(stream, format='jpeg')\n        with frame_lock:\n            frame = stream.getvalue()\n        time.sleep(0.1)  # Adjust as needed for smooth preview\n\ndef generate_frames():\n    while not shutdown_event.is_set():\n        with frame_lock:\n            if frame is not None:\n                yield (b'--frame\\r\\n'\n                       b'Content-Type: image/jpeg\\r\\n\\r\\n' + frame + b'\\r\\n')\n        time.sleep(0.1)  # Adjust as needed for smooth streaming\n\ndef shutdown_server():\n    shutdown_event.set()\n    if picam2:\n        picam2.stop()\n    # Give some time for other threads to finish\n    time.sleep(2)\n    # Send SIGINT to the main process\n    os.kill(os.getpid(), signal.SIGINT)\n\n@app.route('/', methods=['GET', 'POST'])\ndef index():\n    global current_label\n    if request.method == 'POST':\n        current_label = request.form['label']\n        if current_label not in capture_counts:\n            capture_counts[current_label] = 0\n        os.makedirs(os.path.join(base_dir, current_label), exist_ok=True)\n        return redirect(url_for('capture_page'))\n    return render_template_string('''\n        &lt;!DOCTYPE html&gt;\n        &lt;html&gt;\n        &lt;head&gt;\n            &lt;title&gt;Dataset Capture - Label Entry&lt;/title&gt;\n        &lt;/head&gt;\n        &lt;body&gt;\n            &lt;h1&gt;Enter Label for Dataset&lt;/h1&gt;\n            &lt;form method=\"post\"&gt;\n                &lt;input type=\"text\" name=\"label\" required&gt;\n                &lt;input type=\"submit\" value=\"Start Capture\"&gt;\n            &lt;/form&gt;\n        &lt;/body&gt;\n        &lt;/html&gt;\n    ''')\n\n@app.route('/capture')\ndef capture_page():\n    return render_template_string('''\n        &lt;!DOCTYPE html&gt;\n        &lt;html&gt;\n        &lt;head&gt;\n            &lt;title&gt;Dataset Capture&lt;/title&gt;\n            &lt;script&gt;\n                var shutdownInitiated = false;\n                function checkShutdown() {\n                    if (!shutdownInitiated) {\n                        fetch('/check_shutdown')\n                            .then(response =&gt; response.json())\n                            .then(data =&gt; {\n                                if (data.shutdown) {\n                                    shutdownInitiated = true;\n                                    document.getElementById('video-feed').src = '';\n                                    document.getElementById('shutdown-message')\n                                    .style.display = 'block';\n                                }\n\n                            });\n                    }\n\n                }\n\n                setInterval(checkShutdown, 1000);  // Check every second\n            &lt;/script&gt;\n        &lt;/head&gt;\n        &lt;body&gt;\n            &lt;h1&gt;Dataset Capture&lt;/h1&gt;\n            &lt;p&gt;Current Label: {{ label }}&lt;/p&gt;\n            &lt;p&gt;Images captured for this label: {{ capture_count }}&lt;/p&gt;\n            &lt;img id=\"video-feed\" src=\"{{ url_for('video_feed') }}\" width=\"640\" \n            height=\"480\" /&gt;\n            &lt;div id=\"shutdown-message\" style=\"display: none; color: red;\"&gt;\n                Capture process has been stopped. You can close this window.\n            &lt;/div&gt;\n            &lt;form action=\"/capture_image\" method=\"post\"&gt;\n                &lt;input type=\"submit\" value=\"Capture Image\"&gt;\n            &lt;/form&gt;\n            &lt;form action=\"/stop\" method=\"post\"&gt;\n                &lt;input type=\"submit\" value=\"Stop Capture\" \n                style=\"background-color: #ff6666;\"&gt;\n            &lt;/form&gt;\n            &lt;form action=\"/\" method=\"get\"&gt;\n                &lt;input type=\"submit\" value=\"Change Label\" \n                style=\"background-color: #ffff66;\"&gt;\n            &lt;/form&gt;\n        &lt;/body&gt;\n        &lt;/html&gt;\n    ''', label=current_label, capture_count=capture_counts.get(current_label, 0))\n\n@app.route('/video_feed')\ndef video_feed():\n    return Response(generate_frames(),\n                    mimetype='multipart/x-mixed-replace; boundary=frame')\n\n@app.route('/capture_image', methods=['POST'])\ndef capture_image():\n    global capture_counts\n    if current_label and not shutdown_event.is_set():\n        capture_counts[current_label] += 1\n        timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n        filename = f\"image_{timestamp}.jpg\"\n        full_path = os.path.join(base_dir, current_label, filename)\n        \n        picam2.capture_file(full_path)\n    \n    return redirect(url_for('capture_page'))\n\n@app.route('/stop', methods=['POST'])\ndef stop():\n    summary = render_template_string('''\n        &lt;!DOCTYPE html&gt;\n        &lt;html&gt;\n        &lt;head&gt;\n            &lt;title&gt;Dataset Capture - Stopped&lt;/title&gt;\n        &lt;/head&gt;\n        &lt;body&gt;\n            &lt;h1&gt;Dataset Capture Stopped&lt;/h1&gt;\n            &lt;p&gt;The capture process has been stopped. You can close this window.&lt;/p&gt;\n            &lt;p&gt;Summary of captures:&lt;/p&gt;\n            &lt;ul&gt;\n            {% for label, count in capture_counts.items() %}\n                &lt;li&gt;{{ label }}: {{ count }} images&lt;/li&gt;\n            {% endfor %}\n            &lt;/ul&gt;\n        &lt;/body&gt;\n        &lt;/html&gt;\n    ''', capture_counts=capture_counts)\n    \n    # Start a new thread to shutdown the server\n    threading.Thread(target=shutdown_server).start()\n    \n    return summary\n\n@app.route('/check_shutdown')\ndef check_shutdown():\n    return {'shutdown': shutdown_event.is_set()}\n\nif __name__ == '__main__':\n    initialize_camera()\n    threading.Thread(target=get_frame, daemon=True).start()\n    app.run(host='0.0.0.0', port=5000, threaded=True)\n\n\nEseguire questo script:\n```bash python3 get_img_data.py\n\n4. Accedere all'interfaccia web:\n\n    - Sul Raspberry Pi stesso (se si ha una GUI): Aprire un browser web e andare su `http://localhost:5000`\n    - Da un altro dispositivo sulla stessa rete: Aprire un browser web e andare su `http://&lt;raspberry_pi_ip&gt;:5000` (Sostituire `&lt;raspberry_pi_ip&gt;` con l'indirizzo IP del Raspberry Pi). Per esempio: `http://192.168.4.210:5000/`\n\nQuesto script Python crea un'interfaccia basata sul web per catturare e organizzare set di dati di immagini usando un Raspberry Pi e la sua fotocamera. √à utile per progetti di apprendimento automatico che richiedono dati di immagini etichettati.\n\n#### Caratteristiche Principali:\n\n1. **Interfaccia Web**: Accessibile da qualsiasi dispositivo sulla stessa rete del Raspberry Pi.\n2. **Anteprima Telecamera in Tempo Reale**: Mostra un feed in tempo reale dalla telecamera.\n3. **Sistema di Etichettatura**: Consente agli utenti di immettere etichette per diverse categorie di immagini.\n4. **Archiviazione Organizzata**: Salva automaticamente le immagini in sottodirectory specifiche per etichetta.\n5. **Contatori per Etichetta**: Tiene traccia di quante immagini vengono acquisite per ogni etichetta.\n6. **Statistiche di Riepilogo**: Fornisce un riepilogo delle immagini acquisite quando si interrompe il processo di acquisizione.\n\n#### Componenti Principali:\n\n1. **Applicazione Web Flask**: Gestisce il routing e serve l'interfaccia Web.\n2. **Integrazione di Picamera2**: Controlla la telecamera Raspberry Pi.\n3. **Threaded Frame Capture**: Assicura un'anteprima live fluida.\n4. **File Management**: Organizza le immagini catturate in directory etichettate.\n\n#### Funzioni Chiave:\n\n- `initialize_camera()`: Imposta l'istanza Picamera2.\n- `get_frame()`: Cattura continuamente i frame per l'anteprima live.\n- `generate_frames()`: Genera i frame per il feed video live.\n- `shutdown_server()`: Imposta l'evento di arresto, arresta la telecamera e arresta il server Flask\n- `index()`: Gestisce la pagina di input dell'etichetta.\n- `capture_page()`: Visualizza l'interfaccia di acquisizione principale.\n- `video_feed()`: Mostra un'anteprima live per posizionare la telecamera\n- `capture_image()`: Salva un'immagine con l'etichetta corrente.\n- `stop()`: Arresta il processo di acquisizione e visualizza un riepilogo.\n\n#### Flusso di Utilizzo:\n\n1. Avviare lo script sul Raspberry Pi.\n2. Accedere all'interfaccia web da un browser.\n3. Inserire un'etichetta per le immagini da catturare e premere `Start Capture`.\n\n![](images/png/enter_label.png)\n\n4. Utilizzare l'anteprima live per posizionare la telecamera.\n5. Cliccare `Capture Image` per salvare le immagini sotto l'etichetta corrente.\n\n![](images/png/capture.png)\n\n6. Cambiare le etichette come necessario per le diverse categorie, selezionando `Change Label`.\n7. Cliccare `Stop Capture` al termine per vedere un riepilogo.\n\n![](images/png/stop.png)\n\n#### Note Tecniche:\n\n- Lo script usa il threading per gestire la cattura di frame e il web serving simultanei.\n- Le immagini vengono salvate con timestamp nei nomi dei file per renderle uniche.\n- L'interfaccia web √® reattiva e accessibile da dispositivi mobili.\n\n#### Possibilit√† di Personalizzazione:\n\n- Regolare la risoluzione dell'immagine nella funzione `initialize_camera()`. Qui abbiamo utilizzato QVGA (320X240).\n- Modificare i modelli HTML per un aspetto diverso.\n- Aggiungere ulteriori passaggi di elaborazione o analisi delle immagini nella funzione `capture_image()`.\n\n#### Numero di campioni sul Dataset:\n\nSi ottengono circa 60 immagini da ciascuna categoria (`periquito`, `robot` e `background`). Provare ad acquisire con diverse angolazioni, sfondi e condizioni di luce. Sul Raspi, finiremo con una cartella denominata `dataset`, che contiene 3 sottocartelle *periquito*, *robot* e *background*. una per ogni classe di immagini.\n\nSi pu√≤ usare `Filezilla` per trasferire il dataset creato sul computer principale. \n\n## Addestramento del modello con Edge Impulse\n\nUseremo Edge Impulse Studio per addestrare il modello.  Si va nella [Pagina di Edge Impulse](https://edgeimpulse.com/), si inseriscono le credenziali e si crea un nuovo progetto:\n\n![](images/png/new-proj-ei.png)\n\n&gt; Qui si pu√≤ clonare un progetto simile: [Raspi - Img Class](https://studio.edgeimpulse.com/public/510251/live).\n\n### Dataset\n\nEsamineremo quattro passaggi principali usando EI Studio (o Studio). Questi passaggi sono fondamentali per preparare il nostro modello per l'uso sul Raspi: Dataset, Impulse, Test e Deploy (sul dispositivo Edge, in questo caso, il Raspi).\n\n&gt; Per quanto riguarda il Dataset, √® essenziale sottolineare che il nostro Dataset originale, acquisito con il Raspi, sar√† suddiviso in *Training*, *Validation* e *Test*. Il Test Set sar√† separato dall'inizio e riservato per l'uso solo nella fase di Test dopo l'addestramento. Il Validation Set sar√† utilizzato durante l'addestramento.\n\nSu Studio, seguire i passaggi per caricare i dati acquisiti:\n\n1. Si va nella scheda `Data acquisition` e nella sezione `UPLOAD DATA`, si caricano i file dal computer nelle categorie scelte.\n2. Lasciare a Studio la suddivisione del dataset originale in *train and test* e scegliere l'etichetta a riguardo\n3. Ripetere la procedura per tutte e tre le classi. Alla fine, si vedranno i \"raw data\" in Studio:\n\n![](images/png/data-Aquisition.png)\n\nStudio consente di esplorare i dati, mostrando una vista completa di tutti i quelli nel progetto. Si possono cancellare, ispezionare o modificare le etichette cliccando sui singoli elementi di dati. Nel nostro caso, un progetto semplice, i dati sembrano OK.\n\n![](images/png/data-esplorer.png)\n\n## Il Progetto Impulse\n\nIn questa fase, dovremmo definire come:\n\n- Pre-elaborare i nostri dati, il che consiste nel ridimensionare le singole immagini e determinare la `color depth` [profondit√† di colore] da utilizzare (sia RGB che in scala di grigi) e\n\n- Specificare un Modello. In questo caso, sar√† `Transfer Learning (Images)` a mettere a punto un modello di classificazione delle immagini MobileNet V2 pre-addestrato sui nostri dati. Questo metodo funziona bene anche con set di dati di immagini relativamente piccoli (circa 180 immagini nel nostro caso).\n\nTransfer Learning con MobileNet offre un approccio semplificato all'addestramento del modello, che √® particolarmente utile per ambienti con risorse limitate e progetti con dati etichettati limitati. MobileNet, noto per la sua architettura leggera, √® un modello pre-addestrato che ha gi√† appreso funzionalit√† preziose da un ampio set di dati (ImageNet).\n\n![](images/jpeg/model_1.jpg)\n\nSfruttando queste funzionalit√† apprese, possiamo addestrare un nuovo modello per il compito specifico con meno dati e risorse computazionali e raggiungere un'accuratezza competitiva.\n\n![](images/jpeg/model_2.jpg)\n\nQuesto approccio riduce significativamente i tempi di addestramento e i costi computazionali, rendendolo ideale per la prototipazione rapida e l'implementazione su dispositivi embedded in cui l'efficienza √® fondamentale.\n\nSi va alla scheda Impulse Design e si crea l'*impulse*, definendo una dimensione dell'immagine di 160x160 e schiacciandola (forma quadrata, senza ritaglio). Si seleziona Image e i blocchi Transfer Learning. Si salva l'Impulse.\n\n![](images/png/impulse.png)\n\n### Pre-elaborazione delle immagini\n\nTutte le immagini QVGA/RGB565 in ingresso verranno convertite in 76.800 feature (160x160x3).\n\n![](images/png/preproc.png)\n\nPremere `Save parameters` e selezionare `Generate features` nella scheda successiva.\n\n### Progettazione del modello\n\nMobileNet √® una famiglia di reti neurali convoluzionali efficienti progettate per applicazioni di visione mobile e embedded. Le caratteristiche principali di MobileNet sono:\n\n1. Leggero: Ottimizzato per dispositivi mobili e sistemi embedded con risorse di calcolo limitate.\n2. Velocit√†: Tempi di inferenza rapidi, adatti per applicazioni in tempo reale.\n3. Precisione: Mantiene una buona accuratezza nonostante le dimensioni compatte.\n\n[MobileNetV2](https://arxiv.org/abs/1801.04381), introdotto nel 2018, migliora l'architettura MobileNet originale. Le caratteristiche principali includono:\n\n1. Residui Invertiti: Le strutture residue invertite vengono utilizzate quando vengono create connessioni di scelta rapida tra layer di colli di bottiglia sottili.\n2. Colli di Bottiglia Lineari: Rimuove le non linearit√† nei layer stretti per impedire la distruzione delle informazioni.\n3. Convoluzioni Separabili in Profondit√†: Continua a utilizzare questa efficiente operazione da MobileNetV1.\n\nNel nostro progetto, faremo un `Transfer Learning` con `MobileNetV2 160x160 1.0`, il che significa che le immagini utilizzate per l'addestramento (e l'inferenza futura) dovrebbero avere una *input Size* [dimensione di input] di 160x160 pixel e un *Width Multiplier* [moltiplicatore di larghezza] di 1.0 (larghezza completa, non ridotta). Questa configurazione bilancia tra dimensione del modello, velocit√† e accuratezza.\n\n### Training del Modello\n\nUn'altra preziosa tecnica di apprendimento profondo √® il **Data Augmentation**. Il \"data augmentation\" migliora l'accuratezza dei modelli di apprendimento automatico creando dati artificiali aggiuntivi. Un sistema di data augmentation apporta piccole modifiche casuali ai dati di training durante l'addestramento (ad esempio capovolgendo, ritagliando o ruotando le immagini).\n\nGuardando internamente, qui si pu√≤ vedere come Edge Impulse implementa una policy di data Augmentation sui dati:\n\n``` python\n# Implements the data augmentation policy\ndef augment_image(image, label):\n    # Flips the image randomly\n    image = tf.image.random_flip_left_right(image)\n\n    # Increase the image size, then randomly crop it down to\n    # the original dimensions\n    resize_factor = random.uniform(1, 1.2)\n    new_height = math.floor(resize_factor * INPUT_SHAPE[0])\n    new_width = math.floor(resize_factor * INPUT_SHAPE[1])\n    image = tf.image.resize_with_crop_or_pad(image, new_height, new_width)\n    image = tf.image.random_crop(image, size=INPUT_SHAPE)\n\n    # Vary the brightness of the image\n    image = tf.image.random_brightness(image, max_delta=0.2)\n\n    return image, label\nL‚Äôesposizione a queste variazioni durante l‚Äôaddestramento pu√≤ aiutare a impedire al modello di prendere scorciatoie ‚Äúmemorizzando‚Äù indizi superficiali nei dati di addestramento, il che significa che potrebbe riflettere meglio i pattern profondi in esame nel set di dati.\nL‚Äôultimo layer denso del nostro modello avr√† 0 neuroni con un dropout del 10% per prevenire il sovradattamento. Ecco il risultato del Training:\n\nIl risultato √® eccellente, con una latenza ragionevole di 35 ms (per un Raspi-4), che dovrebbe tradursi in circa 30 fps (fotogrammi al secondo) durante l‚Äôinferenza. Un Raspi-Zero dovrebbe essere pi√π lento e il Raspi-5 pi√π veloce.\n\n\nCompromesso: Accuratezza contro Velocit√†\nSe √® necessaria un‚Äôinferenza pi√π rapida, dovremmo addestrare il modello usando alfa pi√π piccoli (0.35, 0.5 e 0.75) o persino ridurre le dimensioni dell‚Äôimmagine in ingresso, a discapito dell‚Äôaccuratezza. Tuttavia, ridurre le dimensioni dell‚Äôimmagine in ingresso e diminuire l‚Äôalfa (moltiplicatore di larghezza) pu√≤ accelerare l‚Äôinferenza per MobileNet V2, ma hanno compromessi diversi. Confrontiamoli:\n\nRiduzione delle Dimensioni dell‚ÄôImmagine in Ingresso:\n\nPro:\n\nRiduce significativamente il costo computazionale su tutti i layer.\nRiduce l‚Äôutilizzo della memoria.\nSpesso fornisce un aumento sostanziale della velocit√†.\n\nContro:\n\nPotrebbe ridurre la capacit√† del modello di rilevare piccole caratteristiche o dettagli fini.\nPu√≤ avere un impatto significativo sulla precisione, specialmente per le attivit√† che richiedono un riconoscimento a grana fine.\n\n\nRiduzione di Alpha (Moltiplicatore di Larghezza):\n\nPro:\n\nRiduce il numero di parametri e calcoli nel modello.\nMantiene la risoluzione di input originale, preservando potenzialmente pi√π dettagli.\nPu√≤ fornire un buon equilibrio tra velocit√† e precisione.\n\nContro:\n\nPotrebbe non accelerare l‚Äôinferenza in modo cos√¨ drastico come la riduzione delle dimensioni di input.\nPu√≤ ridurre la capacit√† del modello di apprendere caratteristiche complesse.\n\nConfronto:\n\nImpatto sulla Velocit√†:\n\nLa riduzione delle dimensioni di input spesso fornisce un aumento di velocit√† pi√π sostanziale perch√© riduce i calcoli in modo quadratico (dimezzando sia la larghezza che l‚Äôaltezza si riducono i calcoli di circa il 75%).\nLa riduzione di Alpha fornisce una riduzione pi√π lineare nei calcoli.\n\nImpatto sulla Precisione:\n\nLa riduzione delle dimensioni di input pu√≤ avere un impatto significativo sulla precisione, specialmente quando si rilevano piccoli oggetti o dettagli fini.\nLa riduzione di alpha tende ad avere un impatto pi√π graduale sulla precisione.\n\nArchitettura del Modello:\n\nLa modifica delle dimensioni di input non altera l‚Äôarchitettura del modello.\nLa modifica di alpha modifica la struttura del modello riducendo il numero di canali in ogni layer.\n\n\nRaccomandazione:\n\nSe l‚Äôapplicazione non richiede il rilevamento di piccoli dettagli e pu√≤ tollerare una certa perdita di accuratezza, ridurre le dimensioni di input √® spesso il modo pi√π efficace per accelerare l‚Äôinferenza.\nRidurre l‚Äôalfa potrebbe essere preferibile se mantenere la capacit√† di rilevare dettagli fini √® fondamentale o se c‚Äô√® bisogno di un compromesso pi√π equilibrato tra velocit√† e accuratezza.\nPer ottenere risultati migliori, si devono sperimentare entrambi:\n\nProvare MobileNet V2 con dimensioni di input come 160x160 o 92x92\nSperimentare con valori alfa come 1.0, 0.75, 0.5 o 0.35.\n\nEseguire sempre il benchmark delle diverse configurazioni sull‚Äôhardware specifico e con il particolare set di dati per trovare l‚Äôequilibrio ottimale per il caso d‚Äôuso.\n\n\nRicordarsi che la scelta migliore dipende dai requisiti specifici di accuratezza, velocit√† e dalla natura delle immagini con cui si sta lavorando. Spesso vale la pena sperimentare combinazioni per trovare la configurazione ottimale per il particolare caso d‚Äôuso.\n\n\n\nTest del Modello\nOra, si deve prendere il set di dati all‚Äôinizio del progetto ed eseguire il modello addestrato usandolo come input. Di nuovo, il risultato √® eccellente (92,22%).\n\n\nDistribuzione del modello\nCome abbiamo fatto nella sezione precedente, possiamo distribuire il modello addestrato come .tflite e usare Raspi per eseguirlo usando Python.\nNella scheda Dashboard, si va su Transfer learning model (int8 quantized) e si clicca sull‚Äôicona di download:\n\n\nScarichiamo anche la versione float32 per il confronto\n\nTrasferire il modello dal computer al Raspi (./models), ad esempio, usando FileZilla. Catturare, inoltre, alcune immagini per l‚Äôinferenza (./images).\nImportare le librerie necessarie:\nimport time\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport tflite_runtime.interpreter as tflite\nDefinire i path e le etichette:\nimg_path = \"./images/robot.jpg\"\nmodel_path = \"./models/ei-raspi-img-class-int8-quantized-model.tflite\"\nlabels = ['background', 'periquito', 'robot']\n\nNotare che i modelli addestrati su Edge Impulse Studio produrranno valori con indice 0, 1, 2, ecc., dove le etichette effettive seguiranno un ordine alfabetico.\n\nCaricare il modello, allocare i tensori e ottenere i dettagli dei tensori di input e output:\n# Load the TFLite model\ninterpreter = tflite.Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n# Get input and output tensors\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\nUna differenza importante da notare √® che il dtype dei dettagli di input del modello √® ora int8, il che significa che i valori di input vanno da -128 a +127, mentre ogni pixel della nostra immagine va da 0 a 255. Ci√≤ significa che dovremmo pre-elaborare l‚Äôimmagine per farla corrispondere. Possiamo controllare qui:\ninput_dtype = input_details[0]['dtype']\ninput_dtype\nnumpy.int8\nQuindi, apriamo l‚Äôimmagine e mostriamola:\nimg = Image.open(img_path)\nplt.figure(figsize=(4, 4))\nplt.imshow(img)\nplt.axis('off')\nplt.show()\n\nEd eseguiamo la pre-elaborazione:\nscale, zero_point = input_details[0]['quantization']\nimg = img.resize((input_details[0]['shape'][1], \n                  input_details[0]['shape'][2]))\nimg_array = np.array(img, dtype=np.float32) / 255.0\nimg_array = (img_array / scale + zero_point).clip(-128, 127).astype(np.int8)\ninput_data = np.expand_dims(img_array, axis=0)\nControllando i dati di input, possiamo verificare che il tensore di input √® compatibile con quanto previsto dal modello:\ninput_data.shape, input_data.dtype\n((1, 160, 160, 3), dtype('int8'))\nAdesso √® il momento di effettuare l‚Äôinferenza. Calcoliamo anche la latenza del modello:\n# Inference on Raspi-Zero\nstart_time = time.time()\ninterpreter.set_tensor(input_details[0]['index'], input_data)\ninterpreter.invoke()\nend_time = time.time()\ninference_time = (end_time - start_time) * 1000  # Convert to milliseconds\nprint (\"Inference time: {:.1f}ms\".format(inference_time))\nIl modello impiegher√† circa 125 ms per eseguire l‚Äôinferenza nel Raspi-Zero, che dura 3 o 4 volte pi√π a lungo di un Raspi-5.\nOra possiamo ottenere le etichette di output e le probabilit√†. √à anche importante notare che il modello addestrato su Edge Impulse Studio ha un softmax nel suo output (diverso dal Movilenet V2 originale) e dovremmo usare l‚Äôoutput grezzo del modello come ‚Äúprobabilit√†‚Äù.\n# Obtain results and map them to the classes\npredictions = interpreter.get_tensor(output_details[0]['index'])[0]\n\n# Get indices of the top k results\ntop_k_results=3\ntop_k_indices = np.argsort(predictions)[::-1][:top_k_results]\n\n# Get quantization parameters\nscale, zero_point = output_details[0]['quantization']\n\n# Dequantize the output\ndequantized_output = (predictions.astype(np.float32) - zero_point) * scale\nprobabilities = dequantized_output\n\nprint(\"\\n\\t[PREDICTION]        [Prob]\\n\")\nfor i in range(top_k_results):\n    print(\"\\t{:20}: {:.2f}%\".format(\n        labels[top_k_indices[i]],\n        probabilities[top_k_indices[i]] * 100))\n\nModifichiamo la funzione creata in precedenza in modo da poter gestire diversi tipi di modelli:\n\ndef image_classification(img_path, model_path, labels, top_k_results=3, \n                         apply_softmax=False):\n    # Load the image\n    img = Image.open(img_path)\n    plt.figure(figsize=(4, 4))\n    plt.imshow(img)\n    plt.axis('off')\n\n    # Load the TFLite model\n    interpreter = tflite.Interpreter(model_path=model_path)\n    interpreter.allocate_tensors()\n    \n    # Get input and output tensors\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n    \n    # Preprocess\n    img = img.resize((input_details[0]['shape'][1], \n                      input_details[0]['shape'][2]))\n    \n    input_dtype = input_details[0]['dtype']\n    \n    if input_dtype == np.uint8:\n        input_data = np.expand_dims(np.array(img), axis=0)\n    elif input_dtype == np.int8:\n        scale, zero_point = input_details[0]['quantization']\n        img_array = np.array(img, dtype=np.float32) / 255.0\n        img_array = (img_array / scale + zero_point).clip(-128, 127).astype(np.int8)\n        input_data = np.expand_dims(img_array, axis=0)\n    else:  # float32\n        input_data = np.expand_dims(np.array(img, dtype=np.float32), axis=0) / 255.0\n    \n    # Inference on Raspi-Zero\n    start_time = time.time()\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n    end_time = time.time()\n    inference_time = (end_time - start_time) * 1000  # Convert to milliseconds\n    \n    # Obtain results\n    predictions = interpreter.get_tensor(output_details[0]['index'])[0]\n    \n    # Get indices of the top k results\n    top_k_indices = np.argsort(predictions)[::-1][:top_k_results]\n    \n    # Handle output based on type\n    output_dtype = output_details[0]['dtype']\n    if output_dtype in [np.int8, np.uint8]:\n        # Dequantize the output\n        scale, zero_point = output_details[0]['quantization']\n        predictions = (predictions.astype(np.float32) - zero_point) * scale\n    \n    if apply_softmax:\n        # Apply softmax\n        exp_preds = np.exp(predictions - np.max(predictions))\n        probabilities = exp_preds / np.sum(exp_preds)\n    else:\n        probabilities = predictions\n    \n    print(\"\\n\\t[PREDICTION]        [Prob]\\n\")\n    for i in range(top_k_results):\n        print(\"\\t{:20}: {:.1f}%\".format(\n            labels[top_k_indices[i]],\n            probabilities[top_k_indices[i]] * 100))\n    print (\"\\n\\tInference time: {:.1f}ms\".format(inference_time))\n\nE lo si testa con immagini diverse e con il modello quantizzato int8 (160x160 alpha =1.0).\n\nScarichiamo un modello pi√π piccolo, come quello addestrato per il Nicla Vision Lab (int8 quantized model, 96x96, alpha = 0.1), come test. Possiamo usare la stessa funzione:\n\nIl modello ha perso un po‚Äô di accuratezza, ma √® ancora OK dato che non cerca molti dettagli. Per quanto riguarda la latenza, siamo circa dieci volte pi√π veloci su Raspi-Zero.",
    "crumbs": [
      "Raspberry Pi",
      "Classificazione delle Immagini"
    ]
  },
  {
    "objectID": "contents/labs/raspi/image_classification/image_classification.it.html#classificazione-delle-immagini-in-tempo-reale",
    "href": "contents/labs/raspi/image_classification/image_classification.it.html#classificazione-delle-immagini-in-tempo-reale",
    "title": "Classificazione delle Immagini",
    "section": "Classificazione delle Immagini in Tempo Reale",
    "text": "Classificazione delle Immagini in Tempo Reale\nSviluppiamo un‚Äôapp per catturare immagini con la fotocamera USB in tempo reale, mostrandone la classificazione.\nUtilizzando nano sul terminale, salvare il codice sottostante, come img_class_live_infer.py.\n\nfrom flask import Flask, Response, render_template_string, request, jsonify\nfrom picamera2 import Picamera2\nimport io\nimport threading\nimport time\nimport numpy as np\nfrom PIL import Image\nimport tflite_runtime.interpreter as tflite\nfrom queue import Queue\n\napp = Flask(__name__)\n\n# Global variables\npicam2 = None\nframe = None\nframe_lock = threading.Lock()\nis_classifying = False\nconfidence_threshold = 0.8\nmodel_path = \"./models/ei-raspi-img-class-int8-quantized-model.tflite\"\nlabels = ['background', 'periquito', 'robot']\ninterpreter = None\nclassification_queue = Queue(maxsize=1)\n\ndef initialize_camera():\n    global picam2\n    picam2 = Picamera2()\n    config = picam2.create_preview_configuration(main={\"size\": (320, 240)})\n    picam2.configure(config)\n    picam2.start()\n    time.sleep(2)  # Wait for camera to warm up\n\ndef get_frame():\n    global frame\n    while True:\n        stream = io.BytesIO()\n        picam2.capture_file(stream, format='jpeg')\n        with frame_lock:\n            frame = stream.getvalue()\n        time.sleep(0.1)  # Capture frames more frequently\n\ndef generate_frames():\n    while True:\n        with frame_lock:\n            if frame is not None:\n                yield (b'--frame\\r\\n'\n                       b'Content-Type: image/jpeg\\r\\n\\r\\n' + frame + b'\\r\\n')\n        time.sleep(0.1)\n\ndef load_model():\n    global interpreter\n    if interpreter is None:\n        interpreter = tflite.Interpreter(model_path=model_path)\n        interpreter.allocate_tensors()\n    return interpreter\n\ndef classify_image(img, interpreter):\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n\n    img = img.resize((input_details[0]['shape'][1], \n                      input_details[0]['shape'][2]))\n    input_data = np.expand_dims(np.array(img), axis=0)\\\n                             .astype(input_details[0]['dtype'])\n\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    predictions = interpreter.get_tensor(output_details[0]['index'])[0]\n    # Handle output based on type\n    output_dtype = output_details[0]['dtype']\n    if output_dtype in [np.int8, np.uint8]:\n        # Dequantize the output\n        scale, zero_point = output_details[0]['quantization']\n        predictions = (predictions.astype(np.float32) - zero_point) * scale\n    return predictions\n\ndef classification_worker():\n    interpreter = load_model()\n    while True:\n        if is_classifying:\n            with frame_lock:\n                if frame is not None:\n                    img = Image.open(io.BytesIO(frame))\n            predictions = classify_image(img, interpreter)\n            max_prob = np.max(predictions)\n            if max_prob &gt;= confidence_threshold:\n                label = labels[np.argmax(predictions)]\n            else:\n                label = 'Uncertain'\n            classification_queue.put({'label': label, \n                                      'probability': float(max_prob)})\n        time.sleep(0.1)  # Adjust based on your needs\n\n@app.route('/')\ndef index():\n    return render_template_string('''\n        &lt;!DOCTYPE html&gt;\n        &lt;html&gt;\n        &lt;head&gt;\n            &lt;title&gt;Image Classification&lt;/title&gt;\n            &lt;script \n                src=\"https://code.jquery.com/jquery-3.6.0.min.js\"&gt;\n            &lt;/script&gt;\n            &lt;script&gt;\n                function startClassification() {\n                    $.post('/start');\n                    $('#startBtn').prop('disabled', true);\n                    $('#stopBtn').prop('disabled', false);\n                }\n\n                function stopClassification() {\n                    $.post('/stop');\n                    $('#startBtn').prop('disabled', false);\n                    $('#stopBtn').prop('disabled', true);\n                }\n\n                function updateConfidence() {\n                    var confidence = $('#confidence').val();\n                    $.post('/update_confidence', {confidence: confidence});\n                }\n\n                function updateClassification() {\n                    $.get('/get_classification', function(data) {\n                        $('#classification').text(data.label + ': ' \n                        + data.probability.toFixed(2));\n                    });\n                }\n\n                $(document).ready(function() {\n                    setInterval(updateClassification, 100);  \n                    // Update every 100ms\n                });\n            &lt;/script&gt;\n        &lt;/head&gt;\n        &lt;body&gt;\n            &lt;h1&gt;Image Classification&lt;/h1&gt;\n            &lt;img src=\"{{ url_for('video_feed') }}\" width=\"640\" height=\"480\" /&gt;\n            &lt;br&gt;\n            &lt;button id=\"startBtn\" onclick=\"startClassification()\"&gt;\n            Start Classification&lt;/button&gt;\n            &lt;button id=\"stopBtn\" onclick=\"stopClassification()\" disabled&gt;\n            Stop Classification&lt;/button&gt;\n            &lt;br&gt;\n            &lt;label for=\"confidence\"&gt;Confidence Threshold:&lt;/label&gt;\n            &lt;input type=\"number\" id=\"confidence\" name=\"confidence\" min=\"0\" \n            max=\"1\" step=\"0.1\" value=\"0.8\" onchange=\"updateConfidence()\"&gt;\n            &lt;br&gt;\n            &lt;div id=\"classification\"&gt;Waiting for classification...&lt;/div&gt;\n        &lt;/body&gt;\n        &lt;/html&gt;\n    ''')\n\n@app.route('/video_feed')\ndef video_feed():\n    return Response(generate_frames(),\n                    mimetype='multipart/x-mixed-replace; boundary=frame')\n\n@app.route('/start', methods=['POST'])\ndef start_classification():\n    global is_classifying\n    is_classifying = True\n    return '', 204\n\n@app.route('/stop', methods=['POST'])\ndef stop_classification():\n    global is_classifying\n    is_classifying = False\n    return '', 204\n\n@app.route('/update_confidence', methods=['POST'])\ndef update_confidence():\n    global confidence_threshold\n    confidence_threshold = float(request.form['confidence'])\n    return '', 204\n\n@app.route('/get_classification')\ndef get_classification():\n    if not is_classifying:\n        return jsonify({'label': 'Not classifying', 'probability': 0})\n    try:\n        result = classification_queue.get_nowait()\n    except Queue.Empty:\n        result = {'label': 'Processing', 'probability': 0}\n    return jsonify(result)\n\nif __name__ == '__main__':\n    initialize_camera()\n    threading.Thread(target=get_frame, daemon=True).start()\n    threading.Thread(target=classification_worker, daemon=True).start()\n    app.run(host='0.0.0.0', port=5000, threaded=True)\n\nSul terminale lanciare:\npython3 img_class_live_infer.py\nE accedere all‚Äôinterfaccia web:\n\nSul Raspberry Pi stesso (se si ha una GUI): si apre un browser web e si va suhttp://localhost:5000\nDa un altro dispositivo sulla stessa rete: aprire un browser web e andare su http://&lt;raspberry_pi_ip&gt;:5000 (Sostituire &lt;raspberry_pi_ip&gt; con l‚Äôindirizzo IP del Raspberry Pi). Per esempio: http://192.168.4.210:5000/\n\nEcco alcuni screenshot dell‚Äôapp in esecuzione su un desktop esterno\n\nQui si pu√≤ vedere l‚Äôapp in esecuzione su YouTube:\n\nIl codice crea un‚Äôapplicazione web per la classificazione delle immagini in tempo reale utilizzando un Raspberry Pi, il suo modulo fotocamera e un modello TensorFlow Lite. L‚Äôapplicazione utilizza Flask per fornire un‚Äôinterfaccia web in cui √® possibile visualizzare il feed della fotocamera e vedere i risultati della classificazione in tempo reale.\n\nComponenti Chiave:\n\nApplicazione Web Flask: Fornisce l‚Äôinterfaccia utente e gestisce le richieste.\nPiCamera2: Cattura le immagini dal modulo fotocamera Raspberry Pi.\nTensorFlow Lite: Esegue il modello di classificazione delle immagini.\nThreading: Gestisce le operazioni simultanee per prestazioni fluide.\n\n\n\nCaratteristiche Principali:\n\nVisualizzazione feed telecamera live\nClassificazione immagini in tempo reale\nSoglia di confidenza regolabile\nAvvia/Arresta classificazione su richiesta\n\n\n\nStruttura del Codice:\n\nImportazioni e Setup:\n\nFlask per applicazione web\nPiCamera2 per controllo telecamera\nTensorFlow Lite per l‚Äôinferenza\nThreading e Queue per operazioni concorrenti\n\nVariabili Globali:\n\nGestione telecamera e frame\nControllo classificazione\nInformazioni modello ed etichetta\n\nFunzioni della Telecamera:\n\ninitialize_camera(): Imposta PiCamera2\nget_frame(): Cattura continuamente frame\ngenerate_frames(): Genera frame per il feed web\n\nFunzioni del Modello:\n\nload_model(): Carica il modello TFLite\nclassify_image(): Esegue l‚Äôinferenza su una singola immagine\n\nWorker per la Classificazione:\n\nGira in un thread separato\nClassifica continuamente i frame quando √® attivo\nAaggiorna una coda con i risultati pi√π recenti\n\nRoute di Flask:\n\n/: Serve la pagina HTML principale\n/video_feed: Trasmette il feed della telecamera\n/start and /stop: Controlla la classificazione\n/update_confidence: Regola la soglia di confidenza\n/get_classification: Restituisce l‚Äôultimo risultato di classificazione\n\nTemplate HTML:\n\nVisualizza il feed della telecamera e la classificazione dei risultati\nFornisce controlli per avviare/arrestare e regolare le impostazioni\n\nEsecuzione Principale:\n\nInizializza la fotocamera e avvia i thread necessari\nEsegue l‚Äôapplicazione Flask\n\n\n\n\nConcetti Chiave:\n\nOperazioni Concorrenti: Utilizzo di thread per gestire l‚Äôacquisizione e la classificazione della telecamera separatamente dal server Web.\nAggiornamenti in Tempo Reale: Aggiornamenti frequenti dei risultati della classificazione senza ricaricare la pagina.\nRiutilizzo del Modello: Caricamento del modello TFLite una volta e il riutilizzo per l‚Äôefficienza.\nConfigurazione Flessibile: Consente agli utenti di regolare la soglia di confidenza al volo.\n\n\n\nUso:\n\nAssicurarsi che tutte le dipendenze siano installate.\nEseguire lo script su un Raspberry Pi con un modulo telecamera.\nAccedere all‚Äôinterfaccia Web da un browser utilizzando l‚Äôindirizzo IP del Raspberry Pi.\nAvviare la classificazione e regolare le impostazioni in base alle esigenze.",
    "crumbs": [
      "Raspberry Pi",
      "Classificazione delle Immagini"
    ]
  },
  {
    "objectID": "contents/labs/raspi/image_classification/image_classification.it.html#conclusione",
    "href": "contents/labs/raspi/image_classification/image_classification.it.html#conclusione",
    "title": "Classificazione delle Immagini",
    "section": "Conclusione:",
    "text": "Conclusione:\nLa classificazione delle immagini √® emersa come un‚Äôapplicazione potente e versatile dell‚Äôapprendimento automatico, con implicazioni significative per vari campi, dall‚Äôassistenza sanitaria al monitoraggio ambientale. Questo capitolo ha dimostrato come implementare un sistema di classificazione delle immagini robusto su dispositivi edge come Raspi-Zero e Raspi-5, mostrando il potenziale per l‚Äôintelligenza in tempo reale sul dispositivo.\nAbbiamo esplorato l‚Äôintera pipeline di un progetto di classificazione delle immagini, dalla raccolta dati e dall‚Äôaddestramento del modello tramite Edge Impulse Studio all‚Äôimplementazione e all‚Äôesecuzione di inferenze su un Raspi. Il processo ha evidenziato diversi punti chiave:\n\nL‚Äôimportanza di una corretta raccolta dati e pre-elaborazione per l‚Äôaddestramento efficace dei modelli.\nLa potenza dell‚Äôapprendimento tramite trasferimento, che ci consente di sfruttare modelli pre-addestrati come MobileNet V2 per un addestramento efficiente con dati limitati.\nI compromessi tra accuratezza del modello e velocit√† di inferenza, particolarmente cruciali per i dispositivi edge.\nL‚Äôimplementazione della classificazione in tempo reale tramite un‚Äôinterfaccia basata sul Web, che dimostra applicazioni pratiche.\n\nLa capacit√† di eseguire questi modelli su dispositivi edge come Raspi apre numerose possibilit√† per applicazioni IoT, sistemi autonomi e soluzioni di monitoraggio in tempo reale. Consente una latenza ridotta, una migliore privacy e il funzionamento in ambienti con connettivit√† limitata.\nCome abbiamo visto, anche con i vincoli computazionali dei dispositivi edge, √® possibile ottenere risultati impressionanti in termini di accuratezza e velocit√†. La flessibilit√† di regolare i parametri del modello, come le dimensioni di input e i valori alfa, consente una messa a punto precisa per soddisfare requisiti di progetto specifici.\nGuardando al futuro, il campo dell‚Äôintelligenza artificiale edge e della classificazione delle immagini continua a evolversi rapidamente. I progressi nelle tecniche di compressione dei modelli, nell‚Äôaccelerazione hardware e nelle architetture di reti neurali pi√π efficienti promettono di espandere ulteriormente le capacit√† dei dispositivi edge nelle attivit√† di visione artificiale.\nQuesto progetto funge da base per applicazioni di visione artificiale pi√π complesse e incoraggia un‚Äôulteriore esplorazione nell‚Äôentusiasmante mondo dell‚Äôintelligenza artificiale edge e dell‚ÄôIoT. Che si tratti di automazione industriale, applicazioni per la casa intelligente o monitoraggio ambientale, le competenze e i concetti trattati qui forniscono un solido punto di partenza per un‚Äôampia gamma di progetti innovativi.",
    "crumbs": [
      "Raspberry Pi",
      "Classificazione delle Immagini"
    ]
  },
  {
    "objectID": "contents/labs/raspi/image_classification/image_classification.it.html#risorse",
    "href": "contents/labs/raspi/image_classification/image_classification.it.html#risorse",
    "title": "Classificazione delle Immagini",
    "section": "Risorse",
    "text": "Risorse\n\nEsempio di Dataset\nImpostazione del Notebook di Prova su un Raspi\nNotebook di Classificazione delle Immagini su un Raspi\nCNN per classificare il dataset Cifar-10 su CoLab\nCifar 10 - Classificazione delle Immagini su un Raspi\nScript Python\nProgetto Edge Impulse",
    "crumbs": [
      "Raspberry Pi",
      "Classificazione delle Immagini"
    ]
  },
  {
    "objectID": "contents/labs/raspi/object_detection/object_detection.it.html",
    "href": "contents/labs/raspi/object_detection/object_detection.it.html",
    "title": "Rilevamento degli Oggetti",
    "section": "",
    "text": "Panoramica\nSulla base della nostra esplorazione della classificazione delle immagini, ora rivolgiamo la nostra attenzione a un‚Äôattivit√† di visione artificiale pi√π avanzata: il rilevamento degli oggetti. Mentre la classificazione delle immagini assegna una singola etichetta a un‚Äôintera immagine, il rilevamento degli oggetti va oltre identificando e localizzando pi√π oggetti all‚Äôinterno di una singola immagine. Questa capacit√† apre molte nuove applicazioni e sfide, in particolare nell‚Äôedge computing e nei dispositivi IoT come Raspberry Pi.\nIl rilevamento degli oggetti combina le attivit√† di classificazione e localizzazione. Non solo determina quali oggetti sono presenti in un‚Äôimmagine, ma individua anche le loro posizioni, ad esempio disegnando riquadri di delimitazione attorno a essi. Questa complessit√† aggiunta rende il rilevamento degli oggetti uno strumento pi√π potente per comprendere le scene visive, ma richiede anche modelli e tecniche di training pi√π sofisticati.\nNell‚ÄôIA edge, dove lavoriamo con risorse computazionali limitate, l‚Äôimplementazione di modelli di rilevamento degli oggetti efficienti diventa cruciale. Le sfide che abbiamo affrontato con la classificazione delle immagini, ovvero bilanciare le dimensioni del modello, la velocit√† di inferenza e l‚Äôaccuratezza, sono amplificate nel rilevamento degli oggetti. Tuttavia, i vantaggi sono anche pi√π significativi, poich√© il rilevamento degli oggetti consente un‚Äôanalisi dei dati visivi pi√π sfumata e dettagliata.\nAlcune applicazioni del rilevamento di oggetti su dispositivi edge includono:\nMettendo le mani nel rilevamento di oggetti, ci baseremo sui concetti e sulle tecniche esplorate nella classificazione delle immagini. Esamineremo le architetture di rilevamento di oggetti pi√π diffuse progettate per l‚Äôefficienza, come:\nEsploreremo quei modelli di rilevamento di oggetti utilizzando\nIn questo laboratorio, tratteremo i fondamenti del rilevamento degli oggetti e le sue differenze rispetto alla classificazione delle immagini. Impareremo anche come addestrare, mettere a punto, testare, ottimizzare e distribuire le architetture di rilevamento degli oggetti pi√π diffuse utilizzando un dataset creato da zero.",
    "crumbs": [
      "Raspberry Pi",
      "Rilevamento degli Oggetti"
    ]
  },
  {
    "objectID": "contents/labs/raspi/object_detection/object_detection.it.html#panoramica",
    "href": "contents/labs/raspi/object_detection/object_detection.it.html#panoramica",
    "title": "Rilevamento degli Oggetti",
    "section": "",
    "text": "Sistemi di sorveglianza e sicurezza\nVeicoli autonomi e droni\nControllo industriale della qualit√†\nMonitoraggio della fauna selvatica\nApplicazioni di realt√† aumentata\n\n\n\n‚ÄúSingle Stage Detectors‚Äù [Rilevatori a stadio singolo], come MobileNet ed EfficientDet,\nFOMO (Faster Objects, More Objects), e\nYOLO (You Only Look Once).\n\n\nPer saperne di pi√π sui modelli di rilevamento di oggetti, seguire il tutorial A Gentle Introduction to Object Recognition With Deep Learning.\n\n\n\nTensorFlow Lite Runtime (ora modificato in LiteRT),\nEdge Impulse Linux Python SDK e\nUltralitics\n\n\n\n\nFondamenti dell‚ÄôObject Detection\nIl rilevamento degli oggetti si basa sulle fondamenta della classificazione delle immagini, ma ne amplia notevolmente le capacit√†. Per comprendere il rilevamento degli oggetti, √® fondamentale innanzitutto riconoscere le sue principali differenze rispetto alla classificazione delle immagini:\n\nClassificazione delle Immagini vs.¬†Rilevamento degli Oggetti\nClassificazione delle Immagini:\n\nAssegna una singola etichetta a un‚Äôintera immagine\nRisponde alla domanda: ‚ÄúQual √® l‚Äôoggetto o la scena principale di questa immagine?‚Äù\nGenera una singola previsione di classe per l‚Äôintera immagine\n\nRilevamento degli oggetti:\n\nIdentifica e individua pi√π oggetti all‚Äôinterno di un‚Äôimmagine\nRisponde alle domande: ‚ÄúQuali oggetti sono presenti in questa immagine e dove si trovano?‚Äù\nGenera pi√π previsioni, ciascuna composta da un‚Äôetichetta di classe e un riquadro di delimitazione\n\nPer visualizzare questa differenza, prendiamo in considerazione un esempio: \nQuesto diagramma illustra la differenza critica: la classificazione delle immagini fornisce un‚Äôunica etichetta per l‚Äôintera immagine, mentre il rilevamento degli oggetti identifica pi√π oggetti, le loro classi e le loro posizioni all‚Äôinterno dell‚Äôimmagine.\n\n\nComponenti Chiave del Rilevamento degli Oggetti\nI sistemi di rilevamento degli oggetti sono in genere costituiti da due componenti principali:\n\nLocalizzazione degli Oggetti: Questo componente identifica dove si trovano gli oggetti nell‚Äôimmagine. In genere genera riquadri di delimitazione, regioni rettangolari che racchiudono ogni oggetto rilevato.\nClassificazione degli Oggetti: Questo componente determina la classe o la categoria di ogni oggetto rilevato, in modo simile alla classificazione delle immagini ma applicato a ogni regione localizzata.\n\n\n\nSfide nel Rilevamento degli Oggetti\nIl rilevamento degli oggetti presenta diverse sfide oltre a quelle della classificazione delle immagini:\n\nOggetti multipli: Un‚Äôimmagine pu√≤ contenere pi√π oggetti di varie classi, dimensioni e posizioni.\nScale variabili: Gli oggetti possono apparire a diverse dimensioni all‚Äôinterno dell‚Äôimmagine.\nOcclusione: Gli oggetti possono essere parzialmente nascosti o sovrapposti.\nDisordine di sfondo: Distinguere gli oggetti da sfondi complessi pu√≤ essere difficile.\nPrestazioni in tempo reale: Molte applicazioni richiedono tempi di inferenza rapidi, soprattutto su dispositivi edge.\n\n\n\nApprocci al Rilevamento di Oggetti\nEsistono due approcci principali al rilevamento di oggetti:\n\nRilevatori a due stadi: Prima propongono le regioni di interesse e poi classificano ciascuna regione. Esempi includono R-CNN e le sue varianti (Fast R-CNN, Faster R-CNN).\nRilevatori a stadio singolo: Questi prevedono bounding box (o centroidi) e probabilit√† di classe in un passaggio ‚Äúforward‚Äù [in avanti] della rete. Esempi includono YOLO (You Only Look Once), EfficientDet, SSD (Single Shot Detector) e FOMO (Faster Objects, More Objects). Questi sono spesso pi√π veloci e pi√π adatti per dispositivi edge come Raspberry Pi.\n\n\n\nMetriche di Valutazione\nIl rilevamento degli oggetti utilizza metriche diverse rispetto alla classificazione delle immagini:\n\nIntersection over Union (IoU): Misura la sovrapposizione tra bounding box previsti e ‚Äúground truth‚Äù [reali].\nMean Average Precision (mAP): Combina precisione e richiamo in tutte le classi e soglie IoU.\nFrames Per Second (FPS): Misura la velocit√† di rilevamento, fondamentale per le applicazioni in tempo reale su dispositivi edge.",
    "crumbs": [
      "Raspberry Pi",
      "Rilevamento degli Oggetti"
    ]
  },
  {
    "objectID": "contents/labs/raspi/object_detection/object_detection.it.html#panoramica-dei-modelli-di-rilevamento-degli-oggetti-pre-addestrati",
    "href": "contents/labs/raspi/object_detection/object_detection.it.html#panoramica-dei-modelli-di-rilevamento-degli-oggetti-pre-addestrati",
    "title": "Rilevamento degli Oggetti",
    "section": "Panoramica dei Modelli di Rilevamento degli Oggetti Pre-Addestrati",
    "text": "Panoramica dei Modelli di Rilevamento degli Oggetti Pre-Addestrati\nCome abbiamo visto nell‚Äôintroduzione, data un‚Äôimmagine o un flusso video, un modello di rilevamento degli oggetti pu√≤ identificare quale, di un set noto di oggetti, potrebbe essere presente e fornire informazioni sulle loro posizioni all‚Äôinterno dell‚Äôimmagine.\n\nSi possono testare alcuni modelli comuni online visitando Object Detection - MediaPipe Studio\n\nSu Kaggle, possiamo trovare i modelli tflite pre-addestrati pi√π comuni da usare con Raspi, ssd_mobilenet_v1 e EfficientDet. Quei modelli sono stati addestrati sul dataset COCO (Common Objects in Context), con oltre 200.000 immagini etichettate in 91 categorie. Scaricare i modelli e caricali nella cartella ./models in Raspi.\n\nIn alternativa, si possono trovare i modelli e le etichette COCO su GitHub.\n\nPer la prima parte di questo laboratorio, ci concentreremo su un modello SSD-Mobilenet V1 300x300 pre-addestrato e lo confronteremo con EfficientDet-lite0 320x320, anch‚Äôesso addestrato utilizzando il set di dati COCO 2017. Entrambi i modelli sono stati convertiti in un formato TensorFlow Lite (4,2 MB per SSD Mobilenet e 4,6 MB per EfficientDet).\n\nSSD-Mobilenet V2 o V3 √® consigliato per progetti di ‚Äútransfer learning‚Äù, ma una volta che il modello TFLite V1 sar√† disponibile al pubblico, lo useremo per questa panoramica.\n\n\n\nImpostazione dell‚ÄôAmbiente TFLite\nDovremmo confermare i passaggi eseguiti nell‚Äôultimo laboratorio pratico, Image Classification, come segue:\n\nAggiornamento di Raspberry Pi\nInstallazione delle Librerie Richieste\nImpostazione di un Ambiente Virtuale (Facoltativo ma Consigliato)\n\nsource ~/tflite/bin/activate\n\nInstallazione di TensorFlow Lite Runtime\nInstallazione di Additional Python Libraries (all‚Äôinterno dell‚Äôambiente)\n\n\n\nCreazione di una Directory di Lavoro:\nConsiderando che abbiamo creato la cartella Documents/TFLITE nell‚Äôultimo Lab, creiamo ora le cartelle specifiche per questo lab di rilevamento degli oggetti:\ncd Documents/TFLITE/\nmkdir OBJ_DETECT\ncd OBJ_DETECT\nmkdir images\nmkdir models\ncd models\n\n\nInferenza e Post-Elaborazione\nApriamo un nuovo notebook per seguire tutti i passaggi per rilevare oggetti su un‚Äôimmagine:\nImportare le librerie necessarie:\nimport time\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport tflite_runtime.interpreter as tflite\nCaricare il modello TFLite e allocare i tensori:\nmodel_path = \"./models/ssd-mobilenet-v1-tflite-default-v1.tflite\"\ninterpreter = tflite.Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\nOttenere i tensori di input e output.\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\nInput details ci informeranno su come il modello dovrebbe essere alimentato con un‚Äôimmagine. La forma di (1, 300, 300, 3) con un dtype di uint8 ci dice che un‚Äôimmagine non normalizzata (intervallo di valori pixel da 0 a 255) con dimensioni (300x300x3) dovrebbe essere inserita una alla volta (Batch Dimension: 1).\nGli output details includono non solo le etichette (‚Äúclasses‚Äù) e le probabilit√† (‚Äúscores‚Äù), ma anche la posizione relativa della finestra dei bounding box (‚Äúbox‚Äù) su dove si trova l‚Äôoggetto nell‚Äôimmagine e il numero di oggetti rilevati (‚Äúnum_detections‚Äù). I dettagli di output ci dicono anche che il modello pu√≤ rilevare un massimo di 10 oggetti nell‚Äôimmagine.\n\nQuindi, per l‚Äôesempio precedente, utilizzando la stessa immagine di gatto utilizzata col Lab di Classificazione Immagine alla ricerca dell‚Äôoutput, abbiamo una probabilit√† del 76% di aver trovato un oggetto con un ID classe di 16 su un‚Äôarea delimitata da un bounding box di [0.028011084, 0.020121813, 0.9886069, 0.802299]. Questi quattro numeri sono correlati a ymin, xmin, ymax e xmax, le coordinate del box.\nConsiderando che y va dall‚Äôalto (ymin) al basso (ymax) e x va da sinistra (xmin) a destra (xmax), abbiamo, di fatto, le coordinate dell‚Äôangolo superiore/sinistro e di quello inferiore/destro. Con entrambi i bordi e conoscendo la forma dell‚Äôimmagine, √® possibile disegnare un rettangolo attorno all‚Äôoggetto, come mostrato nella figura sottostante:\n\nSuccessivamente, dovremmo scoprire cosa significa un ID di classe uguale a 16. Aprendo il file coco_labels.txt, come un elenco, ogni elemento ha un indice associato e ispezionando l‚Äôindice 16, otteniamo, come previsto, cat. La probabilit√† √® il valore restituito dal punteggio.\nCarichiamo ora alcune immagini con pi√π oggetti su di esse per il test.\nimg_path = \"./images/cat_dog.jpeg\"\norig_img = Image.open(img_path)\n\n# Display the image\nplt.figure(figsize=(8, 8))\nplt.imshow(orig_img)\nplt.title(\"Original Image\")\nplt.show()\n\nIn base ai dettagli di input, pre-elaboriamo l‚Äôimmagine, modificandone la forma ed espandendone le dimensioni:\nimg = orig_img.resize((input_details[0]['shape'][1], \n                  input_details[0]['shape'][2]))\ninput_data = np.expand_dims(img, axis=0)\ninput_data.shape, input_data.dtype\nLa nuova forma input_data √®(1, 300, 300, 3) con un dtype di uint8, che √® compatibile con quanto previsto dal modello.\nUtilizzando input_data, eseguiamo l‚Äôinterprete, misuriamo la latenza e otteniamo l‚Äôoutput:\nstart_time = time.time()\ninterpreter.set_tensor(input_details[0]['index'], input_data)\ninterpreter.invoke()\nend_time = time.time()\ninference_time = (end_time - start_time) * 1000  # Convert to milliseconds\nprint (\"Inference time: {:.1f}ms\".format(inference_time))\nCon una latenza di circa 800 ms, possiamo ottenere 4 output distinti:\nboxes = interpreter.get_tensor(output_details[0]['index'])[0] \nclasses = interpreter.get_tensor(output_details[1]['index'])[0]  \nscores = interpreter.get_tensor(output_details[2]['index'])[0]   \nnum_detections = int(interpreter.get_tensor(output_details[3]['index'])[0])\nDa una rapida occhiata, possiamo vedere che il modello ha rilevato 2 oggetti con un punteggio superiore a 0,5:\nfor i in range(num_detections):\n    if scores[i] &gt; 0.5:  # Confidence threshold\n        print(f\"Object {i}:\")\n        print(f\"  Bounding Box: {boxes[i]}\")\n        print(f\"  Confidence: {scores[i]}\")\n        print(f\"  Class: {classes[i]}\")\n\nE possiamo anche visualizzare i risultati:\nplt.figure(figsize=(12, 8))\nplt.imshow(orig_img)\nfor i in range(num_detections):\n    if scores[i] &gt; 0.5:  # Adjust threshold as needed\n        ymin, xmin, ymax, xmax = boxes[i]\n        (left, right, top, bottom) = (xmin * orig_img.width, \n                                      xmax * orig_img.width, \n                                      ymin * orig_img.height, \n                                      ymax * orig_img.height)\n        rect = plt.Rectangle((left, top), right-left, bottom-top, \n                             fill=False, color='red', linewidth=2)\n        plt.gca().add_patch(rect)\n        class_id = int(classes[i])\n        class_name = labels[class_id]\n        plt.text(left, top-10, f'{class_name}: {scores[i]:.2f}', \n                 color='red', fontsize=12, backgroundcolor='white')\n\n\n\nEfficientDet\nEfficientDet non √® tecnicamente un modello SSD (Single Shot Detector), ma condivide alcune somiglianze e si basa su idee provenienti da SSD e altre architetture di rilevamento degli oggetti:\n\nEfficientDet:\n\nSviluppato dai ricercatori di Google nel 2019\nUtilizza EfficientNet come rete backbone\nUtilizza una nuova ‚Äúbi-directional feature pyramid network (BiFPN)‚Äù [rete piramidale di feature bidirezionale]\nUtilizza il ridimensionamento composto per ridimensionare in modo efficiente la rete backbone e i componenti di rilevamento degli oggetti.\n\nSomiglianze con SSD:\n\nEntrambi sono rilevatori a stadio singolo, il che significa che eseguono la localizzazione e la classificazione degli oggetti in un singolo passaggio ‚Äúforward‚Äù [in avanti].\nEntrambi utilizzano mappe di feature multiscala per rilevare oggetti a diverse scale.\n\nDifferenze principali:\n\nBackbone: SSD in genere utilizza VGG o MobileNet, mentre EfficientDet utilizza EfficientNet.\nFusione di funzionalit√†: SSD utilizza una semplice piramide di funzionalit√†, mentre EfficientDet utilizza il pi√π avanzato BiFPN.\nMetodo di ridimensionamento: EfficientDet introduce il ridimensionamento composto per tutti i componenti della rete\n\nVantaggi di EfficientDet:\n\nIn genere, raggiunge migliori compromessi tra accuratezza ed efficienza rispetto a SSD e molti altri modelli di rilevamento di oggetti.\nUn ridimensionamento pi√π flessibile consente una famiglia di modelli con diversi compromessi tra dimensioni e prestazioni.\n\n\nSebbene EfficientDet non sia un modello SSD, pu√≤ essere visto come un‚Äôevoluzione delle architetture di rilevamento a fase singola, che incorpora tecniche pi√π avanzate per migliorare efficienza e precisione. Quando si utilizza EfficientDet, possiamo aspettarci strutture di output simili a SSD (ad esempio, bounding box e punteggi di classe).\n\nSu GitHub, si trova un altro notebook che esplora il modello EfficientDet che abbiamo realizzato con SSD MobileNet.",
    "crumbs": [
      "Raspberry Pi",
      "Rilevamento degli Oggetti"
    ]
  },
  {
    "objectID": "contents/labs/raspi/object_detection/object_detection.it.html#progetto-di-rilevamento-di-oggetti",
    "href": "contents/labs/raspi/object_detection/object_detection.it.html#progetto-di-rilevamento-di-oggetti",
    "title": "Rilevamento degli Oggetti",
    "section": "Progetto di Rilevamento di Oggetti",
    "text": "Progetto di Rilevamento di Oggetti\nOra, svilupperemo un progetto completo di classificazione delle immagini dalla raccolta dei dati all‚Äôaddestramento e all‚Äôimplementazione. Come abbiamo fatto con il progetto di classificazione delle immagini, il modello addestrato e convertito verr√† utilizzato per l‚Äôinferenza.\nUtilizzeremo lo stesso set di dati per addestrare 3 modelli: SSD-MobileNet V2, FOMO e YOLO.\n\nL‚ÄôObiettivo\nTutti i progetti di Machine Learning devono iniziare con un obiettivo. Supponiamo di trovarci in una struttura industriale e di dover ordinare e contare ruote e scatole speciali.\n\nIn altre parole, dovremmo eseguire una classificazione multi-etichetta, in cui ogni immagine pu√≤ avere tre classi:\n\nBackground (nessun oggetto)\nBox [Scatola]\nWheel [Ruota]\n\n\n\nRaccolta Dati Grezzi\nUna volta definito l‚Äôobiettivo del nostro progetto di apprendimento automatico, il passaggio successivo, e pi√π cruciale, √® la raccolta del dataset. Possiamo utilizzare un telefono, il Raspi o un mix per creare il set di dati grezzi (senza etichette). Usiamo la semplice app Web sul nostro Raspberry Pi per visualizzare le immagini QVGA (320 x 240) catturate in un browser.\nDa GitHub, si prende lo script Python get_img_data.py e lo si apre nel terminale:\npython3 get_img_data.py\nAccedere all‚Äôinterfaccia web:\n\nSul Raspberry Pi stesso (se si ha una GUI): si apre un browser web e si va suhttp://localhost:5000\nDa un altro dispositivo sulla stessa rete: aprire un browser web e andare su http://&lt;raspberry_pi_ip&gt;:5000 (Sostituire &lt;raspberry_pi_ip&gt; con l‚Äôindirizzo IP del Raspberry Pi). Per esempio: http://192.168.4.210:5000/\n\nLo script Python crea un‚Äôinterfaccia basata sul Web per catturare e organizzare set di dati di immagini utilizzando un Raspberry Pi e la sua fotocamera. √à utile per progetti di apprendimento automatico che richiedono dati di immagini etichettati o meno, come nel nostro caso.\nAccedere all‚Äôinterfaccia Web da un browser, inserire un‚Äôetichetta generica per le immagini da catturare e premere Start Capture.\n\n\nNotare che le immagini da catturare avranno pi√π etichette che dovranno essere definite in seguito.\n\nUtilizzare l‚Äôanteprima live per posizionare la fotocamera e cliccare su Capture Image per salvare le immagini sotto l‚Äôetichetta corrente (in questo caso, box-wheel.\n\nQuando abbiamo abbastanza immagini, possiamo premere Stop Capture. Le immagini catturate vengono salvate nella cartella dataset/box-wheel:\n\n\nAcquisire circa 60 immagini. Provare ad acquisire con diverse angolazioni, sfondi e condizioni di luce. Filezilla pu√≤ trasferire il dataset raw creato sul computer principale.\n\n\n\nEtichettatura dei Dati\nIl passaggio successivo in un progetto di Object Detect √® creare un dataset etichettato. Dovremmo etichettare le immagini del dataset raw, creando riquadri di delimitazione attorno agli oggetti di ogni immagine (box e ruota). Possiamo usare strumenti di etichettatura come LabelImg, CVAT, Roboflow, o persino Edge Impulse Studio. Dopo aver esplorato lo strumento Edge Impulse in altri laboratori, usiamo Roboflow qui.\n\nStiamo usando Roboflow (versione gratuita) qui per due motivi principali. 1) Possiamo avere un auto-labeler e 2) Il dataset annotato √® disponibile in diversi formati e pu√≤ essere utilizzato sia su Edge Impulse Studio (lo useremo per MobileNet V2 e FOMO train) sia su CoLab (YOLOv8 train), ad esempio. Avendo il dataset annotato su Edge Impulse (account gratuito), non √® possibile utilizzarlo per il training su altre piattaforme.\n\nDovremmo caricare il dataset grezzo su Roboflow. Creare un account gratuito l√¨ e avviare un nuovo progetto, ad esempio (‚Äúbox-versus-wheel‚Äù).\n\n\nNon entreremo nei dettagli del processo Roboflow dato che sono disponibili molti tutorial.\n\n\nAnnotazione\nUna volta creato il progetto e caricato il dataset, si devono effettuare le annotazioni utilizzando lo strumento ‚ÄúAuto-Label‚Äù. Notare che si pu√≤ anche caricare immagini con solo uno sfondo, che dovrebbero essere salvate senza annotazioni.\n\nUna volta annotate tutte le immagini, si devono dividere in training, validation e testing.\n\n\n\nPre-elaborazione dei Dati\nL‚Äôultimo passaggio con il dataset √® la pre-elaborazione per generare una versione finale per il training. Ridimensioniamo tutte le immagini a 320x320 e generiamo versioni aumentate di ogni immagine (augmentation) per creare nuovi esempi di training da cui il nostro modello pu√≤ imparare.\nPer l‚Äôaugmentation, ruoteremo le immagini (+/-15o), ritaglieremo e varieremo la luminosit√† e l‚Äôesposizione.\n\nAlla fine del processo avremo 153 immagini.\n\nOra si deve esportare il dataset annotato in un formato che Edge Impulse, Ultralitics e altri framework/strumenti possano comprendere, ad esempio YOLOv8. Scarichiamo una versione compressa del dataset sul nostro desktop.\n\nQui √® possibile rivedere come √® stato strutturato il dataset\n\nCi sono 3 cartelle separate, una per ogni suddivisione (train/test/valid). Per ognuna di esse ci sono 2 sottocartelle, images e labels. Le immagini sono archiviate come image_id.jpg e images_id.txt, dove ‚Äúimage_id‚Äù √® univoco per ogni immagine.\nIl formato del file delle etichette sar√† class_id coordinate del riquadro di delimitazione, dove nel nostro caso class_id sar√† 0 per box e 1 per wheel. L‚ÄôID numerico (o, 1, 2‚Ä¶) seguir√† l‚Äôordine alfabetico del nome della classe.\nIl file data.yaml contiene informazioni sul set di dati come i nomi delle classi (names: ['box', 'wheel']) seguendo il formato YOLO.\nE questo √® tutto! Siamo pronti per iniziare l‚Äôaddestramento utilizzando Edge Impulse Studio (come faremo nel passaggio successivo), Ultralytics (come faremo quando discuteremo di YOLO) o persino l‚Äôaddestramento da zero su CoLab (come abbiamo fatto col dataset Cifar-10 nel laboratorio di classificazione delle immagini).\n\nIl dataset pre-elaborato si trova sul sito Roboflow, o qui:",
    "crumbs": [
      "Raspberry Pi",
      "Rilevamento degli Oggetti"
    ]
  },
  {
    "objectID": "contents/labs/raspi/object_detection/object_detection.it.html#addestramento-di-un-modello-ssd-mobilenet-su-edge-impulse-studio",
    "href": "contents/labs/raspi/object_detection/object_detection.it.html#addestramento-di-un-modello-ssd-mobilenet-su-edge-impulse-studio",
    "title": "Rilevamento degli Oggetti",
    "section": "Addestramento di un Modello SSD MobileNet su Edge Impulse Studio",
    "text": "Addestramento di un Modello SSD MobileNet su Edge Impulse Studio\nSi va su Edge Impulse Studio, si inseriscono le proprie credenziali in Login (o si crea un account) e si avvia un nuovo progetto.\n\nQui, √® possibile clonare il progetto sviluppato per questo laboratorio pratico: Raspi - Object Detection.\n\nNella scheda Dashboard del progetto, si va in basso e su Project info e per ‚ÄúLabeling method‚Äù si seleziona Bounding boxes (object detection)\n\nCaricamento dei dati annotati\nSu Studio, si va alla scheda Data acquisition e nella sezione UPLOAD DATA si carica dal computer il set di dati non elaborato.\nPossiamo usare l‚Äôopzione Select a folder, scegliendo, ad esempio, la cartella train nel computer, che contiene due sottocartelle, images e labels. Selezionare Image label format, ‚ÄúYOLO TXT‚Äù, caricare nella categoria Training e premere Upload data.\n\nRipetere il processo per i dati di test (caricare entrambe le cartelle, test e convalida). Alla fine del processo di caricamento, si ottiene il set di dati annotato di 153 immagini suddivise in train/test (84%/16%).\n\nNotare che le etichette saranno archiviate nei file di etichette 0 e 1, che sono equivalenti a box e wheel.\n\n\n\n\nImpulse Design\nLa prima cosa da definire quando entriamo nella fase Create impulse √® descrivere il dispositivo target per la distribuzione. Apparir√† una finestra pop-up. Selezioneremo Raspberry 4, un dispositivo intermedio tra Raspi-Zero e Raspi-5.\n\nQuesta scelta non interferir√† con l‚Äôaddestramento; ci dar√† solo un‚Äôidea della latenza del modello su quel target specifico.\n\n\nIn questa fase, si deve definire come:\n\nIl pre-processing consiste nel ridimensionare le singole immagini. Nel nostro caso, le immagini sono state pre-elaborate su Roboflow, a 320x320, quindi teniamole. La modifica delle dimensioni non avr√† importanza qui perch√© le immagini sono gi√† quadrate. Se si carica un‚Äôimmagine rettangolare, la si deve schiacciare (forma quadrata, senza ritagliarla). In seguito, si potrebbe definire se le immagini vengono convertite da RGB a scala di grigi o meno.\nDesign a Model, in questo caso, ‚ÄúObject Detection‚Äù.\n\n\n\n\nPre-elaborazione di tutti i dataset\nNella sezione Image, selezionare Color depth come RGB e premere Save parameters.\n\nLo Studio passa automaticamente alla sezione successiva, Generate features, dove tutti i campioni verranno pre-elaborati, ottenendo 480 oggetti: 207 box e 273 ruote.\n\nL‚Äôesploratore di feature mostra che tutti i campioni evidenziano una buona separazione dopo la generazione delle feature.\n\n\nProgettazione, Addestramento e Test del Modello\nPer l‚Äôaddestramento, dovremmo selezionare un modello pre-addestrato. Usiamo MobileNetV2 SSD FPN-Lite (solo 320x320) . √à un modello di rilevamento oggetti pre-addestrato progettato per individuare fino a 10 oggetti all‚Äôinterno di un‚Äôimmagine, generando un riquadro di delimitazione per ogni oggetto rilevato. Il modello √® di circa 3,7 MB. Supporta un input RGB a 320x320px.\nPer quanto riguarda gli iperparametri di training, il modello verr√† addestrato con:\n\nEpochs: 25\nBatch size: 32\nLearning Rate: 0.15.\n\nPer la convalida durante l‚Äôaddestramento, il 20% del set di dati (validation_dataset) verr√† risparmiato.\n\nDi conseguenza, il modello termina con un punteggio di precisione complessivo (basato su COCO mAP) dell‚Äô88,8%, superiore al risultato ottenuto utilizzando i dati di test (83,3%).\n\n\nDistribuzione del modello\nAbbiamo due modi per distribuire il modello:\n\nModello TFLite, che consente di distribuire il modello addestrato come .tflite affinch√© Raspi lo esegua tramite Python.\nLinux (AARCH64), un binario per Linux (AARCH64), implementa il protocollo Edge Impulse Linux, che consente di eseguire i modelli su qualsiasi scheda di sviluppo basata su Linux, ad esempio con SDK per Python. Consulta la documentazione per maggiori informazioni e le istruzioni di configurazione.\n\nDistribuiamo il modello TFLite. Nella scheda Dashboard, si va su Transfer learning model (int8 quantized) e si clicca sull‚Äôicona di download:\n\nTrasferire il modello dal computer alla cartella Raspi ./models e catturare o ottenere alcune immagini per l‚Äôinferenza e salvale nella cartella ./images.\n\n\nInferenza e Post-Elaborazione\nL‚Äôinferenza pu√≤ essere fatta come discusso nella Panoramica dei Modelli di Rilevamento degli Oggetti Pre-Addestrati. Iniziamo un nuovo notebook per seguire tutti i passaggi per rilevare cubi e ruote su un‚Äôimmagine.\nImportare le librerie necessarie:\nimport time\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nfrom PIL import Image\nimport tflite_runtime.interpreter as tflite\nDefinire il percorso del modello e delle etichette:\nmodel_path = \"./models/ei-raspi-object-detection-SSD-MobileNetv2-320x0320-\\\nint8.lite\"\nlabels = ['box', 'wheel']\n\nRicordare che il modello restituir√† l‚ÄôID della classe come valori (0 e 1), seguendo un ordine alfabetico in base ai nomi delle classi.\n\nCaricare il modello, allocare i tensori e ottenere i dettagli dei tensori di input e output:\n# Load the TFLite model\ninterpreter = tflite.Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n# Get input and output tensors\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\nUna differenza fondamentale da notare √® che il dtype dei dettagli di input del modello √® ora int8, il che significa che i valori di input vanno da -128 a +127, mentre ogni pixel della nostra immagine raw va da 0 a 256. Ci√≤ significa che dovremmo pre-elaborare l‚Äôimmagine per farla corrispondere. Possiamo controllare qui:\ninput_dtype = input_details[0]['dtype']\ninput_dtype\nnumpy.int8\nQuindi, apriamo l‚Äôimmagine e mostriamola:\n# Load the image\nimg_path = \"./images/box_2_wheel_2.jpg\"\norig_img = Image.open(img_path)\n\n# Display the image\nplt.figure(figsize=(6, 6))\nplt.imshow(orig_img)\nplt.title(\"Original Image\")\nplt.show()\n\nEd eseguiamo la pre-elaborazione:\nscale, zero_point = input_details[0]['quantization']\nimg = orig_img.resize((input_details[0]['shape'][1], \n                  input_details[0]['shape'][2]))\nimg_array = np.array(img, dtype=np.float32) / 255.0\nimg_array = (img_array / scale + zero_point).clip(-128, 127).astype(np.int8)\ninput_data = np.expand_dims(img_array, axis=0)\nControllando i dati di input, possiamo verificare che il tensore di input √® compatibile con quanto previsto dal modello:\ninput_data.shape, input_data.dtype\n((1, 320, 320, 3), dtype('int8'))\nAdesso √® il momento di effettuare l‚Äôinferenza. Calcoliamo anche la latenza del modello:\n# Inference on Raspi-Zero\nstart_time = time.time()\ninterpreter.set_tensor(input_details[0]['index'], input_data)\ninterpreter.invoke()\nend_time = time.time()\ninference_time = (end_time - start_time) * 1000  # Convert to milliseconds\nprint (\"Inference time: {:.1f}ms\".format(inference_time))\nIl modello impiegher√† circa 600 ms per eseguire l‚Äôinferenza nel Raspi-Zero, che √® circa 5 volte pi√π lungo di un Raspi-5.\nOra possiamo ottenere le classi di output degli oggetti rilevati, le coordinate dei suoi bounding box e le probabilit√†.\nboxes = interpreter.get_tensor(output_details[1]['index'])[0]  \nclasses = interpreter.get_tensor(output_details[3]['index'])[0]  \nscores = interpreter.get_tensor(output_details[0]['index'])[0]        \nnum_detections = int(interpreter.get_tensor(output_details[2]['index'])[0])\nfor i in range(num_detections):\n    if scores[i] &gt; 0.5:  # Confidence threshold\n        print(f\"Object {i}:\")\n        print(f\"  Bounding Box: {boxes[i]}\")\n        print(f\"  Confidence: {scores[i]}\")\n        print(f\"  Class: {classes[i]}\")\n\nDai risultati, possiamo vedere che sono stati rilevati 4 oggetti: due con ID classe 0 (box) e due con ID classe 1 (wheel), cosa √® corretto!\nVisualizziamo il risultato per un threshold [soglia] di 0,5\nthreshold = 0.5\nplt.figure(figsize=(6,6))\nplt.imshow(orig_img)\nfor i in range(num_detections):\n    if scores[i] &gt; threshold:  \n        ymin, xmin, ymax, xmax = boxes[i]\n        (left, right, top, bottom) = (xmin * orig_img.width, \n                                      xmax * orig_img.width, \n                                      ymin * orig_img.height, \n                                      ymax * orig_img.height)\n        rect = plt.Rectangle((left, top), right-left, bottom-top, \n                             fill=False, color='red', linewidth=2)\n        plt.gca().add_patch(rect)\n        class_id = int(classes[i])\n        class_name = labels[class_id]\n        plt.text(left, top-10, f'{class_name}: {scores[i]:.2f}', \n                 color='red', fontsize=12, backgroundcolor='white')\n\nMa cosa succede se riduciamo la soglia a 0.3, ad esempio?\n\nCominciamo a vedere falsi positivi e rilevazioni multiple, in cui il modello rileva lo stesso oggetto pi√π volte con diversi livelli di confidenza e bounding box leggermente diversi.\nDi solito, a volte, dobbiamo regolare la soglia su valori pi√π piccoli per catturare tutti gli oggetti, evitando falsi negativi, che porterebbero a rilevazioni multiple.\nPer migliorare i risultati di rilevamento, dovremmo implementare Non-Maximum Suppression (NMS), che aiuta a eliminare le bounding box sovrapposte e mantiene solo il rilevamento pi√π affidabile.\nPer questo, creiamo una funzione generale denominata non_max_suppression(), con il ruolo di perfezionare i risultati di rilevamento degli oggetti eliminando le bounding box ridondanti e sovrapposte. Ci√≤ √® possibile selezionando in modo iterativo la rilevazione con il punteggio di confidenza pi√π elevato e rimuovendo altre rilevazioni significativamente sovrapposte in base a una soglia di ‚ÄúIntersection over Union (IoU)‚Äù [intersezione su unione].\ndef non_max_suppression(boxes, scores, threshold):\n    # Convert to corner coordinates\n    x1 = boxes[:, 0]\n    y1 = boxes[:, 1]\n    x2 = boxes[:, 2]\n    y2 = boxes[:, 3]\n\n    areas = (x2 - x1 + 1) * (y2 - y1 + 1)\n    order = scores.argsort()[::-1]\n\n    keep = []\n    while order.size &gt; 0:\n        i = order[0]\n        keep.append(i)\n        xx1 = np.maximum(x1[i], x1[order[1:]])\n        yy1 = np.maximum(y1[i], y1[order[1:]])\n        xx2 = np.minimum(x2[i], x2[order[1:]])\n        yy2 = np.minimum(y2[i], y2[order[1:]])\n\n        w = np.maximum(0.0, xx2 - xx1 + 1)\n        h = np.maximum(0.0, yy2 - yy1 + 1)\n        inter = w * h\n        ovr = inter / (areas[i] + areas[order[1:]] - inter)\n\n        inds = np.where(ovr &lt;= threshold)[0]\n        order = order[inds + 1]\n\n    return keep\nCome funziona:\n\nSorting: Inizia ordinando tutte le rilevazioni in base ai loro punteggi di confidenza, dal pi√π alto al pi√π basso.\nSelection: Seleziona la casella con il punteggio pi√π alto e la aggiunge all‚Äôelenco finale delle rilevazioni.\nComparison: Questa casella selezionata viene confrontata con tutte le restanti caselle con punteggio pi√π basso.\nElimination: Qualsiasi casella che si sovrapponga in modo significativo (oltre la soglia IoU) alla casella selezionata viene eliminata.\nIteration: Questo processo si ripete con la casella con il punteggio pi√π alto successivo finch√© tutte le caselle non vengono elaborate.\n\nOra, possiamo definire una funzione di visualizzazione pi√π precisa che prender√† in considerazione una soglia IoU, rilevando solo gli oggetti selezionati dalla funzione non_max_suppression:\ndef visualize_detections(image, boxes, classes, scores, \n                         labels, threshold, iou_threshold):\n    if isinstance(image, Image.Image):\n        image_np = np.array(image)\n    else:\n        image_np = image\n\n    height, width = image_np.shape[:2]\n    \n    # Convert normalized coordinates to pixel coordinates\n    boxes_pixel = boxes * np.array([height, width, height, width])\n    \n    # Apply NMS\n    keep = non_max_suppression(boxes_pixel, scores, iou_threshold)\n    \n    # Set the figure size to 12x8 inches\n    fig, ax = plt.subplots(1, figsize=(12, 8))\n\n    ax.imshow(image_np)\n    \n    for i in keep:\n        if scores[i] &gt; threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            rect = patches.Rectangle((xmin * width, ymin * height),\n                                     (xmax - xmin) * width,\n                                     (ymax - ymin) * height,\n                                     linewidth=2, edgecolor='r', facecolor='none')\n            ax.add_patch(rect)\n            class_name = labels[int(classes[i])]\n            ax.text(xmin * width, ymin * height - 10,\n                    f'{class_name}: {scores[i]:.2f}', color='red',\n                    fontsize=12, backgroundcolor='white')\n\n    plt.show()\nOra possiamo creare una funzione che chiamer√† le altre, eseguendo l‚Äôinferenza su qualsiasi immagine:\ndef detect_objects(img_path, conf=0.5, iou=0.5):\n    orig_img = Image.open(img_path)\n    scale, zero_point = input_details[0]['quantization']\n    img = orig_img.resize((input_details[0]['shape'][1], \n                      input_details[0]['shape'][2]))\n    img_array = np.array(img, dtype=np.float32) / 255.0\n    img_array = (img_array / scale + zero_point).clip(-128, 127).\\\n    astype(np.int8)\n    input_data = np.expand_dims(img_array, axis=0)\n    \n    # Inference on Raspi-Zero\n    start_time = time.time()\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n    end_time = time.time()\n    inference_time = (end_time - start_time) * 1000  # Convert to ms\n    print (\"Inference time: {:.1f}ms\".format(inference_time))\n    \n    # Extract the outputs\n    boxes = interpreter.get_tensor(output_details[1]['index'])[0]  \n    classes = interpreter.get_tensor(output_details[3]['index'])[0]  \n    scores = interpreter.get_tensor(output_details[0]['index'])[0]        \n    num_detections = int(interpreter.get_tensor(output_details[2]['index'])[0])\n\n    visualize_detections(orig_img, boxes, classes, scores, labels, \n                         threshold=conf, \n                         iou_threshold=iou)\nOra, eseguendo il codice, ottenendo di nuovo la stessa immagine con una soglia di confidenza di 0.3, ma con un piccolo IoU:\nimg_path = \"./images/box_2_wheel_2.jpg\"\ndetect_objects(img_path, conf=0.3,iou=0.05)",
    "crumbs": [
      "Raspberry Pi",
      "Rilevamento degli Oggetti"
    ]
  },
  {
    "objectID": "contents/labs/raspi/object_detection/object_detection.it.html#training-di-un-modello-fomo-su-edge-impulse-studio",
    "href": "contents/labs/raspi/object_detection/object_detection.it.html#training-di-un-modello-fomo-su-edge-impulse-studio",
    "title": "Rilevamento degli Oggetti",
    "section": "Training di un modello FOMO su Edge Impulse Studio",
    "text": "Training di un modello FOMO su Edge Impulse Studio\nL‚Äôinferenza con il modello SSD MobileNet ha funzionato bene, ma la latenza era significativamente alta. L‚Äôinferenza variava da 0.5 a 1.3 secondi su un Raspi-Zero, il che significa circa o meno di 1 FPS (1 frame al secondo). Un‚Äôalternativa per accelerare il processo √® usare FOMO (Faster Objects, More Objects).\nQuesto nuovo algoritmo di apprendimento automatico consente di contare pi√π oggetti e di trovare la loro posizione in un‚Äôimmagine in tempo reale utilizzando fino a 30 volte meno potenza di elaborazione e memoria rispetto a MobileNet SSD o YOLO. Il motivo principale per cui ci√≤ √® possibile √® che mentre altri modelli calcolano le dimensioni dell‚Äôoggetto disegnando un quadrato attorno ad esso (bounding box), FOMO ignora le dimensioni dell‚Äôimmagine, fornendo solo le informazioni sulla posizione dell‚Äôoggetto nell‚Äôimmagine tramite le sue coordinate del centroide.\n\nCome funziona FOMO?\nIn una tipica pipeline di rilevamento degli oggetti, la prima fase consiste nell‚Äôestrarre le feature dall‚Äôimmagine di input. FOMO sfrutta MobileNetV2 per eseguire questa attivit√†. MobileNetV2 elabora l‚Äôimmagine di input per produrre una feature map che cattura le caratteristiche essenziali, come texture, forme e bordi degli oggetti, in modo computazionalmente efficiente.\n\nUna volta estratte queste feature, l‚Äôarchitettura pi√π semplice di FOMO, focalizzata sul rilevamento del punto centrale, interpreta la feature map per determinare dove si trovano gli oggetti nell‚Äôimmagine. L‚Äôoutput √® una griglia di celle, in cui ogni cella rappresenta se √® stato rilevato o meno un centro dell‚Äôoggetto. Il modello restituisce uno o pi√π punteggi di confidenza per ogni cella, indicando la probabilit√† che un oggetto sia presente.\nVediamo come funziona su un‚Äôimmagine.\nFOMO divide l‚Äôimmagine in blocchi di pixel usando un fattore di 8. Per l‚Äôinput di 96x96, la griglia √® 12x12 (96/8=12). Per un 160x160, la griglia sar√† 20x20 e cos√¨ via. Successivamente, FOMO eseguir√† un classificatore attraverso ogni blocco di pixel per calcolare la probabilit√† che ci sia un box o una ruota in ognuno di essi e, successivamente, determiner√† le regioni che hanno la pi√π alta probabilit√† di contenere l‚Äôoggetto (se un blocco di pixel non ha oggetti, verr√† classificato come background). Dalla sovrapposizione della regione finale, FOMO fornisce le coordinate (relative alle dimensioni dell‚Äôimmagine) del centroide di questa regione.\n\nCompromesso Tra Velocit√† e Precisione:\n\nRisoluzione della Griglia: FOMO utilizza una griglia di risoluzione fissa, il che significa che ogni cella pu√≤ rilevare se un oggetto √® presente in quella parte dell‚Äôimmagine. Sebbene non fornisca un‚Äôelevata precisione di localizzazione, fa un compromesso essendo veloce e computazionalmente leggero, il che √® fondamentale per i dispositivi edge.\nRilevamento Multi-Oggetto: Poich√© ogni cella √® indipendente, FOMO pu√≤ rilevare pi√π oggetti contemporaneamente in un‚Äôimmagine identificando pi√π centri.\n\n\n\nImpulse Design, nuovo Training e Test\nTornare a Edge Impulse Studio e nella scheda Experiments, creare un altro impulso. Ora, le immagini di input dovrebbero essere 160x160 (questa √® la dimensione di input prevista per MobilenetV2).\n\nNella scheda Image, generare le feature e spostarsi nella scheda Object detection.\nDovremmo selezionare un modello pre-addestrato per l‚Äôaddestramento. Usiamo FOMO (Faster Objects, More Objects) MobileNetV2 0.35.\n\nPer quanto riguarda gli iperparametri di training, il modello verr√† addestrato con:\n\nEpochs: 30\nBatch size: 32\nLearning Rate: 0.001.\n\nPer la convalida durante l‚Äôaddestramento, il 20% del set di dati (validation_dataset) verr√† risparmiato. Non applicheremo il ‚ÄúData Augmentation‚Äù per il restante 80% (train_dataset) perch√© il dataset √® gi√† stato aumentato durante la fase di etichettatura su Roboflow.\nDi conseguenza, il modello termina con un punteggio F1 complessivo del 93,3% con una latenza impressionante di 8 ms (Raspi-4), circa 60 volte inferiore a quella ottenuta con SSD MovileNetV2.\n\n\nNotare che FOMO ha aggiunto automaticamente una terza etichetta di background ai due box (0) e ruote (1) definite in precedenza.\n\nNella scheda Model testing, possiamo vedere che l‚Äôaccuratezza era del 94%. Ecco uno dei risultati del campione di test:\n\n\nNelle attivit√† di rilevamento di oggetti, l‚Äôaccuratezza non √® in genere la metrica di valutazione primaria. Il rilevamento di oggetti comporta la classificazione degli oggetti e la definizione di riquadri di delimitazione attorno a essi, il che lo rende un problema pi√π complesso della semplice classificazione. Il problema √® che non abbiamo il riquadro di delimitazione, solo i centroidi. In breve, usare l‚Äôaccuratezza come metrica potrebbe essere fuorviante e potrebbe non fornire una comprensione completa delle prestazioni del modello.\n\n\n\nDistribuzione del modello\nCome abbiamo fatto nella sezione precedente, possiamo distribuire il modello addestrato come TFLite o Linux (AARCH64). Ora facciamolo come Linux (AARCH64), un binario che implementa il protocollo Edge Impulse Linux.\nEdge Impulse per i modelli Linux viene fornito in formato .eim. Questo eseguibile contiene il nostro ‚Äúimpulso completo‚Äù creato in Edge Impulse Studio. L‚Äôimpulso √® costituito dai blocchi di elaborazione del segnale e da qualsiasi blocco di apprendimento e anomalia che abbiamo aggiunto e addestrato. √à compilato con ottimizzazioni per il nostro processore o GPU (ad esempio, istruzioni NEON su core ARM), pi√π un semplice layer IPC (su un socket Unix).\nNella scheda Deploy, selezionare l‚Äôopzione Linux (AARCH64), il modello int8 e premere Build.\n\nIl modello verr√† scaricato automaticamente sul computer.\nSul nostro Raspi, creiamo una nuova area di lavoro:\ncd ~\ncd Documents\nmkdir EI_Linux\ncd EI_Linux\nmkdir models\nmkdir images\nRinominare il modello per facilitarne l‚Äôidentificazione:\nAd esempio, raspi-object-detection-linux-aarch64-FOMO-int8.eim e trasferirlo nella nuova cartella Raspi ./models e catturare o ottenere alcune immagini per l‚Äôinferenza e salvarle nella cartella ./images.\n\n\nInferenza e Post-Elaborazione\nL‚Äôinferenza verr√† effettuata utilizzando Linux Python SDK. Questa libreria consente di eseguire modelli di apprendimento automatico e raccogliere dati dei sensori su macchine Linux utilizzando Python. L‚ÄôSDK √® open source e ospitato su GitHub: edgeimpulse/linux-sdk-python.\nImpostiamo un ambiente virtuale per lavorare con Linux Python SDK\npython3 -m venv ~/eilinux\nsource ~/eilinux/bin/activate\nE installare tutte le librerie necessarie:\nsudo apt-get update\nsudo apt-get install libatlas-base-dev libportaudio0 libportaudio2\nsudo apt-get installlibportaudiocpp0 portaudio19-dev\n\npip3 install edge_impulse_linux -i https://pypi.python.org/simple\npip3 install Pillow matplotlib pyaudio opencv-contrib-python\n\nsudo apt-get install portaudio19-dev\npip3 install pyaudio \npip3 install opencv-contrib-python\nConsentire al modello di essere eseguibile.\nchmod +x raspi-object-detection-linux-aarch64-FOMO-int8.eim\nInstallare Jupiter Notebook nel nuovo ambiente\npip3 install jupyter\nEseguire un notebook in locale (su Raspi-4 o 5 con desktop)\njupyter notebook\no sul browser del computer:\njupyter notebook --ip=192.168.4.210 --no-browser\nAvviamo un nuovo notebook seguendo tutti i passaggi per rilevare cubi e ruote su un‚Äôimmagine utilizzando il modello FOMO e l‚ÄôSDK Python di Edge Impulse Linux.\nImportare le librerie necessarie:\nimport sys, time\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nfrom PIL import Image\nimport cv2\nfrom edge_impulse_linux.image import ImageImpulseRunner\nDefinire il percorso del modello e delle etichette:\nmodel_file = \"raspi-object-detection-linux-aarch64-int8.eim\"\nmodel_path = \"models/\"+ model_file # Trained ML model from Edge Impulse\nlabels = ['box', 'wheel']\n\nRicordare che il modello restituir√† l‚ÄôID della classe come valori (0 e 1), seguendo un ordine alfabetico in base ai nomi delle classi.\n\nCaricare e inizializzare il modello:\n# Load the model file\nrunner = ImageImpulseRunner(model_path)\n\n# Initialize model\nmodel_info = runner.init()\nIl model_info conterr√† informazioni critiche sul modello. Tuttavia, a differenza dell‚Äôinterprete TFLite, la libreria EI Linux Python SDK preparer√† ora il modello per l‚Äôinferenza.\nQuindi, apriamo l‚Äôimmagine e mostriamola (ora, per compatibilit√†, useremo OpenCV, la libreria CV utilizzata internamente da EI. OpenCV legge l‚Äôimmagine come BGR, quindi dovremo convertirla in RGB:\n# Load the image\nimg_path = \"./images/1_box_1_wheel.jpg\"\norig_img = cv2.imread(img_path)\nimg_rgb = cv2.cvtColor(orig_img, cv2.COLOR_BGR2RGB)\n\n# Display the image\nplt.imshow(img_rgb)\nplt.title(\"Original Image\")\nplt.show()\n\nOra otterremo le feature e l‚Äôimmagine preelaborata (ritagliata) utilizzando runner:\nfeatures, cropped = runner.get_features_from_image_auto_studio_setings(img_rgb)\nEd eseguiamo l‚Äôinferenza. Calcoliamo anche la latenza del modello:\nres = runner.classify(features)\nOtteniamo le classi di output degli oggetti rilevati, i loro centroidi dei bounding box e le probabilit√†.\nprint('Found %d bounding boxes (%d ms.)' % (\n  len(res[\"result\"][\"bounding_boxes\"]), \n  res['timing']['dsp'] + res['timing']['classification']))\nfor bb in res[\"result\"][\"bounding_boxes\"]:\n    print('\\t%s (%.2f): x=%d y=%d w=%d h=%d' % (\n      bb['label'], bb['value'], bb['x'], \n      bb['y'], bb['width'], bb['height']))\nFound 2 bounding boxes (29 ms.)\n    1 (0.91): x=112 y=40 w=16 h=16\n    0 (0.75): x=48 y=56 w=8 h=8\nI risultati mostrano che sono stati rilevati due oggetti: uno con ID classe 0 (box) e uno con ID classe 1 (wheel), il che √® corretto!\nVisualizziamo il risultato (Il threshold [soglia] √® 0.5, il valore predefinito impostato durante il test del modello su Edge Impulse Studio).\nprint('\\tFound %d bounding boxes (latency: %d ms)' % (\n  len(res[\"result\"][\"bounding_boxes\"]), \n  res['timing']['dsp'] + res['timing']['classification']))\nplt.figure(figsize=(5,5))\nplt.imshow(cropped)\n\n# Go through each of the returned bounding boxes\nbboxes = res['result']['bounding_boxes']\nfor bbox in bboxes:\n\n    # Get the corners of the bounding box\n    left = bbox['x']\n    top = bbox['y']\n    width = bbox['width']\n    height = bbox['height']\n    \n    # Draw a circle centered on the detection\n    circ = plt.Circle((left+width//2, top+height//2), 5, \n                     fill=False, color='red', linewidth=3)\n    plt.gca().add_patch(circ)\n    class_id = int(bbox['label'])\n    class_name = labels[class_id]\n    plt.text(left, top-10, f'{class_name}: {bbox[\"value\"]:.2f}', \n              color='red', fontsize=12, backgroundcolor='white')\nplt.show()",
    "crumbs": [
      "Raspberry Pi",
      "Rilevamento degli Oggetti"
    ]
  },
  {
    "objectID": "contents/labs/raspi/object_detection/object_detection.it.html#esplorazione-di-un-modello-yolo-tramite-ultralitics",
    "href": "contents/labs/raspi/object_detection/object_detection.it.html#esplorazione-di-un-modello-yolo-tramite-ultralitics",
    "title": "Rilevamento degli Oggetti",
    "section": "Esplorazione di un Modello YOLO tramite Ultralitics",
    "text": "Esplorazione di un Modello YOLO tramite Ultralitics\nPer questo laboratorio, esploreremo YOLOv8. Ultralytics YOLOv8 √® una versione dell‚Äôacclamato modello di rilevamento di oggetti in tempo reale e segmentazione delle immagini, YOLO. YOLOv8 √® basato su progressi all‚Äôavanguardia nel deep learning e nella visione artificiale, offrendo prestazioni senza pari in termini di velocit√† e precisione. Il suo design semplificato lo rende adatto a varie applicazioni e facilmente adattabile a diverse piattaforme hardware, dai dispositivi edge alle API cloud.\n\nA proposito del modello YOLO\nIl modello YOLO (You Only Look Once) √® un algoritmo di rilevamento di oggetti altamente efficiente e ampiamente utilizzato, noto per le sue capacit√† di elaborazione in tempo reale. A differenza dei tradizionali sistemi di rilevamento di oggetti che riutilizzano classificatori o localizzatori per eseguire il rilevamento, YOLO inquadra il problema del rilevamento come un singolo compito di regressione. Questo approccio innovativo consente a YOLO di prevedere simultaneamente pi√π bounding box e le relative probabilit√† di classe da immagini complete in un‚Äôunica valutazione, aumentando notevolmente la sua velocit√†.\n\nCaratteristiche Principali:\n\nArchitettura ‚Äúingle Network‚Äù:\n\nYOLO impiega una singola rete neurale per elaborare l‚Äôintera immagine. Questa rete divide l‚Äôimmagine in una griglia e, per ogni cella della griglia, prevede direttamente i bounding box e le relative probabilit√† di classe. Questa formazione end-to-end migliora la velocit√† e semplifica l‚Äôarchitettura del modello.\n\nElaborazione in Tempo Reale:\n\nUna delle caratteristiche distintive di YOLO √® la sua capacit√† di eseguire il rilevamento di oggetti in tempo reale. A seconda della versione e dell‚Äôhardware, YOLO pu√≤ elaborare immagini con frame al secondo (FPS) elevati. Ci√≤ lo rende ideale per applicazioni che richiedono un rilevamento di oggetti rapido e accurato, come videosorveglianza, guida autonoma e analisi di eventi sportivi in diretta.\n\nEvoluzione delle Versioni:\n\nNel corso degli anni, YOLO ha subito miglioramenti significativi, da YOLOv1 all‚Äôultimo YOLOv10. Ogni iterazione ha introdotto miglioramenti in termini di accuratezza, velocit√† ed efficienza. YOLOv8, ad esempio, incorpora progressi nell‚Äôarchitettura di rete, metodologie di training migliorate e un supporto migliore per vari hardware, garantendo prestazioni pi√π robuste.\nSebbene YOLOv10 sia il membro pi√π recente della famiglia con prestazioni incoraggianti in base alla sua documentazione, √® stato appena rilasciato (maggio 2024) e non √® completamente integrato con la libreria Ultralitycs. Al contrario, l‚Äôanalisi della curva di precisione-richiamo suggerisce che YOLOv8 generalmente supera YOLOv9, catturando una percentuale maggiore di veri positivi e riducendo al minimo i falsi positivi in modo pi√π efficace (per maggiori dettagli, vedere questo articolo). Quindi, questo laboratorio si basa su YOLOv8n.\n\n\nPrecisione ed Efficienza:\n\nMentre le prime versioni di YOLO barattavano un po‚Äô di precisione per la velocit√†, le versioni recenti hanno fatto notevoli progressi nell‚Äôequilibrare entrambi. I modelli pi√π recenti sono pi√π veloci e precisi, rilevano piccoli oggetti (come le api) e funzionano bene su set di dati complessi.\n\nAmpia Gamma di Applicazioni:\n\nLa versatilit√† di YOLO ha portato alla sua adozione in numerosi campi. Viene utilizzato nei sistemi di monitoraggio del traffico per rilevare e contare i veicoli, nelle applicazioni di sicurezza per identificare potenziali minacce e nella tecnologia agricola per monitorare raccolti e bestiame. La sua applicazione si estende a qualsiasi dominio che richieda un rilevamento efficiente e accurato degli oggetti.\n\nComunit√† e Sviluppo:\n\nYOLO continua a evolversi ed √® supportato da una solida comunit√† di sviluppatori e ricercatori (essendo YOLOv8 molto forte). Le implementazioni open source e l‚Äôampia documentazione lo hanno reso accessibile per la personalizzazione e l‚Äôintegrazione in vari progetti. Framework di deep learning popolari come Darknet, TensorFlow e PyTorch supportano YOLO, ampliandone ulteriormente l‚Äôapplicabilit√†.\nUltralytics YOLOv8 non solo pu√≤ Detect [rilevare] (il nostro caso qui) ma anche Segmentare e mettere in posa con Pose modelli pre-addestrati sul set di dati COCO e YOLOv8 Classifica modelli pre-addestrati sul set di dati ImageNet. La modalit√† Track √® disponibile per tutti i modelli Detect, Segment e Pose.\n\n\n\n\nAttivit√† supportate da Ultralytics YOLO\n\n\n\n\n\n\nInstallazione\nSul nostro Raspi, disattiviamo l‚Äôambiente corrente per creare una nuova area di lavoro:\ndeactivate\ncd ~\ncd Documents/\nmkdir YOLO\ncd YOLO\nmkdir models\nmkdir images\nImpostiamo un Virtual Environment per lavorare con Ultralytics YOLOv8\npython3 -m venv ~/yolo\nsource ~/yolo/bin/activate\nE installiamo i pacchetti Ultralytics per l‚Äôinferenza locale sul Raspi\n\nAggiorniamo l‚Äôelenco dei pacchetti, installiamo pip ed eseguiamo l‚Äôaggiornamento all‚Äôultima versione:\n\nsudo apt update\nsudo apt install python3-pip -y\npip install -U pip\n\nInstalliamo il pacchetto pip ultralytics con le dipendenze opzionali:\n\npip install ultralytics[export]\n\nRiavviamo il dispositivo:\n\nsudo reboot\n\n\nTest di YOLO\nDopo l‚Äôavvio di Raspi-Zero, attiviamo l‚Äôambiente yolo, andiamo alla directory di lavoro,\nsource ~/yolo/bin/activate\ncd /Documents/YOLO\ned eseguiamo l‚Äôinferenza su un‚Äôimmagine che verr√† scaricata dal sito web di Ultralytics, che utilizza il modello YOLOV8n (il pi√π piccolo della famiglia) nel Terminal (CLI):\nyolo predict model='yolov8n' source='https://ultralytics.com/images/bus.jpg'\n\nLa famiglia di modelli YOLO √® pre-addestrata con il dataset COCO.\n\nIl risultato dell‚Äôinferenza apparir√† nel terminale. Nell‚Äôimmagine (bus.jpg), sono state rilevate 4 persone, 1 autobus e 1 segnale di stop:\n\nInoltre, abbiamo ricevuto un messaggio che indica Results saved to runs/detect/predict. Ispezionando quella directory, possiamo vedere una nuova immagine salvata (bus.jpg). Scarichiamola dal Raspi-Zero sul desktop per l‚Äôispezione:\n\nQuindi, Ultrayitics YOLO √® installato correttamente sul nostro Raspi. Ma, su Raspi-Zero, un problema √® l‚Äôelevata latenza per questa inferenza, circa 18 secondi, anche con il modello pi√π in miniatura della famiglia (YOLOv8n).\n\n\nEsportazione del Modello in formato NCNN\nL‚Äôimplementazione di modelli di visione artificiale su dispositivi edge con potenza di calcolo limitata, come Raspi-Zero, pu√≤ causare problemi di latenza. Un‚Äôalternativa √® quella di utilizzare un formato ottimizzato per prestazioni ottimali. Ci√≤ garantisce che anche i dispositivi con potenza di elaborazione limitata possano gestire bene attivit√† di visione artificiale avanzate.\nDi tutti i formati di esportazione del modello supportati da Ultralytics, NCNN √® un framework di elaborazione inferenziale di reti neurali ad alte prestazioni ottimizzato per piattaforme mobili. Fin dall‚Äôinizio della progettazione, NCNN √® stato profondamente attento all‚Äôimplementazione e all‚Äôuso su telefoni cellulari e non aveva dipendenze di terze parti. √à multipiattaforma e funziona pi√π velocemente di tutti i framework open source noti (come TFLite).\nNCNN offre le migliori prestazioni di inferenza quando si lavora con dispositivi Raspberry Pi. NCNN √® altamente ottimizzato per piattaforme mobili embedded (come l‚Äôarchitettura ARM).\nQuindi, convertiamo il nostro modello e rieseguiamo l‚Äôinferenza:\n\nEsportiamo un modello PyTorch YOLOv8n in formato NCNN, creando: ‚Äò/yolov8n_ncnn_model‚Äô\n\nyolo export model=yolov8n.pt format=ncnn\n\nEseguiamo l‚Äôinferenza con il modello esportato (ora la sorgente potrebbe essere l‚Äôimmagine bus.jpg scaricata dal sito Web nella directory corrente nell‚Äôultima inferenza):\n\nyolo predict model='./yolov8n_ncnn_model' source='bus.jpg'\n\nLa prima inferenza, quando il modello viene caricato, di solito ha una latenza elevata (circa 17 s), ma dalla seconda, √® possibile notare che l‚Äôinferenza scende a circa 2 s.\n\n\n\nEsplorare YOLO con Python\nPer iniziare, chiamiamo l‚Äôinterprete Python in modo da poter esplorare il funzionamento del modello YOLO, riga per riga:\npython3\nOra, dovremmo chiamare la libreria YOLO da Ultralitics e caricare il modello:\nfrom ultralytics import YOLO\nmodel = YOLO('yolov8n_ncnn_model')\nQuindi, eseguire l‚Äôinferenza su un‚Äôimmagine (usiamo di nuovo bus.jpg):\nimg = 'bus.jpg'\nresult = model.predict(img, save=True, imgsz=640, conf=0.5, iou=0.3)\n\nPossiamo verificare che il risultato √® quasi identico a quello che otteniamo eseguendo l‚Äôinferenza a livello di terminale (CLI), tranne per il fatto che la fermata dell‚Äôautobus non √® stata rilevata con il modello NCNN ridotto. Notare che la latenza √® stata ridotta.\nAnalizziamo il contenuto di ‚Äúresult‚Äù.\nAd esempio, possiamo vedere result[0].boxes.data, che mostra il risultato principale dell‚Äôinferenza, che √® un profilo tensoriale (4, 6). Ogni riga √® uno degli oggetti rilevati, ovvero le prime 4 colonne, le coordinate delle bounding box, la quinta, la confidenza e la sesta, la classe (in questo caso, 0: person e 5: bus):\n\nPossiamo accedere a diversi risultati di inferenza separatamente, come il tempo di inferenza, e stamparli in un formato migliore:\ninference_time = int(result[0].speed['inference'])\nprint(f\"Inference Time: {inference_time} ms\")\nOppure possiamo avere il numero totale di oggetti rilevati:\nprint(f'Number of objects: {len (result[0].boxes.cls)}')\n\nCon Python, possiamo creare un output dettagliato che soddisfi le nostre esigenze (vedere Model Prediction with Ultralytics YOLO per maggiori dettagli). Eseguiamo uno script Python invece di inserirlo manualmente riga per riga nell‚Äôinterprete, come mostrato di seguito. Usiamo nano come editor di testo. Per prima cosa, dovremmo creare uno script Python vuoto denominato, ad esempio, yolov8_tests.py:\nnano yolov8_tests.py\nSi inseriscono le righe di codice:\nfrom ultralytics import YOLO\n\n# Load the YOLOv8 model\nmodel = YOLO('yolov8n_ncnn_model')\n\n# Run inference\nimg = 'bus.jpg'\nresult = model.predict(img, save=False, imgsz=640, conf=0.5, iou=0.3)\n\n# print the results\ninference_time = int(result[0].speed['inference'])\nprint(f\"Inference Time: {inference_time} ms\")\nprint(f'Number of objects: {len (result[0].boxes.cls)}')\n\nE si inseriscono i comandi: [CTRL+O] + [INVIO] +[CTRL+X] per salvare lo script Python.\nEseguire lo script:\npython yolov8_tests.py\nIl risultato √® lo stesso dell‚Äôesecuzione dell‚Äôinferenza a livello di terminale (CLI) e con l‚Äôinterprete Python nativo.\n\nChiamare la libreria YOLO e caricare il modello per l‚Äôinferenza per la prima volta richiede molto tempo, ma le inferenze successive saranno molto pi√π veloci. Ad esempio, la prima singola inferenza pu√≤ richiedere diversi secondi, ma in seguito il tempo di inferenza dovrebbe essere ridotto a meno di 1 secondo.\n\n\n\nAddestramento di YOLOv8 su un Dataset Personalizzato\nTorniamo al nostro set di dati ‚ÄúBoxe versus Wheel‚Äù, etichettato su Roboflow. Nell‚Äôopzione Download Dataset, invece dell‚Äôopzione Download a zip to computer eseguita per l‚Äôaddestramento su Edge Impulse Studio, opteremo per Show download code. Questa opzione aprir√† una finestra pop-up con un frammento di codice che dovrebbe essere incollato nel nostro notebook di training.\n\nPer l‚Äôaddestramento, adattiamo uno degli esempi pubblici disponibili da Ultralitytics ed eseguiamolo su Google Colab. Di seguito, si pu√≤ trovare il mio da adattare al progetto:\n\nYOLOv8 Box versus Wheel Dataset Training [Open In Colab]\n\n\nPunti critici sul Notebook:\n\nEseguirlo con GPU (NVidia T4 √® gratuito)\nInstallare Ultralytics tramite PIP.\n\nOra, si pu√≤ importare YOLO e caricare il dataset sul CoLab, incollando il codice di download che riceviamo da Roboflow. Notare che il dataset verr√† montato in /content/datasets/:\n\n\n\n√à essenziale verificare e modificare il file data.yaml con il path corretto per le immagini (copiare il percorso su ogni cartella images).\n\nnames:\n- box\n- wheel\nnc: 2\nroboflow:\n  license: CC BY 4.0\n  project: box-versus-wheel-auto-dataset\n  url: https://universe.roboflow.com/marcelo-rovai-riila/box-versus-wheel-auto-dataset/dataset/5\n  version: 5\n  workspace: marcelo-rovai-riila\ntest: /content/datasets/Box-versus-Wheel-auto-dataset-5/test/images\ntrain: /content/datasets/Box-versus-Wheel-auto-dataset-5/train/images\nval: /content/datasets/Box-versus-Wheel-auto-dataset-5/valid/images\n\nDefinire i principali iperparametri da modificare rispetto ai valori di default, ad esempio:\nMODEL = 'yolov8n.pt'\nIMG_SIZE = 640\nEPOCHS = 25 # For a final project, you should consider at least 100 epochs\nEseguire il training (utilizzando la CLI):\n!yolo task=detect mode=train model={MODEL} data={dataset.location}/data.yaml epochs={EPOCHS} imgsz={IMG_SIZE} plots=True\n\n\n\nimage-20240910111319804\n\n\nL‚Äôaddestramento del modello ha richiesto alcuni minuti e ha prodotto risultati eccellenti (mAP50 pari a 0,995). Al termine del training, tutti i risultati vengono salvati nella cartella elencata, ad esempio: /runs/detect/train/. L√¨ si trova, ad esempio, la matrice di confusione.\n\n\n\nNotare che il modello addestrato (best.pt) viene salvato nella cartella /runs/detect/train/weights/. Ora, si deve convalidare il modello addestrato con valid/images.\n\n!yolo task=detect mode=val model={HOME}/runs/detect/train/weights/best.pt data={dataset.location}/data.yaml\nI risultati erano simili al training.\n\nOra, si deve eseguire l‚Äôinferenza sulle immagini lasciate a parte per il test\n\n!yolo task=detect mode=predict model={HOME}/runs/detect/train/weights/best.pt conf=0.25 source={dataset.location}/test/images save=True\nI risultati dell‚Äôinferenza vengono salvati nella cartella runs/detect/predict. Vediamone alcuni:\n\n\nSi consiglia di esportare i risultati di train, validation e test per un Drive su Google. Per farlo, si deve montare l‚Äôunit√†.\nfrom google.colab import drive\ndrive.mount('/content/gdrive')\ne copiare il contenuto della cartella /runs in una cartella che si deve creare nel Drive, ad esempio:\n!scp -r /content/runs '/content/gdrive/MyDrive/10_UNIFEI/Box_vs_Wheel_Project'\n\n\n\n\nInferenza col modello addestrato, usando Raspi\nScaricare il modello addestrato /runs/detect/train/weights/best.pt nel computer. Utilizzando l‚ÄôFTP FileZilla, trasferiamo best.pt nella cartella dei modelli Raspi (prima del trasferimento, si pu√≤ cambiare il nome del modello, ad esempio, box_wheel_320_yolo.pt).\nCon FileZilla FTP, trasferiamo alcune immagini dal set di dati di prova a .\\YOLO\\images:\nTorniamo alla cartella YOLO e utilizziamo l‚Äôinterprete Python:\ncd ..\npython\nCome prima, importeremo la libreria YOLO e definiremo il nostro modello convertito per rilevare le api:\nfrom ultralytics import YOLO\nmodel = YOLO('./models/box_wheel_320_yolo.pt')\nOra, definiamo un‚Äôimmagine e chiamiamo l‚Äôinferenza (stavolta salveremo il risultato dell‚Äôimmagine per la verifica esterna):\nimg = './images/1_box_1_wheel.jpg'\nresult = model.predict(img, save=True, imgsz=320, conf=0.5, iou=0.3)\nRipetiamo per diverse immagini. Il risultato dell‚Äôinferenza viene salvato sulla variabile result e l‚Äôimmagine elaborata su runs/detect/predict8\n\nCon FileZilla FTP, possiamo inviare il risultato dell‚Äôinferenza al Desktop per la verifica:\n\nPossiamo vedere che il risultato dell‚Äôinferenza √® eccellente! Il modello √® stato addestrato in base al modello base pi√π piccolo della famiglia YOLOv8 (YOLOv8n). Il problema √® la latenza, circa 1 secondo (o 1 FPS su Raspi-Zero). Naturalmente, possiamo ridurre questa latenza e convertire il modello in TFLite o NCNN.",
    "crumbs": [
      "Raspberry Pi",
      "Rilevamento degli Oggetti"
    ]
  },
  {
    "objectID": "contents/labs/raspi/object_detection/object_detection.it.html#rilevamento-di-oggetti-su-un-live-streaming",
    "href": "contents/labs/raspi/object_detection/object_detection.it.html#rilevamento-di-oggetti-su-un-live-streaming",
    "title": "Rilevamento degli Oggetti",
    "section": "Rilevamento di Oggetti su un Live Streaming",
    "text": "Rilevamento di Oggetti su un Live Streaming\nTutti i modelli esplorati in questo laboratorio possono rilevare oggetti in tempo reale utilizzando una telecamera. L‚Äôimmagine catturata dovrebbe essere l‚Äôinput per il modello addestrato e convertito. Per Raspi-4 o 5 con un desktop, OpenCV pu√≤ catturare i frame e visualizzare il risultato dell‚Äôinferenza.\nTuttavia, √® anche possibile creare un live streaming con una webcam per rilevare oggetti in tempo reale. Ad esempio, iniziamo con lo script sviluppato per l‚Äôapp Image Classification e adattiamolo per un‚ÄôApplicazione Web di Rilevamento di Oggetti in Tempo Reale Utilizzando TensorFlow Lite e Flask.\nQuesta versione dell‚Äôapp funzioner√† per tutti i modelli TFLite. Verificare che il modello sia nella cartella corretta, ad esempio:\nmodel_path = \"./models/ssd-mobilenet-v1-tflite-default-v1.tflite\"\nScaricare lo script Python object_detection_app.py da GitHub.\nE sul terminale, si esegue:\npython3 object_detection_app.py\nE accedere all‚Äôinterfaccia web:\n\nSul Raspberry Pi stesso (se si ha una GUI): si apre un browser web e si va suhttp://localhost:5000\nDa un altro dispositivo sulla stessa rete: aprire un browser web e andare su http://&lt;raspberry_pi_ip&gt;:5000 (Sostituire &lt;raspberry_pi_ip&gt; con l‚Äôindirizzo IP del Raspberry Pi). Per esempio: http://192.168.4.210:5000/\n\nEcco alcuni screenshot dell‚Äôapp in esecuzione su un desktop esterno\n\nDiamo un‚Äôocchiata a una descrizione tecnica dei moduli chiave utilizzati nell‚Äôapplicazione di rilevamento degli oggetti:\n\nTensorFlow Lite (tflite_runtime):\n\nScopo: Inferenza efficiente di modelli di machine learning su dispositivi edge.\nPerch√©: TFLite offre dimensioni ridotte del modello e prestazioni ottimizzate rispetto a TensorFlow completo, il che √® fondamentale per dispositivi con risorse limitate come Raspberry Pi. Supporta l‚Äôaccelerazione hardware e la quantizzazione, migliorando ulteriormente l‚Äôefficienza.\nFunzioni chiave: Interpreter per caricare ed eseguire il modello, get_input_details() e get_output_details() per l‚Äôinterfaccia col modello.\n\nFlask:\n\nScopo: Framework web leggero per la creazione del server backend.\nPerch√©: La semplicit√† e la flessibilit√† di Flask lo rendono ideale per lo sviluppo e la distribuzione rapidi di applicazioni web. Richiede meno risorse rispetto a framework pi√π grandi, adatto a dispositivi edge.\nComponenti chiave: decoratori di route per definire endpoint API, oggetti Response per lo streaming video, render_template_string per servire HTML dinamico.\n\nPicamera2:\n\nScopo: Interfaccia con il modulo fotocamera Raspberry Pi.\nPerch√©: Picamera2 √® la libreria pi√π recente per il controllo delle fotocamere Raspberry Pi, che offre prestazioni e funzionalit√† migliorate rispetto alla libreria Picamera originale.\nFunzioni chiave: create_preview_configuration() per impostare la fotocamera, capture_file() per catturare i fotogrammi.\n\nPIL (Python Imaging Library):\n\nScopo: Elaborazione e manipolazione delle immagini.\nPerch√©: PIL fornisce un‚Äôampia gamma di funzionalit√† di elaborazione delle immagini. Viene utilizzato qui per ridimensionare le immagini, disegnare riquadri di delimitazione e convertire tra formati di immagine.\nClassi chiave: Image per caricare e manipolare immagini, ImageDraw per disegnare forme e testo sulle immagini.\n\nNumPy:\n\nScopo: Operazioni array efficienti e calcolo numerico.\nPerch√©: le operazioni su array di NumPy sono molto pi√π veloci delle liste Python pure, il che √® fondamentale per elaborare in modo efficiente i dati delle immagini e gli input/output del modello.\nFunzioni chiave: array() per creare array, expand_dims() per aggiungere dimensioni agli array.\n\nThreading:\n\nScopo: Esecuzione simultanea di attivit√†.\nPerch√©: Il threading consente l‚Äôacquisizione simultanea di frame, il rilevamento di oggetti e il funzionamento del server Web, cruciali per il mantenimento delle prestazioni in tempo reale.\nComponenti chiave: La classe Thread crea thread di esecuzione separati e Lock viene utilizzato per la sincronizzazione dei thread.\n\nio.BytesIO:\n\nScopo: Stream binari in memoria.\nPerch√©: Consente una gestione efficiente dei dati delle immagini in memoria senza bisogno di file temporanei, migliorando la velocit√† e riducendo le operazioni di I/O.\n\ntime:\n\nScopo: Funzioni correlate al tempo.\nPerch√©: Utilizzato per aggiungere ritardi (time.sleep()) per controllare la frequenza dei fotogrammi e per le misure delle prestazioni.\n\njQuery (client-side):\n\nScopo: Manipolazione DOM semplificata e richieste AJAX.\nPerch√©: Semplifica l‚Äôaggiornamento dinamico dell‚Äôinterfaccia web e la comunicazione con il server senza ricaricamenti di pagina.\nFunzioni chiave: .get() e .post() per richieste AJAX, metodi di manipolazione DOM per l‚Äôaggiornamento dell‚Äôinterfaccia utente.\n\n\nPer quanto riguarda l‚Äôarchitettura del sistema dell‚Äôapp principale:\n\nMain Thread: Esegue il server Flask, gestisce le richieste HTTP e serve l‚Äôinterfaccia web.\nCamera Thread: Cattura continuamente i frame dalla fotocamera.\nDetection Thread: Elabora i frame tramite il modello TFLite per il rilevamento degli oggetti.\nFrame Buffer: Spazio di memoria condiviso (protetto da blocchi) che memorizza i frame pi√π recenti e i risultati del rilevamento.\n\nE il flusso di dati dell‚Äôapp, possiamo descriverlo in breve:\n\nLa fotocamera cattura il frame ‚Üí Frame Buffer\nIl thread di rilevamento legge dal Frame Buffer ‚Üí Elabora tramite il modello TFLite ‚Üí Aggiorna i risultati del rilevamento nel Frame Buffer\nFlask indirizza l‚Äôaccesso al Frame Buffer per fornire i risultati pi√π recenti del frame e del rilevamento\nIl client Web riceve gli aggiornamenti tramite AJAX e aggiorna l‚Äôinterfaccia utente\n\nQuesta architettura consente un rilevamento efficiente degli oggetti in tempo reale, mantenendo al contempo un‚Äôinterfaccia Web reattiva in esecuzione su un dispositivo edge con risorse limitate come un Raspberry Pi. Il threading e le librerie efficienti come TFLite e PIL consentono al sistema di elaborare i frame video in tempo reale, mentre Flask e jQuery forniscono un modo intuitivo per interagire con essi.\nSi pu√≤ testare l‚Äôapp con un altro modello pre-elaborato, come EfficientDet, modificando la riga dell‚Äôapp:\nmodel_path = \"./models/lite-model_efficientdet_lite0_detection_metadata_1.tflite\"\n\nPer usare l‚Äôapp per il modello SSD-MobileNetV2, addestrato su Edge Impulse Studio con il set di dati ‚ÄúBox versus Wheel‚Äù, il codice dovrebbe essere adattato anche in base ai dettagli di input, come abbiamo esplorato sul suo notebook.",
    "crumbs": [
      "Raspberry Pi",
      "Rilevamento degli Oggetti"
    ]
  },
  {
    "objectID": "contents/labs/raspi/object_detection/object_detection.it.html#conclusione",
    "href": "contents/labs/raspi/object_detection/object_detection.it.html#conclusione",
    "title": "Rilevamento degli Oggetti",
    "section": "Conclusione",
    "text": "Conclusione\nQuesto laboratorio ha esplorato l‚Äôimplementazione del rilevamento di oggetti su dispositivi edge come Raspberry Pi, dimostrando la potenza e il potenziale dell‚Äôesecuzione di attivit√† avanzate di computer vision su hardware con risorse limitate. Abbiamo trattato diversi aspetti essenziali:\n\nConfronto tra Modelli: Abbiamo esaminato diversi modelli di rilevamento oggetti, tra cui SSD-MobileNet, EfficientDet, FOMO e YOLO, confrontandone le prestazioni e i compromessi sui dispositivi edge.\nTraining e Deployment: Utilizzando un set di dati personalizzato di scatole e ruote (etichettato su Roboflow), abbiamo esaminato il processo di addestramento dei modelli utilizzando Edge Impulse Studio e Ultralytics e la loro distribuzione su Raspberry Pi.\nTecniche di Ottimizzazione: Per migliorare la velocit√† di inferenza sui dispositivi edge, abbiamo esplorato vari metodi di ottimizzazione, come la quantizzazione del modello (TFLite int8) e la conversione del formato (ad esempio, in NCNN).\nApplicazioni in Tempo Reale: Il laboratorio ha esemplificato un‚Äôapplicazione Web di rilevamento oggetti in tempo reale, dimostrando come questi modelli possono essere integrati in sistemi pratici e interattivi.\nConsiderazioni sulle Prestazioni: Durante il laboratorio abbiamo discusso l‚Äôequilibrio tra accuratezza del modello e velocit√† di inferenza, un aspetto fondamentale per le applicazioni di IA edge.\n\nLa capacit√† di eseguire il rilevamento di oggetti su dispositivi edge apre numerose possibilit√† in vari ambiti, dall‚Äôagricoltura di precisione, all‚Äôautomazione industriale e al controllo di qualit√†, alle applicazioni per la casa intelligente e al monitoraggio ambientale. Elaborando i dati localmente, questi sistemi possono offrire latenza ridotta, privacy migliorata e funzionamento in ambienti con connettivit√† limitata.\nGuardando al futuro, le potenziali aree di ulteriore esplorazione includono: - Implementazione di pipeline multi-modello per attivit√† pi√π complesse - Esplorazione di opzioni di accelerazione hardware per Raspberry Pi - Integrazione del rilevamento di oggetti con altri sensori per sistemi AI edge pi√π completi - Sviluppo di soluzioni edge-to-cloud che sfruttano sia l‚Äôelaborazione locale che le risorse cloud\nIl rilevamento di oggetti su dispositivi edge pu√≤ creare sistemi intelligenti e reattivi che portano la potenza dell‚ÄôIA direttamente nel mondo fisico, aprendo nuove frontiere nel modo in cui interagiamo e comprendiamo il nostro ambiente.",
    "crumbs": [
      "Raspberry Pi",
      "Rilevamento degli Oggetti"
    ]
  },
  {
    "objectID": "contents/labs/raspi/object_detection/object_detection.it.html#risorse",
    "href": "contents/labs/raspi/object_detection/object_detection.it.html#risorse",
    "title": "Rilevamento degli Oggetti",
    "section": "Risorse",
    "text": "Risorse\n\nDataset (‚ÄúBox versus Wheel‚Äù)\nSSD-MobileNet Notebook on a Raspi\nEfficientDet Notebook on a Raspi\nFOMO - EI Linux Notebook on a Raspi\nYOLOv8 Box versus Wheel Dataset Training on CoLab\nEdge Impulse Project - SSD MobileNet and FOMO\nScript Python\nModelli",
    "crumbs": [
      "Raspberry Pi",
      "Rilevamento degli Oggetti"
    ]
  },
  {
    "objectID": "contents/labs/raspi/llm/llm.it.html",
    "href": "contents/labs/raspi/llm/llm.it.html",
    "title": "Small Language Models (SLM)",
    "section": "",
    "text": "Panoramica\nNell‚Äôarea in rapida crescita dell‚Äôintelligenza artificiale, l‚Äôedge computing offre l‚Äôopportunit√† di decentralizzare le capacit√† tradizionalmente riservate a server potenti e centralizzati. Questo laboratorio esplora l‚Äôintegrazione pratica di piccole versioni di modelli linguistici tradizionali di grandi dimensioni (LLM) in un Raspberry Pi 5, trasformando questo dispositivo edge in un hub di IA in grado di elaborare dati in tempo reale e in loco.\nMan mano che i modelli linguistici di grandi dimensioni crescono in dimensioni e complessit√†, gli Small Language Model (SLM) offrono un‚Äôalternativa interessante per i dispositivi edge, raggiungendo un equilibrio tra prestazioni ed efficienza delle risorse. Eseguendo questi modelli direttamente su Raspberry Pi, possiamo creare applicazioni reattive e rispettose della privacy che funzionano anche in ambienti con connettivit√† Internet limitata o assente.\nQuesto laboratorio guider√† attraverso la configurazione, l‚Äôottimizzazione e lo sfruttamento degli SLM su Raspberry Pi. Esploreremo l‚Äôinstallazione e l‚Äôutilizzo di Ollama. Questo framework open source ci consente di eseguire LLM localmente sulle nostre macchine (i nostri desktop o dispositivi edge come Raspberry Pi o NVidia Jetson). Ollama √® progettato per essere efficiente, scalabile e facile da usare, il che lo rende una buona opzione per distribuire modelli di IA come Microsoft Phi, Google Gemma, Meta Llama e LLaVa (Multimodal). Integreremo alcuni di questi modelli in progetti utilizzando l‚Äôecosistema Python, esplorandone il potenziale in scenari del mondo reale (o almeno indicheremo questa direzione).",
    "crumbs": [
      "Raspberry Pi",
      "Small Language Models (SLM)"
    ]
  },
  {
    "objectID": "contents/labs/raspi/llm/llm.it.html#setup",
    "href": "contents/labs/raspi/llm/llm.it.html#setup",
    "title": "Small Language Models (SLM)",
    "section": "Setup",
    "text": "Setup\nNei laboratori precedenti avremmo potuto usare qualsiasi modello Raspi, ma qui la scelta deve ricadere sul Raspberry Pi 5 (Raspi-5). √à una piattaforma robusta che aggiorna sostanzialmente l‚Äôultima versione 4, dotata di Broadcom BCM2712, una CPU Arm Cortex-A76 quad-core a 64 bit da 2,4 GHz con Cryptographic Extension e capacit√† di caching migliorate. Vanta una GPU VideoCore VII, due uscite HDMI¬Æ 4Kp60 con HDR e un decoder HEVC 4Kp60. Le opzioni di memoria includono 4 GB e 8 GB di SDRAM LPDDR4X ad alta velocit√†, con 8 GB come nostra scelta per eseguire SLM. Presenta inoltre un‚Äôarchiviazione espandibile tramite uno slot per schede microSD e un‚Äôinterfaccia PCIe 2.0 per periferiche veloci come gli SSD M.2 (Solid State Drive).\n\nPer applicazioni SSL reali, gli SSD sono un‚Äôopzione migliore delle schede SD.\n\nA proposito, come ha spiegato Alasdair Allan, l‚Äôinferenza diretta sulla CPU Raspberry Pi 5, senza accelerazione GPU, √® ora alla pari con le prestazioni del Coral TPU.\n\nPer maggiori informazioni, consultare l‚Äôarticolo completo: Benchmarking TensorFlow e TensorFlow Lite su Raspberry Pi 5.\n\nRaspberry Pi Active Cooler\nSuggeriamo di installare un Active Cooler, una soluzione di raffreddamento clip-on dedicata per Raspberry Pi 5 (Raspi-5), per questo laboratorio. Combina un dissipatore di calore in alluminio con una ventola di raffreddamento a temperatura controllata per mantenere il Raspi-5 in funzione comodamente sotto carichi pesanti, come l‚Äôesecuzione di SLM.\n\nL‚ÄôActive Cooler ha dei pad termici pre-applicati per il trasferimento del calore ed √® montato direttamente sulla scheda Raspberry Pi 5 tramite perni a molla. Il firmware Raspberry Pi lo gestisce attivamente: a 60 ¬∞C, la ventola del soffiatore viene accesa; a 67,5 ¬∞C, la velocit√† della ventola verr√† aumentata; e infine, a 75 ¬∞C, la ventola aumenter√† fino alla massima velocit√†. La ventola del soffiatore rallenter√† automaticamente quando la temperatura scender√† al di sotto di questi limiti.\n\n\nPer evitare il surriscaldamento, tutte le schede Raspberry Pi iniziano a limitare il processore quando la temperatura raggiunge gli 80 ¬∞C e a limitarlo ulteriormente quando raggiunge la temperatura massima di 85 ¬∞C (maggiori dettagli qui).",
    "crumbs": [
      "Raspberry Pi",
      "Small Language Models (SLM)"
    ]
  },
  {
    "objectID": "contents/labs/raspi/llm/llm.it.html#generative-ai-genai",
    "href": "contents/labs/raspi/llm/llm.it.html#generative-ai-genai",
    "title": "Small Language Models (SLM)",
    "section": "Generative AI (GenAI)",
    "text": "Generative AI (GenAI)\nGenerative AI √® un sistema di IA in grado di creare nuovi contenuti originali su vari supporti come testo, immagini, audio e video. Questi sistemi apprendono modelli da dati esistenti e utilizzano tale conoscenza per generare nuovi output che in precedenza non esistevano. Large Language Models (LLM), Small Language Models (SLM) e modelli multimodali possono essere tutti considerati tipi di GenAI quando utilizzati per attivit√† generative.\nGenAI fornisce il framework concettuale per la creazione di contenuti basati sull‚Äôintelligenza artificiale, con gli LLM che fungono da potenti generatori di testo per uso generale. Gli SLM adattano questa tecnologia all‚Äôedge computing, mentre i modelli multimodali estendono le capacit√† di GenAI a diversi tipi di dati. Insieme, rappresentano uno spettro di tecnologie di intelligenza artificiale generativa, ciascuna con i suoi punti di forza e applicazioni, che guidano collettivamente la creazione e la comprensione di contenuti basati sull‚ÄôIA.\n\nLarge Language Models (LLM)\nI ‚ÄúLarge Language Model (LLM)‚Äù sono sistemi avanzati di intelligenza artificiale che comprendono, elaborano e generano testi simili a quelli umani. Questi modelli sono caratterizzati dalla loro enorme scala in termini di quantit√† di dati su cui vengono addestrati e numero di parametri che contengono. Gli aspetti critici degli LLM includono:\n\nDimensioni: Gli LLM contengono in genere miliardi di parametri. Ad esempio, GPT-3 ha 175 miliardi di parametri, mentre alcuni modelli pi√π recenti superano un trilione di parametri.\nDati di Addestramento: Vengono addestrati su grandi quantit√† di dati di testo, spesso inclusi libri, siti Web e altre fonti diverse, che ammontano a centinaia di gigabyte o persino terabyte di testo.\nArchitettura: La maggior parte degli LLM utilizza architetture basate su trasformatori, che consentono loro di elaborare e generare testo prestando attenzione a diverse parti dell‚Äôinput contemporaneamente.\nCapacit√†: Gli LLM possono eseguire un‚Äôampia gamma di attivit√† linguistiche senza una messa a punto specifica, tra cui:\n\nGenerazione di testo\nTraduzione\nRiepilogo\nRisposte a domande\nGenerazione di codice\nRagionamento logico\n\nApprendimento a Intervalli: Spesso possono comprendere ed eseguire nuove attivit√† con esempi o istruzioni minimi.\nRichiede molte Risorse: A causa delle loro dimensioni, gli LLM richiedono in genere risorse di elaborazione significative per funzionare, spesso necessitando di potenti GPU o TPU.\nSviluppo Continuo: Il campo degli LLM √® in rapida evoluzione, con nuovi modelli e tecniche che emergono costantemente.\nConsiderazioni Etiche: L‚Äôuso degli LLM solleva importanti questioni su pregiudizi, disinformazione e impatto ambientale della formazione di modelli cos√¨ grandi.\nApplicazioni: Gli LLM sono utilizzati in vari campi, tra cui la creazione di contenuti, il servizio clienti, l‚Äôassistenza alla ricerca e lo sviluppo di software.\nLimitazioni: Nonostante la loro potenza, gli LLM possono produrre informazioni errate o distorte e non hanno capacit√† di vera comprensione o ragionamento.\n\nDobbiamo notare che utilizziamo modelli di grandi dimensioni oltre al testo, chiamandoli modelli multimodali. Questi modelli integrano ed elaborano informazioni da pi√π tipi di input contemporaneamente. Sono progettati per comprendere e generare contenuti in varie forme di dati, come testo, immagini, audio e video.\nCertamente. Definiamo modelli ‚Äúopen‚Äù [aperti] e ‚Äúclosed‚Äù [chiusi] nel contesto di modelli di linguaggio e IA:\n\n\nModelli Chiusi e Aperti:\nModelli Chiusi, detti anche modelli proprietari, sono modelli di IA il cui funzionamento interno, codice e dati di addestramento non sono divulgati pubblicamente. Esempi: GPT-4 (di OpenAI), Claude (di Anthropic), Gemini (di Google).\nModelli Aperti, noti anche come modelli open source, sono modelli di intelligenza artificiale il cui codice sottostante, architettura e spesso dati di addestramento sono disponibili e accessibili al pubblico. Esempi: Gemma (di Google), LLaMA (di Meta) e Phi (di Microsoft).\nI modelli aperti sono particolarmente rilevanti per l‚Äôesecuzione di modelli su dispositivi edge come Raspberry Pi in quanto possono essere pi√π facilmente adattati, ottimizzati e distribuiti in ambienti con risorse limitate. Tuttavia, √® fondamentale verificare le loro Licenze. I modelli aperti sono dotati di varie licenze open source che possono influenzare il loro utilizzo in applicazioni commerciali, mentre i modelli chiusi hanno termini di servizio chiari, seppur restrittivi.\n\n\n\nAdattato da https://arxiv.org/pdf/2304.13712\n\n\n\n\nSmall Language Models (SLM)\nNel contesto dell‚Äôedge computing su dispositivi come Raspberry Pi, gli LLM su larga scala sono in genere troppo grandi e dispendiosi in termini di risorse per essere eseguiti direttamente. Questa limitazione ha spinto lo sviluppo di modelli pi√π piccoli ed efficienti, come gli Small Language Models (SLM).\nGli SLM sono versioni compatte degli LLM progettate per essere eseguite in modo efficiente su dispositivi con risorse limitate come smartphone, dispositivi IoT e computer a scheda singola come Raspberry Pi. Questi modelli sono significativamente pi√π piccoli in termini di dimensioni e requisiti di calcolo rispetto alle loro controparti pi√π grandi, pur mantenendo impressionanti capacit√† di comprensione e generazione del linguaggio.\nLe caratteristiche principali degli SLM includono:\n\nConteggio ridotto dei parametri: In genere vanno da poche centinaia di milioni a qualche miliardo di parametri, rispetto ai miliardi a due cifre nei modelli pi√π grandi.\nMeno ingombro di memoria: Richiedono, al massimo, pochi gigabyte di memoria anzich√© decine o centinaia di gigabyte.\nTempo di inferenza pi√π rapido: Possono generare risposte in millisecondi o secondi su dispositivi edge.\nEfficienza energetica: Consumano meno energia, rendendoli adatti per dispositivi alimentati a batteria.\nTutela della privacy: Abilitano l‚Äôelaborazione sul dispositivo senza inviare dati ai server cloud.\nFunzionalit√† offline: Funzionano senza una connessione Internet.\n\nGli SLM raggiungono le loro dimensioni compatte attraverso varie tecniche come la distillazione della conoscenza, la potatura del modello e la quantizzazione. Sebbene non possano eguagliare le ampie capacit√† dei modelli pi√π grandi, gli SLM eccellono in compiti e domini specifici, il che li rende ideali per applicazioni mirate su dispositivi edge.\n\nIn genere prenderemo in considerazione gli SLM, modelli linguistici con meno di 5 miliardi di parametri quantizzati a 4 bit.\n\nEsempi di SLM includono versioni compresse di modelli come Meta Llama, Microsoft PHI e Google Gemma. Questi modelli consentono un‚Äôampia gamma di attivit√† di elaborazione del linguaggio naturale direttamente sui dispositivi edge, dalla classificazione del testo e analisi del sentiment alle risposte alle domande e alla generazione di testo limitato.\nPer maggiori informazioni sugli SLM, il documento, LLM Pruning and Distillation in Practice: The Minitron Approach, fornisce un approccio che applica il pruning e la distillazione per ottenere SLM da LLM. E, SMALL LANGUAGE MODELS: SURVEY, MEASUREMENTS, AND INSIGHTS, presenta un‚Äôindagine e un‚Äôanalisi complete di Small Language Models (SLM), che sono modelli linguistici con da 100 milioni a 5 miliardi di parametri progettati per dispositivi con risorse limitate.",
    "crumbs": [
      "Raspberry Pi",
      "Small Language Models (SLM)"
    ]
  },
  {
    "objectID": "contents/labs/raspi/llm/llm.it.html#ollama",
    "href": "contents/labs/raspi/llm/llm.it.html#ollama",
    "title": "Small Language Models (SLM)",
    "section": "Ollama",
    "text": "Ollama\n\n\n\nlogo di ollama\n\n\nOllama √® un framework open source che ci consente di eseguire modelli linguistici (LM), grandi o piccoli, localmente sulle nostre macchine. Ecco alcuni punti critici su Ollama:\n\nEsecuzione del Modello Locale: Ollama consente di eseguire LM su personal computer o dispositivi edge come Raspi-5, eliminando la necessit√† di chiamate API basate su cloud.\nFacilit√† d‚ÄôUso: Fornisce una semplice interfaccia a riga di comando per scaricare, eseguire e gestire diversi modelli linguistici.\nVariet√† di Modelli: Ollama supporta vari LLM, tra cui Phi, Gemma, Llama, Mistral e altri modelli open source.\nPersonalizzazione: Gli utenti possono creare e condividere modelli personalizzati su misura per esigenze o domini specifici.\nLeggero: Progettato per essere efficiente e funzionare su hardware di livello consumer.\nIntegrazione API: Offre un‚ÄôAPI che consente l‚Äôintegrazione con altre applicazioni e servizi.\nIncentrato sulla Privacy: Eseguendo i modelli localmente, affronta i problemi di privacy associati all‚Äôinvio di dati a server esterni.\nMultipiattaforma: Disponibile per sistemi macOS, Windows e Linux (il nostro caso, qui).\nSviluppo Attivo: Aggiornato regolarmente con nuove funzionalit√† e supporto per i modelli.\nGuidato dalla Community: Trae vantaggio dai contributi della community e dalla condivisione dei modelli.\n\nPer saperne di pi√π su cosa sia Ollama e come funziona sotto internamente, si pu√≤ guardare questo breve video di Matt Williams, uno dei fondatori di Ollama:\n\n\nMatt ha un corso completamente gratuito su Ollama che consigliamo: \n\n\nInstallazione di Ollama\nImpostiamo e attiviamo un ambiente virtuale per lavorare con Ollama:\npython3 -m venv ~/ollama\nsource ~/ollama/bin/activate\nEd eseguiamo il comando per installare Ollama:\ncurl -fsSL https://ollama.com/install.sh | sh\nDi conseguenza, un‚ÄôAPI verr√† eseguita in background su 127.0.0.1:11434. D‚Äôora in poi, possiamo eseguire Ollama tramite il terminale. Per iniziare, verifichiamo la versione di Ollama, che ci dir√† anche se √® installata correttamente:\nollama -v\n\nNella pagina della Libreria Ollama, possiamo trovare i modelli supportati da Ollama. Ad esempio, filtrando per Most popular, possiamo vedere Meta Llama, Google Gemma, Microsoft Phi, LLaVa, ecc.\n\n\nMeta Llama 3.2 1B/3B\n\nInstalliamo ed eseguiamo il nostro primo piccolo modello linguistico, Llama 3.2 1B (e 3B). La serie Meta Llama 3.2 comprende un set di modelli linguistici generativi multilingue disponibili in dimensioni di parametri pari a 1 miliardo e 3 miliardi. Questi modelli sono progettati per elaborare input di testo e generare output di testo. Le varianti sintonizzate sulle istruzioni all‚Äôinterno di questa raccolta sono specificamente ottimizzate per applicazioni conversazionali multilingue, tra cui attivit√† che comportano il recupero e la sintesi delle informazioni con un approccio agentico. Rispetto a molti modelli di chat open source e proprietari esistenti, i modelli sintonizzati sulle istruzioni Llama 3.2 dimostrano prestazioni superiori su benchmark di settore ampiamente utilizzati.\nI modelli 1B e 3B sono stati potati da Llama 8B, quindi i logit dai modelli 8B e 70B sono stati utilizzati come target a livello di token (distillazione a livello di token). La distillazione della conoscenza √® stata utilizzata per recuperare le prestazioni (sono stati addestrati con 9 trilioni di token). Il modello 1B ha 1,24B, quantizzati a intero (Q8_0), e i parametri 3B, 3,12B, con una quantizzazione Q4_0, che termina con una dimensione di 1,3 GB e 2 GB, rispettivamente. La sua finestra di contesto √® di 131.072 token.\n\nInstallare ed eseguire il Model\nollama run llama3.2:1b\nEseguendo il modello col comando precedente, dovremmo avere il prompt di Ollama disponibile per inserire una domanda e iniziare a chattare con il modello LLM; ad esempio,\n&gt;&gt;&gt; What is the capital of France?\nQuasi immediatamente, otteniamo la risposta corretta:\nThe capital of France is Paris.\nUtilizzando l‚Äôopzione --verbose quando si richiama il modello, verranno generate diverse statistiche sulle sue prestazioni (Il modello eseguir√† il polling solo la prima volta che eseguiamo il comando).\n\nOgni metrica fornisce informazioni su come il modello elabora gli input e genera gli output. Ecco una ripartizione del significato di ogni metrica:\n\nTotal Duration (2.620170326s): Questo √® il tempo completo impiegato dall‚Äôinizio del comando al completamento della risposta. Comprende il caricamento del modello, l‚Äôelaborazione del prompt di input e la generazione della risposta.\nLoad Duration (39.947908ms): Questa durata indica il tempo necessario per caricare il modello o i componenti necessari nella memoria. Se questo valore √® minimo, pu√≤ suggerire che il modello √® stato precaricato o che √® stata richiesta solo una configurazione minima.\nPrompt Eval Count (32 tokens): Il numero di token nel prompt di input. In NLP, i token sono in genere parole o sotto-parole, quindi questo conteggio include tutti i token che il modello ha valutato per comprendere e rispondere alla query.\nPrompt Eval Duration (1.644773s): Misura il tempo impiegato dal modello per valutare o elaborare il prompt di input. Rappresenta la maggior parte della durata totale, il che implica che comprendere la query e preparare una risposta √® la parte pi√π dispendiosa in termini di tempo del processo.\nPrompt Eval Rate (19.46 tokens/s): Questa frequenza indica la rapidit√† con cui il modello elabora i token dal prompt di input. Riflette la velocit√† del modello in termini di comprensione del linguaggio naturale.\nEval Count (8 token(s)): Questo √® il numero di token nella risposta del modello, che in questo caso era ‚ÄúThe capital of France is Paris.‚Äù\nEval Duration (889.941ms): Questo √® il tempo impiegato per generare l‚Äôoutput in base all‚Äôinput valutato. √à molto pi√π breve della valutazione del prompt, il che suggerisce che generare la risposta √® meno complesso o computazionalmente intensivo rispetto alla comprensione del prompt.\nEval Rate (8.99 tokens/s): Simile alla frequenza di valutazione del prompt, indica la velocit√† con cui il modello genera token di output. √à una metrica fondamentale per comprendere l‚Äôefficienza del modello nella generazione di output.\n\nQuesta ripartizione dettagliata pu√≤ aiutare a comprendere le richieste computazionali e le caratteristiche delle prestazioni dell‚Äôesecuzione di SLM come Llama su dispositivi edge come Raspberry Pi 5. Mostra che mentre la valutazione del prompt richiede pi√π tempo, la generazione effettiva delle risposte √® relativamente pi√π rapida. Questa analisi √® fondamentale per ottimizzare le prestazioni e diagnosticare potenziali colli di bottiglia nelle applicazioni in tempo reale.\nCaricando ed eseguendo il modello 3B, possiamo vedere la differenza nelle prestazioni per lo stesso prompt;\n\nIl tasso di valutazione √® inferiore, 5,3 token/s rispetto ai 9 token/s del modello pi√π piccolo.\nQuando si chiede\n&gt;&gt;&gt; What is the distance between Paris and Santiago, Chile?\nIl modello 1B ha risposto 9,841 kilometers (6,093 miles), il che √® impreciso, e il modello 3B ha risposto 7,300 miles (11,700 km), il che √® vicino alla distanza corretta (11,642 km).\nChiediamo le coordinate di Parigi:\n&gt;&gt;&gt; what is the latitude and longitude of Paris?\nThe latitude and longitude of Paris are 48.8567¬∞ N (48¬∞55' \n42\" N) and 2.3510¬∞ E (2¬∞22' 8\" E), respectively.\n\nSia i modelli 1B che 3B hanno dato risposte corrette.\n\n\nGoogle Gemma 2 2B\nInstalliamo Gemma 2, un modello efficiente e ad alte prestazioni disponibile in tre dimensioni: 2B, 9B e 27B. Installeremo Gemma 2 2B, un modello leggero addestrato con 2 trilioni di token che produce risultati sproporzionati imparando da modelli pi√π grandi tramite distillazione. Il modello ha 2,6 miliardi di parametri e una quantizzazione Q4_0, che termina con una dimensione di 1,6 GB. La sua finestra di contesto √® di 8.192 token.\n\nInstallare ed eseguire il Model\nollama run gemma2:2b --verbose\nEseguendo il modello col comando precedente, dovremmo avere il prompt di Ollama disponibile per inserire una domanda e iniziare a chattare con il modello LLM; ad esempio,\n&gt;&gt;&gt; What is the capital of France?\nQuasi immediatamente, otteniamo la risposta corretta:\nThe capital of France is **Paris**. üóº\nE le statistiche.\n\nPossiamo vedere che Gemma 2:2B ha pi√π o meno le stesse prestazioni di Lama 3.2:3B, ma con meno parametri.\nAltri esempi:\n&gt;&gt;&gt; What is the distance between Paris and Santiago, Chile?\n\nThe distance between Paris, France and Santiago, Chile is \napproximately **7,000 miles (11,267 kilometers)**. \n\nKeep in mind that this is a straight-line distance, and actual \ntravel distance can vary depending on the chosen routes and any \nstops along the way. ‚úàÔ∏è`\nInoltre, una buona risposta ma meno precisa di Llama3.2:3B.\n&gt;&gt;&gt; what is the latitude and longitude of Paris?\n\nYou got it! Here are the latitudes and longitudes of Paris, \nFrance:\n\n* **Latitude:** 48.8566¬∞ N (north)\n* **Longitude:** 2.3522¬∞ E (east) \n\nLet me know if you'd like to explore more about Paris or its \nlocation! üóºüá´üá∑\nUna risposta buona e precisa (un po‚Äô pi√π prolissa delle risposte del lama).\n\n\nMicrosoft Phi3.5 3.8B\nPrendiamo un modello pi√π grande (ma comunque ‚Äútiny‚Äù), il PHI3.5, un modello aperto all‚Äôavanguardia leggero da 3,8B di Microsoft. Il modello appartiene alla famiglia di modelli Phi-3 e supporta la lunghezza del contesto token 128K e le lingue: arabo, cinese, ceco, danese, olandese, inglese, finlandese, francese, tedesco, ebraico, ungherese, italiano, giapponese, coreano, norvegese, polacco, portoghese, russo, spagnolo, svedese, tailandese, turco e ucraino.\nLa dimensione del modello, in termini di byte, dipender√† dal formato di quantizzazione specifico utilizzato. La dimensione pu√≤ variare dalla quantizzazione a 2 bit (q2_k) di 1,4 GB (prestazioni pi√π elevate/qualit√† inferiore) alla quantizzazione a 16 bit (fp-16) di 7,6 GB (prestazioni pi√π basse/qualit√† pi√π elevata).\nEseguiamo la quantizzazione a 4 bit (Q4_0), che richieder√† 2,2 GB di RAM, con un compromesso intermedio per quanto riguarda la qualit√† dell‚Äôoutput e le prestazioni.\nollama run phi3.5:3.8b --verbose\n\nSi pu√≤ usare run o pull per scaricare il modello. Ci√≤ che accade √® che Ollama tiene nota dei modelli estratti e, una volta che PHI3 non esiste, prima di eseguirlo, Ollama lo estrae.\n\nImmettiamo lo stesso prompt usato prima:\n&gt;&gt;&gt; What is the capital of France?\n\n\nThe capital of France is Paris. It' extradites significant \nhistorical, cultural, and political importance to the country as \nwell as being a major European city known for its art, fashion, \ngastronomy, and culture. Its influence extends beyond national \nborders, with millions of tourists visiting each year from around \nthe globe. The Seine River flows through Paris before it reaches \nthe broader English Channel at Le Havre. Moreover, France is one \nof Europe's leading economies with its capital playing a key role \n\n...\nLa risposta √® stata molto ‚Äúverbosa‚Äù, specifichiamo un prompt migliore:\n\nIn questo caso, la risposta √® stata comunque pi√π lunga di quanto ci aspettassimo, con una velocit√† di valutazione di 2,25 token/s, pi√π del doppio di quella di Gemma e Llama.\n\nScegliere il prompt pi√π appropriato √® una delle competenze pi√π importanti da utilizzare con gli LLM, indipendentemente dalle dimensioni.\n\nQuando abbiamo posto le stesse domande su distanza e latitudine/longitudine, non abbiamo ottenuto una buona risposta per una distanza di 13,507 kilometers (8,429 miles), ma andava bene per le coordinate. Di nuovo, avrebbe potuto essere meno verbosa (pi√π di 200 token per ogni risposta).\nPossiamo usare qualsiasi modello come assistente poich√© la loro velocit√† √® relativamente decente, ma al 24 settembre (2023), Llama2:3B √® una scelta migliore. Si dovrebbero provare altri modelli, a seconda delle esigenze. ü§ó Open LLM Leaderboard pu√≤ dare un‚Äôidea sui migliori modelli in termini di dimensioni, benchmark, licenza, ecc.\n\nIl miglior modello da usare √® quello adatto alle specifiche necessit√†. Inoltre, si tenga presente che questo campo si evolve con nuovi modelli ogni giorno,\n\n\n\nModelli Multimodali\nI modelli multimodali sono sistemi di intelligenza artificiale (IA) in grado di elaborare e comprendere informazioni provenienti da pi√π fonti, come immagini, testo, audio e video. Nel nostro contesto, gli LLM multimodali possono elaborare vari input, tra cui testo, immagini e audio, come prompt e convertire tali prompt in vari output, non solo il tipo di sorgente.\nQui lavoreremo con LLaVA-Phi-3, un modello LLaVA ottimizzato da Phi 3 Mini 4k. Ha solidi benchmark di prestazioni che sono alla pari con il modello originale LLaVA (Large Language and Vision Assistant).\nLLaVA-Phi-3 √® un modello multimodale di grandi dimensioni addestrato end-to-end progettato per comprendere e generare contenuti in base a input visivi (immagini) e istruzioni testuali. Combina le capacit√† di un codificatore visivo e di un modello linguistico per elaborare e rispondere a input multimodali.\nInstalliamo il modello:\nollama run llava-phi3:3.8b --verbose\nCominciamo con un input di testo:\n&gt;&gt;&gt; You are a helpful AI assistant. What is the capital of France?\n\nAs an AI language model, I can tell you that the capital of France \nis Paris. It's not only the largest city in the country but also \nserves as its political and administrative center. Paris is known \nfor its iconic landmarks such as the Eiffel Tower, Notre-Dame \nCathedral, and the Louvre Museum. The city has a rich history, \nbeautiful architecture, and is widely considered to be one of the \nmost romantic cities in the world.\nLa risposta ha richiesto circa 30 secondi, con una velocit√† di valutazione di 3,93 token/s! Niente male!\nMa vediamo di immettere un‚Äôimmagine come input. Per questo, creiamo una directory per lavorare:\ncd Documents/\nmkdir OLLAMA\ncd OLLAMA\nScarichiamo un‚Äôimmagine 640x320 da Internet, per esempio (Wikipedia: Paris, France):\n\nUtilizzando FileZilla, ad esempio, carichiamo l‚Äôimmagine nella cartella OLLAMA sul Raspi-5 e chiamiamola image_test_1.jpg. Dovremmo avere l‚Äôintero path dell‚Äôimmagine (possiamo usare pwd per ottenerlo).\n/home/mjrovai/Documents/OLLAMA/image_test_1.jpg\nSe si usa un desktop, si pu√≤ copiare il path dell‚Äôimmagine cliccando sull‚Äôimmagine con il tasto destro del mouse.\n\nImmettiamo questo prompt:\n&gt;&gt;&gt; Describe the image /home/mjrovai/Documents/OLLAMA/image_test_1.jpg\nIl risultato √® stato ottimo, ma la latenza complessiva √® stata significativa: quasi 4 minuti per eseguire l‚Äôinferenza.\n\n\n\nIspezione delle risorse locali\nUtilizzando htop, possiamo monitorare le risorse in esecuzione sul nostro dispositivo.\nhtop\nDurante il periodo in cui il modello √® in esecuzione, possiamo ispezionare le risorse:\n\nTutte e quattro le CPU funzionano a quasi il 100% della loro capacit√† e la memoria utilizzata con il modello caricato √® di 3.24GB. Uscendo da Ollama, la memoria scende a circa 377 MB (senza desktop).\n√à anche essenziale monitorare la temperatura. Quando si esegue Raspberry con un desktop, √® possibile visualizzare la temperatura sulla barra delle applicazioni:\n\nSe si √® ‚Äúheadless‚Äù, la temperatura pu√≤ essere monitorata col comando:\nvcgencmd measure_temp\nSe non si fa nulla, la temperatura √® di circa 50¬∞C per le CPU in esecuzione all‚Äô1%. Durante l‚Äôinferenza, con le CPU al 100%, la temperatura pu√≤ salire fino a quasi 70¬∞C. Questo √® OK e significa che il dissipatore attivo funziona, mantenendo la temperatura al di sotto di 80¬∞C / 85¬∞C (il suo limite).",
    "crumbs": [
      "Raspberry Pi",
      "Small Language Models (SLM)"
    ]
  },
  {
    "objectID": "contents/labs/raspi/llm/llm.it.html#libreria-python-ollama",
    "href": "contents/labs/raspi/llm/llm.it.html#libreria-python-ollama",
    "title": "Small Language Models (SLM)",
    "section": "Libreria Python Ollama",
    "text": "Libreria Python Ollama\nFinora, abbiamo esplorato la capacit√† di chat degli SLM utilizzando la riga di comando su un terminale. Tuttavia, vogliamo integrare quei modelli nei nostri progetti, quindi Python sembra essere la strada giusta. La buona notizia √® che Ollama ha una libreria del genere.\nLa libreria Python Ollama semplifica l‚Äôinterazione con modelli LLM avanzati, consentendo risposte e capacit√† pi√π sofisticate, oltre a fornire il modo pi√π semplice per integrare progetti Python 3.8+ con Ollama.\nPer una migliore comprensione di come creare app usando Ollama con Python, possiamo seguire i video di Matt Williams, come quello qui sotto:\n\nInstallazione:\nNel terminale, eseguire il comando:\npip install ollama\nAvremo bisogno di un editor di testo o di un IDE per creare uno script Python. Se si esegue Raspberry OS su un desktop, diverse opzioni, come Thonny e Geany, sono gi√† state installate di default (accessibili tramite [Menu][Programming]). Si possono scaricare altri IDE, come Visual Studio Code, da [Menu][Recommended Software]. Quando si apre la finestra, si va su [Programming], e si seleziona l‚Äôopzione che si preferisce e si preme [Apply].\n\nSe si preferisce usare Jupyter Notebook per lo sviluppo:\npip install jupyter\njupyter notebook --generate-config\nPer eseguire Jupyter Notebook, si lancia il comando (cambiare l‚Äôindirizzo IP per il proprio):\njupyter notebook --ip=192.168.4.209 --no-browser\nSul terminale, si pu√≤ vedere l‚Äôindirizzo URL locale per aprire il notebook:\n\nPossiamo accedervi da un altro computer inserendo l‚Äôindirizzo IP del Raspberry Pi e il token fornito in un browser web (dovremmo copiarlo dal terminale).\nNella nostra directory di lavoro nel Raspi, creeremo un nuovo notebook Python 3.\nEntriamo con uno script molto semplice per verificare i modelli installati:\nimport ollama\nollama.list()\nTutti i modelli verranno stampati come un dizionario, ad esempio:\n{'name': 'gemma2:2b',\n   'model': 'gemma2:2b',\n   'modified_at': '2024-09-24T19:30:40.053898094+01:00',\n   'size': 1629518495,\n   'digest': '8ccf136fdd5298f3ffe2d69862750ea7fb56555fa4d5b18c04e3fa4d82ee09d7',\n   'details': {'parent_model': '',\n    'format': 'gguf',\n    'family': 'gemma2',\n    'families': ['gemma2'],\n    'parameter_size': '2.6B',\n    'quantization_level': 'Q4_0'}}]}\nRipetiamo una delle domande che abbiamo fatto prima, ma ora usando ollama.generate() dalla libreria Python Ollama. Questa API generer√† una risposta per il prompt specificato con il modello fornito. Questo √® un endpoint di streaming, quindi ci saranno una serie di risposte. L‚Äôoggetto di risposta finale includer√† statistiche e dati aggiuntivi dalla richiesta.\nMODEL = 'gemma2:2b'\nPROMPT = 'What is the capital of France?'\n\nres = ollama.generate(model=MODEL, prompt=PROMPT)\nprint (res)\nNel caso in cui si stia eseguendo il codice come script Python, lo si deve salvare, ad esempio, test_ollama.py. Si pu√≤ usare l‚ÄôIDE per eseguirlo o farlo direttamente sul terminale. Da ricordare, inoltre, che si deve sempre chiamare il modello e definirlo quando si esegue uno script stand-alone.\npython test_ollama.py\nDi conseguenza, avremo la risposta del modello in un formato JSON:\n{'model': 'gemma2:2b', 'created_at': '2024-09-25T14:43:31.869633807Z', \n'response': 'The capital of France is **Paris**. üá´üá∑ \\n', 'done': True, \n'done_reason': 'stop', 'context': [106, 1645, 108, 1841, 603, 573, 6037, 576,\n6081, 235336, 107, 108, 106, 2516, 108, 651, 6037, 576, 6081, 603, 5231, 29437, \n168428, 235248, 244304, 241035, 235248, 108], 'total_duration': 24259469458, \n'load_duration': 19830013859, 'prompt_eval_count': 16, 'prompt_eval_duration': \n1908757000, 'eval_count': 14, 'eval_duration': 2475410000}\nCome possiamo vedere, vengono generate diverse informazioni, come:\n\nresponse: il testo di output principale generato dal modello in risposta al nostro prompt.\n\nThe capital of France is **Paris**. üá´üá∑\n\ncontext: gli ID token che rappresentano l‚Äôinput e il contesto utilizzati dal modello. I token sono rappresentazioni numeriche del testo utilizzate per l‚Äôelaborazione dal modello linguistico.\n\n[106, 1645, 108, 1841, 603, 573, 6037, 576, 6081, 235336, 107, 108, 106, 2516, 108, 651, 6037, 576, 6081, 603, 5231, 29437, 168428, 235248, 244304, 241035, 235248, 108]\n\n\nLe Metriche delle Prestazioni:\n\ntotal_duration: Tempo totale impiegato per l‚Äôoperazione in nanosecondi. In questo caso, circa 24,26 secondi.\nload_duration: Tempo impiegato per caricare il modello o i componenti in nanosecondi. Circa 19,83 secondi.\nprompt_eval_duration: Tempo impiegato per valutare il prompt in nanosecondi. Circa 16 nanosecondi.\neval_count: Numero di token valutati durante la generazione. Qui, 14 token.\neval_duration: Tempo impiegato dal modello per generare la risposta in nanosecondi. Circa 2,5 secondi.\n\nMa ci√≤ che vogliamo √® la semplice ‚Äòresponse‚Äô e, forse per l‚Äôanalisi, la durata totale dell‚Äôinferenza, quindi modifichiamo il codice per estrarlo dal dizionario:\nprint(f\"\\n{res['response']}\")\nprint(f\"\\n [INFO] Total Duration: {(res['total_duration']/1e9):.2f} seconds\")\nOra, abbiamo:\nThe capital of France is **Paris**. üá´üá∑ \n\n [INFO] Total Duration: 24.26 seconds\nUtilizzo di Ollama.chat()\nUn altro modo per ottenere la nostra risposta √® utilizzare ollama.chat(), che genera il messaggio successivo in una chat con un modello fornito. Questo √® un endpoint di streaming, quindi si verificheranno una serie di risposte. Lo streaming pu√≤ essere disabilitato utilizzando \"stream\": false. L‚Äôoggetto di risposta finale includer√† anche statistiche e dati aggiuntivi dalla richiesta.\nPROMPT_1 = 'What is the capital of France?'\n\nresponse = ollama.chat(model=MODEL, messages=[\n{'role': 'user','content': PROMPT_1,},])\nresp_1 = response['message']['content']\nprint(f\"\\n{resp_1}\")\nprint(f\"\\n [INFO] Total Duration: {(res['total_duration']/1e9):.2f} seconds\")\nLa risposta √® la stessa di prima.\nUna considerazione importante √® che usando ollama.generate(), la risposta √® ‚Äúchiara‚Äù dalla ‚Äúmemoria‚Äù del modello dopo la fine dell‚Äôinferenza (usata solo una volta), ma se vogliamo mantenere una conversazione, dobbiamo usare ollama.chat(). Vediamolo in azione:\nPROMPT_1 = 'What is the capital of France?'\nresponse = ollama.chat(model=MODEL, messages=[\n{'role': 'user','content': PROMPT_1,},])\nresp_1 = response['message']['content']\nprint(f\"\\n{resp_1}\")\nprint(f\"\\n [INFO] Total Duration: {(response['total_duration']/1e9):.2f} seconds\")\n\nPROMPT_2 = 'and of Italy?'\nresponse = ollama.chat(model=MODEL, messages=[\n{'role': 'user','content': PROMPT_1,},\n{'role': 'assistant','content': resp_1,},\n{'role': 'user','content': PROMPT_2,},])\nresp_2 = response['message']['content']\nprint(f\"\\n{resp_2}\")\nprint(f\"\\n [INFO] Total Duration: {(response['total_duration']/1e9):.2f} seconds\")\nNel codice sopra, stiamo eseguendo due query e il secondo prompt considera il risultato del primo.\nEcco come ha risposto il modello:\nThe capital of France is **Paris**. üá´üá∑ \n\n [INFO] Total Duration: 2.82 seconds\n\nThe capital of Italy is **Rome**. üáÆüáπ \n\n [INFO] Total Duration: 4.46 seconds\nOttenere una descrizione dell‚Äôimmagine:\nAllo stesso modo in cui abbiamo utilizzato il modello LlaVa-PHI-3 con la riga di comando per analizzare un‚Äôimmagine, lo stesso pu√≤ essere fatto qui con Python. Usiamo la stessa immagine di Parigi, ma ora con ollama.generate():\nMODEL = 'llava-phi3:3.8b'\nPROMPT = \"Describe this picture\"\n\nwith open('image_test_1.jpg', 'rb') as image_file:\n    img = image_file.read()\n\nresponse = ollama.generate(\n    model=MODEL,\n    prompt=PROMPT,\n    images= [img]\n)\nprint(f\"\\n{response['response']}\")\nprint(f\"\\n [INFO] Total Duration: {(res['total_duration']/1e9):.2f} seconds\")\nEcco il risultato:\nThis image captures the iconic cityscape of Paris, France. The vantage point \nis high, providing a panoramic view of the Seine River that meanders through \nthe heart of the city. Several bridges arch gracefully over the river, \nconnecting different parts of the city. The Eiffel Tower, an iron lattice \nstructure with a pointed top and two antennas on its summit, stands tall in the \nbackground, piercing the sky. It is painted in a light gray color, contrasting \nagainst the blue sky speckled with white clouds.\n\nThe buildings that line the river are predominantly white or beige, their uniform\ncolor palette broken occasionally by red roofs peeking through. The Seine River \nitself appears calm and wide, reflecting the city's architectural beauty in its \nsurface. On either side of the river, trees add a touch of green to the urban \nlandscape.\n\nThe image is taken from an elevated perspective, looking down on the city. This \nviewpoint allows for a comprehensive view of Paris's beautiful architecture and \nlayout. The relative positions of the buildings, bridges, and other structures \ncreate a harmonious composition that showcases the city's charm.\n\nIn summary, this image presents a serene day in Paris, with its architectural \nmarvels - from the Eiffel Tower to the river-side buildings - all bathed in soft \ncolors under a clear sky.\n\n [INFO] Total Duration: 256.45 seconds\nIl modello ha impiegato circa 4 minuti (256,45 s) per restituire una descrizione dettagliata dell‚Äôimmagine.\n\nNel notebook 10-Ollama_Python_Library, √® possibile trovare gli esperimenti con la libreria Python Ollama.\n\n\nChiamata di Funzione\nFinora, possiamo osservare che utilizzando la risposta del modello in una variabile, possiamo incorporarla efficacemente in progetti reali. Tuttavia, sorge un problema importante quando il modello fornisce risposte diverse allo stesso input. Ad esempio, supponiamo di aver bisogno solo del nome della capitale di un paese e delle sue coordinate come risposta del modello negli esempi precedenti, senza ulteriori informazioni, anche quando si utilizzano modelli dettagliati come Microsoft Phi. Per garantire risposte coerenti, possiamo utilizzare la ‚Äúchiamata di funzione Ollama‚Äù, che √® completamente compatibile con l‚ÄôAPI OpenAI.\n\nMa cos‚Äô√® esattamente la ‚Äúchiamata di funzione‚Äù?\nNell‚Äôintelligenza artificiale moderna, la chiamata di funzione con Large Language Models (LLM) consente a questi modelli di eseguire azioni che vanno oltre la generazione di testo. Integrandosi con funzioni o API esterne, gli LLM possono accedere a dati in tempo reale, automatizzare attivit√† e interagire con vari sistemi.\nAd esempio, invece di rispondere semplicemente a una query sul meteo, un LLM pu√≤ chiamare un‚ÄôAPI meteo per recuperare le condizioni attuali e fornire informazioni accurate e aggiornate. Questa capacit√† migliora la pertinenza e l‚Äôaccuratezza delle risposte del modello e lo rende uno strumento potente per guidare flussi di lavoro e automatizzare i processi, trasformandolo in un partecipante attivo nelle applicazioni del mondo reale.\nPer maggiori dettagli sulla ‚ÄúFunction Calling‚Äù chiamata di funzione, c‚Äô√® questo video realizzato da Marvin Prison:\n\n\n\nCreiamo un progetto.\nVogliamo creare un‚Äôapp in cui l‚Äôutente inserisce il nome di un Paese e ottiene, come output, la distanza in km dalla capitale di tale Paese e la posizione dell‚Äôapp (per semplicit√†, useremo Santiago del Cile come posizione dell‚Äôapp).\n\nUna volta che l‚Äôutente inserisce il nome di un paese, il modello restituir√† il nome della sua capitale (come stringa) e la latitudine e la longitudine di tale citt√† (in float). Utilizzando queste coordinate, possiamo usare una semplice libreria Python (haversine) per calcolare la distanza tra quei 2 punti.\nL‚Äôidea di questo progetto √® dimostrare una combinazione di interazione del modello linguistico, gestione dei dati strutturati con Pydantic e calcoli geospaziali utilizzando la formula di Haversine (informatica tradizionale).\nPer prima cosa, installiamo alcune librerie. Oltre a Haversine, la principale √® la libreria Python OpenAI, che fornisce un comodo accesso all‚ÄôAPI REST OpenAI da qualsiasi applicazione Python 3.7+. L‚Äôaltra √® Pydantic (e instructor), una libreria robusta di convalida dei dati e gestione delle impostazioni progettata da Python per migliorare la robustezza e l‚Äôaffidabilit√† della nostra base di codice. In breve, Pydantic ci aiuter√† a garantire che la risposta del nostro modello sia sempre coerente.\npip install haversine\npip install openai \npip install pydantic \npip install instructor\nOra, dovremmo creare uno script Python progettato per interagire con il nostro modello (LLM) per determinare le coordinate della capitale di un paese e calcolare la distanza da Santiago del Cile a quella capitale.\nDiamo un‚Äôocchiata al codice:\n\n\n\n1. Importazione delle Librerie\nimport sys\nfrom haversine import haversine\nfrom openai import OpenAI\nfrom pydantic import BaseModel, Field\nimport instructor\n\nsys: Fornisce accesso a parametri e funzioni specifici del sistema. Viene utilizzato per ottenere argomenti dalla riga di comando.\nhaversine: Una funzione della libreria haversine che calcola la distanza tra due punti geografici utilizzando la formula Haversine.\nopenAI: Un modulo per interagire con l‚ÄôAPI OpenAI (anche se viene utilizzato insieme a una configurazione locale, Ollama). Qui tutto √® offline.\npydantic: Fornisce la convalida dei dati e la gestione delle impostazioni utilizzando annotazioni di tipo Python. Viene utilizzato per definire la struttura dei dati di risposta previsti.\ninstructor: Un modulo viene utilizzato per applicare patch al client OpenAI per funzionare in una modalit√† specifica (probabilmente correlata alla gestione dei dati strutturati).\n\n\n\n2. Definizione di Input e Modello\ncountry = sys.argv[1]       # Get the country from command-line arguments\nMODEL = 'phi3.5:3.8b'     # The name of the model to be used\nmylat = -33.33              # Latitude of Santiago de Chile\nmylon = -70.51              # Longitude of Santiago de Chile\n\ncountry: In uno script Python, √® possibile ottenere il nome del paese dagli argomenti della riga di comando. In un notebook Jupyter, possiamo immettere il suo nome, ad esempio,\n\ncountry = \"France\"\n\nMODEL: Specifica il modello utilizzato, che √®, in questo esempio, phi3.5.\nmylat e mylon: Coordinate di Santiago del Cile, utilizzate come punto di partenza per il calcolo della distanza.\n\n\n\n3. Definizione della Struttura dei Dati di Risposta\nclass CityCoord(BaseModel):\n    city: str = Field(..., description=\"Name of the city\")\n    lat: float = Field(..., description=\"Decimal Latitude of the city\")\n    lon: float = Field(..., description=\"Decimal Longitude of the city\")\n\nCityCoord: Un modello Pydantic che definisce la struttura prevista della risposta dal LLM. Si aspetta tre campi: city (nome della citt√†), lat (latitudine) e lon (longitudine).\n\n\n\n4. Impostazione del Client OpenAI\nclient = instructor.patch(\n    OpenAI(\n        base_url=\"http://localhost:11434/v1\",  # Local API base URL (Ollama)\n        api_key=\"ollama\",                      # API key (not used)\n    ),\n    mode=instructor.Mode.JSON,                 # Mode for structured JSON output\n)\n\nOpenAI: Questa configurazione inizializza un client OpenAI con un URL di base locale e una chiave API (ollama). Utilizza un server locale.\ninstructor.patch: Applica patch al client OpenAI per funzionare in modalit√† JSON, abilitando un output strutturato che corrisponde al modello Pydantic.\n\n\n\n5. Generazione della Risposta\nresp = client.chat.completions.create(\n    model=MODEL,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": f\"return the decimal latitude and decimal longitude \\\n            of the capital of the {country}.\"\n        }\n\n    ],\n    response_model=CityCoord,\n    max_retries=10\n)\n\nclient.chat.completions.create: Chiama l‚ÄôLLM per generare una risposta.\nmodel: Specifica il modello da utilizzare (llava-phi3).\nmessages: Contiene il prompt per l‚ÄôLLM, che chiede latitudine e longitudine della capitale del paese specificato.\nresponse_model: Indica che la risposta deve essere conforme al modello CityCoord.\nmax_retries: Numero massimo di tentativi di ripetizione se la richiesta fallisce.\n\n\n\n6. Calcolo della Distanza\ndistance = haversine((mylat, mylon), (resp.lat, resp.lon), unit='km')\nprint(f\"Santiago de Chile is about {int(round(distance, -1)):,} \\\n        kilometers away from {resp.city}.\")\n\nhaversine: Calcola la distanza tra Santiago del Cile e la capitale restituita dall‚ÄôLLM utilizzando le rispettive coordinate.\n(mylat, mylon): Coordinate di Santiago del Cile.\nresp.city: Nome della capitale del paese\n(resp.lat, resp.lon): Le coordinate della capitale sono fornite dalla risposta dell‚ÄôLLM.\nunit=‚Äòkm‚Äô: Specifica che la distanza deve essere calcolata in chilometri.\nprint: Restituisce la distanza, arrotondata ai 10 chilometri pi√π vicini, con migliaia di separatori per una migliore leggibilit√†.\n\nEsecuzione del codice\nSe inseriamo paesi diversi, ad esempio Francia, Colombia e Stati Uniti, possiamo notare che riceviamo sempre le stesse informazioni strutturate:\nSantiago de Chile is about 8,060 kilometers away from Washington, D.C..\nSantiago de Chile is about 4,250 kilometers away from Bogot√°.\nSantiago de Chile is about 11,630 kilometers away from Paris.\nEseguendo il codice come script, il risultato verr√† stampato sul terminale:\n\nE i calcoli sono piuttosto buoni!\n\n\nNel notebook 20-Ollama_Function_Calling, √® possibile trovare esperimenti con tutti i modelli installati.\n\n\n\nAggiunta di immagini\nOra √® il momento raccogliere il tutto! Modifichiamo lo script in modo che invece di immettere il nome del paese (come testo), l‚Äôutente immetta un‚Äôimmagine e l‚Äôapplicazione (basata su SLM) restituisca la citt√† nell‚Äôimmagine e la sua posizione geografica. Con quei dati, possiamo calcolare la distanza come prima.\n\nPer semplicit√†, implementeremo questo nuovo codice in due passaggi. Innanzitutto, LLM analizzer√† l‚Äôimmagine e creer√† una descrizione (testo). Questo testo verr√† passato a un‚Äôaltra istanza, dove il modello estrarr√† le informazioni necessarie per il passaggio.\nInizieremo ad importare le librerie\nimport sys\nimport time\nfrom haversine import haversine\nimport ollama\nfrom openai import OpenAI\nfrom pydantic import BaseModel, Field\nimport instructor\nPossiamo vedere l‚Äôimmagine se si esegue il codice su Jupyter Notebook. Per questo dobbiamo anche importare:\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\nTali librerie non sono necessarie se eseguiamo il codice come uno script.\n\nOra, definiamo il modello e le coordinate locali:\nMODEL = 'llava-phi3:3.8b'\nmylat = -33.33\nmylon = -70.51\nPossiamo effettuare il download di una nuova immagine, ad esempio Machu Picchu da Wikipedia. Sul Notebook possiamo vederla:\n# Load the image\nimg_path = \"image_test_3.jpg\"\nimg = Image.open(img_path)\n\n# Display the image\nplt.figure(figsize=(8, 8))\nplt.imshow(img)\nplt.axis('off')\n#plt.title(\"Image\")\nplt.show()\n\nOra, definiamo una funzione che ricever√† l‚Äôimmagine e che return the decimal latitude and decimal longitude of the city in the image, its name, and what country it is located [restituir√† la latitudine decimale e la longitudine decimale della citt√† nell‚Äôimmagine, il suo nome e il paese in cui si trova].\ndef image_description(img_path):\n    with open(img_path, 'rb') as file:\n        response = ollama.chat(\n            model=MODEL,\n            messages=[\n              {\n                'role': 'user',\n                'content': '''return the decimal latitude and decimal longitude \n                              of the city in the image, its name, and \n                              what country it is located''',\n                'images': [file.read()],\n              },\n            ],\n            options = {\n              'temperature': 0,\n              }\n\n      )\n\n    #print(response['message']['content'])\n    return response['message']['content']\n\nPossiamo stampare l‚Äôintera risposta per debug.\n\nLa descrizione dell‚Äôimmagine generata per la funzione verr√† passata di nuovo come prompt per il modello.\nstart_time = time.perf_counter()  # Start timing\n\nclass CityCoord(BaseModel):\n    city: str = Field(..., description=\"Name of the city in the image\")\n    country: str = Field(..., description=\"\"\"Name of the country where\"\n                                             the city in the image is located\n                                             \"\"\")\n    lat: float = Field(..., description=\"\"\"Decimal Latitude of the city in\"\n                                            the image\"\"\")\n    lon: float = Field(..., description=\"\"\"Decimal Longitude of the city in\"\n                                           the image\"\"\")\n\n# enables `response_model` in create call\nclient = instructor.patch(\n    OpenAI(\n        base_url=\"http://localhost:11434/v1\",\n        api_key=\"ollama\"\n    ),\n    mode=instructor.Mode.JSON,\n)\n\nimage_description = image_description(img_path)\n# Send this description to the model\nresp = client.chat.completions.create(\n    model=MODEL,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": image_description,\n        }\n\n    ],\n    response_model=CityCoord,\n    max_retries=10,\n    temperature=0,\n)\nSe stampiamo la descrizione dell‚Äôimmagine, otterremo:\nThe image shows the ancient city of Machu Picchu, located in Peru. The city is\nperched on a steep hillside and consists of various structures made of stone. It \nis surrounded by lush greenery and towering mountains. The sky above is blue with\nscattered clouds. \n\nMachu Picchu's latitude is approximately 13.5086¬∞ S, and its longitude is around\n72.5494¬∞ W.\nE la seconda risposta del modello (resp) sar√†:\nCityCoord(city='Machu Picchu', country='Peru', lat=-13.5086, lon=-72.5494)\nOra possiamo effettuare un ‚ÄúPost-Processing‚Äù, calcolando la distanza e preparando la risposta finale:\ndistance = haversine((mylat, mylon), (resp.lat, resp.lon), unit='km')\n\nprint(f\"\\n The image shows {resp.city}, with lat:{round(resp.lat, 2)} and \\\n      long: {round(resp.lon, 2)}, located in {resp.country} and about \\\n            {int(round(distance, -1)):,} kilometers away from \\\n            Santiago, Chile.\\n\")\n\nend_time = time.perf_counter()  # End timing\nelapsed_time = end_time - start_time  # Calculate elapsed time\nprint(f\" [INFO] ==&gt; The code (running {MODEL}), took {elapsed_time:.1f} \\\n      seconds to execute.\\n\")\nE otterremo:\nThe image shows Machu Picchu, with lat:-13.16 and long: -72.54, located in Peru\n and about 2,250 kilometers away from Santiago, Chile.\n\n [INFO] ==&gt; The code (running llava-phi3:3.8b), took 491.3 seconds to execute.\nNel notebook 30-Function_Calling_with_images √® possibile trovare gli esperimenti con pi√π immagini.\nScarichiamo ora lo script calc_distance_image.py da GitHub ed eseguiamolo sul terminale con il comando:\npython calc_distance_image.py /home/mjrovai/Documents/OLLAMA/image_test_3.jpg\nImmettere la patch completa dell‚Äôimmagine di Machu Picchu come argomento. Otterremo lo stesso risultato precedente.\n\nPer wuanto riguarda Parigi?\n\nNaturalmente, ci sono molti modi per ottimizzare il codice utilizzato qui. Tuttavia, l‚Äôidea √® di esplorare il notevole potenziale della function calling con SLM nell‚Äôedge, consentendo a tali modelli di integrarsi con funzioni o API esterne. Andando oltre la generazione di testo, gli SLM possono accedere a dati in tempo reale, automatizzare attivit√† e interagire con vari sistemi.",
    "crumbs": [
      "Raspberry Pi",
      "Small Language Models (SLM)"
    ]
  },
  {
    "objectID": "contents/labs/raspi/llm/llm.it.html#slm-tecniche-di-ottimizzazione",
    "href": "contents/labs/raspi/llm/llm.it.html#slm-tecniche-di-ottimizzazione",
    "title": "Small Language Models (SLM)",
    "section": "SLM: Tecniche di Ottimizzazione",
    "text": "SLM: Tecniche di Ottimizzazione\nI Large Language Model (LLM) hanno rivoluzionato l‚Äôelaborazione del linguaggio naturale, ma la loro distribuzione e ottimizzazione presentano sfide uniche. Un problema significativo √® la tendenza degli LLM (e pi√π in particolare degli SLM) a generare informazioni che sembrano plausibili ma di fatto errate, un fenomeno noto come allucinazione. Ci√≤ si verifica quando i modelli producono contenuti che sembrano coerenti ma non sono basati sulla verit√† o sui fatti del mondo reale.\nAltre sfide includono le immense risorse computazionali richieste per l‚Äôaddestramento e l‚Äôesecuzione di questi modelli, la difficolt√† nel mantenere aggiornate le conoscenze all‚Äôinterno del modello e la necessit√† di adattamenti specifici per dominio. Problemi di privacy sorgono anche quando si gestiscono dati sensibili durante l‚Äôaddestramento o l‚Äôinferenza. Inoltre, garantire prestazioni coerenti in diverse attivit√† e mantenere un uso etico di questi potenti strumenti presentano sfide continue. Affrontare questi problemi √® fondamentale per l‚Äôimplementazione efficace e responsabile di LLM in applicazioni del mondo reale.\nLe tecniche fondamentali per migliorare le prestazioni e l‚Äôefficienza di LLM (e SLM) sono Fine-tuning, Prompt engineering e Retrieval-Augmented Generation (RAG).\n\nFine-tuning, sebbene richieda pi√π risorse, offre un modo per specializzare LLM per domini o attivit√† particolari. Questo processo comporta un ulteriore addestramento del modello su set di dati attentamente curati, consentendogli di adattare la sua vasta conoscenza generale ad applicazioni specifiche. Il Fine-tuning pu√≤ portare a miglioramenti sostanziali nelle prestazioni, specialmente in campi specializzati o per casi d‚Äôuso unici.\nPrompt engineering √® in prima linea nell‚Äôottimizzazione LLM. Creando attentamente prompt di input, possiamo guidare i modelli per produrre output pi√π accurati e pertinenti. Questa tecnica comporta la strutturazione di query che sfruttano le conoscenze e le capacit√† pre-addestrate del modello, spesso incorporando esempi o istruzioni specifiche per modellare la risposta desiderata.\nRetrieval-Augmented Generation (RAG) rappresenta un altro approccio potente per migliorare le prestazioni LLM. Questo metodo combina la vasta conoscenza incorporata nei modelli pre-addestrati con la capacit√† di accedere e incorporare informazioni esterne aggiornate. Recuperando dati pertinenti per integrare il processo decisionale del modello, RAG pu√≤ migliorare significativamente l‚Äôaccuratezza e ridurre la probabilit√† di generare informazioni obsolete o false.\n\nPer le applicazioni edge, √® pi√π utile concentrarsi su tecniche come RAG che possono migliorare le prestazioni del modello senza dover effettuare un ‚Äúfine-tuning‚Äù sul dispositivo. Esploriamolo.",
    "crumbs": [
      "Raspberry Pi",
      "Small Language Models (SLM)"
    ]
  },
  {
    "objectID": "contents/labs/raspi/llm/llm.it.html#implementazione-del-rag",
    "href": "contents/labs/raspi/llm/llm.it.html#implementazione-del-rag",
    "title": "Small Language Models (SLM)",
    "section": "Implementazione del RAG",
    "text": "Implementazione del RAG\nIn un‚Äôinterazione di base tra un utente e un modello linguistico, l‚Äôutente pone una domanda, che viene inviata come prompt al modello. Il modello genera una risposta basata esclusivamente sulla sua conoscenza pre-addestrata. In un processo RAG, c‚Äô√® un passaggio aggiuntivo tra la domanda dell‚Äôutente e la risposta del modello. La domanda dell‚Äôutente innesca un processo di recupero da una ‚Äúknowledge base‚Äù.\n\n\nUn semplice progetto RAG\nEcco i passaggi per implementare una Retrieval Augmented Generation (RAG) di base:\n\nDeterminare il tipo di documenti che si utilizzeranno: I tipi migliori sono documenti da cui possiamo ottenere testo pulito e non oscurato. I PDF possono essere problematici perch√© sono progettati per la stampa, non per estrarre testo sensato. Per lavorare con i PDF, dovremmo ottenere il documento di origine o utilizzare strumenti per gestirli.\nSuddividere il testo in blocchi: Non possiamo archiviare il testo come un unico lungo flusso a causa delle limitazioni delle dimensioni del contesto e della potenziale confusione. La suddivisione in blocchi comporta la suddivisione del testo in parti pi√π piccole. Il testo in blocchi ha molti modi, come conteggio dei caratteri, token, parole, paragrafi o sezioni. √à anche possibile sovrapporre i blocchi.\nCrea embedding: Gli ‚Äúembedding‚Äù [incorporamenti] sono rappresentazioni numeriche del testo che catturano il significato semantico. Creiamo incorporamenti passando ogni blocco di testo attraverso un particolare modello di embedding. Il modello genera un vettore, la cui lunghezza dipende dal modello di embedding utilizzato. Dovremmo estrarre uno (o pi√π) modelli di embedding da Ollama, per eseguire questa attivit√†. Ecco alcuni esempi di modelli di embedding disponibili su Ollama.\n\n\n\nModello\nDimensione del parametro\nDimensione dell‚ÄôEmbedding\n\n\n\n\nmxbai-embed-large\n334M\n1024\n\n\nnomic-embed-text\n137M\n768\n\n\nall-minilm\n23M\n384\n\n\n\n\nIn genere, dimensioni dell‚Äôembedding maggiori catturano informazioni pi√π sfumate sull‚Äôinput. Tuttavia, richiedono anche pi√π risorse per l‚Äôelaborazione e un numero maggiore di parametri dovrebbe aumentare la latenza (ma anche la qualit√† della risposta).\n\nMemorizzare i blocchi e gli embedding in un database vettoriale: Avremo bisogno di un modo per trovare in modo efficiente i blocchi di testo pi√π rilevanti per un dato prompt, ed √® qui che entra in gioco un database vettoriale. Useremo Chromadb, un database vettoriale open source nativo IA, che semplifica la creazione di RAG creando conoscenze, fatti e competenze collegabili per LLM. Vengono memorizzati sia embedding che il testo sorgente per ogni blocco.\nCreare il prompt: Quando abbiamo una domanda, creiamo un embedding e interroghiamo il database vettoriale per i blocchi pi√π simili. Poi, selezioniamo i primi risultati e includiamo il loro testo nel prompt.\n\nL‚Äôobiettivo di RAG √® fornire al modello le informazioni pi√π rilevanti dai nostri documenti, consentendogli di generare risposte pi√π accurate e informative. Quindi, implementiamo un semplice esempio di un SLM che incorpora un set particolare di fatti sulle api (‚ÄúBee Facts‚Äù).\nAll‚Äôinterno dell‚Äôambiente ollama, si inserisce il comando nel terminale per l‚Äôinstallazione di Chromadb:\npip install ollama chromadb\nTiriamo fuori un modello di embedding intermedio, nomic-embed-text\nollama pull nomic-embed-text\nE creiamo una directory di lavoro:\ncd Documents/OLLAMA/\nmkdir RAG-simple-bee\ncd RAG-simple-bee/\nCreiamo un nuovo notebook Jupyter, 40-RAG-simple-bee per qualche esplorazione:\nImportare le librerie necessarie:\nimport ollama\nimport chromadb\nimport time\nE definiamo i modelli aor:\nEMB_MODEL = \"nomic-embed-text\"\nMODEL = 'llama3.2:3B'\nInizialmente, dovrebbe essere creata una ‚Äúknowledge base‚Äù sui fatti sulle api. Ci√≤ comporta la raccolta di documenti rilevanti e la loro conversione in embedding vettoriali. Questi embedding vengono poi archiviati in un database vettoriale, consentendo in seguito ricerche di similarit√† efficienti. Si immette il ‚Äúdocument‚Äù, una base di ‚Äúfatti sulle api‚Äù come un elenco:\ndocuments = [\n    \"Bee-keeping, also known as apiculture, involves the maintenance of bee \\\n    colonies, typically in hives, by humans.\",\n    \"The most commonly kept species of bees is the European honey bee (Apis \\\n    mellifera).\",\n    \n    ...\n    \n    \"There are another 20,000 different bee species in the world.\",  \n    \"Brazil alone has more than 300 different bee species, and the \\\n    vast majority, unlike western honey bees, don‚Äôt sting.\", \n    \"Reports written in 1577 by Hans Staden, mention three native bees \\\n    used by indigenous people in Brazil.\",\n    \"The indigenous people in Brazil used bees for medicine and food purposes\",\n    \"From Hans Staden report: probable species: manda√ßaia (Melipona \\\n    quadrifasciata), mandaguari (Scaptotrigona postica) and jata√≠-amarela \\\n    (Tetragonisca angustula).\"\n]\n\nNon abbiamo bisogno di ‚Äúsuddividere‚Äù il documento qui perch√© useremo ogni elemento dell‚Äôelenco e un blocco.\n\nOra creeremo il nostro database di embedding vettoriale bee_facts e memorizzeremo il documento in esso:\nclient = chromadb.Client()\ncollection = client.create_collection(name=\"bee_facts\")\n\n# store each document in a vector embedding database\nfor i, d in enumerate(documents):\n  response = ollama.embeddings(model=EMB_MODEL, prompt=d)\n  embedding = response[\"embedding\"]\n  collection.add(\n    ids=[str(i)],\n    embeddings=[embedding],\n    documents=[d]\n  )\nra che abbiamo creato la nostra ‚ÄúKnowledge Base‚Äù, possiamo iniziare a fare query, recuperando dati da essa:\n\nQuery utente: Il processo inizia quando un utente pone una domanda, come ‚ÄúQuante api ci sono in una colonia? Chi depone le uova e in che quantit√†? E per quanto riguarda parassiti e malattie comuni?‚Äù\nprompt = \"How many bees are in a colony? Who lays eggs and how much? How about\\\n          common pests and diseases?\"\nQuery Embedding: La domanda dell‚Äôutente viene convertita in un embedding vettoriale utilizzando lo stesso modello di embedding  utilizzato per la knowledge base.\nresponse = ollama.embeddings(\n  prompt=prompt,\n  model=EMB_MODEL\n)\nRecupero di Documenti Pertinenti: Il sistema esegue una ricerca nella knowledge base utilizzando il ‚Äúquery embedding‚Äù per trovare i documenti pi√π pertinenti (in questo caso, i 5 pi√π probabili). Ci√≤ avviene tramite una ricerca di similarit√†, che confronta la ‚Äúquery embedding‚Äù con gli embedding di documenti nel database.\nresults = collection.query(\n  query_embeddings=[response[\"embedding\"]],\n  n_results=5\n)\ndata = results['documents']\nPrompt Augmentation: Le informazioni rilevanti recuperate vengono combinate con la query utente originale per creare un prompt ‚Äúaumentato‚Äù. Questo prompt ora contiene la domanda dell‚Äôutente e i fatti pertinenti dalla knowledge base.\nprompt=f\"Using this data: {data}. Respond to this prompt: {prompt}\",\nGenerazione di Risposte: Il prompt aumentato viene poi immesso in un modello linguistico, in questo caso il modello llama3.2:3b. Il modello utilizza questo contesto arricchito per generare una risposta completa. Parametri come temperatura, top_k e top_p vengono impostati per controllare la casualit√† e la qualit√† della risposta generata.\noutput = ollama.generate(\n  model=MODEL,\n  prompt=f\"Using this data: {data}. Respond to this prompt: {prompt}\",\n\n  options={\n    \"temperature\": 0.0,\n    \"top_k\":10,\n    \"top_p\":0.5                          }\n)\nResponse Delivery: Infine, il sistema restituisce all‚Äôutente la risposta generata.\nprint(output['response'])\nBased on the provided data, here are the answers to your questions:\n\n1. How many bees are in a colony?\nA typical bee colony can contain between 20,000 and 80,000 bees.\n\n2. Who lays eggs and how much?\nThe queen bee lays up to 2,000 eggs per day during peak seasons.\n\n3. What about common pests and diseases?\nCommon pests and diseases that affect bees include varroa mites, hive beetles,\nand foulbrood.\nCreiamo una funzione che ci aiuti a rispondere a nuove domande:\ndef rag_bees(prompt, n_results=5, temp=0.0, top_k=10, top_p=0.5):\n    start_time = time.perf_counter()  # Start timing\n    \n    # generate an embedding for the prompt and retrieve the data \n    response = ollama.embeddings(\n      prompt=prompt,\n      model=EMB_MODEL\n    )\n\n    \n    results = collection.query(\n      query_embeddings=[response[\"embedding\"]],\n      n_results=n_results\n    )\n\n    data = results['documents']\n    \n    # generate a response combining the prompt and data retrieved\n    output = ollama.generate(\n      model=MODEL,\n      prompt=f\"Using this data: {data}. Respond to this prompt: {prompt}\",\n\n      options={\n        \"temperature\": temp,\n        \"top_k\": top_k,\n        \"top_p\": top_p                          }\n    )\n\n    \n    print(output['response'])\n\n    \n    end_time = time.perf_counter()  # End timing\n    elapsed_time = round((end_time - start_time), 1)  # Calculate elapsed time\n    \n    print(f\"\\n [INFO] ==&gt; The code for model: {MODEL}, took {elapsed_time}s \\\n          to generate the answer.\\n\")\nOra possiamo creare delle query e chiamare la funzione:\nprompt = \"Are bees in Brazil?\"\nrag_bees(prompt)\nYes, bees are found in Brazil. According to the data, Brazil has more than 300\ndifferent bee species, and indigenous people in Brazil used bees for medicine and\nfood purposes. Additionally, reports from 1577 mention three native bees used by\nindigenous people in Brazil.\n\n [INFO] ==&gt; The code for model: llama3.2:3b, took 22.7s to generate the answer.\nA proposito, se il modello utilizzato supporta pi√π lingue, possiamo utilizzarlo (ad esempio, il portoghese), anche se il set di dati √® stato creato in inglese:\nprompt = \"Existem abelhas no Brazil?\"\nrag_bees(prompt)\nSim, existem abelhas no Brasil! De acordo com o relato de Hans Staden, h√° tr√™s \nesp√©cies de abelhas nativas do Brasil que foram mencionadas: manda√ßaia (Melipona\nquadrifasciata), mandaguari (Scaptotrigona postica) e jata√≠-amarela (Tetragonisca\nangustula). Al√©m disso, o Brasil √© conhecido por ter mais de 300 esp√©cies diferentes de abelhas, a maioria das quais n√£o √© agressiva e n√£o p√µe veneno.\n\n [INFO] ==&gt; The code for model: llama3.2:3b, took 54.6s to generate the answer.\n\n\nAndando Oltre\nI piccoli modelli LLM testati hanno funzionato bene sull‚Äôedge, sia nel testo che con le immagini, ma ovviamente avevano un‚Äôelevata latenza per quanto riguarda quest‚Äôultima. Una combinazione di modelli specifici e dedicati pu√≤ portare a risultati migliori; ad esempio, in casi reali, un modello di rilevamento degli oggetti (come YOLO) pu√≤ ottenere una descrizione generale e un conteggio degli oggetti su un‚Äôimmagine che, una volta passati a un LLM, possono aiutare a estrarre informazioni e azioni essenziali.\nSecondo Avi Baum, CTO di Hailo,\n\nNel vasto panorama dell‚Äôintelligenza artificiale (IA), uno dei viaggi pi√π intriganti √® stata l‚Äôevoluzione dell‚ÄôIA nell‚Äôedge. Questo viaggio ci ha portato dalla classica visione artificiale ai regni dell‚ÄôIA discriminativa, dell‚ÄôIA potenziata e ora, alla rivoluzionaria frontiera dell‚ÄôIA generativa. Ogni passo ci ha avvicinato a un futuro in cui i sistemi intelligenti si integrano perfettamente con la nostra vita quotidiana, offrendo un‚Äôesperienza immersiva non solo di percezione ma anche di creazione nel palmo della nostra mano.",
    "crumbs": [
      "Raspberry Pi",
      "Small Language Models (SLM)"
    ]
  },
  {
    "objectID": "contents/labs/raspi/llm/llm.it.html#conclusione",
    "href": "contents/labs/raspi/llm/llm.it.html#conclusione",
    "title": "Small Language Models (SLM)",
    "section": "Conclusione",
    "text": "Conclusione\nQuesto laboratorio ha dimostrato come un Raspberry Pi 5 pu√≤ essere trasformato in un potente hub AI in grado di eseguire ‚Äúlarge language model (LLM)‚Äù per analisi e approfondimenti dei dati in tempo reale e in loco utilizzando Ollama e Python. La versatilit√† e la potenza del Raspberry Pi, unite alle capacit√† di LLM leggeri come Llama 3.2 e LLaVa-Phi-3-mini, lo rendono un‚Äôeccellente piattaforma per applicazioni di edge computing.\nIl potenziale di esecuzione di LLM sull‚Äôedge si estende ben oltre la semplice elaborazione dei dati, come negli esempi di questo laboratorio. Ecco alcuni suggerimenti innovativi per l‚Äôutilizzo di questo progetto:\n1. Smart Home Automation:\n\nIntegrare gli SLM per interpretare i comandi vocali o analizzare i dati dei sensori per l‚Äôautomazione domestica intelligente. Ci√≤ potrebbe includere il monitoraggio e il controllo in tempo reale di dispositivi domestici, sistemi di sicurezza e gestione energetica, tutti elaborati localmente senza fare affidamento sui servizi cloud.\n\n2. Raccolta e Analisi dei Dati sul Campo:\n\nDistribuire gli SLM su Raspberry Pi in configurazioni remote o mobili per la raccolta e l‚Äôanalisi dei dati in tempo reale. Pu√≤ essere utilizzato in agricoltura per monitorare la salute delle colture, negli studi ambientali per il monitoraggio della fauna selvatica o nella risposta ai disastri per la consapevolezza della situazione e la gestione delle risorse.\n\n3. Strumenti Didattici:\n\nCreare strumenti didattici interattivi che sfruttano gli SLM per fornire feedback immediato, traduzione linguistica e tutoraggio. Pu√≤ essere particolarmente utile nelle regioni in via di sviluppo con accesso limitato a tecnologie avanzate e connettivit√† Internet.\n\n4. Applicazioni Sanitarie:\n\nUtilizzare gli SLM per la diagnosi medica e il monitoraggio dei pazienti. Possono fornire analisi in tempo reale dei sintomi e suggerire potenziali trattamenti. Pu√≤ essere integrato in piattaforme di telemedicina o dispositivi sanitari portatili.\n\n5. Intelligence Aziendale Locale:\n\nImplementare gli SLM in ambienti di vendita al dettaglio o di piccole imprese per analizzare il comportamento dei clienti, gestire l‚Äôinventario e ottimizzare le operazioni. La capacit√† di elaborare i dati localmente garantisce la privacy e riduce la dipendenza dai servizi esterni.\n\n6. IoT Industriale:\n\nIntegrare gli SLM nei sistemi IoT industriali per manutenzione predittiva, controllo qualit√† e ottimizzazione dei processi. Raspberry Pi pu√≤ fungere da unit√† di elaborazione dati localizzata, riducendo la latenza e migliorando l‚Äôaffidabilit√† dei sistemi automatizzati.\n\n7. Veicoli Autonomi:\n\nUtilizzare gli SLM per elaborare dati sensoriali da veicoli autonomi, consentendo decisioni e navigazione in tempo reale. Questo pu√≤ essere applicato a droni, robot e auto a guida autonoma per una maggiore autonomia e sicurezza.\n\n8. Patrimonio Culturale e Turismo:\n\nImplementare gli SLM per fornire siti del patrimonio culturale e guide museali interattive e informative. I visitatori possono utilizzare questi sistemi per ottenere informazioni e approfondimenti in tempo reale, migliorando la loro esperienza senza connettivit√† Internet.\n\n9. Progetti Artistici e Creativi:\n\nUtilizzare SLM per analizzare e generare contenuti creativi, come musica, arte e letteratura. Questo pu√≤ promuovere progetti innovativi nei settori creativi e consentire esperienze interattive uniche in mostre e spettacoli.\n\n10. Tecnologie Assistenziali Personalizzate:\n\nSviluppare tecnologie assistenziali per persone con disabilit√†, fornendo supporto personalizzato e adattivo tramite testo in tempo reale, traduzione di lingue e altri strumenti accessibili.",
    "crumbs": [
      "Raspberry Pi",
      "Small Language Models (SLM)"
    ]
  },
  {
    "objectID": "contents/labs/raspi/llm/llm.it.html#risorse",
    "href": "contents/labs/raspi/llm/llm.it.html#risorse",
    "title": "Small Language Models (SLM)",
    "section": "Risorse",
    "text": "Risorse\n\n10-Ollama_Python_Library notebook\n20-Ollama_Function_Calling notebook\n30-Function_Calling_with_images notebook\n40-RAG-simple-bee notebook\ncalc_distance_image python script",
    "crumbs": [
      "Raspberry Pi",
      "Small Language Models (SLM)"
    ]
  },
  {
    "objectID": "contents/labs/shared/kws_feature_eng/kws_feature_eng.it.html",
    "href": "contents/labs/shared/kws_feature_eng/kws_feature_eng.it.html",
    "title": "KWS Feature Engineering",
    "section": "",
    "text": "Panoramica\nIn questo tutorial pratico, l‚Äôenfasi √® posta sul ruolo critico che la ‚Äúfeature engineering‚Äù [ingegneria delle funzionalit√†] svolge nell‚Äôottimizzazione delle prestazioni dei modelli di machine learning applicati alle attivit√† di classificazione audio, come il riconoscimento vocale. √à essenziale essere consapevoli che le prestazioni di qualsiasi modello di apprendimento automatico dipendono in larga misura dalla qualit√† delle feature utilizzate e ci occuperemo della meccanica ‚Äúsotto il cofano‚Äù dell‚Äôestrazione delle feature, concentrandoci principalmente sui Mel-frequency Cepstral Coefficient (MFCC), una pietra miliare nel campo dell‚Äôelaborazione del segnale audio.\nI modelli di apprendimento automatico, in particolare gli algoritmi tradizionali, non comprendono le onde audio. Comprendono i numeri disposti in modo significativo, ovvero le feature. Queste feature incapsulano le caratteristiche del segnale audio, rendendo pi√π facile per i modelli distinguere tra suoni diversi.",
    "crumbs": [
      "Lab Condivisi",
      "KWS Feature Engineering"
    ]
  },
  {
    "objectID": "contents/labs/shared/kws_feature_eng/kws_feature_eng.it.html#panoramica",
    "href": "contents/labs/shared/kws_feature_eng/kws_feature_eng.it.html#panoramica",
    "title": "KWS Feature Engineering",
    "section": "",
    "text": "Questo tutorial si occuper√† della generazione di feature specificamente per la classificazione audio. Ci√≤ pu√≤ essere particolarmente interessante per l‚Äôapplicazione di machine learning a una variet√† di dati audio, sia per il riconoscimento vocale, la categorizzazione musicale, la classificazione degli insetti basata sui suoni del battito delle ali o altre attivit√† di analisi del suono",
    "crumbs": [
      "Lab Condivisi",
      "KWS Feature Engineering"
    ]
  },
  {
    "objectID": "contents/labs/shared/kws_feature_eng/kws_feature_eng.it.html#il-kws",
    "href": "contents/labs/shared/kws_feature_eng/kws_feature_eng.it.html#il-kws",
    "title": "KWS Feature Engineering",
    "section": "Il KWS",
    "text": "Il KWS\nL‚Äôapplicazione TinyML pi√π comune √® Keyword Spotting (KWS), un sottoinsieme del campo pi√π ampio del riconoscimento vocale. Mentre il riconoscimento vocale generale trascrive tutte le parole pronunciate in testo, il Keyword Spotting si concentra sul rilevamento di ‚Äúparole chiave‚Äù o ‚Äúwake word‚Äù specifiche in un flusso audio continuo. Il sistema √® addestrato a riconoscere queste parole chiave come frasi o parole predefinite, come yes o no. In breve, KWS √® una forma specializzata di riconoscimento vocale con il suo set di sfide e requisiti.\nEcco un tipico processo KWS che utilizza MFCC Feature Converter:\n\n\nApplicazioni di KWS\n\nAssistenti Vocali: In dispositivi come Alexa di Amazon o Google Home, KWS viene utilizzato per rilevare la wake word (‚ÄúAlexa‚Äù o ‚ÄúHey Google‚Äù) per attivare il dispositivo.\nComandi Attivati Tramite Voce: In contesti automobilistici o industriali, KWS pu√≤ essere utilizzato per avviare comandi specifici come ‚ÄúAvvia motore‚Äù o ‚ÄúSpegni luci‚Äù.\nSistemi di Sicurezza: I sistemi di sicurezza attivati tramite voce possono utilizzare KWS per autenticare gli utenti in base a una passphrase pronunciata.\nServizi di Telecomunicazione: Le linee del servizio clienti possono utilizzare KWS per instradare le chiamate in base a parole chiave pronunciate.\n\n\n\nDifferenze dal Riconoscimento Vocale Generale\n\nEfficienza Computazionale: KWS √® solitamente progettato per essere meno intensivo dal punto di vista computazionale rispetto al riconoscimento vocale completo, poich√© deve riconoscere solo un piccolo set di frasi.\nElaborazione in Tempo Reale: KWS spesso funziona in tempo reale ed √® ottimizzato per il rilevamento a bassa latenza delle parole chiave.\nVincoli di Risorse: I modelli KWS sono spesso progettati per essere leggeri, in modo da poter essere eseguiti su dispositivi con risorse computazionali limitate, come microcontrollori o telefoni cellulari.\nAttivit√† Mirata: Mentre i modelli di riconoscimento vocale generali sono addestrati per gestire un‚Äôampia gamma di vocabolario e accenti, i modelli KWS sono ottimizzati per riconoscere parole chiave specifiche, spesso in modo accurato in ambienti rumorosi.",
    "crumbs": [
      "Lab Condivisi",
      "KWS Feature Engineering"
    ]
  },
  {
    "objectID": "contents/labs/shared/kws_feature_eng/kws_feature_eng.it.html#panoramica-sui-segnali-audio",
    "href": "contents/labs/shared/kws_feature_eng/kws_feature_eng.it.html#panoramica-sui-segnali-audio",
    "title": "KWS Feature Engineering",
    "section": "Panoramica sui Segnali Audio",
    "text": "Panoramica sui Segnali Audio\nComprendere le propriet√† di base dei segnali audio √® fondamentale per un‚Äôestrazione efficace delle feature [caratteristiche] e, in ultima analisi, per applicare con successo gli algoritmi di apprendimento automatico nelle attivit√† di classificazione audio. I segnali audio sono forme d‚Äôonda complesse che catturano le fluttuazioni della pressione dell‚Äôaria nel tempo. Questi segnali possono essere caratterizzati da diversi attributi fondamentali: frequenza di campionamento, frequenza e ampiezza.\n\nFrequenza e Ampiezza: La Frequenza si riferisce al numero di oscillazioni che una forma d‚Äôonda subisce per unit√† di tempo e si misura anche in Hz. Nel contesto dei segnali audio, frequenze diverse corrispondono a pitch [toni] diversi. L‚ÄôAmpiezza, d‚Äôaltra parte, misura la grandezza delle oscillazioni e si correla con l‚Äôintensit√† del suono. Sia la frequenza che l‚Äôampiezza sono feature essenziali che catturano le qualit√† tonali e ritmiche dei segnali audio.\nFrequenza di Campionamento: La frequenza di campionamento, spesso indicata in Hertz (Hz), definisce il numero di campioni presi al secondo durante la digitalizzazione di un segnale analogico. Una frequenza di campionamento pi√π elevata consente una rappresentazione digitale pi√π accurata del segnale, ma richiede anche pi√π risorse di elaborazione. Le frequenze di campionamento tipiche includono 44,1 kHz per l‚Äôaudio di qualit√† CD e 16 kHz o 8 kHz per le attivit√† di riconoscimento vocale. Comprendere i compromessi nella selezione di una frequenza di campionamento appropriata √® essenziale per bilanciare accuratezza ed efficienza computazionale. In generale, con i progetti TinyML, lavoriamo con 16 KHz. Sebbene i toni musicali possano essere uditi a frequenze fino a 20 kHz, la voce raggiunge il massimo a 8 kHz. I sistemi telefonici tradizionali utilizzano una frequenza di campionamento di 8 kHz.\n\n\nPer una rappresentazione accurata del segnale, la frequenza di campionamento deve essere almeno il doppio della frequenza pi√π alta presente nel segnale.\n\n\nDominio del Tempo e Dominio della Frequenza: I segnali audio possono essere analizzati nei domini del tempo e della frequenza. Nel dominio del tempo, un segnale √® rappresentato come una forma d‚Äôonda in cui l‚Äôampiezza √® tracciata in funzione del tempo. Questa rappresentazione aiuta a osservare feature temporali come inizio e durata, ma le caratteristiche tonali del segnale non sono ben evidenziate. Al contrario, una rappresentazione del dominio della frequenza fornisce una vista delle frequenze costituenti del segnale e delle rispettive ampiezze, in genere ottenute tramite una trasformata di Fourier. Questo √® prezioso per le attivit√† che richiedono la comprensione del contenuto spettrale del segnale, come l‚Äôidentificazione di note musicali o fonemi vocali (il nostro caso).\n\nL‚Äôimmagine seguente mostra le parole YES e NO con rappresentazioni tipiche nei domini del tempo (audio grezzo) e della frequenza:\n\n\nPerch√© l‚ÄôAudio Grezzo No?\nSebbene l‚Äôutilizzo diretto di dati audio grezzi per attivit√† di apprendimento automatico possa sembrare allettante, questo approccio presenta diverse sfide che lo rendono meno adatto alla creazione di modelli solidi ed efficienti.\nL‚Äôutilizzo di dati audio grezzi per il Keyword Spotting (KWS), ad esempio, su dispositivi TinyML pone delle sfide dovute alla sua elevata dimensionalit√† (utilizzando una frequenza di campionamento di 16 kHz), alla complessit√† computazionale per l‚Äôacquisizione di feature temporali, alla suscettibilit√† al rumore e alla mancanza di feature semanticamente significative, rendendo le tecniche di estrazione di caratteristiche come MFCC una scelta pi√π pratica per applicazioni con risorse limitate.\nEcco alcuni dettagli aggiuntivi sui problemi critici associati all‚Äôutilizzo di audio grezzi:\n\nAlta Dimensionalit√†: I segnali audio, in particolare quelli campionati ad alte velocit√†, generano grandi quantit√† di dati. Ad esempio, una clip audio di 1 secondo campionata a 16 kHz avr√† 16.000 punti singoli. I dati ad alta dimensionalit√† aumentano la complessit√† computazionale, portando a tempi di training pi√π lunghi e costi computazionali pi√π elevati, rendendoli poco pratici per ambienti con risorse limitate. Inoltre, l‚Äôampia gamma dinamica dei segnali audio richiede una quantit√† significativa di bit per campione, mentre trasmette poche informazioni utili.\nDipendenze Temporali: I segnali audio grezzi hanno strutture temporali che i semplici modelli di apprendimento automatico potrebbero trovare difficili da catturare. Sebbene le reti neurali ricorrenti come LSTM possano modellare tali dipendenze, sono computazionalmente intensive e difficili da addestrare su dispositivi di piccole dimensioni.\nRumore e Variabilit√†: I segnali audio grezzi spesso contengono rumore di fondo e altri elementi non essenziali che influenzano le prestazioni del modello. Inoltre, lo stesso suono pu√≤ avere caratteristiche diverse in base a vari fattori come la distanza dal microfono, l‚Äôorientamento della sorgente sonora e le propriet√† acustiche dell‚Äôambiente, aggiungendo complessit√† ai dati.\nMancanza di Significato Semantico: L‚Äôaudio grezzo non contiene intrinsecamente feature semanticamente significative per le attivit√† di classificazione. Feature come tono, tempo e feature spettrali, che possono essere cruciali per il riconoscimento vocale, non sono direttamente accessibili dai dati della forma d‚Äôonda grezza.\nRidondanza del Segnale: I segnali audio spesso contengono informazioni ridondanti, con alcune porzioni del segnale che contribuiscono poco o per niente al compito da svolgere. Questa ridondanza pu√≤ rendere l‚Äôapprendimento inefficiente e potenzialmente portare a un overfitting.\n\nPer queste ragioni, tecniche di estrazione di feature come i Mel-frequency Cepstral Coefficients (MFCCs), le Mel-Frequency Energies (MFEs) e gli spettrogrammi semplici sono comunemente utilizzate per trasformare i dati audio grezzi in un formato pi√π gestibile e informativo. Queste feature catturano le caratteristiche essenziali del segnale audio riducendo dimensionalit√† e rumore, facilitando un apprendimento automatico pi√π efficace.",
    "crumbs": [
      "Lab Condivisi",
      "KWS Feature Engineering"
    ]
  },
  {
    "objectID": "contents/labs/shared/kws_feature_eng/kws_feature_eng.it.html#panoramica-sugli-mfcc",
    "href": "contents/labs/shared/kws_feature_eng/kws_feature_eng.it.html#panoramica-sugli-mfcc",
    "title": "KWS Feature Engineering",
    "section": "Panoramica sugli MFCC",
    "text": "Panoramica sugli MFCC\n\nCosa sono gli MFCC?\nI Mel-frequency Cepstral Coefficients (MFCC) sono un set di feature derivate dal contenuto spettrale di un segnale audio. Si basano sulle percezioni uditive umane e sono comunemente utilizzati per catturare le feature fonetiche di un segnale audio. Gli MFCC vengono calcolati tramite un processo in pi√π fasi che include pre-enfasi, inquadratura, windowing, applicazione della Fast Fourier Transform (FFT) per convertire il segnale nel dominio della frequenza e, infine, applicazione della Discrete Cosine Transform (DCT). Il risultato √® una rappresentazione compatta delle caratteristiche spettrali del segnale audio originale.\nL‚Äôimmagine seguente mostra le parole YES e NO nella loro rappresentazione MFCC:\n\n\nQuesto video spiega i Mel Frequency Cepstral Coefficients (MFCC) e come calcolarli.\n\n\n\nPerch√© gli MFCC sono importanti?\nGli MFCC sono fondamentali per diversi motivi, in particolare nel contesto di Keyword Spotting (KWS) e TinyML:\n\nRiduzione della Dimensionalit√†: Gli MFCC catturano le caratteristiche spettrali essenziali del segnale audio riducendo significativamente la dimensionalit√† dei dati, rendendoli ideali per applicazioni TinyML con risorse limitate.\nRobustezza: Gli MFCC sono meno sensibili al rumore e alle variazioni di tono e ampiezza, offrendo un set di funzionalit√† pi√π stabile e robusto per le attivit√† di classificazione audio.\nModellazione del Sistema Uditivo Umano: La scala Mel negli MFCC approssima la risposta dell‚Äôorecchio umano a diverse frequenze, rendendoli pratici per il riconoscimento vocale in cui √® desiderata una percezione simile a quella umana.\nEfficienza Computazionale: Il processo di calcolo degli MFCC √® efficiente dal punto di vista computazionale, rendendolo adatto per applicazioni in tempo reale su hardware con risorse computazionali limitate.\n\nIn sintesi, gli MFCC offrono un equilibrio tra ricchezza di informazioni ed efficienza computazionale, rendendoli popolari per le attivit√† di classificazione audio, in particolare in ambienti limitati come TinyML.\n\n\nCalcolo degli MFCC\nIl calcolo dei Mel-frequency Cepstral Coefficients (MFCCs) comporta diversi passaggi chiave. Esaminiamoli, che sono particolarmente importanti per le attivit√† di Keyword Spotting (KWS) sui dispositivi TinyML.\n\nPre-enfasi: Il primo passaggio √® la pre-enfasi, che viene applicata per accentuare le componenti ad alta frequenza del segnale audio e bilanciare lo spettro di frequenza. Ci√≤ si ottiene applicando un filtro che amplifica la differenza tra campioni consecutivi. La formula per la pre-enfasi √®: y(t) = x(t) - \\(\\alpha\\) x(t-1), dove \\(\\alpha\\) √® il fattore di pre-enfasi, in genere intorno a 0.97.\nFraming: I segnali audio sono divisi in frame brevi (la frame length), in genere da 20 a 40 millisecondi. Ci√≤ si basa sul presupposto che le frequenze in un segnale siano stazionarie per un breve periodo. Il framing aiuta ad analizzare il segnale in slot di tempo cos√¨ piccoli. Il frame stride (o step) sposter√† un frame e quello adiacente. Questi step potrebbero essere sequenziali o sovrapposti.\nWindowing: Ogni frame viene quindi sottoposto a windowing per ridurre al minimo le discontinuit√† ai confini del frame. Una funzione window comunemente utilizzata √® la finestra di Hamming. Il windowing prepara il segnale per una trasformata di Fourier riducendo al minimo gli effetti alle estremit√†. L‚Äôimmagine sottostante mostra tre frame (10, 20 e 30) e i campioni di tempo dopo il windowing (notare che la lunghezza del frame e il frame stride sono di 20 ms):\n\n\n\nFast Fourier Transform (FFT) La Fast Fourier Transform (FFT) viene applicata a ciascun frame sottoposto a windowing per convertirlo dal dominio del tempo al dominio della frequenza. La FFT ci fornisce una rappresentazione a valori complessi che include sia informazioni di ampiezza che di fase. Tuttavia, per gli MFCC, solo l‚Äôampiezza viene utilizzata per calcolare lo spettro di potenza. Lo spettro di potenza √® il quadrato dello spettro di ampiezza e misura l‚Äôenergia presente in ogni componente di frequenza.\n\n\nLo spettro di potenza \\(P(f)\\) di un segnale \\(x(t)\\) √® definito come \\(P(f) = |X(f)|^2\\), dove \\(X(f)\\) √® la trasformata di Fourier di \\(x(t)\\). Elevando al quadrato l‚Äôampiezza della trasformata di Fourier, mettiamo in risalto le frequenze pi√π forti rispetto a quelle pi√π deboli, catturando cos√¨ caratteristiche spettrali pi√π rilevanti del segnale audio. Ci√≤ √® importante in applicazioni come la classificazione audio, il riconoscimento vocale e il Keyword Spotting (KWS), in cui l‚Äôattenzione √® rivolta all‚Äôidentificazione di distinti pattern di frequenza che caratterizzano diverse classi di audio o fonemi nel parlato.\n\n\n\nMel Filter Banks: Il dominio di frequenza viene poi mappato sulla scala Mel, che approssima la risposta dell‚Äôorecchio umano a diverse frequenze. L‚Äôidea √® di estrarre pi√π feature (pi√π filtri) nelle frequenze pi√π basse e meno in quelle alte. Pertanto, funziona bene sui suoni distinti dall‚Äôorecchio umano. In genere, da 20 a 40 filtri triangolari estraggono le energie di frequenza Mel. Queste energie vengono poi trasformate in logaritmo per convertire i fattori moltiplicativi in quelli additivi, rendendoli pi√π adatti per un‚Äôulteriore elaborazione.\n\n\n\nDiscrete Cosine Transform (DCT): L‚Äôultimo passaggio consiste nell‚Äôapplicare la Trasformata discreta del coseno (DCT) alle energie del log Mel. La DCT aiuta a de-correlare le energie, comprimendo efficacemente i dati e mantenendo solo le feature pi√π discriminanti. Di solito, vengono mantenuti i primi 12-13 coefficienti DCT, formando il vettore finale delle feature MFCC.",
    "crumbs": [
      "Lab Condivisi",
      "KWS Feature Engineering"
    ]
  },
  {
    "objectID": "contents/labs/shared/kws_feature_eng/kws_feature_eng.it.html#pratica-con-python",
    "href": "contents/labs/shared/kws_feature_eng/kws_feature_eng.it.html#pratica-con-python",
    "title": "KWS Feature Engineering",
    "section": "Pratica con Python",
    "text": "Pratica con Python\nApplichiamo quanto discusso lavorando su un campione audio reale. Aprire il notebook su Google CoLab ed estrarre le funzionalit√† MLCC sui campioni audio: [Open In Colab]",
    "crumbs": [
      "Lab Condivisi",
      "KWS Feature Engineering"
    ]
  },
  {
    "objectID": "contents/labs/shared/kws_feature_eng/kws_feature_eng.it.html#conclusione",
    "href": "contents/labs/shared/kws_feature_eng/kws_feature_eng.it.html#conclusione",
    "title": "KWS Feature Engineering",
    "section": "Conclusione",
    "text": "Conclusione\nQuale Tecnica di Estrazione delle Feature dovremmo usare?\nI Mel-frequency Cepstral Coefficient (MFCC), le Mel-Frequency Energie (MFE), o lo Spettrogramma sono tecniche per rappresentare dati audio, che sono spesso utili in diversi contesti.\nIn generale, gli MFCC sono pi√π focalizzati sulla cattura dell‚Äôinviluppo dello spettro di potenza, il che li rende meno sensibili ai dettagli spettrali a grana fine ma pi√π robusti al rumore. Questo √® spesso auspicabile per le attivit√† relative al parlato. D‚Äôaltro canto, gli spettrogrammi o MFE conservano informazioni di frequenza pi√π dettagliate, il che pu√≤ essere vantaggioso in attivit√† che richiedono discriminazione basata su contenuti spettrali a grana fine.\n\nGli MFCC sono particolarmente efficaci per:\n\nRiconoscimento Vocale: Gli MFCC sono eccellenti per identificare il contenuto fonetico nei segnali vocali.\nIdentificazione del Parlante: Possono essere utilizzati per distinguere tra diversi parlanti in base alle caratteristiche della voce.\nRiconoscimento delle Emozioni: Gli MFCC possono catturare le sfumature nel parlato indicative di stati emotivi.\nKeyword Spotting: Specialmente in TinyML, dove la bassa complessit√† computazionale e le piccole dimensioni delle funzionalit√† sono cruciali.\n\n\n\nGli spettrogrammi o MFE sono spesso pi√π adatti per:\n\nAnalisi Musicale: Gli spettrogrammi possono catturare strutture armoniche e timbriche nella musica, il che √® essenziale per attivit√† come la classificazione del genere, il riconoscimento degli strumenti o la trascrizione musicale.\nClassificazione dei Suoni Ambientali: Nel riconoscimento di suoni ambientali non vocali (ad esempio pioggia, vento, traffico), lo spettrogramma completo pu√≤ fornire feature pi√π discriminanti.\nIdentificazione del Canto degli Uccelli: I dettagli intricati dei richiami degli uccelli sono spesso meglio catturati utilizzando gli spettrogrammi.\nElaborazione del Segnale Bioacustico: In applicazioni come l‚Äôanalisi dei richiami dei delfini o dei pipistrelli, le informazioni di frequenza a grana fine in uno spettrogramma possono essere essenziali.\nAudio Quality Assurance: Gli spettrogrammi sono spesso utilizzati nell‚Äôanalisi audio professionale per identificare rumori indesiderati, clic o altri artefatti.",
    "crumbs": [
      "Lab Condivisi",
      "KWS Feature Engineering"
    ]
  },
  {
    "objectID": "contents/labs/shared/kws_feature_eng/kws_feature_eng.it.html#risorse",
    "href": "contents/labs/shared/kws_feature_eng/kws_feature_eng.it.html#risorse",
    "title": "KWS Feature Engineering",
    "section": "Risorse",
    "text": "Risorse\n\nNotebook Audio_Data_Analysis Colab",
    "crumbs": [
      "Lab Condivisi",
      "KWS Feature Engineering"
    ]
  },
  {
    "objectID": "contents/labs/shared/dsp_spectral_features_block/dsp_spectral_features_block.it.html",
    "href": "contents/labs/shared/dsp_spectral_features_block/dsp_spectral_features_block.it.html",
    "title": "Blocco delle Feature Spettrali DSP",
    "section": "",
    "text": "Panoramica\nI progetti TinyML correlati al movimento (o alle vibrazioni) coinvolgono dati da IMU (solitamente accelerometri e giroscopi). Questi dataset di tipo temporale dovrebbero essere pre-elaborati prima di inserirli in un training di modello di apprendimento automatico, che √® un‚Äôarea impegnativa per l‚Äôapprendimento automatico embedded. Tuttavia, Edge Impulse aiuta a superare questa complessit√† con la sua fase di pre-elaborazione dell‚Äôelaborazione del segnale digitale (DSP) e, pi√π specificamente, il Spectral Features Block per i sensori inerziali.\nMa come funziona internamente? Analizziamolo nel dettaglio.",
    "crumbs": [
      "Lab Condivisi",
      "Blocco delle Feature Spettrali DSP"
    ]
  },
  {
    "objectID": "contents/labs/shared/dsp_spectral_features_block/dsp_spectral_features_block.it.html#estrazione-delle-feature-di-revisione",
    "href": "contents/labs/shared/dsp_spectral_features_block/dsp_spectral_features_block.it.html#estrazione-delle-feature-di-revisione",
    "title": "Blocco delle Feature Spettrali DSP",
    "section": "Estrazione delle Feature di Revisione",
    "text": "Estrazione delle Feature di Revisione\nL‚Äôestrazione di feature [caratteristiche] da un dataset catturato con sensori inerziali, come gli accelerometri, comporta l‚Äôelaborazione e l‚Äôanalisi dei dati grezzi. Gli accelerometri misurano l‚Äôaccelerazione di un oggetto lungo uno o pi√π assi (in genere tre, indicati come X, Y e Z). Queste misure possono essere utilizzate per comprendere vari aspetti del movimento dell‚Äôoggetto, come pattern di movimento e vibrazioni. Ecco una panoramica di alto livello del processo:\nRaccolta dati: Per prima cosa, dobbiamo raccogliere i dati dagli accelerometri. A seconda dell‚Äôapplicazione, i dati possono essere raccolti a diverse frequenze di campionamento. √à essenziale assicurarsi che la frequenza di campionamento sia sufficientemente alta da catturare le dinamiche rilevanti del movimento studiato (la frequenza di campionamento dovrebbe essere almeno il doppio della massima frequenza presente nel segnale).\nPre-elaborazione dei dati: I dati grezzi dell‚Äôaccelerometro possono essere rumorosi e contenere errori o informazioni irrilevanti. I passaggi di pre-elaborazione, come il filtraggio e la normalizzazione, possono aiutare a pulire e standardizzare i dati, rendendoli pi√π adatti all‚Äôestrazione di feature.\n\nStudio non esegue la normalizzazione o la standardizzazione, quindi a volte, quando si lavora con Sensor Fusion [gruppi di sensori], potrebbe essere necessario eseguire questo passaggio prima di caricare i dati in Studio. Ci√≤ √® particolarmente cruciale nei progetti di gruppi di sensori, come visto in questo tutorial, Sensor Data Fusion with Spresense and CommonSense.\n\nSegmentazione: A seconda della natura dei dati e dell‚Äôapplicazione, potrebbe essere necessario dividere i dati in segmenti pi√π piccoli o finestre. Ci√≤ pu√≤ aiutare a concentrarsi su eventi o attivit√† specifici all‚Äôinterno del dataset, rendendo l‚Äôestrazione di feature pi√π gestibile e significativa. La scelta della window size [dimensione della finestra] e della sovrapposizione (window span) dipende dall‚Äôapplicazione e dalla frequenza degli eventi di interesse. Come regola generale, dovremmo provare a catturare un paio di ‚Äúcicli di dati‚Äù.\nEstrazione delle feature: Una volta che i dati sono stati pre-elaborati e segmentati, √® possibile estrarre feature che descrivono le caratteristiche del movimento. Alcune feature tipiche estratte dai dati dell‚Äôaccelerometro includono:\n\nLe feature del Time-domain descrivono le propriet√† statistiche dei dati all‚Äôinterno di ciascun segmento, come media, mediana, deviazione standard, asimmetria, curtosi e tasso di attraversamento dello zero.\nLe feature Frequency-domain si ottengono trasformando i dati nel dominio della frequenza utilizzando tecniche come la Fast Fourier Transform (FFT). Alcune feature tipiche del dominio della frequenza includono lo spettro di potenza, l‚Äôenergia spettrale, le frequenze dominanti (ampiezza e frequenza) e l‚Äôentropia spettrale.\nLe feature del dominio Time-frequency combinano le informazioni del dominio del tempo e della frequenza, come la Short-Time Fourier Transform (STFT) o la Discrete Wavelet Transform (DWT). Possono fornire una comprensione pi√π dettagliata di come il contenuto di frequenza del segnale cambia nel tempo.\n\nIn molti casi, il numero di feature estratte pu√≤ essere elevato, il che pu√≤ portare a un overfitting o a una maggiore complessit√† computazionale. Le tecniche di selezione delle feature, come le informazioni reciproche, i metodi basati sulla correlazione o l‚Äôanalisi delle componenti principali (PCA), possono aiutare a identificare le feature pi√π rilevanti per una determinata applicazione e ridurre la dimensionalit√† del dataset. Studio pu√≤ aiutare con tali calcoli rilevanti per le feature.\nEsploriamo pi√π in dettaglio un tipico progetto di classificazione del movimento TinyML trattato in questa serie di esercitazioni pratiche.",
    "crumbs": [
      "Lab Condivisi",
      "Blocco delle Feature Spettrali DSP"
    ]
  },
  {
    "objectID": "contents/labs/shared/dsp_spectral_features_block/dsp_spectral_features_block.it.html#un-progetto-tinyml-di-motion-classification",
    "href": "contents/labs/shared/dsp_spectral_features_block/dsp_spectral_features_block.it.html#un-progetto-tinyml-di-motion-classification",
    "title": "Blocco delle Feature Spettrali DSP",
    "section": "Un progetto TinyML di Motion Classification",
    "text": "Un progetto TinyML di Motion Classification\n\nNel progetto, Motion Classification and Anomaly Detection, abbiamo simulato sollecitazioni meccaniche nel trasporto, dove il nostro problema era classificare quattro classi di movimento:\n\nMaritime (Pallet in navi)\nTerrestrial (pallet in un Camion o Treno)\nLift [Sollevamento] (pallet movimentati da Carrello elevatore)\nIdle (pallet in Magazzini)\n\nGli accelerometri hanno fornito i dati sul pallet (o il container).\n\nDi seguito √® riportato un campione (dati grezzi) di 10 secondi, acquisito con una frequenza di campionamento di 50 Hz:\n\n\nIl risultato √® simile quando questa analisi viene eseguita su un altro set di dati con lo stesso principio, utilizzando una frequenza di campionamento diversa, 62.5Hz invece di 50Hz.",
    "crumbs": [
      "Lab Condivisi",
      "Blocco delle Feature Spettrali DSP"
    ]
  },
  {
    "objectID": "contents/labs/shared/dsp_spectral_features_block/dsp_spectral_features_block.it.html#pre-elaborazione-dei-dati",
    "href": "contents/labs/shared/dsp_spectral_features_block/dsp_spectral_features_block.it.html#pre-elaborazione-dei-dati",
    "title": "Blocco delle Feature Spettrali DSP",
    "section": "Pre-elaborazione dei Dati",
    "text": "Pre-elaborazione dei Dati\nI dati grezzi acquisiti dall‚Äôaccelerometro (dati di ‚Äúserie temporali‚Äù) devono essere convertiti in ‚Äúdati tabellari‚Äù utilizzando uno dei tipici metodi di estrazione delle feature descritti nell‚Äôultima sezione.\nDovremmo segmentare i dati utilizzando una finestra scorrevole sui dati campione per l‚Äôestrazione delle feature. Il progetto ha acquisito i dati dell‚Äôaccelerometro ogni 10 secondi con una frequenza di campionamento di 62.5 Hz. Una finestra di 2 secondi acquisisce 375 punti dati (3 assi x 2 secondi x 62.5 campioni). La finestra scorre ogni 80 ms, creando un dataset pi√π grande in cui ogni istanza ha 375 ‚Äúfeature grezze‚Äù.\n\nSu Studio, la versione precedente (V1) dello Spectral Analysis Block estraeva come caratteristiche del dominio del tempo solo l‚ÄôRMS e, per il dominio della frequenza, i picchi e la frequenza (utilizzando FFT) e le caratteristiche di potenza (PSD) del segnale nel tempo, risultando in un set di dati tabellari fisso di 33 caratteristiche (11 per ogni asse),\n\nQuelle 33 feature erano il tensore di input di un classificatore di reti neurali.\nNel 2022, Edge Impulse ha rilasciato la versione 2 dello Spectral Analysis block, che esploreremo qui.\n\nEdge Impulse - Spectral Analysis Block V.2 funzionamento\nNella Versione 2, le caratteristiche statistiche del dominio del tempo per asse/canale sono:\n\nRMS\nSkewness\nKurtosis\n\nE le caratteristiche spettrali del dominio della frequenza per asse/canale sono:\n\nSpectral Power [Potenza spettrale]\nSkewness [asimmetria] (nella prossima versione)\nCurtosi (nella prossima versione)\n\nIn questo link possiamo avere maggiori dettagli sull‚Äôestrazione delle feature.\n\nClonare il progetto pubblico. Si pu√≤ anche seguire la spiegazione, giocando col codice usando il mio Google CoLab Notebook: Edge Impulse Spectral Analysis Block Notebook.\n\nSi inizia importando le librerie:\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport math\nfrom scipy.stats import skew, kurtosis\nfrom scipy import signal\nfrom scipy.signal import welch\nfrom scipy.stats import entropy\nfrom sklearn import preprocessing\nimport pywt\n\nplt.rcParams['figure.figsize'] = (12, 6)\nplt.rcParams['lines.linewidth'] = 3\nDal progetto studiato, scegliamo un campione di dati dagli accelerometri come di seguito:\n\nDimensione della finestra di 2 secondi: [2,000] ms\nFrequenza di campionamento: [62.5] Hz\nSceglieremo il filtro [None] (per semplicit√†) e una\nLunghezza FFT: [16].\n\nf =  62.5 # Hertz\nwind_sec = 2 # seconds\nFFT_Lenght = 16\naxis = ['accX', 'accY', 'accZ']\nn_sensors = len(axis)\n\nSelezionando Raw Features nella scheda Studio Spectral Analysis, possiamo copiare tutti i 375 punti dati di una particolare finestra di 2 secondi negli appunti.\n\nIncollare i punti dati in una nuova variabile data:\ndata=[-5.6330, 0.2376, 9.8701, -5.9442, 0.4830, 9.8701, -5.4217, ...]\nNo_raw_features = len(data)\nN = int(No_raw_features/n_sensors)\nLe feature grezze totali sono 375, ma lavoreremo con ogni asse singolarmente, dove N= 125 (numero di campioni per asse).\nVogliamo capire come Edge Impulse ottiene le feature elaborate.\n\nQuindi, si devono anche incollare le feature elaborate su una variabile (per confrontare le feature calcolate in Python con quelle fornite da Studio):\nfeatures = [2.7322, -0.0978, -0.3813, 2.3980, 3.8924, 24.6841, 9.6303, ...]\nN_feat = len(features)\nN_feat_axis = int(N_feat/n_sensors)\nIl numero totale di feature elaborate √® 39, il che significa 13 feature/asse.\nOsservando attentamente queste 13 feature, ne troveremo 3 per il dominio del tempo (RMS, Skewness e Kurtosis):\n\n[rms] [skew] [kurtosis]\n\ne 10 per il dominio della frequenza (ci torneremo pi√π avanti).\n\n[spectral skew][spectral kurtosis][Spectral Power 1] ... [Spectral Power 8]\n\nSuddivisione dei dati grezzi per sensore\nI dati hanno campioni da tutti gli assi; dividiamoli e tracciamo un grafico separatamente:\ndef plot_data(sensors, axis, title):\n    [plt.plot(x, label=y) for x,y in zip(sensors, axis)]\n    plt.legend(loc='lower right')\n    plt.title(title)\n    plt.xlabel('#Sample')\n    plt.ylabel('Value')\n    plt.box(False)\n    plt.grid()\n    plt.show()\n\naccX = data[0::3]\naccY = data[1::3]\naccZ = data[2::3]\nsensors = [accX, accY, accZ] \nplot_data(sensors, axis, 'Raw Features')\n\nSottrazione della media\nSuccessivamente, dovremmo sottrarre la media dai dati. La sottrazione della media da un set di dati √® una comune fase di pre-elaborazione dei dati in statistica e apprendimento automatico. Lo scopo della sottrazione della media dai dati √® di centrare i dati attorno allo zero. Questo √® importante perch√© pu√≤ rivelare pattern e relazioni che potrebbero essere nascosti se i dati non sono centrati.\nEcco alcuni motivi specifici per cui la sottrazione della media pu√≤ essere utile:\n\nSemplifica l‚Äôanalisi: Centrando i dati, la media diventa zero, rendendo alcuni calcoli pi√π semplici e facili da interpretare.\nRimuove la distorsione: Se i dati sono distorti, sottraendo la media √® possibile rimuoverli e consentire un‚Äôanalisi pi√π accurata.\nPu√≤ rivelare pattern: Centrare i dati pu√≤ aiutare a scoprire pattern che potrebbero essere nascosti se i dati non sono centrati. Ad esempio, centrare i dati pu√≤ aiutare a identificare i trend nel tempo se si analizza un set di dati di serie temporali.\nPu√≤ migliorare le prestazioni: in alcuni algoritmi di apprendimento automatico, centrare i dati pu√≤ migliorare le prestazioni riducendo l‚Äôinfluenza dei valori anomali e rendendo i dati pi√π facilmente confrontabili. Nel complesso, sottrarre la media √® una tecnica semplice ma potente che pu√≤ essere utilizzata per migliorare l‚Äôanalisi e l‚Äôinterpretazione dei dati.\n\ndtmean = [(sum(x)/len(x)) for x in sensors]\n[print('mean_'+x+'= ', round(y, 4)) for x,y in zip(axis, dtmean)][0]\n\naccX = [(x - dtmean[0]) for x in accX]\naccY = [(x - dtmean[1]) for x in accY]\naccZ = [(x - dtmean[2]) for x in accZ]\nsensors = [accX, accY, accZ]\n\nplot_data(sensors, axis, 'Raw Features - Subctract the Mean')",
    "crumbs": [
      "Lab Condivisi",
      "Blocco delle Feature Spettrali DSP"
    ]
  },
  {
    "objectID": "contents/labs/shared/dsp_spectral_features_block/dsp_spectral_features_block.it.html#feature-statistiche-del-dominio-del-tempo",
    "href": "contents/labs/shared/dsp_spectral_features_block/dsp_spectral_features_block.it.html#feature-statistiche-del-dominio-del-tempo",
    "title": "Blocco delle Feature Spettrali DSP",
    "section": "Feature Statistiche del Dominio del Tempo",
    "text": "Feature Statistiche del Dominio del Tempo\nCalcolo RMS\nIl valore RMS di un set di valori (o di una forma d‚Äôonda a tempo continuo) √® la radice quadrata della media aritmetica dei quadrati dei valori o del quadrato della funzione che definisce la forma d‚Äôonda continua. In fisica, il valore RMS di una corrente elettrica √® definito come il ‚Äúvalore della corrente continua che dissipa la stessa potenza in un resistore‚Äù.\nNel caso di un set di n valori {ùë•1, ùë•2, ‚Ä¶, ùë•ùëõ}, l‚ÄôRMS √®:\n\n\nNOTARE che il valore RMS √® diverso per i dati grezzi originali e dopo aver sottratto la media\n\n# Using numpy and standartized data (subtracting mean)\nrms = [np.sqrt(np.mean(np.square(x))) for x in sensors]\nPossiamo confrontare i valori RMS calcolati qui con quelli presentati da Edge Impulse:\n[print('rms_'+x+'= ', round(y, 4)) for x,y in zip(axis, rms)][0]\nprint(\"\\nCompare with Edge Impulse result features\")\nprint(features[0:N_feat:N_feat_axis])\nrms_accX=  2.7322\nrms_accY=  0.7833\nrms_accZ=  0.1383\nRispetto alle caratteristiche dei risultati di Edge Impulse:\n[2.7322, 0.7833, 0.1383]\nCalcolo di asimmetria e curtosi\nIn statistica, asimmetria e curtosi sono due modi per misurare la forma di una distribuzione.\nQui possiamo vedere la distribuzione dei valori del sensore:\nfig, axes = plt.subplots(nrows=1, ncols=3, figsize=(13, 4))\nsns.kdeplot(accX, fill=True, ax=axes[0])\nsns.kdeplot(accY, fill=True, ax=axes[1])\nsns.kdeplot(accZ, fill=True, ax=axes[2])\naxes[0].set_title('accX')\naxes[1].set_title('accY')\naxes[2].set_title('accZ')\nplt.suptitle('IMU Sensors distribution', fontsize=16, y=1.02)\nplt.show()\n\nLa Skewness [asimmetria] √® una misura dell‚Äôasimmetria di una distribuzione. Questo valore pu√≤ essere positivo o negativo.\n\n\nUn‚Äôasimmetria negativa indica che la coda si trova sul lato sinistro della distribuzione, che si estende verso valori pi√π negativi.\nUn‚Äôasimmetria positiva indica che la coda si trova sul lato destro della distribuzione, che si estende verso valori pi√π positivi.\nUn valore zero indica che non c‚Äô√® alcuna asimmetria nella distribuzione, il che significa che la distribuzione √® perfettamente simmetrica.\n\nskew = [skew(x, bias=False) for x in sensors]\n[print('skew_'+x+'= ', round(y, 4)) for x,y in zip(axis, skew)][0]\nprint(\"\\nCompare with Edge Impulse result features\")\nfeatures[1:N_feat:N_feat_axis]\nskew_accX=  -0.099\nskew_accY=  0.1756\nskew_accZ=  6.9463\nRispetto alle caratteristiche dei risultati di Edge Impulse:\n[-0.0978, 0.1735, 6.8629]\nLa Kurtosis √® una misura che indica se una distribuzione √® a coda pesante o a coda leggera rispetto a una distribuzione normale.\n\n\nLa curtosi di una distribuzione normale √® zero.\nSe una data distribuzione ha una curtosi negativa, si dice che √® platicurtica, il che significa che tende a produrre meno valori anomali e meno estremi rispetto alla distribuzione normale.\nSe una data distribuzione ha una curtosi positiva, si dice che √® leptocurtica, il che significa che tende a produrre pi√π valori anomali rispetto alla distribuzione normale.\n\nkurt = [kurtosis(x, bias=False) for x in sensors]\n[print('kurt_'+x+'= ', round(y, 4)) for x,y in zip(axis, kurt)][0]\nprint(\"\\nCompare with Edge Impulse result features\")\nfeatures[2:N_feat:N_feat_axis]\nkurt_accX=  -0.3475\nkurt_accY=  1.2673\nkurt_accZ=  68.1123\nRispetto alle caratteristiche dei risultati di Edge Impulse:\n[-0.3813, 1.1696, 65.3726]",
    "crumbs": [
      "Lab Condivisi",
      "Blocco delle Feature Spettrali DSP"
    ]
  },
  {
    "objectID": "contents/labs/shared/dsp_spectral_features_block/dsp_spectral_features_block.it.html#feature-spettrali",
    "href": "contents/labs/shared/dsp_spectral_features_block/dsp_spectral_features_block.it.html#feature-spettrali",
    "title": "Blocco delle Feature Spettrali DSP",
    "section": "Feature spettrali",
    "text": "Feature spettrali\nIl segnale filtrato viene trasmesso alla sezione di potenza spettrale, che calcola la FFT per generare le caratteristiche spettrali.\nPoich√© la finestra campionata √® solitamente pi√π grande della dimensione FFT, la finestra verr√† suddivisa in frame (o ‚Äúsotto-finestre‚Äù) e la FFT verr√† calcolata su ogni frame.\nFFT length - La dimensione della FFT. Questo determina il numero di bin FFT e la risoluzione dei picchi di frequenza che possono essere separati. Un numero basso significa che pi√π segnali saranno mediati insieme nello stesso bin FFT, ma riduce anche il numero di feature e la dimensione del modello. Un numero alto separer√† pi√π segnali in bin separati, generando un modello pi√π grande.\n\nIl numero totale di feature di potenza spettrale varier√† a seconda di come si impostano i parametri del filtro e di FFT. Senza filtro, il numero di feature √® 1/2 della lunghezza FFT.\n\nSpectral Power - Metodo di Welch\nDovremmo usare il metodo di Welch per dividere il segnale nel dominio della frequenza in ‚Äúbin‚Äù e calcolare lo spettro di potenza per ogni bin. Questo metodo divide il segnale in segmenti sovrapposti, applica una funzione finestra a ogni segmento, calcola il periodogramma di ogni segmento usando DFT e ne fa la media per ottenere una stima pi√π uniforme dello spettro di potenza.\n# Function used by Edge Impulse instead of scipy.signal.welch().\ndef welch_max_hold(fx, sampling_freq, nfft, n_overlap):\n    n_overlap = int(n_overlap)\n    spec_powers = [0 for _ in range(nfft//2+1)]\n    ix = 0\n    while ix &lt;= len(fx):\n        # Slicing truncates if end_idx &gt; len, and rfft will auto-zero pad\n        fft_out = np.abs(np.fft.rfft(fx[ix:ix+nfft], nfft))\n        spec_powers = np.maximum(spec_powers, fft_out**2/nfft)\n        ix = ix + (nfft-n_overlap)\n    return np.fft.rfftfreq(nfft, 1/sampling_freq), spec_powers\nApplicazione della funzione di cui sopra a 3 segnali:\nfax,Pax = welch_max_hold(accX, fs, FFT_Lenght, 0)\nfay,Pay = welch_max_hold(accY, fs, FFT_Lenght, 0)\nfaz,Paz = welch_max_hold(accZ, fs, FFT_Lenght, 0)\nspecs = [Pax, Pay, Paz ]\nPossiamo tracciare lo spettro di potenza P(f):\nplt.plot(fax,Pax, label='accX')\nplt.plot(fay,Pay, label='accY')\nplt.plot(faz,Paz, label='accZ')\nplt.legend(loc='upper right')\nplt.xlabel('Frequency (Hz)')\n#plt.ylabel('PSD [V**2/Hz]')\nplt.ylabel('Power')\nplt.title('Power spectrum P(f) using Welch's method')\nplt.grid()\nplt.box(False)\nplt.show()\n\nOltre allo spettro di potenza, possiamo anche includere l‚Äôasimmetria e la curtosi delle feature nel dominio della frequenza (dovrebbero essere disponibili in una nuova versione):\nspec_skew = [skew(x, bias=False) for x in specs]\nspec_kurtosis = [kurtosis(x, bias=False) for x in specs]\nElenchiamo ora tutte le feature spettrali per asse e confrontarli con EI:\nprint(\"EI Processed Spectral features (accX): \")\nprint(features[3:N_feat_axis][0:])\nprint(\"\\nCalculated features:\")\nprint (round(spec_skew[0],4))\nprint (round(spec_kurtosis[0],4))\n[print(round(x, 4)) for x in Pax[1:]][0]\nEI Processed Spectral features (accX):\n2.398, 3.8924, 24.6841, 9.6303, 8.4867, 7.7793, 2.9963, 5.6242, 3.4198, 4.2735\nCalculated features:\n2.9069 8.5569 24.6844 9.6304 8.4865 7.7794 2.9964 5.6242 3.4198 4.2736\nprint(\"EI Processed Spectral features (accY): \")\nprint(features[16:26][0:]) #13: 3+N_feat_axis;  26 = 2x N_feat_axis\nprint(\"\\nCalculated features:\")\nprint (round(spec_skew[1],4))\nprint (round(spec_kurtosis[1],4))\n[print(round(x, 4)) for x in Pay[1:]][0]\nEI Processed Spectral features (accY):\n0.9426, -0.8039, 5.429, 0.999, 1.0315, 0.9459, 1.8117, 0.9088, 1.3302, 3.112\nCalculated features:\n1.1426 -0.3886 5.4289 0.999 1.0315 0.9458 1.8116 0.9088 1.3301 3.1121\nprint(\"EI Processed Spectral features (accZ): \")\nprint(features[29:][0:]) #29: 3+(2*N_feat_axis);\nprint(\"\\nCalculated features:\")\nprint (round(spec_skew[2],4))\nprint (round(spec_kurtosis[2],4))\n[print(round(x, 4)) for x in Paz[1:]][0]\nEI Processed Spectral features (accZ):\n0.3117, -1.3812, 0.0606, 0.057, 0.0567, 0.0976, 0.194, 0.2574, 0.2083, 0.166\nCalculated features:\n0.3781 -1.4874 0.0606 0.057 0.0567 0.0976 0.194 0.2574 0.2083 0.166",
    "crumbs": [
      "Lab Condivisi",
      "Blocco delle Feature Spettrali DSP"
    ]
  },
  {
    "objectID": "contents/labs/shared/dsp_spectral_features_block/dsp_spectral_features_block.it.html#dominio-tempo-frequenza",
    "href": "contents/labs/shared/dsp_spectral_features_block/dsp_spectral_features_block.it.html#dominio-tempo-frequenza",
    "title": "Blocco delle Feature Spettrali DSP",
    "section": "Dominio tempo-frequenza",
    "text": "Dominio tempo-frequenza\n\nWavelet\nWavelet √® una tecnica potente per analizzare segnali con feature transitorie o bruschi cambiamenti, come picchi o bordi, che sono difficili da interpretare con i metodi tradizionali basati su Fourier.\nLe trasformazioni wavelet funzionano scomponendo un segnale in diverse componenti di frequenza e analizzandole individualmente. La trasformazione si ottiene convolvendo il segnale con una funzione wavelet, una piccola forma d‚Äôonda centrata su un tempo e una frequenza specifici. Questo processo scompone efficacemente il segnale in diverse bande di frequenza, ciascuna delle quali pu√≤ essere analizzata separatamente.\nUno dei vantaggi fondamentali delle trasformazioni wavelet √® che consentono l‚Äôanalisi tempo-frequenza, il che significa che possono rivelare il contenuto di frequenza di un segnale mentre cambia nel tempo. Ci√≤ le rende particolarmente utili per analizzare segnali non stazionari, che variano nel tempo.\nLe wavelet hanno molte applicazioni pratiche, tra cui la compressione di segnali e immagini, il denoising [rimozione del rumore], l‚Äôestrazione di feature e l‚Äôelaborazione di immagini.\nSelezioniamo Wavelet sul blocco Spectral Features nello stesso progetto:\n\nType: Wavelet\nWavelet Decomposition Level: 1\nWavelet: bior1.3\n\n\nLa Funzione Wavelet\nwavelet_name='bior1.3'\nnum_layer = 1\n\nwavelet = pywt.Wavelet(wavelet_name)\n[phi_d,psi_d,phi_r,psi_r,x] = wavelet.wavefun(level=5)\nplt.plot(x, psi_d, color='red')\nplt.title('Wavelet Function')\nplt.ylabel('Value')\nplt.xlabel('Time')\nplt.grid()\nplt.box(False)\nplt.show()\n\nCome abbiamo fatto prima, copiamo e incolliamo le Processed Features:\n\nfeatures = [3.6251, 0.0615, 0.0615, -7.3517, -2.7641, 2.8462, 5.0924, ...]\nN_feat = len(features)\nN_feat_axis = int(N_feat/n_sensors)\nEdge Impulse calcola la Discrete Wavelet Transform (DWT) per ciascuno dei livelli di Wavelet Decomposition selezionati. Dopodich√©, le feature verranno estratte.\nNel caso di Wavelet, le feature estratte sono valori statistici di base, valori di incrocio ed entropia. Ci sono, in totale, 14 feature per layer come di seguito:\n\n[11] Feature Statistiche: n5, n25, n75, n95, media, mediana, deviazione standard (std), varianza (var) radice quadrata media (rms), curtosi e asimmetria (skew).\n[2] Feature di attraversamento: Il tasso di attraversamento dello zero (zcross) e il tasso di attraversamento medio (mcross) sono rispettivamente i tempi in cui il segnale attraversa la linea di base (y = 0) e il livello medio (y = u) per unit√† di tempo\n[1] Feature di Complessit√†: L‚Äôentropia √® una misura caratteristica della complessit√† del segnale\n\nTutti i 14 valori sopra indicati vengono calcolati per ogni Layer (incluso L0, il segnale originale)\n\nIl numero totale di feature varia a seconda di come si imposta il filtro e del numero di layer. Ad esempio, con il filtro [None] e Level[1], il numero di feature per asse sar√† 14 x 2 (L0 e L1) = 28. Per i tre assi, avremo un totale di 84 feature.\n\n\n\nAnalisi Wavelet\nL‚Äôanalisi wavelet scompone il segnale (accX, accY, e accZ) in diverse componenti di frequenza utilizzando un set di filtri, che separano queste componenti in componenti a bassa frequenza (parti del segnale che variano lentamente e contengono pattern a lungo termine), come accX_l1, accY_l1, accZ_l1 e componenti ad alta frequenza (parti del segnale che variano rapidamente e contengono modelli a breve termine), come accX_d1, accY_d1, accZ_d1, consentendo l‚Äôestrazione di feature per ulteriori analisi o classificazioni.\nVerranno utilizzati solo le componenti a bassa frequenza (coefficienti di approssimazione o cA). In questo esempio, assumiamo un solo livello (Single-level Discrete Wavelet Transform), in cui la funzione restituir√† una tupla. Con una decomposizione multilivello, la ‚ÄúMultilevel 1D Discrete Wavelet Transform‚Äù, il risultato sar√† un elenco (per i dettagli, vedere: Discrete Wavelet Transform (DWT) )\n(accX_l1, accX_d1) = pywt.dwt(accX, wavelet_name)\n(accY_l1, accY_d1) = pywt.dwt(accY, wavelet_name)\n(accZ_l1, accZ_d1) = pywt.dwt(accZ, wavelet_name)\nsensors_l1 = [accX_l1, accY_l1, accZ_l1]\n\n# Plot power spectrum versus frequency\nplt.plot(accX_l1, label='accX')\nplt.plot(accY_l1, label='accY')\nplt.plot(accZ_l1, label='accZ')\nplt.legend(loc='lower right')\nplt.xlabel('Time')\nplt.ylabel('Value')\nplt.title('Wavelet Approximation')\nplt.grid()\nplt.box(False)\nplt.show()\n\n\n\nEstrazione delle Feature\nCominciamo con le feature statistiche di base. Notare che applichiamo la funzione sia per i segnali originali che per i cAs risultanti dal DWT:\ndef calculate_statistics(signal):\n    n5 = np.percentile(signal, 5)\n    n25 = np.percentile(signal, 25)\n    n75 = np.percentile(signal, 75)\n    n95 = np.percentile(signal, 95)\n    median = np.percentile(signal, 50)\n    mean = np.mean(signal)\n    std = np.std(signal)\n    var = np.var(signal)\n    rms = np.sqrt(np.mean(np.square(signal)))\n    return [n5, n25, n75, n95, median, mean, std, var, rms]\n \nstat_feat_l0 = [calculate_statistics(x) for x in sensors]\nstat_feat_l1 = [calculate_statistics(x) for x in sensors_l1]\nAsimmetria e Curtosi:\nskew_l0 = [skew(x, bias=False) for x in sensors]\nskew_l1 = [skew(x, bias=False) for x in sensors_l1]\nkurtosis_l0 = [kurtosis(x, bias=False) for x in sensors]\nkurtosis_l1 = [kurtosis(x, bias=False) for x in sensors_l1]\nZero crossing (zcross) √® il numero di volte in cui il coefficiente wavelet attraversa l‚Äôasse dello zero. Pu√≤ essere utilizzato per misurare il contenuto di frequenza del segnale poich√© i segnali ad alta frequenza tendono ad avere pi√π attraversamenti per lo zero rispetto ai segnali a bassa frequenza.\nMean crossing (mcross), d‚Äôaltra parte, √® il numero di volte in cui il coefficiente wavelet attraversa la media del segnale. Pu√≤ essere utilizzato per misurare l‚Äôampiezza poich√© i segnali ad alta ampiezza tendono ad avere pi√π attraversamenti medi rispetto ai segnali a bassa ampiezza.\ndef getZeroCrossingRate(arr):\n    my_array = np.array(arr)\n    zcross = float(\"{0:.2f}\".format((((my_array[:-1] * my_array[1:]) &lt; 0).su    m())/len(arr)))\n    return zcross\n\ndef getMeanCrossingRate(arr):\n    mcross = getZeroCrossingRate(np.array(arr) - np.mean(arr))\n    return mcross\n\ndef calculate_crossings(list):\n    zcross=[]\n    mcross=[]\n    for i in range(len(list)):\n        zcross_i = getZeroCrossingRate(list[i])\n        zcross.append(zcross_i)\n        mcross_i = getMeanCrossingRate(list[i])\n        mcross.append(mcross_i)\n    return zcross, mcross\n\ncross_l0 = calculate_crossings(sensors)\ncross_l1 = calculate_crossings(sensors_l1)\nNell‚Äôanalisi wavelet, l‚Äôentropia si riferisce al grado di disordine o casualit√† nella distribuzione dei coefficienti wavelet. Qui, abbiamo utilizzato l‚Äôentropia di Shannon, che misura l‚Äôincertezza o la casualit√† di un segnale. Viene calcolata come la somma negativa delle probabilit√† dei diversi possibili risultati del segnale moltiplicata per il loro logaritmo in base 2. Nel contesto dell‚Äôanalisi wavelet, l‚Äôentropia di Shannon pu√≤ essere utilizzata per misurare la complessit√† del segnale, con valori pi√π alti che indicano una maggiore complessit√†.\ndef calculate_entropy(signal, base=None):\n    value, counts = np.unique(signal, return_counts=True)\n    return entropy(counts, base=base)\n\nentropy_l0 = [calculate_entropy(x) for x in sensors]\nentropy_l1 = [calculate_entropy(x) for x in sensors_l1]\nElenchiamo ora tutte le feature wavelet e creiamo un elenco per layer.\nL1_features_names = [\"L1-n5\", \"L1-n25\", \"L1-n75\", \"L1-n95\", \"L1-median\", \"L1-mean\", \"L1-std\", \"L1-var\", \"L1-rms\", \"L1-skew\", \"L1-Kurtosis\", \"L1-zcross\", \"L1-mcross\", \"L1-entropy\"]\n\nL0_features_names = [\"L0-n5\", \"L0-n25\", \"L0-n75\", \"L0-n95\", \"L0-median\", \"L0-mean\", \"L0-std\", \"L0-var\", \"L0-rms\", \"L0-skew\", \"L0-Kurtosis\", \"L0-zcross\", \"L0-mcross\", \"L0-entropy\"]\n\nall_feat_l0 = []\nfor i in range(len(axis)):\n    feat_l0 = stat_feat_l0[i]+[skew_l0[i]]+[kurtosis_l0[i]]+[cross_l0[0][i]]+[cross_l0[1][i]]+[entropy_l0[i]]\n    [print(axis[i]+' '+x+'= ', round(y, 4)) for x,y in zip(L0_features_names, feat_l0)][0]\n    all_feat_l0.append(feat_l0)\nall_feat_l0 = [item for sublist in all_feat_l0 for item in sublist]\nprint(f\"\\nAll L0 Features = {len(all_feat_l0)}\")\n\nall_feat_l1 = []\nfor i in range(len(axis)):\nfeat_l1 = stat_feat_l1[i]+[skew_l1[i]]+[kurtosis_l1[i]]+[cross_l1[0][i]]+[cross_l1[1][i]]+[entropy_l1[i]]\n[print(axis[i]+' '+x+'= ', round(y, 4)) for x,y in zip(L1_features_names, feat_l1)][0]\nall_feat_l1.append(feat_l1)\nall_feat_l1 = [item for sublist in all_feat_l1 for item in sublist]\nprint(f\"\\nAll L1 Features = {len(all_feat_l1)}\")",
    "crumbs": [
      "Lab Condivisi",
      "Blocco delle Feature Spettrali DSP"
    ]
  },
  {
    "objectID": "contents/labs/shared/dsp_spectral_features_block/dsp_spectral_features_block.it.html#conclusione",
    "href": "contents/labs/shared/dsp_spectral_features_block/dsp_spectral_features_block.it.html#conclusione",
    "title": "Blocco delle Feature Spettrali DSP",
    "section": "Conclusione",
    "text": "Conclusione\nEdge Impulse Studio √® una potente piattaforma online in grado di gestire per noi l‚Äôattivit√† di pre-elaborazione. Tuttavia, data la nostra prospettiva ingegneristica, vogliamo capire cosa sta succedendo ‚Äúsotto il cofano‚Äù. Questa conoscenza ci aiuter√† a trovare le migliori opzioni e gli iperparametri per ottimizzare i nostri progetti.\nDaniel Situnayake ha scritto nel suo blog: ‚ÄúI dati grezzi dei sensori sono altamente dimensionali e rumorosi. Gli algoritmi di elaborazione del segnale digitale ci aiutano a separare il segnale dal rumore. Il DSP √® una parte essenziale dell‚Äôingegneria embedded e molti processori edge hanno un‚Äôaccelerazione integrata per il DSP. Come ingegnere ML, imparare il DSP di base d√† dei superpoteri per gestire dati di serie temporali ad alta frequenza nei propri modelli‚Äù. Consiglio di leggere l‚Äôeccellente post di Dan nella sua interezza: nn to cpp: What you need to know about porting deep learning models to the edge [Tutto ci√≤ che devi sapere sul porting dei modelli di deep learning verso l‚Äôedge].",
    "crumbs": [
      "Lab Condivisi",
      "Blocco delle Feature Spettrali DSP"
    ]
  },
  {
    "objectID": "references.it.html",
    "href": "references.it.html",
    "title": "Riferimenti",
    "section": "",
    "text": "Abadi, Martin, Andy Chu, Ian Goodfellow, H. Brendan McMahan, Ilya\nMironov, Kunal Talwar, and Li Zhang. 2016. ‚ÄúDeep Learning with\nDifferential Privacy.‚Äù In Proceedings of the 2016 ACM SIGSAC\nConference on Computer and Communications Security, 308‚Äì18. CCS\n‚Äô16. New York, NY, USA: ACM. https://doi.org/10.1145/2976749.2978318.\n\n\nAbdelkader, Ahmed, Michael J. Curry, Liam Fowl, Tom Goldstein, Avi\nSchwarzschild, Manli Shu, Christoph Studer, and Chen Zhu. 2020.\n‚ÄúHeadless Horseman: Adversarial Attacks on Transfer\nLearning Models.‚Äù In ICASSP 2020 - 2020 IEEE International\nConference on Acoustics, Speech and Signal Processing (ICASSP),\n3087‚Äì91. IEEE. https://doi.org/10.1109/icassp40776.2020.9053181.\n\n\nAddepalli, Sravanti, B. S. Vivek, Arya Baburaj, Gaurang Sriramanan, and\nR. Venkatesh Babu. 2020. ‚ÄúTowards Achieving Adversarial Robustness\nby Enforcing Feature Consistency Across Bit Planes.‚Äù In 2020\nIEEE/CVF Conference on Computer Vision and Pattern Recognition\n(CVPR), 1020‚Äì29. IEEE. https://doi.org/10.1109/cvpr42600.2020.00110.\n\n\nAdolf, Robert, Saketh Rama, Brandon Reagen, Gu-yeon Wei, and David\nBrooks. 2016. ‚ÄúFathom: Reference Workloads for Modern\nDeep Learning Methods.‚Äù In 2016 IEEE International Symposium\non Workload Characterization (IISWC), 1‚Äì10. IEEE; IEEE. https://doi.org/10.1109/iiswc.2016.7581275.\n\n\nAgarwal, Alekh, Alina Beygelzimer, Miroslav Dudƒ±ÃÅk, John Langford, and\nHanna M. Wallach. 2018. ‚ÄúA Reductions Approach to Fair\nClassification.‚Äù In Proceedings of the 35th International\nConference on Machine Learning, ICML 2018, Stockholmsm√§ssan, Stockholm,\nSweden, July 10-15, 2018, edited by Jennifer G. Dy and Andreas\nKrause, 80:60‚Äì69. Proceedings of Machine Learning Research. PMLR. http://proceedings.mlr.press/v80/agarwal18a.html.\n\n\nAgnesina, Anthony, Puranjay Rajvanshi, Tian Yang, Geraldo Pradipta,\nAustin Jiao, Ben Keller, Brucek Khailany, and Haoxing Ren. 2023.\n‚ÄúAutoDMP: Automated DREAMPlace-Based Macro\nPlacement.‚Äù In Proceedings of the 2023 International\nSymposium on Physical Design, 149‚Äì57. ACM. https://doi.org/10.1145/3569052.3578923.\n\n\nAgrawal, Dakshi, Selcuk Baktir, Deniz Karakoyunlu, Pankaj Rohatgi, and\nBerk Sunar. 2007. ‚ÄúTrojan Detection Using IC\nFingerprinting.‚Äù In 2007 IEEE Symposium on Security and\nPrivacy (SP ‚Äô07), 296‚Äì310. Springer; IEEE. https://doi.org/10.1109/sp.2007.36.\n\n\nAhmadilivani, Mohammad Hasan, Mahdi Taheri, Jaan Raik, Masoud\nDaneshtalab, and Maksim Jenihhin. 2024. ‚ÄúA Systematic Literature\nReview on Hardware Reliability Assessment Methods for Deep Neural\nNetworks.‚Äù ACM Comput. Surv. 56 (6): 1‚Äì39. https://doi.org/10.1145/3638242.\n\n\nAledhari, Mohammed, Rehma Razzak, Reza M. Parizi, and Fahad Saeed. 2020.\n‚ÄúFederated Learning: A Survey on Enabling\nTechnologies, Protocols, and Applications.‚Äù #IEEE_O_ACC#\n8: 140699‚Äì725. https://doi.org/10.1109/access.2020.3013541.\n\n\nAlghamdi, Wael, Hsiang Hsu, Haewon Jeong, Hao Wang, Peter Michalak,\nShahab Asoodeh, and Flavio Calmon. 2022. ‚ÄúBeyond Adult and\nCOMPAS: Fair Multi-Class Prediction via\nInformation Projection.‚Äù Adv. Neur. In. 35: 38747‚Äì60.\n\n\nAltayeb, Moez, Marco Zennaro, and Marcelo Rovai. 2022.\n‚ÄúClassifying Mosquito Wingbeat Sound Using\nTinyML.‚Äù In Proceedings of the 2022 ACM\nConference on Information Technology for Social Good, 132‚Äì37. ACM.\nhttps://doi.org/10.1145/3524458.3547258.\n\n\nAmiel, Frederic, Christophe Clavier, and Michael Tunstall. 2006.\n‚ÄúFault Analysis of DPA-Resistant Algorithms.‚Äù In Fault\nDiagnosis and Tolerance in Cryptography, 223‚Äì36. Springer; Springer\nBerlin Heidelberg. https://doi.org/10.1007/11889700\\_20.\n\n\nAnsel, Jason, Edward Yang, Horace He, Natalia Gimelshein, Animesh Jain,\nMichael Voznesensky, Bin Bao, et al. 2024. ‚ÄúPyTorch\n2: Faster Machine Learning Through Dynamic Python Bytecode\nTransformation and Graph Compilation.‚Äù In Proceedings of the\n29th ACM International Conference on Architectural Support for\nProgramming Languages and Operating Systems, Volume 2, edited by\nHanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence\nd‚ÄôAlch√©-Buc, Emily B. Fox, and Roman Garnett, 8024‚Äì35. ACM. https://doi.org/10.1145/3620665.3640366.\n\n\nAnthony, Lasse F. Wolff, Benjamin Kanding, and Raghavendra Selvan. 2020.\nICML Workshop on Challenges in Deploying and monitoring Machine Learning\nSystems.\n\n\nAntol, Stanislaw, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv\nBatra, C. Lawrence Zitnick, and Devi Parikh. 2015. ‚ÄúVQA: Visual\nQuestion Answering.‚Äù In 2015 IEEE International Conference on\nComputer Vision (ICCV), 2425‚Äì33. IEEE. https://doi.org/10.1109/iccv.2015.279.\n\n\nAntonakakis, Manos, Tim April, Michael Bailey, Matt Bernhard, Elie\nBursztein, Jaime Cochran, Zakir Durumeric, et al. 2017.\n‚ÄúUnderstanding the Mirai Botnet.‚Äù In 26th USENIX\nSecurity Symposium (USENIX Security 17), 1093‚Äì1110.\n\n\nArdila, Rosana, Megan Branson, Kelly Davis, Michael Kohler, Josh Meyer,\nMichael Henretty, Reuben Morais, Lindsay Saunders, Francis Tyers, and\nGregor Weber. 2020. ‚ÄúCommon Voice: A\nMassively-Multilingual Speech Corpus.‚Äù In Proceedings of the\nTwelfth Language Resources and Evaluation Conference, 4218‚Äì22.\nMarseille, France: European Language Resources Association. https://aclanthology.org/2020.lrec-1.520.\n\n\nArifeen, Tooba, Abdus Sami Hassan, and Jeong-A Lee. 2020.\n‚ÄúApproximate Triple Modular Redundancy: A\nSurvey.‚Äù #IEEE_O_ACC# 8: 139851‚Äì67. https://doi.org/10.1109/access.2020.3012673.\n\n\nAsonov, D., and R. Agrawal. n.d. ‚ÄúKeyboard Acoustic\nEmanations.‚Äù In IEEE Symposium on Security and Privacy, 2004.\nProceedings. 2004, 3‚Äì11. IEEE; IEEE. https://doi.org/10.1109/secpri.2004.1301311.\n\n\nAteniese, Giuseppe, Luigi V. Mancini, Angelo Spognardi, Antonio Villani,\nDomenico Vitali, and Giovanni Felici. 2015. ‚ÄúHacking Smart\nMachines with Smarter Ones: How to Extract Meaningful Data from Machine\nLearning Classifiers.‚Äù International Journal of Security and\nNetworks 10 (3): 137. https://doi.org/10.1504/ijsn.2015.071829.\n\n\nAttia, Zachi I., Alan Sugrue, Samuel J. Asirvatham, Michael J. Ackerman,\nSuraj Kapa, Paul A. Friedman, and Peter A. Noseworthy. 2018.\n‚ÄúNoninvasive Assessment of Dofetilide Plasma Concentration Using a\nDeep Learning (Neural Network) Analysis of the Surface\nElectrocardiogram: A Proof of Concept Study.‚Äù PLOS ONE\n13 (8): e0201059. https://doi.org/10.1371/journal.pone.0201059.\n\n\nAygun, Sercan, Ece Olcay Gunes, and Christophe De Vleeschouwer. 2021.\n‚ÄúEfficient and Robust Bitstream Processing in Binarised Neural\nNetworks.‚Äù Electron. Lett. 57 (5): 219‚Äì22. https://doi.org/10.1049/ell2.12045.\n\n\nBai, Tao, Jinqi Luo, Jun Zhao, Bihan Wen, and Qian Wang. 2021.\n‚ÄúRecent Advances in Adversarial Training for Adversarial\nRobustness.‚Äù arXiv Preprint arXiv:2102.01356.\n\n\nBains, Sunny. 2020. ‚ÄúThe Business of Building Brains.‚Äù\nNature Electronics 3 (7): 348‚Äì51. https://doi.org/10.1038/s41928-020-0449-1.\n\n\nBamoumen, Hatim, Anas Temouden, Nabil Benamar, and Yousra Chtouki. 2022.\n‚ÄúHow TinyML Can Be Leveraged to Solve Environmental\nProblems: A Survey.‚Äù In 2022 International\nConference on Innovation and Intelligence for Informatics, Computing,\nand Technologies (3ICT), 338‚Äì43. IEEE; IEEE. https://doi.org/10.1109/3ict56508.2022.9990661.\n\n\nBanbury, Colby, Vijay Janapa Reddi, Peter Torelli, Jeremy Holleman, Nat\nJeffries, Csaba Kiraly, Pietro Montino, et al. 2021. ‚ÄúMLPerf Tiny\nBenchmark.‚Äù arXiv Preprint arXiv:2106.07597, June. http://arxiv.org/abs/2106.07597v4.\n\n\nBank, Dor, Noam Koenigstein, and Raja Giryes. 2023.\n‚ÄúAutoencoders.‚Äù Machine Learning for Data Science\nHandbook: Data Mining and Knowledge Discovery Handbook, 353‚Äì74.\n\n\nBannon, Pete, Ganesh Venkataramanan, Debjit Das Sarma, and Emil Talpes.\n2019. ‚ÄúComputer and Redundancy Solution for the Full Self-Driving\nComputer.‚Äù In 2019 IEEE Hot Chips 31 Symposium (HCS),\n1‚Äì22. IEEE Computer Society; IEEE. https://doi.org/10.1109/hotchips.2019.8875645.\n\n\nBarenghi, Alessandro, Guido M. Bertoni, Luca Breveglieri, Mauro\nPellicioli, and Gerardo Pelosi. 2010. ‚ÄúLow Voltage Fault Attacks\nto AES.‚Äù In 2010 IEEE International Symposium on\nHardware-Oriented Security and Trust (HOST), 7‚Äì12. IEEE; IEEE. https://doi.org/10.1109/hst.2010.5513121.\n\n\nBarroso, Luiz Andr√©, Urs H√∂lzle, and Parthasarathy Ranganathan. 2019.\nThe Datacenter as a Computer: Designing Warehouse-Scale\nMachines. Springer International Publishing. https://doi.org/10.1007/978-3-031-01761-2.\n\n\nBau, David, Bolei Zhou, Aditya Khosla, Aude Oliva, and Antonio Torralba.\n2017. ‚ÄúNetwork Dissection: Quantifying\nInterpretability of Deep Visual Representations.‚Äù In 2017\nIEEE Conference on Computer Vision and Pattern Recognition (CVPR),\n3319‚Äì27. IEEE. https://doi.org/10.1109/cvpr.2017.354.\n\n\nBeaton, Albert E., and John W. Tukey. 1974. ‚ÄúThe Fitting of Power\nSeries, Meaning Polynomials, Illustrated on Band-Spectroscopic\nData.‚Äù Technometrics 16 (2): 147. https://doi.org/10.2307/1267936.\n\n\nBeck, Nathaniel, and Simon Jackman. 1998. ‚ÄúBeyond Linearity by\nDefault: Generalized Additive Models.‚Äù Am. J.\nPolit. Sci. 42 (2): 596. https://doi.org/10.2307/2991772.\n\n\nBender, Emily M., and Batya Friedman. 2018. ‚ÄúData Statements for\nNatural Language Processing: Toward Mitigating System Bias\nand Enabling Better Science.‚Äù Transactions of the Association\nfor Computational Linguistics 6 (December): 587‚Äì604. https://doi.org/10.1162/tacl_a_00041.\n\n\nBerger, Vance W, and YanYan Zhou. 2014.\n‚ÄúKolmogorovsmirnov Test:\nOverview.‚Äù Wiley Statsref: Statistics Reference\nOnline.\n\n\nBeyer, Lucas, Olivier J. H√©naff, Alexander Kolesnikov, Xiaohua Zhai, and\nA√§ron van den Oord. 2020. ‚ÄúAre We Done with ImageNet?‚Äù\nArXiv Preprint abs/2006.07159 (June). http://arxiv.org/abs/2006.07159v1.\n\n\nBhagoji, Arjun Nitin, Warren He, Bo Li, and Dawn Song. 2018.\n‚ÄúPractical Black-Box Attacks on Deep Neural Networks Using\nEfficient Query Mechanisms.‚Äù In Proceedings of the European\nConference on Computer Vision (ECCV), 154‚Äì69.\n\n\nBhardwaj, Kshitij, Marton Havasi, Yuan Yao, David M. Brooks, Jos√© Miguel\nHern√°ndez-Lobato, and Gu-Yeon Wei. 2020. ‚ÄúA Comprehensive\nMethodology to Determine Optimal Coherence Interfaces for\nMany-Accelerator SoCs.‚Äù In Proceedings of the\nACM/IEEE International Symposium on Low Power Electronics and\nDesign, 145‚Äì50. ACM. https://doi.org/10.1145/3370748.3406564.\n\n\nBianco, Simone, Remi Cadene, Luigi Celona, and Paolo Napoletano. 2018.\n‚ÄúBenchmark Analysis of Representative Deep Neural Network\nArchitectures.‚Äù IEEE Access 6: 64270‚Äì77. https://doi.org/10.1109/access.2018.2877890.\n\n\nBiega, Asia J., Peter Potash, Hal Daum√©, Fernando Diaz, and Mich√®le\nFinck. 2020. ‚ÄúOperationalizing the Legal Principle of Data\nMinimization for Personalization.‚Äù In Proceedings of the 43rd\nInternational ACM SIGIR Conference on Research and Development in\nInformation Retrieval, edited by Jimmy Huang, Yi Chang, Xueqi\nCheng, Jaap Kamps, Vanessa Murdock, Ji-Rong Wen, and Yiqun Liu, 399‚Äì408.\nACM. https://doi.org/10.1145/3397271.3401034.\n\n\nBiggio, Battista, Blaine Nelson, and Pavel Laskov. 2012.\n‚ÄúPoisoning Attacks Against Support Vector Machines.‚Äù In\nProceedings of the 29th International Conference on Machine\nLearning, ICML 2012, Edinburgh, Scotland, UK, June 26 - July 1,\n2012. icml.cc / Omnipress. http://icml.cc/2012/papers/880.pdf.\n\n\nBiggs, John, James Myers, Jedrzej Kufel, Emre Ozer, Simon Craske, Antony\nSou, Catherine Ramsdale, Ken Williamson, Richard Price, and Scott White.\n2021. ‚ÄúA Natively Flexible 32-Bit Arm Microprocessor.‚Äù\nNature 595 (7868): 532‚Äì36. https://doi.org/10.1038/s41586-021-03625-w.\n\n\nBinkert, Nathan, Bradford Beckmann, Gabriel Black, Steven K. Reinhardt,\nAli Saidi, Arkaprava Basu, Joel Hestness, et al. 2011. ‚ÄúThe Gem5\nSimulator.‚Äù ACM SIGARCH Computer Architecture News 39\n(2): 1‚Äì7. https://doi.org/10.1145/2024716.2024718.\n\n\nBohr, Adam, and Kaveh Memarzadeh. 2020. ‚ÄúThe Rise of Artificial\nIntelligence in Healthcare Applications.‚Äù In Artificial\nIntelligence in Healthcare, 25‚Äì60. Elsevier. https://doi.org/10.1016/b978-0-12-818438-7.00002-2.\n\n\nBolchini, Cristiana, Luca Cassano, Antonio Miele, and Alessandro Toschi.\n2023. ‚ÄúFast and Accurate Error Simulation for CNNs\nAgainst Soft Errors.‚Äù IEEE Trans. Comput. 72 (4):\n984‚Äì97. https://doi.org/10.1109/tc.2022.3184274.\n\n\nBondi, Elizabeth, Ashish Kapoor, Debadeepta Dey, James Piavis, Shital\nShah, Robert Hannaford, Arvind Iyer, Lucas Joppa, and Milind Tambe.\n2018. ‚ÄúNear Real-Time Detection of Poachers from Drones in\nAirSim.‚Äù In Proceedings of the Twenty-Seventh\nInternational Joint Conference on Artificial Intelligence, edited\nby J√©r√¥me Lang, 5814‚Äì16. International Joint Conferences on Artificial\nIntelligence Organization. https://doi.org/10.24963/ijcai.2018/847.\n\n\nBourtoule, Lucas, Varun Chandrasekaran, Christopher A. Choquette-Choo,\nHengrui Jia, Adelin Travers, Baiwu Zhang, David Lie, and Nicolas\nPapernot. 2021. ‚ÄúMachine Unlearning.‚Äù In 2021 IEEE\nSymposium on Security and Privacy (SP), 141‚Äì59. IEEE; IEEE. https://doi.org/10.1109/sp40001.2021.00019.\n\n\nBreier, Jakub, Xiaolu Hou, Dirmanto Jap, Lei Ma, Shivam Bhasin, and Yang\nLiu. 2018. ‚ÄúDeepLaser: Practical Fault Attack on Deep Neural\nNetworks.‚Äù ArXiv Preprint abs/1806.05859 (June). http://arxiv.org/abs/1806.05859v2.\n\n\nBrown, Tom B., Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,\nPrafulla Dhariwal, Arvind Neelakantan, et al. 2020. ‚ÄúLanguage\nModels Are Few-Shot Learners.‚Äù In Advances in Neural\nInformation Processing Systems 33: Annual Conference on Neural\nInformation Processing Systems 2020, NeurIPS 2020, December 6-12, 2020,\nVirtual, edited by Hugo Larochelle, Marc‚ÄôAurelio Ranzato, Raia\nHadsell, Maria-Florina Balcan, and Hsuan-Tien Lin. https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html.\n\n\nBuolamwini, Joy, and Timnit Gebru. 2018. ‚ÄúGender Shades:\nIntersectional Accuracy Disparities in Commercial Gender\nClassification.‚Äù In Conference on Fairness, Accountability\nand Transparency, 77‚Äì91. PMLR.\n\n\nBurnet, David, and Richard Thomas. 1989. ‚ÄúSpycatcher: The\nCommodification of Truth.‚Äù Journal of Law and Society 16\n(2): 210. https://doi.org/10.2307/1410360.\n\n\nBurr, Geoffrey W., Matthew J. BrightSky, Abu Sebastian, Huai-Yu Cheng,\nJau-Yi Wu, Sangbum Kim, Norma E. Sosa, et al. 2016. ‚ÄúRecent\nProgress in Phase-Change?Pub _Newline ?Memory\nTechnology.‚Äù IEEE Journal on Emerging and Selected Topics in\nCircuits and Systems 6 (2): 146‚Äì62. https://doi.org/10.1109/jetcas.2016.2547718.\n\n\nBushnell, Michael L, and Vishwani D Agrawal. 2002. ‚ÄúBuilt-in\nSelf-Test.‚Äù Essentials of Electronic Testing for Digital,\nMemory and Mixed-Signal VLSI Circuits, 489‚Äì548.\n\n\nBuyya, Rajkumar, Anton Beloglazov, and Jemal Abawajy. 2010.\n‚ÄúEnergy-Efficient Management of Data Center Resources for Cloud\nComputing: A Vision, Architectural Elements, and Open\nChallenges.‚Äù https://arxiv.org/abs/1006.0308.\n\n\nCai, Carrie J., Emily Reif, Narayan Hegde, Jason Hipp, Been Kim, Daniel\nSmilkov, Martin Wattenberg, et al. 2019. ‚ÄúHuman-Centered Tools for\nCoping with Imperfect Algorithms During Medical Decision-Making.‚Äù\nIn Proceedings of the 2019 CHI Conference on Human Factors in\nComputing Systems, edited by Jennifer G. Dy and Andreas Krause,\n80:2673‚Äì82. Proceedings of Machine Learning Research. ACM. https://doi.org/10.1145/3290605.3300234.\n\n\nCai, Han, Chuang Gan, Ligeng Zhu, and Song Han 0003. 2020.\n‚ÄúTinyTL: Reduce Memory, Not Parameters for Efficient on-Device\nLearning.‚Äù In Advances in Neural Information Processing\nSystems 33: Annual Conference on Neural Information Processing Systems\n2020, NeurIPS 2020, December 6-12, 2020, Virtual, edited by Hugo\nLarochelle, Marc‚ÄôAurelio Ranzato, Raia Hadsell, Maria-Florina Balcan,\nand Hsuan-Tien Lin. https://proceedings.neurips.cc/paper/2020/hash/81f7acabd411274fcf65ce2070ed568a-Abstract.html.\n\n\nCai, Han, Ligeng Zhu, and Song Han. 2019.\n‚ÄúProxylessNAS: Direct Neural\nArchitecture Search on Target Task and Hardware.‚Äù In 7th\nInternational Conference on Learning Representations, ICLR 2019, New\nOrleans, LA, USA, May 6-9, 2019. OpenReview.net. https://openreview.net/forum?id=HylVB3AqYm.\n\n\nCalvo, Rafael A, Dorian Peters, Karina Vold, and Richard M Ryan. 2020.\n‚ÄúSupporting Human Autonomy in AI Systems:\nA Framework for Ethical Enquiry.‚Äù Ethics of\nDigital Well-Being: A Multidisciplinary Approach, 31‚Äì54.\n\n\nCarlini, Nicholas, Pratyush Mishra, Tavish Vaidya, Yuankai Zhang, Micah\nSherr, Clay Shields, David Wagner, and Wenchao Zhou. 2016. ‚ÄúHidden\nVoice Commands.‚Äù In 25th USENIX Security Symposium (USENIX\nSecurity 16), 513‚Äì30.\n\n\nCarlini, Nicolas, Jamie Hayes, Milad Nasr, Matthew Jagielski, Vikash\nSehwag, Florian Tramer, Borja Balle, Daphne Ippolito, and Eric Wallace.\n2023. ‚ÄúExtracting Training Data from Diffusion Models.‚Äù In\n32nd USENIX Security Symposium (USENIX Security 23), 5253‚Äì70.\n\n\nCarta, Salvatore, Alessandro Sebastian Podda, Diego Reforgiato Recupero,\nand Roberto Saia. 2020. ‚ÄúA Local Feature Engineering Strategy to\nImprove Network Anomaly Detection.‚Äù Future Internet 12\n(10): 177. https://doi.org/10.3390/fi12100177.\n\n\nCavoukian, Ann. 2009. ‚ÄúPrivacy by Design.‚Äù Office of\nthe Information and Privacy Commissioner.\n\n\nCenci, Marcelo Pilotto, Tatiana Scarazzato, Daniel Dotto Munchen, Paula\nCristina Dartora, Hugo Marcelo Veit, Andrea Moura Bernardes, and Pablo\nR. Dias. 2021. ‚ÄúEco-Friendly\nElectronicsA Comprehensive Review.‚Äù\nAdv. Mater. Technol. 7 (2): 2001263. https://doi.org/10.1002/admt.202001263.\n\n\nChallenge, WEF Net-Zero. 2021. ‚ÄúThe Supply Chain\nOpportunity.‚Äù In World Economic Forum: Geneva,\nSwitzerland.\n\n\nChandola, Varun, Arindam Banerjee, and Vipin Kumar. 2009. ‚ÄúAnomaly\nDetection: A Survey.‚Äù ACM Comput. Surv. 41 (3): 1‚Äì58. https://doi.org/10.1145/1541880.1541882.\n\n\nChapelle, O., B. Scholkopf, and A. Zien Eds. 2009.\n‚ÄúSemi-Supervised Learning (Chapelle, O.\nEt Al., Eds.; 2006) [Book Reviews].‚Äù IEEE Trans.\nNeural Networks 20 (3): 542‚Äì42. https://doi.org/10.1109/tnn.2009.2015974.\n\n\nChen, Chaofan, Oscar Li, Daniel Tao, Alina Barnett, Cynthia Rudin, and\nJonathan Su. 2019. ‚ÄúThis Looks Like That: Deep\nLearning for Interpretable Image Recognition.‚Äù In Advances in\nNeural Information Processing Systems 32: Annual Conference on Neural\nInformation Processing Systems 2019, NeurIPS 2019, December 8-14, 2019,\nVancouver, BC, Canada, edited by Hanna M. Wallach, Hugo Larochelle,\nAlina Beygelzimer, Florence d‚ÄôAlch√©-Buc, Emily B. Fox, and Roman\nGarnett, 8928‚Äì39. https://proceedings.neurips.cc/paper/2019/hash/adf7ee2dcf142b0e11888e72b43fcb75-Abstract.html.\n\n\nChen, Emma, Shvetank Prakash, Vijay Janapa Reddi, David Kim, and Pranav\nRajpurkar. 2023. ‚ÄúA Framework for Integrating Artificial\nIntelligence for Clinical Care with Continuous Therapeutic\nMonitoring.‚Äù Nature Biomedical Engineering, November. https://doi.org/10.1038/s41551-023-01115-0.\n\n\nChen, H.-W. 2006. ‚ÄúGallium, Indium, and Arsenic Pollution of\nGroundwater from a Semiconductor Manufacturing Area of\nTaiwan.‚Äù B. Environ. Contam. Tox. 77 (2):\n289‚Äì96. https://doi.org/10.1007/s00128-006-1062-3.\n\n\nChen, Tianqi, Thierry Moreau, Ziheng Jiang, Lianmin Zheng, Eddie Yan,\nHaichen Shen, Meghan Cowan, et al. 2018. ‚ÄúTVM: An Automated\nEnd-to-End Optimizing Compiler for Deep Learning.‚Äù In 13th\nUSENIX Symposium on Operating Systems Design and Implementation (OSDI\n18), 578‚Äì94.\n\n\nChen, Tianqi, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. 2016.\n‚ÄúTraining Deep Nets with Sublinear Memory Cost.‚Äù ArXiv\nPreprint abs/1604.06174 (April). http://arxiv.org/abs/1604.06174v2.\n\n\nChen, Zhiyong, and Shugong Xu. 2023. ‚ÄúLearning\nDomain-Heterogeneous Speaker Recognition Systems with Personalized\nContinual Federated Learning.‚Äù EURASIP Journal on Audio,\nSpeech, and Music Processing 2023 (1): 33. https://doi.org/10.1186/s13636-023-00299-2.\n\n\nChen, Zitao, Guanpeng Li, Karthik Pattabiraman, and Nathan DeBardeleben.\n2019. ‚ÄúiBinFI/i: An Efficient Fault\nInjector for Safety-Critical Machine Learning Systems.‚Äù In\nProceedings of the International Conference for High Performance\nComputing, Networking, Storage and Analysis. SC ‚Äô19. New York, NY,\nUSA: ACM. https://doi.org/10.1145/3295500.3356177.\n\n\nChen, Zitao, Niranjhana Narayanan, Bo Fang, Guanpeng Li, Karthik\nPattabiraman, and Nathan DeBardeleben. 2020.\n‚ÄúTensorFI: A Flexible Fault Injection\nFramework for TensorFlow Applications.‚Äù In 2020\nIEEE 31st International Symposium on Software Reliability Engineering\n(ISSRE), 426‚Äì35. IEEE; IEEE. https://doi.org/10.1109/issre5003.2020.00047.\n\n\nCheng, Eric, Shahrzad Mirkhani, Lukasz G. Szafaryn, Chen-Yong Cher,\nHyungmin Cho, Kevin Skadron, Mircea R. Stan, et al. 2016. ‚ÄúClear:\nuC/u Ross u-l/u Ayer uE/u Xploration for uA/u Rchitecting uR/u Esilience\n- Combining Hardware and Software Techniques to Tolerate Soft Errors in\nProcessor Cores.‚Äù In Proceedings of the 53rd Annual Design\nAutomation Conference, 1‚Äì6. ACM. https://doi.org/10.1145/2897937.2897996.\n\n\nCheng, Yu, Duo Wang, Pan Zhou, and Tao Zhang. 2018. ‚ÄúModel\nCompression and Acceleration for Deep Neural Networks: The\nPrinciples, Progress, and Challenges.‚Äù IEEE Signal Process\nMag. 35 (1): 126‚Äì36. https://doi.org/10.1109/msp.2017.2765695.\n\n\nChi, Ping, Shuangchen Li, Cong Xu, Tao Zhang, Jishen Zhao, Yongpan Liu,\nYu Wang, and Yuan Xie. 2016. ‚ÄúPrime: A Novel Processing-in-Memory\nArchitecture for Neural Network Computation in ReRAM-Based Main\nMemory.‚Äù ACM SIGARCH Computer Architecture News 44 (3):\n27‚Äì39. https://doi.org/10.1145/3007787.3001140.\n\n\nChollet, Fran√ßois. 2018. ‚ÄúIntroduction to Keras.‚Äù March\n9th.\n\n\nChristiano, Paul F., Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg,\nand Dario Amodei. 2017. ‚ÄúDeep Reinforcement Learning from Human\nPreferences.‚Äù In Advances in Neural Information Processing\nSystems 30: Annual Conference on Neural Information Processing Systems\n2017, December 4-9, 2017, Long Beach, CA, USA, edited by Isabelle\nGuyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S.\nV. N. Vishwanathan, and Roman Garnett, 4299‚Äì4307. https://proceedings.neurips.cc/paper/2017/hash/d5e2c0adad503c91f91df240d0cd4e49-Abstract.html.\n\n\nChu, Grace, Okan Arikan, Gabriel Bender, Weijun Wang, Achille Brighton,\nPieter-Jan Kindermans, Hanxiao Liu, Berkin Akin, Suyog Gupta, and Andrew\nHoward. 2021. ‚ÄúDiscovering Multi-Hardware Mobile Models via\nArchitecture Search.‚Äù In 2021 IEEE/CVF Conference on Computer\nVision and Pattern Recognition Workshops (CVPRW), 3022‚Äì31. IEEE. https://doi.org/10.1109/cvprw53098.2021.00337.\n\n\nChua, L. 1971. ‚ÄúMemristor-the Missing Circuit Element.‚Äù\n#IEEE_J_CT# 18 (5): 507‚Äì19. https://doi.org/10.1109/tct.1971.1083337.\n\n\nChung, Jae-Won, Yile Gu, Insu Jang, Luoxi Meng, Nikhil Bansal, and\nMosharaf Chowdhury. 2023. ‚ÄúPerseus: Removing Energy\nBloat from Large Model Training.‚Äù ArXiv Preprint\nabs/2312.06902. https://arxiv.org/abs/2312.06902.\n\n\nCohen, Maxime C., Ruben Lobel, and Georgia Perakis. 2016. ‚ÄúThe\nImpact of Demand Uncertainty on Consumer Subsidies for Green Technology\nAdoption.‚Äù Manage. Sci. 62 (5): 1235‚Äì58. https://doi.org/10.1287/mnsc.2015.2173.\n\n\nColeman, Cody, Edward Chou, Julian Katz-Samuels, Sean Culatana, Peter\nBailis, Alexander C. Berg, Robert Nowak, Roshan Sumbaly, Matei Zaharia,\nand I. Zeki Yalniz. 2022. ‚ÄúSimilarity Search for Efficient Active\nLearning and Search of Rare Concepts.‚Äù Proceedings of the\nAAAI Conference on Artificial Intelligence 36 (6): 6402‚Äì10. https://doi.org/10.1609/aaai.v36i6.20591.\n\n\nColeman, Cody, Daniel Kang, Deepak Narayanan, Luigi Nardi, Tian Zhao,\nJian Zhang, Peter Bailis, Kunle Olukotun, Chris R√©, and Matei Zaharia.\n2019. ‚ÄúAnalysis of DAWNBench, a Time-to-Accuracy Machine Learning\nPerformance Benchmark.‚Äù ACM SIGOPS Operating Systems\nReview 53 (1): 14‚Äì25. https://doi.org/10.1145/3352020.3352024.\n\n\nConstantinescu, Cristian. 2008. ‚ÄúIntermittent Faults and Effects\non Reliability of Integrated Circuits.‚Äù In 2008 Annual\nReliability and Maintainability Symposium, 370‚Äì74. IEEE; IEEE. https://doi.org/10.1109/rams.2008.4925824.\n\n\nCooper, Tom, Suzanne Fallender, Joyann Pafumi, Jon Dettling, Sebastien\nHumbert, and Lindsay Lessard. 2011. ‚ÄúA Semiconductor Company‚Äôs\nExamination of Its Water Footprint Approach.‚Äù In Proceedings\nof the 2011 IEEE International Symposium on Sustainable Systems and\nTechnology, 1‚Äì6. IEEE; IEEE. https://doi.org/10.1109/issst.2011.5936865.\n\n\nCope, Gord. 2009. ‚ÄúPure Water, Semiconductors and the\nRecession.‚Äù Global Water Intelligence 10 (10).\n\n\nCourbariaux, Matthieu, Itay Hubara, Daniel Soudry, Ran El-Yaniv, and\nYoshua Bengio. 2016. ‚ÄúBinarized Neural Networks:\nTraining Deep Neural Networks with Weights and Activations\nConstrained to+ 1 or-1.‚Äù arXiv Preprint\narXiv:1602.02830.\n\n\nCrankshaw, Daniel, Xin Wang, Guilio Zhou, Michael J Franklin, Joseph E\nGonzalez, and Ion Stoica. 2017. ‚ÄúClipper: A {Low-Latency} Online Prediction Serving System.‚Äù\nIn 14th USENIX Symposium on Networked Systems Design and\nImplementation (NSDI 17), 613‚Äì27.\n\n\nD‚Äôignazio, Catherine, and Lauren F Klein. 2023. Data Feminism.\nMIT press.\n\n\nDarvish Rouhani, Bita, Azalia Mirhoseini, and Farinaz Koushanfar. 2017.\n‚ÄúTinyDL: Just-in-Time Deep Learning Solution for Constrained\nEmbedded Systems.‚Äù In 2017 IEEE International Symposium on\nCircuits and Systems (ISCAS), 1‚Äì4. IEEE. https://doi.org/10.1109/iscas.2017.8050343.\n\n\nDavarzani, Samaneh, David Saucier, Purva Talegaonkar, Erin Parker, Alana\nTurner, Carver Middleton, Will Carroll, et al. 2023. ‚ÄúClosing the\nWearable Gap: Footankle\nKinematic Modeling via Deep Learning Models Based on a Smart Sock\nWearable.‚Äù Wearable Technologies 4. https://doi.org/10.1017/wtc.2023.3.\n\n\nDavid, Robert, Jared Duke, Advait Jain, Vijay Janapa Reddi, Nat\nJeffries, Jian Li, Nick Kreeger, et al. 2021. ‚ÄúTensorflow Lite\nMicro: Embedded Machine Learning for Tinyml Systems.‚Äù\nProceedings of Machine Learning and Systems 3: 800‚Äì811.\n\n\nDavies, Emma. 2011. ‚ÄúEndangered Elements: Critical\nThinking.‚Äù https://www.rsc.org/images/Endangered\\%20Elements\\%20-\\%20Critical\\%20Thinking\\_tcm18-196054.pdf.\n\n\nDavies, Mike, Narayan Srinivasa, Tsung-Han Lin, Gautham Chinya,\nYongqiang Cao, Sri Harsha Choday, Georgios Dimou, et al. 2018.\n‚ÄúLoihi: A Neuromorphic Manycore Processor with\non-Chip Learning.‚Äù IEEE Micro 38 (1): 82‚Äì99. https://doi.org/10.1109/mm.2018.112130359.\n\n\nDavies, Mike, Andreas Wild, Garrick Orchard, Yulia Sandamirskaya,\nGabriel A. Fonseca Guerra, Prasad Joshi, Philipp Plank, and Sumedh R.\nRisbud. 2021. ‚ÄúAdvancing Neuromorphic Computing with Loihi:\nA Survey of Results and Outlook.‚Äù Proc.\nIEEE 109 (5): 911‚Äì34. https://doi.org/10.1109/jproc.2021.3067593.\n\n\nDavis, Jacqueline, Daniel Bizo, Andy Lawrence, Owen Rogers, and Max\nSmolaks. 2022. ‚ÄúUptime Institute Global Data Center Survey\n2022.‚Äù Uptime Institute.\n\n\nDayarathna, Miyuru, Yonggang Wen, and Rui Fan. 2016. ‚ÄúData Center\nEnergy Consumption Modeling: A Survey.‚Äù IEEE\nCommunications Surveys &Amp; Tutorials 18 (1): 732‚Äì94. https://doi.org/10.1109/comst.2015.2481183.\n\n\nDean, Jeffrey, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Quoc\nV. Le, Mark Z. Mao, et al. 2012. ‚ÄúLarge Scale Distributed Deep\nNetworks.‚Äù In Advances in Neural Information Processing\nSystems 25: 26th Annual Conference on Neural Information Processing\nSystems 2012. Proceedings of a Meeting Held December 3-6, 2012, Lake\nTahoe, Nevada, United States, edited by Peter L. Bartlett, Fernando\nC. N. Pereira, Christopher J. C. Burges, L√©on Bottou, and Kilian Q.\nWeinberger, 1232‚Äì40. https://proceedings.neurips.cc/paper/2012/hash/6aca97005c68f1206823815f66102863-Abstract.html.\n\n\nDeng, Jia, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Fei-Fei Li.\n2009. ‚ÄúImageNet: A Large-Scale\nHierarchical Image Database.‚Äù In 2009 IEEE Conference on\nComputer Vision and Pattern Recognition, 248‚Äì55. IEEE. https://doi.org/10.1109/cvpr.2009.5206848.\n\n\nDesai, Tanvi, Felix Ritchie, Richard Welpton, et al. 2016. ‚ÄúFive\nSafes: Designing Data Access for Research.‚Äù Economics Working\nPaper Series 1601: 28.\n\n\nDevlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.\n‚ÄúNone.‚Äù In Proceedings of the 2019 Conference of the\nNorth, 4171‚Äì86. Minneapolis, Minnesota: Association for\nComputational Linguistics. https://doi.org/10.18653/v1/n19-1423.\n\n\nDhar, Sauptik, Junyao Guo, Jiayi (Jason) Liu, Samarth Tripathi, Unmesh\nKurup, and Mohak Shah. 2021. ‚ÄúA Survey of on-Device Machine\nLearning: An Algorithms and Learning Theory Perspective.‚Äù ACM\nTransactions on Internet of Things 2 (3): 1‚Äì49. https://doi.org/10.1145/3450494.\n\n\nDong, Xin, Barbara De Salvo, Meng Li, Chiao Liu, Zhongnan Qu, H. T.\nKung, and Ziyun Li. 2022. ‚ÄúSplitNets:\nDesigning Neural Architectures for Efficient Distributed\nComputing on Head-Mounted Systems.‚Äù In 2022 IEEE/CVF\nConference on Computer Vision and Pattern Recognition (CVPR),\n12549‚Äì59. IEEE. https://doi.org/10.1109/cvpr52688.2022.01223.\n\n\nDongarra, Jack J. 2009. ‚ÄúThe Evolution of High Performance\nComputing on System z.‚Äù IBM J. Res. Dev. 53: 3‚Äì4.\n\n\nDuarte, Javier, Nhan Tran, Ben Hawks, Christian Herwig, Jules Muhizi,\nShvetank Prakash, and Vijay Janapa Reddi. 2022.\n‚ÄúFastML Science Benchmarks: Accelerating\nReal-Time Scientific Edge Machine Learning.‚Äù ArXiv\nPreprint abs/2207.07958. https://arxiv.org/abs/2207.07958.\n\n\nDuchi, John C., Elad Hazan, and Yoram Singer. 2010. ‚ÄúAdaptive\nSubgradient Methods for Online Learning and Stochastic\nOptimization.‚Äù In COLT 2010 - the 23rd Conference on Learning\nTheory, Haifa, Israel, June 27-29, 2010, edited by Adam Tauman\nKalai and Mehryar Mohri, 257‚Äì69. Omnipress. http://colt2010.haifa.il.ibm.com/papers/COLT2010proceedings.pdf#page=265.\n\n\nDuisterhof, Bardienus P, Srivatsan Krishnan, Jonathan J Cruz, Colby R\nBanbury, William Fu, Aleksandra Faust, Guido CHE de Croon, and Vijay\nJanapa Reddi. 2019. ‚ÄúLearning to Seek: Autonomous\nSource Seeking with Deep Reinforcement Learning Onboard a Nano Drone\nMicrocontroller.‚Äù ArXiv Preprint abs/1909.11236. https://arxiv.org/abs/1909.11236.\n\n\nDuisterhof, Bardienus P., Shushuai Li, Javier Burgues, Vijay Janapa\nReddi, and Guido C. H. E. de Croon. 2021. ‚ÄúSniffy Bug:\nA Fully Autonomous Swarm of Gas-Seeking Nano Quadcopters in\nCluttered Environments.‚Äù In 2021 IEEE/RSJ International\nConference on Intelligent Robots and Systems (IROS), 9099‚Äì9106.\nIEEE; IEEE. https://doi.org/10.1109/iros51168.2021.9636217.\n\n\nD√ºrr, Marc, Gunnar Nissen, Kurt-Wolfram S√ºhs, Philipp Schwenkenbecher,\nChristian Geis, Marius Ringelstein, Hans-Peter Hartung, et al. 2021.\n‚ÄúCSF Findings in Acute NMDAR and LGI1 Antibody‚ÄìAssociated\nAutoimmune Encephalitis.‚Äù Neurology Neuroimmunology &Amp;\nNeuroinflammation 8 (6). https://doi.org/10.1212/nxi.0000000000001086.\n\n\nDwork, Cynthia, Frank McSherry, Kobbi Nissim, and Adam Smith. 2006.\n‚ÄúCalibrating Noise to Sensitivity in Private Data\nAnalysis.‚Äù In Theory of Cryptography, edited by Shai\nHalevi and Tal Rabin, 265‚Äì84. Berlin, Heidelberg: Springer Berlin\nHeidelberg. https://doi.org/10.1007/11681878\\_14.\n\n\nDwork, Cynthia, and Aaron Roth. 2013. ‚ÄúThe Algorithmic Foundations\nof Differential Privacy.‚Äù Foundations and Trends¬Æ in\nTheoretical Computer Science 9 (3-4): 211‚Äì407. https://doi.org/10.1561/0400000042.\n\n\nEbrahimi, Khosrow, Gerard F. Jones, and Amy S. Fleischer. 2014. ‚ÄúA\nReview of Data Center Cooling Technology, Operating Conditions and the\nCorresponding Low-Grade Waste Heat Recovery Opportunities.‚Äù\nRenewable Sustainable Energy Rev. 31 (March): 622‚Äì38. https://doi.org/10.1016/j.rser.2013.12.007.\n\n\nEgwutuoha, Ifeanyi P., David Levy, Bran Selic, and Shiping Chen. 2013.\n‚ÄúA Survey of Fault Tolerance Mechanisms and Checkpoint/Restart\nImplementations for High Performance Computing Systems.‚Äù The\nJournal of Supercomputing 65 (3): 1302‚Äì26. https://doi.org/10.1007/s11227-013-0884-0.\n\n\nEisenman, Assaf, Kiran Kumar Matam, Steven Ingram, Dheevatsa Mudigere,\nRaghuraman Krishnamoorthi, Krishnakumar Nair, Misha Smelyanskiy, and\nMurali Annavaram. 2022. ‚ÄúCheck-n-Run: A Checkpointing\nSystem for Training Deep Learning Recommendation Models.‚Äù In\n19th USENIX Symposium on Networked Systems Design and Implementation\n(NSDI 22), 929‚Äì43.\n\n\nEldan, Ronen, and Mark Russinovich. 2023. ‚ÄúWho‚Äôs Harry Potter?\nApproximate Unlearning in LLMs.‚Äù ArXiv Preprint\nabs/2310.02238 (October). http://arxiv.org/abs/2310.02238v2.\n\n\nEl-Rayis, A. O. 2014. ‚ÄúReconfigurable Architectures for the Next\nGeneration of Mobile Device Telecommunications Systems.‚Äù :\nhttps://www.researchgate.net/publication/292608967.\n\n\nEshraghian, Jason K., Max Ward, Emre O. Neftci, Xinxin Wang, Gregor\nLenz, Girish Dwivedi, Mohammed Bennamoun, Doo Seok Jeong, and Wei D. Lu.\n2023. ‚ÄúTraining Spiking Neural Networks Using Lessons from Deep\nLearning.‚Äù Proc. IEEE 111 (9): 1016‚Äì54. https://doi.org/10.1109/jproc.2023.3308088.\n\n\nEsteva, Andre, Brett Kuprel, Roberto A. Novoa, Justin Ko, Susan M.\nSwetter, Helen M. Blau, and Sebastian Thrun. 2017.\n‚ÄúDermatologist-Level Classification of Skin Cancer with Deep\nNeural Networks.‚Äù Nature 542 (7639): 115‚Äì18. https://doi.org/10.1038/nature21056.\n\n\nEykholt, Kevin, Ivan Evtimov, Earlence Fernandes, Bo Li, Amir Rahmati,\nChaowei Xiao, Atul Prakash, Tadayoshi Kohno, and Dawn Song. 2017.\n‚ÄúRobust Physical-World Attacks on Deep Learning Models.‚Äù\nArXiv Preprint abs/1707.08945. https://arxiv.org/abs/1707.08945.\n\n\nFahim, Farah, Benjamin Hawks, Christian Herwig, James Hirschauer, Sergo\nJindariani, Nhan Tran, Luca P. Carloni, et al. 2021. ‚ÄúHls4ml:\nAn Open-Source Codesign Workflow to Empower Scientific\nLow-Power Machine Learning Devices.‚Äù https://arxiv.org/abs/2103.05579.\n\n\nFarah, Martha J. 2005. ‚ÄúNeuroethics: The Practical\nand the Philosophical.‚Äù Trends Cogn. Sci. 9 (1): 34‚Äì40.\nhttps://doi.org/10.1016/j.tics.2004.12.001.\n\n\nFarwell, James P., and Rafal Rohozinski. 2011. ‚ÄúStuxnet and the\nFuture of Cyber War.‚Äù Survival 53 (1): 23‚Äì40. https://doi.org/10.1080/00396338.2011.555586.\n\n\nFowers, Jeremy, Kalin Ovtcharov, Michael Papamichael, Todd Massengill,\nMing Liu, Daniel Lo, Shlomi Alkalay, et al. 2018. ‚ÄúA Configurable\nCloud-Scale DNN Processor for Real-Time\nAI.‚Äù In 2018 ACM/IEEE 45th Annual International\nSymposium on Computer Architecture (ISCA), 1‚Äì14. IEEE; IEEE. https://doi.org/10.1109/isca.2018.00012.\n\n\nFrancalanza, Adrian, Luca Aceto, Antonis Achilleos, Duncan Paul Attard,\nIan Cassar, Dario Della Monica, and Anna Ing√≥lfsd√≥ttir. 2017. ‚ÄúA\nFoundation for Runtime Monitoring.‚Äù In International\nConference on Runtime Verification, 8‚Äì29. Springer.\n\n\nFrankle, Jonathan, and Michael Carbin. 2019. ‚ÄúThe Lottery Ticket\nHypothesis: Finding Sparse, Trainable Neural\nNetworks.‚Äù In 7th International Conference on Learning\nRepresentations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.\nOpenReview.net. https://openreview.net/forum?id=rJl-b3RcF7.\n\n\nFriedman, Batya. 1996. ‚ÄúValue-Sensitive Design.‚Äù\nInteractions 3 (6): 16‚Äì23. https://doi.org/10.1145/242485.242493.\n\n\nFurber, Steve. 2016. ‚ÄúLarge-Scale Neuromorphic Computing\nSystems.‚Äù J. Neural Eng. 13 (5): 051001. https://doi.org/10.1088/1741-2560/13/5/051001.\n\n\nFursov, Ivan, Matvey Morozov, Nina Kaploukhaya, Elizaveta Kovtun,\nRodrigo Rivera-Castro, Gleb Gusev, Dmitry Babaev, Ivan Kireev, Alexey\nZaytsev, and Evgeny Burnaev. 2021. ‚ÄúAdversarial Attacks on Deep\nModels for Financial Transaction Records.‚Äù In Proceedings of\nthe 27th ACM SIGKDD Conference on Knowledge Discovery &Amp; Data\nMining, 2868‚Äì78. ACM. https://doi.org/10.1145/3447548.3467145.\n\n\nGale, Trevor, Erich Elsen, and Sara Hooker. 2019. ‚ÄúThe State of\nSparsity in Deep Neural Networks.‚Äù ArXiv Preprint\nabs/1902.09574. https://arxiv.org/abs/1902.09574.\n\n\nGandolfi, Karine, Christophe Mourtel, and Francis Olivier. 2001.\n‚ÄúElectromagnetic Analysis: Concrete Results.‚Äù In\nCryptographic Hardware and Embedded Systems ‚Äî CHES 2001,\n251‚Äì61. Springer; Springer Berlin Heidelberg. https://doi.org/10.1007/3-540-44709-1\\_21.\n\n\nGannot, G., and M. Ligthart. 1994. ‚ÄúVerilog HDL Based\nFPGA Design.‚Äù In International Verilog HDL\nConference, 86‚Äì92. IEEE. https://doi.org/10.1109/ivc.1994.323743.\n\n\nGao, Yansong, Said F. Al-Sarawi, and Derek Abbott. 2020. ‚ÄúPhysical\nUnclonable Functions.‚Äù Nature Electronics 3 (2): 81‚Äì91.\nhttps://doi.org/10.1038/s41928-020-0372-5.\n\n\nGates, Byron D. 2009. ‚ÄúFlexible Electronics.‚Äù\nScience 323 (5921): 1566‚Äì67. https://doi.org/10.1126/science.1171230.\n\n\nGebru, Timnit, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman\nVaughan, Hanna Wallach, Hal Daum√© III, and Kate Crawford. 2021.\n‚ÄúDatasheets for Datasets.‚Äù Commun. ACM 64 (12):\n86‚Äì92. https://doi.org/10.1145/3458723.\n\n\nGeiger, Atticus, Hanson Lu, Thomas Icard, and Christopher Potts. 2021.\n‚ÄúCausal Abstractions of Neural Networks.‚Äù In Advances\nin Neural Information Processing Systems 34: Annual Conference on Neural\nInformation Processing Systems 2021, NeurIPS 2021, December 6-14, 2021,\nVirtual, edited by Marc‚ÄôAurelio Ranzato, Alina Beygelzimer, Yann N.\nDauphin, Percy Liang, and Jennifer Wortman Vaughan, 9574‚Äì86. https://proceedings.neurips.cc/paper/2021/hash/4f5c422f4d49a5a807eda27434231040-Abstract.html.\n\n\nGholami, Dong Kim, Mahoney Yao, and Keutzer. 2021. ‚ÄúA Survey of\nQuantization Methods for Efficient Neural Network Inference).‚Äù\nArXiv Preprint. https://arxiv.org/abs/2103.13630.\n\n\nGlorot, Xavier, and Yoshua Bengio. 2010. ‚ÄúUnderstanding the\nDifficulty of Training Deep Feedforward Neural Networks.‚Äù In\nProceedings of the Thirteenth International Conference on Artificial\nIntelligence and Statistics, 249‚Äì56. http://proceedings.mlr.press/v9/glorot10a.html.\n\n\nGnad, Dennis R. E., Fabian Oboril, and Mehdi B. Tahoori. 2017.\n‚ÄúVoltage Drop-Based Fault Attacks on FPGAs Using Valid\nBitstreams.‚Äù In 2017 27th International Conference on Field\nProgrammable Logic and Applications (FPL), 1‚Äì7. IEEE; IEEE. https://doi.org/10.23919/fpl.2017.8056840.\n\n\nGoodfellow, Ian, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David\nWarde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2020.\n‚ÄúGenerative Adversarial Networks.‚Äù Commun. ACM 63\n(11): 139‚Äì44. https://doi.org/10.1145/3422622.\n\n\nGoodyear, Victoria A. 2017. ‚ÄúSocial Media, Apps and Wearable\nTechnologies: Navigating Ethical Dilemmas and\nProcedures.‚Äù Qualitative Research in Sport, Exercise and\nHealth 9 (3): 285‚Äì302. https://doi.org/10.1080/2159676x.2017.1303790.\n\n\nGoogle. n.d. ‚ÄúInformation Quality Content Moderation.‚Äù https://blog.google/documents/83/.\n\n\nGordon, Ariel, Elad Eban, Ofir Nachum, Bo Chen, Hao Wu, Tien-Ju Yang,\nand Edward Choi. 2018. ‚ÄúMorphNet: Fast\n&Amp; Simple Resource-Constrained Structure Learning of Deep\nNetworks.‚Äù In 2018 IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 1586‚Äì95. IEEE. https://doi.org/10.1109/cvpr.2018.00171.\n\n\nGr√§fe, Ralf, Qutub Syed Sha, Florian Geissler, and Michael Paulitsch.\n2023. ‚ÄúLarge-Scale Application of Fault Injection into\nPyTorch Models -an Extension to PyTorchFI for\nValidation Efficiency.‚Äù In 2023 53rd Annual IEEE/IFIP\nInternational Conference on Dependable Systems and Networks -\nSupplemental Volume (DSN-s), 56‚Äì62. IEEE; IEEE. https://doi.org/10.1109/dsn-s58398.2023.00025.\n\n\nGreengard, Samuel. 2021. The Internet of Things. The MIT Press.\nhttps://doi.org/10.7551/mitpress/13937.001.0001.\n\n\nGrossman, Elizabeth. 2007. High Tech Trash: Digital\nDevices, Hidden Toxics, and Human Health. Island press.\n\n\nGruslys, Audrunas, R√©mi Munos, Ivo Danihelka, Marc Lanctot, and Alex\nGraves. 2016. ‚ÄúMemory-Efficient Backpropagation Through\nTime.‚Äù In Advances in Neural Information Processing Systems\n29: Annual Conference on Neural Information Processing Systems 2016,\nDecember 5-10, 2016, Barcelona, Spain, edited by Daniel D. Lee,\nMasashi Sugiyama, Ulrike von Luxburg, Isabelle Guyon, and Roman Garnett,\n4125‚Äì33. https://proceedings.neurips.cc/paper/2016/hash/a501bebf79d570651ff601788ea9d16d-Abstract.html.\n\n\nGu, Ivy. 2023. ‚ÄúDeep Learning Model Compression (Ii) by Ivy Gu\nMedium.‚Äù https://ivygdy.medium.com/deep-learning-model-compression-ii-546352ea9453.\n\n\nGujarati, Arpan, Reza Karimi, Safya Alzayat, Wei Hao, Antoine Kaufmann,\nYmir Vigfusson, and Jonathan Mace. 2020. ‚ÄúServing DNNs Like\nClockwork: Performance Predictability from the Bottom Up.‚Äù In\n14th USENIX Symposium on Operating Systems Design and Implementation\n(OSDI 20), 443‚Äì62. https://www.usenix.org/conference/osdi20/presentation/gujarati.\n\n\nGuo, Chuan, Jacob Gardner, Yurong You, Andrew Gordon Wilson, and Kilian\nWeinberger. 2019. ‚ÄúSimple Black-Box Adversarial Attacks.‚Äù\nIn International Conference on Machine Learning, 2484‚Äì93. PMLR.\n\n\nGuo, Yutao, Hao Wang, Hui Zhang, Tong Liu, Zhaoguang Liang, Yunlong Xia,\nLi Yan, et al. 2019. ‚ÄúMobile Photoplethysmographic Technology to\nDetect Atrial Fibrillation.‚Äù Journal of the American College\nof Cardiology 74 (19): 2365‚Äì75. https://doi.org/10.1016/j.jacc.2019.08.019.\n\n\nGupta, Maanak, Charankumar Akiri, Kshitiz Aryal, Eli Parker, and\nLopamudra Praharaj. 2023. ‚ÄúFrom ChatGPT to ThreatGPT: Impact of\nGenerative AI in Cybersecurity and Privacy.‚Äù IEEE Access\n11: 80218‚Äì45. https://doi.org/10.1109/access.2023.3300381.\n\n\nGupta, Maya, Andrew Cotter, Jan Pfeifer, Konstantin Voevodski, Kevin\nCanini, Alexander Mangylov, Wojciech Moczydlowski, and Alexander Van\nEsbroeck. 2016. ‚ÄúMonotonic Calibrated Interpolated Look-up\nTables.‚Äù The Journal of Machine Learning Research 17\n(1): 3790‚Äì3836.\n\n\nGupta, Udit, Mariam Elgamal, Gage Hills, Gu-Yeon Wei, Hsien-Hsin S. Lee,\nDavid Brooks, and Carole-Jean Wu. 2022. ‚ÄúAct: Designing\nSustainable Computer Systems with an Architectural Carbon Modeling\nTool.‚Äù In Proceedings of the 49th Annual International\nSymposium on Computer Architecture, 784‚Äì99. ACM. https://doi.org/10.1145/3470496.3527408.\n\n\nGwennap, Linley. n.d. ‚ÄúCertus-NX Innovates\nGeneral-Purpose FPGAs.‚Äù\n\n\nHaensch, Wilfried, Tayfun Gokmen, and Ruchir Puri. 2019. ‚ÄúThe Next\nGeneration of Deep Learning Hardware: Analog\nComputing.‚Äù Proc. IEEE 107 (1): 108‚Äì22. https://doi.org/10.1109/jproc.2018.2871057.\n\n\nHamming, R. W. 1950. ‚ÄúError Detecting and Error Correcting\nCodes.‚Äù Bell Syst. Tech. J. 29 (2): 147‚Äì60. https://doi.org/10.1002/j.1538-7305.1950.tb00463.x.\n\n\nHan, Song, Huizi Mao, and William J Dally. 2015. ‚ÄúDeep\nCompression: Compressing Deep Neural Networks with Pruning,\nTrained Quantization and Huffman Coding.‚Äù arXiv Preprint\narXiv:1510.00149.\n\n\nHan, Song, Huizi Mao, and William J. Dally. 2016. ‚ÄúDeep\nCompression: Compressing Deep Neural Networks with Pruning,\nTrained Quantization and Huffman Coding.‚Äù https://arxiv.org/abs/1510.00149.\n\n\nHandlin, Oscar. 1965. ‚ÄúScience and Technology in Popular\nCulture.‚Äù Daedalus-Us., 156‚Äì70.\n\n\nHardt, Moritz, Eric Price, and Nati Srebro. 2016. ‚ÄúEquality of\nOpportunity in Supervised Learning.‚Äù In Advances in Neural\nInformation Processing Systems 29: Annual Conference on Neural\nInformation Processing Systems 2016, December 5-10, 2016, Barcelona,\nSpain, edited by Daniel D. Lee, Masashi Sugiyama, Ulrike von\nLuxburg, Isabelle Guyon, and Roman Garnett, 3315‚Äì23. https://proceedings.neurips.cc/paper/2016/hash/9d2682367c3935defcb1f9e247a97c0d-Abstract.html.\n\n\nHawks, Benjamin, Javier Duarte, Nicholas J. Fraser, Alessandro\nPappalardo, Nhan Tran, and Yaman Umuroglu. 2021. ‚ÄúPs and Qs: Quantization-aware Pruning for Efficient Low\nLatency Neural Network Inference.‚Äù Frontiers in Artificial\nIntelligence 4 (July). https://doi.org/10.3389/frai.2021.676564.\n\n\nHazan, Avi, and Elishai Ezra Tsur. 2021. ‚ÄúNeuromorphic Analog\nImplementation of Neural Engineering Framework-Inspired Spiking Neuron\nfor High-Dimensional Representation.‚Äù Front. Neurosci.\n15 (February): 627221. https://doi.org/10.3389/fnins.2021.627221.\n\n\nHe, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2015.\n‚ÄúDelving Deep into Rectifiers: Surpassing Human-Level Performance\non ImageNet Classification.‚Äù In 2015 IEEE International\nConference on Computer Vision (ICCV), 1026‚Äì34. IEEE. https://doi.org/10.1109/iccv.2015.123.\n\n\n‚Äî‚Äî‚Äî. 2016. ‚ÄúDeep Residual Learning for Image Recognition.‚Äù\nIn 2016 IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR), 770‚Äì78. IEEE. https://doi.org/10.1109/cvpr.2016.90.\n\n\nHe, Yi, Prasanna Balaprakash, and Yanjing Li. 2020.\n‚ÄúFIdelity: Efficient Resilience Analysis\nFramework for Deep Learning Accelerators.‚Äù In 2020 53rd\nAnnual IEEE/ACM International Symposium on Microarchitecture\n(MICRO), 270‚Äì81. IEEE; IEEE. https://doi.org/10.1109/micro50266.2020.00033.\n\n\nHe, Yi, Mike Hutton, Steven Chan, Robert De Gruijl, Rama Govindaraju,\nNishant Patil, and Yanjing Li. 2023. ‚ÄúUnderstanding and Mitigating\nHardware Failures in Deep Learning Training Systems.‚Äù In\nProceedings of the 50th Annual International Symposium on Computer\nArchitecture, 1‚Äì16. IEEE; ACM. https://doi.org/10.1145/3579371.3589105.\n\n\nH√©bert-Johnson, √örsula, Michael P. Kim, Omer Reingold, and Guy N.\nRothblum. 2018. ‚ÄúMulticalibration: Calibration for\nthe (Computationally-Identifiable) Masses.‚Äù In\nProceedings of the 35th International Conference on Machine\nLearning, ICML 2018, Stockholmsm√§ssan, Stockholm, Sweden, July 10-15,\n2018, edited by Jennifer G. Dy and Andreas Krause, 80:1944‚Äì53.\nProceedings of Machine Learning Research. PMLR. http://proceedings.mlr.press/v80/hebert-johnson18a.html.\n\n\nHegde, Sumant. 2023. ‚ÄúAn Introduction to Separable Convolutions -\nAnalytics Vidhya.‚Äù https://www.analyticsvidhya.com/blog/2021/11/an-introduction-to-separable-convolutions/.\n\n\nHenderson, Peter, Jieru Hu, Joshua Romoff, Emma Brunskill, Dan Jurafsky,\nand Joelle Pineau. 2020. ‚ÄúTowards the Systematic Reporting of the\nEnergy and Carbon Footprints of Machine Learning.‚Äù The\nJournal of Machine Learning Research 21 (1): 10039‚Äì81.\n\n\nHendrycks, Dan, and Thomas Dietterich. 2019. ‚ÄúBenchmarking Neural\nNetwork Robustness to Common Corruptions and Perturbations.‚Äù\narXiv Preprint arXiv:1903.12261.\n\n\nHendrycks, Dan, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn\nSong. 2021. ‚ÄúNatural Adversarial Examples.‚Äù In 2021\nIEEE/CVF Conference on Computer Vision and Pattern Recognition\n(CVPR), 15257‚Äì66. IEEE. https://doi.org/10.1109/cvpr46437.2021.01501.\n\n\nHennessy, John L., and David A. Patterson. 2019. ‚ÄúA New Golden Age\nfor Computer Architecture.‚Äù Commun. ACM 62 (2): 48‚Äì60.\nhttps://doi.org/10.1145/3282307.\n\n\nHeyndrickx, Wouter, Lewis Mervin, Tobias Morawietz, No√© Sturm, Lukas\nFriedrich, Adam Zalewski, Anastasia Pentina, et al. 2023.\n‚ÄúMelloddy: Cross-Pharma Federated Learning at Unprecedented Scale\nUnlocks Benefits in Qsar Without Compromising Proprietary\nInformation.‚Äù Journal of Chemical Information and\nModeling 64 (7): 2331‚Äì44. https://pubs.acs.org/doi/10.1021/acs.jcim.3c00799.\n\n\nHimmelstein, Gracie, David Bates, and Li Zhou. 2022. ‚ÄúExamination\nof Stigmatizing Language in the Electronic Health Record.‚Äù\nJAMA Network Open 5 (1): e2144967. https://doi.org/10.1001/jamanetworkopen.2021.44967.\n\n\nHinton, Geoffrey. 2005. ‚ÄúVan Nostrand‚Äôs Scientific Encyclopedia.‚Äù Wiley.\nhttps://doi.org/10.1002/0471743984.vse0673.\n\n\n‚Äî‚Äî‚Äî. 2017. ‚ÄúOverview of Minibatch Gradient Descent.‚Äù\nUniversity of Toronto; University Lecture.\n\n\nHo Yoon, Jung, Hyung-Suk Jung, Min Hwan Lee, Gun Hwan Kim, Seul Ji Song,\nJun Yeong Seok, Kyung Jean Yoon, et al. 2012. ‚ÄúFrontiers in\nElectronic Materials.‚Äù Wiley. https://doi.org/10.1002/9783527667703.ch67.\n\n\nHoefler, Torsten, Dan Alistarh, Tal Ben-Nun, Nikoli Dryden, and\nAlexandra Peste. 2021. ‚ÄúSparsity in Deep Learning: Pruning and\nGrowth for Efficient Inference and Training in Neural Networks,‚Äù\nJanuary. http://arxiv.org/abs/2102.00554v1.\n\n\nHolland, Sarah, Ahmed Hosny, Sarah Newman, Joshua Joseph, and Kasia\nChmielinski. 2020. ‚ÄúThe Dataset Nutrition Label: A Framework to\nDrive Higher Data Quality Standards.‚Äù In Data Protection and\nPrivacy. Hart Publishing. https://doi.org/10.5040/9781509932771.ch-001.\n\n\nHong, Sanghyun, Nicholas Carlini, and Alexey Kurakin. 2023.\n‚ÄúPublishing Efficient on-Device Models Increases Adversarial\nVulnerability.‚Äù In 2023 IEEE Conference on Secure and\nTrustworthy Machine Learning (SaTML), abs 1603 5279:271‚Äì90. IEEE;\nIEEE. https://doi.org/10.1109/satml54575.2023.00026.\n\n\nHooker, Sara. 2021. ‚ÄúThe Hardware Lottery.‚Äù\nCommunications of the ACM 64 (12): 58‚Äì65. https://doi.org/10.1145/3467017.\n\n\nHosseini, Hossein, Sreeram Kannan, Baosen Zhang, and Radha Poovendran.\n2017. ‚ÄúDeceiving Google‚Äôs Perspective Api Built for Detecting\nToxic Comments.‚Äù ArXiv Preprint abs/1702.08138. https://arxiv.org/abs/1702.08138.\n\n\nHoward, Andrew G., Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun\nWang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. 2017.\n‚ÄúMobileNets: Efficient Convolutional\nNeural Networks for Mobile Vision Applications.‚Äù ArXiv\nPreprint. https://arxiv.org/abs/1704.04861.\n\n\nHsiao, Yu-Shun, Zishen Wan, Tianyu Jia, Radhika Ghosal, Abdulrahman\nMahmoud, Arijit Raychowdhury, David Brooks, Gu-Yeon Wei, and Vijay\nJanapa Reddi. 2023. ‚ÄúMAVFI: An\nEnd-to-End Fault Analysis Framework with Anomaly Detection and Recovery\nfor Micro Aerial Vehicles.‚Äù In 2023 Design, Automation\n&Amp; Test in Europe Conference &Amp; Exhibition (DATE),\n1‚Äì6. IEEE; IEEE. https://doi.org/10.23919/date56975.2023.10137246.\n\n\nHsu, Liang-Ching, Ching-Yi Huang, Yen-Hsun Chuang, Ho-Wen Chen, Ya-Ting\nChan, Heng Yi Teah, Tsan-Yao Chen, Chiung-Fen Chang, Yu-Ting Liu, and\nYu-Min Tzou. 2016. ‚ÄúAccumulation of Heavy Metals and Trace\nElements in Fluvial Sediments Received Effluents from Traditional and\nSemiconductor Industries.‚Äù Scientific Reports 6 (1):\n34250. https://doi.org/10.1038/srep34250.\n\n\nHu, Jie, Li Shen, and Gang Sun. 2018. ‚ÄúSqueeze-and-Excitation\nNetworks.‚Äù In 2018 IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 7132‚Äì41. IEEE. https://doi.org/10.1109/cvpr.2018.00745.\n\n\nHu, Yang, Jie Jiang, Lifu Zhang, Yunfeng Shi, and Jian Shi. 2023.\n‚ÄúHalide Perovskite Semiconductors.‚Äù Wiley. https://doi.org/10.1002/9783527829026.ch13.\n\n\nHuang, Tsung-Ching, Kenjiro Fukuda, Chun-Ming Lo, Yung-Hui Yeh, Tsuyoshi\nSekitani, Takao Someya, and Kwang-Ting Cheng. 2011.\n‚ÄúPseudo-CMOS: A Design Style for\nLow-Cost and Robust Flexible Electronics.‚Äù IEEE Trans.\nElectron Devices 58 (1): 141‚Äì50. https://doi.org/10.1109/ted.2010.2088127.\n\n\nHutter, Michael, Jorn-Marc Schmidt, and Thomas Plos. 2009.\n‚ÄúContact-Based Fault Injections and Power Analysis on RFID\nTags.‚Äù In 2009 European Conference on Circuit Theory and\nDesign, 409‚Äì12. IEEE; IEEE. https://doi.org/10.1109/ecctd.2009.5275012.\n\n\nIandola, Forrest N, Song Han, Matthew W Moskewicz, Khalid Ashraf,\nWilliam J Dally, and Kurt Keutzer. 2016. ‚ÄúSqueezeNet:\nAlexnet-level Accuracy with 50x Fewer\nParameters and 0.5 MB Model Size.‚Äù ArXiv\nPreprint abs/1602.07360. https://arxiv.org/abs/1602.07360.\n\n\nIgnatov, Andrey, Radu Timofte, William Chou, Ke Wang, Max Wu, Tim\nHartley, and Luc Van Gool. 2018. ‚ÄúAI Benchmark:\nRunning Deep Neural Networks on Android\nSmartphones,‚Äù 0‚Äì0.\n\n\nImani, Mohsen, Abbas Rahimi, and Tajana S. Rosing. 2016.\n‚ÄúResistive Configurable Associative Memory for Approximate\nComputing.‚Äù In Proceedings of the 2016 Design, Automation\n&Amp; Test in Europe Conference &Amp; Exhibition (DATE),\n1327‚Äì32. IEEE; Research Publishing Services. https://doi.org/10.3850/9783981537079_0454.\n\n\nIntelLabs. 2023. ‚ÄúKnowledge Distillation - Neural Network\nDistiller.‚Äù https://intellabs.github.io/distiller/knowledge_distillation.html.\n\n\nIppolito, Daphne, Florian Tramer, Milad Nasr, Chiyuan Zhang, Matthew\nJagielski, Katherine Lee, Christopher Choquette Choo, and Nicholas\nCarlini. 2023. ‚ÄúPreventing Generation of Verbatim Memorization in\nLanguage Models Gives a False Sense of Privacy.‚Äù In\nProceedings of the 16th International Natural Language Generation\nConference, 5253‚Äì70. Association for Computational Linguistics. https://doi.org/10.18653/v1/2023.inlg-main.3.\n\n\nIrimia-Vladu, Mihai. 2014.\n‚Äú‚ÄúGreen‚Äù Electronics:\nBiodegradable and Biocompatible Materials and Devices for\nSustainable Future.‚Äù Chem. Soc. Rev. 43 (2): 588‚Äì610. https://doi.org/10.1039/c3cs60235d.\n\n\nIsscc. 2014. ‚ÄúComputing‚Äôs Energy Problem (and What We Can Do about\nIt).‚Äù https://ieeexplore.ieee.org/document/6757323.\n\n\nJacob, Benoit, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang,\nAndrew Howard, Hartwig Adam, and Dmitry Kalenichenko. 2018.\n‚ÄúQuantization and Training of Neural Networks for Efficient\nInteger-Arithmetic-Only Inference.‚Äù In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recognition,\n2704‚Äì13.\n\n\nJaderberg, Max, Valentin Dalibard, Simon Osindero, Wojciech M.\nCzarnecki, Jeff Donahue, Ali Razavi, Oriol Vinyals, et al. 2017.\n‚ÄúPopulation Based Training of Neural Networks.‚Äù arXiv\nPreprint arXiv:1711.09846, November. http://arxiv.org/abs/1711.09846v2.\n\n\nJanapa Reddi, Vijay, Alexander Elium, Shawn Hymel, David Tischler,\nDaniel Situnayake, Carl Ward, Louis Moreau, et al. 2023. ‚ÄúEdge\nImpulse: An MLOps Platform for Tiny Machine Learning.‚Äù\nProceedings of Machine Learning and Systems 5.\n\n\nJha, A. R. 2014. Rare Earth Materials: Properties and\nApplications. CRC Press. https://doi.org/10.1201/b17045.\n\n\nJha, Saurabh, Subho Banerjee, Timothy Tsai, Siva K. S. Hari, Michael B.\nSullivan, Zbigniew T. Kalbarczyk, Stephen W. Keckler, and Ravishankar K.\nIyer. 2019. ‚ÄúML-Based Fault Injection for Autonomous\nVehicles: A Case for Bayesian Fault\nInjection.‚Äù In 2019 49th Annual IEEE/IFIP International\nConference on Dependable Systems and Networks (DSN), 112‚Äì24. IEEE;\nIEEE. https://doi.org/10.1109/dsn.2019.00025.\n\n\nJia, Yangqing, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan\nLong, Ross Girshick, Sergio Guadarrama, and Trevor Darrell. 2014.\n‚ÄúCaffe: Convolutional Architecture for Fast Feature\nEmbedding.‚Äù In Proceedings of the 22nd ACM International\nConference on Multimedia, 675‚Äì78. ACM. https://doi.org/10.1145/2647868.2654889.\n\n\nJia, Zhe, Marco Maggioni, Benjamin Staiger, and Daniele P. Scarpazza.\n2018. ‚ÄúDissecting the NVIDIA Volta\nGPU Architecture via Microbenchmarking.‚Äù ArXiv\nPreprint. https://arxiv.org/abs/1804.06826.\n\n\nJia, Zhenge, Dawei Li, Xiaowei Xu, Na Li, Feng Hong, Lichuan Ping, and\nYiyu Shi. 2023. ‚ÄúLife-Threatening Ventricular Arrhythmia Detection\nChallenge in Implantable\nCardioverterdefibrillators.‚Äù Nature Machine\nIntelligence 5 (5): 554‚Äì55. https://doi.org/10.1038/s42256-023-00659-9.\n\n\nJia, Zhihao, Matei Zaharia, and Alex Aiken. 2019. ‚ÄúBeyond Data and\nModel Parallelism for Deep Neural Networks.‚Äù In Proceedings\nof Machine Learning and Systems 2019, MLSys 2019, Stanford, CA, USA,\nMarch 31 - April 2, 2019, edited by Ameet Talwalkar, Virginia\nSmith, and Matei Zaharia. mlsys.org. https://proceedings.mlsys.org/book/265.pdf.\n\n\nJin, Yilun, Xiguang Wei, Yang Liu, and Qiang Yang. 2020. ‚ÄúTowards\nUtilizing Unlabeled Data in Federated Learning: A Survey and\nProspective.‚Äù arXiv Preprint arXiv:2002.11545, February.\nhttp://arxiv.org/abs/2002.11545v2.\n\n\nJohnson-Roberson, Matthew, Charles Barto, Rounak Mehta, Sharath Nittur\nSridhar, Karl Rosaen, and Ram Vasudevan. 2017. ‚ÄúDriving in the\nMatrix: Can Virtual Worlds Replace Human-Generated\nAnnotations for Real World Tasks?‚Äù In 2017 IEEE International\nConference on Robotics and Automation (ICRA), 746‚Äì53. Singapore,\nSingapore: IEEE. https://doi.org/10.1109/icra.2017.7989092.\n\n\nJouppi, Norman P., Cliff Young, Nishant Patil, David Patterson, Gaurav\nAgrawal, Raminder Bajwa, Sarah Bates, et al. 2017a. ‚ÄúIn-Datacenter\nPerformance Analysis of a Tensor Processing Unit.‚Äù In\nProceedings of the 44th Annual International Symposium on Computer\nArchitecture, 1‚Äì12. ISCA ‚Äô17. New York, NY, USA: ACM. https://doi.org/10.1145/3079856.3080246.\n\n\n‚Äî‚Äî‚Äî, et al. 2017b. ‚ÄúIn-Datacenter Performance Analysis of a Tensor\nProcessing Unit.‚Äù In Proceedings of the 44th Annual\nInternational Symposium on Computer Architecture, 1‚Äì12. ISCA ‚Äô17.\nNew York, NY, USA: ACM. https://doi.org/10.1145/3079856.3080246.\n\n\nJouppi, Norm, George Kurian, Sheng Li, Peter Ma, Rahul Nagarajan, Lifeng\nNai, Nishant Patil, et al. 2023. ‚ÄúTPU V4:\nAn Optically Reconfigurable Supercomputer for Machine\nLearning with Hardware Support for Embeddings.‚Äù In\nProceedings of the 50th Annual International Symposium on Computer\nArchitecture. ISCA ‚Äô23. New York, NY, USA: ACM. https://doi.org/10.1145/3579371.3589350.\n\n\nJoye, Marc, and Michael Tunstall. 2012. Fault Analysis in\nCryptography. Springer Berlin Heidelberg. https://doi.org/10.1007/978-3-642-29656-7.\n\n\nKairouz, Peter, Sewoong Oh, and Pramod Viswanath. 2015. ‚ÄúSecure\nMulti-Party Differential Privacy.‚Äù In Advances in Neural\nInformation Processing Systems 28: Annual Conference on Neural\nInformation Processing Systems 2015, December 7-12, 2015, Montreal,\nQuebec, Canada, edited by Corinna Cortes, Neil D. Lawrence, Daniel\nD. Lee, Masashi Sugiyama, and Roman Garnett, 2008‚Äì16. https://proceedings.neurips.cc/paper/2015/hash/a01610228fe998f515a72dd730294d87-Abstract.html.\n\n\nKalamkar, Dhiraj, Dheevatsa Mudigere, Naveen Mellempudi, Dipankar Das,\nKunal Banerjee, Sasikanth Avancha, Dharma Teja Vooturi, et al. 2019.\n‚ÄúA Study of BFLOAT16 for Deep Learning\nTraining.‚Äù https://arxiv.org/abs/1905.12322.\n\n\nKao, Sheng-Chun, Geonhwa Jeong, and Tushar Krishna. 2020.\n‚ÄúConfuciuX: Autonomous Hardware Resource\nAssignment for DNN Accelerators Using Reinforcement\nLearning.‚Äù In 2020 53rd Annual IEEE/ACM International\nSymposium on Microarchitecture (MICRO), 622‚Äì36. IEEE; IEEE. https://doi.org/10.1109/micro50266.2020.00058.\n\n\nKao, Sheng-Chun, and Tushar Krishna. 2020. ‚ÄúGamma: Automating the\nHW Mapping of DNN Models on Accelerators via Genetic Algorithm.‚Äù\nIn Proceedings of the 39th International Conference on\nComputer-Aided Design, 1‚Äì9. ACM. https://doi.org/10.1145/3400302.3415639.\n\n\nKaplan, Jared, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin\nChess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario\nAmodei. 2020. ‚ÄúScaling Laws for Neural Language Models.‚Äù\nArXiv Preprint abs/2001.08361. https://arxiv.org/abs/2001.08361.\n\n\nKarargyris, Alexandros, Renato Umeton, Micah J. Sheller, Alejandro\nAristizabal, Johnu George, Anna Wuest, Sarthak Pati, et al. 2023.\n‚ÄúFederated Benchmarking of Medical Artificial Intelligence with\nMedPerf.‚Äù Nature Machine Intelligence 5 (7): 799‚Äì810. https://doi.org/10.1038/s42256-023-00652-2.\n\n\nKaur, Harmanpreet, Harsha Nori, Samuel Jenkins, Rich Caruana, Hanna\nWallach, and Jennifer Wortman Vaughan. 2020. ‚ÄúInterpreting\nInterpretability: Understanding Data Scientists‚Äô Use of\nInterpretability Tools for Machine Learning.‚Äù In Proceedings\nof the 2020 CHI Conference on Human Factors in Computing Systems,\nedited by Regina Bernhaupt, Florian ‚ÄôFloyd‚ÄôMueller, David Verweij, Josh\nAndres, Joanna McGrenere, Andy Cockburn, Ignacio Avellino, et al., 1‚Äì14.\nACM. https://doi.org/10.1145/3313831.3376219.\n\n\nKawazoe Aguilera, Marcos, Wei Chen, and Sam Toueg. 1997.\n‚ÄúHeartbeat: A Timeout-Free Failure Detector for\nQuiescent Reliable Communication.‚Äù In Distributed Algorithms:\n11th International Workshop, WDAG‚Äô97 Saarbr√ºcken, Germany, September\n2426, 1997 Proceedings 11, 126‚Äì40. Springer.\n\n\nKhan, Mohammad Emtiyaz, and Siddharth Swaroop. 2021.\n‚ÄúKnowledge-Adaptation Priors.‚Äù In Advances in Neural\nInformation Processing Systems 34: Annual Conference on Neural\nInformation Processing Systems 2021, NeurIPS 2021, December 6-14, 2021,\nVirtual, edited by Marc‚ÄôAurelio Ranzato, Alina Beygelzimer, Yann N.\nDauphin, Percy Liang, and Jennifer Wortman Vaughan, 19757‚Äì70. https://proceedings.neurips.cc/paper/2021/hash/a4380923dd651c195b1631af7c829187-Abstract.html.\n\n\nKiela, Douwe, Max Bartolo, Yixin Nie, Divyansh Kaushik, Atticus Geiger,\nZhengxuan Wu, Bertie Vidgen, et al. 2021. ‚ÄúDynabench: Rethinking\nBenchmarking in NLP.‚Äù In Proceedings of the 2021 Conference\nof the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, 4110‚Äì24. Online:\nAssociation for Computational Linguistics. https://doi.org/10.18653/v1/2021.naacl-main.324.\n\n\nKim, Jungrae, Michael Sullivan, and Mattan Erez. 2015. ‚ÄúBamboo\nECC: Strong, Safe, and Flexible Codes for\nReliable Computer Memory.‚Äù In 2015 IEEE 21st International\nSymposium on High Performance Computer Architecture (HPCA), 101‚Äì12.\nIEEE; IEEE. https://doi.org/10.1109/hpca.2015.7056025.\n\n\nKim, Sunju, Chungsik Yoon, Seunghon Ham, Jihoon Park, Ohun Kwon, Donguk\nPark, Sangjun Choi, Seungwon Kim, Kwonchul Ha, and Won Kim. 2018.\n‚ÄúChemical Use in the Semiconductor Manufacturing Industry.‚Äù\nInt. J. Occup. Env. Heal. 24 (3-4): 109‚Äì18. https://doi.org/10.1080/10773525.2018.1519957.\n\n\nKingma, Diederik P., and Jimmy Ba. 2014. ‚ÄúAdam: A Method for\nStochastic Optimization.‚Äù Edited by Yoshua Bengio and Yann LeCun,\nDecember. http://arxiv.org/abs/1412.6980v9.\n\n\nKirkpatrick, James, Razvan Pascanu, Neil Rabinowitz, Joel Veness,\nGuillaume Desjardins, Andrei A. Rusu, Kieran Milan, et al. 2017.\n‚ÄúOvercoming Catastrophic Forgetting in Neural Networks.‚Äù\nProc. Natl. Acad. Sci. 114 (13): 3521‚Äì26. https://doi.org/10.1073/pnas.1611835114.\n\n\nKo, Yohan. 2021. ‚ÄúCharacterizing System-Level Masking Effects\nAgainst Soft Errors.‚Äù Electronics 10 (18): 2286. https://doi.org/10.3390/electronics10182286.\n\n\nKocher, Paul, Jann Horn, Anders Fogh, Daniel Genkin, Daniel Gruss,\nWerner Haas, Mike Hamburg, et al. 2019a. ‚ÄúSpectre Attacks:\nExploiting Speculative Execution.‚Äù In 2019 IEEE Symposium on\nSecurity and Privacy (SP), 1‚Äì19. IEEE. https://doi.org/10.1109/sp.2019.00002.\n\n\n‚Äî‚Äî‚Äî, et al. 2019b. ‚ÄúSpectre Attacks: Exploiting Speculative\nExecution.‚Äù In 2019 IEEE Symposium on Security and Privacy\n(SP), 1‚Äì19. IEEE. https://doi.org/10.1109/sp.2019.00002.\n\n\nKocher, Paul, Joshua Jaffe, and Benjamin Jun. 1999. ‚ÄúDifferential\nPower Analysis.‚Äù In Advances in Cryptology ‚Äî CRYPTO‚Äô 99,\n388‚Äì97. Springer; Springer Berlin Heidelberg. https://doi.org/10.1007/3-540-48405-1\\_25.\n\n\nKocher, Paul, Joshua Jaffe, Benjamin Jun, and Pankaj Rohatgi. 2011.\n‚ÄúIntroduction to Differential Power Analysis.‚Äù Journal\nof Cryptographic Engineering 1 (1): 5‚Äì27. https://doi.org/10.1007/s13389-011-0006-y.\n\n\nKoh, Pang Wei, Thao Nguyen, Yew Siang Tang, Stephen Mussmann, Emma\nPierson, Been Kim, and Percy Liang. 2020. ‚ÄúConcept Bottleneck\nModels.‚Äù In Proceedings of the 37th International Conference\non Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event,\n119:5338‚Äì48. Proceedings of Machine Learning Research. PMLR. http://proceedings.mlr.press/v119/koh20a.html.\n\n\nKoh, Pang Wei, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin\nZhang, Akshay Balsubramani, Weihua Hu, et al. 2021. ‚ÄúWILDS: A\nBenchmark of in-the-Wild Distribution Shifts.‚Äù In Proceedings\nof the 38th International Conference on Machine Learning, ICML 2021,\n18-24 July 2021, Virtual Event, edited by Marina Meila and Tong\nZhang, 139:5637‚Äì64. Proceedings of Machine Learning Research. PMLR. http://proceedings.mlr.press/v139/koh21a.html.\n\n\nKoren, Yehuda, Robert Bell, and Chris Volinsky. 2009. ‚ÄúMatrix\nFactorization Techniques for Recommender Systems.‚Äù\nComputer 42 (8): 30‚Äì37. https://doi.org/10.1109/mc.2009.263.\n\n\nKrishna, Adithya, Srikanth Rohit Nudurupati, Chandana D G, Pritesh\nDwivedi, Andr√© van Schaik, Mahesh Mehendale, and Chetan Singh Thakur.\n2023. ‚ÄúRAMAN: A Re-Configurable and\nSparse TinyML Accelerator for Inference on Edge.‚Äù https://arxiv.org/abs/2306.06493.\n\n\nKrishnamoorthi. 2018. ‚ÄúQuantizing Deep Convolutional Networks for\nEfficient Inference: A Whitepaper.‚Äù ArXiv\nPreprint. https://arxiv.org/abs/1806.08342.\n\n\nKrishnan, Rayan, Pranav Rajpurkar, and Eric J. Topol. 2022.\n‚ÄúSelf-Supervised Learning in Medicine and Healthcare.‚Äù\nNat. Biomed. Eng. 6 (12): 1346‚Äì52. https://doi.org/10.1038/s41551-022-00914-1.\n\n\nKrishnan, Srivatsan, Natasha Jaques, Shayegan Omidshafiei, Dan Zhang,\nIzzeddin Gur, Vijay Janapa Reddi, and Aleksandra Faust. 2022.\n‚ÄúMulti-Agent Reinforcement Learning for Microprocessor Design\nSpace Exploration.‚Äù https://arxiv.org/abs/2211.16385.\n\n\nKrishnan, Srivatsan, Amir Yazdanbakhsh, Shvetank Prakash, Jason Jabbour,\nIkechukwu Uchendu, Susobhan Ghosh, Behzad Boroujerdian, et al. 2023.\n‚ÄúArchGym: An Open-Source Gymnasium for\nMachine Learning Assisted Architecture Design.‚Äù In\nProceedings of the 50th Annual International Symposium on Computer\nArchitecture, 1‚Äì16. ACM. https://doi.org/10.1145/3579371.3589049.\n\n\nKrizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. 2012.\n‚ÄúImageNet Classification with Deep Convolutional\nNeural Networks.‚Äù In Advances in Neural Information\nProcessing Systems 25: 26th Annual Conference on Neural Information\nProcessing Systems 2012. Proceedings of a Meeting Held December 3-6,\n2012, Lake Tahoe, Nevada, United States, edited by Peter L.\nBartlett, Fernando C. N. Pereira, Christopher J. C. Burges, L√©on Bottou,\nand Kilian Q. Weinberger, 1106‚Äì14. https://proceedings.neurips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html.\n\n\n‚Äî‚Äî‚Äî. 2017. ‚ÄúImageNet Classification with Deep\nConvolutional Neural Networks.‚Äù Edited by F. Pereira, C. J.\nBurges, L. Bottou, and K. Q. Weinberger. Commun. ACM 60 (6):\n84‚Äì90. https://doi.org/10.1145/3065386.\n\n\nKung, Hsiang Tsung, and Charles E Leiserson. 1979. ‚ÄúSystolic\nArrays (for VLSI).‚Äù In Sparse Matrix Proceedings\n1978, 1:256‚Äì82. Society for industrial; applied mathematics\nPhiladelphia, PA, USA.\n\n\nKurth, Thorsten, Shashank Subramanian, Peter Harrington, Jaideep Pathak,\nMorteza Mardani, David Hall, Andrea Miele, Karthik Kashinath, and Anima\nAnandkumar. 2023. ‚ÄúFourCastNet:\nAccelerating Global High-Resolution Weather Forecasting\nUsing Adaptive Fourier Neural Operators.‚Äù In\nProceedings of the Platform for Advanced Scientific Computing\nConference, 1‚Äì11. ACM. https://doi.org/10.1145/3592979.3593412.\n\n\nKuzmin, Andrey, Mart Van Baalen, Yuwei Ren, Markus Nagel, Jorn Peters,\nand Tijmen Blankevoort. 2022. ‚ÄúFP8 Quantization:\nThe Power of the Exponent.‚Äù https://arxiv.org/abs/2208.09225.\n\n\nKuznetsova, Alina, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan\nKrasin, Jordi Pont-Tuset, Shahab Kamali, et al. 2020. ‚ÄúThe Open\nImages Dataset V4: Unified Image Classification, Object\nDetection, and Visual Relationship Detection at Scale.‚Äù\nInternational Journal of Computer Vision 128 (7): 1956‚Äì81.\n\n\nKwon, Jisu, and Daejin Park. 2021. ‚ÄúHardware/Software\nCo-Design for TinyML Voice-Recognition Application on\nResource Frugal Edge Devices.‚Äù Applied Sciences 11 (22):\n11073. https://doi.org/10.3390/app112211073.\n\n\nKwon, Sun Hwa, and Lin Dong. 2022. ‚ÄúFlexible Sensors and Machine\nLearning for Heart Monitoring.‚Äù Nano Energy 102\n(November): 107632. https://doi.org/10.1016/j.nanoen.2022.107632.\n\n\nKwon, Young D., Rui Li, Stylianos I. Venieris, Jagmohan Chauhan,\nNicholas D. Lane, and Cecilia Mascolo. 2023. ‚ÄúTinyTrain:\nResource-Aware Task-Adaptive Sparse Training of DNNs at the Data-Scarce\nEdge.‚Äù ArXiv Preprint abs/2307.09988 (July). http://arxiv.org/abs/2307.09988v2.\n\n\nLai, Liangzhen, Naveen Suda, and Vikas Chandra. 2018a. ‚ÄúCmsis-Nn:\nEfficient Neural Network Kernels for Arm Cortex-m\nCpus.‚Äù ArXiv Preprint abs/1801.06601. https://arxiv.org/abs/1801.06601.\n\n\n‚Äî‚Äî‚Äî. 2018b. ‚ÄúCMSIS-NN:\nEfficient Neural Network Kernels for Arm Cortex-m\nCPUs.‚Äù https://arxiv.org/abs/1801.06601.\n\n\nLakkaraju, Himabindu, and Osbert Bastani. 2020.\n‚Äú‚ÄùHow Do i Fool You?‚Äù:\nManipulating User Trust via Misleading Black Box Explanations.‚Äù\nIn Proceedings of the AAAI/ACM Conference on AI, Ethics, and\nSociety, 79‚Äì85. ACM. https://doi.org/10.1145/3375627.3375833.\n\n\nLam, Remi, Alvaro Sanchez-Gonzalez, Matthew Willson, Peter Wirnsberger,\nMeire Fortunato, Ferran Alet, Suman Ravuri, et al. 2023. ‚ÄúLearning\nSkillful Medium-Range Global Weather Forecasting.‚Äù\nScience 382 (6677): 1416‚Äì21. https://doi.org/10.1126/science.adi2336.\n\n\nLannelongue, Loƒ±Ãàc, Jason Grealey, and Michael Inouye. 2021. ‚ÄúGreen\nAlgorithms: Quantifying the Carbon Footprint of\nComputation.‚Äù Adv. Sci. 8 (12): 2100707. https://doi.org/10.1002/advs.202100707.\n\n\nLeCun, Yann, John Denker, and Sara Solla. 1989. ‚ÄúOptimal Brain\nDamage.‚Äù Adv Neural Inf Process Syst 2.\n\n\nLee, Minwoong, Namho Lee, Huijeong Gwon, Jongyeol Kim, Younggwan Hwang,\nand Seongik Cho. 2022. ‚ÄúDesign of Radiation-Tolerant High-Speed\nSignal Processing Circuit for Detecting Prompt Gamma Rays by Nuclear\nExplosion.‚Äù Electronics 11 (18): 2970. https://doi.org/10.3390/electronics11182970.\n\n\nLeRoy Poff, N, MM Brinson, and JW Day. 2002. ‚ÄúAquatic Ecosystems\n& Global Climate Change.‚Äù Pew Center on Global Climate\nChange.\n\n\nLi, En, Liekang Zeng, Zhi Zhou, and Xu Chen. 2020. ‚ÄúEdge\nAI: On-demand Accelerating Deep\nNeural Network Inference via Edge Computing.‚Äù IEEE Trans.\nWireless Commun. 19 (1): 447‚Äì57. https://doi.org/10.1109/twc.2019.2946140.\n\n\nLi, Guanpeng, Siva Kumar Sastry Hari, Michael Sullivan, Timothy Tsai,\nKarthik Pattabiraman, Joel Emer, and Stephen W. Keckler. 2017.\n‚ÄúUnderstanding Error Propagation in Deep Learning Neural Network\n(DNN) Accelerators and Applications.‚Äù In\nProceedings of the International Conference for High Performance\nComputing, Networking, Storage and Analysis, 1‚Äì12. ACM. https://doi.org/10.1145/3126908.3126964.\n\n\nLi, Jingzhen, Igbe Tobore, Yuhang Liu, Abhishek Kandwal, Lei Wang, and\nZedong Nie. 2021. ‚ÄúNon-Invasive Monitoring of Three Glucose Ranges\nBased on ECG by Using DBSCAN-CNN.‚Äù IEEE Journal of Biomedical\nand Health Informatics 25 (9): 3340‚Äì50. https://doi.org/10.1109/jbhi.2021.3072628.\n\n\nLi, Mu, David G. Andersen, Alexander J. Smola, and Kai Yu. 2014.\n‚ÄúCommunication Efficient Distributed Machine Learning with the\nParameter Server.‚Äù In Advances in Neural Information\nProcessing Systems 27: Annual Conference on Neural Information\nProcessing Systems 2014, December 8-13 2014, Montreal, Quebec,\nCanada, edited by Zoubin Ghahramani, Max Welling, Corinna Cortes,\nNeil D. Lawrence, and Kilian Q. Weinberger, 19‚Äì27. https://proceedings.neurips.cc/paper/2014/hash/1ff1de774005f8da13f42943881c655f-Abstract.html.\n\n\nLi, Qinbin, Zeyi Wen, Zhaomin Wu, Sixu Hu, Naibo Wang, Yuan Li, Xu Liu,\nand Bingsheng He. 2023. ‚ÄúA Survey on Federated Learning Systems:\nVision, Hype and Reality for Data Privacy and\nProtection.‚Äù IEEE Trans. Knowl. Data Eng. 35 (4):\n3347‚Äì66. https://doi.org/10.1109/tkde.2021.3124599.\n\n\nLi, Tian, Anit Kumar Sahu, Ameet Talwalkar, and Virginia Smith. 2020.\n‚ÄúFederated Learning: Challenges, Methods, and Future\nDirections.‚Äù IEEE Signal Processing Magazine 37 (3):\n50‚Äì60. https://doi.org/10.1109/msp.2020.2975749.\n\n\nLi, Xiang, Tao Qin, Jian Yang, and Tie-Yan Liu. 2016. ‚ÄúLightRNN:\nMemory and Computation-Efficient Recurrent Neural Networks.‚Äù In\nAdvances in Neural Information Processing Systems 29: Annual\nConference on Neural Information Processing Systems 2016, December 5-10,\n2016, Barcelona, Spain, edited by Daniel D. Lee, Masashi Sugiyama,\nUlrike von Luxburg, Isabelle Guyon, and Roman Garnett, 4385‚Äì93. https://proceedings.neurips.cc/paper/2016/hash/c3e4035af2a1cde9f21e1ae1951ac80b-Abstract.html.\n\n\nLi, Yuhang, Xin Dong, and Wei Wang. 2020. ‚ÄúAdditive Powers-of-Two\nQuantization: An Efficient Non-Uniform Discretization for\nNeural Networks.‚Äù In 8th International Conference on Learning\nRepresentations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30,\n2020. OpenReview.net. https://openreview.net/forum?id=BkgXT24tDS.\n\n\nLi, Zhizhong, and Derek Hoiem. 2018. ‚ÄúLearning Without\nForgetting.‚Äù IEEE Trans. Pattern Anal. Mach. Intell. 40\n(12): 2935‚Äì47. https://doi.org/10.1109/tpami.2017.2773081.\n\n\nLi, Zhuohan, Lianmin Zheng, Yinmin Zhong, Vincent Liu, Ying Sheng, Xin\nJin, Yanping Huang, et al. 2023. ‚Äú{AlpaServe}:\nStatistical Multiplexing with Model Parallelism for Deep Learning\nServing.‚Äù In 17th USENIX Symposium on Operating Systems\nDesign and Implementation (OSDI 23), 663‚Äì79.\n\n\nLin, Ji, Wei-Ming Chen, Yujun Lin, John Cohn, Chuang Gan, and Song Han.\n2020. ‚ÄúMCUNet: Tiny Deep Learning on\nIoT Devices.‚Äù In Advances in Neural Information\nProcessing Systems 33: Annual Conference on Neural Information\nProcessing Systems 2020, NeurIPS 2020, December 6-12, 2020,\nVirtual, edited by Hugo Larochelle, Marc‚ÄôAurelio Ranzato, Raia\nHadsell, Maria-Florina Balcan, and Hsuan-Tien Lin. https://proceedings.neurips.cc/paper/2020/hash/86c51678350f656dcc7f490a43946ee5-Abstract.html.\n\n\nLin, Ji, Ligeng Zhu, Wei-Ming Chen, Wei-Chen Wang, Chuang Gan, and Song\nHan. 2022. ‚ÄúOn-Device Training Under 256kb Memory.‚Äù\nAdv. Neur. In. 35: 22941‚Äì54.\n\n\nLin, Ji, Ligeng Zhu, Wei-Ming Chen, Wei-Chen Wang, and Song Han. 2023.\n‚ÄúTiny Machine Learning: Progress and Futures Feature.‚Äù\nIEEE Circuits Syst. Mag. 23 (3): 8‚Äì34. https://doi.org/10.1109/mcas.2023.3302182.\n\n\nLin, Tsung-Yi, Michael Maire, Serge Belongie, James Hays, Pietro Perona,\nDeva Ramanan, Piotr Doll√°r, and C Lawrence Zitnick. 2014.\n‚ÄúMicrosoft Coco: Common Objects in Context.‚Äù\nIn Computer VisionECCV 2014: 13th European Conference,\nZurich, Switzerland, September 6-12, 2014, Proceedings, Part v 13,\n740‚Äì55. Springer.\n\n\nLindgren, Simon. 2023. Handbook of Critical Studies of Artificial\nIntelligence. Edward Elgar Publishing.\n\n\nLindholm, Andreas, Dave Zachariah, Petre Stoica, and Thomas B. Schon.\n2019. ‚ÄúData Consistency Approach to Model Validation.‚Äù\n#IEEE_O_ACC# 7: 59788‚Äì96. https://doi.org/10.1109/access.2019.2915109.\n\n\nLindholm, Erik, John Nickolls, Stuart Oberman, and John Montrym. 2008.\n‚ÄúNVIDIA Tesla: A Unified Graphics and\nComputing Architecture.‚Äù IEEE Micro 28 (2): 39‚Äì55. https://doi.org/10.1109/mm.2008.31.\n\n\nLin, Tang Tang, Dang Yang, and Han Gan. 2023. ‚ÄúAWQ:\nActivation-aware Weight Quantization for\nLLM Compression and Acceleration.‚Äù ArXiv\nPreprint. https://arxiv.org/abs/2306.00978.\n\n\nLiu, Yanan, Xiaoxia Wei, Jinyu Xiao, Zhijie Liu, Yang Xu, and Yun Tian.\n2020. ‚ÄúEnergy Consumption and Emission Mitigation Prediction Based\non Data Center Traffic and PUE for Global Data\nCenters.‚Äù Global Energy Interconnection 3 (3): 272‚Äì82.\nhttps://doi.org/10.1016/j.gloei.2020.07.008.\n\n\nLiu, Yingcheng, Guo Zhang, Christopher G. Tarolli, Rumen Hristov, Stella\nJensen-Roberts, Emma M. Waddell, Taylor L. Myers, et al. 2022.\n‚ÄúMonitoring Gait at Home with Radio Waves in Parkinson‚Äôs Disease:\nA Marker of Severity, Progression, and Medication Response.‚Äù\nScience Translational Medicine 14 (663): eadc9669. https://doi.org/10.1126/scitranslmed.adc9669.\n\n\nLoh, Gabriel H. 2008. ‚Äú3D-Stacked Memory\nArchitectures for Multi-Core Processors.‚Äù ACM SIGARCH\nComputer Architecture News 36 (3): 453‚Äì64. https://doi.org/10.1145/1394608.1382159.\n\n\nLopez-Paz, David, and Marc‚ÄôAurelio Ranzato. 2017. ‚ÄúGradient\nEpisodic Memory for Continual Learning.‚Äù Adv Neural Inf\nProcess Syst 30.\n\n\nLou, Yin, Rich Caruana, Johannes Gehrke, and Giles Hooker. 2013.\n‚ÄúAccurate Intelligible Models with Pairwise Interactions.‚Äù\nIn Proceedings of the 19th ACM SIGKDD International Conference on\nKnowledge Discovery and Data Mining, edited by Inderjit S. Dhillon,\nYehuda Koren, Rayid Ghani, Ted E. Senator, Paul Bradley, Rajesh Parekh,\nJingrui He, Robert L. Grossman, and Ramasamy Uthurusamy, 623‚Äì31. ACM. https://doi.org/10.1145/2487575.2487579.\n\n\nLowy, Andrew, Rakesh Pavan, Sina Baharlouei, Meisam Razaviyayn, and\nAhmad Beirami. 2021. ‚ÄúFermi: Fair Empirical Risk\nMinimization via Exponential R√©nyi Mutual Information.‚Äù\n\n\nLubana, Ekdeep Singh, and Robert P Dick. 2020. ‚ÄúA Gradient Flow\nFramework for Analyzing Network Pruning.‚Äù arXiv Preprint\narXiv:2009.11839.\n\n\nLuebke, David. 2008. ‚ÄúCUDA: Scalable\nParallel Programming for High-Performance Scientific Computing.‚Äù\nIn 2008 5th IEEE International Symposium on Biomedical Imaging: From\nNano to Macro, 836‚Äì38. IEEE. https://doi.org/10.1109/isbi.2008.4541126.\n\n\nLundberg, Scott M., and Su-In Lee. 2017. ‚ÄúA Unified Approach to\nInterpreting Model Predictions.‚Äù In Advances in Neural\nInformation Processing Systems 30: Annual Conference on Neural\nInformation Processing Systems 2017, December 4-9, 2017, Long Beach, CA,\nUSA, edited by Isabelle Guyon, Ulrike von Luxburg, Samy Bengio,\nHanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett,\n4765‚Äì74. https://proceedings.neurips.cc/paper/2017/hash/8a20a8621978632d76c43dfd28b67767-Abstract.html.\n\n\nMa, Dongning, Fred Lin, Alban Desmaison, Joel Coburn, Daniel Moore,\nSriram Sankar, and Xun Jiao. 2024. ‚ÄúDr.\nDNA: Combating Silent Data Corruptions in Deep\nLearning Using Distribution of Neuron Activations.‚Äù In\nProceedings of the 29th ACM International Conference on\nArchitectural Support for Programming Languages and Operating Systems,\nVolume 3, 239‚Äì52. ACM. https://doi.org/10.1145/3620666.3651349.\n\n\nMaas, Martin, David G. Andersen, Michael Isard, Mohammad Mahdi\nJavanmard, Kathryn S. McKinley, and Colin Raffel. 2024. ‚ÄúCombining\nMachine Learning and Lifetime-Based Resource Management for Memory\nAllocation and Beyond.‚Äù Commun. ACM 67 (4): 87‚Äì96. https://doi.org/10.1145/3611018.\n\n\nMaass, Wolfgang. 1997. ‚ÄúNetworks of Spiking Neurons:\nThe Third Generation of Neural Network Models.‚Äù\nNeural Networks 10 (9): 1659‚Äì71. https://doi.org/10.1016/s0893-6080(97)00011-7.\n\n\nMadry, Aleksander, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras,\nand Adrian Vladu. 2017. ‚ÄúTowards Deep Learning Models Resistant to\nAdversarial Attacks.‚Äù arXiv Preprint arXiv:1706.06083.\n\n\nMahmoud, Abdulrahman, Neeraj Aggarwal, Alex Nobbe, Jose Rodrigo Sanchez\nVicarte, Sarita V. Adve, Christopher W. Fletcher, Iuri Frosio, and Siva\nKumar Sastry Hari. 2020. ‚ÄúPyTorchFI: A\nRuntime Perturbation Tool for DNNs.‚Äù In 2020\n50th Annual IEEE/IFIP International Conference on Dependable Systems and\nNetworks Workshops (DSN-w), 25‚Äì31. IEEE; IEEE. https://doi.org/10.1109/dsn-w50199.2020.00014.\n\n\nMahmoud, Abdulrahman, Siva Kumar Sastry Hari, Christopher W. Fletcher,\nSarita V. Adve, Charbel Sakr, Naresh Shanbhag, Pavlo Molchanov, Michael\nB. Sullivan, Timothy Tsai, and Stephen W. Keckler. 2021.\n‚ÄúOptimizing Selective Protection for CNN\nResilience.‚Äù In 2021 IEEE 32nd International Symposium on\nSoftware Reliability Engineering (ISSRE), 127‚Äì38. IEEE. https://doi.org/10.1109/issre52982.2021.00025.\n\n\nMahmoud, Abdulrahman, Thierry Tambe, Tarek Aloui, David Brooks, and\nGu-Yeon Wei. 2022. ‚ÄúGoldenEye: A\nPlatform for Evaluating Emerging Numerical Data Formats in\nDNN Accelerators.‚Äù In 2022 52nd Annual IEEE/IFIP\nInternational Conference on Dependable Systems and Networks (DSN),\n206‚Äì14. IEEE. https://doi.org/10.1109/dsn53405.2022.00031.\n\n\nMarkoviƒá, Danijela, Alice Mizrahi, Damien Querlioz, and Julie Grollier.\n2020. ‚ÄúPhysics for Neuromorphic Computing.‚Äù Nature\nReviews Physics 2 (9): 499‚Äì510. https://doi.org/10.1038/s42254-020-0208-2.\n\n\nMartin, C. Dianne. 1993. ‚ÄúThe Myth of the Awesome Thinking\nMachine.‚Äù Commun. ACM 36 (4): 120‚Äì33. https://doi.org/10.1145/255950.153587.\n\n\nMarulli, Fiammetta, Stefano Marrone, and Laura Verde. 2022.\n‚ÄúSensitivity of Machine Learning Approaches to Fake and Untrusted\nData in Healthcare Domain.‚Äù Journal of Sensor and Actuator\nNetworks 11 (2): 21. https://doi.org/10.3390/jsan11020021.\n\n\nMaslej, Nestor, Loredana Fattorini, Erik Brynjolfsson, John Etchemendy,\nKatrina Ligett, Terah Lyons, James Manyika, et al. 2023.\n‚ÄúArtificial Intelligence Index Report 2023.‚Äù ArXiv\nPreprint abs/2310.03715. https://arxiv.org/abs/2310.03715.\n\n\nMattson, Peter, Vijay Janapa Reddi, Christine Cheng, Cody Coleman, Greg\nDiamos, David Kanter, Paulius Micikevicius, et al. 2020a. ‚ÄúMLPerf:\nAn Industry Standard Benchmark Suite for Machine Learning\nPerformance.‚Äù IEEE Micro 40 (2): 8‚Äì16. https://doi.org/10.1109/mm.2020.2974843.\n\n\n‚Äî‚Äî‚Äî, et al. 2020b. ‚ÄúMLPerf: An Industry\nStandard Benchmark Suite for Machine Learning Performance.‚Äù\nIEEE Micro 40 (2): 8‚Äì16. https://doi.org/10.1109/mm.2020.2974843.\n\n\nMazumder, Mark, Sharad Chitlangia, Colby Banbury, Yiping Kang, Juan\nManuel Ciro, Keith Achorn, Daniel Galvez, et al. 2021.\n‚ÄúMultilingual Spoken Words Corpus.‚Äù In Thirty-Fifth\nConference on Neural Information Processing Systems Datasets and\nBenchmarks Track (Round 2).\n\n\nMcCarthy, John. 1981. ‚ÄúEpistemological Problems of Artificial\nIntelligence.‚Äù In Readings in Artificial Intelligence,\n459‚Äì65. Elsevier. https://doi.org/10.1016/b978-0-934613-03-3.50035-0.\n\n\nMcMahan, Brendan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise\nAg√ºera y Arcas. 2017a. ‚ÄúCommunication-Efficient Learning of Deep\nNetworks from Decentralized Data.‚Äù In Proceedings of the 20th\nInternational Conference on Artificial Intelligence and Statistics,\nAISTATS 2017, 20-22 April 2017, Fort Lauderdale, FL, USA, edited by\nAarti Singh and Xiaojin (Jerry) Zhu, 54:1273‚Äì82. Proceedings of Machine\nLearning Research. PMLR. http://proceedings.mlr.press/v54/mcmahan17a.html.\n\n\n‚Äî‚Äî‚Äî. 2017b. ‚ÄúCommunication-Efficient Learning of Deep Networks\nfrom Decentralized Data.‚Äù In Artificial Intelligence and\nStatistics, 1273‚Äì82. PMLR. http://proceedings.mlr.press/v54/mcmahan17a.html.\n\n\nMiller, Charlie. 2019. ‚ÄúLessons Learned from Hacking a\nCar.‚Äù IEEE Design &Amp; Test 36 (6): 7‚Äì9. https://doi.org/10.1109/mdat.2018.2863106.\n\n\nMiller, Charlie, and Chris Valasek. 2015. ‚ÄúRemote Exploitation of\nan Unaltered Passenger Vehicle.‚Äù Black Hat USA 2015 (S\n91): 1‚Äì91.\n\n\nMiller, D. A. B. 2000. ‚ÄúOptical Interconnects to Silicon.‚Äù\n#IEEE_J_JSTQE# 6 (6): 1312‚Äì17. https://doi.org/10.1109/2944.902184.\n\n\nMills, Andrew, and Stephen Le Hunte. 1997. ‚ÄúAn Overview of\nSemiconductor Photocatalysis.‚Äù J. Photochem. Photobiol.,\nA 108 (1): 1‚Äì35. https://doi.org/10.1016/s1010-6030(97)00118-4.\n\n\nMirhoseini, Azalia, Anna Goldie, Mustafa Yazgan, Joe Wenjie Jiang,\nEbrahim Songhori, Shen Wang, Young-Joon Lee, et al. 2021. ‚ÄúA Graph\nPlacement Methodology for Fast Chip Design.‚Äù Nature 594\n(7862): 207‚Äì12. https://doi.org/10.1038/s41586-021-03544-w.\n\n\nMishra, Asit K., Jorge Albericio Latorre, Jeff Pool, Darko Stosic, Dusan\nStosic, Ganesh Venkatesh, Chong Yu, and Paulius Micikevicius. 2021.\n‚ÄúAccelerating Sparse Deep Neural Networks.‚Äù CoRR\nabs/2104.08378. https://arxiv.org/abs/2104.08378.\n\n\nMittal, Sparsh, Gaurav Verma, Brajesh Kaushik, and Farooq A. Khanday.\n2021. ‚ÄúA Survey of SRAM-Based in-Memory Computing\nTechniques and Applications.‚Äù J. Syst. Architect. 119\n(October): 102276. https://doi.org/10.1016/j.sysarc.2021.102276.\n\n\nModha, Dharmendra S., Filipp Akopyan, Alexander Andreopoulos,\nRathinakumar Appuswamy, John V. Arthur, Andrew S. Cassidy, Pallab Datta,\net al. 2023. ‚ÄúNeural Inference at the Frontier of Energy, Space,\nand Time.‚Äù Science 382 (6668): 329‚Äì35. https://doi.org/10.1126/science.adh1174.\n\n\nMohanram, K., and N. A. Touba. 2003. ‚ÄúPartial Error Masking to\nReduce Soft Error Failure Rate in Logic Circuits.‚Äù In\nProceedings. 16th IEEE Symposium on Computer Arithmetic,\n433‚Äì40. IEEE; IEEE Comput. Soc. https://doi.org/10.1109/dftvs.2003.1250141.\n\n\nMonyei, Chukwuka G., and Kirsten E. H. Jenkins. 2018. ‚ÄúElectrons\nHave No Identity: Setting Right Misrepresentations in\nGoogle and Apple‚Äôs Clean Energy Purchasing.‚Äù\nEnergy Research &Amp; Social Science 46 (December): 48‚Äì51.\nhttps://doi.org/10.1016/j.erss.2018.06.015.\n\n\nMoshawrab, Mohammad, Mehdi Adda, Abdenour Bouzouane, Hussein Ibrahim,\nand Ali Raad. 2023. ‚ÄúReviewing Federated Learning Aggregation\nAlgorithms; Strategies, Contributions, Limitations and Future\nPerspectives.‚Äù Electronics 12 (10): 2287. https://doi.org/10.3390/electronics12102287.\n\n\nMukherjee, S. S., J. Emer, and S. K. Reinhardt. 2005. ‚ÄúThe Soft\nError Problem: An Architectural Perspective.‚Äù In\n11th International Symposium on High-Performance Computer\nArchitecture, 243‚Äì47. IEEE; IEEE. https://doi.org/10.1109/hpca.2005.37.\n\n\nMunshi, Aaftab. 2009. ‚ÄúThe OpenCL\nSpecification.‚Äù In 2009 IEEE Hot Chips 21 Symposium\n(HCS), 1‚Äì314. IEEE. https://doi.org/10.1109/hotchips.2009.7478342.\n\n\nMusk, Elon et al. 2019. ‚ÄúAn Integrated Brain-Machine Interface\nPlatform with Thousands of Channels.‚Äù J. Med. Internet\nRes. 21 (10): e16194. https://doi.org/10.2196/16194.\n\n\nMyllyaho, Lalli, Mikko Raatikainen, Tomi M√§nnist√∂, Jukka K. Nurminen,\nand Tommi Mikkonen. 2022. ‚ÄúOn Misbehaviour and Fault Tolerance in\nMachine Learning Systems.‚Äù J. Syst. Software 183\n(January): 111096. https://doi.org/10.1016/j.jss.2021.111096.\n\n\nNakano, Jane. 2021. The Geopolitics of Critical Minerals Supply\nChains. JSTOR.\n\n\nNarayanan, Arvind, and Vitaly Shmatikov. 2006. ‚ÄúHow to Break\nAnonymity of the Netflix Prize Dataset.‚Äù CoRR. http://arxiv.org/abs/cs/0610105.\n\n\nNg, Davy Tsz Kit, Jac Ka Lok Leung, Kai Wah Samuel Chu, and Maggie Shen\nQiao. 2021. ‚ÄúAI Literacy: Definition,\nTeaching, Evaluation and Ethical Issues.‚Äù Proceedings of the\nAssociation for Information Science and Technology 58 (1): 504‚Äì9.\n\n\nNgo, Richard, Lawrence Chan, and S√∂ren Mindermann. 2022. ‚ÄúThe\nAlignment Problem from a Deep Learning Perspective.‚Äù ArXiv\nPreprint abs/2209.00626. https://arxiv.org/abs/2209.00626.\n\n\nNguyen, Ngoc-Bao, Keshigeyan Chandrasegaran, Milad Abdollahzadeh, and\nNgai-Man Cheung. 2023. ‚ÄúRe-Thinking Model Inversion Attacks\nAgainst Deep Neural Networks.‚Äù In 2023 IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR), 16384‚Äì93. IEEE. https://doi.org/10.1109/cvpr52729.2023.01572.\n\n\nNorrie, Thomas, Nishant Patil, Doe Hyun Yoon, George Kurian, Sheng Li,\nJames Laudon, Cliff Young, Norman Jouppi, and David Patterson. 2021.\n‚ÄúThe Design Process for Google‚Äôs Training Chips:\nTpuv2 and TPUv3.‚Äù IEEE Micro\n41 (2): 56‚Äì63. https://doi.org/10.1109/mm.2021.3058217.\n\n\nNorthcutt, Curtis G, Anish Athalye, and Jonas Mueller. 2021.\n‚ÄúPervasive Label Errors in Test Sets Destabilize Machine Learning\nBenchmarks.‚Äù arXiv. https://doi.org/https://doi.org/10.48550/arXiv.2103.14749\narXiv-issued DOI via DataCite.\n\n\nObermeyer, Ziad, Brian Powers, Christine Vogeli, and Sendhil\nMullainathan. 2019. ‚ÄúDissecting Racial Bias in an Algorithm Used\nto Manage the Health of Populations.‚Äù Science 366\n(6464): 447‚Äì53. https://doi.org/10.1126/science.aax2342.\n\n\nOecd. 2023. ‚ÄúA Blueprint for Building National Compute Capacity\nfor Artificial Intelligence.‚Äù 350. Organisation for Economic\nCo-Operation; Development (OECD). https://doi.org/10.1787/876367e3-en.\n\n\nOlah, Chris, Nick Cammarata, Ludwig Schubert, Gabriel Goh, Michael\nPetrov, and Shan Carter. 2020. ‚ÄúZoom in: An\nIntroduction to Circuits.‚Äù Distill 5 (3): e00024‚Äì001. https://doi.org/10.23915/distill.00024.001.\n\n\nOliynyk, Daryna, Rudolf Mayer, and Andreas Rauber. 2023. ‚ÄúI Know\nWhat You Trained Last Summer: A Survey on Stealing Machine Learning\nModels and Defences.‚Äù ACM Computing Surveys 55 (14s):\n1‚Äì41. https://doi.org/10.1145/3595292.\n\n\nOoko, Samson Otieno, Marvin Muyonga Ogore, Jimmy Nsenga, and Marco\nZennaro. 2021. ‚ÄúTinyML in Africa:\nOpportunities and Challenges.‚Äù In 2021 IEEE\nGlobecom Workshops (GC Wkshps), 1‚Äì6. IEEE; IEEE. https://doi.org/10.1109/gcwkshps52748.2021.9682107.\n\n\nOprea, Alina, Anoop Singhal, and Apostol Vassilev. 2022.\n‚ÄúPoisoning Attacks Against Machine Learning: Can\nMachine Learning Be Trustworthy?‚Äù Computer 55 (11):\n94‚Äì99. https://doi.org/10.1109/mc.2022.3190787.\n\n\nPan, Sinno Jialin, and Qiang Yang. 2010. ‚ÄúA Survey on Transfer\nLearning.‚Äù IEEE Transactions on Knowledge and Data\nEngineering 22 (10): 1345‚Äì59. https://doi.org/10.1109/tkde.2009.191.\n\n\nPanda, Priyadarshini, Indranil Chakraborty, and Kaushik Roy. 2019.\n‚ÄúDiscretization Based Solutions for Secure Machine Learning\nAgainst Adversarial Attacks.‚Äù #IEEE_O_ACC# 7: 70157‚Äì68.\nhttps://doi.org/10.1109/access.2019.2919463.\n\n\nPapadimitriou, George, and Dimitris Gizopoulos. 2021.\n‚ÄúDemystifying the System Vulnerability Stack:\nTransient Fault Effects Across the Layers.‚Äù In\n2021 ACM/IEEE 48th Annual International Symposium on Computer\nArchitecture (ISCA), 902‚Äì15. IEEE; IEEE. https://doi.org/10.1109/isca52012.2021.00075.\n\n\nPapernot, Nicolas, Patrick McDaniel, Xi Wu, Somesh Jha, and Ananthram\nSwami. 2016. ‚ÄúDistillation as a Defense to Adversarial\nPerturbations Against Deep Neural Networks.‚Äù In 2016 IEEE\nSymposium on Security and Privacy (SP), 582‚Äì97. IEEE; IEEE. https://doi.org/10.1109/sp.2016.41.\n\n\nParrish, Alicia, Hannah Rose Kirk, Jessica Quaye, Charvi Rastogi, Max\nBartolo, Oana Inel, Juan Ciro, et al. 2023. ‚ÄúAdversarial Nibbler:\nA Data-Centric Challenge for Improving the Safety of\nText-to-Image Models.‚Äù ArXiv Preprint abs/2305.14384. https://arxiv.org/abs/2305.14384.\n\n\nPatterson, David A, and John L Hennessy. 2016. Computer Organization\nand Design ARM Edition: The Hardware Software\nInterface. Morgan kaufmann.\n\n\nPatterson, David, Joseph Gonzalez, Urs Holzle, Quoc Le, Chen Liang,\nLluis-Miquel Munguia, Daniel Rothchild, David R. So, Maud Texier, and\nJeff Dean. 2022. ‚ÄúThe Carbon Footprint of Machine Learning\nTraining Will Plateau, Then Shrink.‚Äù Computer 55 (7):\n18‚Äì28. https://doi.org/10.1109/mc.2022.3148714.\n\n\nPeters, Dorian, Rafael A. Calvo, and Richard M. Ryan. 2018.\n‚ÄúDesigning for Motivation, Engagement and Wellbeing in Digital\nExperience.‚Äù Front. Psychol. 9 (May): 797. https://doi.org/10.3389/fpsyg.2018.00797.\n\n\nPhillips, P Jonathon, Carina A Hahn, Peter C Fontana, David A\nBroniatowski, and Mark A Przybocki. 2020. ‚ÄúFour Principles of\nExplainable Artificial Intelligence.‚Äù Gaithersburg,\nMaryland 18.\n\n\nPlank, James S. 1997. ‚ÄúA Tutorial on\nReedSolomon Coding for Fault-Tolerance in\nRAID-Like Systems.‚Äù Software: Practice and\nExperience 27 (9): 995‚Äì1012.\n\n\nPont, Michael J, and Royan HL Ong. 2002. ‚ÄúUsing Watchdog Timers to\nImprove the Reliability of Single-Processor Embedded Systems:\nSeven New Patterns and a Case Study.‚Äù In\nProceedings of the First Nordic Conference on Pattern Languages of\nPrograms, 159‚Äì200. Citeseer.\n\n\nPrakash, Shvetank, Tim Callahan, Joseph Bushagour, Colby Banbury, Alan\nV. Green, Pete Warden, Tim Ansell, and Vijay Janapa Reddi. 2023.\n‚ÄúCFU Playground: Full-stack Open-Source Framework for Tiny Machine\nLearning (TinyML) Acceleration on\nFPGAs.‚Äù In 2023 IEEE International Symposium on\nPerformance Analysis of Systems and Software (ISPASS). Vol.\nabs/2201.01863. IEEE. https://doi.org/10.1109/ispass57527.2023.00024.\n\n\nPrakash, Shvetank, Matthew Stewart, Colby Banbury, Mark Mazumder, Pete\nWarden, Brian Plancher, and Vijay Janapa Reddi. 2023. ‚ÄúIs\nTinyML Sustainable? Assessing the Environmental Impacts of\nMachine Learning on Microcontrollers.‚Äù ArXiv Preprint.\nhttps://arxiv.org/abs/2301.11899.\n\n\nPsoma, Sotiria D., and Chryso Kanthou. 2023. ‚ÄúWearable Insulin\nBiosensors for Diabetes Management: Advances and Challenges.‚Äù\nBiosensors 13 (7): 719. https://doi.org/10.3390/bios13070719.\n\n\nPushkarna, Mahima, Andrew Zaldivar, and Oddur Kjartansson. 2022.\n‚ÄúData Cards: Purposeful and Transparent Dataset\nDocumentation for Responsible AI.‚Äù In 2022 ACM\nConference on Fairness, Accountability, and Transparency. ACM. https://doi.org/10.1145/3531146.3533231.\n\n\nPutnam, Andrew, Adrian M. Caulfield, Eric S. Chung, Derek Chiou, Kypros\nConstantinides, John Demme, Hadi Esmaeilzadeh, et al. 2014. ‚ÄúA\nReconfigurable Fabric for Accelerating Large-Scale Datacenter\nServices.‚Äù ACM SIGARCH Computer Architecture News 42\n(3): 13‚Äì24. https://doi.org/10.1145/2678373.2665678.\n\n\nQi, Chen, Shibo Shen, Rongpeng Li, Zhifeng Zhao, Qing Liu, Jing Liang,\nand Honggang Zhang. 2021. ‚ÄúAn Efficient Pruning Scheme of Deep\nNeural Networks for Internet of Things Applications.‚Äù EURASIP\nJournal on Advances in Signal Processing 2021 (1): 31. https://doi.org/10.1186/s13634-021-00744-4.\n\n\nQian, Yu, Xuegong Zhou, Hao Zhou, and Lingli Wang. 2024. ‚ÄúAn\nEfficient Reinforcement Learning Based Framework for Exploring Logic\nSynthesis.‚Äù ACM Trans. Des. Autom. Electron. Syst. 29\n(2): 1‚Äì33. https://doi.org/10.1145/3632174.\n\n\nR. V., Rashmi, and Karthikeyan A. 2018. ‚ÄúSecure Boot of Embedded\nApplications - a Review.‚Äù In 2018 Second International\nConference on Electronics, Communication and Aerospace Technology\n(ICECA), 291‚Äì98. IEEE. https://doi.org/10.1109/iceca.2018.8474730.\n\n\nRachwan, John, Daniel Z√ºgner, Bertrand Charpentier, Simon Geisler,\nMorgane Ayle, and Stephan G√ºnnemann. 2022. ‚ÄúWinning the Lottery\nAhead of Time: Efficient Early Network Pruning.‚Äù In\nInternational Conference on Machine Learning, 18293‚Äì309. PMLR.\n\n\nRaina, Rajat, Anand Madhavan, and Andrew Y. Ng. 2009. ‚ÄúLarge-Scale\nDeep Unsupervised Learning Using Graphics Processors.‚Äù In\nProceedings of the 26th Annual International Conference on Machine\nLearning, edited by Andrea Pohoreckyj Danyluk, L√©on Bottou, and\nMichael L. Littman, 382:873‚Äì80. ACM International Conference Proceeding\nSeries. ACM. https://doi.org/10.1145/1553374.1553486.\n\n\nRamaswamy, Vikram V., Sunnie S. Y. Kim, Ruth Fong, and Olga Russakovsky.\n2023a. ‚ÄúOverlooked Factors in Concept-Based Explanations:\nDataset Choice, Concept Learnability, and Human\nCapability.‚Äù In 2023 IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR), 10932‚Äì41. IEEE. https://doi.org/10.1109/cvpr52729.2023.01052.\n\n\nRamaswamy, Vikram V, Sunnie SY Kim, Ruth Fong, and Olga Russakovsky.\n2023b. ‚ÄúUFO: A Unified Method for\nControlling Understandability and Faithfulness Objectives in\nConcept-Based Explanations for CNNs.‚Äù ArXiv\nPreprint abs/2303.15632. https://arxiv.org/abs/2303.15632.\n\n\nRamcharan, Amanda, Kelsee Baranowski, Peter McCloskey, Babuali Ahmed,\nJames Legg, and David P. Hughes. 2017. ‚ÄúDeep Learning for\nImage-Based Cassava Disease Detection.‚Äù Front. Plant\nSci. 8 (October): 1852. https://doi.org/10.3389/fpls.2017.01852.\n\n\nRamesh, Aditya, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss,\nAlec Radford, Mark Chen, and Ilya Sutskever. 2021. ‚ÄúZero-Shot\nText-to-Image Generation.‚Äù In Proceedings of the 38th\nInternational Conference on Machine Learning, ICML 2021, 18-24 July\n2021, Virtual Event, edited by Marina Meila and Tong Zhang,\n139:8821‚Äì31. Proceedings of Machine Learning Research. PMLR. http://proceedings.mlr.press/v139/ramesh21a.html.\n\n\nRanganathan, Parthasarathy. 2011. ‚ÄúFrom Microprocessors to\nNanostores: Rethinking Data-Centric Systems.‚Äù\nComputer 44 (1): 39‚Äì48. https://doi.org/10.1109/mc.2011.18.\n\n\nRao, Ravi. 2021. ‚ÄúTinyML Unlocks New Possibilities\nfor Sustainable Development Technologies.‚Äù\nWww.wevolver.com. https://www.wevolver.com/article/tinyml-unlocks-new-possibilities-for-sustainable-development-technologies.\n\n\nRashid, Layali, Karthik Pattabiraman, and Sathish Gopalakrishnan. 2012.\n‚ÄúIntermittent Hardware Errors Recovery: Modeling and\nEvaluation.‚Äù In 2012 Ninth International Conference on\nQuantitative Evaluation of Systems, 220‚Äì29. IEEE; IEEE. https://doi.org/10.1109/qest.2012.37.\n\n\n‚Äî‚Äî‚Äî. 2015. ‚ÄúCharacterizing the Impact of Intermittent Hardware\nFaults on Programs.‚Äù IEEE Trans. Reliab. 64 (1):\n297‚Äì310. https://doi.org/10.1109/tr.2014.2363152.\n\n\nRatner, Alex, Braden Hancock, Jared Dunnmon, Roger Goldman, and\nChristopher R√©. 2018. ‚ÄúSnorkel MeTaL: Weak\nSupervision for Multi-Task Learning.‚Äù In Proceedings of the\nSecond Workshop on Data Management for End-to-End Machine Learning.\nACM. https://doi.org/10.1145/3209889.3209898.\n\n\nReagen, Brandon, Udit Gupta, Lillian Pentecost, Paul Whatmough, Sae Kyu\nLee, Niamh Mulholland, David Brooks, and Gu-Yeon Wei. 2018. ‚ÄúAres:\nA Framework for Quantifying the Resilience of Deep Neural\nNetworks.‚Äù In 2018 55th ACM/ESDA/IEEE Design Automation\nConference (DAC), 1‚Äì6. IEEE. https://doi.org/10.1109/dac.2018.8465834.\n\n\nReagen, Brandon, Jose Miguel Hernandez-Lobato, Robert Adolf, Michael\nGelbart, Paul Whatmough, Gu-Yeon Wei, and David Brooks. 2017. ‚ÄúA\nCase for Efficient Accelerator Design Space Exploration via\nBayesian Optimization.‚Äù In 2017 IEEE/ACM\nInternational Symposium on Low Power Electronics and Design\n(ISLPED), 1‚Äì6. IEEE; IEEE. https://doi.org/10.1109/islped.2017.8009208.\n\n\nReddi, Sashank J., Satyen Kale, and Sanjiv Kumar. 2019. ‚ÄúOn the\nConvergence of Adam and Beyond.‚Äù arXiv Preprint\narXiv:1904.09237, April. http://arxiv.org/abs/1904.09237v1.\n\n\nReddi, Vijay Janapa, Christine Cheng, David Kanter, Peter Mattson,\nGuenther Schmuelling, Carole-Jean Wu, Brian Anderson, et al. 2020.\n‚ÄúMLPerf Inference Benchmark.‚Äù In 2020\nACM/IEEE 47th Annual International Symposium on Computer Architecture\n(ISCA), 446‚Äì59. IEEE; IEEE. https://doi.org/10.1109/isca45697.2020.00045.\n\n\nReddi, Vijay Janapa, and Meeta Sharma Gupta. 2013. Resilient\nArchitecture Design for Voltage Variation. Springer International\nPublishing. https://doi.org/10.1007/978-3-031-01739-1.\n\n\nReis, G. A., J. Chang, N. Vachharajani, R. Rangan, and D. I. August.\n2005. ‚ÄúSWIFT: Software Implemented Fault\nTolerance.‚Äù In International Symposium on Code Generation and\nOptimization, 243‚Äì54. IEEE; IEEE. https://doi.org/10.1109/cgo.2005.34.\n\n\nRibeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. 2016.\n‚Äú‚Äù Why Should i Trust You?‚Äù Explaining\nthe Predictions of Any Classifier.‚Äù In Proceedings of the\n22nd ACM SIGKDD International Conference on Knowledge Discovery and Data\nMining, 1135‚Äì44.\n\n\nRobbins, Herbert, and Sutton Monro. 1951. ‚ÄúA Stochastic\nApproximation Method.‚Äù The Annals of Mathematical\nStatistics 22 (3): 400‚Äì407. https://doi.org/10.1214/aoms/1177729586.\n\n\nRombach, Robin, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and\nBjorn Ommer. 2022. ‚ÄúHigh-Resolution Image Synthesis with Latent\nDiffusion Models.‚Äù In 2022 IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR). IEEE. https://doi.org/10.1109/cvpr52688.2022.01042.\n\n\nRomero, Francisco, Qian Li 0027, Neeraja J. Yadwadkar, and Christos\nKozyrakis. 2021. ‚ÄúINFaaS: Automated Model-Less Inference\nServing.‚Äù In 2021 USENIX Annual Technical Conference (USENIX\nATC 21), 397‚Äì411. https://www.usenix.org/conference/atc21/presentation/romero.\n\n\nRosenblatt, Frank. 1957. The Perceptron, a Perceiving and\nRecognizing Automaton Project Para. Cornell Aeronautical\nLaboratory.\n\n\nRoskies, Adina. 2002. ‚ÄúNeuroethics for the New Millenium.‚Äù\nNeuron 35 (1): 21‚Äì23. https://doi.org/10.1016/s0896-6273(02)00763-8.\n\n\nRuder, Sebastian. 2016. ‚ÄúAn Overview of Gradient Descent\nOptimization Algorithms.‚Äù ArXiv Preprint abs/1609.04747\n(September). http://arxiv.org/abs/1609.04747v2.\n\n\nRudin, Cynthia. 2019. ‚ÄúStop Explaining Black Box Machine Learning\nModels for High Stakes Decisions and Use Interpretable Models\nInstead.‚Äù Nature Machine Intelligence 1 (5): 206‚Äì15. https://doi.org/10.1038/s42256-019-0048-x.\n\n\nRumelhart, David E., Geoffrey E. Hinton, and Ronald J. Williams. 1986.\n‚ÄúLearning Representations by Back-Propagating Errors.‚Äù\nNature 323 (6088): 533‚Äì36. https://doi.org/10.1038/323533a0.\n\n\nRussakovsky, Olga, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh,\nSean Ma, Zhiheng Huang, et al. 2015. ‚ÄúImageNet Large\nScale Visual Recognition Challenge.‚Äù Int. J. Comput.\nVision 115 (3): 211‚Äì52. https://doi.org/10.1007/s11263-015-0816-y.\n\n\nRussell, Stuart. 2021. ‚ÄúHuman-Compatible Artificial\nIntelligence.‚Äù Human-Like Machine Intelligence, 3‚Äì23.\n\n\nRyan, Richard M., and Edward L. Deci. 2000. ‚ÄúSelf-Determination\nTheory and the Facilitation of Intrinsic Motivation, Social Development,\nand Well-Being.‚Äù Am. Psychol. 55 (1): 68‚Äì78. https://doi.org/10.1037/0003-066x.55.1.68.\n\n\nSamajdar, Ananda, Yuhao Zhu, Paul Whatmough, Matthew Mattina, and Tushar\nKrishna. 2018. ‚ÄúScale-Sim: Systolic Cnn Accelerator\nSimulator.‚Äù ArXiv Preprint abs/1811.02883. https://arxiv.org/abs/1811.02883.\n\n\nSambasivan, Nithya, Shivani Kapania, Hannah Highfill, Diana Akrong,\nPraveen Paritosh, and Lora M Aroyo. 2021a.\n‚Äú‚ÄúEveryone Wants to Do the Model Work,\nNot the Data Work‚Äù: Data Cascades in\nHigh-Stakes AI.‚Äù In Proceedings of the 2021 CHI\nConference on Human Factors in Computing Systems, 1‚Äì15.\n\n\n‚Äî‚Äî‚Äî. 2021b. ‚Äú‚ÄòEveryone Wants to Do the Model Work, Not the\nData Work‚Äô: Data Cascades in High-Stakes AI.‚Äù In\nProceedings of the 2021 CHI Conference on Human Factors in Computing\nSystems. ACM. https://doi.org/10.1145/3411764.3445518.\n\n\nSangchoolie, Behrooz, Karthik Pattabiraman, and Johan Karlsson. 2017.\n‚ÄúOne Bit Is (Not) Enough: An Empirical\nStudy of the Impact of Single and Multiple Bit-Flip Errors.‚Äù In\n2017 47th Annual IEEE/IFIP International Conference on Dependable\nSystems and Networks (DSN), 97‚Äì108. IEEE; IEEE. https://doi.org/10.1109/dsn.2017.30.\n\n\nSch√§fer, Mike S. 2023. ‚ÄúThe Notorious GPT:\nScience Communication in the Age of Artificial\nIntelligence.‚Äù Journal of Science Communication 22 (02):\nY02. https://doi.org/10.22323/2.22020402.\n\n\nSchizas, Nikolaos, Aristeidis Karras, Christos Karras, and Spyros\nSioutas. 2022. ‚ÄúTinyML for Ultra-Low Power\nAI and Large Scale IoT Deployments:\nA Systematic Review.‚Äù Future Internet 14\n(12): 363. https://doi.org/10.3390/fi14120363.\n\n\nSchuman, Catherine D., Shruti R. Kulkarni, Maryam Parsa, J. Parker\nMitchell, Prasanna Date, and Bill Kay. 2022. ‚ÄúOpportunities for\nNeuromorphic Computing Algorithms and Applications.‚Äù Nature\nComputational Science 2 (1): 10‚Äì19. https://doi.org/10.1038/s43588-021-00184-y.\n\n\nSchwartz, Daniel, Jonathan Michael Gomes Selman, Peter Wrege, and\nAndreas Paepcke. 2021. ‚ÄúDeployment of Embedded\nEdge-AI for Wildlife Monitoring in Remote Regions.‚Äù\nIn 2021 20th IEEE International Conference on Machine Learning and\nApplications (ICMLA), 1035‚Äì42. IEEE; IEEE. https://doi.org/10.1109/icmla52953.2021.00170.\n\n\nSchwartz, Roy, Jesse Dodge, Noah A. Smith, and Oren Etzioni. 2020.\n‚ÄúGreen AI.‚Äù Commun. ACM 63 (12):\n54‚Äì63. https://doi.org/10.1145/3381831.\n\n\nSegal, Mark, and Kurt Akeley. 1999. ‚ÄúThe OpenGL\nGraphics System: A Specification (Version 1.1).‚Äù\n\n\nSegura Anaya, L. H., Abeer Alsadoon, N. Costadopoulos, and P. W. C.\nPrasad. 2017. ‚ÄúEthical Implications of User Perceptions of\nWearable Devices.‚Äù Sci. Eng. Ethics 24 (1): 1‚Äì28. https://doi.org/10.1007/s11948-017-9872-8.\n\n\nSeide, Frank, and Amit Agarwal. 2016. ‚ÄúCntk: Microsoft‚Äôs\nOpen-Source Deep-Learning Toolkit.‚Äù In Proceedings of the\n22nd ACM SIGKDD International Conference on Knowledge Discovery and Data\nMining, 2135‚Äì35. ACM. https://doi.org/10.1145/2939672.2945397.\n\n\nSelvaraju, Ramprasaath R., Michael Cogswell, Abhishek Das, Ramakrishna\nVedantam, Devi Parikh, and Dhruv Batra. 2017.\n‚ÄúGrad-CAM: Visual Explanations from Deep\nNetworks via Gradient-Based Localization.‚Äù In 2017 IEEE\nInternational Conference on Computer Vision (ICCV), 618‚Äì26. IEEE.\nhttps://doi.org/10.1109/iccv.2017.74.\n\n\nSeong, Nak Hee, Dong Hyuk Woo, Vijayalakshmi Srinivasan, Jude A. Rivers,\nand Hsien-Hsin S. Lee. 2010. ‚ÄúSAFER: Stuck-at-fault Error Recovery for\nMemories.‚Äù In 2010 43rd Annual IEEE/ACM International\nSymposium on Microarchitecture, 115‚Äì24. IEEE; IEEE. https://doi.org/10.1109/micro.2010.46.\n\n\nSeyedzadeh, Saleh, Farzad Pour Rahimian, Ivan Glesk, and Marc Roper.\n2018. ‚ÄúMachine Learning for Estimation of Building Energy\nConsumption and Performance: A Review.‚Äù\nVisualization in Engineering 6 (1): 1‚Äì20. https://doi.org/10.1186/s40327-018-0064-7.\n\n\nShalev-Shwartz, Shai, Shaked Shammah, and Amnon Shashua. 2017. ‚ÄúOn\na Formal Model of Safe and Scalable Self-Driving Cars.‚Äù ArXiv\nPreprint abs/1708.06374. https://arxiv.org/abs/1708.06374.\n\n\nShan, Shawn, Wenxin Ding, Josephine Passananti, Haitao Zheng, and Ben Y\nZhao. 2023. ‚ÄúPrompt-Specific Poisoning Attacks on Text-to-Image\nGenerative Models.‚Äù ArXiv Preprint abs/2310.13828. https://arxiv.org/abs/2310.13828.\n\n\nShastri, Bhavin J., Alexander N. Tait, T. Ferreira de Lima, Wolfram H.\nP. Pernice, Harish Bhaskaran, C. D. Wright, and Paul R. Prucnal. 2021.\n‚ÄúPhotonics for Artificial Intelligence and Neuromorphic\nComputing.‚Äù Nat. Photonics 15 (2): 102‚Äì14. https://doi.org/10.1038/s41566-020-00754-y.\n\n\nSheaffer, Jeremy W, David P Luebke, and Kevin Skadron. 2007. ‚ÄúA\nHardware Redundancy and Recovery Mechanism for Reliable Scientific\nComputation on Graphics Processors.‚Äù In Graphics\nHardware, 2007:55‚Äì64. Citeseer.\n\n\nShehabi, Arman, Sarah Smith, Dale Sartor, Richard Brown, Magnus Herrlin,\nJonathan Koomey, Eric Masanet, Nathaniel Horner, In√™s Azevedo, and\nWilliam Lintner. 2016. ‚ÄúUnited States Data Center Energy Usage\nReport.‚Äù\n\n\nShen, Sheng, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami,\nMichael W. Mahoney, and Kurt Keutzer. 2020. ‚ÄúQ-BERT:\nHessian Based Ultra Low Precision Quantization of\nBERT.‚Äù Proceedings of the AAAI Conference on\nArtificial Intelligence 34 (05): 8815‚Äì21. https://doi.org/10.1609/aaai.v34i05.6409.\n\n\nSheng, Victor S., and Jing Zhang. 2019. ‚ÄúMachine Learning with\nCrowdsourcing: A Brief Summary of the Past Research and\nFuture Directions.‚Äù Proceedings of the AAAI Conference on\nArtificial Intelligence 33 (01): 9837‚Äì43. https://doi.org/10.1609/aaai.v33i01.33019837.\n\n\nShi, Hongrui, and Valentin Radu. 2022. ‚ÄúData Selection for\nEfficient Model Update in Federated Learning.‚Äù In Proceedings\nof the 2nd European Workshop on Machine Learning and Systems,\n72‚Äì78. ACM. https://doi.org/10.1145/3517207.3526980.\n\n\nShneiderman, Ben. 2020. ‚ÄúBridging the Gap Between Ethics and\nPractice: Guidelines for Reliable, Safe, and Trustworthy Human-Centered\nAI Systems.‚Äù ACM Trans. Interact. Intell. Syst. 10 (4):\n1‚Äì31. https://doi.org/10.1145/3419764.\n\n\n‚Äî‚Äî‚Äî. 2022. Human-Centered AI. Oxford University\nPress.\n\n\nShokri, Reza, Marco Stronati, Congzheng Song, and Vitaly Shmatikov.\n2017. ‚ÄúMembership Inference Attacks Against Machine Learning\nModels.‚Äù In 2017 IEEE Symposium on Security and Privacy\n(SP), 3‚Äì18. IEEE; IEEE. https://doi.org/10.1109/sp.2017.41.\n\n\nSiddik, Md Abu Bakar, Arman Shehabi, and Landon Marston. 2021.\n‚ÄúThe Environmental Footprint of Data Centers in the United\nStates.‚Äù Environ. Res. Lett. 16 (6): 064017. https://doi.org/10.1088/1748-9326/abfba1.\n\n\nSilvestro, Daniele, Stefano Goria, Thomas Sterner, and Alexandre\nAntonelli. 2022. ‚ÄúImproving Biodiversity Protection Through\nArtificial Intelligence.‚Äù Nature Sustainability 5 (5):\n415‚Äì24. https://doi.org/10.1038/s41893-022-00851-6.\n\n\nSingh, Narendra, and Oladele A. Ogunseitan. 2022. ‚ÄúDisentangling\nthe Worldwide Web of e-Waste and Climate Change Co-Benefits.‚Äù\nCircular Economy 1 (2): 100011. https://doi.org/10.1016/j.cec.2022.100011.\n\n\nSkorobogatov, Sergei. 2009. ‚ÄúLocal Heating Attacks on Flash Memory\nDevices.‚Äù In 2009 IEEE International Workshop on\nHardware-Oriented Security and Trust, 1‚Äì6. IEEE; IEEE. https://doi.org/10.1109/hst.2009.5225028.\n\n\nSkorobogatov, Sergei P., and Ross J. Anderson. 2002. ‚ÄúOptical\nFault Induction Attacks.‚Äù In Cryptographic Hardware and\nEmbedded Systems-CHES 2002: 4th International Workshop Redwood Shores,\nCA, USA, August 13‚Äì15, 2002 Revised Papers 4, 2‚Äì12. Springer. https://doi.org/10.1007/3-540-36400-5\\_2.\n\n\nSmilkov, Daniel, Nikhil Thorat, Been Kim, Fernanda Vi√©gas, and Martin\nWattenberg. 2017. ‚ÄúSmoothgrad: Removing Noise by\nAdding Noise.‚Äù ArXiv Preprint abs/1706.03825. https://arxiv.org/abs/1706.03825.\n\n\nSnoek, Jasper, Hugo Larochelle, and Ryan P. Adams. 2012.\n‚ÄúPractical Bayesian Optimization of Machine Learning\nAlgorithms.‚Äù In Advances in Neural Information Processing\nSystems 25: 26th Annual Conference on Neural Information Processing\nSystems 2012. Proceedings of a Meeting Held December 3-6, 2012, Lake\nTahoe, Nevada, United States, edited by Peter L. Bartlett, Fernando\nC. N. Pereira, Christopher J. C. Burges, L√©on Bottou, and Kilian Q.\nWeinberger, 2960‚Äì68. https://proceedings.neurips.cc/paper/2012/hash/05311655a15b75fab86956663e1819cd-Abstract.html.\n\n\nSrivastava, Nitish, Geoffrey E. Hinton, Alex Krizhevsky, Ilya Sutskever,\nand Ruslan Salakhutdinov. 2014. ‚ÄúDropout: A Simple Way to Prevent\nNeural Networks from Overfitting.‚Äù J. Mach. Learn. Res.\n15 (1): 1929‚Äì58. https://doi.org/10.5555/2627435.2670313.\n\n\nStrubell, Emma, Ananya Ganesh, and Andrew McCallum. 2019. ‚ÄúEnergy\nand Policy Considerations for Deep Learning in NLP.‚Äù\nIn Proceedings of the 57th Annual Meeting of the Association for\nComputational Linguistics, 3645‚Äì50. Florence, Italy: Association\nfor Computational Linguistics. https://doi.org/10.18653/v1/p19-1355.\n\n\nSuda, Naveen, Vikas Chandra, Ganesh Dasika, Abinash Mohanty, Yufei Ma,\nSarma Vrudhula, Jae-sun Seo, and Yu Cao. 2016.\n‚ÄúThroughput-Optimized OpenCL-Based FPGA\nAccelerator for Large-Scale Convolutional Neural Networks.‚Äù In\nProceedings of the 2016 ACM/SIGDA International Symposium on\nField-Programmable Gate Arrays, 16‚Äì25. ACM. https://doi.org/10.1145/2847263.2847276.\n\n\nSudhakar, Soumya, Vivienne Sze, and Sertac Karaman. 2023. ‚ÄúData\nCenters on Wheels: Emissions from Computing Onboard\nAutonomous Vehicles.‚Äù IEEE Micro 43 (1): 29‚Äì39. https://doi.org/10.1109/mm.2022.3219803.\n\n\nSze, Vivienne, Yu-Hsin Chen, Tien-Ju Yang, and Joel S. Emer. 2017.\n‚ÄúEfficient Processing of Deep Neural Networks: A\nTutorial and Survey.‚Äù Proc. IEEE 105 (12): 2295‚Äì2329. https://doi.org/10.1109/jproc.2017.2761740.\n\n\nSzegedy, Christian, Wojciech Zaremba, Ilya Sutskever, Joan Bruna,\nDumitru Erhan, Ian J. Goodfellow, and Rob Fergus. 2014.\n‚ÄúIntriguing Properties of Neural Networks.‚Äù In 2nd\nInternational Conference on Learning Representations, ICLR 2014, Banff,\nAB, Canada, April 14-16, 2014, Conference Track Proceedings, edited\nby Yoshua Bengio and Yann LeCun. http://arxiv.org/abs/1312.6199.\n\n\nTambe, Thierry, En-Yu Yang, Zishen Wan, Yuntian Deng, Vijay Janapa\nReddi, Alexander Rush, David Brooks, and Gu-Yeon Wei. 2020.\n‚ÄúAlgorithm-Hardware Co-Design of Adaptive Floating-Point Encodings\nfor Resilient Deep Learning Inference.‚Äù In 2020 57th ACM/IEEE\nDesign Automation Conference (DAC), 1‚Äì6. IEEE; IEEE. https://doi.org/10.1109/dac18072.2020.9218516.\n\n\nTan, Mingxing, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler,\nAndrew Howard, and Quoc V. Le. 2019. ‚ÄúMnasNet: Platform-aware Neural Architecture Search for\nMobile.‚Äù In 2019 IEEE/CVF Conference on Computer Vision and\nPattern Recognition (CVPR), 2820‚Äì28. IEEE. https://doi.org/10.1109/cvpr.2019.00293.\n\n\nTan, Mingxing, and Quoc V. Le. 2023. ‚ÄúDemystifying Deep\nLearning.‚Äù Wiley. https://doi.org/10.1002/9781394205639.ch6.\n\n\nTang, Xin, Yichun He, and Jia Liu. 2022. ‚ÄúSoft Bioelectronics for\nCardiac Interfaces.‚Äù Biophysics Reviews 3 (1). https://doi.org/10.1063/5.0069516.\n\n\nTang, Xin, Hao Shen, Siyuan Zhao, Na Li, and Jia Liu. 2023.\n‚ÄúFlexible Braincomputer Interfaces.‚Äù\nNature Electronics 6 (2): 109‚Äì18. https://doi.org/10.1038/s41928-022-00913-9.\n\n\nTarun, Ayush K, Vikram S Chundawat, Murari Mandal, and Mohan\nKankanhalli. 2022. ‚ÄúDeep Regression Unlearning.‚Äù ArXiv\nPreprint abs/2210.08196 (October). http://arxiv.org/abs/2210.08196v2.\n\n\nTeam, The Theano Development, Rami Al-Rfou, Guillaume Alain, Amjad\nAlmahairi, Christof Angermueller, Dzmitry Bahdanau, Nicolas Ballas, et\nal. 2016. ‚ÄúTheano: A Python Framework for Fast\nComputation of Mathematical Expressions.‚Äù https://arxiv.org/abs/1605.02688.\n\n\n‚ÄúThe Ultimate Guide to Deep Learning Model Quantization and\nQuantization-Aware Training.‚Äù n.d. https://deci.ai/quantization-and-quantization-aware-training/.\n\n\nThompson, Neil C., Kristjan Greenewald, Keeheon Lee, and Gabriel F.\nManso. 2021. ‚ÄúDeep Learning‚Äôs Diminishing Returns:\nThe Cost of Improvement Is Becoming Unsustainable.‚Äù\nIEEE Spectr. 58 (10): 50‚Äì55. https://doi.org/10.1109/mspec.2021.9563954.\n\n\nTill, Aaron, Andrew L. Rypel, Andrew Bray, and Samuel B. Fey. 2019.\n‚ÄúFish Die-Offs Are Concurrent with Thermal Extremes in North\nTemperate Lakes.‚Äù Nat. Clim. Change 9 (8): 637‚Äì41. https://doi.org/10.1038/s41558-019-0520-y.\n\n\nTirtalistyani, Rose, Murtiningrum Murtiningrum, and Rameshwar S. Kanwar.\n2022. ‚ÄúIndonesia Rice Irrigation System:\nTime for Innovation.‚Äù Sustainability 14\n(19): 12477. https://doi.org/10.3390/su141912477.\n\n\nTokui, Seiya, Ryosuke Okuta, Takuya Akiba, Yusuke Niitani, Toru Ogawa,\nShunta Saito, Shuji Suzuki, Kota Uenishi, Brian Vogel, and Hiroyuki\nYamazaki Vincent. 2019. ‚ÄúChainer: A Deep Learning Framework for\nAccelerating the Research Cycle.‚Äù In Proceedings of the 25th\nACM SIGKDD International Conference on Knowledge Discovery &Amp;\nData Mining, 5:1‚Äì6. ACM. https://doi.org/10.1145/3292500.3330756.\n\n\nTram√®r, Florian, Pascal Dupr√©, Gili Rusak, Giancarlo Pellegrino, and Dan\nBoneh. 2019. ‚ÄúAdVersarial: Perceptual Ad Blocking\nMeets Adversarial Machine Learning.‚Äù In Proceedings of the\n2019 ACM SIGSAC Conference on Computer and Communications Security,\n2005‚Äì21. ACM. https://doi.org/10.1145/3319535.3354222.\n\n\nTran, Cuong, Ferdinando Fioretto, Jung-Eun Kim, and Rakshit Naidu. 2022.\n‚ÄúPruning Has a Disparate Impact on Model Accuracy.‚Äù Adv\nNeural Inf Process Syst 35: 17652‚Äì64.\n\n\nTsai, Min-Jen, Ping-Yi Lin, and Ming-En Lee. 2023. ‚ÄúAdversarial\nAttacks on Medical Image Classification.‚Äù Cancers 15\n(17): 4228. https://doi.org/10.3390/cancers15174228.\n\n\nTsai, Timothy, Siva Kumar Sastry Hari, Michael Sullivan, Oreste Villa,\nand Stephen W. Keckler. 2021. ‚ÄúNVBitFI:\nDynamic Fault Injection for GPUs.‚Äù In\n2021 51st Annual IEEE/IFIP International Conference on Dependable\nSystems and Networks (DSN), 284‚Äì91. IEEE; IEEE. https://doi.org/10.1109/dsn48987.2021.00041.\n\n\nTschand, Arya, Arun Tejusve Raghunath Rajan, Sachin Idgunji, Anirban\nGhosh, Jeremy Holleman, Csaba Kiraly, Pawan Ambalkar, et al. 2024.\n‚ÄúMLPerf Power: Benchmarking the Energy Efficiency of Machine\nLearning Systems from Microwatts to Megawatts for Sustainable\nAI.‚Äù arXiv Preprint arXiv:2410.12032, October. http://arxiv.org/abs/2410.12032v1.\n\n\nUddin, Mueen, and Azizah Abdul Rahman. 2012. ‚ÄúEnergy Efficiency\nand Low Carbon Enabler Green IT Framework for Data Centers\nConsidering Green Metrics.‚Äù Renewable Sustainable Energy\nRev. 16 (6): 4078‚Äì94. https://doi.org/10.1016/j.rser.2012.03.014.\n\n\nUn, and World Economic Forum. 2019. A New Circular Vision for\nElectronics, Time for a Global Reboot. PACE - Platform for\nAccelerating the Circular Economy. https://www3.weforum.org/docs/WEF\\_A\\_New\\_Circular\\_Vision\\_for\\_Electronics.pdf.\n\n\nValenzuela, Christine L, and Pearl Y Wang. 2000. ‚ÄúA Genetic\nAlgorithm for VLSI Floorplanning.‚Äù In Parallel\nProblem Solving from Nature PPSN VI: 6th International Conference Paris,\nFrance, September 1820, 2000 Proceedings 6, 671‚Äì80.\nSpringer.\n\n\nVan Noorden, Richard. 2016. ‚ÄúArXiv Preprint Server\nPlans Multimillion-Dollar Overhaul.‚Äù Nature 534 (7609):\n602‚Äì2. https://doi.org/10.1038/534602a.\n\n\nVangal, Sriram, Somnath Paul, Steven Hsu, Amit Agarwal, Saurabh Kumar,\nRam Krishnamurthy, Harish Krishnamurthy, James Tschanz, Vivek De, and\nChris H. Kim. 2021. ‚ÄúWide-Range Many-Core SoC Design\nin Scaled CMOS: Challenges and\nOpportunities.‚Äù IEEE Trans. Very Large Scale Integr. VLSI\nSyst. 29 (5): 843‚Äì56. https://doi.org/10.1109/tvlsi.2021.3061649.\n\n\nVaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion\nJones, Aidan N Gomez, ≈Åukasz Kaiser, and Illia Polosukhin. 2017.\n‚ÄúAttention Is All You Need.‚Äù Adv Neural Inf Process\nSyst 30.\n\n\n‚ÄúVector-Borne Diseases.‚Äù n.d.\nhttps://www.who.int/news-room/fact-sheets/detail/vector-borne-diseases.\n\n\nVelazco, Raoul, Gilles Foucard, and Paul Peronnard. 2010.\n‚ÄúCombining Results of Accelerated Radiation Tests and Fault\nInjections to Predict the Error Rate of an Application Implemented in\nSRAM-Based FPGAs.‚Äù IEEE Trans.\nNucl. Sci. 57 (6): 3500‚Äì3505. https://doi.org/10.1109/tns.2010.2087355.\n\n\nVerma, Naveen, Hongyang Jia, Hossein Valavi, Yinqi Tang, Murat Ozatay,\nLung-Yen Chen, Bonan Zhang, and Peter Deaville. 2019. ‚ÄúIn-Memory\nComputing: Advances and Prospects.‚Äù IEEE\nSolid-State Circuits Mag. 11 (3): 43‚Äì55. https://doi.org/10.1109/mssc.2019.2922889.\n\n\nVerma, Team Dual_Boot: Swapnil. 2022. ‚ÄúElephant\nAI.‚Äù Hackster.io. https://www.hackster.io/dual\\_boot/elephant-ai-ba71e9.\n\n\nVinuesa, Ricardo, Hossein Azizpour, Iolanda Leite, Madeline Balaam,\nVirginia Dignum, Sami Domisch, Anna Fell√§nder, Simone Daniela Langhans,\nMax Tegmark, and Francesco Fuso Nerini. 2020. ‚ÄúThe Role of\nArtificial Intelligence in Achieving the Sustainable Development\nGoals.‚Äù Nat. Commun. 11 (1): 1‚Äì10. https://doi.org/10.1038/s41467-019-14108-y.\n\n\nVivet, Pascal, Eric Guthmuller, Yvain Thonnart, Gael Pillonnet, Cesar\nFuguet, Ivan Miro-Panades, Guillaume Moritz, et al. 2021.\n‚ÄúIntAct: A 96-Core Processor with Six\nChiplets 3D-Stacked on an Active Interposer with\nDistributed Interconnects and Integrated Power Management.‚Äù\nIEEE J. Solid-State Circuits 56 (1): 79‚Äì97. https://doi.org/10.1109/jssc.2020.3036341.\n\n\nWachter, Sandra, Brent Mittelstadt, and Chris Russell. 2017.\n‚ÄúCounterfactual Explanations Without Opening the Black Box:\nAutomated Decisions and the GDPR.‚Äù\nSSRN Electronic Journal 31: 841. https://doi.org/10.2139/ssrn.3063289.\n\n\nWald, Peter H., and Jeffrey R. Jones. 1987. ‚ÄúSemiconductor\nManufacturing: An Introduction to Processes and\nHazards.‚Äù Am. J. Ind. Med. 11 (2): 203‚Äì21. https://doi.org/10.1002/ajim.4700110209.\n\n\nWan, Zishen, Aqeel Anwar, Yu-Shun Hsiao, Tianyu Jia, Vijay Janapa Reddi,\nand Arijit Raychowdhury. 2021. ‚ÄúAnalyzing and Improving Fault\nTolerance of Learning-Based Navigation Systems.‚Äù In 2021 58th\nACM/IEEE Design Automation Conference (DAC), 841‚Äì46. IEEE; IEEE. https://doi.org/10.1109/dac18074.2021.9586116.\n\n\nWan, Zishen, Yiming Gan, Bo Yu, S Liu, A Raychowdhury, and Y Zhu. 2023.\n‚ÄúVpp: The Vulnerability-Proportional Protection\nParadigm Towards Reliable Autonomous Machines.‚Äù In\nProceedings of the 5th International Workshop on Domain Specific\nSystem Architecture (DOSSA), 1‚Äì6.\n\n\nWang, LingFeng, and YaQing Zhan. 2019a. ‚ÄúA Conceptual Peer Review\nModel for arXiv and Other Preprint\nDatabases.‚Äù Learn. Publ. 32 (3): 213‚Äì19. https://doi.org/10.1002/leap.1229.\n\n\n‚Äî‚Äî‚Äî. 2019b. ‚ÄúA Conceptual Peer Review Model for arXiv and Other Preprint Databases.‚Äù\nLearn. Publ. 32 (3): 213‚Äì19. https://doi.org/10.1002/leap.1229.\n\n\nWang, Tianzhe, Kuan Wang, Han Cai, Ji Lin, Zhijian Liu, Hanrui Wang,\nYujun Lin, and Song Han. 2020. ‚ÄúAPQ:\nJoint Search for Network Architecture, Pruning and\nQuantization Policy.‚Äù In 2020 IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), 2075‚Äì84. IEEE. https://doi.org/10.1109/cvpr42600.2020.00215.\n\n\nWarden, Pete. 2018. ‚ÄúSpeech Commands: A Dataset for\nLimited-Vocabulary Speech Recognition.‚Äù arXiv Preprint\narXiv:1804.03209.\n\n\nWarden, Pete, and Daniel Situnayake. 2019. Tinyml:\nMachine Learning with Tensorflow Lite on Arduino and\nUltra-Low-Power Microcontrollers. O‚ÄôReilly Media.\n\n\nWeik, Martin H. 1955. A Survey of Domestic Electronic Digital\nComputing Systems. Ballistic Research Laboratories.\n\n\nWess, Matthias, Matvey Ivanov, Christoph Unger, and Anvesh Nookala.\n2020. ‚ÄúANNETTE: Accurate Neural Network\nExecution Time Estimation with Stacked Models.‚Äù IEEE. https://doi.org/10.1109/ACCESS.2020.3047259.\n\n\nWiener, Norbert. 1960. ‚ÄúSome Moral and Technical Consequences of\nAutomation: As Machines Learn They May Develop Unforeseen Strategies at\nRates That Baffle Their Programmers.‚Äù Science 131\n(3410): 1355‚Äì58. https://doi.org/10.1126/science.131.3410.1355.\n\n\nWilkening, Mark, Vilas Sridharan, Si Li, Fritz Previlon, Sudhanva\nGurumurthi, and David R. Kaeli. 2014. ‚ÄúCalculating Architectural\nVulnerability Factors for Spatial Multi-Bit Transient Faults.‚Äù In\n2014 47th Annual IEEE/ACM International Symposium on\nMicroarchitecture, 293‚Äì305. IEEE; IEEE. https://doi.org/10.1109/micro.2014.15.\n\n\nWinkler, Harald, Franck Lecocq, Hans Lofgren, Maria Virginia Vilari√±o,\nSivan Kartha, and Joana Portugal-Pereira. 2022. ‚ÄúExamples of\nShifting Development Pathways: Lessons on How to Enable\nBroader, Deeper, and Faster Climate Action.‚Äù Climate\nAction 1 (1). https://doi.org/10.1007/s44168-022-00026-1.\n\n\nWong, H.-S. Philip, Heng-Yuan Lee, Shimeng Yu, Yu-Sheng Chen, Yi Wu,\nPang-Shiu Chen, Byoungil Lee, Frederick T. Chen, and Ming-Jinn Tsai.\n2012. ‚ÄúMetalOxide\nRRAM.‚Äù Proc. IEEE 100 (6): 1951‚Äì70. https://doi.org/10.1109/jproc.2012.2190369.\n\n\nWu, Bichen, Kurt Keutzer, Xiaoliang Dai, Peizhao Zhang, Yanghan Wang,\nFei Sun, Yiming Wu, Yuandong Tian, Peter Vajda, and Yangqing Jia. 2019.\n‚ÄúFBNet: Hardware-aware\nEfficient ConvNet Design via Differentiable Neural\nArchitecture Search.‚Äù In 2019 IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), 10734‚Äì42. IEEE. https://doi.org/10.1109/cvpr.2019.01099.\n\n\nWu, Carole-Jean, David Brooks, Kevin Chen, Douglas Chen, Sy Choudhury,\nMarat Dukhan, Kim Hazelwood, et al. 2019. ‚ÄúMachine Learning at\nFacebook: Understanding Inference at the Edge.‚Äù In 2019 IEEE\nInternational Symposium on High Performance Computer Architecture\n(HPCA), 331‚Äì44. IEEE; IEEE. https://doi.org/10.1109/hpca.2019.00048.\n\n\nWu, Carole-Jean, Ramya Raghavendra, Udit Gupta, Bilge Acun, Newsha\nArdalani, Kiwan Maeng, Gloria Chang, et al. 2022. ‚ÄúSustainable Ai:\nEnvironmental Implications, Challenges and\nOpportunities.‚Äù Proceedings of Machine Learning and\nSystems 4: 795‚Äì813.\n\n\nWu, Zhang Judd, and Micikevicius Isaev. 2020. ‚ÄúInteger\nQuantization for Deep Learning Inference: Principles and\nEmpirical Evaluation).‚Äù ArXiv Preprint. https://arxiv.org/abs/2004.09602.\n\n\nXiao, Seznec Lin, Demouth Wu, and Han. 2022.\n‚ÄúSmoothQuant: Accurate and Efficient\nPost-Training Quantization for Large Language Models.‚Äù ArXiv\nPreprint. https://arxiv.org/abs/2211.10438.\n\n\nXie, Cihang, Mingxing Tan, Boqing Gong, Jiang Wang, Alan L. Yuille, and\nQuoc V. Le. 2020. ‚ÄúAdversarial Examples Improve Image\nRecognition.‚Äù In 2020 IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR), 816‚Äì25. IEEE. https://doi.org/10.1109/cvpr42600.2020.00090.\n\n\nXie, Saining, Ross Girshick, Piotr Dollar, Zhuowen Tu, and Kaiming He.\n2017. ‚ÄúAggregated Residual Transformations for Deep Neural\nNetworks.‚Äù In 2017 IEEE Conference on Computer Vision and\nPattern Recognition (CVPR), 1492‚Äì1500. IEEE. https://doi.org/10.1109/cvpr.2017.634.\n\n\nXinyu, Chen. n.d.\n\n\nXiong, Siyu, Guoqing Wu, Xitian Fan, Xuan Feng, Zhongcheng Huang, Wei\nCao, Xuegong Zhou, et al. 2021. ‚ÄúMRI-Based Brain\nTumor Segmentation Using FPGA-Accelerated Neural\nNetwork.‚Äù BMC Bioinf. 22 (1): 421. https://doi.org/10.1186/s12859-021-04347-6.\n\n\nXiu, Liming. 2019. ‚ÄúTime Moore: Exploiting Moore‚Äôs Law from the Perspective of Time.‚Äù\nIEEE Solid-State Circuits Mag. 11 (1): 39‚Äì55. https://doi.org/10.1109/mssc.2018.2882285.\n\n\nXu, Chen, Jianqiang Yao, Zhouchen Lin, Wenwu Ou, Yuanbin Cao, Zhirong\nWang, and Hongbin Zha. 2018. ‚ÄúAlternating Multi-Bit Quantization\nfor Recurrent Neural Networks.‚Äù In 6th International\nConference on Learning Representations, ICLR 2018, Vancouver, BC,\nCanada, April 30 - May 3, 2018, Conference Track Proceedings.\nOpenReview.net. https://openreview.net/forum?id=S19dR9x0b.\n\n\nXu, Hu, Saining Xie, Xiaoqing Ellen Tan, Po-Yao Huang, Russell Howes,\nVasu Sharma, Shang-Wen Li, Gargi Ghosh, Luke Zettlemoyer, and Christoph\nFeichtenhofer. 2023. ‚ÄúDemystifying CLIP Data.‚Äù ArXiv\nPreprint abs/2309.16671 (September). http://arxiv.org/abs/2309.16671v4.\n\n\nXu, Ying, Xu Zhong, Antonio Jimeno Yepes, and Jey Han Lau. 2021.\n‚ÄúGrey-Box Adversarial Attack and Defence for\nSentiment Classification.‚Äù arXiv Preprint\narXiv:2103.11576.\n\n\nXu, Zheng, Yanxiang Zhang, Galen Andrew, Christopher A. Choquette-Choo,\nPeter Kairouz, H. Brendan McMahan, Jesse Rosenstock, and Yuanbo Zhang.\n2023. ‚ÄúFederated Learning of Gboard Language Models with\nDifferential Privacy.‚Äù ArXiv Preprint abs/2305.18465\n(May). http://arxiv.org/abs/2305.18465v2.\n\n\nYang, Tien-Ju, Yonghui Xiao, Giovanni Motta, Fran√ßoise Beaufays, Rajiv\nMathews, and Mingqing Chen. 2023. ‚ÄúOnline Model Compression for\nFederated Learning with Large Models.‚Äù In ICASSP 2023 - 2023\nIEEE International Conference on Acoustics, Speech and Signal Processing\n(ICASSP), 1‚Äì5. IEEE; IEEE. https://doi.org/10.1109/icassp49357.2023.10097124.\n\n\nYao, Zhewei, Zhen Dong, Zhangcheng Zheng, Amir Gholami, Jiali Yu, Eric\nTan, Leyuan Wang, et al. 2021. ‚ÄúHawq-V3: Dyadic\nNeural Network Quantization.‚Äù In International Conference on\nMachine Learning, 11875‚Äì86. PMLR.\n\n\nYe, Linfeng, and Shayan Mohajer Hamidi. 2021. ‚ÄúThundernna:\nA White Box Adversarial Attack.‚Äù arXiv Preprint\narXiv:2111.12305.\n\n\nYeh, Y. C. 1996. ‚ÄúTriple-Triple Redundant 777 Primary Flight\nComputer.‚Äù In 1996 IEEE Aerospace Applications Conference.\nProceedings, 1:293‚Äì307. IEEE; IEEE. https://doi.org/10.1109/aero.1996.495891.\n\n\nYik, Jason, Korneel Van den Berghe, Douwe den Blanken, Younes Bouhadjar,\nMaxime Fabre, Paul Hueber, Denis Kleyko, et al. 2023. ‚ÄúNeuroBench:\nA Framework for Benchmarking Neuromorphic Computing Algorithms and\nSystems,‚Äù April. http://arxiv.org/abs/2304.04640v3.\n\n\nYou, Jie, Jae-Won Chung, and Mosharaf Chowdhury. 2023. ‚ÄúZeus:\nUnderstanding and Optimizing GPU Energy\nConsumption of DNN Training.‚Äù In 20th USENIX\nSymposium on Networked Systems Design and Implementation (NSDI 23),\n119‚Äì39. Boston, MA: USENIX Association. https://www.usenix.org/conference/nsdi23/presentation/you.\n\n\nYou, Yang, Zhao Zhang, Cho-Jui Hsieh, James Demmel, and Kurt Keutzer.\n2017. ‚ÄúImageNet Training in Minutes,‚Äù September. http://arxiv.org/abs/1709.05011v10.\n\n\nYoung, Tom, Devamanyu Hazarika, Soujanya Poria, and Erik Cambria. 2018.\n‚ÄúRecent Trends in Deep Learning Based Natural Language Processing\n[Review Article].‚Äù IEEE Comput. Intell.\nMag. 13 (3): 55‚Äì75. https://doi.org/10.1109/mci.2018.2840738.\n\n\nYu, Yuan, Martƒ±ÃÅn Abadi, Paul Barham, Eugene Brevdo, Mike Burrows, Andy\nDavis, Jeff Dean, et al. 2018. ‚ÄúDynamic Control Flow in\nLarge-Scale Machine Learning.‚Äù In Proceedings of the\nThirteenth EuroSys Conference, 265‚Äì83. ACM. https://doi.org/10.1145/3190508.3190551.\n\n\nZafrir, Ofir, Guy Boudoukh, Peter Izsak, and Moshe Wasserblat. 2019.\n‚ÄúQ8BERT: Quantized 8Bit\nBERT.‚Äù In 2019 Fifth Workshop on Energy\nEfficient Machine Learning and Cognitive Computing - NeurIPS Edition\n(EMC2-NIPS), 36‚Äì39. IEEE; IEEE. https://doi.org/10.1109/emc2-nips53020.2019.00016.\n\n\nZeiler, Matthew D. 2012. ‚ÄúADADELTA: An Adaptive Learning Rate\nMethod,‚Äù December, 119‚Äì49. https://doi.org/10.1002/9781118266502.ch6.\n\n\nZennaro, Marco, Brian Plancher, and V Janapa Reddi. 2022.\n‚ÄúTinyML: Applied AI for\nDevelopment.‚Äù In The UN 7th Multi-Stakeholder Forum on\nScience, Technology and Innovation for the Sustainable Development\nGoals, 2022‚Äì05.\n\n\nZhang, Chengliang, Minchen Yu, Wei Wang 0030, and Feng Yan 0001. 2019.\n‚ÄúMArk: Exploiting Cloud Services for Cost-Effective, SLO-Aware\nMachine Learning Inference Serving.‚Äù In 2019 USENIX Annual\nTechnical Conference (USENIX ATC 19), 1049‚Äì62. https://www.usenix.org/conference/atc19/presentation/zhang-chengliang.\n\n\nZhang, Chen, Peng Li, Guangyu Sun, Yijin Guan, Bingjun Xiao, and Jason\nOptimizing Cong. 2015. ‚ÄúFPGA-Based Accelerator Design\nfor Deep Convolutional Neural Networks Proceedings of the 2015\nACM.‚Äù In SIGDA International Symposium on\nField-Programmable Gate Arrays-FPGA, 15:161‚Äì70.\n\n\nZhang, Dan, Safeen Huda, Ebrahim Songhori, Kartik Prabhu, Quoc Le, Anna\nGoldie, and Azalia Mirhoseini. 2022. ‚ÄúA Full-Stack Search\nTechnique for Domain Optimized Deep Learning Accelerators.‚Äù In\nProceedings of the 27th ACM International Conference on\nArchitectural Support for Programming Languages and Operating\nSystems, 27‚Äì42. ASPLOS ‚Äô22. New York, NY, USA: ACM. https://doi.org/10.1145/3503222.3507767.\n\n\nZhang, Dongxia, Xiaoqing Han, and Chunyu Deng. 2018. ‚ÄúReview on\nthe Research and Practice of Deep Learning and Reinforcement Learning in\nSmart Grids.‚Äù CSEE Journal of Power and Energy Systems 4\n(3): 362‚Äì70. https://doi.org/10.17775/cseejpes.2018.00520.\n\n\nZhang, Hongyu. 2008. ‚ÄúOn the Distribution of Software\nFaults.‚Äù IEEE Trans. Software Eng. 34 (2): 301‚Äì2. https://doi.org/10.1109/tse.2007.70771.\n\n\nZhang, Jeff Jun, Tianyu Gu, Kanad Basu, and Siddharth Garg. 2018.\n‚ÄúAnalyzing and Mitigating the Impact of Permanent Faults on a\nSystolic Array Based Neural Network Accelerator.‚Äù In 2018\nIEEE 36th VLSI Test Symposium (VTS), 1‚Äì6. IEEE; IEEE. https://doi.org/10.1109/vts.2018.8368656.\n\n\nZhang, Jeff, Kartheek Rangineni, Zahra Ghodsi, and Siddharth Garg. 2018.\n‚ÄúThUnderVolt: Enabling Aggressive\nVoltage Underscaling and Timing Error Resilience for Energy Efficient\nDeep Learning Accelerators.‚Äù In 2018 55th ACM/ESDA/IEEE\nDesign Automation Conference (DAC), 1‚Äì6. IEEE. https://doi.org/10.1109/dac.2018.8465918.\n\n\nZhang, Li Lyna, Yuqing Yang, Yuhang Jiang, Wenwu Zhu, and Yunxin Liu.\n2020. ‚ÄúFast Hardware-Aware Neural Architecture Search.‚Äù In\n2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition\nWorkshops (CVPRW). IEEE. https://doi.org/10.1109/cvprw50498.2020.00354.\n\n\nZhang, Qingxue, Dian Zhou, and Xuan Zeng. 2017. ‚ÄúHighly Wearable\nCuff-Less Blood Pressure and Heart Rate Monitoring with Single-Arm\nElectrocardiogram and Photoplethysmogram Signals.‚Äù BioMedical\nEngineering OnLine 16 (1): 23. https://doi.org/10.1186/s12938-017-0317-z.\n\n\nZhang, Tunhou, Hsin-Pai Cheng, Zhenwen Li, Feng Yan, Chengyu Huang, Hai\nHelen Li, and Yiran Chen. 2020. ‚ÄúAutoShrink:\nA Topology-Aware NAS for Discovering Efficient\nNeural Architecture.‚Äù In The Thirty-Fourth AAAI Conference on\nArtificial Intelligence, AAAI 2020, the Thirty-Second Innovative\nApplications of Artificial Intelligence Conference, IAAI 2020, the Tenth\nAAAI Symposium on Educational Advances in Artificial Intelligence, EAAI\n2020, New York, NY, USA, February 7-12, 2020, 6829‚Äì36. AAAI Press.\nhttps://aaai.org/ojs/index.php/AAAI/article/view/6163.\n\n\nZhao, Mark, and G. Edward Suh. 2018. ‚ÄúFPGA-Based Remote Power\nSide-Channel Attacks.‚Äù In 2018 IEEE Symposium on Security and\nPrivacy (SP), 229‚Äì44. IEEE; IEEE. https://doi.org/10.1109/sp.2018.00049.\n\n\nZhao, Yue, Meng Li, Liangzhen Lai, Naveen Suda, Damon Civin, and Vikas\nChandra. 2018. ‚ÄúFederated Learning with Non-IID Data.‚Äù\nArXiv Preprint abs/1806.00582 (June). http://arxiv.org/abs/1806.00582v2.\n\n\nZhou, Bolei, Yiyou Sun, David Bau, and Antonio Torralba. 2018.\n‚ÄúInterpretable Basis Decomposition for Visual Explanation.‚Äù\nIn Proceedings of the European Conference on Computer Vision\n(ECCV), 119‚Äì34.\n\n\nZhou, Chuteng, Fernando Garcia Redondo, Julian B√ºchel, Irem Boybat,\nXavier Timoneda Comas, S. R. Nandakumar, Shidhartha Das, Abu Sebastian,\nManuel Le Gallo, and Paul N. Whatmough. 2021.\n‚ÄúAnalogNets: Ml-hw\nCo-Design of Noise-Robust TinyML Models and Always-on\nAnalog Compute-in-Memory Accelerator.‚Äù https://arxiv.org/abs/2111.06503.\n\n\nZhou, Peng, Xintong Han, Vlad I. Morariu, and Larry S. Davis. 2018.\n‚ÄúLearning Rich Features for Image Manipulation Detection.‚Äù\nIn 2018 IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 1053‚Äì61. IEEE. https://doi.org/10.1109/cvpr.2018.00116.\n\n\nZhu, Hongyu, Mohamed Akrout, Bojian Zheng, Andrew Pelegris, Anand\nJayarajan, Amar Phanishayee, Bianca Schroeder, and Gennady Pekhimenko.\n2018. ‚ÄúBenchmarking and Analyzing Deep Neural Network\nTraining.‚Äù In 2018 IEEE International Symposium on Workload\nCharacterization (IISWC), 88‚Äì100. IEEE; IEEE. https://doi.org/10.1109/iiswc.2018.8573476.\n\n\nZhu, Ligeng, Lanxiang Hu, Ji Lin, Wei-Ming Chen, Wei-Chen Wang, Chuang\nGan, and Song Han. 2023. ‚ÄúPockEngine:\nSparse and Efficient Fine-Tuning in a Pocket.‚Äù In\n56th Annual IEEE/ACM International Symposium on\nMicroarchitecture. ACM. https://doi.org/10.1145/3613424.3614307.\n\n\nZhuang, Fuzhen, Zhiyuan Qi, Keyu Duan, Dongbo Xi, Yongchun Zhu, Hengshu\nZhu, Hui Xiong, and Qing He. 2021. ‚ÄúA Comprehensive Survey on\nTransfer Learning.‚Äù Proceedings of the IEEE 109 (1):\n43‚Äì76. https://doi.org/10.1109/jproc.2020.3004555.\n\n\nZoph, Barret, and Quoc V. Le. 2016. ‚ÄúNeural Architecture Search\nwith Reinforcement Learning,‚Äù November, 367‚Äì92. https://doi.org/10.1002/9781394217519.ch17.",
    "crumbs": [
      "RIFERIMENTI",
      "Riferimenti"
    ]
  },
  {
    "objectID": "contents/labs/labs.it.html",
    "href": "contents/labs/labs.it.html",
    "title": "LABORATORI",
    "section": "",
    "text": "Questa pagina √® stata lasciata intenzionalmente vuota.",
    "crumbs": [
      "LABORATORI"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/nicla_vision.it.html",
    "href": "contents/labs/arduino/nicla_vision/nicla_vision.it.html",
    "title": "Nicla Vision",
    "section": "",
    "text": "Prerequisiti",
    "crumbs": [
      "Nicla Vision"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/nicla_vision.it.html#prerequisiti",
    "href": "contents/labs/arduino/nicla_vision/nicla_vision.it.html#prerequisiti",
    "title": "Nicla Vision",
    "section": "",
    "text": "Nicla Vision Board: Si deve avere la scheda Nicla Vision.\nCavo USB: Per collegare la scheda al computer.\nRete: Con accesso a Internet per scaricare il software necessario.",
    "crumbs": [
      "Nicla Vision"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/nicla_vision.it.html#setup",
    "href": "contents/labs/arduino/nicla_vision/nicla_vision.it.html#setup",
    "title": "Nicla Vision",
    "section": "Setup",
    "text": "Setup\n\nSetup di Nicla Vision",
    "crumbs": [
      "Nicla Vision"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/nicla_vision.it.html#esercizi",
    "href": "contents/labs/arduino/nicla_vision/nicla_vision.it.html#esercizi",
    "title": "Nicla Vision",
    "section": "Esercizi",
    "text": "Esercizi\n\n\n\nModalit√†\nAttivit√†\nDescrizione\nLink\n\n\n\n\nVisione\nClassificazione delle Immagini\nImparare a classificare le immagini\nLink\n\n\nVisione\nRilevamento degli Oggetti\nImplementare il rilevamento degli oggetti\nLink\n\n\nSuono\nIndividuazione delle Parole Chiave\nEsplorare i sistemi di riconoscimento vocale\nLink\n\n\nIMU\nClassificazione del Movimento e Rilevamento delle Anomalie\nClassifica i Dati di Movimento e Rileva le Anomalie\nLink",
    "crumbs": [
      "Nicla Vision"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/xiao_esp32s3.it.html",
    "href": "contents/labs/seeed/xiao_esp32s3/xiao_esp32s3.it.html",
    "title": "XIAO ESP32S3",
    "section": "",
    "text": "Prerequisiti",
    "crumbs": [
      "XIAO ESP32S3"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/xiao_esp32s3.it.html#prerequisiti",
    "href": "contents/labs/seeed/xiao_esp32s3/xiao_esp32s3.it.html#prerequisiti",
    "title": "XIAO ESP32S3",
    "section": "",
    "text": "XIAO ESP32S3 Sense Board: Si deve avere la scheda XIAO ESP32S3 Sense.\nUSB-C Cable: Per collegare la scheda al computer.\nRete: Con accesso a Internet per scaricare il software necessario.\nSD Card e SD card Adapter: Per salvare audio e immagini (opzionale).",
    "crumbs": [
      "XIAO ESP32S3"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/xiao_esp32s3.it.html#setup",
    "href": "contents/labs/seeed/xiao_esp32s3/xiao_esp32s3.it.html#setup",
    "title": "XIAO ESP32S3",
    "section": "Setup",
    "text": "Setup\n\nSetup della XIAO ESP32S3",
    "crumbs": [
      "XIAO ESP32S3"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/xiao_esp32s3.it.html#esercizi",
    "href": "contents/labs/seeed/xiao_esp32s3/xiao_esp32s3.it.html#esercizi",
    "title": "XIAO ESP32S3",
    "section": "Esercizi",
    "text": "Esercizi\n\n\n\nModalit√†\nAttivit√†\nDescrizione\nLink\n\n\n\n\nVisione\nClassificazione delle immagini\nImpara a classificare le immagini\nLink\n\n\nVisione\nRilevamento di Oggetti\nImplementa il rilevamento oggetti\nLink\n\n\nSuono\nIndividuazione Parole Chiave\nEsplora sistemi di riconoscimento vocale\nLink\n\n\nIMU\nClassificazione del Movimento e Rilevamento Anomalie\nClassifica i dati di movimento e rileva anomalie\nLink",
    "crumbs": [
      "XIAO ESP32S3"
    ]
  },
  {
    "objectID": "contents/labs/raspi/raspi.it.html",
    "href": "contents/labs/raspi/raspi.it.html",
    "title": "Raspberry Pi",
    "section": "",
    "text": "Prerequisiti",
    "crumbs": [
      "Raspberry Pi"
    ]
  },
  {
    "objectID": "contents/labs/raspi/raspi.it.html#prerequisiti",
    "href": "contents/labs/raspi/raspi.it.html#prerequisiti",
    "title": "Raspberry Pi",
    "section": "",
    "text": "Raspberry Pi: Si deve avere almeno una delle seguenti schede: Raspberry Pi Zero 2W, Raspberry Pi 4 o 5 per Vision Labs e Raspberry 5 per GenAi Lab.\nAdattatore di Alimentazione: Per alimentare le schede.\n\nRaspberry Pi Zero 2-W: 2,5 W con un adattatore Micro-USB\nRaspberry Pi 4 o 5: 3,5 W con un adattatore USB-C\n\nRete: Con accesso a Internet per scaricare il software necessario e controllare le schede da remoto.\nScheda SD (minimo 32 GB) e un Adattatore per Schede SD: Per il sistema operativo Raspberry Pi.",
    "crumbs": [
      "Raspberry Pi"
    ]
  },
  {
    "objectID": "contents/labs/raspi/raspi.it.html#setup",
    "href": "contents/labs/raspi/raspi.it.html#setup",
    "title": "Raspberry Pi",
    "section": "Setup",
    "text": "Setup\n\nConfigurazione di Raspberry Pi",
    "crumbs": [
      "Raspberry Pi"
    ]
  },
  {
    "objectID": "contents/labs/raspi/raspi.it.html#esercizi",
    "href": "contents/labs/raspi/raspi.it.html#esercizi",
    "title": "Raspberry Pi",
    "section": "Esercizi",
    "text": "Esercizi\n\n\n\nModalit√†\nAttivit√†\nDescrizione\nLink\n\n\n\n\nVisione\nClassificazione delle Immagini\nImparare a classificare le immagini\nLink\n\n\nVisione\nRilevamento degli Oggetti\nImplementare il rilevamento degli oggetti\nLink\n\n\nGenAI\nSmall Language Models\nDeploy SLMs at the Edge\nLink",
    "crumbs": [
      "Raspberry Pi"
    ]
  },
  {
    "objectID": "contents/labs/shared/shared.it.html",
    "href": "contents/labs/shared/shared.it.html",
    "title": "Lab Condivisi",
    "section": "",
    "text": "I lab in questa sezione coprono argomenti e tecniche applicabili a diverse piattaforme hardware. Questi laboratori sono progettati per essere indipendenti da schede specifiche, consentendo di concentrarsi sui concetti fondamentali e sugli algoritmi utilizzati nelle applicazioni (tiny) ML.\nEsplorando questi ‚Äúshared lab‚Äù, si otterr√† una comprensione pi√π approfondita delle sfide e delle soluzioni comuni nell‚Äôapprendimento automatico embedded. Le conoscenze e le competenze acquisite qui saranno preziose indipendentemente dall‚Äôhardware specifico con cui lavorer√† in futuro.\n\n\n\nEsercizio\nNicla Vision\nXIAO ESP32S3\n\n\n\n\nKWS Feature Engineering\n‚úî Link\n‚úî Link\n\n\nBlocco delle Feature Spettrali DSP\n‚úî Link\n‚úî Link",
    "crumbs": [
      "Lab Condivisi"
    ]
  }
]